[
    {
        "title": "Prompt Gradient Projection for Continual Learning"
    },
    {
        "review": {
            "id": "2O07fbdGGk",
            "forum": "EH2O3h7sBI",
            "replyto": "EH2O3h7sBI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes three steps to derive the prompt gradient projection (PGP) approach for continual learning of pre-trained models with prompt tuning. First, using the self-attention mechanism in ViT, the work deduces the gradient restriction conditions for prompt learning based on the feature and the prompt vectors. Second, to simply the solving of these conditions, the work uses a sum space of the feature and the prompt vectors and conducts Singular Value Decomposition (SVD) on this space. Finally, PGP exploits balancing plasticity and stability by rearranging the singular values split on the basis of a threshold."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Given the rising popularity of using pre-trained models for downstream tasks, this is a timely topic to study how such models can be adapted for effective CL.\n\n- Clear explanations and equations built up from scratch.\n- Reasonable experimental settings."
                },
                "weaknesses": {
                    "value": "- Given that the L2P and DualPrompt backbones have been pre-trained on ImageNet, the limited performance gain on the 10-Split-TinyImageNet is a bit concerning (Table 3). Also, the gains on other dataset settings follow a similar trend. Alternatively, the work could have used a backbone trained on a different dataset altogether [1] and then evaluated its performance on the said settings. A third option would be to use other CL datasets like CUB-200, ObjectNet, etc. that have limited domain and style overlap with the ImageNet dataset.\n- The work claims that one of the major advantages of the proposed PGP method is its training time and memory cost. However, Table 2 does not report the training time of the baseline (L2P-R). As such, it is unclear what conclusions can be derived about the training time.\n- What concerns me is the scope of the proposed PGP method. While the authors use the classic L2P and DualPrompt as their baselines, it would have been more interesting to see how the proposed method complements the continual learning of more powerful pre-trained models such as vision-language models [2]. Also, the baselines considered in this paper are rather weak. Many latest methods are missing. Such as LoRA, K-Adapter, and other parameter-expansion methods.\n\nMinor comments:\n\n- Definition of V_t missing - the paper introduces it as a two part column vector (before eq. 11) without explaining what does it contain. The same for U_t (prior to eq. 10).\n\nReferences:\n\n[1] Zhou, Da-Wei et al. \u201cLearning without Forgetting for Vision-Language Models.\u201d ArXiv abs/2305.19270 (2023): n. pag.\n\n[2] Thengane, Vishal G. et al. \u201cCLIP model is an Efficient Continual Learner.\u201d ArXiv abs/2210.03114 (2022): n. pag."
                },
                "questions": {
                    "value": "Please see the weaknesses. Overall, it is a good incremental contribution to the field of prompt-tuning for continual learning. However, I am worried about the limited efficacy of the proposed method as well as the scope of the reported experimental baselines. Therefore, I cannot recommend an acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv",
                        "ICLR.cc/2024/Conference/Submission618/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636716935,
            "cdate": 1698636716935,
            "tmdate": 1700544838818,
            "mdate": 1700544838818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xCYiMDTZrj",
                "forum": "EH2O3h7sBI",
                "replyto": "2O07fbdGGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LEcv For Weakness 1"
                    },
                    "comment": {
                        "value": "Dear reviewer LEcv, thanks for your valuable suggestions. Here are our responses:\n\nFor **Weakness 1** about *''use a backbone trained on a different dataset and use other CL datasets''*:\n\nAs your suggestion, we re-evaluated the effectiveness of our method by **extending two distinct pre-trained models**, namely **ViT-DINO** and **ViT-SAM**. The results are shown in the Table below. Additionally, we tested our method on the **CUB-200** dataset based on three ViT pre-trained backbones: ImageNet-1k, DINO, and SAM, further validating the effectiveness of our method on non-ImageNet datasets.\n\n**Table 1**: Comparison to distinct pretrained-dataset backbones with/without prompt gradient projection.\n|                          |                    | 10-Split-CIFAR100                                    | 10-Split-CIFAR100                                     | 5-Split-CUB200                                       | 5-Split-CUB200                                      |\n| ------------------------ | ------------------ | ---------------------------------------------------- | ----------------------------------------------------- | ---------------------------------------------------- | --------------------------------------------------- |\n| Method                   | Pretrained-Dataset | Accuracy                                             | Forgetting                                            | Accuracy                                             | Forgetting                                          |\n| L2P                      | DINO[1]            | 67.35                                                | 9.69                                                  | 44.10                                                | 9.77                                                |\n| **L2P-PGP(Ours)**        | **DINO**           | **70.60*****(+3.25)*** | **4.73*****(-4.96)***   | **44.80*****(+0.70)***                     | **6.06*****(-3.71)*** |\n| DualPrompt               | DINO               | 64.18                                                | 23.87                                                 | 50.88                                                | 10.10                                               |\n| **DualPrompt-PGP(Ours)** | **DINO**           | **73.33*****(+9.15)*** | **10.27*****(-13.60)*** | **51.03*****(+0.15)***                     | **9.06*****(-1.04)*** |\n| L2P                      | SAM[2]             | 83.93                                                | 6.68                                                  | 73.98                                                | 6.77                                                |\n| **L2P-PGP(Ours**         | **SAM**            | **84.26*****(+0.33)***                     | **5.64*****(-1.04)***   | **76.45*****(+2.55)*** | **5.91*****(-0.86)***                     |\n| DualPrompt               | SAM                | 86.11                                                | 6.08                                                  | 82.02                                                | 4.73                                                |\n| **DualPrompt-PGP(Ours)** | **SAM**            | **86.92*****(+0.81)***                     | **5.04*****(-1.04)***   | **82.28*****(+0.26)***                     | **4.65*****(-0.08)***                     |\n| L2P                      | ImageNet-1K        | 83.77                                                | 6.63                                                  | 74.88                                                | 5.39                                                |\n| **L2P-PGP(Ours)**        | **ImageNet-1K**    | **84.34*****(+0.57)***                               | **5.59*****(-1.04)***   | **75.15*****(+0.27)***                     | **4.51*****(-0.88)***                     |\n| DualPrompt               | ImageNet-1K        | 86.50                                                | 5.77                                                  | 82.02                                                | 4.23                                                |\n| **DualPrompt-PGP(Ours)** | **ImageNet-1K**    | **86.92*****(+0.42)***                     | **5.35*****(-0.42)***                       | **82.46*****(+0.44)***                     | **3.76*****(-0.47)***                     |\n\n[1] DINO pre-trained models (https://arxiv.org/abs/2104.14294)\n\n[2] SAM pre-trained models (https://arxiv.org/abs/2106.01548)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488955291,
                "cdate": 1700488955291,
                "tmdate": 1700488955291,
                "mdate": 1700488955291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NLTOwbH6aH",
                "forum": "EH2O3h7sBI",
                "replyto": "2O07fbdGGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LEcv For Weakness 2"
                    },
                    "comment": {
                        "value": "For **Weakness 2** about *''the training time consumption''*:\n\nTo answer your question, we reconducted experiments about L2P with data rehearsal on the 10-Split-CIFAR100 dataset. we randomly sampled the exemplars and achieved a result akin to the report results in the paper. Then, we measured the time consumption of our reproduced L2P-R and compared it with our method **as shown in Table 2**. The time consumption of *our method* is **0.756 h** and the time consumption of *L2P-R* is **0.787 h**. The reason why our method is faster than L2P-R, we hold the view is that L2P-R needs to train the extra rehearsal data in every epoch, which spends a lot of time. The more rehearsal data is saved, the more training time is consumed. However, our method is rehearsal-free.\n\n**Table 2**: Time Complexity Comparison, Accuracy, Forgetting between our approach and L2P, L2P-R. L2P-R indicates L2P with data rehearsal.\n| Method            | Exemplar | Time        | Accuracy  | Forgetting |\n| ----------------- | -------- | ----------- | --------- | ---------- |\n| L2P               | 0        | 0.74 h      | 83.77     | 6.63       |\n| L2P-R             | 1000     | 0.787 h     | 84.21     | 7.72       |\n| **L2P-PGP(Ours)** | **0**    | **0.756 h** | **84.34** | **5.59**   |\n\nTo sum up, our method not only **exhibits superior performance** and **requires less storage memory** but also **consumes less training time**."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489298371,
                "cdate": 1700489298371,
                "tmdate": 1700489298371,
                "mdate": 1700489298371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Altc7tzJ3Q",
                "forum": "EH2O3h7sBI",
                "replyto": "2O07fbdGGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LEcv For Weakness 3"
                    },
                    "comment": {
                        "value": "For **Weakness 3** about *''the scope of the proposed PGP method''*:\n\nTo further validate our method on the vision-language model, we introduce it to the **CLIP** model. The experimental setting is that, in the vision side, we only set a single trainable image prompt shared by all tasks. As for the text side, we follow the operation as [3], we set trainable text prompt for each class, which is only trained at the corresponding task. We conduct our experiments on two datasets: *10-Split-CIFAR100* and *10-Split-TinyImageNet*, **as shown in Table 3**. Besides that, we also verify the effectiveness of our method under the task incremental setting, **as shown in Table 4**. Results show that our method has improved the performance a lot for all the above settings, which we think proves that our method is also **useful in the vision-language models and further enlarges the scope of our method**.\n\nWe have noticed that, compared with your mentioned method, our method does not exhibit comparable accuracy performance, despite conducting experiments on entirely different datasets with [4]. The reasons are as follows: **Firstly**, in comparison to its **complex method and network structure**, we employ a **very simple setting**, utilizing only a single image prompt, which restricts the performance of our method. **Secondly**, it **employs the data rehearsal method** to further enhance performance. However, our method is **rehearsal-free**.\n\nIn our design, we want to exclude all of the external influential factors and only verify the PGP method. Thus, we choose the simplest experiment setting of a single image prompt. Experiments show that our method works well.\n\n**Table 3**: Comparison to *CLIP* model without/with gradient projection method under **class incremental setting** (**without** task identifier in the test phase).\n|                    | 10-Split-CIFAR100 | 10-Split-CIFAR100 | 10-Split-TinyImageNet | 10-Split-TinyImageNet |\n| ------------------ | ----------------- | ----------------- | --------------------- | --------------------- |\n| Models             | Accuracy          | Forgetting        | Accuracy              | Forgetting            |\n| CLIP               | 58.95             | 6.29              | 56.28                 | 7.19                  |\n| **CLIP-PGP(ours)** | **63.72*****(+4.77)*** | **5.09*****(-1.20)***  | **60.34*****(+4.06)***     | **6.21*****(-0.98)***      |\n\n**Table 4**: Comparison to *CLIP* model without/with gradient projection method under **task incremental setting** (**with** task identifier in the test phase).\n| Models             | Accuracy               | Forgetting            |\n| ------------------ | ---------------------- | --------------------- |\n| CLIP               | 92.69                  | 2.34                  |\n| **CLIP-PGP(ours)** | **93.00*****(+0.31)*** | **1.58*****(-0.76)*** |\n\nAs for the latest methods, such as LoRA, K-Adapter, and other parameter-expansion methods. Since they mainly focus on fine-tuning a large model, whose goal is to adapt the model to the down-stream tasks, we are afraid this is beyond the scope of this paper, which aims to learn new concepts while not forgetting the prompts. Also, the combination of gradient projection and fine-tuning methods would be interesting. We would like to study this in the future.\n\n[3] Zhou, Kaiyang, et al. \"Learning to prompt for vision-language models.\" International Journal of Computer Vision 130.9 (2022): 2337-2348.\n\n[4] Zhou, Da-Wei et al. \u201cLearning without Forgetting for Vision-Language Models.\u201d ArXiv abs/2305.19270 (2023): n. pag."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490317137,
                "cdate": 1700490317137,
                "tmdate": 1700490317137,
                "mdate": 1700490317137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KbQUdWJG0o",
                "forum": "EH2O3h7sBI",
                "replyto": "2O07fbdGGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LEcv For Minor Comments"
                    },
                    "comment": {
                        "value": "For **minor comments** about *''Definition of V_t and U_t missing''*:\n\nThanks for your valuable suggestions. We have **added the missing definition of** ***V_t*** **and** ***U_t***."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490654146,
                "cdate": 1700490654146,
                "tmdate": 1700490654146,
                "mdate": 1700490654146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OOVI1zSce8",
                "forum": "EH2O3h7sBI",
                "replyto": "2O07fbdGGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LEcv For Questions"
                    },
                    "comment": {
                        "value": "To answer **your concerns about our method of the limited efficacy and the scope of the reported experimental baselines**, we conclude our method as follows:\n\n**(1)** **We are the first one to propose the preventing forgetting method from the view of prompt gradient**. As continual learning is not a fine-tuning task, the data is coming as a sequence of tasks, and data from previous tasks would not be seen in future tasks, which brings a heavy decline in performance of models. Thus, how to overcome catastrophic forgetting is the prime aim of continual learning. To the best of our known, **we are the first one to deduct the orthogonal condition to prevent forgetting for prompt-based methods and implement the gradient projection on prompt**. We believe that our method owns a huge potential, which is still uncovered. In future works, we will **further explore the potential of our method to achieve better results, and also release the code**.\n\n**(2)** We conduct our method on multiple and various models, *e,g, ViT, CLIP*, distinct prompt-tuning paradigms, *e,g, prompt-tuning, prefix-tuning*, different pre-trained backbones, *e.g. ImageNet-1K, SAM, and DINO*, under three continual learning settings: *class incremental learning, task incremental learning, and online class incremental learning*. **All experiments show that our method can effectively reduce forgetting and improve the average accuracy**. Besides that, **our method is a plug-in, and can serve in different prompt-based continual learning methods**. Especially nowadays, as the tendency of large models become hotter and hotter, we think **our method can have a promising application future**.\n\n**(3)** We prove that **our method has little time and memory consumption** through experiments. Besides that, our method does out introduce other models or save any exemplars. Based on these facts, we believe that our method can have advantages in training speed and memory, which **could be beneficial when transferring our method to large vision foundation models (VFMs)**.\n\n**(4)** Finally, **we reveal the mechanism of trade-off between the plasticity and stability in the perspective of prompt gradient, and well balance the plasticity and stability of model by adjusting the threshold *\\epsilon***, which brings a new insight as the above trade-off dilemma is a core problem in continual learning."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491522456,
                "cdate": 1700491522456,
                "tmdate": 1700491522456,
                "mdate": 1700491522456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fCGwKd8QxS",
                "forum": "EH2O3h7sBI",
                "replyto": "OOVI1zSce8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. In light of the additional experimental results and explanations of the work's scope, I have decided to increase my rating to 6 (from 3)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545009387,
                "cdate": 1700545009387,
                "tmdate": 1700545009387,
                "mdate": 1700545009387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YBjx4vnTnt",
            "forum": "EH2O3h7sBI",
            "replyto": "EH2O3h7sBI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission618/Reviewer_epe6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission618/Reviewer_epe6"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Prompt Gradient Projection (PGP), a novel approach that combines Prompt Tuning with Gradient Projection to address the challenge of forgetting in continual learning. Prompt Tuning reduces forgetting in class-incremental learning by selecting and updating relevant prompts based on input samples, while Gradient Projection prevents forgetting in task-incremental learning by ensuring gradient updates in orthogonal directions to old features. PGP integrates these techniques, releasing the need for task identifiers in Gradient Projection and providing theoretical guarantees against forgetting. By deducing the orthogonal condition for anti-forgetting in prompt gradients and using Singular Value Decomposition for efficient computation, PGP significantly reduces forgetting and improves accuracy in various learning settings, achieving state-of-the-art results on benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a novel approach, Prompt Gradient Projection (PGP), which combines Prompt Tuning and Gradient Projection to tackle the issue of forgetting in continual learning. The problem statement is well-defined, and related work is thoroughly reviewed, showcasing a strong understanding of the research landscape. The paper's strength lies in its extensive and insightful experimental studies, including ablation studies that highlight the efficiency of PGP and the influence of various factors. Furthermore, the inclusion of proofs, extensive experiments, and detailed algorithms in the appendix enhances the paper's credibility and reproducibility."
                },
                "weaknesses": {
                    "value": "The clarity and readability of the current paper are areas of concern. The paper's overall structure and writing could be improved to enhance its accessibility. Several specific issues were identified:\n\nReadability and Clarity: The paper's overall readability and clarity could be enhanced. Specific instances of unclear language were noted, such as in the abstract and the last paragraph of page 3. Further revisions are needed to make these sections more lucid.\n\nFigure 2: Figure 2 was found to be unclear and challenging to follow, especially for readers seeking to grasp the method's essence. Consider revising or providing additional explanatory details to improve the comprehensibility of this figure.\n\nProof Presentation: While the proof on page 5 is well-explained up to equation 12, the subsequent section, where multiple elements are combined, is challenging to follow. Expanding on the last two paragraphs on page 5 could enhance clarity and understanding.\n\nTerminology and Consistency: In page 6, there is mention of dividing $V_t$ into $V_{t,1}$ and $V_{t,2}$\" while also referencing $V_{t,0}$\". Clarification is needed to ensure consistency and prevent potential confusion, possibly by referring back to the explanation on page 5.\n\nNotation Clarification: On page 6, there is a reference to \"$V_{t,}$\" which requires clarification to make it more understandable to the reader. Providing a clear definition or context for this notation would be beneficial.\n\nOverall, addressing these issues will significantly improve the paper's accessibility and enhance its overall quality, making it more suitable for publication."
                },
                "questions": {
                    "value": "The paper raises several important points that require clarification and further exploration:\n\nReference Request: In the paragraph preceding equation 1, the statement \"If the update direction is orthogonal to the old features, it follows that $\\Delta W_t x_{t,i}=0$\" lacks a specific reference. It would be beneficial to provide a reference or additional context to support this claim and enhance the paper's credibility.\n\nTime Complexity: The paper would benefit from a more detailed discussion of the method's time complexity, along with a comparative analysis against relevant baseline methods. Understanding the computational efficiency of the proposed approach in relation to other methods is crucial for assessing its practical utility.\n\nPerformance vs. Time Complexity Trade-off: The paper mentions a maximum improvement of around 1% in reducing catastrophic forgetting in the reported experimental results. It is essential to provide a more in-depth discussion of the trade-off between the promising performance of the proposed algorithm and its associated time complexity. Explaining why this algorithm should be prioritized over other existing algorithms, considering the modest improvement, would provide valuable insights.\n\nFigure 3 Clarification: Figure 3 requires further elaboration. While it is presented that both algorithms exhibit similar patterns, with Dual-Prompt appearing to be shifted up(accuracy) or down (forgetting) by a certain scale, a more detailed explanation of this observation is necessary. Clarifying the significance of these patterns and the implications for the proposed method's effectiveness is essential.\n\nAddressing these concerns will contribute to a clearer and more comprehensive understanding of the paper's content and its contributions to the field.\n\n================================\nI appreciate the authors for offering a thorough rebuttal! \nI find this paper to be intriguing and innovative, leading me to raise my score to 8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Reviewer_epe6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663042084,
            "cdate": 1698663042084,
            "tmdate": 1700545903692,
            "mdate": 1700545903692,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "peyI2fRRCl",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Weakness 1"
                    },
                    "comment": {
                        "value": "Dear reviewer epe6, thanks for your valuable suggestions. Here are our responses:\n\nFor **Weakness 1** about *''Readability and Clarity''*:\n\nWe have rewritten the abstract and the last paragraph of page 3, and further revised several sections of the paper where language ambiguity exists. We thank your suggestion which makes our paper clearer and more readable than before. The following are two examples of revised version:\n\n*For Abstract*\n\nBefore revised: Prompt-tuning has demonstrated impressive performance in continual learning by querying relevant prompts for novel classes training.\n\n**After revised**: Prompt-tuning has demonstrated impressive performance in continual learning by querying relevant prompts for each input instance, which can avoid the introduction of task identifier.\n\nBefore revised: In PGP, we deduce the orthogonal condition for prompt gradient via the self-attention mechanism in vision-transformer.\n\n**After revised**: In PGP, we deduce that reaching the orthogonal condition for prompt gradient can effectively prevent forgetting via the self-attention mechanism in vision-transformer.\n\n*For the last paragraph of page 3*\n\nBefore revised: one limitation of gradient projection methods is which is only applicable for task incremental learning but will fail in the class-incremental inference, as the projected gradient is a strict constraint for novel classes training.\n\n**After revised**: one limitation of gradient projection methods fail in the class-incremental inference is that the projected gradient needs task identifier to find relevant network parameters."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483145258,
                "cdate": 1700483145258,
                "tmdate": 1700483145258,
                "mdate": 1700483145258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FG6uRvcs3c",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Weakness 2"
                    },
                    "comment": {
                        "value": "For **Weakness 2** about *''Figure 2''*:\n\nWe have **enhanced the clarity** of *Figure 2* by providing a more detailed explanation in the caption, elucidating the content and essence of our method, and **further improving the comprehensibility** of the image."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483274065,
                "cdate": 1700483274065,
                "tmdate": 1700483274065,
                "mdate": 1700483274065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "adTGt01f9M",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Weakness 3"
                    },
                    "comment": {
                        "value": "For **Weakness 3** about *''Proof Presentation''*:\n\nWe have **expanded** the content in the *last two paragraphs* of *page 5*, providing a more compact connection with the preceding and following sections. This adjustment is intended to enhance the clarity and comprehensibility of the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483377535,
                "cdate": 1700483377535,
                "tmdate": 1700483377535,
                "mdate": 1700483377535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKovnQiYyb",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Weakness 4 and Weakness 5"
                    },
                    "comment": {
                        "value": "For **Weakness 4** about *''Terminology and Consistency''*:\n\nIn the revised version, we have **replaced** *V_t, V_1, and V_2* with **new symbolic representations** and **added detailed explanatory text** to ensure consistency with the preceding context and eliminate any potential confusion.\n\nFor **Weakness 5** about *''Notation Clarification''*:\n\nThank you for your question. It has been **addressed along with the previous one**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483596719,
                "cdate": 1700483596719,
                "tmdate": 1700483596719,
                "mdate": 1700483596719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QGhzeTwEKi",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Question 1"
                    },
                    "comment": {
                        "value": "For **Question 1** about *''Reference Request''*:\n\nWe have inserted the following two pertinent references into the statement location as advised.\n\n[1] Saha, Gobinda, Isha Garg, and Kaushik Roy. \"Gradient Projection Memory for Continual Learning.\" International Conference on Learning Representations. 2020.\n\n[2] LiLin, Sen, et al. \"TRGP: Trust Region Gradient Projection for Continual Learning.\" International Conference on Learning Representations. 2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483705751,
                "cdate": 1700483705751,
                "tmdate": 1700483705751,
                "mdate": 1700483705751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6KGwMbbLTB",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Question 2"
                    },
                    "comment": {
                        "value": "For **Question 2** about *''Time Complexity''*:\n\nThanks for pointing out our omission.  \n\nUnder the same hardware platform, experimental setting, and backbone, we compared our method with the original L2P and L2P with data rehearsal on the *10-Split-CIFAR100* dataset. As L2P does not mention detailed information about how to make data rehearsal, we adopted a simple data rehearsal process which randomly sampled a set of exemplars and achieved a result similar to the reported ones.\n\nThe results indicate that, in comparison with the *original L2P*, our approach reduces forgetting by 1.04%, improves accuracy by 0.57%, and *extends* training time by *0.016* hours. In comparison with **L2P with data rehearsal**, our method not only reduces forgetting by 2.13%, improves accuracy by 0.13%, but also **shortens** training time by **0.031** hours.\n\n**Table 1**: Time Complexity Comparison between our approach and L2P, L2P-R. L2P-R indicates L2P with data rehearsal.\n| Method            | Exemplar | Accuracy  | Forgetting | Time        |\n| ----------------- | -------- | --------- | ---------- | ----------- |\n| L2P               | 0        | 83.77     | 6.63       | 0.74 h      |\n| L2P-R             | 1000     | 84.21     | 7.72       | 0.787 h     |\n| **L2P-PGP(Ours)** | **0**    | **84.34** | **5.59**   | **0.756 h** |\n\nFor further analysis, we use **\u201ctime.time()\u201d** function to coarse read the time consumption in each sub-process.\n\n**For data rehearsal**, it concludes in three steps: **1. Collect exemplars (4\\*10-5~26\\*10-5 s)**, which can be ignored. **2. Store exemplars (Simplified in our reimplement method)**. **3. Exemplars join in the training process**. We mainly discuss the time consumption in the third step: In the batch size of 20, one iteration that only exemplars take part in costs a mean time of: 0.196 s. Here, we set epochs as 5 in the experiment. Thus, on the 10-Split-CIFAR100, training with exemplars needs extra (5+10+15+20+25+30+35+40+45) * 5 = 1125 iterations and costs 1125 * 0.196 = **220.50 s**. To sum up, theoretical analysis shows that data rehearsal would add an extra **220.50 s** in the training process.\n\n**For our method**, it also concludes three steps: **1. Collect samples (10-6\\*33\\*5\\*10 s), which can be ignored**. **2. Calculate the projection matrix**, consumption time is (10.65 + 8.78 + 9.42 + 8.72 + 8.68 + 8.68 + 8.53 + 8.81 + 8.72 + 8.69 = 81 s). 3. **Gradient projection process**, consumption time is (2.79 + 3.22 + 2.95 + 2.73 + 3.11 + 3.28 + 2.72 + 3.01 + 2.78 + 2.98 = 29.57 s). To sum up, theoretical analysis shows that gradient projection would add an extra 81 + 29.57 = **110.57 s** in the training process.\n\nNotice that, we set 5 epochs in the training process. If we increase the epoch number, the consumption time of L2P-R would also increase due to more iterations of training exemplars. However, our method is unrelated to the epoch number and can be free of increasing epochs."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484552228,
                "cdate": 1700484552228,
                "tmdate": 1700484608050,
                "mdate": 1700484608050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fWNGIypg00",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Question 3"
                    },
                    "comment": {
                        "value": "For **Question 3** about *''Performance vs. Time Complexity Trade-off''*:\n\nThanks for your valuable suggestions.\n\n(1) Discussion of the promising performance of the proposed algorithm and its associated time complexity:\n\n**i)** For Time: In the answer to time complexity, we have shown detailed information about time consumption in our method. In fact, both the time consumption on calculation of projection matrix and gradient projection process is little, only adding **2%** extra training time on the original baseline.\n\n**ii)** For Performance: Our approach demonstrates a high generalization ability, could be combined with the almost prompt-based continual learning methods. Besides that, from the perspective of gradient projection, we further realize a well-done balance of the model's plasticity and stability. This balance is crucial for continual learning, causing a favorable balance often results in enhanced performance.\n\n(2) Explaining why this algorithm should be prioritized over other existing algorithms:\n\nTo the best of our knowledge, we are the first to propose the gradient projection for reducing forgetting in the context of prompt-tuning. Note that the tendency of vision foundation model has recently caused significant changes in computer vision. Our approach first studies the gradient of prompt-learning and we believe related applications, like fine-tuning a VFM, could benefit from our paper. **We have additionally tested the large vision foundation model for continual learning, and the results turn out our approach is also effective in this scenario. Please kindly refer to Table 2**. Here, for the vision side, we set a single trainable image prompt shared by each task. For the text side, we follow the operation with [3], we set trainable text prompt for each class, only trained at the related task.\n\n**Table 2**: Comparison to *CLIP* model without/with gradient projection method on 10-Split-CIFAR100 with **class incremental setting** (**without** task identifier in the test phase).\n|                    | 10-Split-CIFAR100  | 10-Split-CIFAR100 | 10-Split-TinyImageNet | 10-Split-TinyImageNet |\n| ------------------ | ------------------ | ----------------- | --------------------- | --------------------- |\n| Models             | Accuracy           | Forgetting        | Accuracy              | Forgetting            |\n| CLIP               | 58.95              | 6.29              | 56.28                 | 7.19                  |\n| **CLIP-PGP(ours)** | **63.72*****(+4.77)*** | **5.09*****(-1.20)*** | **60.34*****(+4.06)***    | **6.21*****(-0.98)***     |\n\nHere, we also compare our method with other **two non-gradient and prompt-based methods**, one is data rehearsal, and the other is a paper titled \"Introducing Language Guidance in Prompt-based Continual Learning\", referred to as **LGCL**, presented at ICCV\u201923 [4]. LGCL is slightly lower than our method but with complicated designs, **as shown in Table 3**. Compared with baseline and IGCL, we have found we own the following three advantages.\n\n**1.** For memory, we only need to store a tiny projection matrix. While for data rehearsal, saving exemplars would spend far more memory space than us. For LGCL, although it is a rehearsal-free method, it has to introduce another network, a pre-trained text encoder: CLIP L/14 into the method, which also needs to occupy a significant amount of memory space.\n\n**2.** For time, extra time in our method, mainly costs on calculation of projection matrix and gradient projection process, which is shorter than the retraining process of exemplars in the data rehearsal method, explained in the answer of time complexity. For LGCL, due to the non-open source code, we cannot reproduce the experiments. However, in the paper, it adds an additional text encoding process and adopts extra loss functions to train the model. Therefore, compared to LGCL, our method also exhibits shorter time complexity.\n\n**3.** Our method could serve as a plug-in to reduce forgetting of the models in almost all of the prompt-based continual learning methods. With the hot tendency of the large models nowadays, our method could be transferred to the large Vision Foundation Models (VFMs) and have a promising application future in the fine-tuning paradigm.\n\n**Table 3**: Comparison to Accuracy, Forgetting of L2P-LGCL and L2P-PGP.\n|                   | 10-Split-CIFAR100 | 10-Split-CIFAR100 |\n| ----------------- | ----------------- | ----------------- |\n| Method            | Accuracy          | Forgetting        |\n| L2P-LGCL          | 84.33\u00b10.06        | 5.83\u00b10.23         |\n| **L2P-PGP(Ours)** | **84.34\u00b10.08**    | **5.59\u00b10.05**     |\n\n[3] Zhou, Kaiyang, et al. \"Learning to prompt for vision-language models.\" International Journal of Computer Vision 130.9 (2022): 2337-2348.\n\n[4] Khan, Muhammad Gul Zain Ali, et al. \"Introducing Language Guidance in Prompt-based Continual Learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486792244,
                "cdate": 1700486792244,
                "tmdate": 1700486792244,
                "mdate": 1700486792244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uif0GxPSd3",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer epe6 For Question 4"
                    },
                    "comment": {
                        "value": "For **Question 4** about *''Figure 3 Clarification''*:\n\nOur explanation for this issue is as follows: Firstly, as you can see, Figure 3 is set in the background of online class-incremental learning, strictly limiting each image appearing only once. That is to say, each task is only trained for one epoch. The limited number of epochs results in fewer gradient update processes and fewer gradient projection processes, leading to **insufficient gradient projection**. Here, we emphasize that it is precisely due to the insufficient gradient projection that the phenomenon of \"Dual-Prompt appearing to be shifted up (accuracy) or down (forgetting) by a certain scale\" occurs.\n\nOur rationale is that each projection serves as a correction to the direction of gradient, thereby reducing forgetting. The more projections, the more sufficient the correction, and the more stable and effective the mitigation of forgetting. Conversely, fewer projections lead to insufficient correction, causing the mitigation of forgetting to be highly unstable, oscillating randomly between good and bad states of suppressing forgetting. The term *\"a certain scale\"* represents this **manifestation of random oscillation**. To demonstrate our conclusion, we conducted experiments under two settings: **epoch=1** and **epoch=3** on the same *10-Split-TinyImageNet* dataset, **as shown in Table 5 below**:\n\nThe **\"Diff\"** in the Table represents **the difference between FOR (forgetting) and ACC (accuracy) after projection and before projection**. It can be observed that, at *epoch=1*, **the differences in ACC and FOR across different tasks fluctuate more noticeably, aligning with the oscillating process described in the theoretical explanation, showing an oscillating pattern between good and bad states**. However, at *epoch=3*, **the differences in ACC and FOR increase steadily with the increasing number of tasks, and the variations become highly stable**.\n\n**Table 5**: Comparison to the differences in ACC(Diff_ACC), the differences in FOR(Diff_FOR) at different tasks under settings of distinct epoch.\n| Task    | Epoch=1  | Epoch=1  | Epoch=3  | Epoch=3  |\n| ------- | -------- | -------- | -------- | -------- |\n| Task=1  | Diff_ACC | Diff_FOR | Diff_ACC | Diff_FOR |\n| Task=2  | 0.45     | 0.90     | 1.40     | 0.3      |\n| Task=3  | 1.40     | 0.83     | 1.45     | 0.52     |\n| Task=4  | 1.00     | 1.70     | 1.52     | 0.66     |\n| Task=5  | 1.34     | 1.08     | 1.65     | 0.90     |\n| Task=6  | 1.06     | 0.90     | 1.65     | 0.88     |\n| Task=7  | 0.55     | 0.53     | 1.70     | 0.89     |\n| Task=8  | 0.72     | 0.70     | 1.73     | 0.87     |\n| Task=9  | 0.94     | 0.91     | 1.80     | 1.14     |\n| Task=10 | 0.78     | 0.64     | 1.94     | 1.26     |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487419076,
                "cdate": 1700487419076,
                "tmdate": 1700487419076,
                "mdate": 1700487419076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vC1nQhmxBt",
                "forum": "EH2O3h7sBI",
                "replyto": "YBjx4vnTnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Reviewer epe6"
                    },
                    "comment": {
                        "value": "Thank you for your diligent review of our paper. With your valuable comments and suggestions, especially for the Questions about paper revision, we have significantly improved the clarity, fluency, and details of our paper. We sincerely appreciate you and your advice."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571518435,
                "cdate": 1700571518435,
                "tmdate": 1700571518435,
                "mdate": 1700571518435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dL2f4kGC90",
            "forum": "EH2O3h7sBI",
            "replyto": "EH2O3h7sBI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes to combine prompt-tuning based continual learning methods with gradient projection methods. The submission explains how to project the gradient of the learnable prompt tokens and prompt key such that there is no forgetting. The evaluation is performed on 3 datasets and various different continual learning setups."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Combining prompt tuning with gradient projection and therefore gaining understanding of the learnable prompts' gradient space in context of CL is novel and important\n- The gradient projection derivation is sound and clearly explained.\n- Evaluation is done on multiple datasets and settings, and the proposed method shows superiority against competitors such as L2P and DualPrompt."
                },
                "weaknesses": {
                    "value": "- The comparison of prompt-tuning based CL methods with other CL methods does not seem fair to me. Prompt-tuning based methods use pretrained backbones which will give an performance edge when comparing against non-prompt-tuning based CL methods.\n\n=========== Post-rebuttal changes ========\n- The authors have resolved my concern on this particular issue."
                },
                "questions": {
                    "value": "- Figure 1 is a bit unintuitive as normally, models that fill the radar chart are considered to be more powerful but in this case for FOR metrics being close to the center is better and for ACC metrics being fuller is better. Changing the FOG figure so higher is better would make the Figure more readable in my opinion.\n\n- From what I could understand, the gradients for the learnable prompt tokens and prompt keys in the prompt pool are projected. What about gradients for the classifier attached to the backbone? Are these modified as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816052816,
            "cdate": 1698816052816,
            "tmdate": 1700698833470,
            "mdate": 1700698833470,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3mBmLKUnY7",
                "forum": "EH2O3h7sBI",
                "replyto": "dL2f4kGC90",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q1LH For Weakness 1"
                    },
                    "comment": {
                        "value": "Dear reviewer Q1LH, thanks for your valuable suggestions. Here are our responses:\n\nFor **Weakness 1** about *''The comparison of prompt-tuning based CL methods with other CL methods''*:\n\n**Thanks for the advice. The backbone is actually pre-trained on ImageNet, and therefore it is somewhat unfair to compare with non-prompt-tuning based CL methods.** \n\n**However**, the tendency of the pre-trained large models, or vision foundation model has recently caused significant changes in computer vision. This paper focuses on reducing the forgetting of prompt-tuning paradigm, which, we believe, would bring new insights for continual learning. **We have additionally tested a lot of other baselines, especially including large vision foundation model, and the results turn out our approach is also effective in this scenario. Please kindly refer to Table 1.** Here, the methods in Table 1 are from 1) Replacing the pre-trained backbone of ImageNet-1K in L2P and DualPrompt with SAM and DINO. 2) Introducing prompt gradient projection method to CLIP model. For the vision side, we set a single trainable image prompt shared by each task. For the text side, following the operation with [1], we set trainable text prompt for each class, which is only trained at the corresponding task.\n\n**For the unfair comparison, please kindly note that:** **1)**, in our baseline (L2P and DualPrompt  [2, 3]), they have already compared their methods (adopting pre-trained backbones) with non-prompt-tuning-based CL methods (e,g. LwF, EWC, BiC, DER). We just simply follow their existing practice. We would denote the backbone comparison in our revised paper. **2)**, Our goal is to analyze the gradient space for prompt-tuning-based CL methods and propose a gradient projection method that effectively reduces forgetting. Therefore, our focus is mainly on observing performance improvements compared to the baselines (L2P and DualPrompt), non-prompt-tuning based CL methods are used as references. \n\n**Table 1:** Comparison to distinct baselines with/without prompt gradient projection method on 10-Split-CIFAR100.\n\n| Models               | Pre-trained backbone | Accuracy                                | Forgetting                               |\n| -------------------- | -------------------- | --------------------------------------- | ---------------------------------------- |\n| L2P                  | DINO[4]              | 67.35                                   | 9.69                                     |\n| **L2P-PGP(ours)**    | **DINO**             | **70.60*****(+3.25)*** | **4.73*****(-4.96)***   |\n| L2P                  | SAM[5]               | 83.93                                   | 6.68                                     |\n| **L2P-PGP(ours)**    | **SAM**              | **84.26*****(+0.33)***                        | **5.64*****(-1.04)***   |\n| DualPrompt           | DINO                 | 64.18                                   | 23.87                                    |\n| **DualPrompt(ours)** | **DINO**             | **73.33*****(+9.15)*** | **10.27*****(-13.60)*** |\n| DualPrompt           | SAM                  | 86.11                                   | 6.08                                     |\n| **DualPrompt(ours)** | **SAM**              | **86.92*****(+0.81)***                        | **5.04*****(-1.04)***   |\n| CLIP                 | -                    | 58.95                                   | 6.29                                     |\n| **CLIP-PGP(ours)**   | **-**                | **63.72*****(+4.77)*** | **5.09*****(-1.20)***   |\n\n[1] Zhou, Kaiyang, et al. \"Learning to prompt for vision-language models.\" International Journal of Computer Vision 130.9 (2022): 2337-2348.\n\n[2] Wang, Zifeng, et al. \"Learning to prompt for continual learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[3] Wang, Zifeng, et al. \"Dualprompt: Complementary prompting for rehearsal-free continual learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[4] DINO pre-trained models (https://arxiv.org/abs/2104.14294).\n\n[5] SAM pre-trained models (https://arxiv.org/abs/2106.01548)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481881261,
                "cdate": 1700481881261,
                "tmdate": 1700482349259,
                "mdate": 1700482349259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wie64w6tbo",
                "forum": "EH2O3h7sBI",
                "replyto": "dL2f4kGC90",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q1LH For Question 1"
                    },
                    "comment": {
                        "value": "For **Question 1** about *''Figure 1 is a bit unintuitive as normally''*:\n\nWe have **reconfigured** *Figure 1* according to your suggestion and made it more intuitive. We appreciate your valuable feedback which has enhanced the quality of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482185359,
                "cdate": 1700482185359,
                "tmdate": 1700482400672,
                "mdate": 1700482400672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Pug1Honbz",
                "forum": "EH2O3h7sBI",
                "replyto": "dL2f4kGC90",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q1LH For Question 2"
                    },
                    "comment": {
                        "value": "For **Question 2** about *''modified with gradients for the classifier attached to the backbone''*:\n\nIn fact, in prompt-tuning CL, all follow a **special configuration** in the classifier. The details are as follows:\nThis framework employs a unified classifier, using the 10-Split-CIFAR-100 dataset as an example. In the first task (including classes from 0 to 9), we train the classifier, and obtain the weights on dimensions of 0-9. In the second task (including classes from 10 to 19), we **freeze** the weights on dimensions of 0-9 in the classifier and continue to train the weights on dimensions of 10-19. We do the same thing in the subsequent tasks. It is obvious that, in this setup, the classifier weights on dimensions of the old classes are not updated **(frozen)**. Therefore, **no gradients are involved**, eliminating the need for gradient update.\n\nTherefore, prompt-tuning CL, **eliminates the need for any projection or alteration of gradients in the classifier**. We have some new results when adopting prototype-based classifiers, but this is beyond the scope of this paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482693723,
                "cdate": 1700482693723,
                "tmdate": 1700482693723,
                "mdate": 1700482693723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5vwW3tUJEU",
                "forum": "EH2O3h7sBI",
                "replyto": "3mBmLKUnY7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I feel satisfied with the response and will raise the score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698778888,
                "cdate": 1700698778888,
                "tmdate": 1700698778888,
                "mdate": 1700698778888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]