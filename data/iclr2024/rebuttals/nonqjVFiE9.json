[
    {
        "title": "Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All"
    },
    {
        "review": {
            "id": "x2k6XdUHNK",
            "forum": "nonqjVFiE9",
            "replyto": "nonqjVFiE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_G1mq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_G1mq"
            ],
            "content": {
                "summary": {
                    "value": "This paper performed an in-depth analysis of the state-of-the-art CODA-Prompt for continual learning of pre-trained ViT. The authors attributed the problem as the mismatch in prompt representation between training and testing and feature shifting during inference. The authors then proposed Key-Query orthogonal projection to reduce dependence of old task queries on new task keys and introduced a prototype-based OVA loss to complement the Key-Query orthogonality. The proposed method achieves a surprisingly high performance,  even much higher than the joint training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The analysis of CODA-Prompt is very extensive, and the identified issues are reasonable. In particular, I appreciate the discussion about the different effects of training samples and test samples.\n\n2. The proposed method has strong motivation, which directly targets the identified issues of CODA-Prompt.\n\n3. The proposed method achieves a surprisingly high performance over widely used benchmarks."
                },
                "weaknesses": {
                    "value": "1. My major concern is the surprisingly high performance of KOPPA, which is even significantly higher than the joint training performance by more than 10%. Since the joint training usually serves as the upper bound of continual learning, I would suggest the authors to provide an in-depth analysis and explanation of this abnormal phenomenon.\n\n2. From Table 3, the outstanding performance of KOPPA seems to be largely due to the OVA. The contribution of Key-Query orthogonal projection in Sec. 3.3 seems to be marginal. \n\n3. From Table 5, the performance improvements of KOPPA rely heavily on the number of prototypes. Although the authors have evaluated the effect of CE and OVA in Table 4, how about using the preserved prototypes to compute CE rather than OVA (i.e., identical to regular feature replay for continual learning)?"
                },
                "questions": {
                    "value": "Please refer to the weakness. \n\nBesides, I would encourage the authors to discuss and compare with more advanced prompt-based baselines, such as [1] and [2]. While it is not required, such a discussion could bring this paper to a more advanced position, especially when the results of this paper seem to be far superior to [1] and [2].\n\n[1] Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality, NeurIPS 2023.\n\n[2] RanPAC: Random Projections and Pre-trained Models for Continual Learning, NeurIPS 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698068776510,
            "cdate": 1698068776510,
            "tmdate": 1699636873782,
            "mdate": 1699636873782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l0KKsNs8LB",
                "forum": "nonqjVFiE9",
                "replyto": "x2k6XdUHNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer\u2019s detailed and constructive comments and suggestions. In the following, we provide the main response to your concerns:\n\n**1. \"My major concern is the surprisingly high performance of KOPPA, which is even significantly higher than the joint training performance by more than 10%. Since the joint training usually serves as the upper bound of continual learning, I would suggest the authors to provide an in-depth analysis and explanation of this abnormal phenomenon.\"**\n\nThe reported results of JOINT were obtained from the paper CODA. We would like to convince you that:\n\n- JOINT is better than other baselines but we should not always consider JOINT as their upper bound. \n\n    - *Why JOINT is better than other baselines?*\n            \n       JOINT employs a training strategy where the model learns from data of all tasks simultaneously, treating them as a single task. Thus, JOINT exhibits two key advantages over other baselines: (i) no forgetting or feature shifts, and (ii) the model learns the relationships between observed classes, aligning the corresponding features effectively and minimizing misclassification. This might have been the reason why JOINT has higher results than the remaining baselines.\n\n     - *Why we should not consider JOINT as the upper bound of other baselines?*\n\n         *(i)* From the code released by CODA's authors, we found that JOINT is trained on a single task, with data from all tasks. Specifically, the pre-trained model and a classification head are fine-tuned without any additional parameters such as prompts. Therefore, basically, the design of the backbone in JOINT and the other prompt-based incremental learning methods mentioned is completely different. \n\n        *(ii)* Moreover, taking S-CIFAR-100 as an example, while the JOINT's model must learn to classify 100 different classes simultaneously, that of CODA's (or DualP or L2P's) only needs to learn to classify 10 classes per task. Therefore, when comparing JOINT with baselines in such a small task, JOINT may be outperformed by these baselines in terms of performance. If these methods can effectively avoid forgetting and can utilize task ID information, the final average accuracy $A_N$ can approach the average of accuracies of those small tasks which are learned independently. In other words, it is possible for JOINT to be surpassed.\n                    \n\n- The main reasons that KOPPA surpasses JOINT by such a large margin?\n        \n    *Firstly*, KOPPA's strategies enable the model to achieve the two advantages mentioned above (like JOINT) over other baselines: (i) reducing forgetting better by the proposed key-query strategy, (ii) making use of task ID information when using the OVA-based module in prediction. *Secondly*, by sequentially learning sub-tasks, KOPPA can achieve higher accuracy on each of these small tasks than JOINT (on one task of a bigger dataset). Combining the above arguments, we'd like to convince you that KOPPA surpassing JOINT is reasonable.\n\n    In addition, we would like to provide experimental evidences. Specifically, table below shows the accuracy of each task (small task) of KOPPA on S-CIFAR-100. Moreover, the corresponding triggering rate is 100% for all tasks, as shown in Figure 3 of the main paper. This implies that the final average accuracy obtained is the average of these 'sub-accuracies,' which amounts to 97.99% \u2014 higher than JOINT's 89.3%.\n    \n     | Task | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n     |--------|---|----|---|---|----|---|---|----|---|-----|\n     |Accuracy | 99.11 | 97.78 | 97.44 | 98.11 | 98.22 | 97.78 | 96.22 | 97.44 | 98.56 | 99.22 |"
                    },
                    "title": {
                        "value": "Response to Reviewer G1mq (Part 1)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309540826,
                "cdate": 1700309540826,
                "tmdate": 1700437724348,
                "mdate": 1700437724348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kiuswcghyc",
                "forum": "nonqjVFiE9",
                "replyto": "x2k6XdUHNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**2. \"From Table 3, the outstanding performance of KOPPA seems to be largely due to the OVA. The contribution of Key-Query orthogonal projection in Sec. 3.3 seems to be marginal.\"**\n\n   We would like to clarify that our primary aim is to minimize feature shifts rather than pursuing a general accuracy improvement. The effectiveness of this strategy is evident in the results presented in Table 2 and Figure 2 (main paper), demonstrating the superior ability to avoid shifts compared to CODA. Moreover, the outcomes in the table below highlight that enhanced shift avoidance plays a crucial role in enabling KOPPA to achieve superior results over other baselines, particularly when combined with our proposed OVA-based module, especially on Domainnet and S-Imagenet-10 with gaps greater than 3 %. Therefore, we believe that the reduction of mismatch and semantic drift through key-query orthogonal projection constitutes a noteworthy contribution.\n\n   | Methods | S-CIFAR-100 | S-Imagnet-R-5 | S-Imagnet-R-10 | S-Imagnet-R-20 | DomainNet |\n   |--------------------------|---------------------------------|-----------------------------------|------------------------------------|------------------------------------|-------------------------------|\n   | Deep L2P++ + OVA         | 95.53 \u00b1 0.83       | 84.86 \u00b1 0.39                      | 89.23 \u00b1 0.77                       | 91.92 \u00b1 0.94                       | 80.01 \u00b1 0.54                  |\n   | DualP + OVA              | 96.06 \u00b1 0.75                    | 85.28 \u00b1 0.55                      | 88.11 \u00b1 0.82                       | 92.13 \u00b1 0.84             | 79.83 \u00b1 0.52                  |\n   | CODA + OVA               | 96.88 \u00b1 0.74                    | 85.32 \u00b1 0.51                      | 88.02 \u00b1 0.65                       | 92.10 \u00b1 0.98           | 79.76 \u00b1 0.55                  |\n   | KOPPA                    | **97.82 \u00b1 0.80**             | **86.02 \u00b1 0.42**             | **91.09 \u00b1 1.53**              | **92.89 \u00b1 1.2** | **84.14 \u00b1 0.62**         |\n\n**3. \"From Table 5, the performance improvements of KOPPA rely heavily on the number of prototypes. Although the authors have evaluated the effect of CE and OVA in Table 4, how about using the preserved prototypes to compute CE rather than OVA (i.e., identical to regular feature replay for continual learning)?\"**\n\n  | Methods    | S-CIFAR-100     | S-Imagnet-R-5          | S-Imagnet-R-10         | S-Imagnet-R-20    |\n  |---------|------------|------------|-----------|-----------|\n  | OVA + CE (Ours)| **97.82 \u00b1 0.80** | **86.02 \u00b1 0.42** | **91.09 \u00b1 0.53** | **92.89 \u00b1 1.25** |\n  | Just CE               | 86.28 \u00b1 0.81 | 76.32 \u00b1 0.45 | 75.62 \u00b1 0.43 | 72.42 \u00b1 1.20 |\n  | CE \u2020 + CE           | 94.71 \u00b1 0.85 | 85.08 \u00b1 0.51 | 90.02 \u00b1 0.54 | 90.92 \u00b1 0.88 |\n\nAs you suggestion, we have conducted extended experiments. The results presented in the table above show that our method achieve better performance than the solution which use CE instead of OVA."
                    },
                    "title": {
                        "value": "Response to Reviewer G1mq (Part 2)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309707311,
                "cdate": 1700309707311,
                "tmdate": 1700437743016,
                "mdate": 1700437743016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8lp4LkTvAV",
                "forum": "nonqjVFiE9",
                "replyto": "x2k6XdUHNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G1mq (Part 3)"
                    },
                    "comment": {
                        "value": "**4. \"Besides, I would encourage the authors to discuss and compare with more advanced prompt-based baselines,...\"**\n\nThank you for your suggestion. Among methods using the same settings and pretrained ViT as the backbone, HiDE and RanPAC stand out as the two latest state-of-the-art (SOTA) methods, both featuring interesting ideas and impressive results.\n\n- First, we would like to discuss the methodologies.\n\n  - HiDE introduces a technique for effective representation learning, employing a contrastive regularization strategy in the form of hierarchical constraints. This approach leverages both instructed and uninstructed representations, thereby enhancing the quality of the prompt-based continual learning (CL) model. Similar to KOPPA, HiDE preserves information from old tasks through feature vector encoding to apply constraints when learning new tasks to improve prediction. However, HiDE does not address the issue of feature shifts: uninstructed representations might inadvertently select an incorrect task_id. This can lead to the combination of incorrect prompts with the pre-trained backbone, resulting in uncontrolled representations, which could negatively affect the final classification quality.\n\n  - Unlike HiDE and KOPPA, which are categorized under the strategy of Prompting in transformer networks, RanPAC belongs to the Class-Prototype (CP) accumulation category. RanPAC offers a comprehensive and insightful analysis of CP-based methods and introduces a solution involving a Random-Projection (RP) layer with frozen, untrained weights. This layer enhances the latent space from pre-trained models significantly. However, relying solely on the pre-trained model may hinder the model's adaptability and its ability to learn specific knowledge from new tasks, potentially limiting the method's effectiveness.\n\n- In the next, we would like to provide experimental results comparing these two methods with KOPPA. The results, illustrated in the table below, demonstrate KOPPA's superiority over these two innovative methods.\n\n  |Methods | S-CIFAR-100 |\tS-Imagnet-R-5 |\tS-Imagnet-R-10 |\tS-Imagnet-R-20|\n  | --- | ---- | --- | --- | --- |\n  | RanPAC | 92.20 | 79.90 | 77.90 | 74.50 |\n  | HiDE | 93.02 | 77.82 | 77.12 | 75.03 |\n  | KOPPA | **97.82** | **86.02** | **91.09** | **92.89** |\n\n\n\n  Please note: The RanPAC results were directly obtained from the original paper, while the HiDE results were reproduced using the same pretrained backbone as KOPPA and RanPAC. We have added the discussion about these two work to the revised version.\n\n----------------------------------\n*We sincerely hope that our answers have met your expectations and satisfactorily addressed your inquiries. Please let us know if you would like us to do anything else.*\n\n*Best regards,*\n\n*Authors*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339569825,
                "cdate": 1700339569825,
                "tmdate": 1700437757754,
                "mdate": 1700437757754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b3Jg89TnFQ",
                "forum": "nonqjVFiE9",
                "replyto": "x2k6XdUHNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for taking the time to read and review our submission. We would be happy to address any remaining questions before the discussion period ends today.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667075036,
                "cdate": 1700667075036,
                "tmdate": 1700667075036,
                "mdate": 1700667075036,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6tuW1QT25i",
            "forum": "nonqjVFiE9",
            "replyto": "nonqjVFiE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
            ],
            "content": {
                "summary": {
                    "value": "The paper claimed that current prompt selection methods suffer from the mismatch in prompt representation between training and testing and feature shifting during inference. To this end, the authors proposed a MAML-inspired method to ensure an almost certain perpendicular constraint between future keys and past task queries, effectively eliminating feature shifts. The proposed method also contains a prototype-based One-Versus-All (OVA) component to boost the task classification head distinction. The proposed method shows accuracy much higher than joint training (upper bound of continual learning, using all the data for training)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem that this paper is trying to solve is well-motivated\n- The overall flow of the paper is easy to follow.\n- The proposed method shows very good empirical results."
                },
                "weaknesses": {
                    "value": "- Some parts of the paper are unclear and confusing to me. Please refer to the Question section. \n- There could be mistakes in some calculations.  Please refer to the Question section. \n- The authors did not mention the limitations of their method and potential future work. The paper does not explore or discuss potential failure cases of the proposed methods. Understanding when and why the methods might fail is crucial.\n- typos\n   - \"alpha if corresponding weight vector\" \"if\" should be \"is\" right?\n   - \"It also mitigates the chance that the query q(x\u2032) is a past task uses the prompt\" two verbs in one sentence"
                },
                "questions": {
                    "value": "- It's unclear how S_i is obtained and how Q^t is update from Q^{t-1}. Could the authors elaborate on them? \n- What's the size of Q. Do we keep one Q for all tasks or one Q per task?\n- \"It also mitigates the chance that the query q(x\u2032) is a past task uses the prompts of the current/future tasks, hence the prompts P^t\nfor the task t have more contribution to the prompts P_x of example x in this t\". This is about testing or training? if it\u2019s about training, we don\u2019t see old task sample x\u2019; if it\u2019s about testing, P^t\u2019s contribution to x only depends on q(x) and K^t right? Chance of q(x') use P^t does not impact P^t contribution to x?\n- It seems OVA is the key to the performance boost. In Table 3, the authors provided CODA + OVA. Is it possible for the authors to provide the performance of other baselines + OVA?\n- \"might trigger wrong task classification heads\". CIL only has one prediction head right? \n- the prototype sizes N \u00d7 T \u00d7 d (100 \u00d7 20 \u00d7 768)  should be multiplied by 4bytes (float), thus the size is around 6.1MB and image net image size is 224x224x3x 1bytes (uint8). So, it should be 40 images instead of 10, as stated in the paper, right? ACIFAR image is 32x32x3x1 = 3kb. For 10 tasks, the storage of the prototypes is the same as the storage of 1k images right?\n- The proposed method KOPPA outperforms JOINT by a large margin. JOINT is supposed to be the upper bound of CL. What's the main reason that KOPPA surpasses JOINT by such a large margin?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789775593,
            "cdate": 1698789775593,
            "tmdate": 1699636873673,
            "mdate": 1699636873673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lmeJodmPZI",
                "forum": "nonqjVFiE9",
                "replyto": "6tuW1QT25i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer\u2019s detailed and constructive comments and suggestions. In the following, we provide the main response to your questions and comments:\n\n**1. It's unclear how $\\mathcal{S}^i$ is obtained and how $\\mathcal{Q}^t$ is update from $\\mathcal{Q}^{t-1}$. Could the authors elaborate on them?**\n\nIn our model, we denote $\\mathcal{S}^t$ as the subspace containing the query vectors of task $t$, and $\\mathcal{Q}^{t} = \\bigcup_{i=1}^{t} \\mathcal{S}^i$ as the subspace spanned by query vectors from task 1 to the task $t$. We do not calculate $\\mathcal{S}^t$ directly; instead, we obtain $\\mathcal{Q}^{t}$ from $\\mathcal{Q}^{t-1}$ as follows:\n\nWe use matrix $\\mathbf{Q}^t$ to represent the subspace $\\mathcal{Q}^{t}$, which is spanned by a set of query vectors from task 1 to task $t$, and this matrix is used to constraint $\\mathbf{K}^{t +1}$ when learning task $t+1$. $\\mathbf{Q}^{t}$ was computed by using SVD to choose the most representative bases. Specifically:\n- For $t=1$, let $\\mathbf{R}^0 = [q(\\mathbf{x}^0_1), q(\\mathbf{x}^0_2), ...]$ be the matrix with column vectors as query vectors of task 0, where $\\mathbf{x}^t_i$ is i-th sample of task $t$. We perform SVD on $\\mathbf{R}^0 = \\mathbf{U_0 \\Sigma_0 (V_0)^T}$ and then an orthogonal basis of size $k$ is chosen to form $\\mathbf{R}^0_k$ as the *k-rank approximation* such that $\\left\\| \\mathbf{R}^0_k  \\right\\|^2_F \\geq \\epsilon \\left\\| \\mathbf{R}^0  \\right\\| ^2_F $, given the threshold $\\epsilon \\in (0, 1)$. Finally, matrix $\\mathbf{Q}^0 = [u_1^0, ..., u_k^0]$ is formed with the columns being the selected basis vectors.\n- For $t > 1$, let $\\mathbf{R}^t = [\\mathbf{Q}^{t-1}, q(\\mathbf{x}^t_1), q(\\mathbf{x}^t_2), ...]$. \nBy performing SVD and $k$-rank approximation on $\\mathbf{R}^t$, we obtain $\\mathbf{Q}^{t}$.\n    \n In short, to calculate matrix  $\\mathbf{Q}^{t}$ of subspace $\\mathcal{Q}^{t}$, we only need  $\\mathbf{Q}^{t-1}$ and a small set of query vectors from task t *(200 samples)*. We have added this description in Appendix A.2 in the revision.\n\n**2. What's the size of Q. Do we keep one Q for all tasks or one Q per task?**\n\n In our implementation, the maximum size of $\\mathbf{Q}$ is $768 \\times 768$ because the latent space has a dimension of 768. And we only need to maintain matrix $\\mathbf{Q} = \\mathbf{Q^{t-1}}$ to constrain $\\mathbf{K^{t}}$ when learning task $t>1$. Max size of Q in experiments provided via this link: https://ibb.co/GcxztRk\n\n**3. \"\"It also mitigates the chance that the query q(x\u2032) is a past task uses the prompts of the current/future tasks, hence the prompts P^t for the task t have more contribution to the prompts P_x of example x in this t\". This is about testing or training? ...\"**\n\nThank you for your comment. The correct version is: \"It also mitigates the chance that the query $q(\\mathbf{x})$ of a past task $t$ uses the prompts of future tasks $j>t$ (because $\\gamma (q(\\mathbf{x}), \\mathbf{K}^j) \\rightarrow 0$), hence the prompts $\\mathbf{P}^t$ of the task $t$ might have more contribution to the prompts $\\mathbf{P_x}$ of example $\\mathbf{x}$ of that task\". \n\nThis refers to the **testing time**. We would like to explain more thoroughly as follows: \n\n  - During training, query vectors of task t only interact with keys from task 1 to task t, and the prompt $\\mathbf{P_x}$ corresponding to sample x is synthesized from the prompts up to task t. However, after learning new tasks we have new prompts and corresponding keys. Therefore, when testing after that, the model needs to consider and synthesize $\\mathbf{P_x}$ based on all old and new prompts. This can result in an uncontrolled $\\mathbf{P_x}$, as it is synthesized from prompt components $\\mathbf{P^j}$ (of tasks $j>t$) that were not observed during the training task t, leading to uncontrolled shifts in the latent vectors of x. To mitigate this, we introduce a constraint technique that forces the interaction coefficient $\\gamma (q(\\mathbf{x}), \\mathbf{K}^j) \\rightarrow 0$, limiting the influence of $\\mathbf{P}^j$ on $\\mathbf{P_x}$ .\n\n  - What we mean in the above statement is:\n\n    Given $\\mathbf{P}_{\\mathbf{x}} = \\sum_i {\\alpha}_i \\mathbf{P}^i$,  where $\\alpha_i = \\gamma (q(\\mathbf{x}), \\mathbf{K}^j)$, \n\n    Although alpha is the coefficient of $\\mathbf{P^i}$ when synthesizing $\\mathbf{P_x}$ and depends solely on $\\gamma (q(\\mathbf{x}), \\mathbf{K}^j) \\rightarrow 0$, considering it from the perspective of the contribution proportion of each component $\\mathbf{P^i}$, we view $\\frac{\\alpha_i}{\\sum_{k} \\alpha_k}$ as the contribution coefficient of prompt $\\mathbf{P}^i$ to the establishment of prompt $\\mathbf{P_x}$. Thus if $\\alpha_{j > t} \\rightarrow 0$, then $\\frac{\\alpha_t}{\\sum_{k} \\alpha_k}$ become bigger, and the prompts $\\mathbf{P}^t$ of task $t$ might have more contribution to the prompt $\\mathbf{P_x}$ of example $\\mathbf{x}$."
                    },
                    "title": {
                        "value": "Response to Reviewer WdvC (Part 1)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304545319,
                "cdate": 1700304545319,
                "tmdate": 1700562459906,
                "mdate": 1700562459906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YXdWgjcdYl",
                "forum": "nonqjVFiE9",
                "replyto": "6tuW1QT25i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**4. \"might trigger wrong task classification heads\". CIL only has one prediction head right?**\n\nWe would like to explain more clearly as follows:\n- We all have known that in CIL setting for classification tasks, we usually use a common prediction head. However, when we consider that the classification head can be divided into subheads corresponding to tasks, then in a model where feature shift happens, there is a risk of incorrectly triggering these task-specific classification heads.\n- Taking the experiment on S-CIFAR100-10 as an example, we have an output of size 100 for 100 classes/10 tasks, which is returned from the common head $h_\\theta$. At that time, we can consider sub-head $h^1_\\theta$ for task 1, which returns the first 10 entries of that output; $h^2_\\theta$ for task 2, which returns the next 10 entries of the output; and similarly for the remaining tasks. Assume that sub-heads $h^t_\\theta$ are only learned to fit with $f^c_{\\Phi, P}(\\mathbf{x}^{tr})$ of sample x of task t. Because $f^e_{\\Phi, P}(\\mathbf{x}^{te})$ is different from $f^c_{\\Phi, P}(\\mathbf{x}^{tr})$ due to feature shift, so that when it interact with these sub-heads, these heads can return uncontrollable output, leading to incorrect answers. That is, sample x from task t could yield the highest probability prediction at the class belonging to the subhead corresponding to task $j \\neq t$.\n\n We have revised the paper more comprehensively\n\n**5. Mistakes in some calculations: \"the prototype sizes N \u00d7 T \u00d7 d (100 \u00d7 20 \u00d7 768) should be multiplied by 4bytes (float)\"**\n\nThank you for your careful review and pointing out this oversight. We have updated these in the revised version.\nObviously, by the limited number of images for the ImageNet-R-derived datasets, achieving satisfactory results is extremely challenging. For CIFAR100, when using 1000 images/ 10 tasks for replay, we obtain the results presented in the table below, showing that replaying features is as effective as replaying raw data, in addition, replaying raw data using traditional learning methods with CE loss cannot replace the use of our OVA-based module.\n| Method| $A_N (\\uparrow)$| $F_N (\\downarrow)$|\n|--|--|--|\n|CE + OVA (Replay $\\mathbf{z}$) - Ours | **97.82 \u00b1 0.80** | **0.43 \u00b1 0.12** |\n|CE + OVA (Replay $\\mathbf{x}$ | 97.82 \u00b1 0.80|0.43 \u00b1 0.12|\n|CE (Replay $\\mathbf{x}$) | 93.25 \u00b1 0.73 |0.85 \u00b1 0.15|\n\n**6. It seems OVA is the key to the performance boost. In Table 3, the authors provided CODA + OVA. Is it possible for the authors to provide the performance of other baselines + OVA?**\n\nRegarding the role of our OVA-based module, we did additional experiments when applying OVA to other baselines. The results are given in the table below, showing that the baselines are impressively improved in performance when applying our OVA module. However, due to the effectiveness of the key-query orthogonal projection strategy, our method helps features avoid shifting better, thereby achieving superior results than other baselines, especially on DomainNet and S-Imagenet-10 with gaps greater than 3 \\%.\n\n| Methods | S-CIFAR-100 | S-Imagnet-R-5 | S-Imagnet-R-10 | S-Imagnet-R-20 | DomainNet |\n|----------|----------------|-------------|--------------|---------|-------------|\n| Deep L2P++ + OVA         | 95.53 \u00b1 0.83 | 84.86 \u00b1 0.39  | 89.23 \u00b1 0.77   | 91.92 \u00b1 0.94  | 80.01 \u00b1 0.54                  |\n| DualP + OVA              | 96.06 \u00b1 0.75  | 85.28 \u00b1 0.55  | 88.11 \u00b1 0.82 | 92.13 \u00b1 0.84  | 79.83 \u00b1 0.52                  |\n| CODA + OVA               | 96.88 \u00b1 0.74   | 85.32 \u00b1 0.51    | 88.02 \u00b1 0.65  | 92.10 \u00b1 0.98  | 79.76 \u00b1 0.55                  |\n| KOPPA   | **97.82 \u00b1 0.80**  | **86.02 \u00b1 0.42**  | **91.09 \u00b1 1.53**  | **92.89 \u00b1 1.2** | **84.14 \u00b1 0.62**         |"
                    },
                    "title": {
                        "value": "Response to Reviewer WdvC (Part 2)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307306733,
                "cdate": 1700307306733,
                "tmdate": 1700568580555,
                "mdate": 1700568580555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TqPA8AaXcP",
                "forum": "nonqjVFiE9",
                "replyto": "6tuW1QT25i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WdvC (Part 3)"
                    },
                    "comment": {
                        "value": "**7. \"What's the main reason that KOPPA surpasses JOINT by such a large margin?...\"**\n\nPlease note that the reported results of JOINT were obtained from the paper CODA. We would like to give the reasons for this as:\n        \n- JOINT is better than other baselines, but we should not always consider JOINT as their upper bound.\n       \n    - *Why is JOINT better than other baselines?*\n\n        JOINT employs a training strategy where the model learns from data of all tasks simultaneously, treating them as a single task. Thus, JOINT exhibits two key advantages over other baselines: (i) no forgetting or feature shifts, and (ii) the model learns the relationships between observed classes, aligning the corresponding features effectively and minimizing misclassification. These might be the reasons why JOINT has higher results than the remaining baselines.\n\n    - *Why should we not consider JOINT as the upper bound of other baselines?*\n\n        *(i)* From the code released by CODA's authors, we found that JOINT is trained on a single task, with data from all tasks. Specifically, the pre-trained model and a classification head are finetuned without any additional parameters such as prompts. Therefore, basically, the design of the backbone in JOINT and the other prompt-based incremental learning methods mentioned is completely different. \n\n         *(ii)* In addtition, taking S-CIFAR-100 as an example, while the JOINT's model must learn to classify 100 different classes simultaneously, the CODA's (or DualP's, or L2P's) model only needs to learn to classify 10 classes per task. Therefore, when considering such a small task, JOINT may be outperformed by these baselines in terms of performance. If these methods effectively avoid forgetting and can utilize task ID information, the final average accuracy $A_N$ can approach the average of accuracies of that small tasks which are learned independently. That is, it is possible to overcome JOINT.\n\n-  The main reason that KOPPA surpasses JOINT by such a large margin?\n\n     *Firstly*, KOPPA's strategies enable the model to achieve the two advantages mentioned above (like JOINT) over other baselines: (i) reducing forgetting better by the proposed key-query strategy, (ii) making use of task ID information when using the OVA-based module in prediction. *Secondly*, by sequentially learning sub-tasks, KOPPA can achieve higher accuracy on each of these small tasks, compared to JOINT (on one task of a bigger dataset). Combining the above arguments, we want to convince you that KOPPA surpasses JOINT and it is completely reasonable.\n\n      In addition, we would like to provide experimental evidences. Specifically, table below shows the accuracy of each task (small task) of KOPPA on S-CIFAR-100. Moreover, the corresponding triggering rate is 100% for all tasks, as shown in Figure 3 of the main paper. This implies that the final average accuracy obtained is the average of these 'sub-accuracies,' which amounts to 97.99% \u2014 higher than JOINT's 89.3%.\n\n      | Task     | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     | 10    |\n      |---|----|---|---|---|---|---|---|---|---|---|\n      | Accuracy | 99.11 | 97.78 | 97.44 | 98.11 | 98.22 | 97.78 | 96.22 | 97.44 | 98.56 | 99.22 |\n\n**8. \"The authors did not mention the limitations of their method and potential future work...\".**\n\nThank you for your comment, about the limitations and potential future work, we would like to discuss some points as follows:\n\n   - Although this orthogonality constraint ensures (almost) no forgetting and as well as features shift, it could also hinder potential improvement of past tasks, i.e. positive backward transfer. Such cases may happen when the new task contains positively related knowledge for previous ones, hence model could learn prompts which are useful for them. Therefore, an interesting future work might be to decide when positive backward transfer may happen in the context of prompt-based CL. \n  - In addition, the results in Table 5 in the main paper show that the number of features used to train the OVA-module has a certain influence on the overall performance of the model. Therefore, in the future, we will consider replacing the use of raw data with effective yet flexible solutions that help the model operate more stably. \n  - Furthermore, when a higher level of data privacy is concerned such that no prototypes or latent features are stored, as they can be inverted to generate original images [1], our method with the OVA-based module cannot work.\n\n**9. About typos:** Thank you for your feedback. We have revised the paper more comprehensively!\n\n[1] Data-free knowledge distillation with soft targeted transfer set synthesis. In AAA!, 2021\n\n-----------\n*We sincerely hope that our answers have met your expectations and satisfactorily addressed your inquiries. Please let us know if you would like us to do anything else.*\n\n*Best regards,*\n\n*Authors*"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371934307,
                "cdate": 1700371934307,
                "tmdate": 1700479595538,
                "mdate": 1700479595538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "azqzz8Ob8g",
                "forum": "nonqjVFiE9",
                "replyto": "6tuW1QT25i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for taking the time to read and review our submission. We would be happy to address any remaining questions before the discussion period ends today.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667048524,
                "cdate": 1700667048524,
                "tmdate": 1700667048524,
                "mdate": 1700667048524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kHxVxEdxJL",
                "forum": "nonqjVFiE9",
                "replyto": "TqPA8AaXcP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_WdvC"
                ],
                "content": {
                    "title": {
                        "value": "Questions about JOINT"
                    },
                    "comment": {
                        "value": "What backbone do you use for JOINT? Is it the same backbone that you use for CL learner?"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672710429,
                "cdate": 1700672710429,
                "tmdate": 1700672710429,
                "mdate": 1700672710429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f7ZQ3IIh3k",
                "forum": "nonqjVFiE9",
                "replyto": "6tuW1QT25i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response.\n\nIn the code released by CODA's authors, **JOINT uses the same ViT backbone as in other prompt-based baselines, without prompts**, meaning that the model capacity of JOINT is less than that of ours and prompt-based baselines.\n\nTherefore, for each task, prompt-based methods only have to learn a small subset of all the classes using general knowledge from the pretrained ViT and using additional parameters (prompts) to adapt more effectively to this subset. As a result, they can obtain higher performance compared to JOINT, which just finetunes the pretrained backbone and classification head for all of classes at the same time.\n\n\n----------------\n*We hope that you can reconsider the review score. Please let us know if you would like us to do anything else.*"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676076344,
                "cdate": 1700676076344,
                "tmdate": 1700699245745,
                "mdate": 1700699245745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gIVI3VYpj4",
            "forum": "nonqjVFiE9",
            "replyto": "nonqjVFiE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_m1q8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_m1q8"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the groundwork established by CODA-Prompt, aiming to resolve two primary concerns identified within it: (1) the mismatch in prompt representation between training and testing examples, and (2) the erroneous activation of the task classification head. To address these challenges, the authors introduce a look-ahead orthogonal projection optimization process for the former and employ a one-versus-all loss function for the latter. However, a significant drawback of this paper lies in the absence of important details. Furthermore, the working mechanism of the proposed methods seems not aligned entirely with the claims made by the authors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research focus on continual learning for pre-trained models holds significant significance.\n- The analyses delving into the issues concerning CODA-Prompt are particularly intriguing."
                },
                "weaknesses": {
                    "value": "- Several vital details are missing, preventing a comprehensive evaluation of the proposed method.\n  * Specifically, how is $\\mathcal{Q}^t$ calculated? What is the specific sample size used for this calculation?\n  * How is $g_\\phi$ implemented? Is it similar to a cosine distance between the current prototype and previous prototypes?\n  * What are the hyperparameters employed, such as the prompt length and the prompt pool size? How does your method compare with CODA when these hyperparameters are adjusted?\"\n- The operational mechanism of the proposed method appears to diverge from the authors' assertions.\n  * Upon juxtaposing the results from Table 3 and Table 4, it becomes evident that the orthogonal projection component is minimally effective; the sole operational aspect is the One-Versus-All (OVA) loss, previously introduced by (Saito & Saenko, 2021). This observation is reasonable, given that the orthogonal constraint has already been applied in CODA, albeit between keys. It seems that the first identified issue has not been adequately addressed by the proposed method.\n  * Regarding the OVA loss, I have two hypotheses:\n    - The true effective component might be the prototypes of previous tasks, as indicated in Table 5. In this scenario, an additional ablation study, replacing $h_\\theta$ with a prototype-based classifier, is necessary. If successful, the unique contribution of this work, the OVA loss, may be rendered not valid any more.\n    - Its effectiveness could stem from the similarity between testing and training tasks, which allows for the identification of the most closely related and well-trained training task, thereby resolving the challenge. To validate this, additional experiments involving diverse data splits or datasets are essential to assess the practicality of the proposed methods beyond the current benchmarks.\"\n- The writing is disorganized with a lot of symbols randomly used, quite challenging to follow."
                },
                "questions": {
                    "value": "See the first weakness listed above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699524521457,
            "cdate": 1699524521457,
            "tmdate": 1699636873571,
            "mdate": 1699636873571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Iee50MsCEh",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer\u2019s detailed and constructive comments and suggestions. In the following, we provide the main response to your questions and comments:\n\n**1. About your questions that there are several vital details are missing, we would like to provide further explanations as follows:**\n  - **\"How is $\\mathcal{Q}^t$ calculated?\"**\n\n    We use matrix $\\mathbf{Q}^{t}$ to represent the subspace $\\mathcal{Q}^{t}$, which is spanned by a set of query vectors from task 1 to task $t$, and this matrix is used to constraint $\\mathbf{K}^{t +1}$ when learning task $t+1$. $\\mathbf{Q}^{t}$ was computed by using SVD to choose the most representative bases. Specifically:\n        \n     - For $t=1$, let $ \\mathbf{R}^0 =[q(\\mathbf{x}^0_1), q(\\mathbf{x}^0_2), ...] $  be the matrix with column vectors as query vectors of task 0, where $\\mathbf{x}^t_i$ is i-th sample of task t. We perform SVD on $\\mathbf{R}^0 = \\mathbf{U_0 \\Sigma_0 (V_0)^T}$ and then a orthogonal basis of size $k$ is chosen to form $\\mathbf{R}^0_k$ as the *k-rank approximation* such that $\\left\\| \\mathbf{R}^0_k  \\right\\|^2_F \\geq \\epsilon \\left\\| \\mathbf{R}^0  \\right\\| ^2_F $, given the threshold $\\epsilon \\in (0, 1)$. Finally, matrix $\\mathbf{Q}^0 = [u^0_1, ..., u^0_k]$ is formed with the columns being the selected basis vectors.\n\n      - For $t > 1$, let $\\mathbf{R}^t = [\\mathbf{Q}^{t-1}, q(\\mathbf{x}^t_1), q(\\mathbf{x}^t_2), ...]$. By performing SVD and $k$-rank approximation on $\\mathbf{R}^t$, we obtain $\\mathbf{Q}^{t}$.\n\n    We have added this description in Appendix A.2 in the revision.\n\n  - **\"What is the specific sample size used for this calculation?\"**\n\n    In order to compute $\\mathbf{Q}^{t}$, we use a set of *200 latent vectors* $q(\\mathbf{x})$ of task $t$ (and matrix $\\mathbf{Q}^{t-1}$, if $t>1$). We mentioned this in Appendix A.5, section Protocol.\n\n\n  - **\"How is $g_\\phi$ implemented? Is it similar to a cosine distance between the current prototype and previous prototypes?\"**\n\n    We would like to clarify that our module operates regardless of the cosine distance between old and new prototypes.  Module $g_\\phi$ is implemented as follows:\n\n      Module $g_\\phi$ includes $T$ additional heads for $T$ tasks, each of them includes only a single layer of size $d_z \\times 2C$, where $d_z$ is the dimension of latent space and $C$ is the number of classes per task. $t^{th}$ head will help in identifying whether sample $\\mathbf{x}$ belongs to task $t$ or not by $\\mathbf{x}$'s score on that head, which is described in the equation (6) in the main paper:\n        \n       - The score of $\\mathbf{x}$ on an OVA head is determined as the highest class-wise score corresponding to that head. Specifically, for a specific class $c$, the output score of $\\mathbf{x}$ w.r.t this class is a 2D vector $m_c (\\mathbf{z}) = \\text{softmax}(g_{\\phi, c} (f_{\\Phi, P}(\\mathbf{z})))$ wherein $m_c^1(\\mathbf{z})= p(\\hat{y}=c|\\mathbf{z})$ specifies the in-distribution probability and $m_c^2(\\mathbf{z}) = 1 -m_c^1(\\mathbf{z})$ specifies the out-distribution probability. \n\n      - Finally, for each sample $\\mathbf{x}$ we will have a vector containing $T$ score values of $\\mathbf{x}$ on $T$ tasks. Which will be used to compute predicted results as in equation (7). \n\n       About prototypes: Given an input $\\mathbf{x}$, we have corresponding latent vectors $\\mathbf{z} = f_{\\Phi, P}(\\mathbf{x})$, we also consider $\\mathbf{z}$ as a prototype $\\mathbf{p}$ of the task it belongs to. To train the OVA-based module without rehearsal raw data, we store and replay a small set of these prototypes from all tasks so far. \n  - **\"What are the hyperparameters employed, such as the prompt length and the prompt pool size? How does your method compare with CODA when these hyperparameters are adjusted?\"**\n\n    Regarding the hyperparameters, please refer to our Appendix. Basically, we use the same prompt length and the prompt pool size as CODA. We would like to provide figures in Appendix A.6.5, and through this link: https://ibb.co/7rZZjTb that show the comparison of KOPPA with CODA when we change pool size and prompt length on Imagnet-R-5: *KOPPA always outperforms CODA in all cases, in addition, KOPPA's performance tends to increase clearly when increasing the pool size*."
                    },
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 1)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304152709,
                "cdate": 1700304152709,
                "tmdate": 1700562634133,
                "mdate": 1700562634133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M0oFROoC7H",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**2. Besides, about your comment: \"The operational mechanism of the proposed method appears to diverge from the authors' assertions\",  we would like to explain and convince you as follows:**\n\n- **\"Upon juxtaposing the results from Table 3 and Table 4, it becomes evident that the orthogonal projection component is minimally effective;...\"**\n\n We would like to clarify that the orthogonal projection component has a remarkable effect. The effectiveness of this strategy is evident in the results presented in Table 2 and Figure 2, demonstrating the superior ability to avoid shifts compared to CODA. Moreover, the results in the table below highlight that enhanced shift avoidance plays a crucial role in enabling KOPPA to achieve superior results over CODA, especially on DomainNet and S-Imagenet-10 with the gaps greater than 3 \\%. \n\n   Therefore, we would like to convince you that the reduction of mismatch and semantic drift through key-query orthogonal projection constitutes a noteworthy contribution.\n \n| Methods | S-CIFAR-100 | S-Imagnet-R-5 | S-Imagnet-R-10 | S-Imagnet-R-20 | DomainNet |\n|----------|----------------|-------------|--------------|---------|-------------|\n| Deep L2P++ + OVA         | 95.53 \u00b1 0.83 | 84.86 \u00b1 0.39  | 89.23 \u00b1 0.77   | 91.92 \u00b1 0.94  | 80.01 \u00b1 0.54                  |\n| DualP + OVA              | 96.06 \u00b1 0.75  | 85.28 \u00b1 0.55  | 88.11 \u00b1 0.82 | 92.13 \u00b1 0.84  | 79.83 \u00b1 0.52                  |\n| CODA + OVA               | 96.88 \u00b1 0.74   | 85.32 \u00b1 0.51    | 88.02 \u00b1 0.65  | 92.10 \u00b1 0.98  | 79.76 \u00b1 0.55                  |\n| KOPPA   | **97.82 \u00b1 0.80**  | **86.02 \u00b1 0.42**  | **91.09 \u00b1 1.53**  | **92.89 \u00b1 1.2** | **84.14 \u00b1 0.62**         |"
                    },
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 2)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304452949,
                "cdate": 1700304452949,
                "tmdate": 1700438222359,
                "mdate": 1700438222359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "miMriPAcJi",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 3)"
                    },
                    "comment": {
                        "value": "- **Regarding our OVA-based module.**\n\n    - **In this scenario, an additional ablation study, replacing $h_\\theta$ with a prototype-based classifier, is necessary. If successful, the unique contribution of this work, the OVA loss, may be rendered not valid any more.**\n\n        Firstly, it's important to clarify that OVA loss is a well-established and widely recognized loss function within our community **[1, 2]**. In this work, one of our contributions is finding out the necessity to observe all classes, then introducing a module based on the OVA loss, and how to use it to significantly improve CL performance. The high effectiveness of this module for CL should be significant for the CL literature.\n\n         In addition, to illustrate the effectiveness of our proposed OVA-based module in comparison to alternative solutions, such as using a prototype-based classifier as you suggested, we have conducted extended experiments. The results are presented in the table below in which\n         - Prototype-based classifier (I): remove OVA head; use prototypes of classes to give predictions.\n         - Prototype-based classifier (II): remove OVA head, and use old prototypes and current data to learn CE head together with the backbone; use prototypes of classes to give predictions.\n         - CE $ \\dagger $ + CE: replace OVA head with an additional CE head and do training it by using prototypes. Give predictions in the similar way that described in the main paper.\n         - Just CE: The model is only trained with CE loss and a single classification head\n\n       | Methods    | S-CIFAR-100     | S-Imagnet-R-5   | S-Imagnet-R-10 | S-Imagnet-R-20    |\n       |--|---|---|--|---|\n       | OVA + CE (Ours)| **97.82 \u00b1 0.80** | **86.02 \u00b1 0.42** | **91.09 \u00b1 0.53** | **92.89 \u00b1 1.25** |\n       | Just CE  | 86.28 \u00b1 0.81 | 76.32 \u00b1 0.45 | 75.62 \u00b1 0.43 | 72.42 \u00b1 1.20 |\n       | CE \u2020 + CE  | 94.71 \u00b1 0.85 | 85.08 \u00b1 0.51 | 90.02 \u00b1 0.54 | 90.92 \u00b1 0.88 |\n       | Prototype-based classifier (I) | 67.28 \u00b1 0.77 | 0.38 \u00b1 0.65 | 4.75 \u00b1 0.62 | 41.85 \u00b1 0.92 |\n       | Prototype-based classifier (II) | 69.75 \u00b1 0.75 | 0.49 \u00b1 0.54 | 4.91 \u00b1 0.62 | 43.42 \u00b1 0.85|\n\n       The results in the table above demonstrate: \n         - (i) the essential of using a module which can observe all classes (because CE $ \\dagger $ + CE is better than Just CE);\n         - (ii) using just prototype to predict is a bad option because of the bad distribution of classes (due to the uncontrolled overlapping of features from different tasks); \n         - (iii) our OVA module is a superior choice to all the above alternatives.\n\n\n\n[1] Rifkin, Ryan, and Aldebaro Klautau. \"In defense of one-vs-all classification.\" The Journal of Machine Learning Research, 2004.\n\n[2] Ovanet: One-vs-all network for universal domain adaptation. In ICCV, 2021"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320906061,
                "cdate": 1700320906061,
                "tmdate": 1700539156197,
                "mdate": 1700539156197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FDpcyNynZk",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "-  \n   - **Its effectiveness could stem from the similarity between testing and training tasks, which allows for the identification of the most closely related and well-trained training task, thereby resolving the challenge. To validate this, additional experiments involving diverse data splits or datasets are essential to assess the practicality of the proposed methods beyond the current benchmarks.**\n\n     We respectfully disagree this comment. We strictly follow the setting in the previous benchmark papers CODA, DualPrompt, and L2P in which the testing sets are different and separate from the training/valid sets. This is also a standard setting in deep learning and the fact that our approach can predict well on a different and separate test set demonstrates its generalization ability to a separate and unseen test set. Moreover, the significant improvement on the challenging dataset DomainNet which is remarkably different from the ImageNet dataset on which ViT was pretrained further shows the generalization ability of our approach.  \n\n**3. About the writing**: Thank you for your feedback. We have revised the paper more comprehensively!\n\n--------------------------------------------------\n*We sincerely hope that our answers have met your expectations and satisfactorily addressed your inquiries. Please let us know if you would like us to do anything else*.\n\n*Best regards,*\n\n*Authors*"
                    },
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 4)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436944314,
                "cdate": 1700436944314,
                "tmdate": 1700536134540,
                "mdate": 1700536134540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JJnWsvOyMG",
                "forum": "nonqjVFiE9",
                "replyto": "FDpcyNynZk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_m1q8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_m1q8"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors' repsonse"
                    },
                    "comment": {
                        "value": "I appreciate the authors's thorough response; however, my major concerns remain inadequately addressed.\n(1) I remain unconvinced about the effectiveness of the orthogonal projection component. The lack of clarity regarding the improvement difference between S-Imagnet-R-10 and S-Imagnet-R-15/20, without explanations, raises questions about the working mechanism.\n(2) The effectiveness comparison with other prototype-based continual learning methods is not sufficiently elucidated. Given the existence of several prototype-based CL methods [1,2], I am not sure the naive implementation of prototype-based classifiers (I)(II) here makes sense.\n\n[1] Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams, ICCV 2021\n[2] Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning, ICML 2023\n[3] Computationally Budgeted Continual Learning: What Does Matter? CVPR 2023"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636699088,
                "cdate": 1700636699088,
                "tmdate": 1700636699088,
                "mdate": 1700636699088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "COIwNw2No1",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 5)"
                    },
                    "comment": {
                        "value": "Thank you for your response. We would like to answer to your concerns as follows:\n\n**1. I remain unconvinced about the effectiveness of the orthogonal projection component. The lack of clarity regarding the improvement difference between S-Imagnet-R-10 and S-Imagnet-R-15/20, without explanations, raises questions about the working mechanism.**\n\nRegrading the more improvements of S-Imagnet-R-10 over S-Imagnet-R-5/20, we conjecture this is due to the level of feature shift in these datasets. This could stem from the similarity between tasks due to how these datasets are split: the level of task similarity on S-Imagnet-R-5/20 is less than on S-Imagnet-R-10, leading to less shift of the old-task features. Specifically, we measure the displacement distance (or shift) of CODA and KOPPA (similar to Table 2 in the main paper) on these 3 datasets. Specifically, we consider task 1's data, compute task 1's representations at the end of each task, and evaluate the shift of the features at the end of task 1 and the ends of other tasks to how how shift the features. We report the shift results in the tables below:\n\n\n|**(S-Image-R-5)**| Task  | 1 | 2    | 3    | 4    | 5    |\n|-|-------|---|------|------|------|------|\n|| CODA  | 0 | 0.06 | 0.06 | 0.08 | 0.09 |\n|| KOPPA | 0 | 0.02 | 0.02 | 0.02 | 0.03 |\n\n\n|**(S-Image-R-10)**| Task  | 1 | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|-|-------|---|------|------|------|------|------|------|------|------|------|\n|| CODA  | 0 | 0.09 | 0.09 | 0.08 | 0.1  | 0.09 | 0.11 | 0.13 | 0.15 | 0.17 |\n|| KOPPA | 0 | 0.03 | 0.04 | 0.04 | 0.05 | 0.04 | 0.04 | 0.04 | 0.05 | 0.05 |\n\n\n|**(S-Image-R-20)**| Task  | 1 | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | 19   | 20   |\n|-|-------|---|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\n|| CODA  | 0 | 0.05 | 0.06 | 0.06 | 0.06 | 0.07 | 0.07 | 0.06 | 0.07 | 0.07 | 0.07 | 0.07 | 0.08 | 0.08 | 0.07 | 0.07 | 0.08 | 0.08 | 0.08 | 0.08 |\n|| KOPPA | 0 | 0.02 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 |\n\n- It can be seen that CODA exhibits bigger shift on S-Imagenet-R-10 than on S-Imagnet-R-5 and 20, hence our orthogonal projection component is more efficient on S-Imagenet-R-10 because its purpose is to handle the shift.\n- Therefore, although KOPPA can reduce this shift on all these 3 datasets, its effectiveness on S-Imagnet-R-5 and 20 are not as clear as that on S-Imagenet-R-10. As a result, the orthogonal projection component can improve more effectively on S-Imagenet-R-10. \n\nFurthermore, we provide line plot which shows the difference between the shift of CODA and that of KOPPA over tasks on those datasets (see this link: https://ibb.co/zfSn4Gq). This shows the higher improvement in terms of reducing shift on S-Imagenet-R-10 than on other datasets.\n\nFinally, we believe that  the orthogonal projection component is efficient to improve the performance. Evidently, it improves consistently on all datasets with the gap 3\\%, 5\\% for S-Imagenet-R-10, DomainNet and around 1\\% for other datasets. More importantly, our approach deeply outperforms state-of-the-art baselines. We share and release our code to the community and anyone who interests can reproduce our numbers."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654397296,
                "cdate": 1700654397296,
                "tmdate": 1700676600990,
                "mdate": 1700676600990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bemRpU7tog",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**2. The effectiveness comparison with other prototype-based continual learning methods is not sufficiently elucidated. Given the existence of several prototype-based CL methods [1,2], I am not sure the naive implementation of prototype-based classifiers (I)(II) here makes sense.**\n\nWe respectfully disagree to this comment.\n\n- First, we want to clarify that the prototypes in our method is the features at the penultimate layer of old task used to train OVA head. This OVA head is an additional head used to strengthen the signal of the main classification head over all tasks. We observe that within a task, the classification head corresponding to all classes in this task can predict very well the class label, but across all tasks, it might exist a wrong class belonging to another task that possesses higher prediction probability. Hence, as shown in Eq. (7) in main paper, we use the outputs from the OVA head to fortify the classification signal of the right class of the right task. We can clearly see that when combined with OVA, the prediction results are significantly improved. This is one of our contributions in our paper.\n\n- Second, before you concerned the need for our module based on OVA and whether classifiers based on prototypes could replace our strategy, we performed relevant experiments when the prototypes are used to replace OVA. The results show that due to excessive overlap of features between tasks, naive solutions are not helpful, proving the effectiveness of our OVA module.\n\n- Third, regarding the prototype-based CL methods as you mentioned, we see that [1] is applicable to ours when using raw data to update prototypes, hence it is not rehearsal free. Due to time limit, we will try the strategy to update prototypes of [2] on KOPPA. The reported results will be an ablation study to compare the effectiveness of different approaches, not evidence to negate the role of any ones. However, we believe that using the method in [2] instead of our OVA-based prediction strategy would not be better than our KOPPA, because they don't have any terms explicitly alleviating overlapping of features (as well as) of classes between different tasks.\n\n---------------------\nWe hope we have thoroughly addressed your concerns. We are willing to answer any additional questions."
                    },
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 6)"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659080191,
                "cdate": 1700659080191,
                "tmdate": 1700675522669,
                "mdate": 1700675522669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oCX3q5s4o0",
                "forum": "nonqjVFiE9",
                "replyto": "gIVI3VYpj4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m1q8 (Part 6.2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer m1q8,\n\nWe would like to add the experimental results when applying prototype-based strategy of PRD [2] on KOPPA. In specific, we retain the design of KOPPA on backbone, remove OVA loss and replace CE loss together with the classification head by the objective function in PRD with learnable prototypes. The table below show the results on S-CIFAR-100 and S-Imagnet-R-10:\n | Methods    | S-CIFAR-100     | S-Imagnet-R-10 |\n |--|---|---|\n | OVA + CE (Ours)| **97.82 \u00b1 0.80** | **91.09 \u00b1 0.53** |\n | Just CE  | 86.28 \u00b1 0.81 | 75.62 \u00b1 0.43 |\n | CE \u2020 + CE  | 94.71 \u00b1 0.85 | 90.02 \u00b1 0.54 |\n | Prototype-based classifier (I) | 67.28 \u00b1 0.77 | 4.75 \u00b1 0.62 |\n | Prototype-based classifier (II) | 69.75 \u00b1 0.75 | 4.91 \u00b1 0.62 |\n | *KOPPA + PRD* | *85.88 \u00b1 1.32* |  *74.22  \u00b1 0.56*|\n\nAs expected, the results show that our method still outperforms other alternatives. The results of KOPPA + PRD are slightly lower than JustCE, both of which do not have explicit term to separate features from different tasks. About the PRD, when applying to KOPPA, basically, the distillation term is not really necessary because the effectiveness of KOPPA in reducing shift, while the 2 remain terms only play the roles of distinguishing features in the same tasks and moving prototypes to proper locations. \n\nOne might question why Prototype-based classifier (II)  obtains lower results than KOPPA + PRD even it observes prototypes of all tasks. This might be that the prototypes found by mean of features vectors, which learn by CE loss, are not as well clustered as those learned by PRD (using SupCon loss). \n\n----------------------------------\n\n*We hope that we have thoroughly addressed your concerns and you can reconsider the review score. Please let us know if you would like us to do anything else.*\n\n*Thank you*"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739435149,
                "cdate": 1700739435149,
                "tmdate": 1700740952051,
                "mdate": 1700740952051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iAf7fVhzEZ",
            "forum": "nonqjVFiE9",
            "replyto": "nonqjVFiE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address key challenges in Prompt Learning based Continual Learning (CIL) methods in class incremental setting. The authors highlight two potential issues in prior prompt learning based CIL methods: (1) Mismatch between per task final prompt representation during training and inference phase as prompt keys of one task could correlate with other tasks due to no explicit constraint and (2) The triggering of wrong classifier head due to the shift in the sample features of same task between training and inference. \n\nTo ensure that prompt representations of one task remains consistent during the training of prompts for upcoming tasks, the authors propose to enforce the orthogonality constraint between prompt keys of current task with the subspace of previous task. To address the issue of effective classifier head distinction, prototype based method is used which keeps prototypes from all tasks and eventually refine the classification task score. \n\nThe model is finetuned in class-incremental setting with the combination of above techniques and shows improvement against previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n(1) This paper has identified relevant issues and key challenges in prompt learning based CIL methods such as mismatch the between final prompt representation per task during training and testing. . Methods to improve these limitations can greatly enhance the resulting performance and advance the progress in prompt learning based CIL methods.\n\n(2) The idea of imposing orthogonality between prompt keys and previous task sub-keys is motivating. This aims to reduce the impact of prompt keys of new task to calculate prompt representations of previous task, resulting in correct prompt key activation specially during testing. \n\n(3) The method shows impressive results against previous methods and the proposed technique is motivated with fair ablation studies."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\nMy main concern for this work lies in potential violation of rehearsal-free CIL experimental rules. \n1) In the first proposed module, the authors keep subspace Qt for upcoming new tasks to ensure the orthogonality constraint. This subspace potentially include sample information from previous tasks till t-1, which means that for the current task, information about the previous task samples is explicitly utilized and this possibly violates the rehearsal-free CIL setting where no information about previous task examples is known.\n\n2) Similarly, for the second proposed OVA technique, prototypes are stored from each task which are feature representations of training examples from each task. Therefore, the authors are indirectly utilizing a buffer in the feature space where the task ID of each task is completely known. I am finding it difficult to understand how this method does not belong to rehearsal-based CIL setting. \n\n\n3) The baseline CODA performs additional evaluation on DomainNet which is missing in this comparisons."
                },
                "questions": {
                    "value": "Please refer to weaknesses section for questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699585963510,
            "cdate": 1699585963510,
            "tmdate": 1699636873465,
            "mdate": 1699636873465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q5Blp1Wv7k",
                "forum": "nonqjVFiE9",
                "replyto": "iAf7fVhzEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewer\u2019s positive evaluation, constructive comments and suggestions. In the following, we provide the main response to your concerns:\n\n- **My main concern for this work lies in potential violation of rehearsal-free CIL experimental rules**\n\n    - We would like to clarify that the rehearsal-free setting is a scenario where old task **raw data** is **not** stored and reused directly, which aims to address issues related to storage capacity and security. \n\n    - In our approach, rather than saving raw data, we store information from old tasks in more condensed, efficient, and secure encoding forms, which include prototypes of deep feature vectors and subspace representation matrices. \n    - Notably, within the rehearsal-free group, *HiDe [1]* and *PASS [2]* also suggest saving old task information in the form of Gaussian distributions of latent vectors for replaying when learning new tasks; *GPM [3]* stores information about the input space and representations at each layer in the form of matrices representing the subspace spanned by the corresponding input/representations to constrain gradient updating (in future tasks).\n   -  Our KOPPA needs to store the orthonormal basis of the subspace $\\mathcal{Q}^T$ corresponding to the matrix $\\mathbf{Q}^T$ up to the last task $T$  (similar to GPM but much more economic because we only store the only one subspace for $q(\\mathbf{x})$, whereas GPM needs to store the subspaces for all layers. This matrix has at most dimension of 768 \u00d7 768. In addition, for training the OVA head, we retain a maximum of $100$ prototypes with a dimension of $768$ for each task (similar to HiDe and PASS). We view them as additional parameters to the model, which totally cost the additional memory as 8.5M for S-CIFAR-100, 3.9M  for S-ImageNet-R-5, 5.4M for S-ImageNet-R-10, and 8.5M for S-ImageNet-R-20.  \n \n\n    Therefore, we would like to convince you that our method does not violate the rehearsal-free scenario.\n\n- **The baseline CODA performs additional evaluation on DomainNet which is missing in this comparisons.**\n    \n    Regarding the evaluation on DomainNet, the results reported in the table below indicate that our method outperforms the baselines by a large margin. We also added it in Appendix A.5 of the revised version.              \n\n    | Method     | $A_N(\\uparrow) $       | $F_N (\\downarrow)$   |\n    |------------|------------------------|----------------------|\n    | JOINT      | 79.65                  | -                    |\n    | ER (5000)  | 58.32 \u00b1 0.47           | 26.25 \u00b1 0.24         |\n    | FT         | 18.00 \u00b1 0.26           | 43.55 \u00b1 0.27         |\n    | FT++       | 39.28 \u00b1 0.21           | 44.39 \u00b1 0.31         |\n    | LwF        | 74.78 \u00b1 0.43           | 5.01 \u00b1 0.14          |\n    | L2P++      | 69.58 \u00b1 0.39           | 2.25 \u00b1 0.08          |\n    | Deep L2P++ | 70.54 \u00b1 0.51           | 2.05 \u00b1 0.07          |\n    | DualPrompt | 70.73 \u00b1 0.49           | 2.03 \u00b1 0.22          |\n    | CODA-P     | 73.24 \u00b1 0.59           | 3.46 \u00b1 0.09          |\n    | **Ours**       | **84.14 \u00b1 0.62**  | **0.33 \u00b1 0.10** |\n\n[1] Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality, NeurIPS 2023.\n\n[2] Prototype augmentation and self-supervision for incremental learning. In CVPR, 2021.\n\n[3] Gradient projection memory for continual learning. In ICLR, 2021.\n\n---------------------------------------------------\n*We sincerely hope that our answers have met your expectations and satisfactorily addressed your inquiries. Please let us know if you would like us to do anything else.*\n\n*Best regards,*\n\n*Authors*"
                    },
                    "title": {
                        "value": "Response to Reviewer t1j7"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302073748,
                "cdate": 1700302073748,
                "tmdate": 1700479775746,
                "mdate": 1700479775746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rLQepoI6b9",
                "forum": "nonqjVFiE9",
                "replyto": "iAf7fVhzEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for taking the time to read and review our submission. We would be happy to address any remaining questions before the discussion period ends today.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667022016,
                "cdate": 1700667022016,
                "tmdate": 1700667022016,
                "mdate": 1700667022016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BqXveN7xPg",
                "forum": "nonqjVFiE9",
                "replyto": "iAf7fVhzEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Reviewer_t1j7"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for providing the rebuttal.\n\nThe results on the DomainNet benchmark are encouraging! However, I am still not convinced about the applicability of using feature vectors of past data for new tasks. Storing feature vectors of previous task data and using them during the training of new tasks can be considered a form of rehearsal, as the model is exposed to and leverages information from previous tasks.\n\nTherefore I would like to maintain my current score at this moment. \n\nThank you!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675259666,
                "cdate": 1700675259666,
                "tmdate": 1700675330939,
                "mdate": 1700675330939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JlyRgitQC5",
                "forum": "nonqjVFiE9",
                "replyto": "iAf7fVhzEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer t1j7, \n\nThank you for acknowledging our results on Domain Net. Regarding your concern related to the definition of Rehearsal-free, we would like to further convince you that:\n\n- When discussing *Rehearsal-free*, it's natural to consider settings that avoid recalling any information from old tasks. However, prior to our work, our community has already highlighted in many studies that rehearsal-free refers to ***not saving and reusing raw data directly***, *rather than excluding any indirect information* [1, 4, 5, 6, 7, 8]. \n\n- Moreover, we would like to provide additional evidence from a recently accepted work at WACV 2024: *Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning* [8], where the authors propose **saving features of data from old tasks to implement constraints when learning new tasks**. This work also falls under the Rehearsal-free group.\n\n- In our work, we view the prototypes as parameters which totally cost a few megabytes. This amount of additional overhead is similar to less than 40 images of ImageNet. Certainly, with tiny amount of images for old tasks for example, CODA and other prompt-tuning technique cannot improve their performance. \n\nIs your concern related to privacy issues in using features of old data? \n- We would like to convince you that, at present, feature vectors corresponding to raw images still represent a form of embedding and do not easily lead to information leakage. \n\n\n[4] Rehearsal-free Continual Language Learning via Efficient Parameter Isolation, In ACL 2023.\n\n[5] Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches, In CVPR Workshop 2023.\n\n[6] A Closer Look at Rehearsal-Free Continual Learning, In CVPR Workshop 2023.\n\n[7] CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning, In CVPR 2023.\n\n[8] Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning, In WACV 2024.\n\n--------------------------------------------\n\n*We hope that we have thoroughly addressed your concerns and you can reconsider the review score. Please let us know if you would like us to do anything else.*\n\n*Thank you*"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690379428,
                "cdate": 1700690379428,
                "tmdate": 1700699022703,
                "mdate": 1700699022703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]