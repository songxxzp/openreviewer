[
    {
        "title": "Continual Test-Time Adaptation by Leveraging Source Prototypes and Exponential Moving Average Target Prototypes"
    },
    {
        "review": {
            "id": "m9id0ZsKmv",
            "forum": "eXrUdcxfCw",
            "replyto": "eXrUdcxfCw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_hKy6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_hKy6"
            ],
            "content": {
                "summary": {
                    "value": "The objective of this paper is to address the challenge of adapting a pre-trained Convolutional Neural Network (CNN) to distribution shifts during test time. These shifts stem from corrupted test images, which may include issues such as noise and blur.\n\nTo tackle this problem, the authors propose a method that penalizes abrupt changes in class prototypes by employing exponential moving average (EMA). By leveraging this technique, the authors aim to enhance the adaptability of the model in the face of distribution shifts caused by various forms of image corruption.\n\nThe authors conduct thorough evaluations of their approach on widely-acknowledged continual test time adaptation benchmarks, specifically Imagenet-C and CIFAR-100. Their results demonstrate that their method occasionally outperforms existing state-of-the-art test time adapters, namely EATA, CoTTA, and RMT.\n\nOverall, the paper assembles a simple technique for addressing the challenges posed by distribution shifts, offering some insights into improving the adaptability of pre-trained CNNs under diverse test conditions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1:\nThe paper addresses a crucial yet underexplored scenario: continuous adaptation to test time shifts.\n\nS2:\nThe utilized approach is simple: it enforces gradual shifts in class prototypes instead of abrupt changes, achieved through the application of Exponential Moving Average (EMA).\n\nS3:\nThe paper is well-written, and meticulously executed."
                },
                "weaknesses": {
                    "value": "W1:\nA significant concern regarding this paper is its lack of technical innovation. Despite being an application paper, the method merely applies EMA for continual test time adaptation, employing standard techniques for selecting reliable test exemplars, computing class prototypes, and calculating EMA penalties. These methodologies are well-established within the existing literature.\n\nW2:\nThe empirical evidence presented in the paper lacks persuasiveness. A substantial performance boost could have justified the paper's simplicity and application-oriented nature. However, the minor improvement over EATA and RMT, as indicated primarily in Table 1-2 (i.e., less than +0.5% accuracy), does not substantiate the approach's effectiveness convincingly.\n\nW3:\nOne notable omission in the paper is the absence of a comparison or reference against a significant baseline, namely NOTE [1], which is a robust continual test-time adaptation method designed to handle temporal correlations. This baseline is highly relevant to the authors' objectives and should have been included for a comprehensive evaluation. Furthermore, the paper missed an opportunity for greater depth by limiting its evaluation to simple, artificial distribution shifts induced by corruptions. It could have been more compelling if the authors had explored and evaluated the method against natural shifts or temporal correlations, thus enhancing the paper's overall impact and relevance."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Reviewer_hKy6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698237570441,
            "cdate": 1698237570441,
            "tmdate": 1699636046866,
            "mdate": 1699636046866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QsOrwd0Dou",
                "forum": "eXrUdcxfCw",
                "replyto": "m9id0ZsKmv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1 : Please refer to the official comment to all authors about the novelty of this work.\n\nW2 : We respectfully disagree with the assertion that our proposed method yields marginal performance gains. In the case of ImageNet-C, our proposed term enhances the mean accuracy across all target domains by more than 1% for all three applied methods (EATA, CoTTA, and RMT).  More than 1% performance gain is considered significant, especially given the challenging nature of the continual test-time adaptation setting and the dataset containing 1000 classes. Additionally, for CIFAR100-C, our proposed term leads to meaningful performance improvements for both EATA and CoTTA. The only scenario where marginal performance gains are observed is with RMT on CIFAR100-C, attributed to its intricate incorporation of several loss terms.\nWe are uncertain about the reason for stating that there is an accuracy gain of less than +0.5%.\n\nMoreover, we conducted through ablation study and analysis to validate the efficacy of our proposed method. We respectfully request the reviewer to check our ablation study and analysis as well.\n\nW3 : NOTE addresses the temporal correlation of TTA, primarily focusing on the imbalanced class distribution of incoming target data. While this differs slightly from our problem, which specifically addresses the domain shift of target data, we plan to incorporate NOTE as one of the baseline methods in the revised version. Additionally, we will explore other datasets that closely resemble the natural shifts of continual test-time adaptation setting."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121661966,
                "cdate": 1700121661966,
                "tmdate": 1700123933277,
                "mdate": 1700123933277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vOv6YZY0zx",
            "forum": "eXrUdcxfCw",
            "replyto": "eXrUdcxfCw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_QiZ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_QiZ6"
            ],
            "content": {
                "summary": {
                    "value": "This work tackles the continual test-time adaptation problem. It proposes two enhancements that are orthogonal to several test-time adaptation methods. The first component as regularizing the feature extractor with a cross-entropy loss that leverages a moving average of  prototypical features from the target domain. Such features are initialized with the weights of the linear classifier and then updated using exponential moving average. The second component is to align the features of the target domain with a precomputed set of prototypical features from the source domain. Experiments are carried out on two datasets (CIFAR-100-C and ImageNet-C) where the proposed method showed performance gains when combined with 3 test-time adaptation methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strengths of this work are:\n\n- The problem this paper tackles is both important and practical.\n\n- The approach proposed in this work is simple and easy to implement. Further, the experiments show the applicability of the proposed enhancements when combined with different test-time adaptation methods.\n\n- The online estimates of the prototypical features and the MSE loss makes the proposed approach efficient as demonstrated in Figure 2."
                },
                "weaknesses": {
                    "value": "Despite the stated strengths of this work, there are several weaknesses that need to be addressed before accepting this work.\n\n1- Methodology. While the proposed method is simple to both understand and implement, there are several caveats that need to be discussed:\n\n(1a) How are the hyper parameters tuned? Is the source data used to initialize $P^s$ employing training data or validation data? \n\n(1b) This paper needs to properly state its contributions over TTAC (the other clustering approach in the literature).\n\n(1c) It is unclear whether the predictions in Algorithm 1 line 5 are adjusted before the output phase (line Ensure) as the predictions $z$ are returned as $\\hat y$. Further, the algorithm returns the set of predictions for all data-points and all domains rather than conducting the evaluation in an online manner (return the predictions batch by batch). \n\n2- Experiments. The experimental analysis in the work show marginal performance gains of the proposed approach. Further, there are missing key experimental details and comparisons:\n\n(2a) How is the hyperparemeter search done for the proposed approach? Is a similar effort put into other baselines (e.g. EATA + TTAC)?\n\n(2b) It is unclear why the proposed components degrade the performance of EATA under small batch-sizes?\n\n(2c) While EATA does not regularize for its features to be clustered (unlike the proposed approach), they are still very competitive (better in discriminating different clusters) when the proposed components are absent.\n\n(2d) Generally, I think the analysis and comparison of the proposed method should be against EATA+TTAC rather than EATA (e.g. in Figure s 4 and 5).\n\n(2e) Experiments with different and more powerful architectures that do not use batch normalization (e.g. ViT) layers are missing.\n\n3- Writing. The writing of this work should be vastly improved to enhance its readability. Here are a list of suggestion to be considered in the final version:\n\n(3a) The problem definition is not clear. Both $k$ and $t$ are used to refer to time.\n\n(3b) The introduction should state clearly the contributions this work provides.\n\n(3c) Algorithm 1 is unclear how the prediction is conducted and what predictions are returned (line 5 computes z while last line returns $\n\\hat y$.\n\n(3d) Captions of Tables 3 and 4 should be improved to elaborate what metric is reported and what to pay attention to.\n\n(3e) The related work section is missing from the main paper and put in the appendix. It is essential to have this section to clearly position this work in the literature.\n\nOverall, while I like the simplicity of the proposed approach, the performance gains are generally very marginal (with best choice of hyper parameters) questioning the usefulness of the proposed method."
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504944610,
            "cdate": 1698504944610,
            "tmdate": 1699636046784,
            "mdate": 1699636046784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z5tToEr9Kl",
                "forum": "eXrUdcxfCw",
                "replyto": "vOv6YZY0zx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(1a) Hyper-parameters are tuned via grid search. Source data used to initialize $P^s$ are sub-samples of training data. It is stated in the section 3.2 of our original submission.\n\n(1b) TTAC assumes that each class cluster follows a Gaussian distribution. TTAC pre-calculate the mean and covariance ($\\mu_s \\in \\mathbb{R}^d$ and $\\Sigma_s \\in \\mathbb{R}^{d \\times d}$, where $d$ is the dimension of the feature) for each class of source domain data, computing Gaussian distributions for each source class prior to test-time adaptation. At test-time, it uses the target data to compute the Gaussian distribution for each class cluster of target domain as well. It uses the distributions of source domain class clusters as anchors for the distributions of target domain class clusters to match against. The anchoring of cluster is achieved by directly minimizing the KL-Divergence between the source and the target distributions.\n\nComputing Gaussian distribution for each class cluster and minimizing KL-Divergence between the two distributions require computation overhead. We show that it indeed requires significant time to adapt a single batch in Figure 2 while our proposed method requires a small amount of time for adaptation. Pleas refer to the difference between the green and the blue plot of Figure 2.\nWe also show that in the main performance comparison tables (Table 1 and Table 2), ours outperforms TTAC in both benchmarks, showing superiority over TTAC in both accuracy and adaptation time.\n\nUnlike TTAC, our proposed method does not require the computation of intricate Gaussian distributions or the KL-Divergence between source and target distributions. Instead, it directly minimizes the MSE distance between the target feature and the source prototypes ($P^s$), akin to $\\mu_s$ in TTAC. This makes our method simpler, more efficient, and achieves superior performance over TTAC when combined with $\\mathcal{L}_{ema}$.\n\n(1c) Our algorithm returns the predictions batch by batch in an online manner. Line \"Ensure\" indicates that our algorithm ensures the prediction outputs for all the inputs from all the target domains. It does \"not\" mean that the predictions are returned all together at \"once\" when the test-time adaptation is finished.\n\n\n(2a) As addressed in response to question (1a), we employed grid search to determine the hyperparameters. Reference to Figure 3 in the analysis section provides additional insights. As outlined in the implementation details in Appendix B, we adhered to the hyper-parameters specified in the official code or the original paper of baseline methods. Nevertheless, we dedicated ample effort to fine-tune the hyper-parameters to attain satisfactory performance within the given CTA setting.\n\n(2b) When employing small batch sizes, the model exhibits low performance in batch accuracy. For a majority of batches (iterations), the accuracy is either 0% or less than 50%. This low accuracy hampers the construction of EMA target prototypes with accurate pseudo-labels, as well as the source alignment loss. Furthermore, given the reduced number of reliable samples after entropy-based sample selection due to low accruacy, the target prototypes are infrequently updated and remain unchanged. These challenges result in diminished performance of our proposed loss terms when using small batch sizes.\n\n(2c) EATA uses source samples to compute fisher importance of model weights before performing test-time adaptation. With the computed fisher importance, it employs anti-forgetting loss which prevents the parameters of the model from changing too much from their source trained weights during the test-time adaptation. It also selects reliable and non-redundant samples to facilitate efficient entropy minimization loss.\n\n(2d) The analysis in Figure 4 and 5 are conducted between EATA and EATA+Ours to show the efficacy of our proposed terms. We wanted to analyze how our proposed terms affect the model by ablating the proposed terms. If we compare between EATA+TTAC and EATA+Ours, we can not properly analyze how our proposed method has effect on the model.\n\n(2e) We will try to include experiments using different network architecture in the future version.\n\n(3a) $k$ refers to the domain index as mentioned in the paper while $t$ refers to the training iteration.\n\n(3b) Our contributions are well stated in the last paragraph of the introduction.\n\n(3c) Please refer to our answer on (1c).\n\n(3d) The metric presented in Table 3 is the average classification accuracy across the 16 domains, computed over three separate runs, and reported with standard deviation. The metric of Table 4 is the same as Table 1 and 2. What to pay attention to is well explained in each section, but we will try to briefly explain them in the caption for later version.\n\n(3e) We will include the related work in the main paper in future version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121651966,
                "cdate": 1700121651966,
                "tmdate": 1700122152513,
                "mdate": 1700122152513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "grD3XEkQqL",
            "forum": "eXrUdcxfCw",
            "replyto": "eXrUdcxfCw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_iXmR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_iXmR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method that utilizes prototypes from both source and target domains to enhance continual test-time adaptation. This approach seamlessly integrates with existing CTA methods, using source prototypes to reduce distribution discrepancies and target prototypes to cluster target features, which are updated via an EMA process during test-time. The research showcases improved model performance, reduced adaptation time overhead, and a mitigation of model bias, paving the way for future advancements in continual test-time adaptation techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**: The paper innovatively combines prototypes from both source and target domains, offering a fresh perspective on continual test-time adaptation.\n\n**Quality**: The rigorous methodology and in-depth experiments validate the efficacy of the proposed terms, demonstrating tangible improvements in model performance.\n\n**Clarity**: The paper is well-structured and articulates its methods and findings with precision, making it accessible for readers familiar with the domain.\n\n**Significance**: By addressing model bias and enhancing adaptation efficiency, this research holds potential to shape future work in the realm of continual test-time adaptation."
                },
                "weaknesses": {
                    "value": "(1) Compared to CoTTA, where unsupervised test-time adaptation methods function without relying on source domain data, this paper's reliance on pre-computed source prototypes from the source domain data seems to be an uneven ground. It raises questions about the comparability and fairness of the presented method relative to others, like CoTTA, which function without such dependencies.\n\n(2) The various loss components highlighted in the paper have previously been discussed in many other studies within the Domain Adaptation (DA) and Test-Time Adaptation (TTA) fields. This gives an impression that the proposed method might just be a combination of existing techniques, akin to piecing together different methods like A+B+C. Additionally, the usage of $L_{ema}$, which is commonly found in unsupervised DA, doesn't appear novel.\n\n(3) Figure 1 in the paper comes across as overly intricate and disorganized. A reader would need to invest significant effort to discern and correlate the various methods depicted, which hampers the immediate understanding of the paper's methodology.\n\n(4) The $L_{unsup}$ seems to be inadequately defined in the paper. Even though the authors touched upon it in the \"Problem definition\" section, a more explicit formula or representation would have been beneficial for clarity and a more straightforward comprehension."
                },
                "questions": {
                    "value": "1. **Regarding Comparability:** One of the primary concerns raised revolves around the fairness of comparing the proposed method with traditional CTA methods. Given that the proposed method relies on pre-computed source prototypes from the source domain data, how do the authors justify the comparability of their method, especially when other methods like CoTTA operate without such dependencies?\n\n2. **Concerning Novelty:** The various loss components presented in the paper seem to have been discussed in previous DA and TTA research. Could the authors elaborate on what sets their method apart, particularly concerning its novelty? How does the incorporation of these loss components enhance the uniqueness and effectiveness of the proposed method?\n\n3. **On Clarity:** Figure 1, as mentioned, appears quite intricate. Is there a possibility to streamline or restructure the figure to make it more intuitive for readers? \n\n\nIf the authors can address these questions and take into consideration the suggestions provided, it would greatly enhance the paper's clarity and relevance, and I will consider increasing my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730129524,
            "cdate": 1698730129524,
            "tmdate": 1699636046716,
            "mdate": 1699636046716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tcBw4XgMyJ",
                "forum": "eXrUdcxfCw",
                "replyto": "grD3XEkQqL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Regarding Comparability**\n\nAs we addressed in response to reviewer JC3N, it is crucial to highlight that baseline methods like TTAC, EATA, RMT all leverage source domain data according to their proposed methodologies. Nevertheless, we acknowledge the valid point raised by you, suggesting that comparing with methods that do not utilize any source domain data, such as CoTTA, may appear unfair.\n\nIn the table below, we show that even in the absence of source domain data, specifically without employing $\\mathcal{L}_{src}$ \n\nand solely utilizing $\\mathcal{L}_{ema}$, there is a performance improvement. \n\nThis table shows the average accuracy over the 16 test domains of ImageNet-C benchmark.\n\n|Method|Mean Acc. over the 16 test domains.|\n|----|----|\n|$\\mathcal{L}_{ema}$|44.00%|\n|$L_{ema} + \\mathcal{L}_{src}$|45.96%|\n|EATA|49.81%|\n|EATA+$\\mathcal{L}_{ema}$|50.62%|\n|EATA+**Ours**|51.32%|\n|CoTTA|37.42%|\n|CoTTA+$\\mathcal{L}_{ema}$|46.39%|\n|CoTTA+**Ours**|46.91%|\n|RMT|41.68%|\n|RMT+$\\mathcal{L}_{ema}$|42.29%|\n|RMT+**Ours**|42.78%|\n\nNotably, when $\\mathcal{L}_{ema}$  is applied independently of the baseline method, it already outperforms CoTTA (44.00% vs. 37.42%).\n\nAlso, CoTTA+$\\mathcal{L}_{ema}$ shows remarkable performance compared to CoTTA, \b\n\nsupporting the validity of proposed $\\mathcal{L}_{ema}$.\n\n$\\mathcal{L}_{src}$ can be additionally employed to further boost the performance when a sub-sample of source domain data are available.\n\nIt is essential to underscore that CoTTA necessitates additional computation and memory resources due to its teacher-student framework and augmentation strategy. \n\nThis results in a longer adaptation time for a single batch, as illustrated in Figure 2 of our original submission.\n\n**Concerning Novelty**\n\nPlease refer to the official comment to all authors about the novelty of this work.\n\n**Clarity of Figure 1**\n\nWe will elaborate the Figure 1 for better understanding of the proposed methodology in the future version. \nWe consider having a sperate figure for each proposed loss term and further describe each loss term in more detail.\n\n\n**Formulation of $\\mathcal{L}_{unsup}$**\n\nAs stated in the problem definition, $\\mathcal{L}_{unsup}$ can take a form of entropy minimization loss, akin to the approach in TENT and EATA. Alternatively, it can adopt the form of a distillation loss from the teacher network to the student network, as \bemployed in CoTTA.\n\nEntropy minimization loss term is defined as follows:\n\n$$\n\\mathcal{L}_{ent}=-\\sum^C_c (\\sigma(g(x^t))_c \\cdot \\log(\\sigma(g(x^t))_c))\n$$\n\nWhere $\\sigma$ is the softmax operation and  $g_{\\theta}(x^t)_c$  \n\nrefers to the $c$-th element of the produced logit $z^t=g_{\\theta}(x^t) \\in \\mathbb{R}^C$. \n\nIt is basically a loss to minimize the entropy of the produced logit.\n\nDistillation loss term (or the consistency loss as named in CoTTA) is defined as follows:\n\n$$\n\\mathcal{L}_{ent}=-\\sum^C_c(\\sigma(g_t(x^t))_c \\cdot \\log(\\sigma(g_s(x^t))_c))\n$$\n\nWhere $g_{t}$ and $g_{s}$ refer to the teacher and the student networks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121617251,
                "cdate": 1700121617251,
                "tmdate": 1700122340646,
                "mdate": 1700122340646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P5aMWuLHgU",
            "forum": "eXrUdcxfCw",
            "replyto": "eXrUdcxfCw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_JC3N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_JC3N"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes the idea of prototypes for the problem of continual test-time adaptation.\nThe proposed approach precomputes class prototypes for the source domain data and uses these to perform prototype matching with the target prototypes.\nThe target domain data is used to compute the target prototypes with the features of only the reliable low entropy samples.\nUsing the target samples provided at test time, the target prototypes are updated using the exponential moving average (EMA).\nExperimental results on ImageNetC and CIFAR100C suggest the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Utilizing the prototypical learning approach for test-time adaptation\n* Extensive ablation study to study the effect of EMA weight and different components on the overall objective"
                },
                "weaknesses": {
                    "value": "* Computating the source prototypes requires the source domain data. So, the proposed approach will not work for any off-the-shelf pre-trained model without the source domain data\n* Utilizing class prototypes is limited to classification tasks and not generalizable to other tasks\n* Experiments are limited to ImageNetC and CIFAR100C, and CIFAR10C related experiments are missing"
                },
                "questions": {
                    "value": "1. Can the authors report the experimental results on CIFAR10C, since prior works report their performance for this benchmark?\n2. Utilizing class prototypes is limited to classification tasks and not generalizable to other tasks. Can this approach be generalizable to other tasks, such as segmentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Reviewer_JC3N"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836703248,
            "cdate": 1698836703248,
            "tmdate": 1699636046634,
            "mdate": 1699636046634,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B7lR3i6nYq",
                "forum": "eXrUdcxfCw",
                "replyto": "P5aMWuLHgU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The usage of source domain data**\n\nFirst, we want to mention that our baseline methods also require source domain data. TTAC requires computing source cluster parameters ($\\mu_s \\in \\mathbb{R}^d$ and $\\Sigma_s \\in \\mathbb{R}^{d \\times d}$, where $d$ is the dimension of the feature), the mean and covariance, with the source data prior to test-time adaptation. EATA requires source domain data to compute the importance weights for the anti-forgetting regularization before deploying the model to the target domains. RMT also employs source replay loss which accesses the source domain data during test-time adaption. \nCoTTA, TSD, and other methods do not necessitate source domain data, yet they exhibit comparatively lower performance. \n\nIn the table below, we show that even in the absence of source domain data, specifically without employing $\\mathcal{L}_{src}$\n\nand solely utilizing $\\mathcal{L}_{ema}$, there is a performance improvement. \n\n$\\mathcal{L}_{src}$ can be further incorporated into training the model when there are some source samples available.\n\nThis table shows the average accuracy over the 16 test domains of ImageNet-C benchmark.\n\n|Method|Mean Acc. over the 16 test domains.|\n|----|----|\n|$\\mathcal{L}_{ema}$|44.00%|\n|$L_{ema} + \\mathcal{L}_{src}$|45.96%|\n|EATA|49.81%|\n|EATA+$\\mathcal{L}_{ema}$|50.62%|\n|EATA+**Ours**|51.32%|\n|CoTTA|37.42%|\n|CoTTA+$\\mathcal{L}_{ema}$|46.39%|\n|CoTTA+**Ours**|46.91%|\n|RMT|41.68%|\n|RMT+$\\mathcal{L}_{ema}$|42.29%|\n|RMT+**Ours**|42.78%|\n\n**CIFAR10-C experiments**\n\nWe did not conduct experiments on CIFAR10-C, since CIFAR100-C and ImageNet-C are more difficult tasks with a larger number of classes which are adequate benchmarks to show the validity of our proposed method. \nHowever, we plan to include results on CIFAR10-C in future version.\n\n**Generalization to other tasks**\n\nWhile certain specific implementations need further development, we believe that our proposed method holds the potential for generalization to other tasks, such as semantic segmentation. \nGiven that segmentation involves pixel-wise predictions, the proposed loss terms $\\mathcal{L}_{src}$ \n\nand $\\mathcal{L}_{ema}$ can be applied in a pixel-wise manner.\n\nWe can construct the source prototypes and the EMA prototypes with the features given prior to the prediction head of the segmentation network. \nWe can collect reliable pixel-wise features exploiting the prediction results from the segmentation head, then we can aggregate them to generate the proposed prototypes.  \nThen, with the generated prototypes, we can compute the loss terms $\\mathcal{L}_{src}$ \n\nand $\\mathcal{L}_{ema}$ on predictions given by the segmentation network."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121601915,
                "cdate": 1700121601915,
                "tmdate": 1700122355418,
                "mdate": 1700122355418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "utF3wIU44j",
                "forum": "eXrUdcxfCw",
                "replyto": "B7lR3i6nYq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1201/Reviewer_JC3N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1201/Reviewer_JC3N"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response Acknowledgement"
                    },
                    "comment": {
                        "value": "Thanks to the authors for responding to the queries and providing additional experiments.\n\nThe numbers in the absence of source domain data seem to suggest a marginal improvement for the proposed approach (+Ours).\n\nRegarding generalizing to other problems, even to construct the source and EMA prototypes with the features prior to the prediction head, one would need a notion of class. So, it still seems non-trivial to me as to how to address TTA with the proposed approach for problems where there is no notion of class.\n\nCurrently, I will keep my score unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633055726,
                "cdate": 1700633055726,
                "tmdate": 1700633055726,
                "mdate": 1700633055726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "isypt3elHp",
            "forum": "eXrUdcxfCw",
            "replyto": "eXrUdcxfCw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_eihD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1201/Reviewer_eihD"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of continual test-time model adaptation, where training data is not accessible and only continually changing target domains are available. To keep adapting the model to the continually changing target domains in an online manner, they propose to maintain an exponential moving average target prototype for each class with reliable target samples. In addition, semantic alignment is achieved by matching the target feature to its corresponding pre-computed source prototype. Experiments on standard benchmarks demonstrate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of addressing continuously shifted target domains is a significant yet under-explored topic, especially in the context of test-time adaptation.\n\n- The proposed prototype alignment with EMA target prototypes is simple and easy to implement.\n\n- The experiments conducted in the manuscript provide a comprehensive comparison with the most closely related works, spanning across a broad array of Test-Time Adaptation (TTA) benchmarks and baseline methods."
                },
                "weaknesses": {
                    "value": "- My major concern is about the novelty. Prototypical alignment has been extensively explored in previous domain adaptation methods. The major difference between the proposed approach and those prior efforts seems marginal. I suggest that the authors provide detailed comparisons and showcase why their proposed approach is preferable when applied to test-time adaptation scenarios.\n\n- The writing quality of the article is average, with unclear logical progression and lack of fluency in some parts. There are also numerous instances of imprecise word usage. I recommend that the authors have a native English speaker conduct a thorough proofreading to enhance the clarity, coherence, and overall readability of the manuscript. This will ensure that the paper meets the high standards expected of publications in this field and effectively communicates its contributions to the intended audience."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1201/Reviewer_eihD"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846977029,
            "cdate": 1698846977029,
            "tmdate": 1699636046534,
            "mdate": 1699636046534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "03Gne9vjlD",
                "forum": "eXrUdcxfCw",
                "replyto": "isypt3elHp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1201/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The novelty of the work**\n\nPlease refer to the official comment to all authors about the novelty of this work.\n\n**Writing quality**\n\nRegarding the writing quality, it would be greatly beneficial and constructive if you could pinpoint specific sections and sentences in our paper that do not meet the standards you have mentioned. Your feedback will enable us to enhance the overall writing quality in future version, and we appreciate any guidance you provide in this regard. We also respectfully ask you to consider the reviews of reviewer iXmR and hKy6 that the paper is well-structured and written."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121595682,
                "cdate": 1700121595682,
                "tmdate": 1700121595682,
                "mdate": 1700121595682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]