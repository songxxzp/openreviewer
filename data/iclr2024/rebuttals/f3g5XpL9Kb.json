[
    {
        "title": "LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures"
    },
    {
        "review": {
            "id": "iI31Lf9KzE",
            "forum": "f3g5XpL9Kb",
            "replyto": "f3g5XpL9Kb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8433/Reviewer_d3VV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8433/Reviewer_d3VV"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces LiDAR (Linear Discriminant Analysis Rank) as a novel  metric for assessing the quality of representations within joint embedding (JE) architectures. The authors conduct comprehensive experiments and demonstrate that their proposed LiDAR metric correlates significantly and consistently higher with downstream linear probing performance than RankMe. They further show that LiDAR demonstrates consistently strong performance in hyperparameter selection, outperforming RankMe."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed LiDAR metric for accessing the quality of representations within joint embedding (JE) architectures sounds novel.\n- The theoretical motivation for proposing the LiDAR metric is clear and logical.\n- Comprehensive experiments have been carried out to demonstrate the superiority of LiDAR over RankMe.\n- The paper is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- No disucssions on computational overhead and runtime."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8433/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645209280,
            "cdate": 1698645209280,
            "tmdate": 1699637051473,
            "mdate": 1699637051473,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ApU6OYV8J3",
                "forum": "f3g5XpL9Kb",
                "replyto": "iI31Lf9KzE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer d3VV"
                    },
                    "comment": {
                        "value": "Thank you for carefully reading and reviewing our paper. We are glad to see that the reviewer finds the motivation clear and appreciates the comprehensive nature of our experimental results. We address the weaknesses below:\n\n > No disucssions on computational overhead and runtime.\n\nWe thank the reviewer for bringing up this point as its important for several reasons:\n\n* First is to provide the community with information on run-time overhead of LiDAR to ensure that the overhead is within bounds for the use case considered in the paper. We conduct an experiment where-in we measure the wall-clock time for running RankMe vs LiDAR for I-JEPA evaluation and report our findings in **Appendix 12** and **Table 34**. We include the table at the end of our comment for convenience. We find that the wall-clock time for evaluation on a 1GPU machine has a median value of 0.71 seconds for LiDAR which is very reasonable albeit slower than RankMe which has a median value of 0.38 seconds. The difference is is understandable given the non-optimized nature of our implementation. We find that the times reported above are much smaller than the time taken for feature extraction also reported in **Table 34**; the feature extraction times are of the order of 10s of seconds up to several minutes. Also note that standard linear probing evaluation on the ImageNet-1K dataset takes considerably longer and/or requires multiple GPUs, potential data labeling and curation [can we mention the times we observed from probing runs?].\n* The second reason why the above point raised by the reviewer is important is that our current research findings may serve as a starting point to explore the use of LiDAR directly as a training loss function (regularizer). We think this is a great direction for future work as it motivates exploring techniques from randomized numerical linear algebra (RanNLA) and beyond to ensure an efficient implementation that can work across a range of numerical precisions relevant to practice today.\n\n-----------------\n**Table 34**: Wall-clock time to calculate effective rank for \\emph{RankMe} and \\emph{LiDAR}. The emebddings are gathered from an I-JEPA~\\citep{ijepa} checkpoint for all methods.\n| Method          | Sample count     | Median time (std. dev.) | Feature extraction |\n|-----------------|------------------|-------------------------|--------------------|\n|                 |                  | (seconds)               | (seconds)          |\n| RankMe          | 10K              | 0.38 (2.38)             | 11.29              |\n| LiDAR (Student) | 10K (n=1K, q=10) | 0.53 (0.07)             | 23.22              |\n| LiDAR (Student) | 50K (n=1K, q=50) | 0.71 (0.06)             | 96.95              |\n\nWe once again thank the reviewer for raising a great question. We hope that our response addresses your concerns raised during review. We request the reviewer to provide suggestions that can help us improve the paper further so that it meets the reviewer\u2019s bar for acceptance as a research publication."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264750520,
                "cdate": 1700264750520,
                "tmdate": 1700267421218,
                "mdate": 1700267421218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "viO4WNO2mt",
                "forum": "f3g5XpL9Kb",
                "replyto": "ApU6OYV8J3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Reviewer_d3VV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Reviewer_d3VV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for providing their rebuttal. I have no further questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660232112,
                "cdate": 1700660232112,
                "tmdate": 1700660232112,
                "mdate": 1700660232112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bd8dCj8PYd",
            "forum": "f3g5XpL9Kb",
            "replyto": "f3g5XpL9Kb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
            ],
            "content": {
                "summary": {
                    "value": "The authors present LiDAR, a new metric designed to assess the quality of self-supervised representations in Joint Embedding (JE) architectures using the linear probing protocol, without requiring labeled downstream tasks. LiDAR employs Linear Discriminant Analysis (LDA) to evaluate dimensional collapse and implicitly integrates the SSL objective. Through thorough empirical validation, the authors demonstrate LiDAR's superiority over existing metrics in predicting optimal hyperparameters."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The addressed problem holds significance in self-supervised learning. By evaluating SSL representations without labeled tasks, improvements in hyperparameter selection and algorithm development are facilitated.\n\n2) Utilizing Linear Discriminant Analysis (LDA) to assess the dimensionality collapse of SSL representations is both theoretically grounded and considerably novel. While RankMe assessed SSL methods via the effective rank of the covariance matrix (related to PCA), LiDAR focuses on the rank of the scatter-ratio matrix through LDA.\n\n3) Detailed experiments highlight a significant and consistent positive correlation (measured by Spearman and Kendall coefficients) between LiDAR and linear probing performance on the same source-target datasets, surpassing RankMe across most settings."
                },
                "weaknesses": {
                    "value": "1) The manuscript lacks evaluation on out-of-distribution (OOD) target datasets. Although the ImageNet dataset results are promising, the evidence for LiDAR's effectiveness in more realistic downstream tasks, often involving OOD datasets, is missing. Comparing with datasets like iNaturalist-2018 or Stanford Cars, following the protocol in RankMe, would strengthen LiDAR's position as a useful proxy metric.\n\n2) LiDAR introduces two hyperparameters: the number of surrogate classes (n) and the number of samples per class (q). Yet, the manuscript does not provide guidance on determining these values. Given that these values differ across methods, a hyperparameter sensitivity analysis seems essential.\n\n3) The comparative analysis appears limited, with LiDAR mainly being evaluated against RankMe. Other potential baselines, like $\\alpha$-ReQ mentioned in related work, are overlooked.\n\n4) In Section 4.1, the authors advocate for dimensionality reduction (DR) of embedding features, possibly adding another layer of dependency for LiDAR evaluation. The necessity of DR for LiDAR's effective application, and any specific DR algorithms used in experiments, remain ambiguous.\n\n5) Although the authors claim LiDAR integrates the SSL objective, the manuscript does not delve into the relationship between LiDAR and different SSL losses."
                },
                "questions": {
                    "value": "1) Scatter matrices, $\\Sigma_w$ and $\\Sigma_b$, having dimensions ( p $\\times$ p ), are influenced by both n and the data dimensionality (p). In Section 4.1, the authors note they \"maintain a total of 50 features\" for VICreg. How does this align with rank constraints dictated by the data dimensionality?\n\n2) What guidance can be provided for practitioners to select appropriate values for the number of surrogate classes (n) and the number of samples per class (q)?\n\n3) The authors re-implemented RankMe (without data augmentation) for new architectures, namely I-JEPA and data2vec, using 10k images for rank estimation. This contrasts with the originally suggested 25.6k images. Meanwhile, LiDAR employs at least 50k total samples. What motivated this choice?\n\nThe rebuttal addressed the raised issues well. Therefore, I updated the review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8433/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8433/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8433/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656299870,
            "cdate": 1698656299870,
            "tmdate": 1700813613683,
            "mdate": 1700813613683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wax35Rtd8M",
                "forum": "f3g5XpL9Kb",
                "replyto": "bd8dCj8PYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y3Lm (OOD dataset performance Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for carefully reading and providing a detailed review of our paper! We are very encouraged to read that the reviewer finds the paper has significance to the SSL and representation learning community and recognizes the novelty of the use of Linear Discriminant Analysis (LDA) for evaluating representations! We address the weaknesses and questions raised by the reviewers below:\n\n> The manuscript lacks evaluation on out-of-distribution (OOD) target datasets. Although the ImageNet dataset results are promising, the evidence for LiDAR's effectiveness in more realistic downstream tasks, often involving OOD datasets, is missing. Comparing with datasets like iNaturalist-2018 or Stanford Cars, following the protocol in RankMe, would strengthen LiDAR's position as a useful proxy metric.\n\nThis is a very important and valid point raised by the reviewer. Fortunately, we were able to evaluate the performance of all of the SSL methods considered in our paper namely I-JEPA, data2vec, DINO, VICReg and SimCLR on 5 commonly used datasets that are considered out-of-distribution (OOD) in the SSL literature. We are excited to include them in the paper to strengthen the contribution. The datasets considered here are CIFAR10, CIFAR100, EuroSAT, Food101 and SUN397, which represent a reasonable subset of OOD datasets considered in RankMe [Garrido 2023]. \n\nWe follow the protocol used in RankMe [Garrido 2023] and select the most performant checkpoint for each metric (ImageNet Oracle (based on validation dataset), RankMe, augmented RankMe (where applicable) and LiDAR). These checkpoints are trained and evaluated with the datasets mentioned above. A detailed description of the evaluation protocol is in Appendix Section 10.1 of the paper. The results for the various methods are available in the following tables:\n\n- **Table 22** (average accuracies) and **23** (breakdown by dataset) for I-JEPA where the hyperparameters are set via grid search as done in I-JEPA[Assran2023]\n- **Table 24** (average accuracies) and **25** (breakdown by dataset) for I-JEPA where the hyperparameters are set via random search\n- **Table 26** (average accuracies) and **27** (breakdown by dataset) for data2vec where the hyperparameters are set via grid search\n- **Table 28** (average accuracies) and **29** (breakdown by dataset) for DINO where the hyperparameters are set via random search\n- **Table 30** (average accuracies) and **31** (breakdown by dataset) for VICReg where the hyperparameter values are copied over from RankMe[Garrido2023]\n- **Table 32** (average accuracies) and **33** (breakdown by dataset) for SimCLR where the hyperparameter values are set via random search\n\nWe observe from the average accuracies that LiDAR performs better than RankMe in most cases and matches RankMe performance for a handful of cases considered in our experiments. Notably LiDAR is able to perform better than the ImageNet Oracle in OOD evaluation in certain cases ( see Table 24, Table 28, Table 30) . This observation is consistent with and  similar to the observations made in RankMe[Garrido2023] that suggests new metrics are needed to evaluate downstream performance.\n\nWe have included the average linear probe accuracies calculated on the OOD datasets in the following comments for convenience. These tables and the per-dataset breakdown are also available in the updated draft\n\n**Table 22:** I_JEPA: Average linear probe accuracy recovered by \\emph{RankMe} and \\emph{LiDAR} on OOD datasets. Hyperparameters set via grid search.\n| Metric                            | LR                      | WD                      | Target mask scale            | Context mask scale    | Overall                 |\n|-----------------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|\n| \\textcolor{gray}{ImageNet Oracle} | \\textcolor{gray}{78.49} | \\textcolor{gray}{78.49} | \\textcolor{gray}{78.49} | \\textcolor{gray}{78.49} | \\textcolor{gray}{78.49} |\n| RankMe                            | 72.09                   | 75.45                   | 75.45                   | 75.15                   | 72.09                   |\n| RankMe (aug.) (Student)           | \\textbf{77.78}          | \\textbf{77.78}          | \\textbf{77.78}          | 77.78                   |\\textbf{ 77.78}                   |\n| RankMe (aug.) (Teacher)           | 72.09                   | 72.14                   | 70.20                   | 75.15                   | 72.09                   |\n| LiDAR (Student)                   | \\textbf{77.78}          | \\textbf{77.78}          | \\textbf{77.78}          | 77.78                   | \\textbf{77.78}                   |\n| LiDAR (Teacher)                   | 74.33                   | 76.84                   | 73.91                   | \\textbf{78.30}          | 74.33          |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265861762,
                "cdate": 1700265861762,
                "tmdate": 1700687829406,
                "mdate": 1700687829406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d7kIuheRIt",
                "forum": "f3g5XpL9Kb",
                "replyto": "bd8dCj8PYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y3Lm (OOD dataset performance Part 2)"
                    },
                    "comment": {
                        "value": "**Table 24:** I-JEPA: Average linear probe accuracy recovered by \\emph{RankMe} and \\emph{LiDAR} on OOD datasets. Hyperparameters set via random search.\n| \\textcolor{gray}{ImageNet Oracle} | RankMe | RankMe (aug.) | RankMe (aug.) | LiDAR     | LiDAR          |\n|-----------------------------------|--------|---------------|---------------|-----------|----------------|\n|                                   |        | (Student)     | (Teacher)     | (Student) | (Teacher)      |\n| \\textcolor{gray}{74.85}           | 73.24  | 73.24         | 73.24         | 76.23     | \\textbf{77.34} |\n\n----------------------------\n\n**Table 26:** data2vec: Average linear probe accuracy recovered by \\emph{RankMe} and \\emph{LiDAR} on OOD datasets. Hyperparameters set via grid search.\n| Metric                            | LR                      | Mask ratio              | Overall                 |\n|-----------------------------------|-------------------------|-------------------------|-------------------------|\n| \\textcolor{gray}{ImageNet Oracle} | \\textcolor{gray}{68.69} | \\textcolor{gray}{69.07} | \\textcolor{gray}{68.69} |\n| RankMe                            | 58.86                   | 58.86                   | 58.86                   |\n| RankMe (aug.) (Student)           | 58.86                   | 60.76                   | 60.76                   |\n| RankMe (aug.) (Teacher)           | 58.86                   | 60.76                   | 60.76                   |\n| LiDAR (Student)                   | \\textbf{63.76}          | \\textbf{73.07}          | \\textbf{63.76}          |\n| LiDAR (Teacher)                   | \\textbf{63.76}          | \\textbf{73.07}          | \\textbf{63.76}          |\n\n----------------------------\n\n**Table 28:** DINO: Average linear probe accuracy recovered by \\emph{RankMe} and \\emph{LiDAR} on OOD datasets.\n| Metric                            | Teacher temperature     | Student temperature     | Overall                 |\n|-----------------------------------|-------------------------|-------------------------|-------------------------|\n| \\textcolor{gray}{ImageNet Oracle} | \\textcolor{gray}{84.66} | \\textcolor{gray}{84.59} | \\textcolor{gray}{84.66} |\n| RankMe                            | 84.66                   | 84.46                   | 84.66                   |\n| LiDAR (Student)                   | 84.66                   | 84.72                   | 84.66                   |\n| LiDAR (Teacher)                   | 84.66                   | 84.72                   | 84.66                   |\n\n----------------------------\n\n**Table 30:** VICReg: Average Top-1 probe accuracies (across OOD datasets) computed on representation (following VISSL protocol) when tuning hyperparameters with ImageNet validation performance.\n| Method          | Cov   | inv   | LR    | WD    |\n|-----------------|-------|-------|-------|-------|\n| ImageNet Oracle | 77.83 | 77.61 | 76.94 | 76.97 |\n| RankMe          | 77.53 | 75.53 | 76.94 | 74.93 |\n| LiDAR           | 78.04 | 77.74 | 76.94 | 74.93 |\n\n----------------------------\n\n**Table32:** SimCLR: Average Top-1 probe accuracies (across OOD datasets) computed on representation when tuning hyperparameters (random search) based on ImageNet validation performance.\n\n| Method          | Temp  | LR    | WD    |\n|-----------------|-------|-------|-------|\n| ImageNet Oracle | 80.68 | 80.84 | 80.76 |\n| RankMe          | 78.10 | 79.48 | 79.36 |\n| LiDAR           | 79.88 | 80.30 | 80.24 |\n\n----------------------------------------------------------\n\nWe were unable to use the datasets mentioned by the reviewer for the following reasons: \n\n* StanfordCars was unavailable. We were unable to download the dataset using PyTorch\u2019s torchvision library at the time we conducted our evaluations. \n* iNaturalist-2018 due to its sheer size. Given the short turnaround during rebuttal phase, we did not have sufficient time to run experiments and share the results during the rebuttal phase. \n\nWe will find a different way to download StanfordCars and run evaluations on both StanfordCars and iNaturalist-2018 for the camera-ready version as we agree with the reviewer that it is good to be as comprehensive as possible, though we feel the OOD results are already quite promising."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700266349871,
                "cdate": 1700266349871,
                "tmdate": 1700266501383,
                "mdate": 1700266501383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RQ08zrUD8g",
                "forum": "f3g5XpL9Kb",
                "replyto": "bd8dCj8PYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y3Lm (answers to weaknesses continued Part 3)"
                    },
                    "comment": {
                        "value": "> LiDAR introduces two hyperparameters: the number of surrogate classes (n) and the number of samples per class (q). Yet, the manuscript does not provide guidance on determining these values. Given that these values differ across methods, a hyperparameter sensitivity analysis seems essential.\n\nWe agree with the reviewer on the importance of having guidance on how to set the number of surrogate classes (n) and number of samples per class (q). We first note that the value of n has to be at least as large as the dimension of the features used in LDA analysis to ensure there is no artificial constraint on the rank of these feature matrices. This bound is noted in **Section 4.1 equation (6)**. For q, we note that we only need an adequate number of samples to calculate the scatter matrices. We conduct an empirical study with I-JEPA model that produces feature vectors of length 768. We choose n to be 1000 that clearly satisfies the bound mentioned above. In order to determine the sensitivity of LiDAR score on q, we conduct experiments where we vary q within the range [10, 100] and plot the results in **Figure 23**. Details are included in **Section 11** in the Appendix. We observe from **Figure 23** that the LiDAR score at q=10 is already close to 99% of its final value (and reaches beyond 99% of its final value at q=50). This quick ablation suggests that the value (n=1000, q=50) is reasonable for embeddings provided by I-JEPA. \n\nTo maintain consistency across our implementations, we copy these values for data2vec that outputs embeddings of length 768 and DINO that outputs embeddings of size 256. We note that VICReg and SimCLR models output embedding vectors of length 2048. This forces us to use a larger value for n. We use a value n=5000 for SimCLR and n=10000 for VICReg while keeping q=10 for these models. We acknowledge that the reviewer raises an excellent point but due to heavy computational demands of our experiments, we were not able to perform a detailed sensitivity analysis for all methods. But we note that the heuristics used in choosing hyperparameters are already sufficient to saturate performance. We leave a careful study of these hyperparameters as future work where we may consider this question jointly with the question of LiDAR as a viable training objective.\n\n> The comparative analysis appears limited, with LiDAR mainly being evaluated against RankMe. Other potential baselines, like -ReQ mentioned in related work, are overlooked.\n\nWe thank the reviewer for raising this excellent point. We agree that \\alpha-ReQ is relevant to the topic of representation evaluation. However, we decided to focus on RankMe as it was shown to work better than \\alpha-ReQ in-distribution and comparable OOD (and is currently the state-of-the-art approach for predicting downstream performance for joint-embedding architectures). But we will add comparisons in a future/camera-ready version of the paper.\n\n> In Section 4.1, the authors advocate for dimensionality reduction (DR) of embedding features, possibly adding another layer of dependency for LiDAR evaluation. The necessity of DR for LiDAR's effective application, and any specific DR algorithms used in experiments, remain ambiguous.\n\nWe thank the reviewer for pointing out this ambiguity in our writing. First of all we apologize for the confusion caused by our statement on dimensionality reduction (DR). Our intention is not to argue for dimensionality reduction but to use the trivial bound in equation 6 in Section 4.1 as a guide to select the number of classes. We suggest that the users of LiDAR set n to be greater than the feature vector size to not impose any artificial constraint on the rank (LiDAR score). We follow this rule of thumb to select n for all of the SSL methods considered in our as explained in our answer above to the question on hyperparameter selection. We mention that dimensionality reduction could be potentially used to reduce computational complexity for cases where (p >> n) but we avoid this in our evaluations to ensure fairness. We once again apologize for any confusion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700266674508,
                "cdate": 1700266674508,
                "tmdate": 1700266960783,
                "mdate": 1700266960783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pi5r0452VX",
                "forum": "f3g5XpL9Kb",
                "replyto": "bd8dCj8PYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y3Lm (answers to weaknesses Part 4 and answers to questions Part 1)"
                    },
                    "comment": {
                        "value": "> Although the authors claim LiDAR integrates the SSL objective, the manuscript does not delve into the relationship between LiDAR and different SSL losses.\n\nWe agree with the reviewer that the paper does not attempt to establish any relationship between LiDAR and various SSL objectives used in practice. We proposed LiDAR that captures the discriminative aspect of the SSL methods (using image as a class) and conjecture that the discriminative directions LiDAR is sensitive to are the same discriminative directions learned by various SSL objectives considered in the paper. We focused our efforts in this paper to thoroughly evaluate the effectiveness of LiDAR as a metric that predicts downstream performance for several SSL objectives. We leave the topic of establishing potential relationship between LiDAR and different SSL losses as a topic for future research. We thank the reviewer for their excellent suggestion as we delve deeper into this topic.\n\n**Answers to Questions raised by Reviewer y3Lm**\n> Scatter matrices, and , having dimensions ( p p ), are influenced by both n and the data dimensionality (p). In Section 4.1, the authors note they \"maintain a total of 50 features\" for VICreg. How does this align with rank constraints dictated by the data dimensionality?\n\nWe thank the reviewer for the question. We apologize to the reviewer for the confusion caused above due to multiple typos in our initial draft. We note that the above (n=5K, q=10) LiDAR hyperparamters are used for SimCLR leading to a dataset with 50000 (and not 50) features. Secondly, we use (n=10K, q=10) for VICReg as noted in Section 9.1 in the Appendix of the original draft. We have updated our draft to remove any confusion. Once gain we apologize to the reviewer for causing confusion in our presentation. \n\nThe value of n are chosen for various SSL objectives to ensure that the number of surrogate classes exceeds the length of feature vectors (p). Thus, our choice of n ensures that we do not impose any artificial constraints on the rank estimated by LiDAR.\n\n> What guidance can be provided for practitioners to select appropriate values for the number of surrogate classes (n) and the number of samples per class (q)?\n\n(We repeat our answer from a related question above)\nWe thank the reviewer for raising this important question of how to set number of surrogate classes (n) and number of samples per class (q). We first note that the value of n has to be at least as large as the dimension of the features used in LDA analysis to ensure there is no artificial constraint imposed by LiDAR on these feature vectors. This bound is noted in Section 4.1 equation (6). For q, we note that we only need an adequate number of samples to calculate the scatter matrices. We conduct an empirical study with I-JEPA model that produces feature vectors of length 768. We choose n to be 1000 that clearly satisfies the bound mentioned above. In order to determine the sensitivity of LiDAR score on q, we conduct experiments where we vary q from [10, 100] and plot the results in Figure 23 and include the details in Section 11 in the Appendix. We can observe from Figure 23 that the LiDAR score is already close to 99% of its final value at q=10 and reaches more than 99% of its value for q=50. This quick ablation suggests that the value (1000, 50) is reasonable for embeddings provided by I-JEPA. \n\nTo maintain consistency across our implementations we copy these values for data2vec that outputs embeddings of length 768 and DINO that outputs embeddings of size 256. We note that VICReg and SimCLR models output embedding vectors of length 2048 which forces us to use a larger value for n. We use a value n=5000 for SimCLR and n=10000 for VICReg while keeping q=10 for these models. While a sensitivity study for each method would be ideal, we note that the heuristics used to choose hyperparameters are already sufficient to saturate performance. We leave a careful study of these hyperparameters as future work where we may consider this question jointly with making LiDAR a viable training objective."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700266829608,
                "cdate": 1700266829608,
                "tmdate": 1700266975815,
                "mdate": 1700266975815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b4Olg0hjeq",
                "forum": "f3g5XpL9Kb",
                "replyto": "bd8dCj8PYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y3Lm (answers to questions Part 2)"
                    },
                    "comment": {
                        "value": "> The authors re-implemented RankMe (without data augmentation) for new architectures, namely I-JEPA and data2vec, using 10k images for rank estimation. This contrasts with the originally suggested 25.6k images. Meanwhile, LiDAR employs at least 50k total samples. What motivated this choice?\n\nThe reviewer is correct on using 10000 samples for our rank estimation. Our choice was motivated by a rank convergence analysis for RankMe we conducted (using I-JEPA as our sample architecture which is not covered in RankMe [Garrido 2023]). **Figure 24** summarizes the outcome of the analysis. We observe that we get to greater than 99% of the rank estimate at 25600 samples with just using 10000 samples. This gives us confidence that the number of samples used in our experiments is reasonable. To complete the story we will take the reviewer\u2019s feedback and rerun our analysis with 25600 samples for the camera-ready version of the paper. \n\n**To ensure fairness in our empirical analysis, we evaluate a version of RankMe that we call augmented-RankMe (basically, RankMe evaluated on the same dataset created to evaluate LiDAR).** The results of these comparisons are reported **Table 1, Table 2, Table 3 and Table 4** for **I-JEPA** and in **Table 5 and Table 6** for **data2vec**. We observe from these tables that LiDAR shows better performance both in terms of correlation and linear probe accuracy recovered for various hyperparameter settings used in our experiments. We thank the reviewer for their questions and will address any remaining feedback in the camera-ready version of the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700266914270,
                "cdate": 1700266914270,
                "tmdate": 1700266988791,
                "mdate": 1700266988791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c4EhBbCnoo",
                "forum": "f3g5XpL9Kb",
                "replyto": "b4Olg0hjeq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed rebuttal. If there are follow-up questions, I will post a follow-up comment."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389383747,
                "cdate": 1700389383747,
                "tmdate": 1700389383747,
                "mdate": 1700389383747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YdeIvOVyS3",
                "forum": "f3g5XpL9Kb",
                "replyto": "c4EhBbCnoo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Reviewer_y3Lm"
                ],
                "content": {
                    "comment": {
                        "value": "A follow-up question on some detail:\n\nThe choice of 10k samples for rank estimation for RankMe (baseline) motivated by the convergence analysis (new experiments in Appendix 13): The figure shows that RankMe estimation improves as the number of samples increases to the amount recommended in the original paper (25.6k). What is the motivation for choosing a lower amount (at 99% of rank quality), which probably leads to lower results? What is the performance obtained at 100% quality (25.6 samples)? In the ablation of LiDAR (new experiments in the Appendix 11), we observe the choice of the minimum amount of samples (1000 x 50) that gives the maximum rank quality."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635868453,
                "cdate": 1700635868453,
                "tmdate": 1700635868453,
                "mdate": 1700635868453,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oborAo3bsZ",
                "forum": "f3g5XpL9Kb",
                "replyto": "bd8dCj8PYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Impact of number of samples on RankMe"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their continued engagement with our paper and for their very insightful questions.\n\n> The choice of 10k samples for rank estimation for RankMe (baseline) motivated by the convergence analysis (new experiments in Appendix 13): The figure shows that RankMe estimation improves as the number of samples increases to the amount recommended in the original paper (25.6k). What is the motivation for choosing a lower amount (at 99% of rank quality), which probably leads to lower results?\n\nWe first note here that we use 25600 samples in our analysis of SimCLR, ViCReg and DINO in our paper. \n\nThe choice of 10000 samples applies to I-JEPA and data2vec which are new methods considered in our paper. Our choice of 10000 is motivated in part by the fact that I-JEPA and data2vec output smaller feature vectors compared to the methods in RankMe paper, 768 vs 2048 dimensions and by our ablation in Appendix 13 suggests that 10000 samples can capture 99% of maximum rank. Our conjecture here is that the shape of probe accuracy versus RankMe plot (scatterplot) is important and not necessarily the raw numbers for RankMe and went with 10000 samples in our analysis.\n\n> What is the motivation for choosing a lower amount (at 99% of rank quality), which probably leads to lower results? What is the performance obtained at 100% quality (25.6 samples)?\n\nThe reviewer raises an excellent concern that we share on our side as well. To understand the impact of the number of samples on our analysis, we ran an ablation where we use 25600 for vanilla RankMe and report the results in Appendix 14. Our results suggest that the differences in Kendall's-Tau correlation is minimal. More importantly, there are no changes in the linear probe accuracies recovered with in-domain data (Imagenet-1K) and the OOD performance remains the same as the checkpoints identified by the analysis are the same. We thank the reviewer for pushing for this analysis as it's scientifically useful to understand the properties of RankMe (and other methods) used in this work. \n\n-----------------------------------------\n\nWe include the tables for the number of samples ablation with I-JEPA and data2vec below:\n\n**Table 35:** I-JEPA: Kendall's $\\tau$ coefficient between effective ranks of \\emph{RankMe} and linear probe accuracy. \\emph{RankMe} estimated with  10000 samples and 25600 samples. Hyperparameters are varied via grid search.\n| Hyperparameter     | 10000 samples | 25600 samples |\n|--------------------|---------------|---------------|\n|                    | (RankMe)      | (RankMe)      |\n| Learning rate      | 0.6835        | 0.6835        |\n| Weight decay       | 0.5040        | 0.5050        |\n| Target mask scale  | 0.2867        | 0.2847        |\n| Context mask scale | 0.4246        | 0.4256        |\n| Overall            | 0.5830        | 0.5829        |\n\n**Table 36:** I-JEPA: Kendall's $\\tau$ coefficient between effective ranks of \\emph{RankMe} and linear probe accuracy. \\emph{RankMe} estimated with  10000 samples and 25600 samples. Hyperparameters are varied via random sampling.\n| Hyperparameter | 10000 samples | 25600 samples |\n|----------------|---------------|---------------|\n|                | (RankMe)      | (RankMe)      |\n| Overall        | 0.8314        | 0.8308        |\n\n**Table 37:** data2vec: Kendall's $\\tau$ coefficient between effective ranks of \\emph{RankMe} and linear probe accuracy. \\emph{RankMe} estimated with  10000 samples and 25600 samples. Hyperparameters are varied via grid sampling.\n| Hyperparameter | 10000 samples | 25600 samples |\n|----------------|---------------|---------------|\n|                | (RankMe)      | (RankMe)      |\n| Learning rate  | 0.2410        | 0.2392        |\n| Mask ratio     | 0.2683        | 0.2706        |\n| Overall        | 0.2238        | 0.2216        |\n\nFurthermore, **Figure 25**, **Figure 26** and **igure 27** in **Appendix 14** shows a scatter plot of probe accuracy vs RankMe calculated with 10000 and 25600 samples. These figures show that the differences in RankMe values are small and the impact on our analysis with in-domain and OOD is negligible.\n\n---------------------------------------------\nWe once again thank the reviewer for pushing for this analysis as it's scientifically useful to understand the properties of RankMe (and other methods) used in this work. We hope that our response addresses your concerns raised during review. We request the reviewer to provide suggestions that can help us improve the paper further so that it meets the reviewer\u2019s bar for acceptance as a research publication."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687481346,
                "cdate": 1700687481346,
                "tmdate": 1700688603224,
                "mdate": 1700688603224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3sJviFnc5B",
            "forum": "f3g5XpL9Kb",
            "replyto": "f3g5XpL9Kb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8433/Reviewer_bVwZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8433/Reviewer_bVwZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new metric for the measurement of the representation quality within joint embedding architectures, which is called Linear Discriminat Analysis Rank.\nThis metric discriminate between informative and uninformative features by quantifying the rank of the Linear Discriminant Analysis matrix.\nThe experiments on several downstream tasks show the proposed metric improves the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper generally explain their motivation and inspiration clearly.\n2. The experiments are sufficient to show the effectiveness of the proposed method on the downstream tasks.\n3. The supplementary material shows the details of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The abbreviation of the proposed method \"LiDAR\" is irrelavent of the problem it tries to solve. The irrelavent abbreviation misleads the readers what the paper want to express. Some readers may think the paper is on the lidar.\n\n2. Some typos:\n\na. In section 2.1: In practice, a set of downstream tasks {T_j} are used to asses ->  In practice, a set of downstream tasks {T_j} are used to assess\n\nb. in Section 4.0.1: There is a blank line, where a sentance seem missed."
                },
                "questions": {
                    "value": "Actually, I have no background of the topics of the paper. I cannot assess and ask any questions on this paper. I do not know why this paper is assigned to me. I think the reason maybe the abbreviation of the proposed method \"LiDAR\" is related with my previous paper, however, this paper is not related with lidar at all."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8433/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8433/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8433/Reviewer_bVwZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8433/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658508279,
            "cdate": 1698658508279,
            "tmdate": 1699637051170,
            "mdate": 1699637051170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fI0whHuMWX",
                "forum": "f3g5XpL9Kb",
                "replyto": "3sJviFnc5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8433/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bVwZ"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort in reviewing our paper. \n\n> The abbreviation of the proposed method \"LiDAR\" is irrelavent of the problem it tries to solve. The irrelavent abbreviation misleads the readers what the paper want to express. Some readers may think the paper is on the lidar.\n\nWe recognize the potential for confusion which can often occur when producing names for methods that are intended to be memorable and creative, which we feel strongly about. Therefore, to reduce potential for confusion, we expand the term LiDAR in the abstract to **Li**near **D**iscriminant **A**nalysis **R**ank. We hope that along with the title referring to the goal of the paper this makes it clear to future readers that LiDAR refers to a metric for evaluating representations of joint embedding methods where the name is meant to be a playful way to refer to \u2018sensing\u2019 representation quality, as mentioned in the title.\n\n> a. In section 2.1: In practice, a set of downstream tasks {T_j} are used to asses \u2192 In practice, a set of downstream tasks {T_j} are used to assess\n\n> b. in Section 4.0.1: There is a blank line, where a sentance seem missed.\n\nWe thank the reviewer for carefully reading our paper and bringing the above issues to our notice. We have fixed the issues in the current revision of our draft. We commit to fixing any issues that remain in a camera-ready version of our paper and will ensure the paper goes through a round of proofreading and spellcheck."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8433/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264284796,
                "cdate": 1700264284796,
                "tmdate": 1700264284796,
                "mdate": 1700264284796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]