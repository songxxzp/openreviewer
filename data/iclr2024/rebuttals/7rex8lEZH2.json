[
    {
        "title": "Prompt Tuning with Diffusion for Few-Shot Pre-trained Policy Generalization"
    },
    {
        "review": {
            "id": "jtO04Ep7ge",
            "forum": "7rex8lEZH2",
            "replyto": "7rex8lEZH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_GaVK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_GaVK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach called Prompt Diffuser to prompt-tuning for few-shot pre-trained policy generalization in meta-RL tasks. The approach eliminates the reliance on initial prompts and instead generates high-quality prompts from random noise using a conditional diffusion model. The authors conduct experiments to demonstrate the effectiveness of this approach and compare it to other prompt-tuning techniques. The results show that the Prompt Diffuser outperforms other methods regarding sample efficiency and generalization performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "> To my knowledge, this paper presents a new approach to overcome the limitations of prompt-tuning for PromptDT.\n\n> Based on the experimental results, it is evident that the Prompt Diffuser outperforms the previous method in terms of performance."
                },
                "weaknesses": {
                    "value": "> While the experimental performance of the algorithm is commendable, this paper primarily represents a combination of existing methods and needs more substantial innovation.\n\n> The motivation of the paper needs some clarification. In the abstract, the authors emphasize the limitations of previous prompt-tuning methods when applied to previously **unseen** tasks, highlighting the need for enhanced generalization and diversity in generative models. There appears to be a discrepancy in this viewpoint, as the subsequent sections of the paper emphasize the **accuracy** of generating prompts within the existing prompt distribution. However, the diversity and the accuracy seem to be a trade-off for the generation."
                },
                "questions": {
                    "value": "> Could the authors clarify whether Prompt Diffuser should output prompts that are more generalized for previously unseen tasks or more precise within the existing prompt distribution?\n\n> Could the authors visualize or show the similarity between the generated prompts from Prompt Diffuser and the ground truth prompts under unseen target tasks? \n\n> Is there any reason for using DM rather than DT for prompt generation? Can the prompt generation be integrated into DT directly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688181277,
            "cdate": 1698688181277,
            "tmdate": 1699636261321,
            "mdate": 1699636261321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IIcdmA8aB2",
                "forum": "7rex8lEZH2",
                "replyto": "jtO04Ep7ge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> While the experimental performance of the algorithm is commendable, this paper primarily represents a combination of existing methods and needs more substantial innovation.\n\nThanks for your comment. We appreciate the opportunity to further elucidate the innovative aspects of our work, which we believe significantly advance the field of prompt-tuning in RL.\n\n+ **Significance of Prompt Tuning in RL**: In the current landscape of large-scale models, fine-tuning represents a crucial research paradigm. The adaptability and efficiency of fine-tuning methodologies directly impact the applicability and performance of these expansive models across various domains. While prompt tuning has been previously explored in fields like NLP, its application and phenomena in the RL domain remain largely uncharted territory. Extending prompt tuning to RL not only broadens its scope but also provides an opportunity to explore its efficacy and challenges in a new context. This exploration constitutes a significant contribution to the field, as it opens up a fresh avenue for applying well-established concepts in novel scenarios.\n\n+ **Innovative Application of Conditional Generative Modeling**: As demonstrated in Section 3.3, high-quality prompts play a crucial role in enhancing the performance of RL pre-trained models. However, traditional prompt tuning methods often struggle to achieve optimal results and tend to remain in sub-optimal domains due to the nature of RL formulation. Our approach represents a novel adaptation of prompt-tuning, conceptualized as a form of conditional generative modeling. This is a significant shift from the conventional prompt-tuning methods prevalent in natural language processing (NLP). By applying this concept to the RL domain, we introduce a methodology that goes beyond the traditional boundaries of prompt initialization techniques. This adaptation is a noteworthy innovation, as it leverages the generative capabilities of models to produce prompts from a state of random noise, thereby addressing the limitations inherent in traditional methods.\n+ **Empirical Validation and Ablation Studies**: The empirical validation of our approach is comprehensive. We have conducted a series of rigorous ablation studies, comparative analyses with baseline methods, and detailed scenario-based evaluations. These efforts collectively demonstrate the practical relevance and applicability of our method across various RL settings. This thorough empirical investigation not only validates the performance of our model but also provides insights into its adaptability and efficacy in diverse scenarios.\n+ **Theoretical Contributions**: In addition to empirical validation, our work also makes theoretical contributions to the understanding of prompt-tuning and generative models in the context of RL. These contributions, which we have elaborated on during the rebuttal stage as supplementary material, offer a deeper understanding of the interplay between prompt-tuning, generative modeling, and RL. They provide a foundation for further research and development in this area, highlighting the potential for innovative applications and advancements.\n\nIn conclusion, our work represents a substantial leap in the application of prompt-tuning techniques within the RL domain. The integration of conditional generative modeling, along with our comprehensive empirical and theoretical contributions, underscores the innovative nature of our research. We firmly believe that our approach not only extends the current understanding of prompt-tuning in RL but also opens avenues for future exploration and development in this exciting field."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481290441,
                "cdate": 1700481290441,
                "tmdate": 1700483086739,
                "mdate": 1700483086739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mCUlhWtHHY",
                "forum": "7rex8lEZH2",
                "replyto": "jtO04Ep7ge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q3\n> Could the authors visualize or show the similarity between the generated prompts from Prompt Diffuser and the ground truth prompts under unseen target tasks?\n\nThanks for this suggestion. We have conducted both quantitative and qualitative analyses to evaluate the diversity of the generated prompts, comparing them with baseline methods to underscore their quality and variation. \n\nQuantitative Analysis Using CKA Metric:\n\n+ Our quantitative analysis utilizes the Centered Kernel Alignment (CKA) metric [1], a sophisticated measure widely recognized in machine learning and deep learning research. CKA evaluates the similarity between two sets of features or representations by comparing the alignment of their centered kernel matrices. It produces a single value that quantifies this similarity, with higher values indicating greater resemblance. \n+ To provide a concrete example, we examine the results from the Ant-dir-OOD environment. Here, CKA allows us to quantitatively assess the diversity of prompts generated by different methods. By comparing the CKA scores across various methods, we can determine the extent to which our model generates diverse prompts in comparison to other approaches.\n+ As depicted in the following table, it is important to note that similarity, in isolation, is not a definitive measure of performance. However, the lower similarity score achieved by our Prompt Diffuser, relative to baseline methods, indicates a significant divergence from the established expert distribution. Notably, this divergence does not compromise performance; in fact, our Prompt Diffuser attains the highest performance metrics among the compared methods. This outcome not only demonstrates the effectiveness of our method but also highlights its capability to generate diverse prompts that effectively enhance performance in offline RL scenarios.\n\n\n\n|             | Expert distribution | Prompt-Tuning DT[2] | Prompt Diffuser |\n| ----------- | ------------------- | ---------------- | --------------- |\n| Similarity  | 1.0                 | 0.9514           | 0.9092          |\n| Performance | 526.0 $\\pm$ 1.5     | 540 $\\pm$ 8.7    | 546.8 $\\pm$ 9.3 | \n\n\nQualitative Analysis:\n+ Alongside the quantitative approach, we also conduct a qualitative analysis. This involves visually inspecting and evaluating the prompts generated by our Prompt Diffuser and comparing them with those generated by baseline methods. This qualitative assessment provides a more intuitive understanding of the diversity in the generated prompts.\n+ To visualize the diversity of the prompts, we utilize the t-SNE method for dimensionality reduction and present these prompts in a two-dimensional plane. This visual representation is detailed in the Appendix H of our updated draft. The t-SNE visualization corroborates our quantitative findings, showing that both the Prompt-Tuning DT and Prompt Diffuser deviate from the original distribution to different extents, leading to performance improvements.\n\nIn conclusion, through a comprehensive combination of quantitative (CKA metric) and qualitative analyses, we have rigorously evaluated the diversity of the prompts generated by our model. The results affirm the quality and variation in these prompts, underscoring the effectiveness of our approach in generating diverse and useful prompts for offline RL."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481413025,
                "cdate": 1700481413025,
                "tmdate": 1700661514342,
                "mdate": 1700661514342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HRLxF7F9XN",
                "forum": "7rex8lEZH2",
                "replyto": "jtO04Ep7ge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q4\n> Is there any reason for using DM rather than DT for prompt generation? Can the prompt generation be integrated into DT directly? \n    \nThanks for the comment. The rationale for this choice is grounded in the distinct capabilities of these models, which we elaborate upon as follows:\n+ DMs are renowned for their proficiency in generating high-quality data samples, an essential trait for effective prompt generation in reinforcement learning (RL). In our research, we explored other generative models, such as Variational Autoencoders (VAE), alongside DMs. The comparative results, presented in Appendix Table 5, suggest that the inherent attributes of the generative model significantly influence performance. Specifically, the capacity of DMs to produce in-distribution outcomes and cater to personalized conditions is crucial for our method. This capability has enabled us to reconceptualize prompt tuning within the framework of generative modeling.\n+ DT, fundamentally a discriminative model, is optimized for decision-making based on pre-existing prompts. It relies on these prompts to extract information pertinent to unseen tasks. We experimented with treating the prompt trajectory as part of the input, training it with $L_{DT}$ loss, and then using DT to generate prompts at the outset of the inference stage. As depicted in the table below, the suboptimal performance in this setup can be attributed to the following factors:\n    + Limited Generative Capability: DT\u2019s core strength lies in decision-making informed by given prompts. However, its capacity to generate new, high-quality prompts is inherently limited. This shortfall can result in less effective prompts that may not sufficiently guide the model in unfamiliar scenarios.\n    + Complexity in Training Dynamics: Integrating prompt generation with decision-making in DT adds complexity to the training dynamics. Balancing these two aspects effectively can be challenging, potentially leading to compromises in the performance of either or both tasks.\n\n\n| Env | Prompt Diffuser | All-in-DT |\n| -------- | -------- | -------- |\n| Ant-Dir-OOD     | 546.8 $\\pm$ 9.3     | 479.9 $\\pm$ 5.8     |\n\n\nIn conclusion, while theoretically feasible, the integration of prompt generation into DT presents several practical challenges. These include the limitations of DT in generative tasks, its reliance on the quality of initial prompts, and the complexities introduced in training dynamics. Consequently, our decision to utilize DMs for prompt generation is informed by their superior generative capabilities and adaptability to diverse scenarios, which are pivotal for effective and robust performance in RL settings.\n\n### Reference\n\n[1] Simon, Kornblith, et al. \"Similarity of neural network representations revisited.\" ICML 2019.\n\n[2] Hu, Shengchao, et al. \"Prompt-tuning decision transformer with preference ranking.\" arXiv preprint arXiv:2305.09648, 2023."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481472028,
                "cdate": 1700481472028,
                "tmdate": 1700661585148,
                "mdate": 1700661585148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hc4JTc6YSI",
            "forum": "7rex8lEZH2",
            "replyto": "7rex8lEZH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_zrXj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_zrXj"
            ],
            "content": {
                "summary": {
                    "value": "the author propose Prompt Diffuser (PD), a generative module to augment the prompt tuning decision transformer for reinforcement learning. The authors train a reward-to-go conditioned generative model in order to obtain a better initialization of the prompt. The author provide empirical evidence showing its effectiveness on multiple benchmarks and perform proper ablations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a good paper because it reveals the important insight about prompt turning in RL, which is the importance of visualization. The movitating example in Fig2 is very helpful to establish the hypothesis. The choice of using a conditional diffusion model is well motivated. The empircal results are solid, and the ablation in Table 2 show the effect of the better initialization from the diffusion model."
                },
                "weaknesses": {
                    "value": "I might miss this, but I would encourage the author to also show the OOD generalization to novel environments as is done in the PDT paper section 6.4 to make the results stronger? \n\nMore results on how the diffusion model generalize to unseen reward-to-go?\n\nAny visualization on the diversity of generated prompts/trajectories?"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877865958,
            "cdate": 1698877865958,
            "tmdate": 1699636261242,
            "mdate": 1699636261242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L564seovGl",
                "forum": "7rex8lEZH2",
                "replyto": "hc4JTc6YSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> I would encourage the author to also show the OOD generalization to novel environments as is done in the PDT paper section 6.4 to make the results stronger?\n\nThanks for your valuable suggestion. We acknowledge the importance of such an evaluation to strengthen the results and validate the robustness of our model in novel environments.\n\nTo assess the OOD generalization, we have conducted tests in the Ant-Dir environment with a specific focus on scenarios that fall outside the distribution of our training set. The training and testing sets for this OOD evaluation are detailed as follows:\n\n| Environment | Set Description        | Task IDs                        |\n| ------- | ---------------------- | ------------------------------- |\n| Ant-Dir | Training set of size 8 | [8, 13, 16, 20, 22, 26, 32, 37] |\n| Ant-Dir | Testing set of size 3  | [1, 4, 41]                      | \n\nThe results of this OOD evaluation are presented below. Due to time constraints, we restrict our comparison to PDT (using expert prompts)[2], Prompt-Tuning DT[3], and our Prompt Diffuser. The outcomes indicate that the Prompt Diffuser demonstrates notable efficacy even in these OOD tasks:\n\n| Env         | Prompt-DT-Expert | Prompt-Tuning DT | Prompt Diffuser |\n| ----------- | ---------------- | ---------------- | --------------- |\n| Ant-Dir-OOD | 526.0 $\\pm$ 1.5  | 540.8 $\\pm$ 8.7  | 546.8 $\\pm$ 9.3 |\n\nThese findings suggest that our Prompt Diffuser not only adapts to and performs well in environments it was trained on but also exhibits commendable generalization to novel, out-of-distribution tasks. This robustness in OOD scenarios is a crucial aspect of offline RL and speaks to the effectiveness of our model in diverse and unforeseen environments.\n\n### Q2\n> More results on how the diffusion model generalize to unseen reward-to-go?\n\nThanks for the comment. To address your concern, we have conducted analyses under two distinct scenarios:\n\n**Scenario 1: Inference with Altered Return-to-Go Tokens**:\n\nIn this setup, while the Prompt Diffuser is fine-tuned using few-shot prompt datasets, during the inference stage, we modify the return-to-go tokens. Instead of using the original rtg from the given datasets, we add Gaussian noise to these tokens, thereby presenting unseen return-to-go values to the model. The performance under this scenario is outlined below:\n\n| Env         | Prompt-DT-Expert | Prompt Diffuser | Prompt Diffuser with unseen rtg |\n| ----------- | ---------------- | --------------- | ------------------------------- |\n| Ant-Dir-OOD | 526.0 $\\pm$ 1.5  | 546.8 $\\pm$ 9.3 |       538 $\\pm$ 8.6       |\n\nDespite the Prompt Diffuser not having prior exposure to these altered return-to-go (rtg) values, it can still generate effective prompts that guide the model to perform competently in the Ant-Dir-OOD environment. This outcome underscores the model's resilience and its ability to adapt to variations in task-specific parameters, even in the absence of direct prior exposure to such modified conditions. This ability may be attributed to the robustness of generative modeling and  the diverse set of scenarios and rtg values in the training datasets.\n\n**Scenario 2: Zero-Shot Setting with Unseen Reward-to-Go and Tasks:**\n\nIn a more stringent setting, we evaluate the Prompt Diffuser under zero-shot conditions, where it lacks access to target prompt datasets for reference. This scenario necessitates handling not only unseen reward-to-go but also previously unseen tasks. The results of this rigorous test are as follows:\n\n| Env         | Prompt-DT-zero-shot | Prompt Diffuser zero-shot |\n| ----------- | ------------------- | ------------------------- |\n| Ant-Dir-OOD | 52.7 $\\pm$ 1.9      | 329.2 $\\pm$ 21.8          |\n\nNotably, in the zero-shot setting, the Prompt Diffuser significantly outperforms traditional Prompt-DT, underscoring its ability to adapt to new tasks and rtg values without prior task-specific tuning or exposure. However, it is crucial to acknowledge that the performance in zero-shot settings still trails behind what is achieved in few-shot scenarios.\n\nThese findings demonstrate the versatility and adaptability of our Prompt Diffuser in handling unseen rtg values and tasks. The model's performance in both altered rtg and zero-shot settings highlights its robust generalization capabilities, an essential feature for RL models operating in dynamic and unpredictable environments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481127408,
                "cdate": 1700481127408,
                "tmdate": 1700481127408,
                "mdate": 1700481127408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u0m07SwndH",
                "forum": "7rex8lEZH2",
                "replyto": "hc4JTc6YSI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q3\n> Any visualization on the diversity of generated prompts/trajectories?\n\nThanks for this suggestion. We have conducted both quantitative and qualitative analyses to evaluate the diversity of the generated prompts, comparing them with baseline methods to underscore their quality and variation. \n\nQuantitative Analysis Using CKA Metric:\n\n+ Our quantitative analysis utilizes the Centered Kernel Alignment (CKA) metric [1], a sophisticated measure widely recognized in machine learning and deep learning research. CKA evaluates the similarity between two sets of features or representations by comparing the alignment of their centered kernel matrices. It produces a single value that quantifies this similarity, with higher values indicating greater resemblance. \n+ To provide a concrete example, we examine the results from the Ant-dir-OOD environment. Here, CKA allows us to quantitatively assess the diversity of prompts generated by different methods. By comparing the CKA scores across various methods, we can determine the extent to which our model generates diverse prompts in comparison to other approaches.\n+ As depicted in the following table, it is important to note that similarity, in isolation, is not a definitive measure of performance. However, the lower similarity score achieved by our Prompt Diffuser, relative to baseline methods, indicates a significant divergence from the established expert distribution. Notably, this divergence does not compromise performance; in fact, our Prompt Diffuser attains the highest performance metrics among the compared methods. This outcome not only demonstrates the effectiveness of our method but also highlights its capability to generate diverse prompts that effectively enhance performance in offline RL scenarios.\n\n\n\n|             | Expert distribution | Prompt-Tuning DT | Prompt Diffuser |\n| ----------- | ------------------- | ---------------- | --------------- |\n| Similarity  | 1.0                 | 0.9514           | 0.9092          |\n| Performance | 526.0 $\\pm$ 1.5     | 540 $\\pm$ 8.7    | 546.8 $\\pm$ 9.3 | \n\n\nQualitative Analysis:\n+ Alongside the quantitative approach, we also conduct a qualitative analysis. This involves visually inspecting and evaluating the prompts generated by our Prompt Diffuser and comparing them with those generated by baseline methods. This qualitative assessment provides a more intuitive understanding of the diversity in the generated prompts.\n+ To visualize the diversity of the prompts, we utilize the t-SNE method for dimensionality reduction and present these prompts in a two-dimensional plane. This visual representation is detailed in the Appendix H of our updated draft. The t-SNE visualization corroborates our quantitative findings, showing that both the Prompt-Tuning DT and Prompt Diffuser deviate from the original distribution to different extents, leading to performance improvements.\n\nIn conclusion, through a comprehensive combination of quantitative (CKA metric) and qualitative analyses, we have rigorously evaluated the diversity of the prompts generated by our model. The results affirm the quality and variation in these prompts, underscoring the effectiveness of our approach in generating diverse and useful prompts for offline RL.\n\n### Reference \n\n[1] Simon, Kornblith, et al. \"Similarity of neural network representations revisited.\" ICML 2019.\n\n[2] Xu, Mengdi, et al. \"Prompting decision transformer for few-shot policy generalization.\" ICML 2022.\n\n[3] Hu, Shengchao, et al. \"Prompt-tuning decision transformer with preference ranking.\" arXiv preprint arXiv:2305.09648, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481205584,
                "cdate": 1700481205584,
                "tmdate": 1700661493648,
                "mdate": 1700661493648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hEFNTbXUni",
            "forum": "7rex8lEZH2",
            "replyto": "7rex8lEZH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_4ueS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_4ueS"
            ],
            "content": {
                "summary": {
                    "value": "The work introduces using generative modeling using a diffusion process to do prompt tuning where the prompts are generated using random noise. The method uses signals from downstream tasks to generate better prompts, thus gaining advantage over methods which do not use this form of signal propagation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method provides an interesting way to use diffusion methods for few-shot policy learning.\n2. By using downstream task information and gradient guidance, the task can achieve good performance as compared to baseline methods.\n3. The work is an interesting direction for prompt-tuning methods."
                },
                "weaknesses": {
                    "value": "1. The work doesn't discuss the diversity of the prompts generated. An analysis, quantitative or qualitative, can help understand if there is enough variation in the generated prompts and their comparison with baseline methods.\n2. The length of the prompts is considered small. An ablation over the effect of prompt sizes can be important.\n3. The effect of downstream tasks seems to be an important factor in contributing towards the success of the method. An ablation on the effect of this vs the rest of the method will be useful - one example is - if the downstream task is restricted in some manner.\n4. Although the method considers few-shot settings, an additional examination of the zero-shot setting can be useful to evaluate the transfer capabilities and analyze how tried the generated prompts are to the downstream method."
                },
                "questions": {
                    "value": "1. How is the quality of the prompts generated? Is there enough diversity?\n2. Have different prompt lengths been tried and what is their impact?\n3. What is the impact of quality and size of downstream tasks, and zero-shot performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891265986,
            "cdate": 1698891265986,
            "tmdate": 1699636261156,
            "mdate": 1699636261156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yiUjol2BFk",
                "forum": "7rex8lEZH2",
                "replyto": "hEFNTbXUni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> The work doesn't discuss the diversity of the prompts generated. An analysis, quantitative or qualitative, can help understand if there is enough variation in the generated prompts and their comparison with baseline methods. How is the quality of the prompts generated? Is there enough diversity?\n\nThanks for this suggestion. We have conducted both quantitative and qualitative analyses to evaluate the diversity of the generated prompts, comparing them with baseline methods to underscore their quality and variation. \n\nQuantitative Analysis Using CKA Metric:\n\n+ Our quantitative analysis utilizes the Centered Kernel Alignment (CKA) metric [1], a sophisticated measure widely recognized in machine learning and deep learning research. CKA evaluates the similarity between two sets of features or representations by comparing the alignment of their centered kernel matrices. It produces a single value that quantifies this similarity, with higher values indicating greater resemblance. \n+ To provide a concrete example, we examine the results from the Ant-dir-OOD environment. Here, CKA allows us to quantitatively assess the diversity of prompts generated by different methods. By comparing the CKA scores across various methods, we can determine the extent to which our model generates diverse prompts in comparison to other approaches.\n+ As depicted in the following table, it is important to note that similarity, in isolation, is not a definitive measure of performance. However, the lower similarity score achieved by our Prompt Diffuser, relative to baseline methods, indicates a significant divergence from the established expert distribution. Notably, this divergence does not compromise performance; in fact, our Prompt Diffuser attains the highest performance metrics among the compared methods. This outcome not only demonstrates the effectiveness of our method but also highlights its capability to generate diverse prompts that effectively enhance performance in offline RL scenarios.\n\n\n\n|             | Expert distribution | Prompt-Tuning DT[3] | Prompt Diffuser |\n| ----------- | ------------------- | ---------------- | --------------- |\n| Similarity  | 1.0                 | 0.9514           | 0.9092          |\n| Performance | 526.0 $\\pm$ 1.5     | 540 $\\pm$ 8.7    | 546.8 $\\pm$ 9.3 | \n\n\nQualitative Analysis:\n+ Alongside the quantitative approach, we also conduct a qualitative analysis. This involves visually inspecting and evaluating the prompts generated by our Prompt Diffuser and comparing them with those generated by baseline methods. This qualitative assessment provides a more intuitive understanding of the diversity in the generated prompts.\n+ To visualize the diversity of the prompts, we utilize the t-SNE method for dimensionality reduction and present these prompts in a two-dimensional plane. This visual representation is detailed in the Appendix H of our updated draft. The t-SNE visualization corroborates our quantitative findings, showing that both the Prompt-Tuning DT and Prompt Diffuser deviate from the original distribution to different extents, leading to performance improvements.\n\nIn conclusion, through a comprehensive combination of quantitative (CKA metric) and qualitative analyses, we have rigorously evaluated the diversity of the prompts generated by our model. The results affirm the quality and variation in these prompts, underscoring the effectiveness of our approach in generating diverse and useful prompts for offline RL.\n\n### Q2\n> The length of the prompts is considered small. An ablation over the effect of prompt sizes can be important. Have different prompt lengths been tried and what is their impact?\n    \nThanks for the comment. The majority of methods tend to set the prompt length at 5 [2,3], encompassing essential information to identify the distribution of the given dataset while avoiding direct imitation. Additionally, opting for shorter prompts is more practical as longer prompts require additional manual effort.\n\nWe have also conducted an ablation study focusing on the effects of varying prompt sizes, particularly in the context of the Cheetah-vel environment. This study aims to find an optimal balance between the richness of information provided by longer prompts and the efficiency of the prompt generation process.\n\nOur study explores four different prompt length settings, carefully examining their impact on the performance of our Prompt Diffuser model. The central consideration here is the trade-off between longer prompts, which offer more detailed target tasks information, and the increased computational burden they impose on the diffusion loss, particularly due to the need for more denoising iterations to accurately model the complex, extended prompt distributions.\n\n| Prompt Length| Prompt Diffuser  |\n| -| -  |\n| 2   | -40.2 $\\pm$ 11.2 |\n| 5 | -35.3 $\\pm$  2.4 |\n| 10 | -38.3 $\\pm$ 7.2  |\n| 40  | -45.3 $\\pm$ 4.3  |\n| 40 (with denoising number N increase) | -42.2  $\\pm$ 3.3 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480533195,
                "cdate": 1700480533195,
                "tmdate": 1700661477201,
                "mdate": 1700661477201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a458ue3vmK",
            "forum": "7rex8lEZH2",
            "replyto": "7rex8lEZH2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_V1Dp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3141/Reviewer_V1Dp"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies the generalization challenges in offline reinforcement learning due to the lack of data quantity and quality. The author(s) build a connection with pre-trained large-scale models that share a similar difficulty. Based on that, a new prompt-tuning method using generative modeling is proposed to improve the performance of offline RL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea to identify similar difficulties with PLM and align with state-of-the-art technical is innovative.\n\nThe paper provides a detailed review and a smooth transition from the existing work of both PLMs and offline RL to the proposed work.\n\nThe methodology demonstration is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "The presentation of figures can be improved, e.g., the text in Figure 2 is too small compared to the bar size, so as in Figure 3.\n\nThe proposed method needs theoretical support other than empirical analysis to avoid being a mix of heuristic designs.\n\nFor experiments, the testing scenarios are limited to answering the three important questions at the beginning of Sec. 5. For example, the results of the ablation study in Figure 3 don\u2019t have a common conclusion and need more analysis. Without comprehensive experiments and insights, It\u2019s hard to generalize the approach to other problems."
                },
                "questions": {
                    "value": "How big is the difference between seen and unseen environments and tasks in the testing cases? As this part of the motivation, the results may be sensitive to the scenario changes. Have you quantified the difference that makes the prompt-tuning useful for offline RL?\n\nThe reviewer doubts the effectiveness of using guidance from downstream tasks. Interestingly, Figure 3, which does not have the label for performance metric, doesn\u2019t present the same conclusion for different tasks. Guidance loss may not be useful sometimes. Can you explain why? \n\nWhen considering the two losses, the gradient projection seems beneficial, but is there a comparison to validate the superiority over a linear combination?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3141/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3141/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3141/Reviewer_V1Dp"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699509034800,
            "cdate": 1699509034800,
            "tmdate": 1699636261093,
            "mdate": 1699636261093,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5hzE5f5uFi",
                "forum": "7rex8lEZH2",
                "replyto": "a458ue3vmK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> The proposed method needs theoretical support other than empirical analysis to avoid being a mix of heuristic designs.\n\nThanks for this suggestion! From the perspective of optimization, motivated by [3], we give the following theoretical support for our gradient projection technique for the final performance. \n\n*Assume $L_{DM}$ and $L_{DT}$ are convex and differentiable. Suppose the gradient of $L = L_{DM} + L_{DT}$ is $L$-Lipschitz with $L > 0$. Then, the gradient projection technique with step size $t \\leq \\frac{1}{L}$ will converge to either (1) a location in the optimization landscape where $\\cos(\\phi) = -1$ or (2) the optimal value $L(\\\\theta^\\*)$.* \n\nThe convergence to a point where $\\cos(\\phi) = -1$ indicates a scenario where the gradients of $L_{DM}$ and $L_{DT}$ are in opposite directions. This situation typically arises when there is a conflict between the objectives of the two loss functions. In practical implementation, our priority is to ensure that the DM loss converges normally, even if it means sacrificing the convergence of the DT loss to some extent. This is because the $L_{DM}$ loss plays a crucial role in approximating the distribution of the existing dataset, which sets a foundational benchmark for the quality of the generated prompts.\n\nOn the other hand, convergence to $L(\\theta^*)$ signifies that the optimization process successfully finds the optimal point that minimizes the combined loss. This outcome is the ideal scenario, demonstrating the effectiveness of our gradient projection technique in jointly optimizing both loss functions.\n\nThis theoretical framework addresses a critical challenge in combining multiple loss functions, particularly when these functions have potentially conflicting gradients. Our approach ensures stable and effective optimization, even in complex scenarios where balancing multiple objectives is necessary.\n\nFor ease of understanding,  we present the detailed proof process below (simplified, see the updated draft for the full version):\n\nWe will use the shorthand $|| \\cdot ||$ to denote the $L_2$-norm and $\\nabla L = \\nabla_\\theta L$, where $\\theta$ is the parameter vector.  let $g_1 = \\nabla L_{DM}$, $g_2 = \\nabla L_{DT}$, $g = \\nabla L = g_1 + g_2$, and $\\phi_{12}$ be the angle between $g_1$ and $g_2$.\n\nOur assumption that $\\nabla L$ is Lipschitz continuous with constant $L$ implies that $\\nabla^2 L(\\theta) - LI$ is a negative semi-definite matrix. Using this fact, we can perform a quadratic expansion of $L$ around $L(\\theta)$ and obtain the following inequality:\n\\begin{align*}\nL(\\theta^+) &\\leq L(\\theta) + \\nabla L(\\theta)^T (\\theta^+ - \\theta) + \\frac{1}{2} \\nabla^2 L(\\theta) ||\\theta^+ - \\theta||^2 \n\\leq L(\\theta) + \\nabla L(\\theta)^T (\\theta^+ - \\theta) + \\frac{1}{2} L ||\\theta^+ - \\theta||^2\n\\end{align*}\n\nNow, we can plug in the gradient projection technique by letting $\\theta^+ = \\theta - t\\cdot(g - \\frac{g_1 \\cdot g_2}{||g_1||^2}g_1 - \\frac{g_1 \\cdot g_2}{||g_2||^2}g_2)$. We then get:\n\n\\begin{align*}   \nL(\\theta^+) &\\leq \n     L(\\theta) - (t  - \\frac{1}{2}Lt^2) [(1 - \\cos^2(\\phi_{12})) ||g_1||^2 + (1 - \\cos^2(\\phi_{12})) ||g_2||^2 ]\n    - Lt^2 (1 - \\cos^2(\\phi_{12})) ||g_1|| ||g_2|| \\cos(\\phi_{12}) \\\\\n\\end{align*}\n\nUsing $t \\leq \\frac{1}{L}$, we know that $-(1 - \\frac{1}{2} Lt) = \\frac{1}{2}Lt - 1 \\leq \\frac{1}{2}L(1/L) - 1 = \\frac{-1}{2}$ and $Lt^2 \\leq t$. \n\nPlugging this into the expression above and $\\cos({\\phi_{12}}) < 0$, we can conclude the following:\n\\begin{align*}\n    L(\\theta^+) &\\leq \n     L(\\theta) - \\frac{1}{2}t (1 - \\cos^2(\\phi_{12})) \\|g\\|^2\n\\end{align*}\n\nIf $\\cos(\\phi_{12}) > -1$, then $\\frac{1}{2}t (1 - \\cos^2(\\phi_{12})) \\|g\\|^2$ will always be positive unless $g = 0$. This inequality implies that the objective function\nvalue strictly decreases with each iteration where $\\cos(\\phi_{12}) > -1$.\n\nHence repeatedly applying gradient projection technique process can either reach the optimal value $L(\\theta) = L(\\theta^*)$ or $\\cos(\\phi_{12}) = -1$, in which case $\\frac{1}{2}t (1 - \\cos^2(\\phi_{12})) \\|g\\|^2 = 0$. Note that this result only holds when we choose $t$ to be small enough, i.e. $t \\leq \\frac{1}{L}$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479423415,
                "cdate": 1700479423415,
                "tmdate": 1700483197226,
                "mdate": 1700483197226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vJpUP0QJej",
                "forum": "7rex8lEZH2",
                "replyto": "a458ue3vmK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3141/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(Continuing Q4)\n\nTo further validate the generalization capabilities of our Prompt Diffuser, we also conduct evaluations on out-of-distribution (OOD) tasks. These OOD tasks are specifically chosen to test the model's adaptability and performance under conditions that deviate from the training scenarios. The settings for these OOD tasks in the Ant-Dir environment are as follows:\n\n| Environment | Set Description        | Task IDs                        |\n| ------- | -- | - |\n| Ant-Dir | Training set of size 8 | [8, 13, 16, 20, 22, 26, 32, 37] |\n| Ant-Dir | Testing set of size 3  | [1, 4, 41]                      | \n\nFor these OOD tasks, we present comparative results between PDT, Prompt-Tuning DT, and our Prompt Diffuser as follows. Despite the limited time for conducting extensive tests, our findings indicate that the Prompt Diffuser maintains its efficacy even when faced with OOD tasks. This outcome underscores the robustness of our method, demonstrating its potential to effectively handle a wide array of scenarios, including those that diverge from the training distribution.\n\n| Env         | Prompt-DT-Expert | Prompt-Tuning DT | Prompt Diffuser |\n| -- | - | ---------------- | --------------- |\n| Ant-Dir-OOD | 526.0 $\\pm$ 1.5  |   540.8 $\\pm$ 8.7               | 546.8 $\\pm$ 9.3 |\n\n### Q5\n> When considering the two losses, the gradient projection seems beneficial, but is there a comparison to validate the superiority over a linear combination?\n\nThanks for the comment. To address this, we indeed rely on the results presented in Figure 3, which serve as the empirical foundation for our argument. We also present the detailed table about each result:\n\n\n\n|             | Equation 18 | Equation 19 | Equation 20 | Equation 15 |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| Cheetah-dir | 935.4       | 936.1       | 933.0       | 945.3       |\n| Cheetah-vel | -38.4       | -37.5       | -40.1       | -35.3       |\n| Ant-dir     | 431.7       | 419.3       | 428.9       | 432.1       |\n| MW reach-v2 | 500.5       | 550.0       | 510.8       | 555.7       | \n\n\nIn our study, Equation 15 represents the implementation of our gradient projection technique, while Equation 20 symbolizes the approach of a linear combination of losses. The comparative analysis conducted across all four environments, as depicted in Figure 3, is pivotal in demonstrating the superiority of our method.\n\n+ **Empirical Evidence**: The empirical evidence, as illustrated in Figure 3, consistently shows that Equation 15 outperforms Equation 20 across all environments tested. This consistent outperformance is not just marginal but significant, indicating a clear advantage of the gradient projection technique in enhancing the overall efficacy of our model.\n+ **Theoretical Consideration**: From a theoretical perspective, the gradient projection approach is designed to meticulously balance the contributions of different losses, ensuring that the optimization process is guided efficiently towards the desired objectives. In contrast, a linear combination might not adequately account for the nuanced interactions between the losses, potentially leading to suboptimal outcomes.\n\nIn summary, the empirical results presented in Figure 3, coupled with theoretical considerations, strongly support the superiority of our gradient projection technique over a simple linear combination of losses.\n\n### Q6\n> The presentation of figures can be improved, e.g., the text in Figure 2 is too small compared to the bar size, so as in Figure 3.\n\nThanks for pointing these out. We will improve the figure in the updated version.\n\n### Reference\n[1] Xu, Mengdi, et al. \"Prompting decision transformer for few-shot policy generalization.\" ICML 2022.\n\n[2] Mitchell, E., et al. \"Offline meta-reinforcement learning with advantage weighting.\" ICML 2021.\n\n[3] Yu, Tianhe, et al. \"Gradient surgery for multi-task learning.\" NeurIPS 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480120282,
                "cdate": 1700480120282,
                "tmdate": 1700482832514,
                "mdate": 1700482832514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]