[
    {
        "title": "VIDEOPROMPTER: AN ENSEMBLE OF FOUNDATIONAL MODELS FOR ZERO-SHOT VIDEO UNDERSTANDING"
    },
    {
        "review": {
            "id": "EYpZBP60Ni",
            "forum": "9F0xInGNBF",
            "replyto": "9F0xInGNBF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_FnrC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_FnrC"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes an ensemble framework for video understanding based on pre-trained visual-language models, which involves utilizing LLM to enhance the descriptiveness of text labels and leveraging a video-to-text model to enhance video representations. The authors conducted comprehensive experiments on action recognition, video-text retrieval, and time-sensitive video tasks, demonstrating the effectiveness of the approach in zero-shot scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experiments and ablation studies are conducted comprehensively, validated on different pre-existing architectures, and taken into account various types of video data.\n\nEmploying the video-to-text model (not limited to VGPT and caption models) is novel and worthy to explore in the video field. Fusing the text and video representations is depicted to be beneficial in bridging the gap between video and textual labels in the embedding space.\n\nThe manuscript provides a detailed explanation and examples of prompting the GPT to refine the simple textual label, which in turn enhances reproducibility."
                },
                "weaknesses": {
                    "value": "In the 'video-to-text guided visual feature enhancement' (section 2.2), the adopted VGPT relies on CLIP-ViT-L and vicuna, where the computational cost of performing multiple inferences (including text embedding and filtering) far exceeds that of the basic video understanding model. This limits the practical value of the proposed approach. \n\nExcept for CLIP, an image-language pre-trained model targeted specifically for the image field, the proposed approach shows relatively limited performance gain in other video-based models (ViFi-CLIP, AIM, ActionCLIP), considering the additional computational requirements.\n\nThe configurations of adopted pre-trained models (AIM, ActionCLIP, \u2026) remain unclear, which datasets are these models pre-trained on (e.g. K400, K700, \u2026)? For AIM, do the authors directly remove the classification layers?"
                },
                "questions": {
                    "value": "Are the high-level action contexts mentioned in the manuscript manually designed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698315403732,
            "cdate": 1698315403732,
            "tmdate": 1699636584524,
            "mdate": 1699636584524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ruIDzLgvsT",
                "forum": "9F0xInGNBF",
                "replyto": "EYpZBP60Ni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer  FnrC"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review, below we address the key concerns.\n\n**Q1: *The computational cost of performing multiple inferences far\nexceeds that of the basic video understanding model.***\n\nKindly note that the VideoPrompter offers a flexible framework, allowing\nfor the substitution of various modules, including the video-2-text\nmodel (VGPT), text-2-text model (GPT-3.5), and vision-language model\n(CLIP). During our initial submission, Video-ChatGPT (VGPT) \\[1\\] was\nchosen as the video-2-text model due to its competitive performance when\ncompared to Video-Chat and VideoLLaMA. However, the recent introduction\nof Video-LLaVA \\[2\\], *a model released this week*, has set a new\nstate-of-the-art (SOTA) in zero-shot question-answer and video-based\ngenerative benchmarks. We performed an experiment where we replaced VGPT\nwith Video-LLaVA and generated video-textual descriptions for HMDB-51\nand UCF-101 benchmarks. Conducting a single inference (as opposed to the\n10 inferences with VGPT) without any filtering applied (as now we have\nonly one description per video), we observed a 0.21% and 0.75%\nimprovement on previous scores for HMDB-51 and UCF-101 benchmarks,\nrespectively, as shown in the table below. The adoption of Video-LLaVA\nover VGPT reduced the number of required inferences from 10 to 1,\nleading to a significant decrease in computational cost. Additionally,\nthe per-video inference time for Video-LLaVA is approximately 6 times\nless than that of VGPT, with 3.79s per video, while, for reference, the\ninference time of the CLIP vision encoder is 0.74s per video. We\nanticipate that ongoing advancements in multi-model methods will further\nreduce the inference cost of VideoPrompter. We will include the updated\nresults with Video-LLaVA on all benchmarks and models in our final\nmanuscript.\n|         **Dataset**          | **Baseline (CLIP)** | **Video-Prompter (VGPT)** | **Video-Prompter (LLaVA)** |\n|:---------------------------:|:-------------------:|:--------------------------:|:---------------------------:|\n|           HMDB-51            |         37.5        |           52.51            |          **52.72**          |\n|           UCF-101            |         61.72       |           73.88            |          **74.63**          |\n\n\n**Q2: *Except for CLIP, the proposed approach shows relatively limited\nperformance gain in other video-based models.***\n\nPlease note that the large gain on the vanilla-CLIP can be linked to the\nfact that its not adopted for videos and when its used with our proposed\nframework, without any further training, the VideoPrompter brings\nsignificant boost in the performance. The video-based models such as\nViFi-CLIP are already trained on Kinectics-400 and are adopted to videos\nto some extent. These models when combined with VideoPrompter increase\nthe performance by 6.12%, 4.27%, and 5.3% for ViFi-CLIP, AIM, and\nActionCLIP respectively on HMDB-51 benchmark and similar trend is seen\non other benchmarks as shown in Table-2 and Table-6. Kindly, considering\nthat no further training is performed, and the computational cost has\nbeen substantially reduced after Video-LLaVA, these score increments can\nbe considered significant.\n\n**Q3: *The configurations of adopted pre-trained models (AIM,\nActionCLIP, ...) remain unclear. For AIM, do the authors directly remove\nthe classification layers?***\n\nKindly note that all of the pre-trained models used in this study\n(ViFi-CLIP, AIM, and ActionCLIP) are pre-trained on Kinectics-400.\nSecondly, the original AIM model removed the text classifier from the\nvanilla CLIP and only trained the visual encoder. We removed the\nclassification layers from AIM and used a text classifier from the\nvanilla CLIP with the AIM-visual encoder to get zero-shot results. These\npoints have been made more clear in the original paper, as highlighted\nin red.\n\n\\[1\\] Maaz, et al. \\\"Video-ChatGPT: Towards Detailed Video Understanding\nvia Large Vision and Language Models.\\\" arXiv.\n\n\\[2\\] Lin, et al. \\\"Video-LLaVA: Learning United Visual Representation\nby Alignment Before Projection.\\\" arxiv."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706782815,
                "cdate": 1700706782815,
                "tmdate": 1700708919707,
                "mdate": 1700708919707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7IhGy5eD3U",
            "forum": "9F0xInGNBF",
            "replyto": "9F0xInGNBF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_ypLm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_ypLm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel framework for zero-shot video understanding. The proposed framework, named VideoPromoter, is built by enhancing the visual features as well as the class representations. Experimental results indicate that the proposed method could improve the zero-shot performance of various VLMs across multiple tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper studies an important problem of adapting pre-trained vision-language models to downstream tasks in zero-shot settings.\n2.\tThe introduced method is lucid and holds promise for extension across a wide range of VLMs.\n3.\tThe experimental results look good. VideoPrompter is able to increase the zero-shot performance of VLMs across multiple tasks.\n4.\tThe paper is well-presented."
                },
                "weaknesses": {
                    "value": "1.\tThe efficiency of VideoPrompter hasn't been thoroughly examined. Given that VideoPrompter appears to require generating 10 times the number of samples and the use of an additional text-to-video model, it could substantially raise the inference costs, both for evaluating existing VLMs and in practical applications.\n2.\tThe selection of Video-ChatGPT as the video-to-text model seems arbitrary. Alternative models, such as Video-LLaMA [A], should be considered and discussed.\n3.\tAn ablation study on the video-specific language descriptors is missing.\n\n[A] Zhang, H., Li, X., & Bing, L. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5632/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5632/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5632/Reviewer_ypLm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760528434,
            "cdate": 1698760528434,
            "tmdate": 1699636584426,
            "mdate": 1699636584426,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hhUdkKnGM8",
                "forum": "9F0xInGNBF",
                "replyto": "7IhGy5eD3U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ypLm"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review, below we address the key concerns.\n\n**Q1: *The efficiency of VideoPrompter hasn't been thoroughly examined.\nIt could substantially raise the inference costs.***\n\nKindly note that the VideoPrompter offers a flexible framework, allowing\nfor the substitution of various modules, including the video-2-text\nmodel (VGPT), text-2-text model (GPT-3.5), and vision-language model\n(CLIP). During our initial submission, Video-ChatGPT (VGPT) \\[1\\] was\nchosen as the video-2-text model due to its competitive performance when\ncompared to Video-Chat and VideoLLaMA. However, the recent introduction\nof Video-LLaVA \\[2\\], *a model released this week*, has set a new\nstate-of-the-art (SOTA) in zero-shot question-answer and video-based\ngenerative benchmarks. We performed an experiment where we replaced VGPT\nwith Video-LLaVA and generated video-textual descriptions for HMDB-51\nand UCF-101 benchmarks. Conducting a single inference (as opposed to the\n10 inferences with VGPT) without any filtering applied (as now we have\nonly one description per video), we observed a 0.21% and 0.75%\nimprovement on previous scores for HMDB-51 and UCF-101 benchmarks,\nrespectively, as shown in the table below. The adoption of Video-LLaVA\nover VGPT reduced the number of required inferences from 10 to 1,\nleading to a significant decrease in computational cost. Additionally,\nthe per-video inference time for Video-LLaVA is approximately 6 times\nless than that of VGPT, with 3.79s per video, while, for reference, the\ninference time of the CLIP vision encoder is 0.74s per video. We\nanticipate that ongoing advancements in multi-model methods will further\nreduce the inference cost of VideoPrompter. We will include the updated\nresults with Video-LLaVA on all benchmarks and models in our final\nmanuscript.\n\n|         **Dataset**          | **Baseline (CLIP)** | **Video-Prompter (VGPT)** | **Video-Prompter (LLaVA)** |\n|:---------------------------:|:-------------------:|:--------------------------:|:---------------------------:|\n|           HMDB-51            |         37.5        |           52.51            |          **52.72**          |\n|           UCF-101            |         61.72       |           73.88            |          **74.63**          |\n\n**Q2: *The selection of Video-ChatGPT? Alternative models should be\nconsidered and discussed.***\n\nPlease note that our choice of video-to-text generative model is defined\nby the competitive performance \\[1\\] *(at the time of submission)* of\nVideo-ChatGPT compared to Video-Chat and VideoLLaMA. However, the recent\nintroduction of Video-LLaVA \\[2\\], *a model released this week*, has set\na new state-of-the-art (SOTA) in zero-shot question-answer and\nvideo-based generative benchmarks. We used it as an alternative model\nand show the results below for HMDB-51 and UCF-101 benchmarks. Note\nthat, for Video-LLaVA single inference is performed (as opposed to the\n10 inferences with VGPT) and no filtration is applied (as now we have\nonly one description per video). Thank you for suggesting this analysis.\nWe will include this ablation in our final manuscript.\n\n|         **Dataset**          | **Baseline (CLIP)** | **Video-Prompter (VGPT)** | **Video-Prompter (LLaVA)** |\n|:---------------------------:|:-------------------:|:--------------------------:|:---------------------------:|\n|           HMDB-51            |         37.5        |           52.51            |          **52.72**          |\n|           UCF-101            |         61.72       |           73.88            |          **74.63**          |\n\n\n**Q3: *An ablation study on the video-specific language descriptors is\nmissing.***\n\nAs recommended, detailed ablation on language descriptors is provided\nbelow. We individually analyze the following components: 1)\nclass-attributes, 2) class-descriptions, and 3) action-context.\nMoreover, we also analyzed various combinations of these components such\nas 1) class-attributes + class-descriptions, 2) class-attributes +\nclass-descriptions + action-context. Thank you for suggesting this\nanalysis. We will include this ablation in our final manuscript.\n\n| **Dataset** | **CLIP** | **Att** | **Des** | **Att + Des** | **Att + Des + Action** |\n|:-----------:|:--------:|:-------:|:-------:|:-------------:|:------------------------:|\n| HMDB-51     |   37.5   |  39.50  |  43.35  |     46.11     |       **48.57**          |\n| UCF-101     |  61.72   |  68.88  |  65.87  |     71.61     |       **72.07**          |\n|    SSv2     |   2.72   |   2.81  |   3.19  |   **3.34**    |            --              |\n\n\n\\[1\\] Maaz, et al. \\\"Video-ChatGPT: Towards Detailed Video Understanding\nvia Large Vision and Language Models.\\\" arXiv.\n\n\\[2\\] Lin, et al. \\\"Video-LLaVA: Learning United Visual Representation\nby Alignment Before Projection.\\\" arxiv."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707896635,
                "cdate": 1700707896635,
                "tmdate": 1700708976256,
                "mdate": 1700708976256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gt5qdFFKFi",
            "forum": "9F0xInGNBF",
            "replyto": "9F0xInGNBF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_1vg9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_1vg9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to ensemble multiple large foundation models to enhance the zero-shot inference performance on video understanding tasks (namely VideoPrompter), including video action recognition, video-to-text and text-to-video retrieval, and time-sensitive (before/after) video tasks. The main architecture is based on CLIP, where classification can be performed by ranking the cosine similarity between visual and text representations, and the main idea is to enrich both the video and text embeddings. For the video part, the authors employ Video-Chat GPT (VGPT) (Maaz et al., 2023) to extract the text description of the query video and convert it into a video-to-text embedding with the text encoder in CLIP. The video-to-text embedding is then ensembled with the visual embedding encoded by the original CLIP visual encoder as the final visual embedding. For the text part, they prompt GPT-3.5 to rephrase the class names with parent context, language attributes, and language descriptions. All the descriptions are ensembled to generate the final text embedding. Experiments show that VideoPrompter can improve over plain zero-shot inference performance with CLIP and its variants."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The studied problem is interesting. Video understanding with large foundation models is of wide interest in the community.\n2. The authors put together state-of-the-art large foundation models and improve the zero-shot inference performance on video understanding tasks."
                },
                "weaknesses": {
                    "value": "1. The idea of generating more descriptions for class names and using high-level context is not new in prompting large foundation models (e.g. the prior works cited in this paper). This is model ensembling for enhancing zero-shot performance. Can the authors justify the main novelty of this paper?\n2. VGPT is used to generate the text description of the query video, and which is then converted to an image-like text embedding. Why not just prompting VGPT for the downstream applications (e.g. action classification)? Comparison to this baseline is an important justification to the proposed method.\n3. Several components are added to the solution, while the ablations are not sound enough. For example, how important are the three description types (parent context, language attributes, and language descriptions)?\n4. The claim for the comparison to CUPL (Pratt et al., 2022) is not very clear (section 3.1.4). The authors claim that VideoPrompter only requires 3 text descriptions instead of 50 descriptions adopted in CUPL. However, VideoPrompter adopts a VGPT model while CUPL does not. Is using VGPT a better choice in terms of the cost?\n5. The paper criticizes prior work that \u201cthese methods require access to the true distribution of the target task, which can be prohibitive in test-time adaptation and data-scarce environments\u201d. However, the proposed method optimizes the selection of hyperparameters (e.g. temperature) directly on the target dataset (see Figure 3). \n6. The high-level action context is restricted to a tree-type relation. However, some child classes may belong to multiple parent concepts. For example, \u201csurfing\u201d can belong to both \u201cplaying sports\u201d and \u201cwater activities\u201d."
                },
                "questions": {
                    "value": "My questions are listed in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5632/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5632/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5632/Reviewer_1vg9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788269173,
            "cdate": 1698788269173,
            "tmdate": 1699636584258,
            "mdate": 1699636584258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2IFt7atGGC",
                "forum": "9F0xInGNBF",
                "replyto": "Gt5qdFFKFi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review, below we address the key concerns.\n\n**Q1: *Can the authors justify the main novelty of this paper?***\n\nKindly note that our work is the first one to study the role of\nvideo-to-text models for video understanding and highlights how video descriptions can\nfurther enhance visual representations. So far, the existing methods\n\\[1,2\\] are only limited to text-to-text generative models for enhancing\nthe text classifier representations only. We summarize our contributions\nas follows:\n\n1.  So far, only text-to-text generative models (e.g. GPT) are used to\n    enhance the representations of the text classifier. Our work\n    discusses the impact of having video-to-text generative models (e.g.\n    VGPT) to enrich the visual representations as well, in addition to\n    only text-based models (Section 2.2).\n\n2.  We propose a novel way of querying LLMs called action context\n    (Section 2.3.2). Action context can classify semantically close\n    videos under a single high-level context (category). In the related\n    work, we discuss the limitations of the existing methods \\[3,4\\] and\n    discuss how these methods are only applicable to image benchmarks\n    and are unsuited for video benchmarks where the classes can be\n    fine-grained as well can have a diverse range.\n\n3.  We test our framework in three different video settings namely:\n    action recognition, video-to-text, and text-to-video retrieval, and\n    time-aware video tasks. We show results with 4 different models on 7\n    benchmarks.\n\n**Q2: *Why not just prompting VGPT for the downstream applications?***\n\nAs recommended, we designed an experiment where only VGPT is directly\nused to classify the input video. We found this setting to degrade the\nresults by a large margin. We relate this to the training of VGPT as it\nis trained on generating long contexts about the input, rather than\nspecific categories \\[5\\]. We also present a more extended setting,\nwhere the descriptions from VGPT are given to GPT-3.5 to predict the\nclass name. Here, the GPT-3.5 is also provided by the list of class\nnames. It can be seen that VideoPrompter outperforms both of\nthe aforementioned settings which endorses its design choice. Thank you\nfor suggesting this analysis. We will include this ablation in our final\nmanuscript.\n|     **Datasets**     | **Vanilla-CLIP** | **VGPT only** | **VGPT+GPT** | **VideoPrompter** |\n|:--------------------:|:----------------:|:-------------:|:------------:|:------------------:|\n|       HMDB-51        |       37.5       |      16.43    |      46.0      |      **52.51**     |\n|       UCF-101        |       61.5       |      39.21    |      68.0      |      **73.88**     |\n\n**Q3: *The comparison with CUPL.***\n\nWe regret any confusion, it's important to note that a direct comparison\nwith CUPL may not be straightforward, as it can be viewed as a\ncomplementary approach to our method. As discussed in \\[3\\], the\nperformance of such methods benefits from averaging over a large number\nof descriptors. When we combined CUPL with\nVideoPrompter i.e. weights of language descriptors are averaged,  performance\nincreased by  0.12% for UCF-101. We will further update\nthis discussion in our final manuscript.\n\n**Q4: *The proposed method optimizes the selection of hyperparameters\ndirectly on the target dataset.***\n\nKindly note that Figure 3 is mainly added to show an ablation analysis.\nThe selection of hyper-parameters in our work follows the standard\nsettings in recent studies \\[1,6\\]. For instance, \\[1\\] includes a\nthorough discussion of how a high value of temperature leads to more\ndiverse descriptions. And \\[6\\] discusses a similar filtering method.\n\n**Q5: *The Action context is restricted to a tree-type relation.***\n\nWe thank the reviewer for the pointer, this indeed could be true for\nsome benchmarks as some classes may overlap. We designed an experiment\nwhere we prompted the LLM and explicitly mentioned that one child class\ncan be assigned to multiple parent classes. We found such cases to be\nrelatively small, e.g. we didn't find any such case for HMDB-51 and only\na few for UCF-101, which increased our performance by 0.06% on top of\nthe previous score. We will include the updated results on all\nbenchmarks and models in our final manuscript.\n\n\\[1\\] Pratt, et al. \\\"What does a platypus look like? generating\ncustomized prompts for zero-shot image classification.\\\" ICCV.\n\n\\[2\\] Menon. \\\"Visual classification via description from large language\nmodels.\\\" arXiv.\n\n\\[3\\] Roth, et al. \\\"Waffling around for Performance: Visual\nClassification with Random Words and Broad Concepts.\\\" arXiv.\n\n\\[4\\] Novack, et al. \\\"Chils: Zero-shot image classification with\nhierarchical label sets.\\\" ICML.\n\n\\[5\\] Maaz, et al. \\\"Video-ChatGPT: Towards Detailed Video Understanding\nvia Large Vision and Language Models.\\\" arXiv.\n\n\\[6\\] Li, et al.\\\"Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation.\\\" ICML."
                    },
                    "title": {
                        "value": "Response to Reviewer 1vg9"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708842152,
                "cdate": 1700708842152,
                "tmdate": 1700709041006,
                "mdate": 1700709041006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CsiiZNAJmc",
            "forum": "9F0xInGNBF",
            "replyto": "9F0xInGNBF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_8fLg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5632/Reviewer_8fLg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework for zero-shot video understanding by using various foundation models including VLMs, i.e., CLIP, LLMs, i.e., GPT, and Video-to-Text model, i.e., VGPT. Experiments are conducted on three different problem settings and showing good results. Ablations are thorough and enough to justify the framework design choices. Written presentation is fair, but could be improved."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a set of experiments on various problem settings: action recognition, video-to-text and text-to-video retrieval, time-sensitive tasks and on different datasets.\n- The ablations are solid and thorough.\n- Experiments show strong improvement w.r.t baselines."
                },
                "weaknesses": {
                    "value": "- Since at least 3 foundation models have been used (CLIP, GPT, VGPT), how do we know if those models are trained with examples overlapped with the downstream datasets (e.g., HMDB-51, UCF101, SSv2, K400, MSR-VTT, Charades).\n\n- The novelty seems moderate if not low. As the paper mentions the main contributions are 1) introducing video-to-text to enhance visual embeddings and 2) applications to videos.  \n\n- The written presentation could be further improved:\n     1) section 2.1 could be renamed to \"Overview\" and try to capture the big picture of the framework. The author(s) can refer back to Fig. 1 for the big picture (in the current flow of presentation, there is no big picture and it flows in with overwhelming many details and notations). Then sections 2.2 and 2.3 can be further followed up from 2.1 to provide detailed of components.\n     2) table 6 is presented in page 7, yet never been referred from the text?"
                },
                "questions": {
                    "value": "- My main concerns are the leaking examples from downstream datasets to foundation models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797226215,
            "cdate": 1698797226215,
            "tmdate": 1699636584165,
            "mdate": 1699636584165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j4jAwQdayG",
                "forum": "9F0xInGNBF",
                "replyto": "CsiiZNAJmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8fLg"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review, below we address the key concerns.\n\n**Q1: *Leaking examples from downstream datasets to foundation\nmodels*.**\n\nKindly note that the CLIP is trained on 400M image-text pairs and the\nVGPT employs video-text pairs and is trained on a subset of\nActivityNet-200. While the GPT is only trained on the text data. To\ninvestigate any overlapping with the downstream datasets we design the\nfollowing experiment and present results on HMDB-51 and UCF-101\nbenchmarks:\n\n1.  We show zero-shot results of vanilla-CLIP on HMDB-51 and UCF-101\n    benchmarks to understand the extent of knowledge the CLIP model\n    contains about these datasets.\n\n2.  Second, to understand any data overlap of VGPT, we only employ VGPT\n    directly to classify the input video. We found this setting to\n    degrade the results by a large margin. We relate this to the\n    training of VGPT as it is trained on generating long contexts about\n    the input, rather than specific categories \\[1\\].\n\nKindly, from the table below, it can be seen that the individual\nperformance of these modules (vanilla-CLIP and VGPT) is substantially\nlow, which shows minimum data overlap. Thank you for suggesting this\nanalysis. We will include this ablation in our final manuscript.\n\n|   **Datasets**   | **CLIP** | **VGPT only** | **VideoPrompter** |\n|:----------------:|:--------:|:-------------:|:------------------:|\n|     HMDB-51      |   37.5   |     16.43     |      **52.51**     |\n|     UCF-101      |   61.5   |     39.21     |      **73.88**     |\n\n\n**Q2: *The novelty seems moderate if not low.***\n\nKindly note that our work is the first one to study the role of\nvideo-to-text models for video understanding. Our work highlights how\nthe recent advances in the video-to-text models can be used in the\ngeneral video-understanding pipeline, and how video descriptions can\nfurther enhance the visual representations. So far, the existing methods\n\\[2,3\\] are only limited to text-to-text generative models for enhancing\nthe text classifier representations only. We summarize our contributions\nas follows:\n\n1.  So far, only text-to-text generative models (e.g. GPT) are used to\n    enhance the representations of the text-classifier. Our work\n    discusses the impact of having video-to-text generative models (e.g.\n    VGPT) to enrich the visual representations as well, in addition to\n    only text-based models (Section 2.2).\n\n2.  We propose a novel way of querying LLMs called action context\n    (Section 2.3.2). Action context can classify semantically close\n    videos under a single high-level context (category). In the related\n    work, we discuss the limitations of the existing methods \\[4,5\\] and\n    discuss how these methods are only applicable to image benchmarks\n    and are unsuited for video benchmarks where the classes are\n    fine-grained as well as have diverse ranges.\n\n3.  We test our framework in three different video settings namely:\n    action recognition, video-to-text, and text-to-video retrieval, and\n    time-aware video tasks. We show results on 7 benchmarks including\n    HMDB-51, UCF-101, SSv2, K400, MSRVTT, Charades, and the time-aware\n    synthetic dataset. Moreover, as our framework offers plug and play\n    module, we show results with various models such as CLIP, ViFi-CLIP,\n    Action-CLIP, and AIM.\n\n**Q3 (a): *The written presentation could be further improved.***\n\nPlease note that, sections 2.1, 2.2, and 2.3 have been updated. Now, for\nmore clarity we have re-named section 2.2 from video-specific language\ndescriptors to class-specific language descriptors. The revised text is\nhighlighted in red.\n\n**Q3(b): \"*table 6 is presented in page 7, yet never been referred from\nthe text?***\n\nKindly note that table 6 is referred in video action recognition section\non page 6.\n\n\n\\[1\\] Maaz, et al. \\\"Video-ChatGPT: Towards Detailed Video Understanding\nvia Large Vision and Language Models.\\\" arXiv.\n\n\\[2\\] Pratt, et al. \\\"What does a platypus look like? generating\ncustomized prompts for zero-shot image classification.\\\" ICCV.\n\n\\[3\\] Menon. \\\"Visual classification via description from large language\nmodels.\\\" arXiv.\n\n\\[4\\] Roth, et al. \\\"Waffling around for Performance: Visual\nClassification with Random Words and Broad Concepts.\\\" arXiv.\n\n\\[5\\] Novack, et al. \\\"Chils: Zero-shot image classification with\nhierarchical label sets.\\\" ICML."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706291532,
                "cdate": 1700706291532,
                "tmdate": 1700709092015,
                "mdate": 1700709092015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]