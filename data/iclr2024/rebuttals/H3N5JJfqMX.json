[
    {
        "title": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning"
    },
    {
        "review": {
            "id": "f7ulG3mNUp",
            "forum": "H3N5JJfqMX",
            "replyto": "H3N5JJfqMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_tEiT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_tEiT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use semi-supervised learning methods for Bayesian optimization (BO). The idea is to use an alternative paradigm for BO instead of fitting a regressor to the observed data. The authors suggest to use density ratio estimation BO instead. In DRE-BO one uses a classifier as the model to guide the search and the acquisition function is computed in terms of the class probability ratios. The authors suggest to strengthen the classifier accuracy by using semi-supervised learning techniques. The method is compared to other strategies in synthetic and real problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Extensive experimental evaluation."
                },
                "weaknesses": {
                    "value": "The introduction is poor. It does not introduce properly the problem addressed in the paper.\n\nIn general the writing of the paper has to be improved a lot. It does not introduce the concept of DRE-based BO right. If the reader is not familiar with it, they cannot understand it properly. The authors have failed in this task.\n\nIt is not clear what is the motivation for DRE-based BO. It is also unclear how the threshold value y^t is chosen.\n\nThe authors have to better explain DRE-based BO, why does it work, why is it interesting and why it is better than regression based BO.\n\nFigure 1 is not explained properly.\n\nThe proposed method is not very well motivated. It seems it is simply using a better classifier. The authors claim that they use semi-supervised learning techniques to train the classifier. However, the semi-supervised data seems to be generated by sampling from a truncated Gaussian and then label propagation is used to generate the associated labels. Therefore, the proposed method can be understood simply as using a better classifier. Given this, I do not find that much novelty in the proposed method."
                },
                "questions": {
                    "value": "Please, explain better figure 1 and why it illustrates the over-confidence problem.\n\nPlease, explain how the threshold value y_t is chosen.\n\nPlease, explain why the proposed method needs to sample from a truncated multivariate Gaussian distribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4079/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4079/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4079/Reviewer_tEiT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663529617,
            "cdate": 1698663529617,
            "tmdate": 1700665313662,
            "mdate": 1700665313662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mki8MJ9Vlo",
                "forum": "H3N5JJfqMX",
                "replyto": "f7ulG3mNUp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tEiT (1/n)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your constructive comment to improve our work.\n\n> The introduction is poor. It does not introduce properly the problem addressed in the paper.\n\n> In general the writing of the paper has to be improved a lot. It does not introduce the concept of DRE-based BO right.\n\nWe will revise our paper more carefully in the final version.\n\n> It is not clear what is the motivation for DRE-based BO. It is also unclear how the threshold value $y^t$ is chosen.\n\n> Please, explain how the threshold value $y_t$ is chosen.\n\nWe followed the previous work (Tiao et al., 2021; Song et al., 2022). In order to focus on the use of unlabeled points in the process of DRE-based Bayesian optimization, the selection of $y^t$ is built on the previous literature. Please refer to the prior work.\n\n> The authors have to better explain DRE-based BO, why does it work, why is it interesting and why it is better than regression based BO.\n\nWe have included why DRE-based Bayesian optimization works in **Section 3**. Moreover, compared to GP-based Bayesian optimization, surrogate model learning is more efficient. In particular, while GP scales cubically with the number of points, learning a classifier is likely to be faster. Moreover, as shown in the experiment **Minimum Multi-Digit MNIST Search** of **Section 5.2**, the use of classifiers helps learn the representation of high-dimensional data.\n\n> Figure 1 is not explained properly.\n\n> Please, explain better figure 1 and why it illustrates the over-confidence problem.\n\nTo accommodate your comment, we have updated **Section 1.1**. Please see **Section 1.1**.\n\n> The proposed method is not very well motivated. It seems it is simply using a better classifier. The authors claim that they use semi-supervised learning techniques to train the classifier.\n\nWe do not agree with this comment. Our method offers a way to employ unlabeled points into the procedure of Bayesian optimization. In particular, in the scenario of pool-based Bayesian optimization, the vanilla Bayesian optimization does not utilize unlabeled points in a pool. Our method with semi-supervised learning enables us to utilize them in Bayesian optimization. Therefore, our contributions are more than simply using a better classifier.\n\n> Please, explain why the proposed method needs to sample from a truncated multivariate Gaussian distribution.\n\nThe search space of Bayesian optimization is generally defined as a hypercube. Thus, we require utilizing a truncated multivariate Gaussian distribution so that the sampled points should be confined in the hypercube."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522609220,
                "cdate": 1700522609220,
                "tmdate": 1700522609220,
                "mdate": 1700522609220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "16xoFdkYPh",
                "forum": "H3N5JJfqMX",
                "replyto": "mki8MJ9Vlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4079/Reviewer_tEiT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4079/Reviewer_tEiT"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I acknowledge the efforts made by the authors for improving their paper and hence I have increased a bit my score in consequence."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665296136,
                "cdate": 1700665296136,
                "tmdate": 1700665296136,
                "mdate": 1700665296136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jl03A3DXos",
            "forum": "H3N5JJfqMX",
            "replyto": "H3N5JJfqMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_NoA7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_NoA7"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the binary classifier-based Bayesian optimization such that the classifier is trained in a semi-supervised manner. The authors argue that the semi-supervised classifier expands the region of ${\\bf x}$ associated with high probability $P(y \\leq y^\\dagger | {\\bf x}, \\mathcal{D})$, which leads to more efficient exploration of black-box optimization. \nProposed method, called DRE-BO-SSL, is compared with the ordinary BO as well as DRE-BO with sueprvised classification approaches for function optimization as well as hyperparameter tuning tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea sounds sensible to incorporate semi-supervised learning for encouraging the exploration of DRE-based Bayesian optimization method. \nFigure 1 illustrates how the search space is explored by the proposed framework. \nWith that said, the figure needs more explanation for better comprehension of its content. \nFor example, the color bar is labeled as *Class Probability*, which suggests something like $p(y \\leq y^\\dagger | {\\bf x}, \\mathcal{D})$.\nHowever, in the text most probability is shown as the distribution over the input such as $p({\\bf x} | y \\leq y^\\dagger, \\mathcal{D})$. \nWhat does the figure specifically illustrate?"
                },
                "weaknesses": {
                    "value": "The presentation of current manuscript is problematic in that many things are uncertain from the text. \nSee *Questions* part below for details. The reviewer believes this information is necessary to better understand the method and enhance the reproducibility of results.\n\nUnfortunately, the efficacy of proposed method in the performance of optimization over iterations is hardly distinguishable in Figs. 5 or 6. \nI acknowledge a clear victory is not so common in comparisons of black-box optimization methods. \nThe bigger problem is that it is unclear  from the text on what condition and why the proposed method outperforms the existing approaches."
                },
                "questions": {
                    "value": "1. Truncated normal distribution for sampling unlabeled data points. \n\nHow is matrix ${\\bf A}$ designed? What covariance is taken for this disbiribution, for what? \nWhat are lower and upper bounds ${\\bf l}$ and ${\\bf u}$? \nAre they boundaries of search space $\\mathcal{X}$? Is it assumed to be rectangular?\n\n2. Fixed-size pool. \n\nDoes *fixed-size pool* scenario mean some finite number of candidate points ${\\bf x}_i$ are selected in advance and that optimized over these points? \nIf true, is the set of points dynamically updated or fixed in advance? \n\n3. Definition of simple regret. \n\nWhat is the definition of simple regret? \nIs it $f({\\bf x}_n) - f^*$ in the $n$th iteration, where $f^*$ is the minimizer of function $f$. \nI am wondering why the regret is monotonically decreasing in Fig. 2. \nDoes this plot $\\min_n f({\\bf x}_n) - f^*$?\n\n4. What is dimensionality of ${\\bf x}$ in four benchmark tasks in Fig. 2. \n\nAre they all two? This is crucial information on the difficulty of black-box optimization problems. \n\n5. Hyperparameter optimization of Fig. 6\n\nHow do you define the distance $\\|{\\bf x}_i - {\\bf x}_j \\|$, especially when categorical values are involved such as activation function and learning rate schedule."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4079/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4079/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4079/Reviewer_NoA7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751966229,
            "cdate": 1698751966229,
            "tmdate": 1699636372341,
            "mdate": 1699636372341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ICjxlgIsjK",
                "forum": "H3N5JJfqMX",
                "replyto": "jl03A3DXos",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NoA7 (1/n)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your constructive comment to improve our work.\n\n> What does the figure specifically illustrate?\n\nWe illustrated $p(\\mathbf{x} | y \\leq y^\\dagger, \\mathcal{D})$. We would like to emphasize that we do not model $y \\leq y^\\dagger$ and instead it indicates some class, e.g., Class 1, which implies points that might have function values smaller than $y^\\dagger$. By following the conventional expression of machine learning, the probability we illustrated is expressed as $p(\\mathbf{x} | c = 1, \\mathcal{D})$ where the probability of interest is the probability of $c = 1$.\n\n> Unfortunately, the efficacy of proposed method in the performance of optimization over iterations is hardly distinguishable in Figs. 5 or 6.\n\nWe disagree with this comment. Our methods quickly converge to solutions compared to other baseline methods. In particular, **Figure 6** shows the superior performance of our methods.\n\n> The bigger problem is that it is unclear from the text on what condition and why the proposed method outperforms the existing approaches.\n\nBased on the use of unlabeled points and semi-supervised learning, our methods can estimate more informative probabilities, thereby easing the over-confidence problem discussed in **Figures 1 and 8** and **Section 1.1**.\n\n> How is matrix $\\mathbf{A}$ designed? What covariance is taken for this disbiribution, for what? What are lower and upper bounds $\\mathbf{l}$ and $\\mathbf{u}$? Are they boundaries of search space $\\mathcal{X}$? Is it assumed to be rectangular?\n\nCovariance matrix was designed as an identity matrix, and then $\\mathbf{A}$ can be calculated. Since the search space of Bayesian optimization is often assumed to be a hypercube, we also used a hypercube space. Thus, $\\mathbf{l}$ and $\\mathbf{u}$ can be determined by the search space given. We have updated our submission accordingly.\n\n> Does fixed-size pool scenario mean some finite number of candidate points $\\mathbf{x}\\_i$ are selected in advance and that optimized over these points? If true, is the set of points dynamically updated or fixed in advance?\n\nYes, it is optimized over a fixed number of points. The set of points is not dynamically updated by following the general formulation of pool-based optimization.\n\n> What is the definition of simple regret? Is it $f(\\mathbf{x}\\_n) - f^*$ in the $n$th iteration, where $f^*$ is the minimizer of function $f$. I am wondering why the regret is monotonically decreasing in Fig. 2. Does this plot $\\min\\_{n} f(\\mathbf{x}\\_n) - f^*$?\n\nThe definition of simple regret is $\\min\\_{n} f(\\mathbf{x}\\_n) - f^*$, which is widely used in comparing Bayesian optimization algorithms. We have included this definition; please see **Equation (12)**.\n\n> Are they all two? This is crucial information on the difficulty of black-box optimization problems.\n\nYes, they are two-dimensional problems. We have added the specifics of the synthetic benchmarks; see **Section D.2**.\n\n> How do you define the distance $|\\mathbf{x}\\_i - \\mathbf{x}\\_j|$, especially when categorical values are involved such as activation function and learning rate schedule.\n\nTo handle categorical and discrete variables, we treat them as integer variables by following the previous literature (Garrido-Merchan & Hernandez-Lobato, 2020). After converting them to integer variables, the distance between two integer variables is easily defined. We have updated **Section D.3** accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522276701,
                "cdate": 1700522276701,
                "tmdate": 1700600578928,
                "mdate": 1700600578928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dy7ZkzRiDY",
            "forum": "H3N5JJfqMX",
            "replyto": "H3N5JJfqMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_fMcY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_fMcY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method called DRE-BO-SSL which combines SSL with DRE-based BO.\nThe intention is to improve the exploration-exploitation trade-off as previous DRE-based BO (BORE and LFBO) tends to focus on exploitation due to the over-confident classifiers.\nThe paper explores two types of SSL method (label propagation and label spreading).\nEmpirical results show that the proposed method work better than competitive BO methods on a wide range of tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**originality** The proposed method is novel.\n\n**quality** The proposed method is sound and the empirical results are promising.\n\n**clarity** The technical part of the paper is good.\n\n**significance** The proposed method is a good contribution to DRE-based BO and can be potentially useful to solve the over-confidence problem in DRE in general."
                },
                "weaknesses": {
                    "value": "The presentation could be improved.\nSpecifically, the focus on over-confidence in the beginning is confusing and I only understand the main point until I read section 3.1 where the relation to exploitation is mentioned.\nPerhaps this should be moved towards the front.\n\nSome limitations of the work should be explicitly mentioned/discussed.\nFor example, assumption 4.1. seems to imply that the method is only intended to work on smooth functions.\n\nSome related work is missing; see questions below."
                },
                "questions": {
                    "value": "The over-confident problem of DRE is recently studied by [1,2]. \nCan the author(s) comment on how the construction of auxiliary distribution in these work is related to the sampling distribution of DRE-BO-SSL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809715905,
            "cdate": 1698809715905,
            "tmdate": 1699636372249,
            "mdate": 1699636372249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H5Sl6ahMGM",
                "forum": "H3N5JJfqMX",
                "replyto": "Dy7ZkzRiDY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fMcY (1/n)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your constructive comment to improve our work.\n\n> The presentation could be improved. Specifically, the focus on over-confidence in the beginning is confusing and I only understand the main point until I read section 3.1 where the relation to exploitation is mentioned. Perhaps this should be moved towards the front.\n\nThank you for pointing this out. We moved Section 3.1 towards the front.\n\n> Some limitations of the work should be explicitly mentioned/discussed. For example, assumption 4.1. seems to imply that the method is only intended to work on smooth functions.\n\nWe have described the limitations and societal impacts in **Sections H and I**. Moreover, we think that the limitation on the smoothness of an objective, which is mentioned by the reviewer, is also related to kernel selection. In Bayesian optimization common stationary kernels work well when an objective is smooth. Therefore, Assumption 4.1 does not significantly add the additional limitation of Bayesian optimization.\n\n> The over-confident problem of DRE is recently studied by [1,2]. Can the author(s) comment on how the construction of auxiliary distribution in these work is related to the sampling distribution of DRE-BO-SSL?\n\nWe cannot find [1, 2] in your comment. Please let us know which papers you mentioned. We can add a response to this concern later."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521973631,
                "cdate": 1700521973631,
                "tmdate": 1700521973631,
                "mdate": 1700521973631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2G7MTFKnYT",
                "forum": "H3N5JJfqMX",
                "replyto": "H5Sl6ahMGM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4079/Reviewer_fMcY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4079/Reviewer_fMcY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and revised draft.\n\nHere are the missing references I mentioned regarding the use of auxiliary distributions:\n- [1] Telescoping Density-Ratio Estimation\n- [2] Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression\n\nIn particular, I was interested in seeing some relations between the sampling distribution of DRE-BO-SSL and their use of auxiliary in the sense that with the proposed sampling distribution the DRE problem \"because harder\" therefore avoids over-confidence (and performs better)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685228776,
                "cdate": 1700685228776,
                "tmdate": 1700685228776,
                "mdate": 1700685228776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QR3GZ6eOje",
            "forum": "H3N5JJfqMX",
            "replyto": "H3N5JJfqMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_WaiC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4079/Reviewer_WaiC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an extension to bayesian optimisation (BO) using density rato estimation (DRE) to tackle the problem of overconfidence in the estimators used.  Specifically, the authors suggest using semi-supervised learning (transduction) to increase the accuracy of the model (overcome the difficulties typically caused by the imbalance between dataset sizes above and below the threshold)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well written.  I quite like the underlying idea, and the implementation appears reasonable."
                },
                "weaknesses": {
                    "value": "One doubt I have with this paper perhaps stems from unfamiliarity with semi-supervised algorithms in general.  My understanding of such approaches is that they tend to assume that the unlabelled training points are nevertheless generated from the underlying x distribution.  In most applications of BO, however, this concept is nonsensical: the only x distribution is the points sampled by BO, which are (in some sense) arbitrary (depending on the acquisition function they may cluster around the optimum as time passes, but not necessarily).  Am I missing a key point here?"
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821245821,
            "cdate": 1698821245821,
            "tmdate": 1699636372157,
            "mdate": 1699636372157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JlSEhg2h8X",
                "forum": "H3N5JJfqMX",
                "replyto": "QR3GZ6eOje",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4079/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WaiC (1/n)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your constructive comment to improve our work.\n\n> One doubt I have with this paper perhaps stems from unfamiliarity with semi-supervised algorithms in general. My understanding of such approaches is that they tend to assume that the unlabelled training points are nevertheless generated from the underlying x distribution. In most applications of BO, however, this concept is nonsensical: the only x distribution is the points sampled by BO, which are (in some sense) arbitrary (depending on the acquisition function they may cluster around the optimum as time passes, but not necessarily). Am I missing a key point here?\n\nWe have solved two scenarios, pool-based Bayesian optimization and generic Bayesian optimization (defined on a continuous search space). Notably, the formulation of pool-based Bayesian optimization has access to a fixed-size pool, which can be used as unlabeled points. The availability of the fixed-size pool makes DRE-based Bayesian optimization naturally built on semi-supervised learning. On the other hand, as pointed out by your comment, for the scenario of generic Bayesian optimization, the generation of unlabeled points may be nonsensical. However, our sampling process based on the truncated Gaussian distribution allows us to focus on the proximity of the query points that have already evaluated. This is aligned with the nature of Bayesian optimization, i.e., the consideration of both exploitation and exploration."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521819918,
                "cdate": 1700521819918,
                "tmdate": 1700600689459,
                "mdate": 1700600689459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]