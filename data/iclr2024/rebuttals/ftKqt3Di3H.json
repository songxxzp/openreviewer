[
    {
        "title": "Text-Free Federated Transformers Knowledge Distillation Without GAN"
    },
    {
        "review": {
            "id": "rk3BWzn1HG",
            "forum": "ftKqt3Di3H",
            "replyto": "ftKqt3Di3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_PazT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_PazT"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenges in Federated Learning (FL) for NLP tasks, specifically the complications arising from using GANs and auxiliary data. By leveraging the embedding structure of Transformers, the authors propose a novel method to generate pseudo data inspired by soft prompts. This approach sidesteps the need for GANs, reduces computational overhead, and outperforms auxiliary data methods on the SuperGLUE Benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper has a clear presentation."
                },
                "weaknesses": {
                    "value": "* **Motivation.** The motivation of this paper did not convince me. It seems that the target problem is ambiguous and meaningless. The authors seem to just make a minor modification to replace GAN in FL's knowledge distillation for NLP tasks and it lacks motivations and scenarios. GAN is actually rarely used in NLP and NLP is also less studied in FL before. Not using GAN in NLP is trivial and common, which cannot be the main motivation. An appropriate motivation is the problems raised in actual scenarios and previous works, not the \"a + b\" pattern. Also, the authors think GAN will leak privacy and the proposed method can protect privacy, but the authors didn't provide evidence to support that point.\n* **Novelty.** I think the proposed method is not novel. First, knowledge distillation is not a novel thing in FL. Second, such a design in Transformers is also not novel. \n* **Baselines.** The authors missed some important baselines in the experimental part, which weakens the validity of the proposed method. Specifically, the authors should compare the following methods in the experiments: [1] [2] [3].\n\n----\n\n[1] Zhang L, Shen L, Ding L, et al. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10174-10183.\n\n[2] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning[C]//International conference on machine learning. PMLR, 2021: 12878-12889.\n\n[3] Wang H, Li Y, Xu W, et al. DaFKD: Domain-aware Federated Knowledge Distillation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 20412-20421."
                },
                "questions": {
                    "value": "See the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698231351349,
            "cdate": 1698231351349,
            "tmdate": 1699636417439,
            "mdate": 1699636417439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xEfPxk0Xso",
                "forum": "ftKqt3Di3H",
                "replyto": "rk3BWzn1HG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Motivation Clarification for Federated Learning with Non-GAN-based Text Distillation"
                    },
                    "comment": {
                        "value": "We appreciate your time and expertise in reviewing my manuscript. Your constructive feedback has been instrumental in enhancing the quality of the paper.\n\nWe acknowledge that our discussion regarding GANs and motivation has not been sufficiently elucidated. However, we persist in asserting the scarcity and necessity of research on non-text distillation in Federated Learning  within the domain of NLP. We possess compelling evidence that delineates the points of contribution we make to the field. Kindly afford us another opportunity to articulate our motivation more clearly in this context and present a more precise exposition.\n\n1. Previous Data-free distillation of FL research (e.g., FedGEN, FedFTG) based on GAN often relies on image tasks (e.g., MNIST, CIFAR100) as primary benchmarks, neglecting NLP tasks. Other studies involving NLP tasks, such as FedAUX and FedDF, typically circumvent GANs and opt for auxiliary data distillation. Given the substantial gap in research on text tasks within FL's data-free distillation frameworks, exploring this area is both valuable and meaningful.\n\n2. A practical issue arises with GAN in text generation [1](2023 NIPS). It is widely acknowledged that a notable obstacle in NLP GANs is their inability to generate differentiable outputs, given the discrete nature of language models (words). This lack of differentiability hampers mainstream Federated Learning (FL) Data-Free distillation frameworks, such as FedGEN[6] and FedFTG[7]. These frameworks utilize GANs for target learning functions but are ineffective in transmitting errors back to the generator, rendering them unsuitable for generating synthetic text in NLP distillation tasks under federated learning.\n\n3. Can alternative text generation models be employed aside from GANs? Our response is that the proposed solution entails substantial costs in terms of privacy preservation and computational overhead. \n\nThe current state-of-the-art in text generation, such as large-scale Transformer models like GPT-4, necessitates self-supervised training, involving memorizing client-side text data. However, this introduces privacy concerns, requiring intricate training mechanisms[2] and defense mechanisms[3] to ensure Transformer models remember the text while safeguarding privacy. Additionally, it raises the risk of attackers reconstructing text through pre-trained models[4,10], adding computational and communication overhead.\n\n\n- **Concerning privacy** complex training[2] and defense mechanisms[3] are required to ensure the Transformer remembers the text while protecting privacy. Simultaneously, precautions must be taken to prevent attackers from reconstructing text through pre-trained models[4,10], introducing both computational and communication overhead.\n\n- **Regarding computational overhead** deeper generative models (e.g., BERT, GPT) capable of memorizing more client-side text may exceed the computational capacity of the original task model, imposing a greater computational burden on clients.\n\n - **Concerning communication overhead** the baseline Transformer for the original task already consumes substantial communication, such as FedKD[9] using shallow TinyBERT resulting in 0.17-1.03GB per mobile client. Introducing language generation models like FedGEN would further amplify communication overhead.\n\n\nIn summary, GANs are unsuitable for text generation, rendering mainstream FL data-free distillation frameworks inadequate for text tasks. Instead of modifying the FedGEN framework, we propose a method that originates from the Transformer itself, generating data and distilling it. This approach aims to enhance model efficiency while circumventing the privacy, computational, and communication challenges associated with GANs. Compared to FedAUX and FedDF methods, our approach demonstrates superior performance in enhancing model efficiency."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119367832,
                "cdate": 1700119367832,
                "tmdate": 1700269214611,
                "mdate": 1700269214611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9v6oicppUR",
                "forum": "ftKqt3Di3H",
                "replyto": "rk3BWzn1HG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "[1] Alvarez-Melis, D., Garg, V., & Kalai, A. (2022). Are GANs overkill for NLP?. Advances in Neural Information Processing Systems, 35, 9072-9084.\n\n[2] Ponomareva, N., Bastings, J., & Vassilvitskii, S. (2022, May). Training text-to-text transformers with privacy guarantees. In Findings of the Association for Computational Linguistics: ACL 2022 (pp. 2182-2193).\n\n[3] Shangwei Guo, Chunlong Xie, Jiwei Li, L. Lyu, and Tianwei Zhang. Threats to pre-trained language models: Survey and taxonomy. ArXiv, abs/2202.06862, 2022\n\n[4] Zhang, R., Hidano, S., & Koushanfar, F. (2022). Text revealer: Private text reconstruction via model inversion attacks against transformers. arXiv preprint arXiv:2209.10505.\n\n[5] Song, C., & Raghunathan, A. (2020, October). Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC conference on computer and communications security (pp. 377-390).\n\n[6] Zhang L, Shen L, Ding L, et al. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10174-10183.\n\n[7] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning[C]//International conference on machine learning. PMLR, 2021: 12878-12889.\n\n[8] Wang H, Li Y, Xu W, et al. DaFKD: Domain-aware Federated Knowledge Distillation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 20412-20421.\n\n[9] Wu, C., Wu, F., Lyu, L., Huang, Y., & Xie, X. (2022). Communication-efficient federated learning via knowledge distillation. Nature communications, 13(1), 2032.\n\n[10] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., ... & Raffel, C. (2021). Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) (pp. 2633-2650)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120043272,
                "cdate": 1700120043272,
                "tmdate": 1700120071943,
                "mdate": 1700120071943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gRoj8EK9m8",
                "forum": "ftKqt3Di3H",
                "replyto": "9v6oicppUR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_PazT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_PazT"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the authors' response. I have checked the rebuttal, but it seems it didn't convince me. \n\nSpecifically, there may be no revision of the paper; even if it has, I didn't see the colored texts of change, making it hard to tell whether the authors can reclarify their contributions clearly. Also, no additional baseline experiments are added. The reason why the authors didn't add these baselines also didn't convince me.\n\nI appreciate the efforts they made during the rebuttal but I am sorry I cannot recommend acceptance currently. I wish this would not bother the authors and hope they can have a more solid version in the future."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467634049,
                "cdate": 1700467634049,
                "tmdate": 1700467634049,
                "mdate": 1700467634049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lBImfAbSQH",
            "forum": "ftKqt3Di3H",
            "replyto": "ftKqt3Di3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_hSV1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_hSV1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight approach for knowledge distillation in federated learning (FL), particularly in the context of Transformer models. The authors address the challenges posed by Generative Adversarial Networks (GANs) and auxiliary data in FL by sampling from the embedding structure of Transformers and learning a set of pseudo data for the distillation process. This approach, called FedDRS, draws inspiration from the concept of soft prompts and does not require GANs or auxiliary data. It incurs no communication overhead and yields improved model performance with relatively lower computational costs on the server side.\n\nThe authors propose three methods for sampling from embeddings: random sampling, target sampling, and adversary sampling. They demonstrate that their approach outperforms methods relying on auxiliary data on complex NLP tasks such as the SuperGLUE Benchmark. The paper also presents ablation experiments that elucidate the unique advantages of models equipped with embeddings over those without embeddings, showcasing the efficiency and quality of sampling in embedding-enhanced models.\n\nIn summary, the paper introduces a novel text-free approach for knowledge distillation in federated learning, specifically for Transformer models. The proposed FedDRS method addresses the challenges posed by GANs and auxiliary data and yields improved model performance with lower computational costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### **Originality:**\n\nThe paper presents a novel approach for knowledge distillation in federated learning, particularly focusing on Transformer models. The proposed FedDRS method is unique in its text-free approach, which samples from the embedding structure of Transformers and learns pseudo data for the distillation process. This approach addresses the challenges posed by GANs and auxiliary data in FL, offering a creative combination of existing ideas.\n\n### **Quality:**\n\nThe paper is well-written and provides a clear explanation of the proposed method. The authors demonstrate the effectiveness of FedDRS through experiments on the SuperGLUE benchmark, showing improved performance compared to methods relying on auxiliary data. The paper also includes ablation studies that elucidate the advantages of models equipped with embeddings.\n\n### **Clarity:**\n\nThe paper is well-organized and presents its ideas in a clear and coherent manner. The authors provide a thorough explanation of the proposed method, its components, and the experimental setup. The results are presented in a clear and concise manner, making it easy for readers to understand the contributions of the paper.\n\n### **Significance:**\n\nThe proposed FedDRS method addresses an important problem in federated learning, particularly in the context of Transformer models. By offering a lightweight approach that does not require GANs or auxiliary data, the method has the potential to advance the field of federated learning and improve the performance of Transformer models in FL settings. The paper also contributes to the understanding of the challenges posed by GANs and auxiliary data in FL, providing valuable insights for future research.\n\nOverall, the paper presents a novel and creative approach to knowledge distillation in federated learning, focusing on Transformer models. The proposed FedDRS method demonstrates improved performance compared to existing methods and addresses the challenges posed by GANs and auxiliary data. The paper is well-written clear, and significantly contributes to the field of federated learning."
                },
                "weaknesses": {
                    "value": "1. Privacy concerns (important): The paper does not address the potential privacy concerns arising from sampling from the model. Incorporating privacy-preserving measures, such as differential privacy, could help ensure the privacy of the pseudo-samples and enhance the overall robustness of the proposed method. \n\n2. Limited exploration of sampling methods: The paper focuses on three sampling methods (random, target, and adversary sampling) but does not explore other potential sampling strategies. Investigating alternative sampling techniques could lead to further improvements in the performance of the proposed method. \n\n3. Limited exploration of model architectures: The paper focuses on two Transformer models (RoBERTa and T5) but does not explore other popular Transformer architectures, such as BERT or GPT. Investigating the performance of the proposed method on a broader range of Transformer models could provide more insights into its applicability and effectiveness. \n\n4. The illustration of Figure 1 seems chaotic."
                },
                "questions": {
                    "value": "1. Although the authors mentioned about this weakness in the conclusion, it still requires some interpretation of how likely a generative model could leak private data. Therefore, I suggest authors add text inference attack experiments to show this risk. \n2. In Table 3, I am curious about the performance of Fedavg + random sample + adv. sample. I suspect that the improvement of including a target sample in MixSample is negelactble."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698579615641,
            "cdate": 1698579615641,
            "tmdate": 1699636417366,
            "mdate": 1699636417366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RsdnGRPQjR",
                "forum": "ftKqt3Di3H",
                "replyto": "lBImfAbSQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness Response."
                    },
                    "comment": {
                        "value": "Thank you very much for highlighting the weaknesses. Firstly, we acknowledge that privacy concerns constitute a weakness (and also a regret) in our work. Incorporating DP into Transformer is indeed a worthwhile research direction. We had considered integrating DP-SGD into the training process, but due to the inefficiency of per-sample gradients in the Transformer, we currently lack an ideal tool (such as JAX or Opacus) to address this issue, and thus, we had to abandon this approach.\n\nThe three sampling methods are indeed limited, and in future work, we plan to introduce additional contrastive learning losses to explore the possibility of expanding sampling methods. Our focus is primarily on the two main training methods of Transformers, where RoBERTa represents self-supervised training, and T5 represents autoregressive training\u2014both being mainstream and comprehensive paradigms. We did not consider structural diversity extensively because Transformers have several structural variants (BERT, GPT, T5, BART, etc.). The feasibility of incorporating experiments with other Transformer models will be discussed further.\n\nWe are actively considering how to make the logic diagram clearer, such as reducing unnecessary process lines, and these modifications will be addressed in the next version of the paper. Once again, we appreciate your diligent work and valuable suggestions during the review process; they have positively influenced our research.\n\nThank you once again to the reviewer. Your review has provided valuable guidance for our research. We will carefully consider the weaknesses and suggestions you pointed out and make the necessary revisions in the next version of the paper. Your professional insights have had a positive impact on our study, and we appreciate your efforts."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138445657,
                "cdate": 1700138445657,
                "tmdate": 1700138445657,
                "mdate": 1700138445657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k9tbkjMTZJ",
                "forum": "ftKqt3Di3H",
                "replyto": "lBImfAbSQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions Response"
                    },
                    "comment": {
                        "value": "**Q:Although the authors mentioned about this weakness in the conclusion, it still requires some interpretation of how likely a generative model could leak private data. Therefore, I suggest authors add text inference attack experiments to show this risk.**\n\nA: Thank you for your suggestion. We believe that existing research can demonstrate this risk [1-4], and incorporating these studies, we intend to include the conclusions in the next version of the paper.\n\n\n[1] Ponomareva, N., Bastings, J., & Vassilvitskii, S. (2022, May). Training text-to-text transformers with privacy guarantees. In Findings of the Association for Computational Linguistics: ACL 2022 (pp. 2182-2193).\n\n[2] Shangwei Guo, Chunlong Xie, Jiwei Li, L. Lyu, and Tianwei Zhang. Threats to pre-trained language models: Survey and taxonomy. ArXiv, abs/2202.06862, 2022\n\n[3] Zhang, R., Hidano, S., & Koushanfar, F. (2022). Text revealer: Private text reconstruction via model inversion attacks against transformers. arXiv preprint arXiv:2209.10505.\n\n[4] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., ... & Raffel, C. (2021). Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) (pp. 2633-2650).\n\n\n**Q: In Table 3, I am curious about the performance of Fedavg + random sample + adv. sample. I suspect that the improvement of including a target sample in MixSample is negelactble.**\n\nA: We are pleased to conduct this experiment! Here are the experimental results (averaged over 10 runs):\n\nFedAvg + Random Sample + Target Sample = 36.8232138 (up 1.87)\n\nBased on these experimental results, it is evident that the contribution of the target sample is not negligible. We posit that the majority of the sample points obtained from target sample and adv sample sampling are not entirely redundant.\n\n### Metrics for All Tasks:\n\n- AX-b  : 0.04466032370826787\n- AX-g  : 0.5120786516853932\n- BoolQ  : 0.3782874617737003\n- CB  : 0.034226190476190466\n- COPA  : 0.55\n- MultiRC  : 0.26065406168654304\n- ReCoRD  : 0.3278020452273851\n- RTE  : 0.5252707581227437\n- WiC  : 0.5042319749216301\n- WSC  : 0.36538461538461525\n\n**Scores** : 0.36823213844910097"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140731769,
                "cdate": 1700140731769,
                "tmdate": 1700142012684,
                "mdate": 1700142012684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F0flF7hzjK",
                "forum": "ftKqt3Di3H",
                "replyto": "k9tbkjMTZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_hSV1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_hSV1"
                ],
                "content": {
                    "comment": {
                        "value": "This is a great supplementary experiment, which has addressed my concerns.  For the rest of the weaknesses, the authors did not convince me, as they decided to leave the improvement further."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636922643,
                "cdate": 1700636922643,
                "tmdate": 1700636922643,
                "mdate": 1700636922643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "crB3U8crJE",
            "forum": "ftKqt3Di3H",
            "replyto": "ftKqt3Di3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight approach for knowledge distillation in federated learning without using GANs or auxiliary data. The approach samples from the embedding structure of Transformers and learns a set of pseudo data for the distillation process, resulting in improved model performance with relatively lower computational cost. The paper suggests that this approach can be applied to other large-scale NLP tasks beyond Transformers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The approach does not require GANs or auxiliary data, incurs no communication overhead, and yields improved model performance with relatively lower computational costs on the server side.\n* The experiments conducted in the paper show that the proposed approach yields superior results compared to methods that rely on auxiliary data on complex NLP tasks such as the SuperGLUE Benchmark."
                },
                "weaknesses": {
                    "value": "* The challenge addressed in this paper may not be comprehensive. Although some papers utilize GANs to generate data for model distillation, it's important to note that GANs are not the sole method for data generation. Therefore, the scope of this paper appears to be limited.\n* The assertion that \"The GANs approach faces numerous challenges in recent popular large-scale Transformer-based NLP tasks\" prompts the question: Were the models employed in the experiments considered large-scale?\n* This paper does not specifically address the challenges associated with GAN-based methods for Federated Learning (FL) in its experimental section.\n* Is this method applicable to other NLP tasks aside from text classification?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4428/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk",
                        "ICLR.cc/2024/Conference/Submission4428/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762428464,
            "cdate": 1698762428464,
            "tmdate": 1700663226752,
            "mdate": 1700663226752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9m8mhXh7IY",
                "forum": "ftKqt3Di3H",
                "replyto": "crB3U8crJE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Motivation Clarification for Federated Learning with Non-GAN-based Text Distillation"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and thoughtful review of our manuscript.\n\nWe acknowledge that our discussion regarding GANs and motivation was not sufficiently clear. However, we persist in asserting the scarcity and necessity of research on non-text distillation in FL under NLP, and we present clear evidence supporting our contributions. Please grant us another opportunity to articulate our motivation more clearly and provide a more lucid argument:\n\n1. Previous Data-free distillation of FL research (e.g., FedGEN, FedFTG) based on GAN often relies on image tasks (e.g., MNIST, CIFAR100) as primary benchmarks, neglecting NLP tasks. Other studies involving NLP tasks, such as FedAUX and FedDF, typically circumvent GANs and opt for auxiliary data distillation. Given the substantial gap in research on text tasks within FL's data-free distillation frameworks, exploring this area is both valuable and meaningful.\n\n2. A practical issue arises with GAN in text generation [1](2023 NIPS). It is widely acknowledged that a notable obstacle in NLP GANs is their inability to generate differentiable outputs, given the discrete nature of language models (words). This lack of differentiability hampers mainstream Federated Learning (FL) Data-Free distillation frameworks, such as FedGEN[6] and FedFTG[7]. These frameworks utilize GANs for target learning functions but are ineffective in transmitting errors back to the generator, rendering them unsuitable for generating synthetic text in NLP distillation tasks under federated learning.\n\n3. Can alternative text generation models other than GANs be employed? Our answer is: the solution's complexity is too high. The reasons are as follows:\n\nThe current state-of-the-art text generation models, such as large Transformer models like GPT-4, exhibit the best performance. Using Transformer models for text generation necessitates training in a self-supervised manner, essentially requiring the model to memorize client-side text data.\n\nHowever, memorizing client-side text data raises the following issues:\n\n- Privacy concerns require intricate training mechanisms [2] and defense mechanisms [3] to ensure that the Transformer remembers this text while preserving privacy. Simultaneously, precautions must be taken to prevent attackers from reconstructing the text through pre-trained models [4]. This will increase both computation and communication overhead.\n\n- In terms of computation, the deeper the generative model (such as BERT or GPT-like self-supervised models), the more client-side text data it can remember. However, this may lead to computational overhead beyond the original task model, imposing a greater computational burden on clients.\n\n- In terms of communication overhead, the basemodel Transformer for the original task already consumes a substantial amount of communication, as seen in FedKD [9], where the use of a shallow model like TinyBERT incurs 0.17-1.03GB per mobile client. Incorporating language generation models, as in FedGEN, will further increase communication overhead.\n\nIn summary, due to the escalated privacy, computational, and communication costs associated with incorporating text generation models into the FedGEN framework, we do not intend to address these issues. Instead, we propose an approach that originates from the Transformer itself, offering a method for distilling pseudo-data to Transformer models without the need for GANs. The effectiveness of this method surpasses that of using auxiliary data, as seen in FedAUX and FedDF, where the avoidance of GANs is also a deliberate choice."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120970790,
                "cdate": 1700120970790,
                "tmdate": 1700269259026,
                "mdate": 1700269259026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0iRFZptqdY",
                "forum": "ftKqt3Di3H",
                "replyto": "crB3U8crJE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal : Other weakness"
                    },
                    "comment": {
                        "value": "**\u2022Q: The assertion that \"The GANs approach faces numerous challenges in recent popular large-scale Transformer-based NLP tasks\" prompts the question: Were the models employed in the experiments considered large-scale?**\n\nA:Taking reference from the papers on RoBERTa [9] and T5 [10-11], we utilized base models consistent with the original papers in our experimental section. The parameter sizes of these models are significantly larger, with RoBERTa having 102 million parameters [9], and T5 having 220 million parameters. These values far exceed the parameter sizes commonly used in federated learning, such as ResNet18 (11 million) and MLP, VGG, CNN (fewer parameters). Thus, we affirm that our experiments involve \"large-scale\" models.\n\n**\u2022Q: Is this method applicable to other NLP tasks aside from text classification?**\n\nYes, and not only limited to text classification tasks.\n\nIn our paper, we regrettably did not explicitly provide detailed information about SuperGLUE, and we acknowledge this oversight. Thank you to the reviewer for pointing out this aspect.\n\nSuperGLUE encompasses a diverse set of text tasks beyond mere classification, including question answering, multiple-choice tasks, reading comprehension, and word sense disambiguation tasks [1-13]. It is a composite collection of tasks derived from various datasets, reflecting a combination of multiple tasks. The complexity of these tasks highlights the generality of our algorithm in addressing a wide range of natural language processing (NLP) challenges. For further details, please refer to: https://super.gluebenchmark.com/faq."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121154763,
                "cdate": 1700121154763,
                "tmdate": 1700190714282,
                "mdate": 1700190714282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZmbmheR3mT",
                "forum": "ftKqt3Di3H",
                "replyto": "UhSBYBxhkA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk"
                ],
                "content": {
                    "comment": {
                        "value": "some of my concerns have been addressed by the response of the authors, I would raise my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663164844,
                "cdate": 1700663164844,
                "tmdate": 1700663164844,
                "mdate": 1700663164844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BdLVF9ZicC",
            "forum": "ftKqt3Di3H",
            "replyto": "ftKqt3Di3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_PY1Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4428/Reviewer_PY1Q"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author propose a method to sample the embedding layer of transformer models and use for knowledge distillation in Federated Learning. The paper provides a good motivation to come-up with privacy preserving methods for knowledge distillation and identifies the gaps in GAN based methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides an interesting method to sample the embeddings of the transformer models for knowledge distillation in federated learning and thereby reducing the communication overhead and improving the accuracy."
                },
                "weaknesses": {
                    "value": "The paper lack some important details about the proposed method and hence very difficult to read. In the abstract, it is mentioned, \"This lightweight approach does not require GANs or auxiliary data, incurs no communication overhead, and yields improved model performance with relatively lower computational costs on the server side.\". However, I don't see any discussion of the saving in communication cost later in the paper. Since the difference in accuracy is quite moderate as compared to FedAUX for various values of \\alpha in Dirichlet distribution, we need to see what's the saving in communication cost and trade-off with additional computation cost at server. \n\nFurther, in the Ablation study, it's not clear that what numbers in Table 1 should be compared with the accuracy numbers given in Table 3.\n\nWhy do we see decaying performance difference between FedDRS and other techniques in Table 1 with increasing value of \\alpha?"
                },
                "questions": {
                    "value": "please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699238950731,
            "cdate": 1699238950731,
            "tmdate": 1699636417209,
            "mdate": 1699636417209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NBNqyxJtRj",
                "forum": "ftKqt3Di3H",
                "replyto": "BdLVF9ZicC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion of the saving in communication cost"
                    },
                    "comment": {
                        "value": "Thank you for your suggestions regarding communication overhead. The expression is not entirely clear to us. The absence of additional communication overhead refers to the comparison with the fundamental FedAVG approach, indicating the lack of additional communication costs.\n\nWe will incorporate an analysis of this aspect in the experimental section of the next version of the paper, as outlined below:\n\nFedAUX necessitates the transmission of a substantial auxiliary dataset during the initial communication phase to compute the similarity between local and client datasets. Apart from model transmission, a score information matrix also needs to be communicated to clients in each communication round. This results in more communication overhead compared to FedAVG, encompassing both auxiliary data and similarity scores. Other non-data distillation solutions require the transmission of a generator based on FedAVG, thus adding to the communication overhead. FedAVG only requires the transmission of local client updates at time t during communication, and our algorithm's communication overhead is essentially equivalent to FedAVG (considering only label distribution information, with model costs being negligible)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207444372,
                "cdate": 1700207444372,
                "tmdate": 1700207444372,
                "mdate": 1700207444372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]