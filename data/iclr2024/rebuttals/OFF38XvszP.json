[
    {
        "title": "Slot Structured World Models"
    },
    {
        "review": {
            "id": "lpg68NYxnJ",
            "forum": "OFF38XvszP",
            "replyto": "OFF38XvszP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_ZMVF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_ZMVF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to improve contrastive learning of structured world model with slot-attention mechanisms in its visual encoders. The argument lies in the original feed-forward networks can be challenged by scene objects of similar appearances and a variation of object numbers in inference time. The adopted slot-attention is expected to address this for its object-centric properties. The validation includes predicting GNN dynamics for an interactive spriteworld task, where geometry shapes with different colours can have some simple interactions. The suggested slot structured world model outperforms the baseline when multiple prediction steps are considered. It is also shown to yield more accurate masks to associate scene objects."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation and idea are clear and straightforward to follow.\n2. The writing is clean in general and does not have much readability issue.\n3. The approach might be promising to address more complicated scenarios."
                },
                "weaknesses": {
                    "value": "1. Limited novelty. Both GNN latent dynamics and slot attention for object-centric representation are not new.\n2. Lacking relevant literature review. GNN dynamics learned from image data have been extensively researched in tasks with more physics realism, e.g. see (a) and (b).\n3. The experiment results could be stronger. The original C-SSM paper includes multiple benchmarks including interactions beyond simple geometry shapes such as Atari environments. It would be more convincing to see the comparison on these benchmarks and even more as in (a) given physics simulation data is easily to acquire nowadays.\n\n(a) Li et al, Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids, ICLR 2019\n(b) Shi et al, RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks, RSS 2022"
                },
                "questions": {
                    "value": "1. The core contribution seems more related to the awareness of the objectiveness of entities in the image. Given the paper also demonstrates the usage of a pre-trained slot attention model, what makes it preferred comparing to an obvious pipeline: using some object segmentation foundation models to obtain the mask and then apply the shared encoder and GNN? \n\n2. Is there any implicit assumption about the object appearance, such as always fully observable and with a cosntant appearance? How will it work on first-person-view scenarios where a robot gripper might be partially or fully viewed? Can it work well on more realistic data such as 3D objects whose appearance may vary according to perspective and more complex physical interaction as in (a)?\n\n3. How the number of message propagation steps in GNN transition would scale when a long-distance effect is expected? Will the short-cut edge for rigid bodies in (a) be necessary?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9151/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9151/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9151/Reviewer_ZMVF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698417702276,
            "cdate": 1698417702276,
            "tmdate": 1699637151642,
            "mdate": 1699637151642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "D4gBVf48pV",
            "forum": "OFF38XvszP",
            "replyto": "OFF38XvszP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_wSZY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_wSZY"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a slot structured world model to learn object-centric representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. SSWM learns distinct attention masks for each object.\n2. SSWM outperform C-SWM on Interactive Spriteworld."
                },
                "weaknesses": {
                    "value": "1. Baselines: There are other works including slotformer[1] and slotdiffusion[2] which also combined slot attention and other temporal modules to learn object-centric representations. The authors should compare SSWM with these baseline models.\n\n2. Data: These two related works mentioned early also evaluated their methods on much more complicated data than Interactive Spriteworld. The authors should show the effectiveness of SSWM on more challenging benchmarks.\n\n3. Lack of novelty: Slot attention plus GNN updating seems incremental.\n\n\n[1]. Wu, Z., Dvornik, N., Greff, K., Kipf, T. and Garg, A., 2022. Slotformer: Unsupervised visual dynamics simulation with object-centric models. arXiv preprint arXiv:2210.05861.\n\n[2]. Wu, Z., Hu, J., Lu, W., Gilitschenski, I. and Garg, A., 2023. SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. arXiv preprint arXiv:2305.11281."
                },
                "questions": {
                    "value": "I think this work is not ready for publication."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698462615952,
            "cdate": 1698462615952,
            "tmdate": 1699637151491,
            "mdate": 1699637151491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "VKWOz64eLl",
            "forum": "OFF38XvszP",
            "replyto": "OFF38XvszP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_ZFTs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_ZFTs"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Slot Structured World models (SSWM), a object centric world model to that uses pretrained slot attention to extract the object representations and a GNN based dynamics model to train the world model. The authors show experiments on an Interactive Spriteworld environment where the agent moves among a set of other objects colliding with them. Compared to a previous work, Contrastive Structured world models, SSWM outperforms significantly on Mean reciprocal rank and Hits at rank k metrics which intuitively measure the rollout capability of the world model"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Overall, the paper is well written and is fairly easy to understand in a single read."
                },
                "weaknesses": {
                    "value": "1. **Claims**: I find the core claims of this paper to be exaggerated. \n\n\n  (a) Towards the end of page 1, the authors say:\n\n> This paper therefore proposes a new type of dynamics model that embeds an object-centric encoder\nand a GNN-based world model.\n\nThis is not true. Works like [2] and Interaction networks [3] have the same core idea of using object-centric representations and a GNN based dynamcis model to predict next state of objects. However, these works don't get the object representations in an unsupervised fashion. However, the claim of the above sentence is the proposal of a new dynamics model that integrates GNNs and object centric encoder, which is incorrect.\n\n\n\n(b) While enumerating the core contributions of the paper, the authors say that :\n\n> this paper proposes the first learned dynamics model that can isolate individual objects and reason about their (action-conditional) interactions from raw pixel input and can disambiguate between multiple objects with similar appearance. \n\nTo the best of my knowledge, both these statements are inaccurate as several works have shown to address these two things. \n\nFor example: SlotFormer [4], SILOT [5], STOVE [6], SCALOR [7] and numerous other works in the field of unsupervised object-centric video tracking, can all disambiguate between multiple objects and have a learned dynamics model to separate individual objects and reason about them.\n\nBecause of the following reasons, I feel that the claims of the paper seem exaggerated to me. I am happy to discuss this with the authors actively during the rebuttal phase.\n\n\n2. **Methodology**: I do not see this method as a significant change from what SlotFormer does where they use a Transformer as their dynamics interaction module. The primary difference is the choice of modeling the transition dynamics as opposed to a new framework. So, in order to claim that GNN based modeling is more suitable, a comparison should be shown. In the SlotFormer paper, the authors compare against a GNN-based model DCL where SlotFormer outperforms DCL on CLEVERER based reasoning tasks (VQA). I'm curious if the authors have performed any such experiments.\n\n3. **Ablation Study**: The iterative GNN module is definitely a bottleneck as the message passing needs to be done $K-1$ times in the worst case. An ablation study of how much this matters for the environments considered would be an important experiment for showing the iterative mechanism's importance.\n\n4. **Environment**: There are several already existing benchmarks that can be used such as BBS dataset from [1].\nI'm curious so as to why the authors didn't use the BBS dataset or any other existing benchmarks such as MOVie, CLEVERER for validating their experiments.\n\nC-SWM was introduced in ICLR 2020, and the field of unsupervised (generative) object-centric world modeling has progressed significantly where works typically show their performance on complex datasets and environmets. I do feel that this works lacks concrete evaluation on that end. I would encourage the authors to look into this for the next iteration of the manuscript.\n\n5. **Metrics**: Given that the object centric encoding is obtained via pre-training SlotAttention and a decoder can be used to see the reconstruction of the the predicted latent states adding MSE of the rollouts would be beneficial as well to see how accuracte the reconstruction is.\n\n6. **Experiments on Reasoning tasks**: The core claim of the paper suggests that SSWM are good at reasoning (Contribution 1), however there are no experiments to show this ability of the world model. Results of SSWM on benchmarks such as the Visual Question Answering in the CLEVERER dataset would validate these claims.\n\n\n-----\n## **References**:\n\n[1] Learning Robust Dynamics through Variational Sparse Gating, NeurIPS 2022 (https://github.com/arnavkj1995/BBS)\n\n[2] Compositional Video Prediction, ICCV 2019\n\n[3] Learning Long-term Visual Dynamics with Region Proposal Interaction Networks, ICLR 2021\n\n[4] Slotformer: Unsupervised visual dynamics simulation with object-centric models, ICLR 2023\n\n[5] Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking, AAAI 2020\n\n[6] Structured object-aware physics prediction for video modeling and planning, ICLR 2020\n\n[7] Scalor: Generative world models with scalable object representations, ICLR 2020\n\n----\n\n### **Rationale for current rating**\nAs mentioned above, I don't believe the core contribution of the work is any different from already proposed dynamics models and the environments on which SSWM has been evaluated is insufficient. Based on these two primary concerns, I would like to vote for the rejection of the paper. I will, however, make a final decision after (a) rebuttal by authors and (b) discussion with the other reviewers."
                },
                "questions": {
                    "value": "7. World models are being used extensively in Visual model based RL -- so I am curious so as to if the authors had tried running any RL experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508503375,
            "cdate": 1698508503375,
            "tmdate": 1699637151378,
            "mdate": 1699637151378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "QvDm9Wm84z",
            "forum": "OFF38XvszP",
            "replyto": "OFF38XvszP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_puqz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9151/Reviewer_puqz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to combine slot-attention encoder with Graph Neural Network (as a dynamic model) to model the dynamic of each slot based on a state and an action. The authors also enhance Spriteworld environment with physics to allow GNN to model physical interaction between slots-objects. The proposed model outperforms the baseline (C-SWM) on the Spriteworld benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) SSWM clearly outperforms C-SWM as a baseline on the proposed Spriteworld benchmark.\n2) The qualitative analysis confirms it and shows nice disentanglement of objects in the SSWM model. \n3) Simple design of the SSWM model (slot-attention + GNN)."
                },
                "weaknesses": {
                    "value": "1) I believe a failure in the slot-attention mechanism to effectively disentangle objects is likely to compromise the entire method. Given that slot-attention has shown limited success in parsing objects in real-world image datasets, this can limit robustness and applicability of the SSWM method. So it would be interesting to check how the SSWM performance degrades as object disentanglement produced by the encoder degrades.\n2) The novelty is limited as the paper suggested a simple combination of two ideas. \n3) The authors consider only one baseline. It would be nice to have more baselines, such as a simple autoencoder, and a latent next state predictor utilizing only one slot. Plus, it would be beneficial if authors can consider different slot-encoders as well. \n4) The authors test their method only on Spriteworld environment. It would be beneficial for the paper to include additional environments, for instance, 2D shapes, some Atari games, or even more complex settings like Minecraft. It would be interesting to see failure cases of object disentanglement in these environments along with success cases."
                },
                "questions": {
                    "value": "Will it help in terms of metrics if one propagates the dynamic loss from GNN to encoder?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "--"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868892618,
            "cdate": 1698868892618,
            "tmdate": 1699637151276,
            "mdate": 1699637151276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]