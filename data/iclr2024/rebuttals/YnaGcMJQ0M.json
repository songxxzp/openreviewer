[
    {
        "title": "Detecting Out-of-Distribution Samples via Conditional Distribution Entropy with Optimal Transport"
    },
    {
        "review": {
            "id": "M6P5Rf9OvW",
            "forum": "YnaGcMJQ0M",
            "replyto": "YnaGcMJQ0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_MtSB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_MtSB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new OOD detection method based on the optimal transport distance between the test input and that of the ID training samples. The transport plan is calculated by standard entropic regularized optimal transport between the empirical distributions of training and test sets. Empirically, the method demonstrates superior OOD detection performance on small-scale benchmarks such as CIFAR-10 and CIFAR-100."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The empirical evaluations and ablations on small-scale benchmarks are comprehensive."
                },
                "weaknesses": {
                    "value": "- The organization and writing of the paper need to be improved. Notations are overly complicated and scattered around. Multiple theorems and definitions can be omitted or miss citations.\n  - For example, basic concepts in machine learning such as Definition 3.1 (entropy of two random variables) and Definition 3.4 (condition distribution) can be moved to the Appendix or omitted. \n  - Theorem 3.2 should be cited or put as a footnote or remark, as it is a classic result of the entropic regularized optimal transport problem Eq (3). The dual formulation (Proposition 3.3) should also be cited. It might be better to put the standard formulations and results in the Preliminaries Section instead of the Method Section. \n \n   - The formal definition of the OOD detection problem is missing. Therefore, it could be hard to understand why \"the OOD detection problem can be formulated as a discrete optimal transport problem with entropic regularization\" in P4 (Sec 3.1). Note that the OOD detection problem is a binary classification problem, which is related but **not equal to** solving the optimal transport problem.\n\n  - A transition is needed from Sec 3.2 and Sec 3.3. It is unknown why optimal transport is related to contrastive learning. It seems ill-motivated why \"in this work, we employ supervised contrastive training\" (Sec 3.3, P6). Is the feature quality of models trained with standard cross entropy loss undesirable? \n\n- The methodology is somewhat vague. It remains unclear how OOD scores are calculated. If I understand correctly, $\\nu$ (p3) is the empirical distribution of the validation set, where in practice we only have access to the ID validation set. For OOD inputs, how is the distance calculated?"
                },
                "questions": {
                    "value": "- Can authors explain how OOD scores are calculated? As one only has access to the ID validation set, how is the empirical distribution used for OOD detection? \n\n- Why is the method better than KNN or SSD for hard OOD tasks? (Table 3) In principle, it seems that hard OOD samples are challenging for distance-based approaches due to the proximity of OOD samples and ID samples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823056200,
            "cdate": 1698823056200,
            "tmdate": 1699636098147,
            "mdate": 1699636098147,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tBcp2PEiXO",
                "forum": "YnaGcMJQ0M",
                "replyto": "M6P5Rf9OvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MtSB"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments, which helps us improve the manuscript.\n\n### __Answers to Weaknesses__\n\n__Weakness 1. \"The organization and writing of the paper need to be improved. Notations are...\"__\n\n__A1.1 & A1.2__ We aim to ensure the paper is self-contained and presented clearly. While there is room for improvement, particularly in the points you highlighted, we plan to enhance these aspects in the revised version.\n\n__A1.3__ Please see the Def 2.1(P2) for the definition of OOD detection. \n\nOur definition aligns with your viewpoint that OOD detection is indeed a binary classification problem. In fact, the entropic regularized OT in our paper is used to model the mass transport between test inputs and ID training data, which is a crucial aspect of our method. Therefore, we agree that it is better to rephrase the sentence that you mentioned to as \"we leverage optimal transport with entropic regularization [1] for measuring the discrepancy of empirical distributions between test inputs and ID data.\" in P4 (Sec 3.1). We will modify it in the revised version.\n\n[1] Cuturi, Marco. \"Sinkhorn distances: Lightspeed computation of optimal transport.\" Advances in neural information processing systems 26 (2013).\n\n__A1.4__ We clarify that our approach is model-agnostic, meaning that the testing procedure is applicable to different model architectures and training losses, including cross entropy loss and contrastive training loss (as shown in Table 1). This has been confirmed in the line of research on distance-based OOD detections. Also, the advantage of contrastive training over cross entropy was discussed in previous works (e.g., KNN and SSD). In our implementation, we employ supervised training, for being in line with existing distance-based OOD detection approaches (e.g., KNN and SSD) and for empirically fair comparison. \n\nWe will incorporate the above discussion to make a smooth transition between Sections 3.2 and 3.3.\n\n__Weakness 2. \"The methodology is somewhat vague. It remains...\"__\n\n__A2.__ Please see Q1 for the calculation of scoring function. The $\\nu$ means the empirical distribution of a set of test inputs. \nIt is worth noting that our method only uses ID data to train model and obtain training data features without requiring any OOD validation set. Following with distance-based methods (KNN and SSD), we use the trained model to extract the feature of a test input. Then, the distances between a test input and training samples can be calculated by some distance metrics, such as Euclidean distance and cosine distance.\n\n\n### __Answers to Questions__\n\n__A1.__ We apologize if the calculation of our proposed scoring function has caused confusion.\n\nAs detailed in Section 3.2, we introduce a novel scoring function called conditional distribution entropy (Definition 3.5), derived from the optimal transport plan. In this context, we interpret the mass transported from one test input to training data as a conditional distribution, a concept further explained in our discussion on the part of Uncertainty Modeling. Additionally, to quantify the uncertainty of a test input being an OOD sample, we suggest using the entropy of the conditional distribution as the scoring function.\nFor an intuitive understanding of the uncertainty in the transport plan, we refer to Figure 1.\nFurthermore, we theoretically explore how entropic regularized optimal transport affects the conditional distribution entropy (Proposition 3.6).\n\n__A2.__ Current challenges faced by existing distance-based methods in handling OOD samples that closely resemble ID in difficult OOD tasks arise from their simplistic utilization of distance information, as seen in methods like SSD or KNN. These approaches rely solely on the distance from test samples to either the centroid of training data or some ID samples.\nIn contrast, our approach leverages the empirical distribution of training data and test inputs. By integrating pair-wise distance information with distributional insights through optimal transport, our method develops a scoring function that effectively captures the distinctions between OOD and ID samples. This enables more accurate discrimination of OOD samples that are closely located to ID data."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030085642,
                "cdate": 1700030085642,
                "tmdate": 1700030106642,
                "mdate": 1700030106642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "byTbVVbKNr",
                "forum": "YnaGcMJQ0M",
                "replyto": "tBcp2PEiXO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Reviewer_MtSB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Reviewer_MtSB"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for the responses and clarifications provided. After reviewing the response and considering other reviewers' comments, I find that there are still a few major concerns:\n\n> Missing definition of OOD detection\n\nWhile the general definition of OOD detection is given in Def 2.1, the current description seems too general and insufficient to help readers understand the approach.  It would be great to show the concrete formula for OOD detection **based on the optimal transport distance score** beyond the general statement such as \"the goal of OOD detection is to identify whether an input x is from ID or OOD\".\n\n> Vagueness of the method \n\nCan authors elaborate on \"our method only uses ID data to train model and obtain training data features without requiring any OOD validation set\"?  According to Sec 3.1, $\\nu$ denotes the discrete empirical probability of the test set $\\mathcal{D}_{te}$. At test time, different from prior works that only require access to a single test input, **the whole OOD test datasets are required for the proposed approach to calculate the empirical distribution of the OOD samples**, if I understand correctly. Can authors provide further explanations regarding the requirements for the OOD score calculation? \n\n> Writing and organization\n\nThe organization of the paper currently suffers from redundancy, and the notations used are unclear in places. These issues suggest that a major revision might be necessary to enhance the paper's overall presentation. Also, it appears that the manuscript has not yet been updated to reflect these changes.\n\nGiven these unresolved issues, it's challenging for me to recommend acceptance of the paper in its current state."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702259284,
                "cdate": 1700702259284,
                "tmdate": 1700702259284,
                "mdate": 1700702259284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MHxkHF8ZR9",
            "forum": "YnaGcMJQ0M",
            "replyto": "YnaGcMJQ0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_c9Kw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_c9Kw"
            ],
            "content": {
                "summary": {
                    "value": "The key idea is to employ optimal transport for OOD detection which is especially useful in settings where OOD detection is perform on an entire test set of samples rather than a single test sample. The intuition is that, when estimating Wasserstein distance between a training ID set and a test set, OOD test points will be treated differently from ID test points. While I like the general direction of research, I have problems with details on the approach, especially empirical evaluation and it's similarities to recent works.\n\nPost Rebuttal:\nAs per my opinion, the authors seem to lack understanding of literature, missing/ignoring many important baselines. Evaluation is far from thorough. The arguments presented in rebuttal are in particular erroneous. For instance, all the well known baselines include (Garg et al, 2023) are unsupervised, not utilizing or assuming knowledge of a validation set when training OOD detector. Using a validation set obtained from ID set itself~(Hendrycks et al., 2019), is a standard practice for tuning the OOD detector so as to maximize OOD detection on OOD validation set while maintaining 5% OOD rate for ID set. While this is an optional step in all the present methods including (Garg et al., 2023), there is no reason to not utilize outlier exposure in any method. Ignoring this ground breaking work by (Hendrycks et al., 2019) as a justification for missing important baselines is misleading to the community. So I am updating my score to strong rejection. \n\nDEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE. Hendrycks et al. ICLR 2019."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The general idea of employing information theory for OOD detection, not having to relying upon distributional assumptions, is highly appealing. More such works are required in this direction."
                },
                "weaknesses": {
                    "value": "My concerns are as following. Similar work is accomplished in (Garg et al, 2023) where, instead of Wasserstein distance, KL-divergence is estimated in its dual form between an ID training set and a test set. From that perspective, these two works are highly similar. Especially, in the dual form, expression for Wasserstein distance and optimal transport are similar. While the authors cite the work in the very beginning of the paper in an abstract fashion, the connections between the two works are completely ignored, be it experiments, theory or methodology. Furthermore, as per my understanding, existing methods perform OOD detection for a given test point whereas the proposed method utilizes entire test set. While it is appreciated that, in continual learning settings, a test set is indeed available for better OOD detection rather than restricting, I would have liked to see details on this in the evaluation setup. How do you relate these two different class of methods in same setup? A lot more details are required on this particular aspects. Furthermore, while the framework of optimal transport is appealing despite being a straightforward extension of the previous work (Garg et al, 2023), the other proposed techniques of using contrastive learning, feature extraction, seem unnecessary. As per understanding, as in (Garg et al, 2023), dual function being a neural or kernel similarity like function, would take care of representation learning (feature extraction, contrastive learning), etc. Otherwise, the approach becomes unnecessarily complex, hard to decipher, and looses it appeal to some extent, especially in the era of end to end learning. Last but not least, experimental evaluation should be more thorough in general and it must include experiments on Imagenet as ID dataset as done in the recent works. In particular, the comparison between CIFAR10 vs CIFAR100 as (ID vs OOD) is not intuitive."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Reviewer_c9Kw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699635154971,
            "cdate": 1699635154971,
            "tmdate": 1700516548020,
            "mdate": 1700516548020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9MhqXO1bMb",
                "forum": "YnaGcMJQ0M",
                "replyto": "MHxkHF8ZR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c9Kw -- Part I"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and approval of the general research direction.\n\nLet's try to summarize your comments into five weak points.\n\n__W1.__ Our method is similar to the method that you have mentioned (Garg et al.).\n\n__W2.__ Our techniques look similar to the techniques of the method that you have mentioned (Garg et al.).\n\n__W3.__ Some evaluation setup details are not clear.\n\n__W4.__ Results on large-scale dataset, e.g., ImageNet.\n\n__W5.__ Reasons of using CIFAR10 vs CIFAR100 as (ID vs OOD). \n\n\n\n### __Answer to W1__\n\nIn fact, our work differs significantly from the study that you have mentioned (Garg et al., 2023) because they operate under different problem settings. The work by Garg et al. (2023) assumes the availability of an OOD validation set. However, it is not practical to assume the explicit knowledge of unknowns due to the vulnerability of OOD inputs [1, 2]. \n\nIn contrast, our method is distance-based and does not require any OOD validation set. That is why we compared our method with the latest distance-based approaches, such as KNN [3] and SSD [4], as they share the same problem setting but differ from Garg et al. (2023).\n\nWe all know it is not fair to compare a method with prior knowledge to one without. But we can compel the comparison to happen, by __using Place365 as OOD validation set for Garg's method.__ The result  shows that our method still outperforms Garg's method in all cases but Place365. __Notice that our method is without the aid of OOD validation set.__\n\n|        | SVHN  |       | ISUN  |       | LSUN  |       | Texture |       | Place365 |       |\n|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-----:|:--------:|:-----:|\n| Method | FPR   | AUROC | FPR   | AUROC | FPR   | AUROC | FPR     | AUROC | FPR      | AUROC |\n| KL     | 83.24 | 70.31 | 42.72 | 91.37 | 75.24 | 80.71 | 88.06   | 70.57 | __45.94__    | __90.24__ |\n| Ours   | __12.77__ | __97.64__ | __30.01__ | __94.18__ | __9.55__  | __98.22__ | __38.47__   | __92.25__ | 52.15    | 89.93 |\n\n \n### __Answer to W2__\n\nWe would like to argue the techniques of our method are significantly different from those in Garg's paper.\n\n__1. The Usage of Statistical Distance.__ The work (Garg et al. 2023) directly uses the dual form of KL divergence, a statistical distance, to estimate the chance of a test input being an OOD sample. In contrast, we don't directly use the Wasserstein distance (another statistical distance) to detect an OOD sample. Instead, we formulate the problem as an entropic regularized OT problem and define a novel scoring function over the optimal transport plan to measure the uncertainty of a test input being an OOD sample.\n\n__2. The Role of Dual Form.__ Despite the involvement of the dual form concept in both papers, it serves a distinct role in each.\nIn Garg's work, the dual form of the KL divergence is employed to avoid the estimation of the density function, and the final dual function can be viewed as a function approximator. Differently, our method utilizes the dual problem of entropic regularized OT to efficiently obtain the optimal transport plan. __So, the dual form of our work cannot take care of the representation learning like they did.__\n\n\n### __Answer to W3__\n\n\n__In Same Setup.__ Our work put an emphasis on utilizing empirical distributions with geometric information for OOD detection, which is overlooked by existing distance-based approaches, such as KNN [3] and SSD [4]. \n\nIn our experiments, we have tried to evaluate the effect of the cardinality of a test input batch to the performance of OOD detection, as shown in Figure 2. In this setting, an extreme case is to handle a test input as a batch (i.e., batch cardinality is one).\nHere, we provide more results in the table below. \n| No.TestInput (Batch Cardinality)| Method | FPR   | AUROC |\n|:---------:|:------:|:-----:|:-----:|\n| 1         | SSD    | __51.42__ | __88.65__ |\n|           | Ours   | 56.17 | 85.71 |\n| 16        | SSD    | 51.43 | 88.66 |\n|           | Ours   | __40.89__ | __89.58__ |\n| 256       | SSD    | 51.49 | 88.64 |\n|           | Ours   | __30.06__ | __93.53__ |\n| 4096      | SSD    | 51.81 | 88.59 |\n|           | Ours   | __28.80__ | __93.97__ |\n\nThe above results show: 1) The empirical distribution information of test inputs is beneficial for OOD detection; 2) As the batch cardinality increases, our method outperforms the distance-based method SSD.\n\nWe will incorporate these results in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057140024,
                "cdate": 1700057140024,
                "tmdate": 1700059408485,
                "mdate": 1700059408485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p0yXQAsFW2",
                "forum": "YnaGcMJQ0M",
                "replyto": "MHxkHF8ZR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c9Kw -- Part II"
                    },
                    "comment": {
                        "value": "__W4.__ Results on large-scale dataset, e.g., ImageNet.\n\n### __Answer to W4__\n__Evaluation on large-scale ImageNet.__ We take ViT-B/16 as the backbone and follow the parameter settings of KNN. We use ImageNet-1k as the ID dataset and five datasets (in Table 1) as the OOD datasets. The average performance is reported in the table below.\n\n| Model | ID          | Method | FPR   | AUROC |\n|:-----:|:-----------:|:------:|:-----:|:-----:|\n|       |             | SSD    | 22.23 | 94.80 |\n| ViT   | ImageNet-1k | KNN    | 21.39 | 94.70 |\n|       |             | Ours   | __14.34__ | __96.50__ |\n\nThe results show the superiority of our proposal over large datasets and models. We will incorporate these results in the revised manuscript.\n\n__W5.__ Reasons of using CIFAR10 vs CIFAR100 (ID vs OOD).\n\n### __Answer to W5__\n\nIn our study, CIFAR10 vs. CIFAR100 are employed in three contexts: hard task, unsupervised detection, and ablation studies. Since classes share multiple similar semantics in these two datasets, they were employed as a challenging task to fully evaluate the OOD detection method in previous works, such as KNN [3], SSD [4], and CIDER [5]. \n\nIn line with those studies, we chose it for evaluating our method and for comparison with baselines.\n\n#### __References__\n\n[1] Du, Xuefeng, et al. \"VOS: Learning What You Don't Know by Virtual Outlier Synthesis.\" International Conference on Learning Representations. 2021.\n\n[2] Ming, Yifei, Ying Fan, and Yixuan Li. \"Poem: Out-of-distribution detection with posterior sampling.\" International Conference on Machine Learning. PMLR, 2022.\n\n[3] Sun, Yiyou, et al. \"Out-of-distribution detection with deep nearest neighbors.\" International Conference on Machine Learning. PMLR, 2022.\n\n[4] Sehwag, Vikash, Mung Chiang, and Prateek Mittal. \"SSD: A Unified Framework for Self-Supervised Outlier Detection.\" International Conference on Learning Representations. 2020.\n\n[5] Ming, Yifei, et al. \"How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?.\" The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057281774,
                "cdate": 1700057281774,
                "tmdate": 1700059459551,
                "mdate": 1700059459551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jc567yLYip",
                "forum": "YnaGcMJQ0M",
                "replyto": "MHxkHF8ZR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Post Rebuttal"
                    },
                    "comment": {
                        "value": "###  __More Evaluations__ \nGiven our method's distance-based nature, our comparisons primarily focused on other distance-based methods, including SOTA baselines like KNN and SSD. Notably, the pioneering work by Hendrycks et al. (2019) is not typically regarded as a baseline in current literature discussing distance-based methods, such as KNN and SSD. Consequently, we did not include it in our baseline comparisons.\n\nHowever, we are open to incorporate more comparisons into our paper if deemed necessary.\nTo reproduce OE (Hendery et al. 2019), we use CIFAR100+Tiny ImageNet as ID data, following its settings (See Sec. 4.2.2 & 4.3 in OE).\nWe report the performance comparison with OE (Hendery et al. 2019) as follows. \n\n|        | SVHN  |       | iSUN  |       | LSUN  |       | Texture |       | Place365 |       |\n|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-----:|:--------:|:-----:|\n| Method | FPR   | AUROC | FPR   | AUROC | FPR   | AUROC | FPR     | AUROC | FPR      | AUROC |\n| OE     | 71.55 | 84.84 | 68.46 | 82.68 | 78.36 | 82.27 | 79.57   | 76.93 | 79.56    | 78.80 |\n| Ours   | __5.68__  | __98.96__ | __23.28__ | __95.21__ | __5.70__  | __98.90__ | __29.89__   | __94.16__ | __49.26__    | __88.32__ |\n\n\n\n\n### __Clarity on comments__\nWe clarify that the term \u201cOOD validation set\u201d in our comments refer to auxiliary OOD data,  as presented in the seminal work (Hendery et al. 2019). We feel sorry about the confusion and misunderstanding caused by this terminology.\n\nIt is known that the auxiliary OOD data mainly includes three categories: \n1. Generating OOD data from ID sample\n2. Subset of OOD test data \n3. Another non-ID and non-OOD-test data \n\nHere, we reclaim the method in that paper (Garg et al. 2023) is significantly different from our method, due to **the usage of OOD data during training**.\n\n\nIn the paper (Garg et al. 2023 ), KL dual divergence is computed by training data and the subset of OOD test data, which has been described in P3 as follows: \u201cThe key idea is that estimating the divergence measure in its dual form is naturally informative about the subset of samples in the test set that are OOD.\u201d, also affirmed by their [official implementation](https://github.com/morganstanley/MSML/tree/main/papers/OOD_Detection_via_Dual_Divergence_Estimation). In contrast, **our method does not require any auxiliary OOD data during the training stage**.  We guess that this confusion may have led to your misunderstanding. \n\nIn particular, the paper by Garg et al. (2023) also utilizes the first type of auxiliary OOD data, i.e., generating OOD data from ID samples, to analyze the generalization of the estimator. \n\nMore, your viewpoint: \u201cthere is no reason to not utilize outlier exposure in any method.\u201d, is open to discussion in OpenReview."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560121114,
                "cdate": 1700560121114,
                "tmdate": 1700569792533,
                "mdate": 1700569792533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BCqjs1iUmq",
                "forum": "YnaGcMJQ0M",
                "replyto": "jc567yLYip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Reviewer_c9Kw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Reviewer_c9Kw"
                ],
                "content": {
                    "title": {
                        "value": "Please read the literature carefully and then resubmit with proper evaluation"
                    },
                    "comment": {
                        "value": "(1) (Hendery et al. 2019) is important one to consider not as a baseline but for their idea of auxiliary OOD data. There is not reason for any model not to use auxiliary OOD dataset, as it requires only the knowledge of ID data to generate it. It is a standard practice in the literature of OOD detection. \n(2) There are so many baselines that your paper miss or avoided by flimsy arguments on your settings being different from rest of the baselines when it is not.\n(3) There are standard neural architectures considered in previous works. No reason to completely change your evaluation setup by considering your own choice of architectures.\n(4) There are 51 OOD test sets considered for Imagenet as the ID set. I don't see any reason to avoid those datasets.\n(5) Your understanding of the work by (Garg et al. 23) seems erroneous. Please take sometime to read the paper carefully. They do not use OOD data for training. It is an approach based on estimation. Divergence is estimated between an ID training set and a given test set so that test set is split into test ID vs test OOD samples. \n\nWhile I appreciate the back and forth discussion between authors and reviewers, there is value in respecting reviewers' opinions. If not, why even bother to get it reviewed. \n\nThis is my last comment in regards to this paper. I wish the authors best of luck and sincerely hope that they will put in the required efforts on a thorough experimental validation, and let their brilliant idea mature to be an influential paper in the coming future. To reiterate, I never questioned the proposed idea. I very much appreciate this line of work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617411458,
                "cdate": 1700617411458,
                "tmdate": 1700617411458,
                "mdate": 1700617411458,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hc55nt3itm",
            "forum": "YnaGcMJQ0M",
            "replyto": "YnaGcMJQ0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_uQxg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_uQxg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use discrete optimal transport for OOD detection. The arguments are as follows:\n\nIn short, \n- a test dataset is not OOD if it equal or close in empirical distribution to the training set \n- W_p is a measure of distance between two distributions \n- That distance is the inf over couplings of a distance between samples , where a coupling is a joint over pairs from two distributions.\n- OT algorithms recover this inf coupling which lets one compute a distance\n- Under the coupling, given a test point (a point from one distribution) the higher the entropy of the train points (the other distribution), the \"less paired\" the two distributions are under the couplings\n- This means the two distributions are sufficiently different \n- The conditional entropy is taken to be a scoring function for OOD detection of an instance from a test set."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I really like this paper. While OT has caught on in generative models, it is nice to see it used in distinct but closely related fields like OOD detection. \n\nThe presentation of W_p + OT is clean (crisp + no extra details).\n\nThe use of the entropy of the transport plan is well motivated.\n\nThe numerical experiments are extensive."
                },
                "weaknesses": {
                    "value": "In general the paper is good.\n\n1. (exploring relationship between OT and dim. of encoding in more detail)\n\nDimension is not discussed too too much in this work, but greatly affects usefulness + ability-to-solve OT and approx. OT such as Sinkhorn. Given that I believe the work is fairly complete as far as other aspects of experiments go (datasets, benchmarks etc) I will choose to push on this. Do you have any experimental results varying the encoder/feature-space dimension? It would be great to know how this affects performance, especially for higher dim image datasets.\n\n2. Could discuss the role of the whole test set in making decisions on each test point, in more detail. See QUESTIONS.\n\n\n\n3.  (less a weakness, more a suggestion for helping readers)\n\nIf there is a main thing to improve, I think it would be help the not-familiar-with-OT reader gain a little more intuition for higher entropy of the transport plan (resulting from running Sinkhorn, etc) correlating with more-likely-to-be-OOD.\n\nThere is a worked example with a few datapoints in figure 1, that is good. You could maybe supplement it and help the reader by pointing out some other visual examples like e.g. if P and Q are same and you have very large N for both, you would get closer to the identity mapping being the solution , which has zero variance/entropy, etc...\n\nIn short, I think your typical reader might be quite familiar with OOD but not familiar with OT so you could add just a little more hand-holding in that part of the text since the main intuition for the method comes from understanding why the conditional entropy should be high or low."
                },
                "questions": {
                    "value": "- See Weakness (1) (discussing dimension of encoding)\n\n- Related to the dimension of the encoding is the dimension of the data. Could you clarify all image dataset resolutions? E.g. for LSUN and Imagenet? \n\n-  If I understand correctly, the score for one test point is determined in part by the whole test set's W_p distance to the train set. This is both interesting and raises questions. One analogy is the subset of OOD methods that try to fit a density to the test points to do likelihood ratio tests (the density is evaluated on each test datapoint but is the result of learning on all test points). \n\nIf I understand correctly, though you briefly acknowledge using geometry of the whole test set as a motivation, you eventually do not return to a discussion on this particular aspect of the setup. I think it's worth differentiating between methods that use the whole test set for each test point's decision or not. Could you provide more discussion on this (what to take away from it, and what could be inspired by it in the future)? You could potentially consider introducing some new tasks as well such as making decisions on sets of points. Apologies if you discuss this in more detail and I missed it.\n\n- (minor) Could you consider changing the terminology \"score function\" to \"scoring function\" or something similar? \"Score function\" already has a few meanings so for example it's hard to read \"conditional distribution score function\" and not think of \"nabla_u log p(u|v)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Reviewer_uQxg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1698/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699659767734,
            "cdate": 1699659767734,
            "tmdate": 1699659767734,
            "mdate": 1699659767734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a5BvFt00NK",
                "forum": "YnaGcMJQ0M",
                "replyto": "Hc55nt3itm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uQxg"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and suggestions, which help us to improve the manuscript.\n\n### __Answer to Weakness 1__\nPlease see answer to Question 1.\n\n### __Answer to Weakness 2__\nPlease see answer to Question 3.\n\n### __Answer to Weakness 3__\n\nWe agree that the readability of paper is very important.\nWe will incorporate more visual examples as follows in the revised manuscript (we feel sorry about that we can't provide figure in comments.), to enhance the readability of our paper.\n\n__Showcase.__ When $\\mu$ and $\\nu$ are identical empirical distributions, the transport of a test input is exactly, suggesting the smallest entropy in the respective conditional distribution.\n\n\n\n### __Answer to Question 1__\n\n__Dimension of Encoding.__ We provide the ablation study on the dimension of encoding in the table below. Here, we adopt the ResNet18 as the backbone and the same dataset settings as Table 1.\n\n| Backbone    | Feature Dimension | FPR   | AUROC |\n|:--------:|:---------:|:-----:|:-----:|\n| ResNet18 | 128       | 28.25 | 93.79 |\n|          | 256       | 28.31 | 94.50 |\n|          | 512       | 28.58 | 93.94 |\n|          | 1024      | 29.07 | 93.94 |\n|          | 2048      | 33.28 | 92.92 |\n| ResNet50 | 2048      | __22.76__ | __95.11__ |\n\nWe can find the effect of feature dimension on performance is not monotonic on a single model. For example, on the backbone ResNet18, the performance decreases when the feature dimension exceeds 1024.\nHowever, a higher feature dimension yields better performance for larger models, such as ResNet50.\n\n\n\n### __Answer to Question 2__\nWe clarify all test inputs are with the same input dimension of training samples. Thus, we describe the dimension of ID data used in our empirical study.\n\n| ID Dataset | Backbone |Input Dimension |Feature Dimension|\n|:-------------:|:---------:|:---------:|:---------:|\n| CIFAR10/100      |  ResNet18     |     32\\*32\\*3         |     512        |\n| ImageNet   |   ViT           |     384\\*384\\*3        |    768          |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616450717,
                "cdate": 1700616450717,
                "tmdate": 1700616450717,
                "mdate": 1700616450717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G9nkMDcbDk",
                "forum": "YnaGcMJQ0M",
                "replyto": "Hc55nt3itm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uQxg"
                    },
                    "comment": {
                        "value": "### __Answer to Question 3__\nThanks for your comments. \nAs you have pointed out, there are several works (Serr\u00e0, Joan, et al. 2019; Schirrmeister, Robin, et al. 2020; Ren, Jie, et al. 2019) that employ the likelihood ratio for OOD detection. Notably, (Serr\u00e0, Joan, et al. 2019; Schirrmeister, Robin, et al. 2020;) necessitate training models beyond ID data, requiring additional knowledge, in addition to using multiple samples for making decision on a test point. The work by (Ren, Jie, et al. 2019) doesn't require extra data, but it computes the likelihood ratio by training a background model on perturbed ID data. In contrast, our method doesn't have access to additional data, such as generated data or OOD data, for model training. Despite there being some differences, we agree that it would be an interesting line to consider the likelihood ratio in-depth for OOD detection. \n\nIn the sequel, we have tried to discuss the strategy of taking the whole test set in making decisions, covering comparisons with other distance-based competitors, addressing limitations, and contemplating potential future directions. \n\n__Differentiating with methods not using empirical distribution.__ We discussed the relationship between our method and existing distance-based methods. For example, SSD, it uses Mahalanobis distance as a detection metric. As mentioned in page 13, we remark that the Mahalanobis distance can be viewed as a special case of OT. Another baseline KNN, which is a degenerated version of our method (as described in page 9) when OT is eliminated from our method, using the $k$-th distance to ID sample as a detection metric.\n\n\n__Limitations.__ Nevertheless, relying on the entire test input may pose challenges in terms of efficiency, which is the cost paid for performance enhancement. In the current manuscript, we have investigated how the scale of test inputs impacts performance. We split the whole test set into multiple batches with varied batch sizes. As presented in Figure 2, our method benefits from an increase in the size of the test batch and shows a gradual rise in performance until convergence.\n\nThus, it is of interest to further study the balance between the batch size and the performance variance. It would also give the promise for adapting the techniques to different applications with various performance requirements and affordable computational resources. Also, when test data arrives in batches, another interesting question arises: Can we leverage the information from previously detected batches to enhance OOD detection in subsequent batches?\n\n\n__Future works.__ Shifting our attention to training samples, a mirroring problem arises: how to effectively utilize training samples, which remains underexplored in existing distance-based methods that use all training samples. \n\nOne possible solution is to find effective representations in the feature space. Following this line of thought, in our study, one way that we can try to explore these effective representations in the probability space, such as the barycenters of empirical probability distribution.\n\nAnother possible solution is to generate virtual samples as effective representations to substitute for all training samples. Certainly, the application of generation is not limited to this. It is known that a kind of research on OOD detection involves accessing known OOD samples during the training stage (OOD exposure). However, assuming the explicit knowledge of unknowns is not practical due to the vulnerability of OOD inputs. This leads to a potential problem: Can we randomly generate virtual OOD samples for OOD exposure?\n\nThese discussions can be integrated into a revised manuscript.\n\n\n####  __References__\n\nSerr\u00e0, Joan, et al. \"Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models.\" International Conference on Learning Representations. 2019.\n\nSchirrmeister, Robin, et al. \"Understanding anomaly detection with deep invertible networks through hierarchies of distributions and features.\" Advances in Neural Information Processing Systems 33 (2020): 21038-21049.\n\nRen, Jie, et al. \"Likelihood ratios for out-of-distribution detection.\" Advances in neural information processing systems 32 (2019)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617214129,
                "cdate": 1700617214129,
                "tmdate": 1700617214129,
                "mdate": 1700617214129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7C2IeTGL5j",
            "forum": "YnaGcMJQ0M",
            "replyto": "YnaGcMJQ0M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_kQwJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1698/Reviewer_kQwJ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to perform OOD detection using _conditional distribution entropy_ score, which is the average entropy of the conditional distribution of training data given the test example (Eq 4) under the optimal transport probability (Prop 3.3) which is computed based on the pairwise cosine similarity matrix based on the learned feature representation of a DNN extractor. Furthermore, author proposes to learn the feature representation using supervised contrastive learning (SupCon, Section 3.3). Authors conducted experiments on simple academic benchmarks (CIFAR-100 vs SVHN/iSUN/LSUN/Textures/Places365) using basic architecture (ResNet18), and compared to previous methods with and without contrastive learning, illustrating strong performance. Authors also conducted detailed ablation study on the effect of temperature, contrastive learning, training recipe, and choice of metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A novel approach to OOD detection via conditional entropy under optimal transport distribution.\n2. The technical approach is clearly described with sufficient background information and detailed derivation.\n3. The experiment section is done with extensive baselines and academic benchmarks."
                },
                "weaknesses": {
                    "value": "1. The experiments are mostly based on basic architecture and rather stylized academic benchmarks. Experiments on larger models (ResNet-50 / ViT) and more realistic benchmarks (e.g., ImageNet-O) can make the result considerably more convincing.\n2. It would be great to see an ablation study of how OOD performance is impacted by the choice of \"distance matrix\" (C vs P)  and the choice of metrics (probability, entropy, conditional probability, conditional entropy). This may also be related to my question about Table 4 below.\n3. In author's description of Section 3.2, there is a slight notation disconnect for how $\\pi(u|v)$ / $\\pi(u, v)$ / $\\pi(v)$ are related to $P^*$ described in previous section. It would be great to spell that out explicitly in terms of how these quantities can be obtained from the matrix (e.g., in the paragraph \"Uncertainty Modeling\" )."
                },
                "questions": {
                    "value": "Table 4: How are the result from \"w/o OT\" rows obtained? Specifically, what metric is used for OOD detection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1698/Reviewer_kQwJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1698/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699943116834,
            "cdate": 1699943116834,
            "tmdate": 1699943116834,
            "mdate": 1699943116834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mhR2qFISLt",
                "forum": "YnaGcMJQ0M",
                "replyto": "7C2IeTGL5j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kQwJ -- Part I"
                    },
                    "comment": {
                        "value": "We are encouraged that reviewer finds our work novel, effective, and with extensive experiments.\nThanks for your valuable comments and suggestions, which we address below:\n\n### __Answer to weakness 1__\nThanks for your suggestions. We have added more experimental results for larger models and datasets as suggested.\n\n* __Evaluation on ResNet50.__ Following the experimental settings of SSD, we alter the backbone to ResNet50 and use the CIFAR100 as ID dataset. The average performance over five datasets (in Table 1) is presented in the table below.\n\n|        | SVHN  |       | iSUN  |       | LSUN  |       | Texture |       | Place365 |       |\n|--------|-------|-------|-------|-------|-------|-------|---------|-------|----------|-------|\n| Method | FPR   | AUROC | FPR   | AUROC | FPR   | AUROC | FPR     | AUROC | FPR      | AUROC |\n| SSD+   | 9.63  | 98.14 | 96.48 | 76.89 | 81.00 | 83.49 | 65.79   | 86.71 | 79.04    | 79.03 |\n| KNN+   | 21.49 | 96.00 | 75.67 | 83.07 | 44.53 | 92.32 | 50.37   | 90.13 | 77.57    | 79.40 |\n| Ours   | __5.68__  | __98.96__ | __23.28__ | __95.21__ | __5.70__  | __98.90__ | __29.89__   | __94.16__ | __49.26__    | __88.32__ |\n\n\n* __Evaluation on ImageNet with ViT.__ Following the parameter settings of KNN, we take ViT-B/16 as the backbone. We use ImageNet-1k as the ID dataset and five datasets (in Table 1) as the OOD datasets. The average performance is presented in the table below.\n\n| Model | ID          | Method | FPR   | AUROC |\n|:-----:|:-----------:|:------:|:-----:|:-----:|\n|       |             | SSD    | 22.23 | 94.80 |\n| ViT   | ImageNet-1k | KNN    | 21.39 | 94.70 |\n|       |             | Ours   | __14.34__ | __96.50__ |\n\n\nThe above results show the superiority of our method under the settings of larger models and datasets, which is consistent with the result shown in the manuscript. We will incorporate these results in the revised version.\n\n\n### __Answer to weakness 2__\n\nWe agree that doing more ablation studies is good for analyzing and improving the performance, interpretability, and robustness of the work. In the sequel, we provide the results of ablation studies as suggested.\n\n* __Ablation on Distance.__ We enhance our ablation studies by examining the impact of different distance metrics on out-of-distribution (OOD) detection performance. As shown in the table below, it is evident that Euclidean distance and Cosine distance produce comparable results, given their transformability into each other. In general, Euclidean distance served as the cost metric for optimal transport (OT) due, in part, to its well-analyzed theoretical properties. In this paper, we adopt Cosine distance to align with normalized feature training.\n\n| Training Loss | Distance  | FPR   | AUROC |\n|---------------|-----------|-------|-------|\n| SimCLR        | Euclidean | 22.45 | 94.59 |\n|               | Cosine    | 24.86 | 94.43 |\n| SupCon        | Euclidean | 28.40 | 93.48 |\n|               | Cosine    | 28.58 | 94.04 |\n\n* __Ablation on Detection Metric.__  We improve our ablation studies by examining the effect of detection metrics, specifically the scoring function, on out-of-distribution (OOD) detection performance. The 'Max' metric represents the maximum value of the transport plan for each test input. The 'Probability Distance' metric refers to the inner product over the transport plan and distance. The 'Mean' metric indicates the average value of distances between a test input and training samples. The empirical results highlight the efficacy of our uncertainty modeling and underscore the superiority of our proposed conditional distribution entropy scoring function.\n\n| Training Loss | Metric | FPR   | AUROC |\n|---------------|----------------|-------|-------|\n| SimCLR        | Max            | 33.23 | 91.58 |\n|               | Probability Distance       | 29.33 | 93.27 |\n|               | Mean   | 99.28 | 37.97 |\n|               | Conditional Distribution Entropy        | __24.86__ | __94.43__ |\n| SupCon        | Max            | 40.24 | 90.32 |\n|               | Probability Distance       | 34.60 | 92.73 |\n|               | Mean   | 91.60 | 66.17 |\n|               | Conditional Distribution Entropy        | __28.58__ | __94.04__ |\n\nThese ablation studies are conducted on the same settings as in Table 1. The reported result is the average performance over five OOD datasets.\nWe will incorporate the above results in the revised manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305262551,
                "cdate": 1700305262551,
                "tmdate": 1700308204910,
                "mdate": 1700308204910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nl01wdzTxz",
                "forum": "YnaGcMJQ0M",
                "replyto": "7C2IeTGL5j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1698/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kQwJ -- Part II"
                    },
                    "comment": {
                        "value": "### __Answer to Weakness 3__\nThanks. We acknowledge the slight notation discrepancy you pointed out, and we will address it in the revised version as follows:\n\n__Modified Uncertainty Modeling.__ \nIn Definition 3.1, it shows that the transport plan $\\mathbf{P}$ can be viewed as a joint probability distribution $\\pi(\\mu,\\nu)$, with marginals $\\mu$ and $\\nu$.\nGiven a transport plan in the form of a matrix, we use two random variables, $U$ and $V$, to describe the randomness of the mass distributed in rows and columns, respectively.\nFor a test input $v \\in dom(V)$, there is a corresponding column in the transport plan, which is essentially a conditional probability distribution $\\pi_{U|V}(u|v)$ computed by joint probability $\\pi(u,v)$ and marginal probability $\\pi(v)$, as defined in Definition 3.4.\nAccordingly, the entropy of the conditional probability distribution indicates the level of uncertainty regarding a test input belonging to the OOD. To this end, formally, we define the transport that happens on the test input $v$ as a random event, represented by a random variable $T$ that follows the conditional distribution, i.e., $T\\sim \\pi_{U|V}(u|v)$.\nThen, the score function is denoted as the entropy of\nconditional distribution $\\mathbb{H}(U|V=v)$, as defined in Definition 3.5.\n\n\n### __Answer to Questions__\nIncorporating optimal transport (OT) into our approach allows us to go beyond traditional distance-based methods that rely solely on the distance information of training samples and test inputs. Our method harnesses OT to capture empirical distribution information in the feature space, enhancing out-of-distribution (OOD) detection.\n\nTo assess the impact of optimal transport (OT) on OOD detection performance, we conducted an ablation study. The results are presented in Table 4, where the row labeled (w/o OT) represents calculations based solely on the distance between a test input and training samples. The distance metric employed in this case is the mean Euclidean distance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305358816,
                "cdate": 1700305358816,
                "tmdate": 1700305358816,
                "mdate": 1700305358816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]