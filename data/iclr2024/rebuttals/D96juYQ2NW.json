[
    {
        "title": "Coresets for Clustering with Noisy Data"
    },
    {
        "review": {
            "id": "araDEMKcxE",
            "forum": "D96juYQ2NW",
            "replyto": "D96juYQ2NW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_a2UA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_a2UA"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of coreset for $k$-means clustering in the noisy setting.\nGiven a dataset $P \\subset\\mathbb{R}^d$ and a set $C$ of $k$ centers, one can define the cost to be $\\sum_{x\\in P} \\min_{c\\in C}\\| x-c \\|^2$.\nThe $k$-means clustering problem is to find the set $C$ that minimize the cost given a dataset $P$.\nHowever, when the size of $P$ is huge, one common technique to improve the storage and computation is to extract a smaller subset $S$ of $P$ and find the set $C$ such that the $k$-means cost is minimized for $S$.\nDifferent previous results showed that we can construct such a subset $S$ such that the size of $S$ is small and the difference between the $k$-means cost for $P$ and $S$ is also small.\nIn reality, the data is often noisy.\nTherefore, we would like to construct a coreset from the noisy dataset $\\widehat{P}$.\nThe authors showed that the standard notion of coreset for $k$-means is too strong for noisy dataset and defined a new notion for the noisy setting.\nThe authors then showed that if $S$ is a good coreset for $\\widehat{P}$ under some mild assumptions on the noise then $S$ is also a good coreset for $P$.\nThe main idea is to first show that $\\widehat{P}$ itself is a good coreset of $P$ even though the size is the same.\nThen, using the composition property, the authors proved the main theorem.\nAlso, the authors provided some experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem is well-motivated.\nMost of the previous results did not consider the noisy version of the problem while the real world data is often noisy.\nHence, I believe it is a good starting point to investigate this line of work.\n\n- The paper is well-written.\nReaders of all levels of expertise should be able to understand this paper."
                },
                "weaknesses": {
                    "value": "- Most techniques are straightforward calculations.\nI am not sure if there are any fundamentally new techniques introduced in this paper."
                },
                "questions": {
                    "value": "na"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Reviewer_a2UA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698609791670,
            "cdate": 1698609791670,
            "tmdate": 1699636621439,
            "mdate": 1699636621439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q0sKxSjULq",
                "forum": "D96juYQ2NW",
                "replyto": "araDEMKcxE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a2UA"
                    },
                    "comment": {
                        "value": "Thanks for appreciating the motivation of our problem and the writing. As you noted, one of the main contributions of our paper is to initiate the study of coresets in the noisy setting. \n\nWe think that establishing the separation between the two error measures $\\mathrm{Err}$ and $\\mathrm{Err}_{AR}$, both theoretically and empirically, is also interesting.\n\nMoreover, in formulating responses to other reviewers we have presented additional features/generalizations of our results that may be of interest.\n\n* In responses to Reviewers gA35 and QbwN, we provide an example to illustrate that, without assuming well-separated clusters, the two error measures $\\mathrm{Err}$ and $\\mathrm{Err}_{AR}$ may be the same; showing the necessity of the assumption of well-separated clusters.\n* Addressing the concern of Reviewer QbwN about the assumption of independent noise across dimensions, we illustrate how our techniques can be extended to handle non-independent noise across attributes of data points. \n* Based on the suggestion of Reviewer QbwN, we evaluate our results on the Census1990 dataset whose dimension is 68. We use the same setup as in Section 4 and the new results can be found in the following file in the supplementary material (https://openreview.net/attachment?id=D96juYQ2NW&name=supplementary_material). These empirical findings validate our theoretical results on datasets with a large number of attributes."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500545508,
                "cdate": 1700500545508,
                "tmdate": 1700500545508,
                "mdate": 1700500545508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7QqSj8TQud",
                "forum": "D96juYQ2NW",
                "replyto": "Q0sKxSjULq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Reviewer_a2UA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Reviewer_a2UA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I will take it into consideration during the AC discussion."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680095453,
                "cdate": 1700680095453,
                "tmdate": 1700680095453,
                "mdate": 1700680095453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IxqqE0ejw9",
            "forum": "D96juYQ2NW",
            "replyto": "D96juYQ2NW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_QbwN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_QbwN"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates the performance of the optimal solution $\\hat{C}$ in the original data $P$ when dealing with noisy data $\\hat{P}$. The analysis is based on the coreset technique, but the traditional measure for determining coreset quality is too strong when the data is noisy. To address this issue, the authors propose the AR-coreset, which restricts itself to a local sublevel-region and considers the cost ratio. This new measure allows the authors to obtain refined estimates of $\\hat{P}$. The authors demonstrate that $\\hat{P}$ is a (1+nd)-coreset and a (1+kd)-AR-coreset of $P$, meaning that $\\hat{C}$ is a (1+nd)-approximation and a (1+kd)-approximation solution, respectively."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1, The motivation is clear. It is instructive to consider the local sub-optimal region of the solution space in the AR-coreset.\n\n2, Utilizing the coreset technique to analyze the approximation of the solution is an interesting approach.\n\n3, The lower bound of the approximation is also discussed."
                },
                "weaknesses": {
                    "value": "1. The failure of the 'Err()' seems natural since it considers the worst case in the entire solution space.\n2. This paper does not provide a detailed comparison of the proposed method with other existing analysis methods for clustering with noisy data.\n3. This paper considers independent noise across dimensions, the real noise might be correlated. The experiment part is too simple, which considers only two datasets; also the dimensions are low (6 and 10).\n4. The noise model and the assumptions make the result to be relatively narrow."
                },
                "questions": {
                    "value": "1, the authors say \u2018Intuitively, how good a center set we can obtain from $\\hat{P}$ is affected by the number of sign changes. \u2019. Please provide more thorough interpretations.\n\n2, Besides determining the quality of a coreset in the presence of noise, are there other potential applications of the proposed AR-coreset?\n\n3, Can we conclude that the (near) optimal solution(s) are robust to noise for other problems?\n\n4, In the definition of AR-coreset, why should we consider the ratio of the cost for a given center set to the minimum cost? What would happen if we only consider the denominator of $r_P(\\widehat{c})=\\frac{\\operatorname{cost}(P, \\widehat{c})}{\\mathrm{OPT}}$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Reviewer_QbwN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734378875,
            "cdate": 1698734378875,
            "tmdate": 1699636621333,
            "mdate": 1699636621333,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rav2Et5YjB",
                "forum": "D96juYQ2NW",
                "replyto": "IxqqE0ejw9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QbwN"
                    },
                    "comment": {
                        "value": "Thanks for appreciating our motivations and results for the lower bound. We address your concerns and questions below.\n\n>1. The failure of the 'Err()' seems natural since it considers the worst case in the entire solution space.\n\nThanks for raising this point. Even in the noise-free setting, $\\mathrm{Err}(\\cdot)$ considers the worst case in the entire solution space but, as the example below shows, it may not fail and capture the quality of the optimal center set of a coreset. The non-trivial contribution is in identifying a measure $\\mathrm{Err}\\_{AR}(\\cdot)$ and presenting quantitative separation between two error measures $\\mathrm{Err}(\\cdot)$ and $\\mathrm{Err}\\_{AR}(\\cdot)$ in the presence of noise. \n \nConsider the worst case instance constructed in [Cohen-Addad et al., 2022] where $P=\\\\{ e_1, e_2,\\ldots, e_d \\\\} $ consists of unit basis vectors in $\\mathbb{R}^{d}$ where $d=2k \\varepsilon^{-2}$. Let $S\\subset P$ be a (weighted) subset of size at most $k \\varepsilon^{-2}$. The optimal center set of $S$ must lie on the spanned subspace of $S$, and hence can not be a $(1+\\varepsilon)$-approximate center set of $P$. This geometric observation implies that $S$ is neither an $\\varepsilon$-coreset nor a $(0,\\varepsilon)$-approximation coreset. Then the tight sizes of both an $\\varepsilon$-coreset and an $(0,\\varepsilon)$-approximation coreset of such $P$ are $\\Theta(k \\varepsilon^{-2})$, which implies that $\\mathrm{Err}(\\cdot)$ and $\\mathrm{Err}\\_{AR}(\\cdot)$ are the same on this dataset $P$. \n\nWe will add this example in the final version.\n\n>2. This paper does not provide a detailed comparison of the proposed method with other existing analysis methods for clustering with noisy data.\n\nWe have included a discussion of the literature on clustering with noisy data in Appendix A, along with a comparison of their analysis methods with our work. If you have any preferences regarding additional content or the placement of this section in the final version, please feel free to let us know -- we would be happy to do so.\n\n>3. This paper considers independent noise across dimensions, the real noise might be correlated. \n\nThanks. We acknowledge that real noise may exhibit correlations across dimensions. As mentioned in the introduction, the attributes of the data may demonstrate weak correlations or interactions. For this type of real data, it is common to rely on the assumption of independent noise across dimensions, as observed in studies by [Zhu & Wu, 2004; Freitas, 2001; Langley et al., 1992].\n\nA positive aspect of our techniques is that they can be extended to handle non-independent noise across dimensions. Consider a scenario where the covariance matrix of each noise vector $\\xi_p$ is $\\Sigma\\in \\mathbb{R}^{d\\times d}$ (with $\\Sigma = \\theta \\cdot I_d$ when each $D_j = N(0,\\theta)$ under the noise model II). Our proof of Theorem 3.1 relies on certain concentration properties of the terms $\\sum\\_{p\\in P} \\|\\xi_P\\|_2^2$ and $\\sum\\_{p\\in P}\\langle \\xi_p, p-c\\rangle$. Note that $\\mathbb{E} \\|\\xi_p\\|_2^2 = \\mathrm{trace}(\\Sigma)$. Hence, by a similar argument as in the proof of Claim 3.6, one can show that $\\sum\\_{p\\in P} \\|\\xi_P\\|_2^2$ concentrates on $n \\cdot \\mathrm{trace}(\\Sigma)$ and $\\sum\\_{p\\in P}\\langle \\xi_p, p-c\\rangle \\leq \\|c-c^\\star\\|_2\\cdot O(\\sqrt{n\\cdot \\mathrm{trace}(\\Sigma)})$. Consequently, an $\\varepsilon$-coreset $S$ of $\\widehat{P}$ is an $O(\\varepsilon + \\frac{n \\cdot \\mathrm{trace}(\\Sigma)}{\\mathrm{OPT}} + \\sqrt{\\frac{n \\cdot \\mathrm{trace}(\\Sigma)}{\\mathrm{OPT}}})$-coreset of $P$ and a $(0, O(\\varepsilon + \\frac{k \\cdot \\mathrm{trace}(\\Sigma)}{\\mathrm{OPT}}))$-approximation-ratio coreset of $P$. The only difference with Theorem 3.1 is that we replace the original variance term $\\theta d$ to $\\mathrm{trace}(\\Sigma)$.\n\nWe will add a remark on how our techniques can be extended to the non-independent noise case in the final version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499990340,
                "cdate": 1700499990340,
                "tmdate": 1700500846803,
                "mdate": 1700500846803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ob57Lw6cRe",
                "forum": "D96juYQ2NW",
                "replyto": "IxqqE0ejw9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": ">4. The experiment part is too simple, which considers only two datasets; also the dimensions are low (6 and 10).\n\nBased on your comment, we evaluated our results on the Census1990 dataset whose dimension is 68. We use the same setup as in Section 4 and the new results can be found in the following file in the supplementary material (https://openreview.net/attachment?id=D96juYQ2NW&name=supplementary_material). These empirical findings validate our theoretical results on datasets with large number of attributes, including 1) a numerical separation between $\\mathrm{Err}$ and $\\mathrm{Err}\\_{AR}$; 2) both $\\mathrm{Err}$ and $\\mathrm{Err}\\_{AR}$ initially decrease and then stabilize as the coreset size increases; 3) variance of noise (or noise level) is the main parameter that determines both $\\mathrm{Err}$ and $\\mathrm{Err}\\_{AR}$. Specifically, under the noise model II with Gaussian noise, $\\mathrm{Err}(S)$ decreases from 0.282 to 0.270 as $|S|$ increases from 500 to 2000, and then remains around 0.270 as $|S|$ continues to increase to 5000. We will incorporate these results in the final version.\n\n>5. The noise model and the assumptions make the result to be relatively narrow.\n\nWe have addressed your concern about the noise model in our response 3 above.  \n\nRegarding the assumptions, in addition to the discussion in Section 3 explaining why both balancedness and well-separated clusters are reasonable and standard assumptions in the clustering literature, we provide an example below to illustrate that, without assuming well-separated clusters, the two error measures $\\mathrm{Err}$ and $\\mathrm{Err}\\_{AR}$ may be the same.\n\nConsider a 3-Means problem over $\\mathbb{R}$, i.e., $k=3$ and $d=1$. Let $P\\subset \\mathbb{R}$ consist of $\\frac{n}{4}$ points each located at -1.01, -0.99, 0.99 and 1.01. A simple calculation shows that the optimal center set $C^\\star$ is either $\\\\{-1.01, -0.99, 1\\\\}$ or $\\\\{-1, 0.99, 1.01\\\\}$, and $\\mathrm{OPT} = 0.0005n$ (same in both cases). \n\nFirst note that, because there exist two clusters of points that are close to each other in both optimal center sets, $P$ does not satisfy the well-separateness assumption. (The distance between the nearby clusters is 0.02 while our assumption requires that it is at least $\\sqrt{3}$.) \n\nNext, let $\\widehat{P} \\subset \\mathbb{R}$ be an observed dataset drawn from $P$ under the noise model II with each $D_j = N(0,1)$. Since $\\mathbb{E}\\_{x\\sim N(0,1)}[|x|\\mid x<0] = \\mathbb{E}\\_{x\\sim N(0,1)}[|x|\\mid x\\geq 0] = 1$, the optimal partition of $\\widehat{P}$ is likely to be $\\\\{\\widehat{p}: \\widehat{p}\\leq -1 \\\\}$, $\\\\{\\widehat{p}: -1\\leq \\widehat{p}\\leq 1 \\\\}$, $\\\\{\\widehat{p}: \\widehat{p}\\geq 1\\\\}$ and the mean points of these three partitions are roughly -2, 0, 2 respectively. In this case, the optimal center set of $\\widehat{P}$ is likely to be $\\widehat{C} = \\\\{-2,0,2\\\\}$. Note that $\\mathrm{cost}_2(P, \\widehat{C}) \\approx n \\gg \\mathrm{OPT}$. By Theorem E.1, we also have $\\widehat{P}$ is a $\\frac{n}{\\mathrm{OPT}}$-coreset of $P$. Thus, $\\mathrm{Err}(\\widehat{P})\\approx \\mathrm{Err}\\_{AR}(\\widehat{P})\\approx \\frac{n}{\\mathrm{OPT}}$. \n\nWe will add this in the final version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500380289,
                "cdate": 1700500380289,
                "tmdate": 1700500886465,
                "mdate": 1700500886465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lPbumQgBhk",
                "forum": "D96juYQ2NW",
                "replyto": "IxqqE0ejw9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": ">6. the authors say \u2018Intuitively, how good a center set we can obtain from $\\hat P$ is affected by the number of sign changes.\u2019. Please provide more thorough interpretations.\n\nThanks for your question. Below we explain how the quality of the optimal center set of $\\hat P$ may be affected by the number of sign changes.\n\nConsider the case of 1-Means. Let $P \\subset \\mathbb{R}^d$ be an underlying dataset. Let $\\widehat{P}_1$ and $\\widehat{P}_2$ be observed datasets drawn from $P$ under the noise model II with noise parameters $\\theta_1$ and $\\theta_2$ respectively. We assume that $\\theta_1 < \\theta_2$. Recall that the optimal centers of $P$, $\\widehat{P}_1$, and $\\widehat{P}_2$ are denoted by $\\mu(P)$, $\\mu(\\widehat{P}_1)$, and $\\mu(\\widehat{P}_2)$ respectively. We note that for any two centers $c$ and $c'$ lying on the line segment $\\mu(P)-\\mu(\\widehat{P}_1)$, the signs of $\\mathrm{cost}_2(P,c) - \\mathrm{cost}_2(P,c')$ and $\\mathrm{cost}_2(\\widehat{P}_1,c) - \\mathrm{cost}_2(\\widehat{P}_1,c')$ must be different. This is besause the center that is closer to $\\mu(P)$ has a smaller cost for $P$ but a larger cost for $\\widehat{P}_1$ when compared to the other center. Hence, the number of sign changes from $P$ to $\\widehat{P}_1$ is proportional to the distance $d(\\mu(P), \\mu(\\widehat{P}_1))$. The same observation holds for $\\widehat{P}_2$. Also, note that $d(\\mu(P), \\mu(\\widehat{P}_1)) < d(\\mu(P), \\mu(\\widehat{P}_2))$ since $\\theta_1 < \\theta_2$. Consequently, the number of sign changes from $P$ to $\\widehat{P}_1$ is smaller than the number of sign changes from $P$ to $\\widehat{P}_2$. Meanwhile, the quality of the optimal center $\\mu(\\widehat{P}_1)$ is $\\mathrm{cost}_2(P, \\mu(\\widehat{P}_1)) = \\mathrm{cost}_2(P, \\mu(P)) + |P|\\cdot d^2(\\mu(P), \\mu(\\widehat{P}_1))$, which is better than the quality of $\\mu(\\widehat{P}_2)$ since $\\mathrm{cost}_2(P, \\mu(\\widehat{P}_1)) < \\mathrm{cost}_2(P, \\mu(\\widehat{P}_2))$. \n\nHence, as the number of sign changes increases, the quality of the optimal center for the observed dataset deteriorates. We will add this example in the final version. \n\n>7. Besides determining the quality of a coreset in the presence of noise, are there other potential applications of the proposed AR-coreset?\n\nThanks for asking this. Yes, the notion of AR-coreset may have potential applications in (noise-free cases of) machine learning tasks when one only wants to preserve near-optimal solutions, e.g., in regression. The notion of AR-coreset may be useful to further reduce the size of coreset. This is an interesting future direction and we will mention it in Section 5.\n\n>8. Can we conclude that the (near) optimal solution(s) are robust to noise for other problems?\n\nThis is a great question. We have listed it as a future direction in Section 5. At present, the existing results seem insufficient to conclude the robustness of (near) optimal solutions to noise for other problems -- the answer is likely to be problem dependent.\n\n>9. In the definition of AR-coreset, why should we consider the ratio of the cost for a given center set to the minimum cost? What would happen if we only consider the denominator $r_P(\\widehat{C})$.\n\nSince considering the denominator $r_P(\\widehat{C})$ corresponds to a specific instance of the definition of AR-coreset (Definition 2.2) with $\\alpha = 1$, using $r_P(\\widehat{C})$ will only provide a guarantee that $S$ is a $(0, O(\\varepsilon + \\frac{\\theta k d}{\\mathrm{OPT}}))$-AR coreset of $P$. In contrast, considering the ratio $r_S(C)$ of the cost for a given center set $C$ corresponds to the case $\\alpha \\geq 1$, and enables a quantitative analysis of the affect of noise on center sets $C$ as we vary $\\alpha$, for a given $S$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500416501,
                "cdate": 1700500416501,
                "tmdate": 1700500916633,
                "mdate": 1700500916633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YA7ciVIcJ9",
                "forum": "D96juYQ2NW",
                "replyto": "rav2Et5YjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Reviewer_QbwN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Reviewer_QbwN"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for your response, and I will consider it in the next discussion stage."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700813308,
                "cdate": 1700700813308,
                "tmdate": 1700700813308,
                "mdate": 1700700813308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1xpiUb7h4r",
            "forum": "D96juYQ2NW",
            "replyto": "D96juYQ2NW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_gA35"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_gA35"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of computing coreset for a dataset from its noisy perturbation. The paper considers the most apparent approach: compute a coreset for the perturbed version, and use it directly as a coreset for the original dataset. The paper showed the following results:\n1. The coreset from the noisy dataset can be very bad for the original dataset, in terms of the relative error Err commonly used to measure a coreset's approximation quality.\n2. The authors notice that the traditional measure (the relative error mentioned in 1) is too strong because it is the supreme over all possible center sets, while in practice people is more interested on how well the coresets can approximate costs for \"not so bad\" center sets. This motivates the authors to design a new relative error $Err_{AR}$ (which they call \"approximate error ratio\") that only takes supreme only over center sets that approximates the optimal solution. \n3. The authors show that this new definition of relative error can help give tigher approximation ratio estimation for center set computed on the coreset (obtained from the noisy dataset). In particular, a coreset $S$ with large Err can have much smaller $Err_{AR}$, which means a good approximate solution on $S$ will also have a small cost on the original dataset. While if we use $Err$ for estimation, the bound obtained is much looser."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the definition of $Err_{AR}$ is quite neat. The authors show that there is a strong separation between the traditional relative error $Err$ and their new measure $Err_{AR}$. I like this separation result in particular."
                },
                "weaknesses": {
                    "value": "Although the authors claim the two assumptions (i.e. $O(1)$-balancedness and well-separation) are \"mild\" and only for \"overcoming technical difficulty\", I feel they are quite strong.\nAlso, one possibility is that the separation between $Err$ and $Err_{AR}$ is actually a result of these two assumptions. It would be great if the authors can show a quantitive analysis on the dependence between the separation and the two assumptions. For example, would it be possible that when the data become less balanced / well-separated, the two measures $Err$ and $Err_{AR}$ converge to each other. (My gut's feeling is the degree of well-separation could likely have a non-trivial effect on $Err$ / $Err_{AR}$). If that's the case, I feel this would somewhat strengthen the paper's result since $Err_{AR}$ can be viewed as a unifying measure that's tighter in extreme parameter ranges."
                },
                "questions": {
                    "value": "There is no space between \"$(k,z)$-Clustering\" and the text following it. I guess the author should ad a `\\xspace` after their macro for \"$(k,z)$-Clustering\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5866/Reviewer_gA35"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820135031,
            "cdate": 1698820135031,
            "tmdate": 1699636621221,
            "mdate": 1699636621221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TlvYMIlYS1",
                "forum": "D96juYQ2NW",
                "replyto": "1xpiUb7h4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gA35"
                    },
                    "comment": {
                        "value": "Thanks for appreciating the novelty of our AR-notion and our results. We address your specific questions below.\n\n>The assumptions are quite strong ... one possibility is that the separation between $\\mathrm{Err}$ and $\\mathrm{Err}\\_{AR}$ is actually a result of these two assumptions. It would be great if the authors can show a quantitive analysis on the dependence between the separation and the two assumptions...\n\nThank you for suggesting to investigate this. Below we give an example that illustrates that if one does not assume that the clusters are well-separated, the two error measures $\\mathrm{Err}$ and $\\mathrm{Err}\\_{AR}$ may be the same. \n\nConsider a 3-Means problem over $\\mathbb{R}$, i.e., $k=3$ and $d=1$. Let $P\\subset \\mathbb{R}$ consist of $\\frac{n}{4}$ points each located at -1.01, -0.99, 0.99, and 1.01. A simple calculation shows that the optimal center set $C^\\star$ is either $\\\\{-1.01, -0.99, 1\\\\}$ or $\\\\{-1, 0.99, 1.01 \\\\}$, and $\\mathrm{OPT} = 0.0005n$ (same in both cases). \n\nFirst note that, because there exist two clusters of points that are close to each other in both optimal center sets, $P$ does not satisfy the well-separateness assumption. (The distance between the nearby clusters is 0.02 while our assumption requires that it is at least $\\sqrt{3}$.) \n\nNext, let $\\widehat{P} \\subset \\mathbb{R}$ be an observed dataset drawn from $P$ under the noise model II with each $D_j = N(0,1)$. Since $\\mathbb{E}\\_{x\\sim N(0,1)}[|x|\\mid x<0] = \\mathbb{E}\\_{x\\sim N(0,1)}[|x|\\mid x\\geq 0] = 1$, the optimal partition of $\\widehat{P}$ is likely to be $\\\\{\\widehat{p}: \\widehat{p}\\leq -1 \\\\}$, $\\\\{\\widehat{p}: -1\\leq \\widehat{p}\\leq 1 \\\\}$, $\\\\{\\widehat{p}: \\widehat{p}\\geq 1 \\\\}$ and the mean points of these three partitions are roughly -2, 0, 2 respectively. In this case, the optimal center set of $\\widehat{P}$ is likely to be $\\widehat{C} = \\\\{-2,0,2\\\\}$. Note that $\\mathrm{cost}\\_2(P, \\widehat{C}) \\approx n \\gg \\mathrm{OPT}$. By Theorem E.1, we also have $\\widehat{P}$ is a $\\frac{n}{\\mathrm{OPT}}$-coreset of $P$. Thus, $\\mathrm{Err}(\\widehat{P})\\approx \\mathrm{Err}\\_{AR}(\\widehat{P})\\approx \\frac{n}{\\mathrm{OPT}}$. \n\nWe will explain this formally in the final version.\n\n\n>There is no space between \"$(k,z)$-Clustering\" and the text following it. I guess the author should ad a \\xspace after their macro for \"$(k,z)$-Clustering\"\n\nThanks, we will fix it."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498742421,
                "cdate": 1700498742421,
                "tmdate": 1700500708618,
                "mdate": 1700500708618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "49whZ0mNiN",
            "forum": "D96juYQ2NW",
            "replyto": "D96juYQ2NW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_C3U6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5866/Reviewer_C3U6"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows how to construct coreset for clustering with noisy data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces new measures that are used to construct coreset that guarantees eps approximations.\n \n- The paper also presents a lower bound for 1-mean with noisy data. \n\n- The theory is backed by a good number of empirical evaluations on real dataset."
                },
                "weaknesses": {
                    "value": "- Some intuition and relation between the claims in section 2 will be helpful.\n\n- A formal algorithm, even in the appendix, will increase its impact."
                },
                "questions": {
                    "value": "- Please give an example of how the number of sign changes affects the goodness/quality of centers. \n\n- Coreset size inversely proportional to n and d, in coreset measure is counter-intuitive. Comments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699453572271,
            "cdate": 1699453572271,
            "tmdate": 1699636621101,
            "mdate": 1699636621101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C3UIJGWCf2",
                "forum": "D96juYQ2NW",
                "replyto": "49whZ0mNiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C3U6"
                    },
                    "comment": {
                        "value": "Thanks for appreciating our new measure and theoretical results. We address your questions below.\n\n>Comment: Some intuition and relation between the claims in Section 2 will be helpful. \n\nThanks for the comment. The intuition behind Claim 2.3 is as follows: By definition, a $\\beta$-approximate center set $C$ of $S$, for some $\\beta \\leq \\alpha$, must be a $\\beta(1+\\varepsilon)$-approximate center set of $P$. This allows us to find a near-optimal center set of $P$ from $S$. \n\nThis transitivity implies Claim 2.4 because we can obtain a near-optimal center set from a coreset or an approximation-ratio coreset $S$ of another coreset $S'$ of $P$. A bit more formally, Claim 2.3 ensures that an $\\frac{\\alpha}{1+O(\\varepsilon)}$-approximation $C$ for $S$ is an $\\alpha$-approximation for $S'$. Since $S'$ is an $\\varepsilon'$-coreset of $P$, we conclude that $C$ is an $\\alpha(1+\\varepsilon')$-approximation for $P$. \n\nWe will explain this in Section 2 in the final version.\n\n>Comment: A formal algorithm, even in the appendix, will increase its impact.\n\nIt seems like there is some confusion. We do not propose a new coreset algorithm; instead, we show how any coreset algorithm can be used to construct a coreset in the presence of noise. In particular, Lemma 3.4 asserts that when $\\varepsilon > \\frac{\\theta n d}{\\mathrm{OPT}} + \\sqrt{\\frac{\\theta n d}{\\mathrm{OPT}}}$, one can apply a given coreset algorithm $A$ to construct an $\\varepsilon$-coreset of $P$ given the noise parameter $\\theta$. \n\nSpecifically, when provided with the noise parameter $\\theta$, the noisy dataset $\\widehat{P}$, $k$, and $\\varepsilon > \\frac{\\theta n d}{\\mathrm{OPT}} + \\sqrt{\\frac{\\theta n d}{\\mathrm{OPT}}}$, one can simply return the coreset obtained from coreset algorithm $A$, where we set $P' = \\widehat{P}$, $k' = k$, and $\\varepsilon' = \\varepsilon - \\frac{\\theta n d}{\\mathrm{OPT}} + \\sqrt{\\frac{\\theta n d}{\\mathrm{OPT}}}$. \n\nThe lemma also implies that when $\\varepsilon \\leq \\frac{\\theta n d}{\\mathrm{OPT}} + \\sqrt{\\frac{\\theta n d}{\\mathrm{OPT}}}$, it is not possible to construct an $\\varepsilon$-coreset of $P$.\n\nWe will clarify this in the final version.\n\n>Please give an example of how the number of sign changes affects the goodness/quality of centers.\n\nThanks for bringing this up. Below we explain how the quality of the optimal center set of $\\hat P$ may be affected by the number of sign changes.\n\nConsider the case of 1-Means. Let $P \\subset \\mathbb{R}^d$ be an underlying dataset. Let $\\widehat{P}_1$ and $\\widehat{P}_2$ be observed datasets drawn from $P$ under the noise model II with noise parameters $\\theta_1$ and $\\theta_2$ respectively. We assume that $\\theta_1 < \\theta_2$. Recall that the optimal centers of $P$, $\\widehat{P}_1$, and $\\widehat{P}_2$ are denoted by $\\mu(P)$, $\\mu(\\widehat{P}_1)$, and $\\mu(\\widehat{P}_2)$ respectively. We note that for any two centers $c$ and $c'$ lying on the line segment $\\mu(P)-\\mu(\\widehat{P}_1)$, the signs of $\\mathrm{cost}_2(P,c) - \\mathrm{cost}_2(P,c')$ and $\\mathrm{cost}_2(\\widehat{P}_1,c) - \\mathrm{cost}_2(\\widehat{P}_1,c')$ must be different. This is besause the center that is closer to $\\mu(P)$ has a smaller cost for $P$ but a larger cost for $\\widehat{P}_1$ when compared to the other center. Hence, the number of sign changes from $P$ to $\\widehat{P}_1$ is proportional to the distance $d(\\mu(P), \\mu(\\widehat{P}_1))$. The same observation holds for $\\widehat{P}_2$. Also, note that $d(\\mu(P), \\mu(\\widehat{P}_1)) < d(\\mu(P), \\mu(\\widehat{P}_2))$ since $\\theta_1 < \\theta_2$. Consequently, the number of sign changes from $P$ to $\\widehat{P}_1$ is smaller than the number of sign changes from $P$ to $\\widehat{P}_2$. Meanwhile, the quality of the optimal center $\\mu(\\widehat{P}_1)$ is $\\mathrm{cost}_2(P, \\mu(\\widehat{P}_1)) = \\mathrm{cost}_2(P, \\mu(P)) + |P|\\cdot d^2(\\mu(P), \\mu(\\widehat{P}_1))$, which is better than the quality of $\\mu(\\widehat{P}_2)$ since $\\mathrm{cost}_2(P, \\mu(\\widehat{P}_1)) < \\mathrm{cost}_2(P, \\mu(\\widehat{P}_2))$. \n\nHence, as the number of sign changes increases, the quality of the optimal center for the observed dataset deteriorates. We will add this example in the final version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498616129,
                "cdate": 1700498616129,
                "tmdate": 1700500670874,
                "mdate": 1700500670874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IVXOPDeeJZ",
                "forum": "D96juYQ2NW",
                "replyto": "49whZ0mNiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5866/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": ">Coreset size inversely proportional to $n$ and $d$, in coreset measure is counter-intuitive. Comments?\n\nThanks for your question. Below we explain why this is not counter-intuitive. \n\nFirst, the coreset size suggested by Lemma 3.4 contains an $\\mathrm{OPT}^2$ term in the factor $\\frac{\\mathrm{OPT}^2}{n^2 d^2}$. $\\mathrm{OPT}$ typically increases linearly with $n$, e.g., in the example in Appendix B,  $\\mathrm{OPT} = n$. Thus,  $\\frac{\\mathrm{OPT}^2}{n^2 d^2}$ typically scales with $\\frac{1}{d^2}$. \n\nSecond, by Theorem 3.3, the error measure $\\mathrm{Err}(\\widehat{P}) \\geq \\Omega(\\frac{\\theta nd}{\\mathrm{OPT}})$, which increases as $d$ increases. Thus, Lemma 3.4 implies that we cannot achieve an $\\varepsilon$-coreset of $P$ for $\\varepsilon \\leq  \\frac{\\theta nd}{\\mathrm{OPT}}$. This is different from the noise-free setting where we can achieve an $\\varepsilon$-coreset for any $\\varepsilon > 0$. Since a coreset algorithm usually constructs a coreset of size proportional to $\\varepsilon^{-2}$, an $\\varepsilon = \\frac{\\theta nd}{\\mathrm{OPT}}$-coreset of $P$ is typically of size inversely proportional to $d^2$. \n\nFor instance, applying the algorithm of [Cohen-Addad et al., 2022], one can construct an $\\frac{\\theta nd}{\\mathrm{OPT}}$-coreset of $P$ with size $\\tilde{O}(\\frac{k^{1.5}}{\\theta^2 d^2})$ when $\\mathrm{OPT} \\approx n$. We will explain this in the final version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498666757,
                "cdate": 1700498666757,
                "tmdate": 1700498666757,
                "mdate": 1700498666757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]