[
    {
        "title": "Why Sanity Check for Saliency Metrics Fails?"
    },
    {
        "review": {
            "id": "o5fEJegam5",
            "forum": "Pev2ufTzMv",
            "replyto": "Pev2ufTzMv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_VUjF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_VUjF"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates perturbation-based metrics for evaluating the reliability of saliency maps for  image classification explanations. Perturbation-based approaches \u201cdelete\u201d pixels to evaluate how much each pixel contributes to the model decision, but the method used to \u201cdelete\u201d pixels (e.g., replacing them with 0s vs. random noise) has an impact on the evaluation result. This paper demonstrates that impact using blur as the deletion method and proposes a metric to measure how much the choice of replacement value in a perturbation analysis will affect the result."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "There are some interesting ideas in this paper -- it does seem worthwhile to do a more in-depth analysis of how the choice of replacement value affects the results in perturbation-based methods."
                },
                "weaknesses": {
                    "value": "The writing is frequently unclear, which makes many sections of the paper difficult to understand.\n\nThe explanation of the problem and previous literature is fairly shallow and sometimes incorrect. For example, pg 3 para 2 claims: \u201cThese metrics rely on perturbations (setting pixels to random values or 0), but do not justify the reason to do so.\u201d The justification is that these perturbations \u201cdelete\u201d the information in those pixels by replacing them with values that have no relation to the image class.\n\nThe main finding that different pixel replacements will produce different results in perturbation-based methods is unsurprising and was previously demonstrated in Tomsett et al., 2020.\n\nI\u2019m not sure the proposed metric has wide application, and this study isn\u2019t sufficient to validate it \u2013 it should be validated on a wider range of perturbations (including methods like inpainting or replacing pixels with values drawn from the same/different classes)."
                },
                "questions": {
                    "value": "\u201cOur choice of Gaussian Blur is based on the fact that it preserves the semantics of images compared to other random perturbations with mean or random values of the image.\u201d This isn\u2019t true for the CNN models used in this paper \u2013 blurring images significantly reduces the model\u2019s ability to recognize the image, similar to adding pixel noise (Geirhos et al., 2018: https://arxiv.org/pdf/1808.08750.pdf).\n\nIs the \u201cImagenette\u201d dataset used in this paper a typo for ImageNet?\n\nWhy exclude inpainting / generative models from the set of possible perturbations? This seems like a completely valid type of perturbation that is very relevant if the goal is to show that different perturbations will have different"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632430542,
            "cdate": 1698632430542,
            "tmdate": 1699636388304,
            "mdate": 1699636388304,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9SDSYf3trk",
                "forum": "Pev2ufTzMv",
                "replyto": "o5fEJegam5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal: Official Review of Submission4212 by Reviewer VUjF"
                    },
                    "comment": {
                        "value": "Overall Comment: We thank the reviewer for the helpful comments and suggestions.\n\nW1: \"The writing is frequently unclear, which makes many sections of the paper difficult to understand.\"\n\nAuthor's comment: Thanks for the suggestion. We will work on the language and make it more readable in the final version.\n\nW2: \"The explanation of the problem and previous literature is fairly shallow and sometimes incorrect. For example, pg 3 para 2 claims: \u201cThese metrics rely on perturbations (setting pixels to random values or 0), but do not justify the reason to do so.\u201d The justification is that these perturbations \u201cdelete\u201d the information in those pixels by replacing them with values that have no relation to the image class.\"\n\nAuthor's comment: We discuss it in Section 2.1, \"The main problem with perturbations of 0 or a random RGB value is that such perturbations destroy meaningful image semantics. Thus, the change in image semantics would also contribute to the change in classification probability. It is, therefore, challenging to compartmentalize if the change in classification/decision\nprobability results from perturbation or changed image semantics (Hooker et al., 2019).\". Our work is motivated by the observations of Hooker et al., 2019 who, in their study, proved that replacing image pixels with random value or \"0\" will have unwanted impact on the classification probability.\n\nW3: \"The main finding that different pixel replacements will produce different results in perturbation-based methods is unsurprising and was previously demonstrated in Tomsett et al., 2020.\"\n\nAuthor's comment: We believe the reviewer has misinterpreted our contribution. We do not dispute the obvservation so Tomsett et al., 2020 as discussed in page no 3, paragraph no 3. We try to investigate why such inconsistencies were observed by Tomsett et al., 2020 (refer Section 2.1 and Section 3) as our main contribution.\n\nW4: \"I\u2019m not sure the proposed metric has wide application, and this study isn\u2019t sufficient to validate it \u2013 it should be validated on a wider range of perturbations (including methods like inpainting or replacing pixels with values drawn from the same/different classes).\"\n\nAuthor's comment: Our goal, in this paper, was to investigate the reason behind the inconsistencies observed by Tomsett et al., 2020. For this, we proposed the metrics ARPPD and APRC. Any fidelity analysis without investigating ARPPD and APRC would be ambiguous (as discussed in Section 3 and experimentally supported in Section 5). We show the applicability of the metrics by a case study of Gaussian Blur. However, our framework can be easily extended to study any other perturbation for fidelity analysis. \n\nQ1: \"\u201cOur choice of Gaussian Blur is based on the fact that it preserves the semantics of images compared to other random perturbations with mean or random values of the image.\u201d This isn\u2019t true for the CNN models used in this paper \u2013 blurring images significantly reduces the model\u2019s ability to recognize the image, similar to adding pixel noise (Geirhos et al., 2018: https://arxiv.org/pdf/1808.08750.pdf).\"\n\nAuthor's comment: Please refer to Author's comment for W2. The provided reference talks about the drop in classification accuracy for different values of hyper parameters for different types of perturbations. As illustrated in Figure 3(c) and 3(d) in Geirhos et al., 2018, the drop in classification accuracy for blur decays much slower than that of uniform noise indicating that blurring images does not significantly reduce the model's ability to recognize the image for lower sigma values.\nFurther, it is not enough to draw the conclusion that blur has similar effect on the drop in output probability as compared to other perturbation methods like mean or random values, both of which have significant effect of introducing perturbations that can change the meaning of image. Further, in scenarios where the output probability for the top class is close to the next class, a small change in the output probability might flip the class and increase the misclassification rate without significant change in the output probability.\n\nQ2: \"Is the \u201cImagenette\u201d dataset used in this paper a typo for ImageNet?\"\nAuthor's Comment: As stated in the Section 4 Experimental Setup, Imagenette is a subset of the ImageNet Dataset with 10 easily identifiable classes. Hence, it is not a typo. \n\nQ3: \"Why exclude inpainting / generative models from the set of possible perturbations? This seems like a completely valid type of perturbation that is very relevant if the goal is to show that different perturbations will have different\"\n\nAuthor's comments: Please refer to Author's comment for W4."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520634489,
                "cdate": 1700520634489,
                "tmdate": 1700520634489,
                "mdate": 1700520634489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sMrr6jLvoO",
                "forum": "Pev2ufTzMv",
                "replyto": "9SDSYf3trk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Reviewer_VUjF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Reviewer_VUjF"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\nTo respond to your clarification about the main goal/contribution of the paper, namely:\n\"We try to investigate why such inconsistencies were observed by Tomsett et al., 2020 (refer Section 2.1 and Section 3) as our main contribution.\"\n\nI don't agree that this is a good thing to investigate -- I think it's already very well understood why different perturbations have different effects. It's just hard to predict precisely what effect a given perturbation will have.\n\nEither way, if this is the goal/contribution, then it would be important to investigate a much wider range of perturbations than just blur, including generative model perturbations."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633820454,
                "cdate": 1700633820454,
                "tmdate": 1700633820454,
                "mdate": 1700633820454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r4apqv11aM",
                "forum": "Pev2ufTzMv",
                "replyto": "woj5KMHviK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Reviewer_VUjF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Reviewer_VUjF"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Different perturbations produce different results because they change the image in different ways, and thus are interpretted in different ways by the neural network.\n\nFor example, suppose instead of replacing pixels with 0 (black) you replaced them with specific colours (e.g., red, or blue, or green). You should expect each of these colours to have a different result, because they will change the model's decision-making process in different ways depending on the image/perturbed area and the ground truth class/potential alternate classes. If you perturb an image of an apple by replacing the apple pixels with black or blue pixels that should change the model's decision a lot. If you replace the apple with red or yellow pixels, that might not change the model's decision much at all, since a blob or red or yellow pixels might still be interpretted as an apple.\n\nI don't know if there's any publication that specifically states this, since it follows straightforwardly from how neural networks work (or how any vision method works). It's not the kind of thing you would explain in a paper. It's a bit like asking for a citation for the claim that 25 is greater than 3 -- I don't think you can find a publication that specifically states that, it just follows straightforwardly from basic math.\n\nAs I said before, predicting exactly *how* a specific perturbation will change the model's decision is not well understood. But *why* it changes is well understood."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703846167,
                "cdate": 1700703846167,
                "tmdate": 1700703846167,
                "mdate": 1700703846167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9uaHTNU3Js",
                "forum": "Pev2ufTzMv",
                "replyto": "o5fEJegam5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer's comment"
                    },
                    "comment": {
                        "value": "We thank the reviewer for going through our comments and replying with additional comments. Please find our response below:\n\nWe agree with the reviewer's statement: \n\n> \"predicting exactly how a specific perturbation will change the model's decision is not well understood.\"   ---- (1)\n\n> \"But why it changes is well understood.\"  -------------(2)\n\nInfact point(1) is the main basis of our study.\n\nAs stated in section 2.1, perturbation-based saliency metrics make an implicit assumption regarding how perturbations change the model's decision (i.e., the drop in output probability is proportional to the relevance/importance of the pixel).  \\----(3)\n\nThe assumption in point (3) has two main parts viz:\n\n1. There will always be a positive drop (i.e., Prob(unperturbed_image) - Prob(perturbed_image) > 0) in output probability if a relevant pixel is removed (i.e. perturbed as removing or deleting a pixel is not possible. We can at best change the value of the pixel and not delete it in images)  \\-------------------(3a)\n\n2. The drop in output probability is proportional to the relevance/importance of pixels, i.e., the output probability will drop to a larger extent for a more important pixel than a less important one.  \\-------------------(3b)\n\nHence, we see that both parts of assumption of perturbation-based saliency metrics (i.e., 3a and 3b) might deviate from point (1) i.e. violating the belief in point (1). We will now show how our proposed approach and metrics helps in quantifying this deviation.\n\nTo quantify the deviation of point (3a), we proposed ARPPD, which is the fraction of pixels under investigation with a positive drop in output probability. Thus lower ARPPD would indicate higher deviation of point (3a) from point (1) indicating inconsistency of saliency metrics. ----------------------------------------------------------- (A)\n\nPoint 3b talks about the rank list of pixels (Ig) which is different from the rank list of pixels given by saliency maps. 'Ig' is calculated by looking at the amount of drop in output probability by perturbing different pixels and is calculated internally (implicitly) by the saliency metrics. So, in other words the saliency metrics assume Ig as some kind of ground truth and gives a higher score to a saliency map whose pixel ranks is closer to Ig. \n\nHowever, Ig may or maynot change for different perturbations. This is because although we know that the output probability changes for different perturbations but we dont know if such changes impact Ig. For example, it might happen that for a milder perturbation, the drops in output probability for the pixels a, b and c were 0.01, 0.02 and 0.03 respectively and for a stronger perturbation (like higher sigma is Gaussian blur) the drops in probabilities for a,b and c were 0.3, 0.4 and 0.5. In both the mentioned cases the amount of drops were different but Ig (i.e. the relevance or importance order of a,b and c) remains the same. Similarly, there might be scenarios where the importance order of a,b, and c changes. So, our proposed metric, APRC, quantifies this variation in Ig by looking at a perturbation(Gaussian blur as a case study) and changing its hyper-parameter (sigma). If APRC is 1 then it indicates that Ig remains the same inspite of the changes in the amount of drop in output probability and hence the perturbation based saliency metrics would be consistent. On the other hands if APRC is low then it indicates that the perturbations based saliency metrics would be inconsistent. \n\nThat is, APRC quantifies the deviation of point (3b) from point (1). ------------------------------------------(B)\n\nOverall summary: We agree with the reviewer's comment: \"predicting exactly how a specific perturbation will change the model's decision is not well understood. But why it changes is well understood.\" and our paper is based on it. The concluding points i.e. (A) and (B) show that our work is in agreement with the reviewer's comment. Further, Gaussian blur was used as a case study and without the loss of generality, any other perturbation can be used to check its suitability for perturbation based saliency metrics. We hope our study enables the research community to choose correct perturbation for fidelity analysis and it leads to consistent fidelity results."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719869134,
                "cdate": 1700719869134,
                "tmdate": 1700729067661,
                "mdate": 1700729067661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wNUm7T6NSu",
            "forum": "Pev2ufTzMv",
            "replyto": "Pev2ufTzMv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_WPYL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_WPYL"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the saliency map created by CAM or its variation. Gaussian Blur is applied with different std values to perturb the image saliency. Then two metrics are proposed to measure the fidelity of the saliency map, denoted as Average Ratio of Positive Probability Drops, and Average Pixel Rank Correlation. The use of Gaussian with the proposed metrics is claimed can better measure saliency CAM."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This study attempts to analyze an important task in AI, how to understand the behaviour of a model, and how to measure if the understanding is correct."
                },
                "weaknesses": {
                    "value": "The current draft requires some improvement before publishing, it is a little difficult to follow the idea.\nThe draft attempt to solve the problem in the fidelity measure for CAM, but the presentation did not show this main issue clearly. Some of the paragraphs are not related or unnecessary, please see the questions below. More importantly, the use of Gaussian Blur is not well introduced or explained, it seems an arbitrary solution jumping into the draft. The only reason discussed in Section 3.3 is to preserve the semantic information instead of removing it. However, this is exactly opposite to those previous perturbation used for fidelity measure. If the semantic content is always needed, how can we measure the CAM? or we do we still need to perturb? This draft seems missed the purpose of the perturbation, so the whole story is less solid. Besides the general idea, I highly doubted if Gaussian blur can retain the semantic information when the std is high enough. The two \"proposed\" metrics are very similar to previous solutions but using the Gaussian perturbation, thus the metrics cannot be considered as new proposals. To sum up, this draft requires more improvement at this moment."
                },
                "questions": {
                    "value": "1. Generally, the abstract and the introduction show the inconsistency in those saliency metric, which is used to measure the fidelity of those XAI methods. However, Fig.1 only shows the tiny difference in the CAM visualization, and this showcase seems not connected to the main story. Is it better to show the behaviour of those metrics when perturbing with different strategies? \n\n2.The definition of the hierarchy in Table is not clear, could be removed.\n3. Page 3, third paragraph, \"the reason for such unreliability.. lies at model level\", this claim is questionable, and I cannot find any explanation for this claim.\n4. The same paragraph, \"introduce two new metrics,....at model level 1\", this is confusing without any explanation, the proposed metrics are also based on the change of confidence scores or pixel rankings, why they are at different levels than the previously proposed? Again, I suggest introducing the idea without relying on the confusing hierarchy could be more clear.\n5. The extensive related work is not really necessary, the most related work seems (Tomsett 2020) and those fidelity metrics. However, they are not properly presented regarding the key problem, why different perturbation may lead to different results.\n6. Section 2.1, first paragraph is confusing to me. It seems the whole paragraph is repeating how those metrics works, without introducing what the limitation is and why it is a limitation.\n7. Section 2.2 Contribution 1: \"to justify the unreliability\"?? Again, the draft didnot show clearly what the unreliability is, and why does it need to be justified??\n8. Section 2.2 Contribution 2: \"Unlike...\", The proposed methods are very similar to the previously used for fidelity measure, this claim is unclear to me.\n9. Section 2.2, Contribution 3: ablation studies on different models and data are not considered as contributions.\n10. Section 3.1 \"Let R0 be the ranks of pixel\", this sentence is wrong and inconsistent with the story after, it is not a list of ranks, it is a list of ranked pixels.\n11. Section 3.1, the presented \"framework\" is not necessary, this is the common assumption behind most of the fidelity measure.\n12. Eq(6), why is the rank invariant to the std value applied? \n13. what is the meaning of the brackets in Eq 7?\n14. For both metric, why are they both approaching 1 ideally? how can we know which saliency map is better? Strong gaussian blur will damage the semantic content, why we are still expecting 1?\n\nTiny problems:\nThe use of acronym is inconsistent."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731047503,
            "cdate": 1698731047503,
            "tmdate": 1699636388208,
            "mdate": 1699636388208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n7vwuUB2K7",
                "forum": "Pev2ufTzMv",
                "replyto": "wNUm7T6NSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal: Official Review of Submission4212 by Reviewer WPYL"
                    },
                    "comment": {
                        "value": "Overall Comment: We thank the reviewer for the helpful comments and suggestions. However, our work seems to be misinterpreted and we respectfully clarify the same. We will use \"assumption\" to denote \"output probability is proportional to the relevance/importance of the pixel\" in all subsequent text:\n\n\u201cThis paper studies the saliency map created by CAM or its variation.\u201d\n\nAuthor\u2019s comment: We do not study the saliency maps created by CAM or it variants. \n\n\u201c Gaussian Blur is applied with different std values to perturb the image saliency. \u201d\n\nAuthor\u2019s Comment: We perturb the original image and this is done to validate the assumption. We therefore take Gaussian blur as a case study to validate the assumption.\n\n\u201cThen two metrics are proposed to measure the fidelity of the saliency map, denoted as Average Ratio of Positive Probability Drops, and Average Pixel Rank Correlation.\u201d\n\nAuthor\u2019s comment: We are proposing ARPPD and APRC metrics to validate the assumption, and not to measure the fidelity of the saliency maps. Our experiments are designed to answer why existing fidelity metrics are were observed as inconsistent. \n\n\u201cThe use of Gaussian with the proposed metrics is claimed can better measure saliency CAM.\u201d\n\nAuthor\u2019s comment: Our study does not propose any fidelity metric for CAM. However, we consider Gaussian blur for illustrating a case study for using our metrics. Our metrics to study the inconsistency can be used with any other perturbation.\n\n----\n\nAuthor\u2019s comment for Q1: We focused on understanding the inconsistent behavior in saliency metrics, as already observed by Tomsett et.al, rather than repeating their experiments. Our goal was to understand why such inconsistent behavior was observed. However, we will consider aligning the content to eliminate any confusion and make the message clearer.\n\nAuthor\u2019s comment for Q2: Without Table 1 it is likely that the reader might confuse our work as another metric to measure fidelity of saliency maps. \n\nAuthor's comment for Q3: The claim is discussed in detail in the third paragraph in page 3 (i.e., above the mentioned lines). Further, it has been discussed in detail in Section 2.1 and in Section 3 along with the theoretical justification (Sec 3.1) and the proposed metric (Sec 3.2). The low value observed for ARPPD and APRC in the Results and Discussion (Section 5) provide evidence for the unreliability at the model level.\n\nAuthor's comment for Q4: Saliency metrics use the change in output probability for quantifying the fidelity of the saliency maps. However, we use the same drop in output probability with different formulation (refer Section for saliency metrics and Section 3.2 for proposed metrics) to validate the assumption and provide a score for it. \n\nAuthor's comment for Q5: We have provided extensive set of related works for the better understanding of the problem. However, we will make it concise by aligning our arguments based on the problem our paper is addressing.\n\nAuthor's comment for Q6: We discuss the limitation and why it is a limitation in Section 2.1. However, we will revise this section to make the limitation explicit based on the suggestion of the reviewer.\n\nAuthor's comment for Q7: We'll replace \u201cjustify the unreliability\u201d with \u201cinvestigate the inconsistency\u201d for clarity.\n\nAuthor's comment for Q8:: Define what is different to previous metrics (levels and what they measure)\n\nThe previous metrics like AOPC, Average Drop(AD%), Increase in Confidence(IC%), Win(W%) etc measure the fidelity of saliency maps. These metrics use the observed change in output probability by removing pixels from an image to calculate the fidelity score (as explained in Section 2.1).\n\nOur proposed metrics too use the observed change in output probability before and after perturbation, but we use the change to determine if the assumption holds true or not and quantify it (discussed Section 3 and Section 3.1). The context of \u201cunlike\u201d is to indicate not the observation, but the reason behind the observation.\n\nAuthor's comment for Q11: The theoretical framework is necessary as it enables us to evaluate if the assumption holds true or not. While we agree that it is a common assumption, we also mention this explicitly in our paper to illustrate the fact that it does not always hold true. Further, the low values of ARPPD and APRC in Section 5 supports that the assumption does not always hold true.\n\nAuthor's comment for Q12: Rank invariance is key to avoid fidelity score ambiguities due to different perturbations, addressing the inconsistency observed by Tomsett et.al.\n\nAuthor's comment for Q13: The brackets in Eqn 7 indicate an Indicator function, which we'll elaborate on in the equation's description.\n\nAuthor's comment for Q14: Ideal ARPPD and APRC values of 1 suggest perturbation invariance, indicating consistent fidelity analysis, regardless of perturbation type.\n\nAuthor's comment for Q9, Q10 and Q15: Thanks for the comments. We will address them in the final version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586963380,
                "cdate": 1700586963380,
                "tmdate": 1700586963380,
                "mdate": 1700586963380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2yACCZQgub",
                "forum": "Pev2ufTzMv",
                "replyto": "n7vwuUB2K7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Reviewer_WPYL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Reviewer_WPYL"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Thanks the author for answering those questions, however, the answer cannot make the draft clear.\n1. \"Author\u2019s comment: We do not study the saliency maps created by CAM or it variants.\"\nCould you please add one more sentence explaining what the draft is studying? If this is not the case, then the writing is even more confusing, and I could not figure out what Fig1 is for. Is it the assumption that pixel-wise perturbation on CAM maps is not reliable?\n\n2.\"Author\u2019s Comment: We perturb the original image and this is done to validate the assumption. We therefore take Gaussian blur as a case study to validate the assumption.\" \nThis answer seems a statement not an explanation. could you please explain what problem this draft is solving? Gaussian Blur to validate what assumption? Pixel-wise measure is not reliable? Why Gaussian can measure validate this and how it is different from the pixel-wise? If the saliency map is pixel-wise, why we have to apply gaussian? The Guassian is also applied pixel-wise, why the std region helps or showcases the problem of the previous perturbation?\n\n3. \"We are proposing ARPPD and APRC metrics to validate the assumption, and not to measure the fidelity of the saliency maps. Our experiments are designed to answer why existing fidelity metrics are were observed as inconsistent.\"\nI didn't see how the proposed metrics can evaluate the inconsistency. Page 6 3.2, \"critical questions 1 and 2\", you can use gaussian to show the output is different than pixel-wise attack without explaining why? One can use any perturbation to show a different result but it cannot show why pixel-wise is a problem.\n\n4. \"Our study does not propose any fidelity metric for CAM. However, we consider Gaussian blur for illustrating a case study for using our metrics. Our metrics to study the inconsistency can be used with any other perturbation.\"\nThen the abstract and introduction can be re-written for a better readability. Maybe it is better to show why pixel-wise attack is a problem then why your metric can showcase the problem. Simply showing \"difference\" is not meaningful.\n\n6. \"Author\u2019s comment for Q1\": Again, showing a difference is not informative, showing why pixel-wise perturbation is a problem could be more clear.\n7. \"Author\u2019s comment for Q2\": Maybe this is just to me, it is my first time to see this categorization and it is not clear to me why they are grouped like that. This is not a main problem.\n8. \"Author\u2019s comment for Q3\": This is still questionable to me, why the rank of that pixel has to be invariant to any perturbation? If I apply a Guassian to the top rank pixel, the spatial neighbourhood will also be affected, which makes the rank changes significantly. I cannot see why the assumption is true.\n9. \"Author\u2019s comment for Q4:\"  Isnt \"drop in output probability\" considered a change in the output probability?? The \"proposed\" metric is measuring the same property.\n10 \"Author\u2019s comment for Q8:\" Again, I doubt if the assumption holds true, it is only possible to theoretically discuss why the rank change is problem, and why your metric can show this. To me, they are both measuring the drop in the output space based on perturbation on pixels. Or let me raise this question, if I apply Gaussian perturnbation with the old metrics, and the rank changes, can I say the old rank is enough to showcase the \"problem\"(questionable if it is a problem within the scope of this draft)?\n11. \"Author's comment for Q14\": Ideal ARPPD and APRC values of 1 suggest perturbation invariance, I highly doubt if Gaussian blur can give you invariance in this case, it could make the explanation even more confusing considering other pixels in the neighbourhood into account."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598163622,
                "cdate": 1700598163622,
                "tmdate": 1700598163622,
                "mdate": 1700598163622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AXdZLNKEg9",
                "forum": "Pev2ufTzMv",
                "replyto": "wNUm7T6NSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewers questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reviewing our comments and putting the next set of questions. We will first humbly describe an example and then answer the questions.\n\nSuppose we want to evaluate the fidelity of saliency maps generated by two CAM methods (GradCAM and GradCAM++) for an image I and model M. \n\nLet\u2019s assume that there are only 6 pixels(for simplicity) in the image which are a,b,c,d,e,f. Based on GradCAM's saliency map, the importance of the pixels(I1) is: a,c,b,d,e,f and based on GradCAM++, the importance ranks of the pixels(I2) is: a,d,f,e,c,b. The importance order is most important first. The task of any saliency metric is to idenity which one of I1 and I2 has higher fidelity. \n\nSuppose the saliency metric uses perturbation technique p1. Based on the drop in probability, the saliency metric calculates a score like AOPC or insertion AUC or deletion AUC etc for I1 and I2. Based on this score, the saliency metric would score either the saliency map from GradGAM (i.e. I1) or the one from GradCAM++(I2) to have higher fidelity. However, the saliency metrics have an implicit assumption that the drop in output probability is proportional to the importance of the pixel (section 2.1). \n\nTo explain it further, let Ig be the importance rank of pixels (lg: a,c,d,b,e,f for p1) obtained by perturbing pixels. Ig is different from I1 and I2 as it is calculated internally (or implicitly) by the saliency metrics. Since, l1 is more similar to lg (i.e. higher AOPC or insertion-AUC score etc) as compared to I2 so, I1 would be rated as having higher fidelity. However, for perturbation(p2), lg' might be (a,d, f,e, b,c) and so, for p2, I2 would be rated as having higher fidelity(discussed in section 2.1). Thats is to say, if for different perturbations lg changes, then for some perturbation it would rate I1 as having more fidelity and for some other perturbation it might rate I2 to have more fidelity (discussed in section 2.1). But this inconsistency would not be there if for all perturbations, there is no variation in Ig. In our paper, we show that the inconsistency observed by Tomsett et al 2020 was because of this change in Ig for different perturbations (discussed in 3.1 eqn 6). And so to evaluate the inconsistency we can bypass the analysis of the the saliency maps(i.e. I1 and I2) generated by the CAM methods and directly use ARPPD and APRC(which are derived from section 3.1 eqn 6). This is one positive aspect of our approach i.e. we can evaluate the inconsistency in saliency metrics without analyzing the saliency maps from different CAMs. Although, we use change in output probability (like saliency metrics) but we use it to calculate variation in Ig for different perturbations.\n\nComment for Q1: We are not studying the saliency maps created by CAM and its variants but we are studying the reason behind the inconsistency of perturbation based saliency metrics to evaluate the fidelity of saliency maps (kindly refer above example). This inconsistency (as observed by Tomsett et al. 2020) was apparent in how different CAM methods were scored by existing saliency metrics, with the scores varying based on the perturbations applied. Consequently, the fidelity of different CAM methods was rated differently under different perturbations.\n\nComment for Q2: Tomsett et al. 2020 used perturbations like making the pixels 0 and replacing with random valueswhich are quite different. Therefore, we used Gaussian Blur with different sigmas so that the perturbations we use, were similar to each other than those used by Tomsett et al. 2020. If the ranks of the pixels \u2018Ig\u2019(calculated by looking at the drop in output probability) vary for small changes in sigma of Gaussian blur then it indicates that the ranks would vary even more with entirely different perturbation technique (like assigning 0 or random value). \n\nComment for Q3: As observed by the reviewer \"One can use any perturbation to show a different result\" i.e. based on different perturbations, Ig would be different and as such saliency metrics would rate different CAM method to have higher fidelity (explained in the above example)\n\nComment for Q4: We will make the abstract and introduction more readable as per the suggestion of the reviewer.\n\nComment for Q5 and Q7: Kindly refer to the example provided above.\n\nComment for Q8: The example above discussed different uses of drop in output probability.Gaussian blur is used as a case study but the theoretical justification in section 3.1 and the proposed metrics 3.2 can be used to evaluate if Ig changes, for other types of perturbations too. \n\nComment for Q9: As observed by the reviewer using stronger perturbations (like Gaussian blur with high sigma) would alter the output probability significantly. Hence, this makes it imperative to choose the perturbation technique carefully and accept the results of fidelity analysis as dependent on perturbation technique used."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626745222,
                "cdate": 1700626745222,
                "tmdate": 1700626789116,
                "mdate": 1700626789116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "h9pfV8qgLY",
            "forum": "Pev2ufTzMv",
            "replyto": "Pev2ufTzMv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_k88p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_k88p"
            ],
            "content": {
                "summary": {
                    "value": "This paper indicates that the existing saliency metrics based on pixel importance are unreliable when subjected to perturbations. To quantify the inconsistencies in saliency metrics, the Average Ratio of Positive Probability Drops(ARPPD) and the Average Pixel Rank Correlation(APRC) are introduced to measure the unreliability at model level."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe authors investigated the reason of statistical inconsistencies in the existing saliency metrics which is omitted in previous studies.\n\n2.\tExtensive experiments demonstrated the applicability of the proposed metrics."
                },
                "weaknesses": {
                    "value": "1.\tThe captions in Figure 2 are illegible, and Figure 2 needs more detail legends.\n\n2.\tI wonder if the perturbation is similar to some kinds of Data Augmentation, the results may be impacted."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4212/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4212/Reviewer_k88p"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770948513,
            "cdate": 1698770948513,
            "tmdate": 1699636388134,
            "mdate": 1699636388134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q1sR11Ig68",
                "forum": "Pev2ufTzMv",
                "replyto": "h9pfV8qgLY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal: Official Review of Submission4212 by Reviewer k88p"
                    },
                    "comment": {
                        "value": "Overall Comment: We thank the reviewer for the helpful comments and suggestions.\n\n\nQ1: \"The captions in Figure 2 are illegible, and Figure 2 needs more detail legends.\"\n\nAuthor's comment: Thanks for the feedback. We will take address the points regarding Figure 2 in the final version.\n\n\nQ2: \"I wonder if the perturbation is similar to some kinds of Data Augmentation, the results may be impacted.\"\n\nAuthor's comment: We acknowledge that different data augmentation, representing different perturbations, can be interesting for future works. In the scope of this work, we focused on the theoretical framework and the proposed metrics which can be easily extended for other perturbations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516644183,
                "cdate": 1700516644183,
                "tmdate": 1700516644183,
                "mdate": 1700516644183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w2ojmAuWPj",
            "forum": "Pev2ufTzMv",
            "replyto": "Pev2ufTzMv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_1hvD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4212/Reviewer_1hvD"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the problem of disturbance in saliency metric, and holds that pixel importance become unreliable for measuring the fidelity of saliency maps. The authors proposed two metrics to quantify the inconsistencies using prediction probability change and variation of pixel ranks and make two related experimental analysis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problems discussed are of research significance and worthy of attention.\n2. Two metric are proposed to analyze the influence of pixel sorting disturbance."
                },
                "weaknesses": {
                    "value": "1. Motivation is not sufficient. It is far-fetched to introduce the necessity of metric research of saliency map from XAI. Especially in Figure 1 and Table 1, the conclusions drawn have limited relevance to the background of the topic introduced in this paper.\n2. The argument is not rigorous enough. The article mentioned many times that we should investigate the reasons behind the inconsistency, but we didn't see the relevant analytical statements and experimental analysis in the article, and only gave Figure 1, but Figure 1 can only show that there is no difference between models and it is not directly related to the disturbance emphasized in this paper.\n3. The experiment is not reasonable enough. In this paper, a total of 26,267 photos before and after the disturbance are directly tested together, which does not reasonably distinguish the experimental comparison before and after the disturbance, and can not clearly see the inconsistency of metirc after the disturbance.\n4. The conclusion is incomplete. From the analysis of arppd in table2, the conclusion of \"much more compliant\" cannot be drawn by hovering between 0.6 and 0.8. In addition, the final experimental conclusion has no exact correlation with the title of this paper, and the reasons behind it are not given."
                },
                "questions": {
                    "value": "See the weakness listed above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834531362,
            "cdate": 1698834531362,
            "tmdate": 1699636388065,
            "mdate": 1699636388065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xTiCPDPmtB",
                "forum": "Pev2ufTzMv",
                "replyto": "w2ojmAuWPj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal: Official Review of Submission4212 by Reviewer 1hvD"
                    },
                    "comment": {
                        "value": "Overall Comment: We thank the reviewer for the helpful comments and suggestions. \n\nQ1: \"Motivation is not sufficient. It is far-fetched to introduce the necessity of metric research of saliency map from XAI. Especially in Figure 1 and Table 1, the conclusions drawn have limited relevance to the background of the topic introduced in this paper.\"\n\nAuthor's comment:\nFigure 1 explains the need for saliency metrics as different saliency methods highlight different regions for the same model and the same image to be explained. However, these saliency metrics are not thoroughly analyzed for their reliability   as mentioned by Tomsett et al., 2020. Hence, we take this as the background of our work.\nTable 1 illustrates the 4 levels and helps in explaining why the inconsistencies are not just the property of the saliency metrics but a property of the model itself which is not studied so far . We prove that the inconsistency is a model property by theoretical justifications [Section 3] and the low value of ARPPD and APRC in Results [Section 5]\n\nQ2: \"The argument is not rigorous enough. The article mentioned many times that we should investigate the reasons behind the inconsistency, but we didn't see the relevant analytical statements and experimental analysis in the article, and only gave Figure 1, but Figure 1 can only show that there is no difference between models and it is not directly related to the disturbance emphasized in this paper.\"\n\nAuthor's comment: Our main argument is to question the validity of the assumption that \u201cOutput probability drop is proportionate to the importance of a pixel\u201d. This assumption is central to all metrics mentioned in Section 2 but there has been no study to verify the same . Section 2.1 and in section 3 we propose our framework with theoretical justification and noting the deficiency, we propose two metrics  validate our hypothesis . As a case study, we present experimental analysis with Gaussian Blur to disprove that the mentioned assumption does not hold true for all cases. This explains why Tomsett et al., 2020 observed the inconsistencies.\n\nQ3: \"The experiment is not reasonable enough. In this paper, a total of 26,267 photos before and after the disturbance are directly tested together, which does not reasonably distinguish the experimental comparison before and after the disturbance, and can not clearly see the inconsistency of metirc after the disturbance.\"\n\nAuthor's comment: The results for each dataset and model explained in table 2 and figure 2 for ARPPD and APRC has been clearly mentioned. As proved in section 3, the metrics ARPPD and APRC capture the impact of applying different perturbations (as a case study we analyzed Gaussian Blur with three sigma values). As per the proof of Sec 3 and Sec 3.1, the ideal value of these metrics should be 1 but as seen in Table 2 and Figure 3, the observed values are far below it.  The observation proves the very assumption (that the drop in output probability is proportional to the importance of the pixel) does not hold true necessitating the careful application of saliency metrics for fidelity analysis. \n\nQ4: \"The conclusion is incomplete. From the analysis of arppd in table2, the conclusion of \"much more compliant\" cannot be drawn by hovering between 0.6 and 0.8. In addition, the final experimental conclusion has no exact correlation with the title of this paper, and the reasons behind it are not given.\"\n\nAuthor's comment: The title says: \u201cWhy Sanity Check for Saliency Metrics Fails\u201d and in Section 5, we answer this question as: \u201cAs seen from Table 2, APRC values are low at each class level and also over the whole dataset. This indicates that there is high variance in the ranks of the pixels for the same model and the same image for different types of perturbations. This would mean that the same pixel would be given different ranks for different perturbations. Thus, the idea of verifying pixel importance ranks of saliency maps leveraging perturbations would be unreliable due to this high variance in pixel ranks. Consequently, all metrics like AOPC and faithfulness etc. that implicitly rely on the invariance of pixel ranks would be rendered ineffective in verifying the fidelity of saliency maps.\u201d. \n\nFurther, Eqn 4 for ARPPD measures what fraction of pixels adhered to the assumption that the output probability drops on removing a relevant pixel. An ARPPD score of 0.8 would mean that 80% of the analyzed pixels had a drop in output probability but for 20% there was an increase in the output probability (contrary to our assumption of drop). Hence a higher score is better.  As explained in Section 3, a higher score indicates consistency and a lower score indicates inconsistency in fidelity analysis using saliency metrics."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515951660,
                "cdate": 1700515951660,
                "tmdate": 1700515951660,
                "mdate": 1700515951660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]