[
    {
        "title": "Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks"
    },
    {
        "review": {
            "id": "w36UlVPQZQ",
            "forum": "3z60EWfh1p",
            "replyto": "3z60EWfh1p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_NCqT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_NCqT"
            ],
            "content": {
                "summary": {
                    "value": "- The paper introduces a novel transfer algorithm called GATE based on Riemannian differential geometry.\n- GATE is designed for regression tasks in inductive transfer learning and outperforms conventional methods in molecular property regressions.\n- The authors propose a method to match coordinate patches on a manifold using an autoencoder scheme and consistency loss.\n- The approach allows for the flow of information from the source domain to the target domain, improving the model's generalization capabilities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Experiment demonstrates superior performance of GATE in extrapolation tasks, with 14.3% lower error in scaffold split compared to conventional methods.\n- This paper shows stable underlying geometry of GATE's latent space and robust behavior in the presence of data corruption.\n- This paper provides ablation studies and further analysis to understand the role of distance loss and the stability of the latent space."
                },
                "weaknesses": {
                    "value": "- The experiments are based on 14 molecular datasets and most counts are limited (largest one contains 73k count, while smallest only has 241). This limits the generalization of such method on other large dataset.\n- Lack of analysis on the computational efficiency or scalability of the GATE algorithm.\n- The proposed method, in abstract, is an encoder-decoder-based method, which is kind of trivial in the transfer learning setup."
                },
                "questions": {
                    "value": "- The distance loss is simplified as equation 12, which is basically Euclidean distance. This is contradict to the manifold setup. Can the author discussed more about the distance loss, which seems to be the major difference the author proposed compared with other existing methods.\n- The ablation study shows that the problem is an overfitting problem. So it basically means the data is not enough/ early stopping should help. So the real contribution of such method is slightly not distinguishable. \n- Some tiny piece: \n  - 80-20 split of train and test, is not 4-fold cross-validation. \n  - Figure 3 shows GATE as circle, does this mean all equal or it takes GATE as reference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733770508,
            "cdate": 1698733770508,
            "tmdate": 1699636404032,
            "mdate": 1699636404032,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "35Qg7C51S7",
                "forum": "3z60EWfh1p",
                "replyto": "w36UlVPQZQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Weaknesses:\nThe experiments are based on 14 molecular datasets and most counts are limited (largest one contains 73k count, while smallest only has 241). This limits the generalization of such method on other large dataset.\n \n- The reviewer appears to have a misunderstanding about transfer learning in general contexts. Transfer learning is a methodology aimed at transferring useful information from datasets with abundant or robust model knowledge to tasks where data is scarce or learning is challenging. To test this, it is natural to use datasets with a limited amount of data. Furthermore, the applicability of a model is not restricted by what dataset we used to demonstrate the performance. Given that the primary contribution of this paper lies in enhancing generalization ability, the reviewer's comment appears to be unreasonable.\n \n \nLack of analysis on the computational efficiency or scalability of the GATE algorithm.\n\n- For the same reasons mentioned above, I do not consider scalability to be a central issue in this paper. However, upon closer examination of this aspect, fundamentally, the scalability is similar to training two single-task models. The difference lies in the additional parameters introduced by the transfer module and inv transfer module. It is not a setup that poses a significant scalability challenge to the extent that it requires a thorough analysis of computational efficiency.\n\n \nThe proposed method, in abstract, is an encoder-decoder-based method, which is kind of trivial in the transfer learning setup.\n \n- The reviewer's description is entirely different from the technical aspects of this paper. The use of an encoder-decoder is merely a small part adopted for the implementation of the proposed technique. For more detailed information, please refer to the distance loss section in the general comments.\n \n \nQuestions:\nThe distance loss is simplified as equation 12, which is basically Euclidean distance. This is contradict to the manifold setup. Can the author discussed more about the distance loss, which seems to be the major difference the author proposed compared with other existing methods.\n \n- It is not contradictory. For more detailed information, please refer to the distance loss section in the general comments.\n \n \nThe ablation study shows that the problem is an overfitting problem. So it basically means the data is not enough/ early stopping should help. \nSo the real contribution of such method is slightly not distinguishable.\n \n- First and foremost, as is in most standard setups, we applied early stopping method based on the validation loss. This is explicitly stated in Appendix E of the paper. We observed the trend of the validation loss, as shown in Fig 4 of the paper, and there is absolutely no reason not to apply early stopping when we are aware of it.\n\nFurthermore, discussing issues that cannot be addressed by early stopping is a fundamental setup in researching generalization problems. As mentioned in the general comments section regarding molecular property prediction, generalization issues are crucial in deep learning, particularly when obtaining additional data comes with significant costs.\n \nSome tiny piece:\n80-20 split of train and test, is not 4-fold cross-validation.\n \n- We performed cross-validation by dividing 80% of the training set into 4 folds, using 20% as the validation set for each fold. The statistics for the four models trained in this manner are test for the 20% of distinct test set and recorded in the paper's table.\n \n \nFigure 3 shows GATE as circle, does this mean all equal or it takes GATE as reference?\n \n- It appears that the reviewer may have overlooked Section 4.2 which is a crucial part of the paper. Similarly, it seems that the caption for Fig 3 was also overlooked. The detailed explanation of how the figure was generated is provided in the main text; therefore, additional clarification will not be provided in this comment."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030786376,
                "cdate": 1700030786376,
                "tmdate": 1700030786376,
                "mdate": 1700030786376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tg2RVsDelx",
            "forum": "3z60EWfh1p",
            "replyto": "3z60EWfh1p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_nPy4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_nPy4"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of inductive transfer learning in regression tasks. Assuming that the latent vectors of the model lie on a smooth Riemannian manifold, the paper suggests that for effective transfer learning, source and target tasks need to be mapped to regions with large overlap. The paper describes how diffeomorphisms between pairs of tasks can be learned using parametric encoders and decoders, such that individual data points are confined to a locally flat frame in the overlap region. This is a local Euclidean approximation that helps simplify distance calculation when only small perturbations are concerned.\n\nThe proposed method (GATE) adds 3 losses to the regression loss: an auto-encoder loss for the choice of diffeomorphism modeling, a consistency loss that pivots points on the overlapping region, a mapping loss that enforces predictions to be preserved through the transforms, and a distance loss that forces distances between pivot points and perturbations to be equal across tasks. The distance loss can be viewed as a regularizer.\n\nThe proposed method is evaluated on a transfer learning task for molecular property prediction and is shown to outperform alternatives in a majority of the transfer tasks considered. The overall RMSE is also significantly lower than the baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality and significance: The Riemannian view of the latent space is likely not new to this work, but two of the loss functions are novel to this work (to the best of my knowledge). The empirical improvement over the baselines in the studied task of molecular property prediction is significant.\n\nQuality and clarity: The paper is overall well motivated. The descriptions are accompanied by formulas and helpful schematic diagrams. The empirical analysis covers several aspects of the proposed solution, including ablation studies to gauge the effect of each part of the loss function."
                },
                "weaknesses": {
                    "value": "Even though the method is likely applicable and useful in other domains, the paper only studies it on the molecular property prediction task. This significantly cuts into the impact of the paper as the results cannot (in good faith) be extrapolated to a completely different domain of tasks.\n\nWith a single model architecture for molecular property prediction as the only domain, a more detailed description is missing from the main body of the paper (e.g. input/output/latent dimensions, range of values, SMILES format, DMPNN layers, etc.). The paper is not self-contained when it comes to model components.\n\nThe paper is hard to follow in parts, as the overall picture takes more than 4 pages to be completely laid out. I think an overview of all the losses can be included earlier in the paper to help with the flow and clarity of the paper.\n\nSince the heavy machinery of Riemannian geometry is not really used in the paper (apart from freedom for choice of local coordinates and the overall smoothness assumptions), IMO the elaborate dive into the nuances of dealing with geodesics in an arbitrary metric space is unnecessary, if not needlessly confusing. There is no point introducing the Christoffel symbol just to scare off a reader that might think a nice closed-form solution is possible in the generic case. I\u2019d suggest removing derivations and definitions that do not contribute to the flow of the paper."
                },
                "questions": {
                    "value": "Notes and questions:\n- The typo in equation 7 was particularly hard to resolve while reviewing this paper. The choice of notation might have contributed to this.\n- It\u2019s not clear what you mean by \"stable characteristics\" in section 5.2. Please elaborate on what makes MTL relatively unstable in this case.\n- Lower bounds are not shown in Figure 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Reviewer_nPy4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735383666,
            "cdate": 1698735383666,
            "tmdate": 1700520844334,
            "mdate": 1700520844334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z0l40wYtAI",
                "forum": "3z60EWfh1p",
                "replyto": "Tg2RVsDelx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Weaknesses:\nEven though the method ~\n \n- As mentioned in the general comments regarding molecular property prediction, such assertions are entirely unfounded. Upon reviewing ICLR 23 papers, it is evident that there are numerous papers solely focused on molecular property prediction, many of which are influential and well-regarded.\n \n \nWith a single model architecture for ~\n \n- The detailed information is all provided in Appendix E, and it appears that the reviewer might have overlooked this section. Please refer to that part for comprehensive details.\n \nThe paper is hard to follow in parts, ~\n \n- We will gladly update Fig. 2 to include an overview of all the losses. However, due to the short rebuttal period, we plan to address this in future revisions\n \nSince the heavy machinery of Riemannian geometry is ~\n \n- We strongly disagree with the reviewer's comment. As explained in the general comments regarding the distance loss, a fundamental understanding of Riemannian geometry is essential to comprehend the impact of this loss. While some readers may already possess this knowledge, many may not be familiar with it. Therefore, we have opted not to exclude this information from the main text. This section is intended to provide assistance to a broad audience, including the reviewer, in understanding our algorithm. Despite the seemingly straightforward implementation of distance loss, the underlying mathematical background and Riemannian geometry are by no means trivial. Implementing a model is one thing, but understanding why a model works is crucial. To grasp diffeomorphism accurately, one must understand coordinate transformation rules, be aware of covariance in curved space (requiring an understanding of Christoffel symbols), and even discuss Riemannian curvature for a thorough understanding of diffeomorphism invariance in Riemannian geometry. Due to the mathematical complexity, we did not delve into the details of Riemannian curvature in the main text. Therefore, the content is not intended to intimidate the reader; on the contrary, it is an omission of even more complex mathematical details underlying our algorithm. The mention of geodesics in the Conclusion section is included in the Appendix to aid in the understanding of future research directions. All these details are included to ensure the self-containedness of the paper.\n \n \nQuestions:\nNotes and questions:\n \nThe typo in equation 7 was particularly hard to ~\n \n- We have identified some missing content in certain mathematical expressions, and we are aware that there may be confusion due to the different labels between Fig 2 and Section 3.2 in the main text. To address this, we have uploaded a revised version unifying the formulas and notations in Fig 2 and Section 3.2. Additionally, we have made the overall mathematical expressions and explanations in Section 3.2 more accessible.\n\n \nIt\u2019s not clear what you mean by \"stable~\n \n- Multi-Task Learning (MTL) and transfer learning can experience negative transfer, leading to potential performance degradation, depending on the types of source and target tasks. This phenomenon is well-documented, as indicated in the paper titled 'A Survey on Negative Transfer,' published in IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in 2021. The survey discusses the following points:\n\"Multi-task learning solves multiple learning tasks jointly, by exploiting commonalities and differences across them. Similar to TL, it needs to facilitate positive transfer among tasks to improve the overall learning performance on all tasks. Previous studies have observed that conflicting gradients among different tasks may induce NT (also known as negative interference). Various techniques have been explored to remedy negative interference, such as altering the gradients directly, weighting tasks, learning task relatedness, routing networks, and searching for Pareto solutions, etc.\"\n \nFurthermore, the relationship between the alignment of the latent space and performance based on target values can be found in the paper titled 'Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules,' published in ACS Cent. Sci. 2018. In our paper, additional ablation studies were conducted, confirming that there is deformation in the latent space when negative transfer occurs. As shown in the Fig. 5, it becomes evident that the shape of the latent space in GATE remains consistently maintained according to the source task. In contrast, MTL exhibits significant deformations in the shape depending on the source task.\n \n \nLower bounds are not shown in Figure 6.\n \n- We have opted for a notation that represents the overall error in the upper part, rather than separately calculating and indicating upper and lower errors. Therefore, the lower error range is identical to the upper error range, and in the current experimental setup, we believe there is no need to distinguish between upper and lower errors."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029857364,
                "cdate": 1700029857364,
                "tmdate": 1700029857364,
                "mdate": 1700029857364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JVNZ2qdPBO",
                "forum": "3z60EWfh1p",
                "replyto": "z0l40wYtAI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Reviewer_nPy4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Reviewer_nPy4"
                ],
                "content": {
                    "title": {
                        "value": "Revisions look good"
                    },
                    "comment": {
                        "value": "The revised submission fixes several major issues discussed in the reviewers and is much easier to follow with the cleaned up notation. I'm updating the overall score accordingly.\n\n- Re use case: I still think that any claims beyond the application of method to molecular property prediction is unsupported by the evidence put forth by the paper. This particular use case is important and relevant to the community, but I would like the paper to avoid any hard claims that the method is applicable elsewhere. Any such claim should be clearly stated as an educated guess since the analysis is not covered by the paper.\n\n- Riemannian geometry discussions: Other reviewers have mirrored some of my initial discussion that the overly compressed overview of the unused mathematical machinery is not helpful with the flow of the paper and is unlikely to help readers that are not already familiar with the topic. I suggest moving unused math to the appendix so that the discussion is not compressed to the point that is not easy to follow. IMO the space in the main body of the paper can be better used to for discussions and additions suggested by the reviews, e.g. more details about the experimental setup or an expanded overview of the molecular property prediction task."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520810177,
                "cdate": 1700520810177,
                "tmdate": 1700520810177,
                "mdate": 1700520810177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wmDhDPBRwi",
            "forum": "3z60EWfh1p",
            "replyto": "3z60EWfh1p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_Gg1C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_Gg1C"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to improve multi-task learning by training separate learners for each task but transferring latent representations between them under the assumption that the latent spaces can be modeled as Riemannian manifolds.  This is accomplished by introducing encoders and decoders between the latent spaces of the source and target task, and training by use of the original loss function plus a cycle consistency loss for the encoder-decoder pair for each direction, and a distance loss based on perturbations to the input to ensure that the latent space mapping from source to target preserves the local metric.  The resulting method, GATE, is applied to a set of molecular prediction tasks, and compared to existing methods such as single-task learning, multi-task learning, transfer learning, knowledge distillation, and global structure-preserving knowledge distillation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes an interesting approach to transfer learning, and the contribution of each proposed feature (consistency loss, distance loss) is analyzed in an ablation study.  It is very interesting that the distance loss can prevent overfitting.  The choice of a dataset with 14 different tasks is suitable for a multi-task setup.  The use of two different random splits shows careful consideration of the testing setup.  The graphical check of the latent space across tasks helps builds confidence in the method.  The results are reported in detail in the appendix which allows deeper analysis by interested readers."
                },
                "weaknesses": {
                    "value": "The idea of enforcing cycle-consistency is not new, and it seems appropriate to cite related literature on cycle-consistency within transfer learning such as CyCADA (Hoffman et al. 2018).\n\nI found it difficult to follow the notation introduced to explain the method given the lack of explanation on the notations.  More details are needed to describe the method precisely.\n\nIt is not clear to me how distance loss helps prevent overfitting.  A toy example would help with the intuition.\n\nThe fact that only chemistry applications were considered limits the generality of this paper to folks working in other domains.\n\nIt is not clear how much the concepts of Riemannianian geometry actually add to the paper (what surprising results or insights depend on deep findings from differential geometry?) but instead seem to obfuscate the relatively simple and intuitive ideas which are implemented in the method GATE."
                },
                "questions": {
                    "value": "1. What surprising results or insights depend on deep findings from differential geometry?\n2. To what degree does GATE succeed at learning a cycle-consistent and metric-preserving map?\n3. What are limitations of the method?  Under what conditions would it do worse than conventional multi-task learning?\n4. What is X'' in equation (7)?\n5. What are the details for how STL, MTL, transfer learning, KD and GSP-KD were trained?\n6. Could the fact that distance loss helps prevent overfitting be related to training with adversarial examples?\n7. What happens if you only remove the cycle-consistency loss, but not the distance loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Reviewer_Gg1C"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699370644576,
            "cdate": 1699370644576,
            "tmdate": 1700065288513,
            "mdate": 1700065288513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CHKLshj4QV",
                "forum": "3z60EWfh1p",
                "replyto": "wmDhDPBRwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Weaknesses:\nThe idea of enforcing cycle-consistency ~\n \n- We acknowledge the pre-existence of the concept of cycle-consistency, and this aspect is entirely unrelated to the contributions of this paper. The contribution of this paper lies in the application of distance loss to enhance generalization, as detailed explanations can be found in the general comments section\n \n \nI found it difficult to follow the notation introduced~\n \n- We have identified some missing content in certain mathematical expressions, and we are aware that there may be confusion due to the different labels between Fig 2 and Section 3.2 in the main text. To address this, we have uploaded a revised version unifying the formulas and notations in Fig 2 and Section 3.2. Additionally, we have made the overall mathematical expressions and explanations in Section 3.2 more accessible.\n \n\nIt is not clear to me how distance loss ~\n \n- Please refer to the explanation in the general comments section regarding the distance loss.\n \n \nThe fact that only chemistry applications ~\n \n- Please refer to the explanation in the general comments section regarding the molecular property prediction.\n \n \nIt is not clear how much the concepts of Riemannianian ~\n \n- Please refer to the explanation in the general comments section regarding the distance loss.\n \n \nQuestions:\nWhat surprising results or insights depend ~\n \n- Please refer to the explanation in the general comments section regarding the distance loss.\n \n \nTo what degree does GATE succeed at ~\n \n- Upon investigation, it has been observed that the average standard deviation of the encoded vectors is around 0.07, while consistency is learned at an error level of 0.01. Additionally, the standard deviation of the reference distance, taken as a benchmark, is approximately 3.1*E-06, whereas the learned distance is at an error level of 7.6 * E-08. This confirms that training process is valid enough, and these values may vary depending on the scaling factor of the loss parameter.\n \n \nWhat are limitations of the method? Under ~\n \n- It seems challenging to pinpoint a specific scenario where the performance is notably inferior to MTL. However, it should be noted that, in comparison to MTL, the model complexity increases, and there is an increase in the number of parameters, leading to a slightly slower learning speed (in linear order).\n \n \nWhat is X'' in equation (7)?\n \n- We have identified some missing content in certain mathematical expressions, and we are aware that there may be confusion due to the different labels between Fig 2 and Section 3.2 in the main text. To address this, we have uploaded a revised version unifying the formulas and notations in Fig 2 and Section 3.2. Additionally, we have made the overall mathematical expressions and explanations in Section 3.2 more accessible.\n \n \n \nWhat are the details for how STL, MTL, ~\n \n- In Appendix E, detailed information about the network parameters, hyperparameters, and architecture are explicitly provided. As described in Appendix E.2, for a fair experiment, the base architecture was consistently employed across all methods. Additionally, differences in training details among these methods have been carefully documented. We feel that all necessary elements, such as epochs, learning rate, etc., are already included in the paper. If there are specific additions deemed necessary, please specify, and we will refer to it for revision\n \n \nCould the fact that distance loss helps prevent ~\n\n- The mapping loss is computed only for input points, and there is no downstream task for neighboring points. Therefore, adversarial examples do not exist in the training setup. For detailed information on the distance loss, please refer to the general comments. As we are confused about what adversarial expamples are in this context, it may be worth mentioning that, similar to the response to another reviewer's question, distance loss has no relevance to contrastive learning.\n\n \nWhat happens if you only remove the ~\n \n- We conducted experiments by removing only the consistency loss. In this case, information about the input point is absent in the distance loss, it is impossible to pin point the reference point to align the latent spaces. Therefore, theoretically, aligning the source and target latent spaces is not possible, and as a result, we expected a performance degradation. Indeed, during the actual experiment, as anticipated, overfitting occurred right after 50 epochs of training, leading to a decline in performance from around 0.2 MSE to over 0.4, which is a similar scale in the mapping only case. Since this case is theoretically trivial enough, we choose not to include it in the manuscript. However, since the comment system in openreview seems to not accept any images, we added the experiment graph to the supplementary materials."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028933478,
                "cdate": 1700028933478,
                "tmdate": 1700052007611,
                "mdate": 1700052007611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OLwPj1pFQV",
                "forum": "3z60EWfh1p",
                "replyto": "wmDhDPBRwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Reviewer_Gg1C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Reviewer_Gg1C"
                ],
                "content": {
                    "title": {
                        "value": "Revision much clearer"
                    },
                    "comment": {
                        "value": "Thanks for straightening out the notation in the paper.  Actually being able to understand the method deepens my appreciation of it.  I am happy to increase my presentation rating from Poor to Fair and raise my score from 5 to 8.\n\nAs both nPy4 and myself think that the inclusion of detailed discussions of Riemannian geometry are helpful to this paper, I will respond to your comment where you state \"As explained in the general comments regarding the distance loss, a fundamental understanding of Riemannian geometry is essential to comprehend the impact of this loss.\"  It may be true that exposure to Riemannian geometry gives deep insight to the method, but a practitioner does not need formal mathematical training to appreciate that the distance loss helps constrain the mapping in a way that prevents overfitting.  Also, it is worth considering the practicality of attempting to give a crash course on differential geometry within a few pages.  Assuming the possibility of a reader who lacks exposure to differential geometry but is interested in learning more about the concepts that led to the paper, such would be better served by looking up a good tutorial rather than trying to decipher the highly compressed overview given in the paper.  Hence, it would be far better to direct readers to such material rather than to overload the main paper with background material which is unlikely to be of much use to either the beginner (who cannot understand it) nor the expert (for whom it is redundant.)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065253882,
                "cdate": 1700065253882,
                "tmdate": 1700065316885,
                "mdate": 1700065316885,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vxeHv6ZTCz",
            "forum": "3z60EWfh1p",
            "replyto": "3z60EWfh1p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_6Cr1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4338/Reviewer_6Cr1"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a novel formulation of regression transfer learning by embedding the latent space in a Riemannian manifold, allowing the notion of consistency across tasks and the mapping of different points together."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novel regularization procedure which also has the potential of being used outside of the scope of this paper.\n- Superior performance when compared to other methods for transfer learning.\n- Intuitive idea and easy to implement.\n- Good experimental section, with a nice exploration of overfitting."
                },
                "weaknesses": {
                    "value": "- The writing quality needs to be improved, there are both distracting grammar issues and, more importantly, the mathematical formulation of the method and description of the prerequisites for understanding this work have not been adequately presented.\n- Section 5.2 is not well-supported, specifically the assertion \"Ideally, if a model is well-guided by the right information and regularized properly, the overall geometry of the latent space may remain stable and not depend on the type of source tasks. However, if the target task is overwhelmed by the source task and regularization is not enough, latent space will be heavily deformed according to the source tasks\" which needs more detail, or a few corroborating citations."
                },
                "questions": {
                    "value": "- In Section 5.3, it seems that using the word \"significant\" when comparing the GATE and MTL results is not precise enough. Can the authors add a statistical test? \n- In the same section, I would also be interested in re-running this experiment with higher deviations, both of the same sign as the original data, just made more extreme (a data point that is 2 sigma away from the mean is made to be 10 sigma away) and the opposite sign (2 sigma more than the mean is changed to 10 sigma less than the mean). Is there a point at which MTL and GATE have more similar performance, or does the gap increase?\n- How is this approach connected to contrastive learning and metric learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4338/Reviewer_6Cr1",
                        "ICLR.cc/2024/Conference/Submission4338/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699543512980,
            "cdate": 1699543512980,
            "tmdate": 1700312794804,
            "mdate": 1700312794804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mus4FhPa1F",
                "forum": "3z60EWfh1p",
                "replyto": "vxeHv6ZTCz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Weaknesses:\nThe writing quality needs to be improved,~\n \n- We have identified some missing content in certain mathematical expressions, and we are aware that there may be confusion due to the different labels between Fig 2 and Section 3.2 in the main text. To address this, we have uploaded a revised version unifying the formulas and notations in Fig 2 and Section 3.2. Additionally, we have made the overall mathematical expressions and explanations in Section 3.2 more accessible.\n\n \nSection 5.2 is not well-supported, specifically the assertion ~\n \n- It is known that MTL or transfer learning may experience negative transfer depending on the types of source and target tasks, leading to performance degradation. The paper titled \"A Survey on Negative Transfer,\" published in IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in 2021, provides insights into this phenomenon. According to the survey, \n\n\"Multi-task learning solves multiple learning tasks jointly, by exploiting commonalities and differences across them. Similar to TL, it needs to facilitate positive transfer among tasks to improve the overall learning performance on all tasks. Previous studies have observed that conflicting gradients among different tasks may induce NT (also known as negative interference). Various techniques have been explored to remedy negative interference, such as altering the gradients directly, weighting tasks, learning task relatedness, routing networks, and searching for Pareto solutions, etc.\"\n\nconflicting gradients among different tasks may induce negative transfer, and various techniques have been explored to address this issue. \n\nAdditionally, the relationship between the alignment of the latent space based on target values and performance can be found in \"Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules,\" published in ACS Cent. Sci. 2018. We have added references to these findings in the revised manuscript.\n \n \nQuestions:\nIn Section 5.3, it seems that using the word ~\n \n- We performed a significance analysis based on p-values for the entire set of 23 tasks in the experiment, comparing the performance difference between the GSP-KD method (2nd best performance) and GATE. In random split, GATE showed significant improvement in 11 tasks with one star or more, while GSP-KD outperformed in 3 tasks. For three stars or more, GATE excelled in 11 tasks, and GSP-KD excelled in 1 task. In scaffold split, GATE showed significant improvement in 9 tasks with one star or more, while GSP-KD outperformed in 2 tasks. For three stars or more, GATE excelled in 7 tasks, and GSP-KD showed no instances of significant improvement. Overall, as there are numerous tasks with three stars or more, indicating a substantial difference, we have chosen not to make specific modifications regarding significance in the manuscript.\n \n \nIn the same section, I would also be interested in ~\n \n- Through experimentation, we increased the corruption level to 10 sigma, and for heat of vaporization, GATE exhibited a 11.5% better performance than MTL, which is similar to the 11.7% improvement shown in Fig 6. For collision cross section, at 10 sigma, GATE showed approximately 6.3% better performance than MTL, slightly decreasing from the 10.3% at 2 sigma. While we could not thoroughly investigate due to time constraints, even in extreme corruption scenarios, GATE demonstrated better performance compared to MTL.\n\n\nHow is this approach connected to contrastive learning and metric learning?\n \n- There is no connection between the proposed GATE algorithm and contrastive learning or metric learning. Contrastive learning deals with the distance between latent vectors from two different models, whereas the proposed method aims to align the shape of the latent space by comparing the distances between neighboring points and input points for each source and target model. Therefore, there is no common concept between these methodologies. Further details have been provided in the general comments section."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028872459,
                "cdate": 1700028872459,
                "tmdate": 1700028872459,
                "mdate": 1700028872459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jygBHVwmYN",
                "forum": "3z60EWfh1p",
                "replyto": "mus4FhPa1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4338/Reviewer_6Cr1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4338/Reviewer_6Cr1"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "The paper is now clearer and includes statistical analysis for the experiments done. I have thus raised my score from 5 to 6 and think that it should be accepted."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700312776702,
                "cdate": 1700312776702,
                "tmdate": 1700312776702,
                "mdate": 1700312776702,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]