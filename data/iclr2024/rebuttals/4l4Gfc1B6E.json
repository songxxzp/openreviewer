[
    {
        "title": "Offline Robustness of Distributional Actor-Critic Ensemble Reinforcement Learning"
    },
    {
        "review": {
            "id": "tjL9HNbPdV",
            "forum": "4l4Gfc1B6E",
            "replyto": "4l4Gfc1B6E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission86/Reviewer_kZn3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission86/Reviewer_kZn3"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to solve the offline RL problem considering a possibly slightly perturbed environment (e.g., the state observation is noisy). It proposed a new algorithm called ORDER and evaluated its theoretical and empirical performance success in D4RL benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic of this work is of great interest. The goal is not to consider the performance of a clean environment but can be a slightly perturbed environment w.r.t. state, dynamics, and so on.\n2. The background and preliminary is introduced clearly."
                },
                "weaknesses": {
                    "value": "1. The problem formulation of this work is not introduced clearly. It seems there is a lack of introduction or mathematical formulation of what exactly the offline robust RL that this paper targets. The definition of robust RL [1-3] usually refers to the distributionally robust RL against model (transition) perturbation. While the one that this paper considers seems more related to state-adversarial robust RL [4]. The terminology and the real goal of this paper need to be made more clear and explicit.\n2. There are five components in the proposed ORDER algorithm, which is also introduced in detail. However, it is hard to see which part is new in ORDER and which dominates the performance in the later experiments, without ablation study of different components as well.\n3. The experimental results seem not sufficient since this work only compared the performance of OEDER to that of some basic offline RL algorithms (e.g., CQL, BEAR), while a lot of advanced ones have not been considered, such as IQL.\n\n\n[1] Moos, Janosch, et al. \"Robust reinforcement learning: A review of foundations and recent advances.\" Machine Learning and Knowledge Extraction 4.1 (2022): 276-315.\n[2] Zhou, Z., Bai, Q., Zhou, Z., Qiu, L., Blanchet, J., and Glynn, P. (2021). Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 3331\u20133339. PMLR.\n[3] Shi, Laixi, et al. \"The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model.\" arXiv preprint arXiv:2305.16589 (2023).\n\n[4] Zhang, Huan, et al. \"Robust deep reinforcement learning against adversarial perturbations on state observations.\" Advances in Neural Information Processing Systems 33 (2020): 21024-21037."
                },
                "questions": {
                    "value": "1. What is the average/overall performance of ORDER compared to baselines, since there is only per-task performance in Table 1.\n2. There are too many details of the experiments in Section 5.2, which may be better to leave to the appendix and leave space for a more comprehensive analysis/evaluation of the proposed method ORDER.\n3. The setting of the experiments seems not clear to the reviewer. As the goal of this work is to handle possible state perturbation, should the experiments be conducted in some settings where the state is perturbed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission86/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698626891536,
            "cdate": 1698626891536,
            "tmdate": 1699635933373,
            "mdate": 1699635933373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZYCo0mn7XA",
                "forum": "4l4Gfc1B6E",
                "replyto": "tjL9HNbPdV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer kZn3 1"
                    },
                    "comment": {
                        "value": "*``What is the average/overall performance of ORDER compared to baselines, since there is only per-task performance in Table 1. \u201d*\n\n**Answer.** Thanks for your suggestion. We have added the average performance of ORDER in Table 1 in the new version. Meanwhile, the average performance of ORDER surpasses most basic offline RL algorithms.\n\n*``There are too many details of the experiments in Section 5.2, which may be better to leave to the appendix and leave space for a more comprehensive analysis/evaluation of the proposed method ORDER. \u201d*\n\n\n**Answer.** Thank you for pointing this issue. We will add the results of ORDER's experiments in more environments in Section 5.2 for better analysis. \n\n*``The setting of the experiments seems not clear to the reviewer. As the goal of this work is to handle possible state perturbation, should the experiments be conducted in some settings where the state is perturbed? \u201d*\n\n**Answer.** Thanks for your valuable suggestion. In further work, we will supplement the ablation study of ORDER performance with varying perturbation scale."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission86/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617490123,
                "cdate": 1700617490123,
                "tmdate": 1700617490123,
                "mdate": 1700617490123,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ovpDSbEnji",
            "forum": "4l4Gfc1B6E",
            "replyto": "4l4Gfc1B6E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission86/Reviewer_fvaB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission86/Reviewer_fvaB"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an offline RL algorithm ORDER that leverages distributional RL to learn a policy robust to the distribution shift. Besides the conventional distributional RL loss, the proposed method adopts an ensemble of quantile networks, introduces smoothness regularizers for both policy and value functions, and penalizes the value of OOD actions. The authors further conduct experiments on D4RL to compare the proposed methods with baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper empirically evaluates the proposed method on 12 tasks from the Gym-MuJoCo domain of the D4RL benchmark.\n\n2. This paper provides some theoretical analysis of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The proposed method lacks enough novelty. First, as mentioned by the authors, the techniques of distributional RL have already been introduced into the offline RL settings. Second, adopting an ensemble of value networks is a widely known technique that can improve performance in the RL community. Third, adding smoothness constraints in the offline RL settings has already been explored in the offline RL settings [1].\n\n2. The ablation studies are weak. I suggest the authors investigate how different regularizing coefficients (e.g., $\\alpha$, $\\beta$) impact the final performance.\n\n3. The authors claim existing distributional offline RL algorithms \"leverage a conservative return distribution to impair the robustness, and will make policies highly sensitive\" without providing enough empirical support. To me, I cannot see why leveraging a conservative return distribution can impair the robustness of the learned policy.\n\n4. The authors miss a related work [2], which also leverages distributional RL to learn value functions. I suggest the authors include [2] as another baseline method in Table 1.\n\n[1] Sinha et al., S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning in Robotics. 5th Annual Conference on Robot Learning (CoRL)\n\n[2] Li et al., Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. ICML 2023."
                },
                "questions": {
                    "value": "See the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission86/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission86/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission86/Reviewer_fvaB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission86/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728721419,
            "cdate": 1698728721419,
            "tmdate": 1699635933277,
            "mdate": 1699635933277,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qujWNxf2lK",
                "forum": "4l4Gfc1B6E",
                "replyto": "ovpDSbEnji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer fvaB 1"
                    },
                    "comment": {
                        "value": "*``The proposed method lacks enough novelty. First, as mentioned by the authors, the techniques of distributional RL have already been introduced into the offline RL settings. Second, adopting an ensemble of value networks is a widely known technique that can improve performance in the RL community. Third, adding smoothness constraints in the offline RL settings has already been explored in the offline RL settings. \u201d*\n\n**Answer.** First, distributional RL has been applied to offline RL [1,2]. However, it is well known that distributional RL is widely used in online setting while its academic exploration is still incomplete in offline setting. Existing distributional offline RL methods only focus on the safety of the learned policy. These methods leverage a conservative return distribution to impair the robustness. In contrast, our work considers improving the robustness of the learned policies under control of the distribution shift.\n\nSecond, adopting an ensemble of value networks is a widely known technique that can improve performance in the RL community. On the one hand, double $Q$ networks are often used in RL algorithms to alleviate the overestimation of $Q$ functions. On the other hand, the number of distributed integrated networks has been explored in online setting [3]. However, offline RL is different from online reinforcement learning in that it has some additional problems, such as distribution shift and OOD problems. Many scholars' hasty use of the double network technique has hindered the exploration of the number of value networks in ensemble in the field of offline RL.\n\nThird, it is known that adding smoothness constraints to offline RL has been explored in the offline settings, such as S4RL [4] and RORL [5]. However, some previous related work enforced smoothness in $Q$ functions, and the smoothing treatment in our work applied to distribution action value functions. We can use the framework of distributional RL to consider risk-sensitive learning and the performance of the algorithm under different risk measures, so as to ensure the security of learned polices. Besides, when the $Q$ value is updated, the loss function takes the form of mean square error. Huber loss is adopted in distributional RL, which can better fit our research. Therefore, the problem we studied is an interesting and meaningful work.\n\n[1]Will Dabney, et al., Implicit quantile networks for distributional reinforcement learning. In International Conference on Machine Learning, pp. 1096-1105, 2018a.\n\n[2] Xiaoteng Ma, et al., DSAC: Distributional soft actor critic for risk-sensitive reinforcement learning. arXiv preprint arXiv:2004.14547, 2020.\n\n[3] Lan, Q., et al., Maxmin q-learning: Controlling the estimation bias of q-learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n[4] Sinha et al., S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning in Robotics. 5th Annual Conference on Robot Learning (CoRL).\n\n[5] Rui Yang, et al., RORL: Robust\noffline reinforcement learning via conservative smoothing. In Advances in Neural Information\nProcessing Systems, volume 35, pp. 23851\u201323866, 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission86/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617210948,
                "cdate": 1700617210948,
                "tmdate": 1700617210948,
                "mdate": 1700617210948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z9f3cUsRuX",
            "forum": "4l4Gfc1B6E",
            "replyto": "4l4Gfc1B6E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission86/Reviewer_1eAG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission86/Reviewer_1eAG"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the realm of offline robust reinforcement learning, introducing the Offline Robustness of Distributional Ensemble Reinforcement Learning (ORDER) as its primary contribution. ORDER aims to strike a balance between conservatism and robustness in offline settings, utilizing an ensemble of multiple quantile networks to enhance its resilience. Notably, it incorporates a smoothing technique for policies and distributional functions, primarily focusing on perturbed states. The paper reinforces its claims with a theoretical proof of ORDER's convergence to a conservative lower bound, substantiated through empirical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The utilization of an ensemble of multiple quantile networks holds promise in bolstering the approach's robustness.\n\n- The inclusion of a theoretical proof demonstrating convergence to a conservative lower bound adds weight to ORDER's efficacy in enhancing robustness."
                },
                "weaknesses": {
                    "value": "- While the paper introduces ORDER as a modification of the prior work RORL into a distributional version, it is essential to engage in a comprehensive discussion comparing this work with RORL to underscore its significance and contributions more effectively.\n\n- The experiments presented in Table 1 do not convincingly showcase the superiority of ORDER over the previous state-of-the-art offline RL algorithm, RORL. This raises concerns about the relevance and significance of the studied problem: offline distributional RL.\n\n- The ablation experiments, considering the number of introduced components, are rather limited. For a more comprehensive understanding, it would be valuable to explore the individual impacts of components like the policy smooth loss, OOD penalty, and state perturbation on the proposed algorithm's performance.\n\nIn summary, it is crucial for this work to accentuate the importance of the problem it addresses and the uniqueness of the proposed method. In the realm of sophisticated algorithms, significant advantages are generally expected. Additionally, a more comprehensive set of ablation experiments would further strengthen the paper."
                },
                "questions": {
                    "value": "Most questions are listed in the weakness part. There are also some typos and notations need to be defined:\n\n- The parentheses in the definition of $Q^\\pi$ are incorrectly positioned.\n\n- In the definition of $\\hat{\\pi}$: Define and unify the notation of the indicator function $\\mathbf{1}$.\n\n- Define the notation $\\mathcal{P}(\\mathbb{R})$ in Section 2.2. \n\n- Define the notation $\\mathbb{B}_d(s, \\epsilon)$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission86/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767412378,
            "cdate": 1698767412378,
            "tmdate": 1699635933189,
            "mdate": 1699635933189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g2vsAuIE9i",
                "forum": "4l4Gfc1B6E",
                "replyto": "Z9f3cUsRuX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 1eAG 1"
                    },
                    "comment": {
                        "value": "*`` There are also some typos and notations need to be defined: \u201d*\n\n*``The parentheses in the definition of $Q^{\\pi}$ are incorrectly positioned.  \u201d*\n\n*``In the definition of $\\hat{\\pi}$: Define and unify the notation of the indicator function ${1}$. \u201d*\n\n*``Define the notation $\\mathcal{P}(\\mathbb{R})$ in Section 2.2. \u201d*\n\n*``Define the notation $\\mathbb{B}_{d}(s,\\epsilon)$. \u201d*\n\n**Answer.** Thanks for your suggestion. We have added the definitions of the indicator function ${1}$ and the notation $\\mathcal{P}(\\mathbb{R})$. We also have modified the pasentheses in the definition of $Q^{\\pi}$ and the inconsistency of the indicator function ${1}$. In fact, the definition of $\\mathbb{B}_{d}(s,\\epsilon)$ has already been given in Section 3.1. \n\n*``While the paper introduces ORDER as a modification of the prior work RORL into a distributional version, it is essential to engage in a comprehensive discussion comparing this work with RORL to underscore its significance and contributions more effectively. \u201d*\n\n*``The experiments presented in Table 1 do not convincingly showcase the superiority of ORDER over the previous state-of-the-art offline RL algorithm, RORL. This raises concerns about the relevance and significance of the studied problem: offline distributional RL. \u201d* \n\n*``The ablation experiments, considering the number of introduced components, are rather limited. For a more comprehensive understanding, it would be valuable to explore the individual impacts of components like the policy smooth loss, OOD penalty, and state perturbation on the proposed algorithm's performance. \u201d*\n\n**Answer.** Thank you for pointing this out. You may have some misconceptions about what we do. In this paper, we introduce smoothing techniques from RORL into CODAC to enhance the robustness of offline distributional reinforcement learning. In our experiments, the performance of ORDER surpasses offline distributional reinforcement learning algorithms, such as CODAC, MQN-CQR. Meanwhile, ORDER also outperformed RORL results on datasets such as hopper-medium-replay and walker2d-random."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission86/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618314779,
                "cdate": 1700618314779,
                "tmdate": 1700618314779,
                "mdate": 1700618314779,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8aJCSXBMrl",
            "forum": "4l4Gfc1B6E",
            "replyto": "4l4Gfc1B6E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission86/Reviewer_cjFe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission86/Reviewer_cjFe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the algorithm ORDER (Offline Robustness of Distributional actor-critic Ensemble Reinforcement learning) to improve the robustness of policies in offline reinforcement learning (RL) settings. ORDER introduces two approaches to enhance robustness: i) smoothing technique to policies and distribution functions for states near the dataset, and ii) strengthening the quantile network. The algorithm incorporates a dynamic entropy regularizer of the quantile function to ensure sufficient exploration and controls the distribution shift. The paper theoretically proves that ORDER converges to a conservative lower bound, which helps alleviate the distribution shift. Experimental validation on the D4RL benchmark demonstrates the effectiveness of ORDER in improving policy robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces the algorithm ORDER, which addresses the challenges of distribution shift and robustness in offline RL settings.\n- The paper provides theoretical proofs of the convergence of ORDER to a conservative lower bound, which helps alleviate the distribution shift.\n- The paper clearly presents the algorithm ORDER and its components, including the smoothing technique, strengthening the quantile network, and dynamic entropy regularizer."
                },
                "weaknesses": {
                    "value": "- The paper lacks a thorough analysis of the computational complexity and scalability of the ORDER algorithm, which could be important considerations for real-world applications. As the distributional RL needs more computational resources.\n\n- The experimental validation of ORDER is limited to the D4RL benchmark, and it would be beneficial to evaluate the algorithm on a wider range of tasks and datasets to demonstrate its generalizability.\n\n- RORL seems better than ORDER."
                },
                "questions": {
                    "value": "- Could the authors provide insights into the computational complexity and scalability of the ORDER algorithm?\n- It would be beneficial to evaluate the ORDER algorithm on a wider range of tasks and datasets beyond the D4RL benchmark. This would demonstrate the generalizability of ORDER and its effectiveness in different domains."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission86/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841293669,
            "cdate": 1698841293669,
            "tmdate": 1699635933124,
            "mdate": 1699635933124,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kroE6s3aif",
                "forum": "4l4Gfc1B6E",
                "replyto": "8aJCSXBMrl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission86/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer cjFe 1"
                    },
                    "comment": {
                        "value": "*``Could the authors provide insights into the computational complexity and scalability of the ORDER algorithm? \u201d*\n\n**Answer.** On the one hand, for some offline reinforcement learning algorithms, such as RORL [1], our algorithm is to model quantile networks of action values which will introduce more network parameters. On the other hand, for offline distributional reinforcement learning algorithms such as CODAC [2], we add a smooth term loss. Although these will increase the computational complexity to some extent, the performance of ORDER is much higher than the current advanced offline distributional reinforcement learning algorithms, such as CODAC, MQN-CQR. Meanwhile, ORDER also outperforms RORL results on datasets such as hopper-medium-replay and walker2d-random.\n\n*``It would be beneficial to evaluate the ORDER algorithm on a wider range of tasks and datasets beyond the D4RL benchmark. This would demonstrate the generalizability of ORDER and its effectiveness in different domains. \u201d*\n\n**Answer.** Thanks for your comment. The wider application of our algorithm in real-world scenarios will be a future work. \n\n[1] Rui Yang, et al., RORL: Robust offline reinforcement learning via conservative smoothing. In Advances in Neural Information Processing Systems, volume 35, pp. 23851-23866, 2022.\n\n[2] Yecheng Ma, et al., Conservative offline distributional reinforcement learning. In Advances in Neural Information Processing Systems, volume 34, pp. 19235-19247, 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission86/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616354311,
                "cdate": 1700616354311,
                "tmdate": 1700616354311,
                "mdate": 1700616354311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]