[
    {
        "title": "Multi-modality Adversarial Attacks on Latent Diffusion Models"
    },
    {
        "review": {
            "id": "t78ryboHy0",
            "forum": "QeX0YFt4iW",
            "replyto": "QeX0YFt4iW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_ufds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_ufds"
            ],
            "content": {
                "summary": {
                    "value": "The proposed MMA is a multimodal adversarial attack against img2img latent diffusion models in a white-box and black-box setting. which searches for the most optimal adversarial image and adversarial text iteratively. Compared with the single-modality attacks, MMA can generate adversarial samples with smaller perturbation magnitudes, and mislead the target models successfully."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written and easy to follow. \n2. It's the first study of multimodal attack on latent diffusion models, which demonstrates the advantage of multimodal attacks over single modality attacks on img2img tasks.\n3. The evaluation metrics in the paper are from two perspectives: imperceptibility and effectiveness, which are better for evaluating the performance of the baselines and MMA."
                },
                "weaknesses": {
                    "value": "1. Related missing references:\n[1] Diffusion Models for Imperceptible and Transferable Adversarial Attack\n[2] Towards Adversarial Attack on Vision-Language Pre-training Models\n2. In the transfer-based black-box setting, there is a lack of comparison with image transfer-based attacks such as TI-FGSM, SINIFGSM, and ... in experiments\n3. There is no error analysis. When does the method fail and why? What are the limitations of this method?"
                },
                "questions": {
                    "value": "1. what's the difference between the Image and MMA w/o Image, and Text and MMA w/o Text in experiments, please describe in detail.\n2. Co-attack is a multimodal adversarial attack designed for vision-language pretraining models, integrating both BERT-attack and PGD methods. Would it be feasible to deploy Co-attack against the LDMs discussed in the paper and subsequently contrast its efficacy with the proposed MMA?\nCo-attack: Towards Adversarial Attack on Vision-Language Pre-training Models\n3. In the transfer-based attack, can the methods, such as TI-FGSM and DI-FGSM be used to attack the target LDMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6240/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6240/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6240/Reviewer_ufds"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698326737662,
            "cdate": 1698326737662,
            "tmdate": 1699636682382,
            "mdate": 1699636682382,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E8RQL4tGww",
                "forum": "QeX0YFt4iW",
                "replyto": "t78ryboHy0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Related work.\n\nWe focus on attacking diffusion models. In contrast, [1] uses diffusion models to generate adversarial examples against image classifiers, and [2] focuses on attacking vision-language pre-training models. We have added the discussion in our paper.\n\nQ2: Transfer attacks.\n\nWe compare our method with two traditional transfer-based attacks, DIM [3] and SIM [4]. Our approach can outperform the baselines.\n\nWhite-box on SD2-1\n\n| Methods | $L_2$ | $Sim_{img}$ | PSNR | SSIM | MSSSIM | CLIP |FID | IS |\n| :------: | :-----: | :-----: | :-----: | :-----: | :-----: | :-------: | :----: | :----: |\n| DIM | 32.16 | 0.37 | 10.79 |0.160 |0.302 |31.97 | 196.62 |13.82 |\n| SIM | 33.65 | 0.35 | 10.64 |0.153 |0.294 |31.52 | 197.66 |13.67 |\n| Ours | **16.51** | **0.56** |**10.14** |**0.123** |**0.263** |**28.65** | **205.06** |**13.38** |\n\nBlack-box on SD1-5\n\n| Methods | PSNR | SSIM | MSSSIM | CLIP |FID | IS |\n| :------: |  :-----: | :-----: | :-----: | :-------: | :----: | :----: |\n| DIM | 12.82 |0.250 |0.417 |33.86 | 176.77 |**16.70** |\n| SIM | 12.67 |0.233 |0.405 |33.67 | 177.82 |16.24 |\n| Ours | **11.26** |**0.181** |**0.343** |**30.13** | **189.27** |16.31 |\n\n\nQ3: Difference of baselines.\n\nAs we introduce in Section 5.1, the four baselines are single-modality attacks in our MMA framework. Suppose we update the adversarial pairs for $T$ iterations by our MMA. We perturb images for $T_1$ iterations and text for $T_2$ iterations. Therefore, $T = T_1 + T_2$. \u201cImage\u201d and \u201cText\u201d run the MMA framework only on one modality. \u201cImage\u201d only perturbs the input image for $T$ iterations by the MMA framework without changing the input text. Similarly, \u201cText\u201d only perturbs the input text for $T$ iterations by the MMA framework without changing the input image. \u201cMMA w/o Image\u201d and \u201cMMA w/o Text\u201d mean that we run MMA on both modalities, but we only use the perturbations from one modality. Specifically, \u201cMMA w/o Text\u201d means we only perturb the input image by the generated image perturbation of MMA, which is to perturb the image for $T_1$ iterations. \u201cMMA w/o Image\u201d means we only perturb the input text by the generated text perturbation of MMA, which is to perturb the text for $T_2$ iterations.\n\nQ4: Co-attack.\n\nWe compare our method with Co-attack. We can see that our approach achieves better performance.\n\n| Methods | $L_2$ | $Sim_{img}$ | $Lev$ | $Sim_{text}$ | PSNR | SSIM | MSSSIM | CLIP |FID | IS |\n| :------: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-------: | :----: | :----: |\n| Co-Attack | 23.62 | 0.42 | 3.86 | 0.77| 10.44 |0.136 |0.284 |29.32 | 202.27 |13.51 |\n| Ours | **16.51** | **0.56** | **3.78** | **0.80** |**10.14** |**0.123** |**0.263** |**28.65** | **205.06** |**13.38** |\n\nQ5: Transfer setting.\n\nWe can incorporate DIM and SIM into our method to improve adversarial transferability. We find that the adversarial transferability is improved.\n\nBlack-box on SD1-5\n\n| Methods | PSNR | SSIM | MSSSIM | CLIP |FID | IS |\n| :------: |  :-----: | :-----: | :-----: | :-------: | :----: | :----: |\n| Ours | 11.26 |0.181 |0.343 |30.13 | 189.27 |16.31 |\n| Ours + DIM | 11.18 |0.172 |0.327 |29.97 | 189.76 |16.92 |\n| Ours + SIM | **11.12** |**0.168** |**0.322** |**29.93** | **189.92** |**17.13** |\n\n\n[1] Chen J, Chen H, Chen K, et al. Diffusion Models for Imperceptible and Transferable Adversarial Attack[J]. arXiv preprint arXiv:2305.08192, 2023.\n\n[2] Zhang J, Yi Q, Sang J. Towards adversarial attack on vision-language pre-training models[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 5005-5013.\n\n[3] Xie C, Zhang Z, Zhou Y, et al. Improving transferability of adversarial examples with input diversity[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 2730-2739.\n\n[4] Lin J, Song C, He K, et al. Nesterov accelerated gradient and scale invariance for adversarial attacks[J]. arXiv preprint arXiv:1908.06281, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452467725,
                "cdate": 1700452467725,
                "tmdate": 1700452467725,
                "mdate": 1700452467725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "weSCw1Ai0g",
                "forum": "QeX0YFt4iW",
                "replyto": "t78ryboHy0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your valuable reviews. Since the discussion phase is going to finish in one day, we look forward to your further feedback about our latest response. Do you still have any concerns about our paper after the rebuttal? Do we address your questions? We are happy to discuss with you in more detail. We greatly appreciate your time and feedback.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659417892,
                "cdate": 1700659417892,
                "tmdate": 1700659417892,
                "mdate": 1700659417892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wu4RjOe7uH",
            "forum": "QeX0YFt4iW",
            "replyto": "QeX0YFt4iW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_gcjf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_gcjf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-modality adversarial attack method called MMA for latent diffusion models, which perturbs both the image and text inputs simultaneously to generate more effective and imperceptible adversarial examples compared to single-modality attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The methods are intuitive and easy to implement."
                },
                "weaknesses": {
                    "value": "1. The method in the paper is not particularly innovative and is overly simplistic. The approach, especially the text search mechanism, seems rather basic and lacks novelty. No theoretical analysis is provided on the distortion induced by multi-modality attacks compared to single modality.\n\n2. The paper's primary method appears constrained to untargeted attacks. This limitation curtails its adaptability and potentially its effectiveness in targeted adversarial scenarios.\n\n3. The human evaluation is limited to using CLIP similarity as a proxy. Direct human studies evaluating imperceptibility would be more convincing.\n\n4. The efficiency and computational overhead of the method are not analyzed or compared to baselines."
                },
                "questions": {
                    "value": "See the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675719115,
            "cdate": 1698675719115,
            "tmdate": 1699636682254,
            "mdate": 1699636682254,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TG1jKR9UUs",
                "forum": "QeX0YFt4iW",
                "replyto": "wu4RjOe7uH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review!\n\nQ1: Theoretical analysis.\n\nCompared with single-modality attacks, multi-modality attacks have a lower upper bound of the perturbation. The proof is as follows.\n\nWe suppose the perturbation of each step is $(\\delta^j_i, \\delta^j_t)$ for multi-modality adversarial attacks, where $\\delta^j_i$ and $\\delta^j_t$ are the perturbation on the image and text respectively. \n\nIn our MMA approach, we only update image or text in each step, so $\\delta^j_i = \\textbf{0}$ or $\\delta^j_t = \\textbf{0}$. \n\nWithout loss of generalization, we focus on the perturbation on the image. Thus, the total perturbation on the image $\\Delta_i = \\sum_{j}^{T} \\delta^j_i$, where $T$ is the total iteration. \n\nThen the $L_2$ norm of the total image perturbation holds the inequality $|| \\Delta_i ||_2 = || \\delta^1_i + \\cdot + \\delta^T_i||_2 \\leq  || \\delta^1_i||_2 + \\cdot + || \\delta^T_i||_2$. For each perturbation $|| \\delta^j_i||_2$, we have the constraint that $|| \\delta^j_i||_2 = \\alpha$, where is the step length for each iteration. \n\nAs a result, the $|| \\Delta_i ||_2 \\leq \\alpha * M$, where $M \\leq T$ is the iteration times to update image. \n\nHowever, for single-modality attack, the total image perturbation $|| \\Delta_i ||_2 \\leq \\alpha * T$. \n\nAll in all, multi-modality attacks have a lower upper bound of the perturbation unless the multi-modality attack is degenerated to single-modality attack.\n\nQ2: Targeted attack.\n\nOur approach can be adapted to the targeted adversarial scenario. Specifically, we minimize the distance between the adversarial input pair and the target image-prompt pair in the latent space of the cross-attention module. The results are as follows. Our attack is still effective and imperceptible in targeted adversarial scenarios.\n\n| Methods | $L_2$ | $Sim_{img}$ | $Lev$ | $Sim_{text}$ | PSNR | SSIM | MSSSIM | CLIP |FID | IS |\n| :------: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-------: | :----: | :----: |\n| LDMR| 29.14 | 0.38 | - | - | 9.66 |0.232 |0.425 |32.22 | 292.53 |14.87 |\n| QF| - | - | 9.95 | **0.83**| 10.16 |0.268 |0.484 |32.56 | 189.61 |16.73 |\n| Ours | **16.72** | **0.53** | **3.74** | 0.81 |**8.62** |**0.214** |**0.403** |**31.92** | **203.72** |**14.63** |\n\nQ3: User study. \n\nPlease see Q3 of Reviewer FHsv.\n\nQ4: Efficiency.\n\nWe compare our time efficiency with baselines. We compute the average time to generate one adversarial example. MMA, LDMR, and QF use 53.2, 49.2, and 44.7 seconds, respectively. Although our time efficiency is similar to the previous works, we can achieve better imperceptibility and attacking performance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452416619,
                "cdate": 1700452416619,
                "tmdate": 1700452416619,
                "mdate": 1700452416619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjOr3KErUF",
                "forum": "QeX0YFt4iW",
                "replyto": "wu4RjOe7uH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your valuable reviews. Since the discussion phase is going to finish in one day, we look forward to your further feedback about our latest response. Do you still have any concerns about our paper after the rebuttal? Do we address your questions? We are happy to discuss with you in more detail. We greatly appreciate your time and feedback.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659409873,
                "cdate": 1700659409873,
                "tmdate": 1700659409873,
                "mdate": 1700659409873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cA4BjWxBP9",
            "forum": "QeX0YFt4iW",
            "replyto": "QeX0YFt4iW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_dU51"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_dU51"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an adversarial attack framework aiming to conduct attacks on the text and image modalities in a unified framework, to reduce the attack magnitude on a single modality and make the attack unnoticeable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The argued first consideration of a unified multi-modal adversarial attack framework.\n\nGood writing and easy to follow."
                },
                "weaknesses": {
                    "value": "lacking deep consideration about the multi-modal generation problem. \n\nThe evaluation is not convincing."
                },
                "questions": {
                    "value": "All my concerns come from the main challenge, the core contribution of the paper, all of which are more important and should be clarified rather than the first work on the adversarial attack on multiple modalities, in my opinion. Besides, there also exist some questions. \n\n1. The authors only present a solution for a multi-modal adversarial attack via attacking two modalities and selecting a prominent one. I cannot capture the core problem in the multi-modal adversarial attack and difficulties. Please make a thorough illustration. \n2. The evaluation metric designed by the authors is confusing. The paper evaluates PSNR between the original generated image and the adversarial generated image, but I am not sure whether this comparison pair has convincing effects.  I understand the authors aim to estimate the generation effect, but directly comparing two generated images seems to lack stable indications. \n3. The adversarial between the two modalities has no relationship in the paper, also verified in Table 1. Under this condition, I think the paper requires further improvement. \n4. Please clarify the differences in the utilization of SD-2-1, SD-1-4, and SD-1-5.\n5. The imperceptibility has no visible verification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807693288,
            "cdate": 1698807693288,
            "tmdate": 1699636682119,
            "mdate": 1699636682119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EztZXe2ikm",
                "forum": "QeX0YFt4iW",
                "replyto": "cA4BjWxBP9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review!\n\nQ1: Challenges & contributions.\n\nAs we stated in the third paragraph of Section 1, perturbing the text and image at the same time distributes the needed perturbation to both modalities, making them more imperceptible to humans on the whole. Besides, compared with single-modality attacks, multi-modality attacks have the potential to discover more robustness issues. Therefore, the challenge resides on the proper interaction between two modalities to find the optimal perturbation in each step for both good attacking performance and imperceptibility. Therefore, we propose MMA, which uses a unified query ranking framework to properly combine the updating on both modalities.\n\nFurthermore, we are the first to study the multi-modality attacks on diffusion models. We formally define the multi-modality attacks, specifying the attack objective, the perturbation space, and the attack constraints. \n\nMulti-modality attacks also have multiple applications. The generated adversarial examples can be utilized to retrain the model for improving the robustness [3], and to fingerprint the models [4] and watermark the examples [5] for solving privacy issues. \n\nQ2: Evaluation.\n\nWe follow previous works [1,2] to utilize PSNR as a metric to quantify the attacking performance. PSNR measures the difference between the generated images of the original input and the adversarial input. In addition to PSNR, we also deploy other metrics, like SSIM, and MSSSIM to measure the difference. The results on these metrics validate the effectiveness of MMA.\n\nQ3: Two modalities.\n\nThe two modalities have different influences on the output results. Text perturbations mislead the high-level features and the overall function of image editing, while image perturbations influence the low-level features and reduce the generation quality. As shown in the third row of Figure 3 in the paper, the scenario of the edited image (a cat and a bed) keeps the same under image perturbations, but the text perturbations change the scenario. Besides, as shown in the last row of Figure 3 in the paper, combining perturbations from two modalities can effectively break the normal functionality of diffusion models, while the single-modality perturbation fails to do so.\n\n\nQ4: Model differences.\n\nThe difference between SDv1 and SDv2 is on the encoder of the image. SDv1 uses the pre-trained CLIP, while SDv2 retrains CLIP on their own dataset. The difference between SDv1-4 and SD-v1-5 is on the model weights. The latter version is further trained based on the previous version.\n\nQ5: Qualitative examples.\nPlease refer to the top 2 rows of Figure 3 in our paper. We can see that both the image and text perturbations are imperceptible to humans.\n\n\n[1] Salman H, Khaddaj A, Leclerc G, et al. Raising the cost of malicious ai-powered image editing[J]. arXiv preprint arXiv:2302.06588, 2023.\n\n[2] Zhang J, Xu Z, Cui S, et al. On the Robustness of Latent Diffusion Models[J]. arXiv preprint arXiv:2306.08257, 2023.\n\n[3] Shafahi A, Najibi M, Ghiasi M A, et al. Adversarial training for free![J]. Advances in Neural Information Processing Systems, 2019, 32.\n\n[4] Peng Z, Li S, Chen G, et al. Fingerprinting deep neural networks globally via universal adversarial perturbations[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 13430-13439.\n\n[5] Liang C, Wu X, Hua Y, et al. Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples[J]. 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452359180,
                "cdate": 1700452359180,
                "tmdate": 1700452359180,
                "mdate": 1700452359180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lnIAv9WbYe",
                "forum": "QeX0YFt4iW",
                "replyto": "cA4BjWxBP9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your valuable reviews. Since the discussion phase is going to finish in one day, we look forward to your further feedback about our latest response. Do you still have any concerns about our paper after the rebuttal? Do we address your questions? We are happy to discuss with you in more detail. We greatly appreciate your time and feedback.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659402139,
                "cdate": 1700659402139,
                "tmdate": 1700659402139,
                "mdate": 1700659402139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4dLSRGK0YN",
            "forum": "QeX0YFt4iW",
            "replyto": "QeX0YFt4iW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_FHsv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6240/Reviewer_FHsv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Multi-Modality Adversarial Attack (MMA) algorithm, specifically designed for latent diffusion models (LDMs). MMA simultaneously modifies text and image components within a unified framework, effectively addressing multi-modal robustness. Extensive experiments demonstrate MMA's effectiveness in triggering LDM failures while requiring a smaller perturbation budget compared to single modality attacks, enhancing invisibility."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Exploration of an intriguing research topic focused on attacking latent diffusion models, shedding light on a relatively unexplored area.\n2. Straightforward and easy-to-understand approach to address the challenges associated with the subject matter."
                },
                "weaknesses": {
                    "value": "1. The evaluation metrics for the attack used in the paper cannot be used to quantitatively measure the success rate of the proposed attack. The proposed measurements are all performance metrics related to the image quality of the latent diffusion model and are relative indicators that only show that the attack is working roughly well.\n2. The rationale behind the necessity of this attack is not thoroughly convincing. The paper lacks a clear articulation of the contributions that the proposed attack brings to the field of latent diffusion models.\n3. The absence of human evaluation metrics to quantitatively assess the effectiveness of the attack in generating adversaries is a notable limitation.\n4. The paper does not provide sufficient details regarding the success rate of the attack. Information on how many instances the attack succeeded and the overall success rate is missing, leaving a gap in the evaluation of its performance.\n5. The use of citation methods such as \\citep and \\citet could be improved for better clarity and presentation in the paper, to distinguish between citations with or without parentheses."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844195005,
            "cdate": 1698844195005,
            "tmdate": 1699636681995,
            "mdate": 1699636681995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O7MnlyqTch",
                "forum": "QeX0YFt4iW",
                "replyto": "4dLSRGK0YN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review!\n\nQ1: Measurement.\n\nA diffusion model is a generative model. Therefore, attacking diffusion models is different from the classic adversarial attacking scenario, which targets image classifiers and has an explicit attacking objective and measurement as we discussed in Section 3. Therefore, we cannot quantify the performance of the attack by the success rate. Instead, we utilize the performance metrics in the generative model literature [1,2], which allows us to quantify the attacking performance by the generation quality and the similarity between the generated images and prompts under attacks. Besides, the selected metrics are widely used in previous studies of adversarial attacks on diffusion models [3,4].\n\n\nQ2: Contribution.\n\nAdversarial attacks can be used to evaluate the robustness of the target model. As we discussed in the third paragraph of Section 1, we need multi-modality attacks for the below reasons.\n\n(1)\tCompared with the single-modality attacks on diffusion models, multi-modality attacks can more thoroughly explore the perturbation space. Therefore, multi-modality attacks can find more failures of diffusion models than single-modality attacks.\n\n(2)\tPerturbing the text and image at the same time distributes the needed perturbation to both modalities, making them more imperceptible to humans on the whole. \n\n(3)\tThe generated adversarial examples can also be utilized to retrain the model for improving the robustness [5], and to fingerprint the models [6] and watermark the examples [7] for solving privacy issues. \n\nQ3: User study.\n\nIn order to measure the success of the attack and its imperceptibility, we launch a preliminary user study, where we randomly select 100 examples, and ask three annotators to give their scores. We compare our approach with baselines LDMR and QF. For measuring the ASR, we ask the annotator the following question: How well does the generated image correspond with the prompt? Please score it from 1 to 5, where the higher score means a higher consistency. For measuring the imperceptibility, we ask the annotator the following question: How similar are the two data pairs (the adversarial pair and the original pair)? Please score it from 1 to 5, where the higher score means a higher similarity.\n\nThe results are shown in the table below. The agreement of the user study is 80.7 %. We can see that our approach achieves the best attacking performance and imperceptibility.\n\n| Methods |Consistency (ASR) | Similarity (Imperceptibility) |\n| :------: | :----: | :----: |\n| LDMR| 3.52 | 2.87 |\n| QF| 3.14 | 2.15 | \n| Ours | **2.47** | **4.58** |  \n\n\n\nQ4: ASR.\n\nAs we mentioned in Q1, the attack success rate is not suitable for quantifying the performance of attacks on diffusion models. If needed, we can set a threshold on the drop of the CLIP score to represent whether the attack is successful or not. We set the threshold to be 2.0. The ASR of MMA is 99.6\\%, while the ASR of LDMR and QF are 10.5\\% and 21.3\\%, respectively. Our approach outperforms the baselines with a large margin on ASR.\n\nQ5: Presentation.\n\nThe citation in the paper is modified as you suggested.\n\n[1] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.\n\n[2] Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.\n\n[3] Salman H, Khaddaj A, Leclerc G, et al. Raising the cost of malicious ai-powered image editing[J]. arXiv preprint arXiv:2302.06588, 2023.\n\n[4] Zhang J, Xu Z, Cui S, et al. On the Robustness of Latent Diffusion Models[J]. arXiv preprint arXiv:2306.08257, 2023.\n\n[5] Shafahi A, Najibi M, Ghiasi M A, et al. Adversarial training for free![J]. Advances in Neural Information Processing Systems, 2019, 32.\n\n[6] Peng Z, Li S, Chen G, et al. Fingerprinting deep neural networks globally via universal adversarial perturbations[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 13430-13439.\n\n[7] Liang C, Wu X, Hua Y, et al. Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples[J]. 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452272745,
                "cdate": 1700452272745,
                "tmdate": 1700452272745,
                "mdate": 1700452272745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TLTs9XW7CM",
                "forum": "QeX0YFt4iW",
                "replyto": "4dLSRGK0YN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your valuable reviews. Since the discussion phase is going to finish in one day, we look forward to your further feedback about our latest response. Do you still have any concerns about our paper after the rebuttal? Do we address your questions? We are happy to discuss with you in more detail. We greatly appreciate your time and feedback.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659387791,
                "cdate": 1700659387791,
                "tmdate": 1700659387791,
                "mdate": 1700659387791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]