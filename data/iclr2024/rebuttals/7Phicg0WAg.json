[
    {
        "title": "FlexCap: Generating Rich, Localized, and  Flexible Captions in Images"
    },
    {
        "review": {
            "id": "G8TiXiJghU",
            "forum": "7Phicg0WAg",
            "replyto": "7Phicg0WAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1451/Reviewer_sAQH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1451/Reviewer_sAQH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel archtiecture to let LLM deeply understand an image in detail. \n\nThe proposed FlexCap consists of two important ideas:\n(1) Train a captioning model that can generate captions for specific bounding box inside an image.\n(2) Using Prompt engineering, this paper also propose a way to extend an LLM into a multimodal LLM by providing it different captions based on different bounding boxes.\n\nBy doing this, FlexCap-LLM is successfully turned into a multimodal agent that can perform various tasks including VQA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The framework that combines LLM with a captioning model is a great alternative to save budget it takes to train a multimodal agent. \n\nWhile previous works train a single model that can serve as a multimodal agent, authors utilize captioning model and LLM to build a multimodal agent. \n\nWhile there can be possible performance degradation, this 2 stage model can still shows great performance compared to other one stage model."
                },
                "weaknesses": {
                    "value": "While the 2-stage concept of FlexCap-LLM shows effectiveness in various downstream tasks, LLM can't directly understand an image in latent space.\n\nFrom the result of this work and previous works, it's not still sure that 1 stage or 2 stage is better than the other.\n\nFlexCap-LLM also relies on the performance of FlexCap's captioning performance. If FlexCap generates wrong caption for each bbox, this can lead to hallucination."
                },
                "questions": {
                    "value": "As I mentioned in weakness section, FlexCap-LLM seems to rely on the captioning performance of FlexCap.\n \nIs there any way to filter out mismatching caption with input bounding box?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658072940,
            "cdate": 1698658072940,
            "tmdate": 1699636073951,
            "mdate": 1699636073951,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dP0AKRAUi5",
                "forum": "7Phicg0WAg",
                "replyto": "G8TiXiJghU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response for Reviewer sAQH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their effort and time reviewing our work. We address the concerns raised below:\n\n*W1. While the 2-stage concept of FlexCap-LLM shows effectiveness in various downstream tasks, LLM can't directly understand an image in latent space.*\n\nYes, we agree that in this work the LLM does not directly understand the image in the latent space. But this might not be a limitation. First, we show that without having to train LLMs to understand image latents, we can get comparable or better zero-shot performance on visual question answering tasks by providing more informative input text. Hopefully this work inspires alternative research in ways to produce more informative inputs for LLMs. Whether it is image latents or text or a combination of both will remain to be seen. \n\nFurthermore, we believe a model like FlexCap will be useful in producing descriptive and localized image datasets that will be useful for training 1 stage models. Present image-text datasets are sparsely labeled in the sense that they do not describe all the objects and regions present in the image. We believe that work in this direction will lead to more densely labeled datasets.\n\n*W2. From the result of this work and previous works, it's not still sure that 1 stage or 2 stage is better than the other.*\n\nPerhaps rather than picking a winner on one metric, we can look at both approaches from multiple angles where one would be more advantageous than the other. For instance some advantages of 2 stage approach are: \n1) Easy interpretability of the inputs to the LLM. If the object detection or localization is wrong we can work on better recognition or detection models to improve performance of the system.\n2) Identify the source of hallucinations. Since it is a two stage process we can easily identify if the source of a hallucination is the vision or language model when it happens in a conversation with a user.\n3) Using pre-trained LLMs for vision tasks shows we can improve performance for reasoning and question answering tasks by working on more informative inputs without the need for a large number of resources required to train language models together with visual inputs. \n\n*W3. FlexCap-LLM also relies on the performance of FlexCap's captioning performance. If FlexCap generates wrong caption for each bbox, this can lead to hallucination.\n...\nAs I mentioned in weakness section, FlexCap-LLM seems to rely on the captioning performance of FlexCap.*\n\nWe agree with the reviewer that the final performance of FlexCap-LLM is dependent on both FlexCap and LLM\u2019s performance. The novelty of our work is the FlexCap model and FlexCap-LLM is the name of the system used to evaluate the performance of the FlexCap model. FlexCap-LLM works by passing an image through FlexCap first to get many boxes and their respective captions. These localized captions are passed onto an already trained LLM. Through this experiment, we want to highlight the fact that the captions generated by FlexCap are good enough for solving downstream tasks with LLMs. While errors can occur during captioning, we find the combination of FlexCap+LLM is at par or better than other zero-shot baselines that are trained in an end-to-end manner (Table 1 and 2). \n\nThe answers to the reviewer's questions are below:\n\n*Q1. If FlexCap generates wrong caption for each bbox, this can lead to hallucination.\nIs there any way to filter out mismatching caption with input bounding box?*\n\nYes it is possible to use an open-vocabulary object detector like OWL-ViT to filter out any hallucinated captions. The text embedding of a wrong caption will have a lower matching score with the embedding of the object in the image. We use this method in the VizWiz experiments to indicate how well the predicted sentence matches with the region in the image. We did not find this score to be that useful for the other VQA tasks. We will add this detail to the paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506154275,
                "cdate": 1700506154275,
                "tmdate": 1700506154275,
                "mdate": 1700506154275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zcJFuXiQI5",
            "forum": "7Phicg0WAg",
            "replyto": "7Phicg0WAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1451/Reviewer_N2tn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1451/Reviewer_N2tn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a model to generate controllable localized visual descriptions of bounding box, and proposed a large-scale dataset generated from web- scale image-text pairs. The experiments show that FlexCap exceeds SOTA performance on several benchmarks in the zero-shot setup, and achieves superior localized captioning performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A large-scale dataset is generated by the author, which might enable the training of multi-modal tasks.\n2. The experimental results are abundant, eg. visual question answering tasks, localized captioning, and the proposed model achieves promising results on several tasks."
                },
                "weaknesses": {
                    "value": "1. The architectures are mostly a compilation of pre-defined visual models and large language models. However, the novelty and inspiration for the latter works are questionable.\n\n2. Another concern is the precision or accuracy of the proposed large scale dataset. The authors claimed they use the filtered n-grams as text queries for pre-trained region proposal models (i.e. OWL- ViT (Minderer et al., 2022)) to extract boxes and select text-box pairs based on the similarity score (> 0.1). Therefore, the upper bound of precision of this dataset is determined by OWL- ViT, this might restrict the downstream tasks on the dataset.\n\n3. The implementation details should be removed to the experiments section."
                },
                "questions": {
                    "value": "1. The description about loss is imprecise, please use some formulations to formalized it. Additionally, the authors claimed that 'The loss is ignored over length-prefix tokens and...'. However, as shown in Figure2, the prediction of length-prefix tokens should be 'grey', the first token in the sentence. Is this ignored in the training process?\n2. Why is the 'length conditioning' works? The length conditioning is implemented simply by adding an additional token that indicates the desired length of the output caption, and there is no training constrains. How to guarantee the additional token constrains the length?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741695879,
            "cdate": 1698741695879,
            "tmdate": 1699636073875,
            "mdate": 1699636073875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nSSawNZpCr",
                "forum": "7Phicg0WAg",
                "replyto": "zcJFuXiQI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response for Reviewer N2tn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their effort and time reviewing our work. We address the concerns raised below:\n\n*W1: The architectures are mostly a compilation of pre-defined visual models and large language models. However, the novelty and inspiration for the latter works are questionable.*\n\nWe use standard transformer-based architectures (which is quite common and has been successful in a variety of domains) for the vision encoder and the vision-bounding-box-text decoder. However, both the encoder and decoder are trained from scratch using our image-box-caption triplet dataset end-to-end for the localized captioning task. In other words our visual models may be considered pre-defined in terms of architecture, however, they are not pre-trained. The novelty in this work is that the resulting FlexCap model is a localized image captioner that can be conditioned using bounding boxes. We also propose a method to obtain the large-scale and diverse dataset required to train such a model.\n\n*W2: Another concern is the precision or accuracy of the proposed large scale dataset. The authors claimed they use the filtered n-grams as text queries for pre-trained region proposal models (i.e. OWL- ViT (Minderer et al., 2022)) to extract boxes and select text-box pairs based on the similarity score (> 0.1). Therefore, the upper bound of precision of this dataset is determined by OWL- ViT, this might restrict the downstream tasks on the dataset.*\n\nThere is an inherent trade-off between precise, small-scale, well curated data (e.g. COCO captions) and large-scale diverse but noisy data (e.g. CLIP training set). The existing research on full image captioning (e.g. CLIP, CoCa, CAPPA) shows that large-scale noisy data could be more effective even if it is noisy data, mainly because it has diversity and enables training at scale. In this work we also choose to explore using a large-scale albeit noisy dataset as we want to generate a rich caption for any region in an image. While there will be a certain level of noise when using internet caption data, we find we can still train models with strong object localization using this data as our results demonstrate.\n\nThe answers to the reviewer's questions are below:\n\n*Q1: The description about loss is imprecise, please use some formulations to formalized it. Additionally, the authors claimed that 'The loss is ignored over length-prefix tokens and...'. However, as shown in Figure2, the prediction of length-prefix tokens should be 'grey', the first token in the sentence. Is this ignored in the training process?*\n\nThank you for catching this. We acknowledge that the loss is not ignored over prediction of length-prefix tokens (that is 'grey' in the example will be considered for the loss). We will remove the sentence \u201cThe loss is ignored over length-prefix tokens\u201d from the paper .  \n\nBelow we formalize the loss function and will include in the paper:\n\nWe have a triplet $T=(x, b, W)$ for image $x$, bounding box $b$ and box caption $W$. \n\nIn our work, \n\n$$W={w_0,w_1,w_2,...w_M} = {Length_K,w_1,w_2,...w_M}$$   \n\nwhere $Length_K$ is the number of words present in the box caption $W$. In the example in Figure 2, $W = [Length_7 $, grey, and, white, cat, lying, on, shoes, <eos>$]$.\n\nFor a given data triplet $T$, our objective is to maximize the following log-likelihood.\n$l(T) = l(x, b, W) = \\sum_{i=1}^{M} \\log P(w_i| w_{<i}, x, b)$.\n\nAssume that we have a dataset $D = {T_1, T_2, ..., T_N}$. The overall loss function is:\n$L = \\frac{\\sum_{j=1}^N l(T_j)}{N} = \\frac{\\sum_{j=1}^N l(x_j , b_j, W_j)}{N}$. \n\n*Q2: Why is the 'length conditioning' works? The length conditioning is implemented simply by adding an additional token that indicates the desired length of the output caption, and there is no training constrains. How to guarantee the additional token constrains the length?*\n\nWe train the entire model including the text decoder from scratch for the task of length conditioned caption generation. For dataset generation, we associate n-gram textual captions with bounding boxes (refer fig 3). During training, the length token is prefixed to each textual caption to indicate to the model the desired length of the caption. The training constraint of adhering to the length conditioning exists in the loss function. Throughout training the model needs to predict outputs of a certain length and then \u201c<eos>\u201d token (end-of-sentence) or it will result in a high loss. \n\nIn Section 5.3, We measure how often the trained model complies to the length constraint. We observe that if we train with length conditioning the model complies with the desired length most of the time (97 out of 100). The detailed statistics on compliance can be viewed in Table 4. Our length-conditioning claims are mainly backed with strong empirical evaluations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506163623,
                "cdate": 1700506163623,
                "tmdate": 1700506163623,
                "mdate": 1700506163623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iTb8bhwntf",
                "forum": "7Phicg0WAg",
                "replyto": "nSSawNZpCr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1451/Reviewer_N2tn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1451/Reviewer_N2tn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the response of authors. Some of my concerns have been solved. Although the experiments demonstrate the trained model could generate the sentences with proper length, however, it is still difficult to figure out why the length constraint works. \nThus,  I'd like to maintain my borderline rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636811696,
                "cdate": 1700636811696,
                "tmdate": 1700636811696,
                "mdate": 1700636811696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7w868MqJGi",
            "forum": "7Phicg0WAg",
            "replyto": "7Phicg0WAg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes a module called FlexCap, which can generate flexible-length of captions for a bounding-box. FlexCap is trained using a self-collected dataset containing variable length of captions. The experiments show that such captions can be used to prompt LLMs to achieve good performance for vision-language tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The work proposed a new end-to-end model which can generate a variable length (as condition) of caption of a region. Such a model is new (as far as I know), and can be beneficial for future research.\n- The experiments demonstrates effectiveness of variable length captions by FlexCap."
                },
                "weaknesses": {
                    "value": "-  In introduction, the authors claim that they diverge from Flamingo and BLIP2 (which use latent representations to connect LLMs), and use textual representation of images. However, the idea of using textual description of images to prompt LLMs for vision-language tasks is not new.\n- The major contribution, comparing to prior works, is producing a desired length of caption. Thus a baseline is missing: Simply use a caption model to predict (1) whole image description, (2) a detection model to predict the object in the box, and prompt a LLM conditioned on (1) and (2) to generate desired length of captions. This baseline circumvents the collecting of a new dataset and training a model like FlexCap.\n- In fact, I am wondering if other carefully filtered/designed localized captions could better prompt the LLM for those tasks. The work fails to compare or propose other baselines to demonstrate the superiority of FlexCap."
                },
                "questions": {
                    "value": "- Will the dataset be released?\n- Will the pre-trained model (weights) be released?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825167125,
            "cdate": 1698825167125,
            "tmdate": 1699636073775,
            "mdate": 1699636073775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P2HhjYFfMn",
                "forum": "7Phicg0WAg",
                "replyto": "7w868MqJGi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response for Reviewer 1skp - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their effort and time reviewing our work. We address the concerns raised below:\n\n*W1: In introduction, the authors claim that they diverge from Flamingo and BLIP2 (which use latent representations to connect LLMs), and use textual representation of images. However, the idea of using textual description of images to prompt LLMs for vision-language tasks is not new.*\n\nWe agree with the reviewer that \u201cusing textual description of images to prompt LLMs for vision-language tasks\u201d is not new. What is new in our work is providing more informative textual inputs in the form of localized captions produced in a controllably-rich manner. To achieve this, we also had to create a large-scale dataset of localized captions that enables the training of such a model. We will clarify this in the paper.\n\n*W2: The major contribution, comparing to prior works, is producing a desired length of caption.*\n\nWe would like to point out that besides producing a caption of desired length, a key contribution of our work is producing a localized caption that describes the content in any given bounding box using the context of the whole image. The captioning model needs to describe objects and regions of all sizes in the image. \n\n*W2: Thus a baseline is missing: Simply use a caption model to predict (1) whole image description, (2) a detection model to predict the object in the box, and prompt a LLM conditioned on (1) and (2) to generate desired length of captions. This baseline circumvents the collecting of a new dataset and training a model like FlexCap.*\n\nWe thank the reviewer for this suggestion. In theory, this is a sound approach. However, we found that the current leading image captioning models (such as BLIP2) do not recover all the objects in an image. We experimented initially with this approach while trying to localize objects with captions generated by VLMs. However image captioning models usually only generate captions mentioning salient objects in an image. This is what inspired us to work on a model that generates descriptions for any region in an image instead of matching existing descriptions with regions.\n\nAs an example we ran BLIP2 on the same images for which we have FlexCap results here:\n\n* [Image 1](https://flex-cap.github.io/images/00.html):\n\n    * BLIP2 captioner returns the following output: *two people walking down a sidewalk.*\n    * BLIP2 VQA with the prompt \u201cList all the objects in the image.\u201d returns the following output: *A man walking down the sidewalk, a man on a skateboard, a man on a bike, a man on a bench, a man on.*\n    * Objects that FlexCap described in the image (subset shown here): *brick-paved street, a black trash can, window, orange jacket, trees lining the street, blue jeans, a man is wearing a orange jacket, a bench on the sidewalk, a yellow car, red and white sign on the side of the building, a white pole on the sidewalk, a white street sign, a black bag on a man\u2019s shoulder, car on the street.*\n\n* [Image 2](https://flex-cap.github.io/images/09.html):\n\n    * BLIP2 captioner returns the following output: *a group of people playing a video game.*\n    * BLIP2 VQA with \u201cList all the objects in the image.\u201d returns the following output: *people, a dog, a cat, a tv, a computer, a couch, a window, a door, a chair, a table.*\n    * Objects that FlexCap described in the image (subset shown here): *red shirt on a woman, brown couch, wii controller, watch, black glasses, cap, magazines, framed picture on the wall, white blinds on window, papers on table, a watch on man\u2019s wrist, a pair of black shorts, remote strap, a stuffed animal on the couch, a stack of magazines, a framed picture on the wall.*\n\nAs the first part of the suggested approach misses on detecting a large number of objects, an LLM will be unable to produce reasonable descriptions of varying lengths for all objects in the image. \n\n*W3: In fact, I am wondering if other carefully filtered/designed localized captions could better prompt the LLM for those tasks. The work fails to compare or propose other baselines to demonstrate the superiority of FlexCap.*\n\nWe thank the reviewer for this feedback. We are also optimistic about using better localized captions to prompt LLMs. In Section 5, we explore how FlexCap\u2019s outputs (localized descriptions) can be used to prompt existing LLMs for various VQA tasks. \nWe also compare the usefulness of such localized captions for downstream tasks against other baselines such as Flamingo, BLIPv2 and ViperGPT. We think these are strong baselines that use LLMs in different ways for VQA tasks. We are happy to include comparisons with any particular baseline that the reviewer has in mind."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506166318,
                "cdate": 1700506166318,
                "tmdate": 1700506166318,
                "mdate": 1700506166318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D0rD9TzN1K",
                "forum": "7Phicg0WAg",
                "replyto": "7w868MqJGi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response for Reviewer 1skp - Part 2"
                    },
                    "comment": {
                        "value": "The answers to the reviewer's questions are below:\n\n*Q1: Will the dataset be released?*\n\nWe will not be able to release this dataset as it is based on WebLI, which is not yet open-sourced. However, we will release the code used to produce image-box-caption triplets from any image-text pair dataset (like YFCC100M). The data generation code is based on the OWL-ViT models, which are open-sourced. We believe the community will benefit from having a method to produce localized region-level captions from image-text pairs.\n\n*Q2: Will the pre-trained model (weights) be released?*\n\nWe are actively looking into ways to release the model weights."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506531370,
                "cdate": 1700506531370,
                "tmdate": 1700506531370,
                "mdate": 1700506531370,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fb6BkFGjcX",
                "forum": "7Phicg0WAg",
                "replyto": "P2HhjYFfMn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. However, I am not fully impressed by the contributions of the work and maintain my borderline rating."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622817799,
                "cdate": 1700622817799,
                "tmdate": 1700622817799,
                "mdate": 1700622817799,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]