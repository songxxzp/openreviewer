[
    {
        "title": "Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization"
    },
    {
        "review": {
            "id": "beEi2TeZMV",
            "forum": "huwR9N2ea0",
            "replyto": "huwR9N2ea0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_oRJe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_oRJe"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses a representative case of data inequality problem across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. It proposes a novel algorithm, ProUD, designed for progressive generalization across domains by leveraging domain-aware prototypes and uncertaintyadaptive mixing strategies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The direction of the research in the paper is meaningful and has the potential to generate positive impact.\n\n2. The description of the algorithm in the paper is clear, making it easy to read and reproduce.\n\n3. The description of experimental settings in the appendix is detailed."
                },
                "weaknesses": {
                    "value": "1. The paper is not well-written, as the algorithm lacks both theoretical support and adequate explanation, making it difficult to understand the authors' rationale behind the algorithm design.\n2. The experiment, as it stands currently, is not sufficiently refined for the following reasons: 1). The datasets used in the experiment are all simple and small-scale; 2). On some datasets, the performance improvement compared to EID is relatively small, and the choice of random seed is not general enough; 3). There is only one partition for each dataset, and the proportions are not uniform (9:1 and 8:2). The author did not provide an explanation for the choice of different partitions."
                },
                "questions": {
                    "value": "1. The algorithm uses prototypes obtained from soft labels when acquiring pseudo-labels (Equation 1), and prototypes obtained from hard labels are used in subsequent calculations of uncertainty and loss functions (Equation 3). What is the reason for these choices, and what is the difference in effectiveness between the two types of prototypes?\n\n2. How was Equation 7 derived, and what is the rationale behind choosing it?\n\n3. What is the improvement brought about by data augmentation, and how would the results compare if all algorithms used the same number of data augmentations in the comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Reviewer_oRJe",
                        "ICLR.cc/2024/Conference/Submission1702/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697539326747,
            "cdate": 1697539326747,
            "tmdate": 1700684666678,
            "mdate": 1700684666678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dOOW1gW9sO",
                "forum": "huwR9N2ea0",
                "replyto": "beEi2TeZMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's recognition of the meaningful direction and potential impact of our research, as well as the clarity of the algorithm's description. We are eager to clarify the points raised in your questions and concerns regarding the experimental design and rationale behind our algorithm.\n\n>The algorithm uses prototypes obtained from soft labels when acquiring pseudo-labels (Equation 1), and prototypes obtained from hard labels are used in subsequent calculations of uncertainty and loss functions (Equation 3). What is the reason for these choices, and what is the difference in effectiveness between the two types of prototypes?\n\nThe alternating use of soft and hard labels is followed by the previous work [1]. Our contribution involves extending the pseudo-labeling method of Liang et al. to be effective in Semi-Supervised Domain Generalization (SSDG) by leveraging domain-aware class prototypes.\n\n>How was Equation 7 derived, and what is the rationale behind choosing it?\n\nThe definition of  $\\lambda_\\epsilon = \\exp (-\\epsilon/\\tau_{\\lambda})/(1+\\exp(-\\epsilon/\\tau_{\\lambda}))$ assumes that the uncertainty of the \u201clabeled\u201d samples to be zero, which is the minimum value defined by Eq. 4. However, due to the inherent noisiness of the real-world data, it is not feasible for the uncertainty of unlabeled domains to be exactly zero. To promote more diverse augmentation, we introduce the threshold $\\lambda^*$ in Eq. 7, and in case $\\lambda > \\lambda^*$, we randomly choose the mixing ratio from a uniform distribution between 0 and 1, so that it is possible to mix with higher portion of unlabeled samples when its uncertainty is below a certain level of threshold. We found that the value could be fixed to 0.35, as the same value consistently yielded good results across all datasets and experiments.\n\n>What is the improvement brought about by data augmentation, and how would the results compare if all algorithms used the same number of data augmentations in the comparison?\n\nWe conducted an additional experiment on the Digits-DG dataset, reducing the number of data augmentations from 3 to 1. The average accuracy decreased by 0.24, yet this performance still outperforms all the baselines.\n\n[1] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International conference on machine learning, pp. 6028\u20136039. PMLR, 2020."
                    },
                    "title": {
                        "value": "A Response to Questions"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488238986,
                "cdate": 1700488238986,
                "tmdate": 1700488302385,
                "mdate": 1700488302385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qnKmPAb2Ro",
                "forum": "huwR9N2ea0",
                "replyto": "beEi2TeZMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Weakness 1"
                    },
                    "comment": {
                        "value": ">The paper is not well-written, as the algorithm lacks both theoretical support and adequate explanation, making it difficult to understand the authors' rationale behind the algorithm design.\n\nIn order to clarify the rationale behind the algorithm design, we would like to provide a theoretical background behind our work. Semi-supervised domain generalization, a specialized form of semi-supervised learning, is particularly relevant in scenarios where labeled and unlabeled data exhibit differing distributions or domains. To address this issue, we utilize the measure of uncertainty inherent in each pseudo-label to modulate the involvement of each sample in the training process. \n\nSpecifically, we employ MixUp between labeled and unlabeled data, applying a reduced mixing ratio in domains with higher uncertainty. By assigning a low mixing ratio to samples with high uncertainty, we prevent these unreliable samples from overly influencing the training process, thereby allowing for a more progressive integration of unlabeled domains. This strategy successfully addresses the scalability issues found in the previous state-of-the-art method [1], as we do not require a distinct DA model for each domain. \n\nHowever, directly applying MixUp to domain generalization can be problematic, as it does not distinguish between domain and class information. This entanglement of domain and class knowledge can result in performance degradation, as identified in [2]. To circumvent this issue, we first limit the application of MixUp to images from identical classes (DomainMix) and implement the Prototype Merging Loss, which reduces intra-class distance across varying domains to enhance generalization capabilities, while simultaneously, maintaining a clear separation between data points of different classes for effective discrimination. This approach prevents the entanglement of domain and class knowledge, enabling the effective learning of domain-invariant features that perform class discrimination across datasets from different domains.\n\n[1] Luojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, and Lei Zhang. Semi-supervised domain generalization with evolving intermediate domain. arXiv preprint arXiv:2111.10221, 2023.\n\n[2] Wang Lu, Jindong Wang, Han Yu, Lei Huang, Xiang Zhang, Yiqiang Chen, and Xing Xie. Fixed: Frustratingly easy domain generalization with mixup. arXiv preprint arXiv:2211.05228, 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488435132,
                "cdate": 1700488435132,
                "tmdate": 1700488435132,
                "mdate": 1700488435132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYE7w1tp9R",
                "forum": "huwR9N2ea0",
                "replyto": "beEi2TeZMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Weakness 2"
                    },
                    "comment": {
                        "value": ">The datasets used in the experiment are all simple and small-scale\n\nWhile they seem relatively simple and small-scale, PACS, Digits-DG, or Office-home have been used as important benchmark datasets in many of the recent papers published in major ML conferences, such as CVPR([1,2,3]), ICCV([4,5,6]), or ICLR([7,8]).\n\n>On some datasets, the performance improvement compared to EID is relatively small, and the choice of random seed is not general enough\n\nAlthough the margin of performance improvement may seem relatively small, we would like to emphasize that the major advantage of ProUD over EID lies in its scalability. EID requires separate domain adaptation models and their respective training processes for each unlabeled source domains, to address SSDG. This imposes a significant limitation on its practical application to SSDG problems with a large number of source domains. The same challenge applies to attempts to exploit different combinations of SoTA models for DA and DG. Therefore, our single model-based approach offers a significant practical advantage in handling numerous source domains in a scalable manner.\n\nWe would also like to clarify that the choice of random seed has been made with no intention of influencing, or any relation to, the experimental results. Please note that the standard deviations of the average accuracy over different seeds are marginal, and the lower bounds of the average accuracy $(\\mu- \\sigma)$ still outperform all the baselines.\n| Seed |  PACS | Office-Home | Digits-DG |\n|:----:|:-----:|:-----------:|:---------:|\n| 2022 | 75.08 |    56.33    |   74.37   |\n| 2023 | 75.09 |    56.24    |   74.26   |\n| 2024 | 75.02 |    56.28    |   74.10   |\n| Avg. | 75.06 |    56.28    |   74.24   |\n| Std. |  0.03 |     0.04    |    0.56   |\n\n>There is only one partition for each dataset, and the proportions are not uniform (9:1 and 8:2). The author did not provide an explanation for the choice of different partitions.\n\nFor the partitioning of each dataset, we followed EID for fair comparison. \n\n[1] Shiqi Lin, Zhizheng Zhang, Zhipeng Huang, Yan Lu, Cuiling Lan, Peng Chu, Quanzeng You, Jiang Wang, Zicheng Liu, Amey Parulkar, Viraj Navkal, and Zhibo Chen. \"Deep frequency filtering for domain generalization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2] Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. \"Rethinking federated learning with domain shift: A prototype view.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Jin Chen, Zhi Gao, Xinxiao Wu, and Jiebo Luo. \"Meta-causal Learning for Single Domain Generalization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[4] Sheng Cheng, Tejas Gokhale, and Yezhou Yang. \"Adversarial Bayesian Augmentation for Single-Source Domain Generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[5] Xiran Wang, Jian Zhang, and Lei Qi. \"Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[6] Chaoqi Chen, Luyao Tang, Leitian Tao, Hong-Yu Zhou, Yue Huang, Xiaoguang Han, and Yizhou Yu. \"Activate and Reject: Towards Safe Domain Generalization under Category Shift.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[7] Ziqiao Wang, and Yongyi Mao. \"Information-Theoretic Analysis of Unsupervised Domain Adaptation.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[8] Yan Yan, and Yuhong Guo. \"Partial Label Unsupervised Domain Adaptation with Class-Prototype Alignment.\" The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488710465,
                "cdate": 1700488710465,
                "tmdate": 1700488710465,
                "mdate": 1700488710465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEBNNbvnzC",
                "forum": "huwR9N2ea0",
                "replyto": "SYE7w1tp9R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Reviewer_oRJe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Reviewer_oRJe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful response, which addressed some of my concerns. For me, this is a paper teetering on the edge of acceptance, and it's quite nuanced. I'm inclined to give it a score of 5.5. Elevating it to a 6 would be a way to commend the author for the earnest attitude displayed during the rebuttal process. The final decision, however, will depend on a comparison with the quality of other submissions. The paper's strength lies in its meaningful research direction and overall well-written content. Areas for improvement include a lack of clear innovation in the algorithm, rigid details in algorithm design that lack persuasive power, and a relatively modest practical performance improvement. The paper falls short of reaching an outstanding level of appeal."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684581910,
                "cdate": 1700684581910,
                "tmdate": 1700684581910,
                "mdate": 1700684581910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X4jdaZan3A",
                "forum": "huwR9N2ea0",
                "replyto": "beEi2TeZMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your decision to raise the score to 6. We are pleased that our response has addressed some of your concerns. Thank you for your thoughtful feedback."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705453303,
                "cdate": 1700705453303,
                "tmdate": 1700705948397,
                "mdate": 1700705948397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aC9hBhUCzz",
            "forum": "huwR9N2ea0",
            "replyto": "huwR9N2ea0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_Rcny"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_Rcny"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new problem setting across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. The paper proposed a semi-supervised learning method called ProUD, by leveraging domain-aware prototypes, uncertainty adaptive mixing strategies, and pseudo labels. The authors conduct experiments on three datasets (PACS, Digits-DG, Office-Home) to demonstrate the effectiveness of ProUD."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper proposes an interesting setting Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. In the introduction, the paper lists some data inequality scenarios where SSDG may be used (Table 1). \n- It is a good try to introduce semi-supervised learning to the domain generalization community, e.g., how to construct pseudo labels."
                },
                "weaknesses": {
                    "value": "- The major concern, from my perspective, is the experiments section. (1) The paper misses many important benchmark datasets, such as VLCS, TerraInc, and DomainNet. See detail in DomainBed [1]. In particular, DomainBed is an important benchmark. (2) The paper fails to compare lots of state-of-the-art methods. The latest works compared in the paper are published in 2021. There are lots of good works in 2022 and 2023 that should be included, such as [2,3,4] and many more. (3) The results in Table 2, Table 3, and Table 4 cannot even show that ProUD outperforms EID by a large margin. I am not convinced that the proposed method ProUD is a useful method.\n- Some minor weaknesses. (1) The proposed methods need domain labels for domain-aware prototypes. This will restrict the methods when applied to the application. It would be good to consider whether there is a way to generalize the method to be domain label-free. (2) The methods are simple and trivial. In my understanding, PML loss (Equation 8) is pretty similar to SupCon [5]. (3) It would be good if the paper could introduce some theoretical analysis to give more insights or intuition about the ProUD. \n\n[1] Gulrajani, Ishaan, and David Lopez-Paz. In search of lost domain generalization. arXiv 2020.\n\n[2] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regularization with pre-trained models. ECCV 2022.\n\n[3] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. NeurIPS 2021. \n\n[4] Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, and Ziwei Liu. Sparse mixture-of-experts are domain generalizable learners. ICLR 2023.\n\n[5] Khosla, Prannay, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS 2020."
                },
                "questions": {
                    "value": "See the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Reviewer_Rcny"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639060480,
            "cdate": 1698639060480,
            "tmdate": 1700715592023,
            "mdate": 1700715592023,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lr5xF6favs",
                "forum": "huwR9N2ea0",
                "replyto": "aC9hBhUCzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, we would like to express our gratitude for your critical reviews and valuable insights aimed at improving our work. We acknowledge the concerns raised regarding certain aspects of our research. In the following comments, we will provide a detailed, point-by-point response, which we hope will ensure a comprehensive understanding of our methods and contributions.\n\n>The paper misses many important benchmark datasets, such as VLCS, TerraInc, and DomainNet. See detail in DomainBed [1]. In particular, DomainBed is an important benchmark.\n\nPACS, Digits-DG, and Office-home have been predominantly used as key benchmarks without including VLCS, TerraInc, or DomainNet, in many recent papers published in major ML conferences,  such as CVPR [6,7,8], ICCV [9,10,11], or ICLR [12,13].\n\n>The paper fails to compare lots of state-of-the-art methods. The latest works compared in the paper are published in 2021. There are lots of good works in 2022 and 2023 that should be included, such as [2,3,4] and many more.\n\nSSDG is a new problem setting that has not been extensively discussed previously.  The works [2,3,4] you mentioned are effective SoTA methods in DG. However, those methods are primarily multi-source domain generalization approaches, which are not directly applicable to SSDG, due to the absence of labels on most source domains. Instead, here we present an additional comparison of our method with the latest SoTA methods on single-source DG on the PACS dataset, including the one published in CVPR 2023 [8], thereby demonstrating the effectiveness of our method:\n\n|    Method    |   P  |   A  |   C  |   S  | Avg. |\n|:------------:|:----:|:----:|:----:|:----:|:----:|\n| RSC+ASR [14] | 54.6 | 76.7 | 79.3 | 61.6 | 68.1 |\n|   MCL [8]   | 59.6 | 77.1 | 80.1 | 62.6 | 69.9 |\n|     Ours     | 70.5 | 81.0 | 81.3 | 67.4 | 75.1 |\n\n>The results in Table 2, Table 3, and Table 4 cannot even show that ProUD outperforms EID by a large margin. I am not convinced that the proposed method ProUD is a useful method.\n\nBeyond its superior performance, it is important to highlight ProUD's major advantage over EID [15] in terms of scalability. EID requires a separate domain adaptation model and training process for each unlabeled source domain, which becomes impractical for SSDG problems involving many source domains. This limitation also applies to attempts at combining different SoTA models for DA and DG. In contrast, ProUD utilizes a single model capable of handling multiple source domains through its progressive generalization algorithm in a scalable manner."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485585153,
                "cdate": 1700485585153,
                "tmdate": 1700485585153,
                "mdate": 1700485585153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2i3kfTpiVI",
                "forum": "huwR9N2ea0",
                "replyto": "aC9hBhUCzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">The proposed methods need domain labels for domain-aware prototypes. This will restrict the methods when applied to the application. It would be good to consider whether there is a way to generalize the method to be domain label-free.\n\nSSDG is a new problem setting that assumes the presence of implicit domain labels. This setting is realistic in many scenarios as illustrated in Table 1. For instance, in biomedical imaging, lots of hospitals (domains) obtain medical image data but only few of them undergo additional process for labeling the images. Our method aims to present how to effectively utilize data from unlabeled domains along with implicit domain labels in a scalable manner. Existing methods for semi-supervised learning can also be applied for SSDG, but it is difficult for them to be as effective due to their inability to utilize available domain labels, as presented in our experimental results (Table 2,3, and 4).\n\n>The methods are simple and trivial. In my understanding, PML loss (Equation 8) is pretty similar to SupCon [5].\n\nContrastive learning itself is a conventional approach, and various studies have derived their own methods for applying this framework to their specific problem settings. The Supervised Contrastive Loss in SupCon [5] is designed to attract features of samples within the same class to each other, yet it lacks consideration for different domains, which is crucial in SSDG. \n\nOn the other hand, PML aims to attract the features of samples towards their corresponding $\\bar{C}_k$, which represents the average of the same class prototypes from all source domains. Merging the class prototypes from different domains not only enhances the model\u2019s generalization ability by making features domain-invariant, as visualized in Fig. 2 (further analysis is added in Appendix C of the revised manuscript), but also offers a significant advantage in representing under-represented domains. This approach prevents representation bias caused by the uneven distribution of samples across domains.\n\n>It would be good if the paper could introduce some theoretical analysis to give more insights or intuition about the ProUD.\n\nIn order to clarify the intuition about the ProUD, we would like to provide a theoretical rationale behind our work. Semi-supervised domain generalization, a specialized form of semi-supervised learning, is particularly relevant in scenarios where labeled and unlabeled data exhibit differing distributions or domains. To address this issue, we utilize the measure of uncertainty inherent in each pseudo-label to modulate the involvement of each sample in the training process. \n\nSpecifically, we employ MixUp between labeled and unlabeled data, applying a reduced mixing ratio in domains with higher uncertainty. By assigning a low mixing ratio to samples with high uncertainty, we prevent these unreliable samples from overly influencing the training process, thereby allowing for a more progressive integration of unlabeled domains. This strategy successfully addresses the scalability issues found in the previous state-of-the-art method [15], as we do not require a distinct DA model for each domain. \n\nHowever, directly applying MixUp to domain generalization can be problematic, as it does not distinguish between domain and class information. This entanglement of domain and class knowledge can result in performance degradation, as identified in [16]. To circumvent this issue, we first limit the application of MixUp to images from identical classes (DomainMix) and implement the Prototype Merging Loss, which reduces intra-class distance across varying domains to enhance generalization capabilities, while simultaneously, maintaining a clear separation between data points of different classes for effective discrimination. This approach prevents the entanglement of domain and class knowledge, enabling the effective learning of domain-invariant features that perform class discrimination across datasets from different domains."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486246666,
                "cdate": 1700486246666,
                "tmdate": 1700486246666,
                "mdate": 1700486246666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nui8vC3grE",
                "forum": "huwR9N2ea0",
                "replyto": "aC9hBhUCzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[1] Gulrajani, Ishaan, and David Lopez-Paz. In search of lost domain generalization. arXiv 2020.\n\n[2] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regularization with pre-trained models. ECCV 2022.\n\n[3] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. NeurIPS 2021.\n\n[4] Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, and Ziwei Liu. Sparse mixture-of-experts are domain generalizable learners. ICLR 2023.\n\n[5] Khosla, Prannay, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS 2020.\n\n[6] Shiqi Lin, Zhizheng Zhang, Zhipeng Huang, Yan Lu, Cuiling Lan, Peng Chu, Quanzeng You, Jiang Wang, Zicheng Liu, Amey Parulkar, Viraj Navkal, and Zhibo Chen. \"Deep frequency filtering for domain generalization.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[7] Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. \"Rethinking federated learning with domain shift: A prototype view.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[8] Jin Chen, Zhi Gao, Xinxiao Wu, and Jiebo Luo. \"Meta-causal Learning for Single Domain Generalization.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[9] Sheng Cheng, Tejas Gokhale, and Yezhou Yang. \"Adversarial Bayesian Augmentation for Single-Source Domain Generalization.\"\u00a0*Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2023.\n\n[10] Xiran Wang, Jian Zhang, and Lei Qi. \"Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization.\"\u00a0*Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2023.\n\n[11] Chaoqi Chen, Luyao Tang, Leitian Tao, Hong-Yu Zhou, Yue Huang, Xiaoguang Han, and Yizhou Yu. \"Activate and Reject: Towards Safe Domain Generalization under Category Shift.\"\u00a0*Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2023.\n\n[12] Ziqiao Wang, and Yongyi Mao. \"Information-Theoretic Analysis of Unsupervised Domain Adaptation.\"\u00a0*The Eleventh International Conference on Learning Representations*. 2022.\n\n[13] Yan Yan, and Yuhong Guo. \"Partial Label Unsupervised Domain Adaptation with Class-Prototype Alignment.\"\u00a0*The Eleventh International Conference on Learning Representations*. 2022.\n\n[14] Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong, and Mingyuan Zhou. Adversarially adaptive normalization for single domain generalization. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2021.\n\n[15] Luojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, and Lei Zhang. Semi-supervised domain generalization with evolving intermediate domain. arXiv preprint arXiv:2111.10221, 2023.\n\n[16] Wang Lu, Jindong Wang, Han Yu, Lei Huang, Xiang Zhang, Yiqiang Chen, and Xing Xie. Fixed: Frustratingly easy domain generalization with mixup. arXiv preprint arXiv:2211.05228, 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486274625,
                "cdate": 1700486274625,
                "tmdate": 1700486274625,
                "mdate": 1700486274625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t1WPlo4Mfd",
                "forum": "huwR9N2ea0",
                "replyto": "aC9hBhUCzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "With the hope that our response addresses your concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer Rcny,\n\nAs the discussion period draws to a close, we eagerly await your response. We greatly appreciate your valuable contributions to the review and enhancement of our paper.\n\nWe have provided detailed responses to each of your concerns. Please review our responses again and kindly let us know whether they adequately address your concerns and if our explanations are heading in the right direction. Any additional feedback would be highly appreciated.\n\nKind regards,\n\nAuthors of Paper1702"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708893473,
                "cdate": 1700708893473,
                "tmdate": 1700708893473,
                "mdate": 1700708893473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GOIqGawA1x",
                "forum": "huwR9N2ea0",
                "replyto": "t1WPlo4Mfd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Reviewer_Rcny"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Reviewer_Rcny"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed rebuttal. It partially fixed my concern, and I updated my score from 3 to 5. Please include a full comparison with MCL in future revisions. \n\nThe reason I raised my score is that I accept the SSDG with domain labels as a meaningful problem setting and the proposed method ProUD can beat MCL, although there are only limited baselines in this direction. \n\nThe reason I still do not tend to accept:\n- The method lacks novelty as Reviewer sXNN comments.\n- The paper can be stronger if authors evaluate different baselines and their methods on DomainNet.\n- The theoretical analysis part is vague. See a related sample paper below [1] for reference. \n\n[1] Deng, Yihe, Yu Yang, Baharan Mirzasoleiman, and Quanquan Gu. Robust Learning with Progressive Data Expansion Against Spurious Correlation. NeurIPS 2023."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715565204,
                "cdate": 1700715565204,
                "tmdate": 1700715565204,
                "mdate": 1700715565204,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KjnUuzmVx4",
            "forum": "huwR9N2ea0",
            "replyto": "huwR9N2ea0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_sXNN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_sXNN"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a practically important problem called Semi-Supervised Domain Generalization (SSDG), where the multiple source domains contain both labeled samples and unlabelled samples. Its goal is to generalize the model trained on the source domains to an unseen target domain. To address this issue, the authors propose a novel algorithm called ProUD, which leverages domain-aware prototypes and uncertainty-adaptive mixing strategies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **[The problem of this paper is critical in practice.]** In the previous studies under domain generalization, they always assume that labels of multiple source domains are available. However, it is sometimes infeasible to obtain such perfect source domains, which gives rise to the importance of SSDG.\n2. **[This paper is well written and easy to follow.]** Background, motivation, details about the proposed method,  and experiments are well introduced."
                },
                "weaknesses": {
                    "value": "1. **[The proposed method is lack of novelty.]** Essentially, the proposed method is still a combination of DA +DG. The step for assigning pseudo labels for unlabelled source domains can be regarded as DA, while the step for learning domain-invariant representations via a contrastive loss can be regarded as DG. Tools used in each step are also widely used in methods for DA and DG basically\n2. **[Motivation of this proposed method is unclear.]** It is unclear why the authors propose this method. For example, if there exists a research gap for current studies on SSGD?\n3. **[The propsed method is not explored deeply.]** Firstly, this paper does not provide a theoretical analysis to certify the effectiveness of the proposed method. Furthermore, it is unclear how the accuracy of pseudo-labels affects the final performance. In detail, in Equation (6), you mix labeled and unlabeled samples with the same class up, whose performance may heavily rely on the quality of the pseudo labels. Additionally, in the t-SNE Visualization, I want to know if Equation (8) contributes to the representation most."
                },
                "questions": {
                    "value": "1. How do you choose the value of the threshold $\\lambda^*$?\n2. Can you provide insights to answer the questions in Weakness 2 and Weakness 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810251147,
            "cdate": 1698810251147,
            "tmdate": 1699636098481,
            "mdate": 1699636098481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X9iDxa0z19",
                "forum": "huwR9N2ea0",
                "replyto": "KjnUuzmVx4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Questions"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and constructive criticisms regarding our paper. We appreciate your recognition of the practical importance of SSDG and the clarity of presentation in our work. We'd like to address the points you raised in the 'Questions' section and resolve the concerns you brought up under 'Weaknesses.\u2019\n\n>$\\textit{How do you choose the value of the threshold\u00a0$\\lambda^\u2217$?}$\n\nWe chose the value of the threshold $\\lambda^\u2217$ from the following set of values {0.3, 0.35, 0.4, 0.45} by evaluating on validation dataset. However, we found that the value $\\lambda^\u2217$ could be fixed to 0.35, as the same value consistently yielded good results across all datasets and experiments.\n\n>$\\textit{Can you provide insights to answer the questions in Weakness 2 and Weakness 3?}$\n\n>  $\\textbf{A Response to Weakness 2 (along with Weakness 1)}$\n\n>   [The proposed method is lack of novelty.] Essentially, the proposed method is still a combination of DA +DG. The step for assigning pseudo labels for unlabelled source domains can be regarded as DA, while the step for learning domain-invariant representations via a contrastive loss can be regarded as DG. Tools used in each step are also widely used in methods for DA and DG basically \n\n>   [Motivation of this proposed method is unclear.]\u00a0It is unclear why the authors propose this method. For example, if there exists a research gap for current studies on SSGD?\n\nThe current state-of-the-art in SSDG [1], requires as many separate domain adaptation models and training processes as there are unlabeled source domains. This requirement imposes a significant limitation on its practical application to SSDG problems with a large number of source domains. The same obstacle applies to any attempts to exploit different combinations of state-of-the-art models for DA and DG. \n\nHowever, ProUD\u2019s contribution lies in overcoming the limitations of previous methods that encounter scalability problems. ProUD\u2019s progressive generalization algorithm allows for handling multiple unlabeled source domains simultaneously with a single, scalable model through the effective use of a measure of uncertainty based on domain-aware prototypes. \n\nFurthermore, our DomainMix method not only modulates the involvement of samples from each unlabeled domain based on their uncertainty but also enhances the diversity of samples from under-represented unlabeled domains by augmenting them through an adaptive mix with labeled samples. The Prototype Merging Loss (PML), another key component of ProUD, constructs mean prototypes ($\\bar{C}_k$) by assigning equal weights to the same class prototypes from each domain, irrespective of the number of samples in each domain. This approach offers a significant advantage in representing under-represented domains, preventing representation bias caused by uneven sample distribution across domains.\n\n> $\\textbf{A Response to Weakness 3}$\n\n>  [The proposed method is not explored deeply.]\u00a0Firstly, this paper does not provide a theoretical analysis to certify the effectiveness of the proposed method. Furthermore, it is unclear how the accuracy of pseudo-labels affects the final performance. In detail, in Equation (6), you mix labeled and unlabeled samples with the same class up, whose performance may heavily rely on the quality of the pseudo labels. Additionally, in the t-SNE Visualization, I want to know if Equation (8) contributes to the representation most.\n\nTo assess the impact of Prototype Merging Loss (PML) on t-SNE visualization, we included an additional t-SNE visualization in Appendix C of the revised draft. This visualization illustrates the performance of our algorithm without the implementation of PML. In contrast to the distinct clustering observed in Fig. 2, the samples are only loosely clustered around their respective prototypes here, resulting in more ambiguous cluster boundaries than those seen with PML. Moreover, several prototypes remain intermingled, demonstrating a lack of distinct discrimination. These observations underscore the vital role of PML, as formulated in Equation (8). PML enhances the algorithm's ability to learn domain-invariant representations by effectively merging prototypes of the same class from diverse domains, while simultaneously ensuring their separation from prototypes of different classes.\n\n[1] Luojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, and Lei Zhang. Semi-supervised domain generalization with evolving intermediate domain. arXiv preprint arXiv:2111.10221, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482439409,
                "cdate": 1700482439409,
                "tmdate": 1700719433528,
                "mdate": 1700719433528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uUiMBMBUZN",
                "forum": "huwR9N2ea0",
                "replyto": "KjnUuzmVx4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "With the hope that our response addresses your concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer sXNN,\n\nAs the discussion period draws to a close, we eagerly await your response. We greatly appreciate your valuable contributions to the review and enhancement of our paper.\n\nWe have provided detailed responses to each of your concerns. Please review our responses again and kindly let us know whether they adequately address your concerns and if our explanations are heading in the right direction. Any additional feedback would be highly appreciated.\n\nKind regards,\n\nAuthors of Paper1702"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708858445,
                "cdate": 1700708858445,
                "tmdate": 1700708858445,
                "mdate": 1700708858445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5PmmMG5dVh",
            "forum": "huwR9N2ea0",
            "replyto": "huwR9N2ea0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_8HJ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1702/Reviewer_8HJ1"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a new algorithm (ProUD) for the problem of semi-supervised domain generalization. In this problem setting, \nwe have K different domains, and we have access to labeled data for 1 domain and unlabeled data for the remaining K-1 domains.\nThe goal is to solve a learning problem with respect to all domains, by leveraging information from both the labeled data and the unlabeled data. This is an important problem, as labeled data may be unavailable for some source domains (a possible case of data inequality).\n\nThe paper proposes a new algorithm to solve this task. The algorithm combines different ideas, some are new, and some are an adaptation from previous work, to provide an accurate solution to this problem. Most of the important techniques involve pseudo-labeling to provide labels to the unlabeled data, and a method called Domain mix that tries to control and manage the contribution for very uncertain pseudo-labeling (very important in the first steps of the algorithm, where the pseudo-labels are not going to be precise).\n\nThe authors run extensive different experiments on real datasets, comparing their method to a wide range of baselines in the literature. They find that their methods perform on average better than all the baselines. They also run ablation studies to understand and assess the impact of the most important components of their architecture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses a very important problem: studying situations when we have limited access to labeled data is of paramount importance in practice.\n\nFrom my perspective, the algorithm has two strengths. (1) it combines different techniques in order to provide a good solution to this problem, and it verifies that each of those individual components plays an important role in the ablation study, (2) it provides extensive analysis and comparison with the state-of-the-art to verify the superiority of their method.\n\nApart from a few details (see Weaknesses and comments below), I think the presentation of the paper is good."
                },
                "weaknesses": {
                    "value": "It is not clear to me why your architecture only learns a single model, and what is the motivation behind it. Although there may be domain-invariant features that we can learn from multiple sources, different features may provide different information depending on the domain.\nAs an example assume that there are two features f_A and f_B,  domain A may get a very good classification from a feature f_A (and not f_B), and domain B may get a very good classification from a feature f_B (and not f_A). In this case, it would make more sense to build different models that can exploit different features across the different domains. In your algorithm, this means that we keep the feature extractor g equal, but we build a function h for each different domain (similar strategies are also applied in ZSL).\n\nIn the data inequality model, some data sources may have a different number of examples. In particular, it could be that we have access to comparatively fewer data points for a given unlabeled domain. It looks to me that your model gives the same weight to each sample from each domain. In this case, if an unlabeled domain t is unrepresented (we have less data  N_t from it), then your model would still suffer from a data inequality issue, as the loss would be less influenced by the fewer samples on this unlabeled domain.\n\n\nOn the experiments:\n- Is it the accuracy with respect to a held-out dataset? Or is the model evaluated on the same unlabeled data that is also used during training?\n- Why is the accuracy averaged over the last 5 train epochs rather than only on the last epoch?\n- I cannot find an explanation on how the hyper-parameters are chosen for your algorithms (is there a validation step?), and how the hyper-parameters are chosen for the baseline algorithms. I think this is important when evaluating methods on a new dataset (without overfitting due to the choice of the hyper-parameters).\n- I believe that the standard deviation for the average accuracy is not reported. In particular, you obtain an average accuracy over 3 runs (with 3 different seeds), but it is not clear what is the variance of this value, which is important for comparison with other methods. (The reported STD is across the domain combinations rather than on the 3 seeded runs)."
                },
                "questions": {
                    "value": "(1) See the question above on the experiments, in particular for the standard deviation, hyper-parameters, and the choice of how the accuracy is reported.\n\n(2) See the points above on the proposed model/architecture. Why do you learn a single model for each domain, and how do you handle the case when an unlabeled domain is under-represented (it has fewer samples than other unlabeled domains)?\n\n------------\n\n\n\nA couple of suggestions:\n- \"dist is a function to measure the cosine distance\" -> \"dist is the cosine distance\". \n- It would be useful for the reader to get some intuitions behind some equations (in simple words), such as (1) and (3).\n- I would add a very synthetic explanation of what a \"prototype\" is for clarity.\n- I would briefly clarify that the shift that you are considering is only on the distribution of the features (unless I am missing something), but the classification problem is the same across all tasks.\n- There are other settings that are \"similar\" to this that would maybe be worth discussing in the introduction / related work as further motivation. One setting is Zero-shot Learning (ZSL), where we do not have access to unlabeled data for the other domains (and the classification may change, and one is provided with a description of the classes). Another setting is (programmatic) weak supervision, where the goal is to design simple rules to label an unlabeled domain, and pseudo-labeling and noise-aware losses are also used (e.g., see [A]).\n\n[A]: Ratner, Alexander, et al. \"Snorkel: Rapid training data creation with weak supervision.\" Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases. Vol. 11. No. 3. NIH Public Access, 2017."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1702/Reviewer_8HJ1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699042893440,
            "cdate": 1699042893440,
            "tmdate": 1699636098398,
            "mdate": 1699636098398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tgzkH5i0Ni",
                "forum": "huwR9N2ea0",
                "replyto": "5PmmMG5dVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Detailed Explanation of the Experiments"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review of our paper. We appreciate the depth of your analysis and your recognition of the significance of our work in addressing the crucial challenge of data inequality. Your constructive feedback on the strengths and weaknesses of our approach, as well as your questions and suggestions, are highly valuable for refining our research. We will carefully address all your questions below.\n>(1) See the question above on the experiments, in particular for the standard deviation, hyper-parameters, and the choice of how the accuracy is reported.\n\n>Is it the accuracy with respect to a held-out dataset? Or is the model evaluated on the same unlabeled data that is also used during training?\n\nWe evaluate our model on an unseen test domain, distinct from the unlabeled domains used during training. For instance, in a dataset comprising four domains, we designate one domain as the labeled source domain, two domains as unlabeled source domains, and the remaining domain as the unseen target domain.\n\n>Why is the accuracy averaged over the last 5 train epochs rather than only on the last epoch?\n\nWe report the average accuracy over the last five training epochs, following the approach of EID [1], the state-of-the-art baseline for SSDG, to ensure a fair comparison. For our method, the difference between the average accuracy of the last five epochs and the accuracy of the final epoch is negligible.\n\n>I cannot find an explanation on how the hyper-parameters are chosen for your algorithms (is there a validation step?), and how the hyper-parameters are chosen for the baseline algorithms. I think this is important when evaluating methods on a new dataset (without overfitting due to the choice of the hyper-parameters).\n    \nWe split source domain dataset into training and validation sets, using the validation set for hyper-parameter tuning. The evaluation is conducted on an unseen test domain dataset.\n\n>I believe that the standard deviation for the average accuracy is not reported. In particular, you obtain an average accuracy over 3 runs (with 3 different seeds), but it is not clear what is the variance of this value, which is important for comparison with other methods. (The reported STD is across the domain combinations rather than on the 3 seeded runs).\n    \nThe standard deviations of the average accuracy over 3 seeds are marginal, and the lower bounds of the average accuracy $(\\mu- \\sigma)$ still outperform all the baselines. The detailed experimental results are presented below.\n| Seed |  PACS | Office-Home | Digits-DG |\n|:----:|:-----:|:-----------:|:---------:|\n| 2022 | 75.08 |    56.33    |   74.37   |\n| 2023 | 75.09 |    56.24    |   74.26   |\n| 2024 | 75.02 |    56.28    |   74.10   |\n| Avg. | 75.06 |    56.28    |   74.24   |\n| Std. |  0.03 |     0.04    |    0.56   |\n\n[1] Luojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, and Lei Zhang. Semi-supervised domain generalization with evolving intermediate domain. arXiv preprint arXiv:2111.10221, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443782656,
                "cdate": 1700443782656,
                "tmdate": 1700448080882,
                "mdate": 1700448080882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5tLaBt9QgI",
                "forum": "huwR9N2ea0",
                "replyto": "5PmmMG5dVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">(2) See the points above on the proposed model/architecture. Why do you learn a single model for each domain, and how do you handle the case when an unlabeled domain is under-represented (it has fewer samples than other unlabeled domains)?\n\n>It is not clear to me why your architecture only learns a single model, and what is the motivation behind it. Although there may be domain-invariant features that we can learn from multiple sources, different features may provide different information depending on the domain. As an example assume that there are two features $f_A$ and $f_B$, domain $A$ may get a very good classification from a feature $f_A$ (and not $f_B$), and domain $B$ may get a very good classification from a feature $f_B$ (and not $f_A$). In this case, it would make more sense to build different models that can exploit different features across the different domains. In your algorithm, this means that we keep the feature extractor $g$ equal, but we build a function $h$ for each different domain (similar strategies are also applied in ZSL).\n\nThank you for your insights. I agree that it is feasible to train a single feature extractor, $g$, along with different classifiers, $h_i$, for distinct source domains. This approach resembles a multi-task learning setting, albeit focused on multi-domains rather than multi-tasks, which may lead to optimal performance for each domain with its corresponding classifier. However, it is important to note that the ultimate goal in SSDG is Domain Generalization, aiming to perform well on \u201cunseen\u201d domains (e.g., after training with domains $A$, $B$, and $C$, we evaluate test performance on domain $D$). If we train multiple classifiers, such as {$h_A$, $h_B$, $h_C$}, we face the issue of deciding which classifier to use for a newly provided domain $D$. \n\nOn the other hand, you might suggest utilizing different domain adaptation models for each unlabeled domain to generate pseudo-labels, so we can apply domain generalization to pseudo-labeled datasets. Indeed, this approach has been proposed in the previous work [1] on SSDG. However, it requires separate domain adaptation models and their respective training processes for each unlabeled source domain, to address SSDG. This imposes a significant limitation on its practical application to SSDG problems with a large number of source domains. The same challenge applies to attempts to exploit different combinations of state-of-the-art (SoTA) models for Domain Generalization (DG) and Domain Adaptation (DA). Therefore, our single model-based approach offers a significant practical advantage in handling numerous source domains in a scalable manner.\n\n>In the data inequality model, some data sources may have a different number of examples. In particular, it could be that we have access to comparatively fewer data points for a given unlabeled domain. It looks to me that your model gives the same weight to each sample from each domain. In this case, if an unlabeled domain t is unrepresented (we have less data $N_t$ from it), then your model would still suffer from a data inequality issue, as the loss would be less influenced by the fewer samples on this unlabeled domain.\n\nWe appreciate your constructive feedback. This is an important point that has been under-emphasized in our paper, yet it has been successfully addressed with our method. Our DomainMix method not only modulates the involvement of samples from each unlabeled domain based on their uncertainty, but also enhances the diversity of samples from under-represented unlabeled domains by mixing them with labeled samples. A similar approach has proven effective in previous work on few-shot domain adaptation [2], which paired a few target domain samples with a large number of source domain samples to learn domain-invariant features across both domains. Furthermore, our Prototype Merging Loss (PML) constructs  mean prototypes ($\\bar{C}_k$) by assigning equal weights to the same class prototypes from each domain, regardless of the number of samples each domain contains. This offers a significant advantage in representing under-represented domains, as it prevents representation bias due to the uneven sample distribution across domains.\n\n[1] Luojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, and Lei Zhang. Semi-supervised domain generalization with evolving intermediate domain. arXiv preprint arXiv:2111.10221, 2023.\n\n[2] Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and  Gianfranco Doretto. Few-shot adversarial domain adaptation. Advances in neural information processing systems 30, 2017."
                    },
                    "title": {
                        "value": "Detailed Explanation of the Proposed Model"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443833887,
                "cdate": 1700443833887,
                "tmdate": 1700445185885,
                "mdate": 1700445185885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3rxwjStDuC",
                "forum": "huwR9N2ea0",
                "replyto": "5PmmMG5dVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Response to Suggestions"
                    },
                    "comment": {
                        "value": ">\"dist is a function to measure the cosine distance\" -> \"dist is the cosine distance\".\n\n>It would be useful for the reader to get some intuitions behind some equations (in simple words), such as (1) and (3).\n\n>I would add a very synthetic explanation of what a \"prototype\" is for clarity.\n\nWe sincerely appreciate your thorough and insightful suggestions for our work. We have uploaded the revised version of the draft. Modifications have been made based on your suggestions, which can be found in 'Prototype-based Pseudo-labeling' of Section 3.2. \n\n>I would briefly clarify that the shift that you are considering is only on the distribution of the features (unless I am missing something), but the classification problem is the same across all tasks.\n\nThe distribution shift we consider is on image space (input level), and the classification problem is same across all domains.\n\n>There are other settings that are \"similar\" to this that would maybe be worth discussing in the introduction / related work as further motivation. One setting is Zero-shot Learning (ZSL), where we do not have access to unlabeled data for the other domains (and the classification may change, and one is provided with a description of the classes). Another setting is (programmatic) weak supervision, where the goal is to design simple rules to label an unlabeled domain, and pseudo-labeling and noise-aware losses are also used (e.g., see [A]).\n\nThank you for your valuable insights deepening our discussion on the relevant studies. We will need some time to familiarize ourselves with the recent developments in the areas you suggested. These discussions will be included in the 'Related Work' section of our final paper. If you have any additional suggestions or references, please share them with us. Your input would be greatly appreciated and would contribute to the enrichment of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448577444,
                "cdate": 1700448577444,
                "tmdate": 1700448577444,
                "mdate": 1700448577444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uhpoUvAgWU",
                "forum": "huwR9N2ea0",
                "replyto": "5PmmMG5dVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1702/Authors"
                ],
                "content": {
                    "title": {
                        "value": "With the hope that our response addresses your concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8HJ1,\n\nAs the discussion period draws to a close, we eagerly await your response. We greatly appreciate your valuable contributions to the review and enhancement of our paper.\n\nWe have provided detailed responses to each of your concerns. Please review our responses again and kindly let us know whether they adequately address your concerns and if our explanations are heading in the right direction. Any additional feedback would be highly appreciated.\n\nKind regards,\n\nAuthors of Paper1702"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708686063,
                "cdate": 1700708686063,
                "tmdate": 1700708699606,
                "mdate": 1700708699606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]