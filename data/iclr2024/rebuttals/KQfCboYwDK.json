[
    {
        "title": "Adiabatic replay for continual learning"
    },
    {
        "review": {
            "id": "Y86loNu5o7",
            "forum": "KQfCboYwDK",
            "replyto": "KQfCboYwDK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_po4B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_po4B"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of continual learning. The paper focuses on the generative replay approach for continual learning and proposes a new setting where the new information from the incoming task is not too significant but rather incremental based on the previous tasks (the AR setting). The paper proposes to use a Gaussian mixture model to simultaneously act as the generator as well as the learner for continual learning under the AR setting. The paper shows superior performance of the proposed method under some of the restricted settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a new setting that might be interesting to inspire new research direction where the information of the new incoming task is not too significant compared to the old ones.\n\n2. The paper uses a Gaussian mixture model, which can be used both as the generator and the learning model, and shows improved performance on multiple settings."
                },
                "weaknesses": {
                    "value": "1. The paper lacks justification for the adiabatic assumption. The paper creates specific settings for the experiments and makes some discussions, but are there many real-world scenarios in which such adiabatic assumptions can be applied? Also, there seems to require a more detailed formulation of such an adiabatic assumption. Say if the new information is too little, then it gets back to the ordinary training and there is no reason to do continual learning. Some mathematical formulation of the adiabatic assumption should be defined.\n\n2. In addition to the mathematical definition of the adiabatic assumption, it is also not clear why the selected settings in the experiments satisfy such an assumption. Why the tasks are chosen in such a way? From a more practical perspective, if we are dealing with real-world tasks, how do we check if the incoming task satisfy the adiabatic assumption and we can use the proposed method?\n\n3. The paper only tests for certain restricted settings and uses a relatively simple model (GMM). Though the paper mentions that there could be more advanced version of the GMM model that could solve the capacity problem, but there does not seem to be much evidence to support such a claim.\n\n4. It is claimed that the proposed method does not have the problem of scaling up as the number of tasks increases. However, for GMM, there is the number of clusters and I wonder should such a number be set according to the total number of tasks? If there are infinite number of classes coming in a stream, should the number of clusters also increase? Even though the change of size could be small, if we want to use a much more capable model as mentioned in the paper, will the model size go up as the tasks increase?\n\n\nMinor:\nPage 1: \"On the one hand, there are \u201dtrue\u201d replay\" (the quote symbols)"
                },
                "questions": {
                    "value": "Please check the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698483596170,
            "cdate": 1698483596170,
            "tmdate": 1699636296724,
            "mdate": 1699636296724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YlwpHW2wRc",
                "forum": "KQfCboYwDK",
                "replyto": "Y86loNu5o7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal reply"
                    },
                    "comment": {
                        "value": "Concerning your comments:\n\n1a. In virtually all articles about CL, the same amount of samples is added for each task, so the adiabatic assumption is automatically fulfilled after a few tasks. E.g., a 10-fold split of CIFAR along class boundaries will add 3000 samples per task, so at task t>=2 the number of new samples is smaller than the number of previous samples. So we feel the adiabatic assumption is adhered to to good approximation.\nOf course you could imagine tasks where this assumption is always violated, but we argue that learning in humans or robots get more and more adiabatic over time, since additions tend to have the same size and the pool of existing knowledge is ever-growing.\n\n1b. We will add a more formal statement of the adiabatic assumption. We will also add experiments on 5-fold and 10-fold equal splits to show that a violation does not break AR.\n\n2. The tasks are chosen such that the a.a. is always fulfilled, and are otherwise modeled after common continual learning benchmarks. As stated in 1b, a violation of the a.a. does not break AR, and we will show this by adding more experiments. From a practical perspective, it is very easy to detect whether a new task violates the a.a.: we just need to measure how many different existing GMM components are best-matching ones when exposed to the new task's data. If there are few of them, then the a.a. is valid. This will be discussed as well.\n\n3. We will remove the section about deep GMMs as this is not followed up in the article. Yes, there are deep convolutional extensions to GMMs that can be used, but thanks to latent replay, the flat GMMs do a sufficiently good job as shown by the experimental results.\n\n4a. No, our GMM does not grow over time. Instead, we choose K as high as our memory allows it because, as stated in app. B, more is always better when it comes to choosing K. This was shown in Gepperth&Pf\u00fclb(2021). If the number of classes is higher than K, performance will suffer, to be sure. So yes, we need to know or guess the total number of classes in advance, but any other DNN model needs to do this as well.\n\n4b. Even the most sophisticated DNN will fail for an infinite number of classes. Virtually *all* recent works on CL that we are aware of assume that the total number of classes is known beforehand, and all works use static DNNs/CNNs that do not grow in size. A priori knowledge of the number of classes is required for tuning generator and solver structure, as well as for choosing the size of the classification head. So our method is no different in that respect."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699794616364,
                "cdate": 1699794616364,
                "tmdate": 1699796661981,
                "mdate": 1699796661981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IXefEKuEpL",
                "forum": "KQfCboYwDK",
                "replyto": "Y86loNu5o7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Paper changes"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthis is what we adapted in the revised version of the paper. We hope this reflects the issues you raised, in any case it has made the paper a whole lot more solid.\n\n- formalized the assumptions AR makes in the introduction, and how to measure whether they are fulfilled. We also made it clear that AR does not \"break\" if these assumptions are not fulfilled.\n\n- included MIR and DGR-MerGAN as baselines. More was not possible to achieve given the time constraints.\n\n- introduced re-weighting of prior data in the loss for VAE-DGR and ER\n\n- performed where AR is used in a balanced replay scenario, i.e., the nr of samples increases linearly with the task as in vanilla deep generative replay. Since this did not really change the results in any way, we did not include these experiments in the results table. Instead, we added a paragraph to the discussion.\n\n- adapted our use of the term \"foundation models\" in favor of pre-trained feature extractors\n\n- discussed differences notably to MIR in the discussion section\n\n- compared constant-time replay in AR to how it is generally performed in related work\n\n- discussed additional related work, mainly  Klasson et al. (2023, TMLR; https://openreview.net/pdf?id=Q4aAITDgdP) and McClelland et al. (2020, Phil Trans R Soc B; https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0637) as conceptual foundations of our approach\n\n- added pseudocode for AR\n\nThanks again for contributiong your remarks!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738025276,
                "cdate": 1700738025276,
                "tmdate": 1700738025276,
                "mdate": 1700738025276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WL5qfqpfL7",
            "forum": "KQfCboYwDK",
            "replyto": "KQfCboYwDK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_m3N7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_m3N7"
            ],
            "content": {
                "summary": {
                    "value": "This paper first describes an important issue with replay in continual learning: whenever learning something new, it is generally required to replay all past tasks to avoid forgetting. This means there is an unbounded linear growth of to-be-replayed samples. The paper then proposes that this might be addressable with *adiabatic replay*: when learning something new, we should only replay samples \u2013 and only update the parts of the network \u2013 that are closely related to the newly learned information. To provide a proof-of-principle demonstration of this idea, the authors turn to GMMs in which they only replay and update the mixture component(s) most similar to the new data samples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an important problem in an original way. It is a fundamental limitation of current replay approaches that all tasks thus far need to be replayed. In the last few years there have been several continual learning studies that explored whether benefits could be gained by deciding what to replay in a smart way, but generally the conclusion of these studies has been that it is very hard to do better than simply doing balanced, random sampling from all tasks so far. This study takes a novel and promising approach to this problem by using GMMs, in an attempt to provide a proof-of-principle demonstration that it is possible to do better."
                },
                "weaknesses": {
                    "value": "**Issue 1: important comparisons are missing**\n\nI am afraid critical comparisons are missing for a convincing proof-of-principle demonstration that selective replay can provide substantial benefits. The paper compares AR (using GMMs) with DGR and ER (both using VAEs), and because AR performs better than DGR, the authors conclude that selective replay is beneficial. But I do not think that this can be concluded from this comparison. There are several important differences between AR and DGR \u2013 not only the use of selective replay, that could explain the difference in performance. Perhaps the authors would argue that DGR is a \u201cstate-of-the-art\u201d technique, and that showing improved performance with a method using selective replay would be enough demonstration, but I do not agree with that. Firstly, it is not clear how DGR is implemented and it seems this is not done in an optimal way (see issues 2 and 3). Secondly, the performance obtained by DGR is simply not good (e.g., for MNIST it is substantially lower than a linear classifier could obtain). Improvements on top of that are thus not necessarily meaningful.\nI would like to encourage the authors to instead include a comparison that directly assesses the benefit of using selective replay over normal replay (e.g., AR against the exact same version but using normal replay).\n\n**Issue 2: relevant past work is not discussed (appropriately)**\n\nDiscussion of past literature is insufficient, and some important aspects are ignored. Firstly, it is claimed at several places that current replay methods must replay an amount of samples proportional to the number of past tasks, but Van de Ven et al. (2020) empirically showed that it is possible to do better than this because \u201cpreventing forgetting is easier than learning\u201d. Although to make this possible, it is important that the loss terms from the replayed data and the current data are balanced appropriately. (It is also unclear to me whether this is done in the current study when a limited amount of data is replayed with DGR and ER, see also the last question under issue 3 below.) Another paper that would be good to discuss is Klasson et al. (2023, TMLR; https://openreview.net/pdf?id=Q4aAITDgdP), as they show that benefits can be obtained by being smart about what to replay. I think it is also relevant to discuss McClelland et al. (2020, Phil Trans R Soc B; https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0637), as they make a conceptual argument that it should not be necessary to replay everything.\n\nWhile I think it is important that these past studies on this topic are appropriately discussed, I do not think that the existence of these studies means that the novelty of the current study is compromised, as the current study approaches the problem in a novel and original way. In fact, these past studies illustrate the importance of the topic that is addressed by the current study.\n\n**Issue 3: insufficient experimental details**\n\n- How is the solver of AR trained? Is the solver also trained with \u201cadiabatic replay\u201d? Or is the solver trained with \u201cregular replay\u201d, and suffers from a linear increase in the amount of replayed data? How are the labels for the replayed data obtained?\n\n- For DGR, how are the labels obtained for training the solver?\n\n- For ER and DGR, how are the loss terms from the replayed data and the current data weighed? Are they balanced as in Van de Ven et al. (2020), or are they simply added?\n\nMinor issues:\n\n- Many of the in-text citations are formatted incorrectly\n\n- Towards top of p3: Maximally interference replay -> Maximally interfered replay\n\n- Bottom of p4: the notation DX-Y is used, but has not (yet) been introduced\n\n- Given the importance of ER in the current manuscript, the buffer size that is used should be mentioned in the main text"
                },
                "questions": {
                    "value": "For the main issues to address, please see the first three issues raised under \u201cWeaknesses\u201d.\n\nAs I think this paper takes an original angle to an important problem, I hope the authors will be able to address these raised issues.\n\nI would be happy to actively engage in the discussion period."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663742151,
            "cdate": 1698663742151,
            "tmdate": 1699636296641,
            "mdate": 1699636296641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o6hBYU9EXf",
                "forum": "KQfCboYwDK",
                "replyto": "WL5qfqpfL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal reply"
                    },
                    "comment": {
                        "value": "Dear reviewer, thanks for the constructive review! Here are some of our thought w.r.t. the weaknesses you raised.\n\n1a. All DGR and ER experiments are conducted in the constant-time setting, i.e., as many generated/replayed samples as there are new samples.  Normally, one would train DGR in a balanced fashion, i.e., generate more samples for each new task. Since we do this differently, the results for DGR are \"not good\", as you remarked. However, this ensures that the comparison to AR is fair, and this is why we conclude that selective  replay is beneficial since DGR obviously cannot cope with constant-time replay.\n\n1b. To show that our DGR implementation is sound, we will include results when  DGR is trained in the  standard fashion. We actually did this long ago, and results are indeed stronger and more in agreement with the literature.\n\n1c. We will compare  AR to the exact same version but with normal replay. This will probably not change the results, and thus prove that selective replay performs similarly to normal replay, but at far better efficiency. \n\n2. We will discuss these references appropriately. \n\n3. We will include these experimental  details which are indeed a bit shakily described. \n* The AR \"solver\", which is just a linear classifier, is trained with AR as well and thus does not compromise AR's sample efficiency.\n* The labels are obtained by running the generated samples through the solver. We experimented with conditional replay but found that this enormously impaired the quality of generated samples. \n* The ER buffer  size is 50. \n* The loss terms are not weighted: all generated samples have the same weight in the loss. The reason: we found at several occasions that specific weights for each replay task can lead to complications. As more and more tasks are added, the weights for long-ago tasks must be chosen higher and higher. If coefficients are too high for a given task, gradient descent can fail completely because the effective learning rate is too high. In addition, we feel it would be unfair if task-specific weights were used for DGR and ER, since they are neither used nor required for AR, resulting in a biased comparison. We can of course include such results for reference."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699796581934,
                "cdate": 1699796581934,
                "tmdate": 1699881416114,
                "mdate": 1699881416114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PvCXjhWtUj",
                "forum": "KQfCboYwDK",
                "replyto": "o6hBYU9EXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Reviewer_m3N7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Reviewer_m3N7"
                ],
                "content": {
                    "title": {
                        "value": "Response to initial author rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the fast response. At the moment, I have two questions/comments in response.\n\n{1} Could you provide some intuition why it is OK for the AR \"solver\" to be trained with AR? Why does this solver not suffer from the problem of data imbalance?\n\n{2} I do not agree with your claim that it would be \"unfair\" to use \"task-specific weights\" for DGR and ER. This approach does not use additional information, it is simply a way to decouple the amount of samples that is replayed and the relative importance that they are given. I agree that using such task-specific weights can have problems as well -- that is one of the reasons why I am excited about the approach proposed in your paper, but it seems clear that using such task-specific weights is a more sensible approach than letting the weight of the replay loss being determined by the amount of samples that can be replayed. So I think that using the task-specific weights is the more important baseline relative to which to demonstrate improvements.\n\nOtherwise, I look forward to seeing the outcome of points 1c and 2."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060550035,
                "cdate": 1700060550035,
                "tmdate": 1700060550035,
                "mdate": 1700060550035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jMJFj5OPcb",
                "forum": "KQfCboYwDK",
                "replyto": "WL5qfqpfL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Paper changes"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthis is what we adapted in the revised version of the paper. We hope this reflects the issues you raised, in it has made the paper a whole lot more solid. And btw: thanks a lot for those two references, namely the McClelland&al., paper we did not have that on our radar...\n\n- formalized the assumptions AR makes in the introduction, and how to measure whether they are fulfilled. We also made it clear that AR does not \"break\" if these assumptions are not fulfilled.\n\n- included MIR and DGR-MerGAN as baselines. More was not possible to achieve given the time constraints.\n\n- introduced re-weighting of prior data in the loss for VAE-DGR and ER\n\n- performed where AR is used in a balanced replay scenario, i.e., the nr of samples increases linearly with the task as in vanilla deep generative replay. Since this did not really change the results in any way, we did not include these experiments in the results table. Instead, we added a paragraph to the discussion.\n\n- adapted our use of the term \"foundation models\" in favor of pre-trained feature extractors\n\n- discussed differences notably to MIR in the discussion section\n\n- compared constant-time replay in AR to how it is generally performed in related work\n\n- discussed additional related work, mainly  Klasson et al. (2023, TMLR; https://openreview.net/pdf?id=Q4aAITDgdP) and McClelland et al. (2020, Phil Trans R Soc B; https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0637) as conceptual foundations of our approach\n\n- added pseudocode for AR\n\nThanks again for contributing your remarks!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737994410,
                "cdate": 1700737994410,
                "tmdate": 1700737994410,
                "mdate": 1700737994410,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o3dXSWN4XX",
            "forum": "KQfCboYwDK",
            "replyto": "KQfCboYwDK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_ew63"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_ew63"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on replay-based approaches to continual learning and proposes a new method called AR which results in the following contributions:\n\n1. Selective replay: Previous knowledge is not replayed indiscriminately, but only where significant overlap with new data exists.\n\n2. Selective update: Previous knowledge is only modified by new data where an overlap exists.\n\n3. Near-Constant time complexity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The high time cost of replay methods is a significant problem in continual learning. I appreciate the motivation of this paper which tries to solve this problem by GMM."
                },
                "weaknesses": {
                    "value": "1. The writing needs to be improved.\n\n2. I usually don't focus too much on the experiments, but i do believe that the experiments should be improved where more baselines and time cost experiments are necessary.\n\n3. The reasonable way of chosing hyperparameter $K$ is necessary. Please discuss more about $K$.\n\n4. Lack of novelty. The key technique in this paper is well-known.\n\n5. It seems that the main text of this paper is over the limit of 9 pages, where Section 5.1 is in page 10.\n\nDetails of my concerns are listed in Questions."
                },
                "questions": {
                    "value": "First, I am concerned about the writing and the presentation of this paper. Specifically, the descriptions of proposed method should be transparency and easy to follow. It is better to show your method by pseudo codes.\n\nSecond, I am concerned about the proposed method AR. \n\n1. Is the hyperparameter $K$ fixed and how to determine a reasonable $K$? \n\n2. In my view, one component of GMM corresponds to one distribution of several classes. When $K>$ the number of seen classes, is it possible to chose an unknown component at the query step and what dose the generated samples look like in this scenario? It seems that the ability of preventing forgetting is determined by $K$. When there are so many classes whose number $\\gg K$, it is hard to prevent forgetting due to the overlapping. If I was wrong, please correct it.\n\n Thrid, I usually don't focus too much on the experiments, but i do believe that the experiments should be improved.\n\n1. There are only two old replay methods (DGR and ER). It is better to compare it with more and new replay baselines.\n\n2. In my opinion, the potential low time cost of proposed method is a significant advantage. It is better to demonstrate this by more time cost experiments.\n\n3. The splited tasks $T_2, T_3, \\dots$ contain only one class. How dose the proposed method perform if there are more classes in one task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750730359,
            "cdate": 1698750730359,
            "tmdate": 1699636296574,
            "mdate": 1699636296574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Mm1udziISG",
                "forum": "KQfCboYwDK",
                "replyto": "o3dXSWN4XX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal reply"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthank you for your evaluation of our paper, this is valuable  advice for improving the paper. \nIn order to do this, we would ask you to provide a bit more information w.r.t. a few points you raise.\n\nWeaknesses:\n1. we will add pseudocodes, no problem. Could you be more specific in what ways the writing should be improved?\n2. What baselines are you referring to? Could you provide references?\n3. As stated in appendix B, we use the best-practice procedures from Gepperth&Pf\u00fclb(2022) for choosing the hyperparameters of the GMM. In this reference, it is shown through experimentation that K follows a \"the more the better\" principle. This is also stated in appendix B, but we will include this info and a citation in the main text.\n4. You do not seem to provide a reference for your claim, what works are you referring to? \n5. It is clearly stated in the ICLR 2024 author's guidelines that the reproducibility statement does not count towards the page limit. Please be sure to take this into account in your evaluation.\n---\nQuestions concerning AR:\n1. see above\n2. It is true that K must be >= #(classes), so there is at least one component per class. So we need to guess the total #(classes) beforehand. But: with DNNs, we need to choose appropriate layer sizes beforehand, too, so the situation is the same. For solvers  and generators, this is usually done by cross-validation. For our method, no cross-validation is required.\n\nQuestions concerning experiments:\n1. again, we would be grateful for references here. There are many variants of generative replay, some are very specific to certain settings. \n2. We will include a table with runtime measurements. The time cost will be basically proportional to the nr. of samples in Fig. 4.\n3. There is no problem at all with more than one class per task. We will include splitMNIST and splitCIFAR (5-fold split with two classes per split) experiments to demonstrate this."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699793103606,
                "cdate": 1699793103606,
                "tmdate": 1699796763368,
                "mdate": 1699796763368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8ANWCEcmPG",
                "forum": "KQfCboYwDK",
                "replyto": "Mm1udziISG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Reviewer_ew63"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Reviewer_ew63"
                ],
                "content": {
                    "title": {
                        "value": "More comments"
                    },
                    "comment": {
                        "value": "Thank you for your reply. According to your responses, I provide some additional comments.\n\n1. To my knowledge, I recommend some replay baselines: MeRGAN [1], SPB [2], PASS [3]. I think more comparisons will reflect more advantages of your proposal.\n\n2. The key technique is GMM which is well-known. It is likely that this paper is a A+B work which leverages GMM in CL. It is OK but not surprising.\n\n3. I am still a bit concerned about the proposed AR. \n\na. Since $K$ must be $\\geq$ #(classes), is it possible to chose an unknown component at the query step and what dose the generated samples look like in this scenario? \n\nb. It seems that the number of classes are fixed before the entire training. This may indicate that you implement your method by fixing the class number. I think this is not a valid way of implementing the class incremental scenario. Please refer to the implementing way of [4].\n\n[1] Memory Replay GANs: learning to generate images from new categories without forgetting. NeurIPS 2018.\n\n[2] Striking a balance between stability and plasticity for class-incremental learning. ICCV 2021.\n\n[3] Prototype augmentation and self-supervision for incremental learning. CVPR 2021.\n\n[4] Mnemonics Training: Multi-Class Incremental Learning without Forgetting, CVPR 2020.\n\nIf authors considered my comments and improved this work well, I think this paper is worth of rating 6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699808397997,
                "cdate": 1699808397997,
                "tmdate": 1699808397997,
                "mdate": 1699808397997,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kcRGpFFDiw",
                "forum": "KQfCboYwDK",
                "replyto": "o3dXSWN4XX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to reviewer questions"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthanks a lot for your clarification, this is very helpful. Concerning your remarks:\n\n(1a) we will discuss references [1],[2],[3].\n\n(1b) we do not seem to be able to get the code in the repositories provided in [1,2,3] to run (for SPB, the provided repository seems to be for a different paper), so we are currently re-implementing MerGAN and SPB. To compensate for this, we include baseline measurements for Maximally Interfered Retrieval (NeurIPS 2019). \n\n(3a) At every task, all components are used, so it is not possible to choose an unknown/unconverged component. For each subsequent task, components will be partially re-allocated to describe new classes.\n\n(3b) We do not preallocate components to certain classes (this is how we interpret your question,did we understand you correctly?). Neither do we make any other assumptions regarding classes other than the maximal number of classes that will occur. This is a common assumption that all other works make, even [4]. Essentially, this information is needed to determine the size of the single-head classifier (which we use as well, not a multi-head one).\nComponent re-allocation for each new task is a dynamic process where AR adapts those components which are most similar to the new tasks' data. And leaves the ones that are dissimilar unchanged. However, components are not simply overwritten but must now interpolate between past and new knowledge."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143309372,
                "cdate": 1700143309372,
                "tmdate": 1700427430488,
                "mdate": 1700427430488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9PYMDjGwYR",
                "forum": "KQfCboYwDK",
                "replyto": "o3dXSWN4XX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Paper changes"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthis is what we adapted in the revised verison of the paper. We hope this reflects the issues you raised, in any case it has made the paper a whole lot more solid.\n\n- formalized the assumptions AR makes in the introduction, and how to measure whether they are fulfilled. We also made it clear that AR does not \"break\" if these assumptions are not fulfilled.\n\n- included MIR and DGR-MerGAN as baselines. More was not possible to achieve given the time constraints.\n\n- introduced re-weighting of prior data in the loss for VAE-DGR and ER\n\n- performed where AR is used in a balanced replay scenario, i.e., the nr of samples increases linearly with the task as in vanilla deep generative replay. Since this did not really change the results in any way, we did not include these experiments in the results table. Instead, we added a paragraph to the discussion.\n\n- adapted our use of the term \"foundation models\" in favor of pre-trained feature extractors\n\n- discussed differences notably to MIR in the discussion section\n\n- compared constant-time replay in AR to how it is generally performed in related work\n\n- discussed additional related work, mainly  Klasson et al. (2023, TMLR; https://openreview.net/pdf?id=Q4aAITDgdP) and McClelland et al. (2020, Phil Trans R Soc B; https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0637) as conceptual foundations of our approach\n\n- added pseudocode for AR\n\nThanks again for contributing your remarks!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737922272,
                "cdate": 1700737922272,
                "tmdate": 1700737922272,
                "mdate": 1700737922272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X4kqoWZCSc",
            "forum": "KQfCboYwDK",
            "replyto": "KQfCboYwDK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_M6q9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3444/Reviewer_M6q9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes adiabatic replay, a method for replay in continual learning that tailors the replay sample retrieval step to the incoming samples to be learned. This approach can theoretically avoid replaying all previous tasks, by focusing only on the modes of the distribution which overlap the most with the new task in the model's latent space. The proposed approach is a variant of deep generative replay, using a VAE with a GMM prior as a generative model. The authors argue that this choice of prior is better suited for *variant generation* which enables sampling of datapoints most likely to be interfered from the new task. The authors explore their method in offline class incremental settings, comparing to ER and DGR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  The motivation for the paper is sound : there is significant value in carefully selecting the replay data and avoid full replay of past samples, as the latter is very computationally demanding."
                },
                "weaknesses": {
                    "value": "1.  Overall, the paper's empirical execution is weak. The experimental protocol is limited to small sequences of tasks using arbitrarily small models. There is no discussion about the computational cost of the method. This is disappointing, as I do believe that the essence of the method has potential in different settings (for example targeting compute restricted settings, with pretrained models). \n2. The baseline numbers are very weak, which raises concerns about the empirical rigor of the work. For example, deep generative replay with a VAE of similar complexity in MIR [1] gets 80% on split mnist, *trained online* with 2 classes per task, which is arguably a much more difficult setting than the one in the paper. On that note, the paper would benefit from including this baseline in the paper. \n3. To the best of my understanding, the conceptual differences with MIR are small : the idea of fetching points close in the VAE's latent space is exactly what MIR does. In other words, AR is similar to Gen-MIR where 0 gradient ascent steps to maximize interference are done. \n4. The paper could benefit from some tweaks in the main paper. For example, it is unclear at which level latent replay operates; adding a detailed figure would greatly aid understanding. Moreover, I think the authors' interpretation of Foundation Models are merely any pretrained model, rather than a \"generalist\" model trained on vast, diverse data."
                },
                "questions": {
                    "value": "1. What is the buffer size used for ER ?\n2. When is latent replay (vs standard replay) performed ? it seems that only SVHN and CIFAR use latent replay ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698963837515,
            "cdate": 1698963837515,
            "tmdate": 1699636296504,
            "mdate": 1699636296504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Rp6XhUpEqZ",
                "forum": "KQfCboYwDK",
                "replyto": "X4kqoWZCSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal reply"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthere are some points to be made w.r.t. weaknesses you address:\n\n1a. We are not sure what you mean by \"arbitrarily small models\". The baseline models, e.g., in VAE-replay or the solvers in ER, are comparable to those used in related work. The GMMs are complex enough to solve the task at hand, which is all that should matter.\n\n1b. The \"hardness\" of a CL task is mainly due to the number of tasks, not due to their inherent complexity. By selecting CL tasks that at least 5  additional tasks after the first one, we ensure a sufficient level of difficulty. The usual tasks in the literature, like 2-2-2-2-2 splits on CIFAR or MNIST, can be included without any problems. We will update the paper accordingly.\n\n1c. Fig. 4 directly addresses the computational cost. We do not measure execution times because those are very dependent on hardware, but complexity as a function of the # of tasks. This will be directly proportional to actual execution times.\n\n2. The baselines for VAE-replay are not comparable to those reported elsewhere, since we are training in the compute-restricted constant-time setting. I.e., we generate a constant number of samples for each task, instead of increasing this number at each task, see also Fig. 4. If we used a balanced replay strategy, of course the numbers would be a bit better but it would no longer be a fair comparison.\n\n3. The key difference to MIR is selective updating, which the VAEs in GEN-MIR cannot do. They have to be trained with a balanced dataset. The idea of selective replay is of course quite similar, although our method does not require gradient descent to do it.\n\n4. Point taken, this will be incorporated. \n-----\n\nConcerning your questions:\n1. Buffer size is 50\n2. Correct, latent replay is done for CIFAR and SVHN."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699777481277,
                "cdate": 1699777481277,
                "tmdate": 1699796728147,
                "mdate": 1699796728147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TXDzuhMJyM",
                "forum": "KQfCboYwDK",
                "replyto": "Rp6XhUpEqZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Reviewer_M6q9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Reviewer_M6q9"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "Thank you for your answers. \n\n1a. I respectfully disagree; I don't think that getting 80% accuracy on MNIST tasks is solved. \n1c. (please correct me if I am wrong) Fig. 4 indicates how many samples are required to create a balanced stream. What I would like to see instead is how many samples are required for DGR and AR to reach the same performance *in practice*.  \n\n2. In most existing online CL literature (e.g. MIR), approaches do operate in the compute-restricted constant-time setting, where the number of replay steps or replay compute does not grow as a function of number of tasks seen. A standard practice is to allocate 50% of the compute to replay and 50% to learning the current task. Hence I don't see why the numbers in your setting (which does multiple epochs and is more compute hungry) shows lower numbers. \n\n3. Again, this is *false*. MIR does not build balanced datasets. \n\nThank you for clarifying my other questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173523477,
                "cdate": 1700173523477,
                "tmdate": 1700173523477,
                "mdate": 1700173523477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3bgXSrpVOT",
                "forum": "KQfCboYwDK",
                "replyto": "X4kqoWZCSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "Thank you for your comments, we are glad to start a discussion. \nConcerning the issues you raise:\n\n1a. Well, the task is not solved, obviously, but 80% on MNIST in the compute-restricted setting is what many other approaches obtain, too. MIR, for example, does 82% on average on MNIST for the 2-2-2-2-2 split. So we believe our models in general do not do worse than those of other approaches. Hence we believe we do not use \"arbitrarily small models\" but models of similar capacity as other approaches do.\n\n1c. Point taken, we could determine the amount of samples to be replayed per task via cross-validation. We do not do this because cross-validation in CL is not really admissible (even if many articles use it) since it requires advance knowledge of all tasks. If we do not make use of such a \"look into the future\", one can reasonably use either the constant-time setting or the balanced setting.\n\n2.Our issue with the constant-time settings used in other approaches is that samples are replayed indiscriminately, even though the number of samples remains constant. So, over time, the number of replayed samples for each past task will go to 0, at which point forgetting is ensured. \n\n3. Thank you for pointing this out! Indeed MIR uses a constant-time replay strategy, we will adapt this part of the text. The balancing in MIR is achieved by giving replayed samples a higher weight in the loss. However, the weight (3.0 in the case of MNIST 5-fold split) has to be chosen via cross-validation over all tasks, which we consider problematic in CL (see above). it is highly sensible to the task setup: if we train MIR on MNIST using a 10-fold split with the same settings, the final accuracy drops to 65%, with the \"oldest\" classes showing very high forgetting. So this balancing-via-loss-weights approach has issues as well, in addition to issues mentioned in 2. In our approach, the generated and the current samples have the same weight in the loss, which avoids tuning this  hyper-parameter."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415835448,
                "cdate": 1700415835448,
                "tmdate": 1700427925690,
                "mdate": 1700427925690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DEuOI0LYqf",
                "forum": "KQfCboYwDK",
                "replyto": "X4kqoWZCSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3444/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Paper changes"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nthis is what we adapted in the revised verison of the paper. We hope this reflects the issues you raised, in any case it has made the paper a whole lot more solid.\n\n- formalized the assumptions AR makes in the introduction, and how to measure whether they are fulfilled. We also made it clear that AR does not \"break\" if these assumptions are not fulfilled.\n\n- included MIR and DGR-MerGAN as baselines. More was not possible to achieve given the time constraints.\n\n- introduced re-weighting of prior data in the loss for VAE-DGR and ER\n\n- performed where AR is used in a balanced replay scenario, i.e., the nr of samples increases linearly with the task as in vanilla deep generative replay. Since this did not really change the results in any way, we did not include these experiments in the results table. Instead, we added a paragraph to the discussion.\n\n- adapted our use of the term \"foundation models\" in favor of pre-trained feature extractors\n\n- discussed differences notably to MIR in the discussion section\n\n- compared constant-time replay in AR to how it is generally performed in related work\n\n- discussed additional related work, mainly  Klasson et al. (2023, TMLR; https://openreview.net/pdf?id=Q4aAITDgdP) and McClelland et al. (2020, Phil Trans R Soc B; https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0637) as conceptual foundations of our approach\n\n- added pseudocode for AR\n\nThanks again for contributing your remarks!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737895177,
                "cdate": 1700737895177,
                "tmdate": 1700737895177,
                "mdate": 1700737895177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]