[
    {
        "title": "Neurosymbolic Grounding for Compositional Generalization"
    },
    {
        "review": {
            "id": "B7RJbq2egb",
            "forum": "4KZpDGD4Nh",
            "replyto": "4KZpDGD4Nh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4021/Reviewer_QCEm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4021/Reviewer_QCEm"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a framework for object-centric world modeling in compositional generalization settings. They propose a neuro-symbolic scene encoding, consisting of real vectors and vectors of symbols describing attributes, as well as a neuro-symbolic attention mechanism, which binds entities to rules of interaction. They use foundation models to extract each entity\u2019s symbolic attributes, and show their method\u2019s performance on a block-pushing domain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I am appreciative of the neural-symbolic attention mechanism, computed between ordered symbolic and neural rule encodings to determine the most applicable rule-slot pair. I also appreciate that this is end-to-end differentiable as a permutation equivariant action attention module. Using a frozen foundation model to capture the symbolic attributes in the scene encodings is also a simple but elegant way of decomposing entities into attributes."
                },
                "weaknesses": {
                    "value": "W1. I am not convinced that the proposed neuro-symbolic scene encoding is the optimal formulation. I would have appreciated ablations in this paper, including ones that explored whether or not having both real vectors and a vector of symbolic attributes is actually helpful. What should the real vector capture that the symbolic attribute is not capturing, especially given that the rules of the evaluation domain seem to only rely on these attributes?\n\nW2. An additional note is that I would have appreciated more clarity of what information is given to the model, and whether other methods have access to the same information. From my understanding, a strong assumption is that the symbolic labeling module requires a predefined list of attributes that is important for the downstream task. I think in many downstream tasks this may not be reasonable to know a priori. Potentially, you can run experiments showing that with a superset of attributes, directly predicted by some foundation model, that you can still learn the correct correspondence to rules given this noise. \n\nW3. Similarly, are there assumptions made on how many rules there are in the evaluation domain? I understand that the rule is a learnable encoding, but is the amount of rules learned as well? One can imagine that the method discovers rules that are correct, but not optimal (e.g., decompose rules into many smaller rules that overfit to the train set). \n\nW4. I would have appreciated evaluation on a different domain, such as maybe Physion, and learn more complex and less obvious rules such as rigid and soft-body collisions, stable multi-object configurations, etc. In the block-pushing domain, it seems like the rules are tied to clear attributes such as shape and color, without having to further learn whether non-uniform combinations of these would lead to certain downstream effects. Also related to the known attributes assumption in W2."
                },
                "questions": {
                    "value": "Q1. Can you clarify how SAM is used with ResNet to produce a set-structured hidden representation for each object? \n\nQ2. Is there a way to interpret the learned rules and qualitatively see how well it aligns with the ground truth rules?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Reviewer_QCEm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698285565261,
            "cdate": 1698285565261,
            "tmdate": 1699636364572,
            "mdate": 1699636364572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zdTJkn3Uqp",
                "forum": "4KZpDGD4Nh",
                "replyto": "B7RJbq2egb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QCEm"
                    },
                    "comment": {
                        "value": "Thank you for reading our paper in detail. We address concerns inline:\n\n> *I am not convinced that the proposed neuro-symbolic scene encoding is the optimal formulation. I would have appreciated ablations in this paper, including ones that explored whether or not having both real vectors and a vector of symbolic attributes is actually helpful. What should the real vector capture that the symbolic attribute is not capturing, especially given that the rules of the evaluation domain seem to only rely on these attributes?*\n\nWe appreciate your insightful comment regarding the neuro-symbolic scene encoding. In our paper (Section 4, Evaluation), we included the AlignedNPS baseline as a \u201cfully neural\u201d ablation to demonstrate the effectiveness of our model. Based on your suggestion, we have now conducted additional experiments with a \u201cfully symbolic\u201d ablation. The results, detailed below, indicate that the \u201csymbols-only\u201d model significantly underperforms compared to COSMOS. We believe this is because the symbolic embedding is constructed by concatenating symbolic attributes, and the rule module is not aware of this structure. This causes the MLP to overfit to the attribute compositions seen at train time. COSMOS sidesteps this issue by using the symbolic embedding in the key-query attention module to select the relevant rule module, while allowing the real vector to learn local features useful for modeling action-conditioned transitions. We will update our writing in Section 4, so this is more clear.\n\n| **Dataset**          | **Only Symbols (MSE)** | **Only Neural / AlignedNPS (MSE)** | **COSMOS (MSE)** |\n| -------------------- | ---------------------- | ---------------------------------- | ---------------- |\n| 3 Object OC - Sticky | 1.36E-02               | 1.14E-02                           | **4.23E-03**     |\n| 3 Object OC - Team   | 1.39E-02               | 1.24E-02                           | **4.60E-03**     |\n| 3 Object SC          | 1.21E-02               | 3.51E-03                           | **7.66E-04**     |\n\n> *W2. An additional note is that I would have appreciated more clarity of what information is given to the model, and whether other methods have access to the same information. From my understanding, a strong assumption is that the symbolic labeling module requires a predefined list of attributes that is important for the downstream task. I think in many downstream tasks this may not be reasonable to know a priori. Potentially, you can run experiments showing that with a superset of attributes, directly predicted by some foundation model, that you can still learn the correct correspondence to rules given this noise.*\n\nWe use a closed vocabulary with 6 colors and 5 shapes that we used to generate the dataset. These colors and shapes were used for all models. We appreciate the suggestion from the reviewer, and ran the suggested experiment with a superset of attributes. Specifically, we increased the number of colors to 10 colors and the number of shapes to 9 shapes. Due to compute and time limitations during the rebuttal, we ran this for one experimental configuration where we retrained an open vocabulary ablation of our model for a fixed number of epochs (50). The preliminary results indicate that the performance remains comparable even with the expanded set of attributes. This finding suggests that our model can adapt to a broader range of attributes without significant loss in performance. We will include these discussions in the final version of the paper in Section 4.\n\n|                                      | COSMOS(Validation MSE) | COSMOS-OpenVocab(Validation MSE) |\n| ------------------------------------ | ---------------------- | -------------------------------- |\n| OC-Sticky 3 objects(After 50 epochs) | 6.17E-3                | 6.03E-3                          |\n\n\n\n> *W3. Similarly, are there assumptions made on how many rules there are in the evaluation domain? I understand that the rule is a learnable encoding, but is the amount of rules learned as well? One can imagine that the method discovers rules that are correct, but not optimal (e.g., decompose rules into many smaller rules that overfit to the train set).*\n\nThe number of rules is fixed ahead of time and was selected based on what empirical performance - we use 5 rules and 3 application steps. Each rule embedding is paired with a rule module. These rule modules are implemented as MLP\u2019s and have the capacity to model many action/slot interactions.\n\n*(continued in next thread)*"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160947186,
                "cdate": 1700160947186,
                "tmdate": 1700160947186,
                "mdate": 1700160947186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qT5pRdanvH",
                "forum": "4KZpDGD4Nh",
                "replyto": "B7RJbq2egb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QCEm (continued; part 2)"
                    },
                    "comment": {
                        "value": ">  *Q1. Can you clarify how SAM is used with ResNet to produce a set-structured hidden representation for each object?*\n\nWe prompt SAM with the image ($I$) and a 8x8 grid of points. This yields 64*3 potential masks (as there are three channels). To ensure a set structured representation, we must ensure that (1) each mask captures a specific property of the image, (2) collectively, all masks describe the entire image. We ensure (1) by removing duplicate masks and (2) by evaluating all combinations of remaining slots and selecting the $k$ tuple (where $k$ is the number of slots) that, when summed, most closely matches the image. The resulting masks $\\{M_1, M_2\\dots M_k\\}$ are point-wise multiplied with the image to yield $\\{I_1, I_2, \\dots I_k\\}$. Each image is passed through a Resnet pretrained on Imagenet with the final layer finetuned to yield $\\{S_1, S_2, \\dots S_k\\}$. We will add these implementation details to the Appendix.\n\n>  *Q2. Is there a way to interpret the learned rules and qualitatively see how well it aligns with the ground truth rules?*\n\nWe don\u2019t expect the rules to be interpretable or correspond to ground truth rules because the rule modules are implemented as MLPs and usually encode many state/action transitions. The modular architecture can be utilized to facilitate a more interpretable design \u2013 for instance, by using linear layers instead of MLPs for rule modules. We leave this direction on interpretability for future work.\n\n**References**\n\n- `[1]`: http://arxiv.org/abs/1911.12247 (ICLR 2019)\n- `[2]`: https://arxiv.org/abs/2103.01937 (NeurIPS 2021)\n- `[3]`: https://arxiv.org/abs/2204.13661 (ICML 2022)\n- `[4]`: https://arxiv.org/abs/2107.00848 (ICLR 2021)\n- `[5]`: https://arxiv.org/abs/2210.05861 (ICLR 2023).\n- `[6]`: https://arxiv.org/abs/2301.04104 (Preprint)\n- `[7]`: https://arxiv.org/abs/2303.05501 (NeurIPS 2022)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161037515,
                "cdate": 1700161037515,
                "tmdate": 1700161037515,
                "mdate": 1700161037515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ZF5gCpHXU",
                "forum": "4KZpDGD4Nh",
                "replyto": "qT5pRdanvH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Reviewer_QCEm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Reviewer_QCEm"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the thoughtful response! I think it is promising that the open vocabulary ablation seems to yield comparable performance, though I would like to see the final results. On the other hand, I believe that fixing the number of rules ahead of time is a limitation, and limits the application of this method to real world tasks. Hence, I am keeping my score as is."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513887816,
                "cdate": 1700513887816,
                "tmdate": 1700513887816,
                "mdate": 1700513887816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OM51e16qWb",
            "forum": "4KZpDGD4Nh",
            "replyto": "4KZpDGD4Nh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4021/Reviewer_CQ89"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4021/Reviewer_CQ89"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a neurosymbolic approach to learning a transition model in the pixel space. The proposed model, namely Cosmos, takes in the current state as an image, the action (encoded as one-hot vectors), and predicts the next state. The model is trained on a dataset of state-action-state transition tuples and evaluated on unseen state-action combinations. The model focuses on two types of compositional generalization tests: scene composition and object composition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper tackles an important problem that is interesting to the ICLR community, specifically learning world models from pixels. The overall presentation of the paper is good. The organization of the method section clearly illustrates the number of modules in the system and how they are connected with each other, which is obviously helpful for readers. The description of experimental setups is clear, and the authors have done a sufficient number of ablation studies and comparisons with baselines."
                },
                "weaknesses": {
                    "value": "There are two main weaknesses of the paper.\n\nFirst, the problem setting of object composition seems very contrived to me, for two reasons.\n- In the physical world, it is unclear what's a concrete example where such kind of metarules would apply. In particular, the authors are training the model on seeing two red blocks moving together and two green blocks moving together, and hope that the model would generalize to predict two blue blocks would also move together. Such kind of \"attribute-relationship\" based generalization doesn't seem natural to me. Arguably, this kind of generalization can be dangerous: two blocks can be stacked together; two cylinders can be stacked together, but not two spheres, in the physical world.\n- There is some serious machine learning identifiability issue with this setting. If the model does not have inductive biases in training, there is no way that it can generalize.\n--- Based on these two concerns, the arguments around object compositional generalization is weak.\n\nSecond, the model is only trained and evaluated on a fairly toy environment, and the downstream application to planning is only shown in a very simple setting. It is unclear how this approach can be generalized to more complicated scenarios.\n\nSlightly minor is that the paper missed some important related work along the direction of learning neuro-symbolic transition models. For example,\n- PDSketch: Integrated Planning Domain Programming and Learning https://arxiv.org/abs/2303.05501\n- Learning Object-Oriented Dynamics for Planning from Text https://openreview.net/forum?id=B6EIcyp-Rb7\n\nThey are not exactly the same setting but a lot of the high-level ideas are definitely the same, including learning lifted transition rules, using factorized embeddings (colors, shapes, etc.) to represent objects.\n\nWhile I overall like the presentation of the paper---it's well-organized and overall good. I found the description of some details very unclear. In particular:\n- Page 3, the object composition part. I have to check the appendix and read through paragraphs/figures several times in order to understand what this object composition means. I think the name is not very descriptive. The authors should consider change it to a better name that describes such kind of \"metarules\" (e.g., two objects have relations if they share the same color) and present concrete examples in the main text.\n- Parge 4-5: the authors should keep the \"...\" in the sets. Otherwise it's very confusing to look at \"{c1, c2, cp}\"\n- The writing of the method section could be further improved by having a running example (and referring back to this example in 3.1, 3.2, and 3.3).\n\nFinally, the paper does not have a limitation discussion section.\n\nMinor notes on the writing: I think using CG as an acronym for \"compositional generalization\" is a bit uncommon. The term is easily confused with other concepts like \"computer graphics.\""
                },
                "questions": {
                    "value": "I don't have particular questions. Please address the missing related works; and consider reframing and better illustrating the object compositional generalization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Reviewer_CQ89"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778415888,
            "cdate": 1698778415888,
            "tmdate": 1700523711192,
            "mdate": 1700523711192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "foIRhQMOSk",
                "forum": "4KZpDGD4Nh",
                "replyto": "OM51e16qWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CQ89"
                    },
                    "comment": {
                        "value": "Thank you for reading our paper in detail. We address concerns inline:\n\n> *The problem setting of object composition seems very contrived to me.*\n\nOur definition of \u201cobject compositionality\u201d refers to compositions of objects in any setting where entity dynamics are dependent on attributes of other entities. This problem setting commonly occurs in real-world scenarios (ex: object stacking, team sports, autonomous driving, modeling almost any type of agent interactions, etc.). On the other hand, scene compositionality is where entity dynamics are independent of the dynamics of other entities. We evaluate on the same benchmark as existing work `[1, 2, 3]` (at ICLR 2019, NeurIPS 2021, ICML 2022 respectively) and choose our evaluation domain based on four properties: (1) object-oriented state and action space, (2) history-invariant dynamics, (3) action conditioned (plannable) dynamics, and (4) ease of generating new configurations (to evaluate scene and object composition). See the detailed response below.\n\n>  *In the physical world, it is unclear what's a concrete example where such kind of metarules would apply\u2026Such kind of \"attribute-relationship\" based generalization doesn't seem natural to me.*\n\nThere are many instances where composition of objects are governed by observable attributes. For instance, \u201cteam composition\u201d naturally occurs in sports (soccer players wearing the same colored jersey move in similar ways), autonomous driving (yellow taxi cabs share macro-behaviors), object manipulation tasks (color-coded gas cylinders or color-coded wires must be handled in the same way, etc.) \u201csticky composition\u201d occurs naturally in pick-and-place tasks.\n\n> *In particular, the authors are training the model on* **[(1)]** *seeing two red blocks moving together and two green blocks moving together, and hope that the model would generalize to predict two blue blocks would also move together. Arguably, this kind of generalization can be dangerous:* **[(2)]** *two blocks can be stacked together; two cylinders can be stacked together, but not two spheres, in the physical world*\n\nWe don\u2019t study this scenario since it requires the model to **generalize to new attributes not seen during training** (in (1) this is color, in (2) this is 3D shape). In our object compositionality scenarios, we are training the model to **generalize across entity combinations, with each attribute/entity already seen during training time**. For instance, at training time in (1), the model might observe all shape-color pairs and compositions of (circle, triangle) / (triangle, square) sharing the same color move in unison. At test time, we expect the model to work for a scenario where (circle, square) share the same color.\n\nIn scenario (2) brought up by the reviewer, in our definition of \u201csticky\u201d object compositionality, we enforce that each object occurs at least once on top and once on the bottom during training. Therefore, the model would always have seen a sphere/cone at the bottom; allowing it to learn the rule that no object can be stacked on top of a \u201csphere/cone\u201d. Our definition necessitates that the distribution of atoms doesn\u2019t shift across data splits (ie: all shape-color combinations are seen at training time). We will clarify Section 2 so this is more clear, and see a more detailed discussion below (relational compositonality section).\n\n>  *There is some serious machine learning identifiability issue with this setting. If the model does not have inductive biases in training, there is no way that it can generalize. --- Based on these two concerns, the arguments around object compositional generalization is weak.*\n\nWe make sure that our dataset is constructed so that for every input image and action, there is a deterministic one-to-one function that produces the next image. This holds for the scene composition and the object composition datasets. We ensure that the distribution of atoms is fixed between training and test sets, while the distribution of compositions has no overlap between sets. This ensures that the model has seen all objects with a particular shape-color at train time, but the particular composition of objects it sees at train and test time vary. The compositions placed in train and test distributions are shuffled evenly as well. (Appendix A.3)\n\n*(continued in next thread)*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997467311,
                "cdate": 1699997467311,
                "tmdate": 1699997467311,
                "mdate": 1699997467311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DiRUTZnHix",
                "forum": "4KZpDGD4Nh",
                "replyto": "OM51e16qWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CQ89 (continued; part 1)"
                    },
                    "comment": {
                        "value": "> *Second, the model is only trained and evaluated on a  fairly toy environment, and the downstream application to planning is only shown in a very simple setting. It is unclear how this approach can be generalized to more complicated scenarios.*\n\nThe focus of our paper is to demonstrate the first neuro-symbolic framework leveraging foundation models for compositional object-oriented world modelling, and we evaluate on the same benchmarks as existing work `[1, 2, 3]` (at ICLR 2019, NeurIPS 2021, ICML 2022 respectively). We chose our evaluation domain based on four properties: (1) object-oriented state and action space, (2) history-invariant dynamics, (3) action conditioned (plannable) dynamics, and (4) ease of generating new configurations (to evaluate scene and object composition). We curate the following list of domains from related work to explain what properties are missing for each dataset:\n\n| **Dataset**                         | Relevant Works  | **Object Oriented state and action space** | **History Independent** | **Action Conditioned (Plannable)** | **Configurable** |\n| ----------------------------------- | --------------- | ------------------------------------------ | ----------------------- | ---------------------------------- | ---------------- |\n| 2D Block Pushing                    | `[1, 2, 3,  4]` | \u2705                                          | \u2705                       | \u2705                                  | \u2705                |\n| Physion                             | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u2705                |\n| Phyre                               | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u2705                |\n| Clevrer                             | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u274c                |\n| 3DObj                               | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u2705                |\n| MineRL                              | `[6]`           | \u274c                                          | \u274c                       | \u2705                                  | \u274c                |\n| Atari (Pong/Space invaders/Freeway) | `[2]`           | \u2705                                          | \u274c                       | \u274c                                  | \u274c                |\n| Minigrid/BabyAI                     | `[7]`           | \u2705                                          | \u274c                       | \u2705                                  | \u2705                |\n\nFollowing the reviewer\u2019s suggestions, to demonstrate the application of our framework in more complex domains, we also present preliminary results on CLEVRER utilizing a transformer based framework similar to `[5]` (ICLR, 2023). Note that CLEVRER is a video prediction dataset like Physion/Phyre, so we cannot do planning on this dataset. Furthermore, we cannot evaluate generalization w.r.t object compositionality in this dataset as the video generation scripts are not available.\n\n| Algorithm            | Validation MSE after 5k steps ($\\downarrow$) |\n| -------------------- | -------------------------------------------- |\n| COSMOS + Transformer | **8.024E-3**                                 |\n| `[5]` (ICLR,2023)    | 8.975E-3                                     |\n\nWith respect to the downstream application to planning, we do not claim to develop SOTA planning methods for world models. Our experiment aims to evaluate the downstream efficacy of our world models using off the shelf planning algorithms leveraged by related work `[8]`.\n\n>  *Slightly minor is that the paper missed some important related work along the direction of learning neuro-symbolic transition models.*\n\nThank you for the references. We shall include these works in the final manuscript. Some comments on each work: \n\n- PDSketch: PDSketch\u2019s representation consists of symbols lifted from images using pretrained models, which are expressed using tensors (neural relaxations of variables; similar to NS-CL cited in Section 5.2). The problem with such a representation is that the symbols become a bottleneck for representation learning. We sidestep this bottleneck by parsimoniously using the symbolic encodings only for selecting representative encodings. More information is in Section 5.3.\n\n- Learning OOD for Planning from Text: OOTD also learns an object-oriented latent representation and learns a neural network to predict the next state conditioned on an action. There seem to be two big architectural differences: 1) COSMOS uses key-query attention to locally updates selected objects, while OOTD uses a GNN to globally update all objects simultaneously, and 2) OOTD doesn\u2019t use any symbolic grounding.\n\n*(continued in next thread)*"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997588015,
                "cdate": 1699997588015,
                "tmdate": 1699997588015,
                "mdate": 1699997588015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V5A1MsOONb",
                "forum": "4KZpDGD4Nh",
                "replyto": "OM51e16qWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CQ89 (continued; part 2)"
                    },
                    "comment": {
                        "value": "> *I think the name [object composition] is not very descriptive. The authors should consider a better name that describes such kind of \"metarules\" (e.g., two objects have relations if they share the same color) and present concrete examples in the main text.*\n\nThank you for pointing this out. We initially chose this name to describe compositionality over levels of scenes vs. objects, and we agree with the reviewer that another name would be more clear. We plan to revise the naming convention based on the independence w.r.t dynamics. \n\n- *Entity composition (previously named scene composition)*: The dynamics of individual entities are independent of the dynamics of other entities.\n- *Relational composition (previously named object composition)*: The entity dynamics are dependent/related to attributes of other entities.\n  - Team composition: shared attribute is color\n  - Sticky composition: shared attribute is color+adjacency \n\nWe really appreciate your feedback, and we would be happy to further clarify this if needed.\n\n>  *The authors should keep the \"...\" in the sets. Otherwise it's very confusing to look at \"{c1, c2, cp}.\"* \n\nWe shall make these changes. Thank you for the suggestions!\n\n>  *The writing of the method section could be further improved by having a running example (and referring back to this example in 3.1, 3.2, and 3.3).*\n\nGreat suggestion. We shall incorporate the example used in Figure 3 in section 3.1, 3.2, 3.3, etc. if space permits.\n\n>  *Finally, the paper does not have a limitation discussion section.*\n\nDue to space limitations, we decided to discuss limitations and future work together in Section 6, last paragraph. We plan to add an explicit limitations section to the appendix. The two main limitations of our method are:\n\n1. Dependence on CLIP: The symbol grounding depends on the quality of the foundation model. However, foundation models are an exciting area with rapid developments - as a result, we expect the performance of our framework to improve as new methods are released.\n2. More benchmarks and datasets: As the reviewer has pointed out, there is a lack of datasets with real world physics interactions. In an earlier response, we show preliminary results on CLEVRER (a block pushing dataset with collision physics) demonstrating that our framework can be used to handle more complex domains.\n\n>  *I think using CG as an acronym for \"compositional generalization\" is a bit uncommon.*\n\nWe initially chose this for brevity, but agree with the reviewer that it could be confusing. We will refer to compositional generalization as \u201cCompGen.\u201d We are also happy to discuss other suggestions.\n\n**References**\n\n- `[1]`: http://arxiv.org/abs/1911.12247 (ICLR 2019)\n- `[2]`: https://arxiv.org/abs/2103.01937 (NeurIPS 2021)\n- `[3]`: https://arxiv.org/abs/2204.13661 (ICML 2022)\n- `[4]`: https://arxiv.org/abs/2107.00848 (ICLR 2021)\n- `[5]`: https://arxiv.org/abs/2210.05861 (ICLR 2023).\n- `[6]`: https://arxiv.org/abs/2301.04104 (Preprint)\n- `[7]`: https://arxiv.org/abs/2303.05501 (NeurIPS 2022)\n- `[8]`: https://arxiv.org/abs/1910.12827 (CoRL 2019)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997664693,
                "cdate": 1699997664693,
                "tmdate": 1699997664693,
                "mdate": 1699997664693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYbSgGXZrc",
                "forum": "4KZpDGD4Nh",
                "replyto": "OM51e16qWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Reviewer_CQ89"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Reviewer_CQ89"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I appreciate the responses from the authors. I have also read reviews from other reviewers. I decided to increase my score by 1."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523702421,
                "cdate": 1700523702421,
                "tmdate": 1700523702421,
                "mdate": 1700523702421,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XI5kSWc2v4",
            "forum": "4KZpDGD4Nh",
            "replyto": "4KZpDGD4Nh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4021/Reviewer_5cti"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4021/Reviewer_5cti"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed COSMOS framework for compositional generalization in world modeling, that involves a blend of neural and symbolic representations to understand scene entities and their interactions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.     This paper proposes an end-to-end differentiable framework with a novel neuro-symbolic attention\n2.\tThis is a well written and well-structured paper.\n3.\tMost of the traditional neuro-symbolic methods map the representations to symbol manually while this paper does it without any manual effort"
                },
                "weaknesses": {
                    "value": "1.\tThe effectiveness of the framework might be constrained dealing with larger and more complex real-world scenarios. Since scalability of neuro-symbolic methods are required to handle more diverse environments.\n2.\tWhen the model will encounter with noisy or incomplete input, how will the model perform?\n3.\tCombining neural and symbolic inputs might be computationally heavy, but there is no significant discussion about the computational complexity.\n4.\tAlthough the model showcases strong performance in the 2D block pushing domain with MSE but that\u2019s not the case for MRR and Eq.MRR always. More experimental results are required to establish this as the new state-of-the-art.\n\n5. There are typos in introduction section."
                },
                "questions": {
                    "value": "point 2 from weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4021/Reviewer_5cti"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4021/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699765323435,
            "cdate": 1699765323435,
            "tmdate": 1699765323435,
            "mdate": 1699765323435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OTI6dwuaRb",
                "forum": "4KZpDGD4Nh",
                "replyto": "XI5kSWc2v4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5cti"
                    },
                    "comment": {
                        "value": "Thank you for reading our paper in detail. We address concerns inline:\n\n> *The effectiveness of the framework might be constrained dealing with larger and more complex real-world scenarios. Since scalability of neuro-symbolic methods are required to handle more diverse environments.*\n\nThe focus of our paper is to demonstrate the first neuro-symbolic framework leveraging foundation models for compositional object-oriented world modelling, and we evaluate on the same benchmarks as existing work `[1, 2, 3]` (at ICLR 2019, NeurIPS 2021, ICML 2022 respectively). We chose our evaluation domain based on four properties: (1) object-oriented state and action space, (2) history-invariant dynamics, (3) action conditioned (plannable) dynamics, and (4) ease of generating new configurations (to evaluate scene and object composition). We curate the following list of domains from related work to explain what properties are missing for each dataset:\n\n| **Dataset**                         | Relevant Works  | **Object Oriented state and action space** | **History Independent** | **Action Conditioned (Plannable)** | **Configurable** |\n| ----------------------------------- | --------------- | ------------------------------------------ | ----------------------- | ---------------------------------- | ---------------- |\n| 2D Block Pushing                    | `[1, 2, 3,  4]` | \u2705                                          | \u2705                       | \u2705                                  | \u2705                |\n| Physion                             | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u2705                |\n| Phyre                               | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u2705                |\n| Clevrer                             | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u274c                |\n| 3DObj                               | `[5]`           | \u2705                                          | \u274c                       | \u274c                                  | \u2705                |\n| MineRL                              | `[6]`           | \u274c                                          | \u274c                       | \u2705                                  | \u274c                |\n| Atari (Pong/Space invaders/Freeway) | `[2]`           | \u2705                                          | \u274c                       | \u274c                                  | \u274c                |\n| Minigrid/BabyAI                     | `[7]`           | \u2705                                          | \u274c                       | \u2705                                  | \u2705                |\n\nTo demonstrate the application of our framework in more complex domains, we also present preliminary results on the CLEVRER dataset utilizing a transformer based framework similar to `[5]` (ICLR, 2023). Note that CLEVRER is a video prediction dataset, so we cannot do planning on this dataset. Furthermore, we cannot evaluate generalization w.r.t object compositionality in this dataset as the video generation scripts are not available.\n\n| Algorithm            | Validation MSE after 5k steps ($\\downarrow$) |\n| -------------------- | -------------------------------------------- |\n| COSMOS + Transformer | **8.024E-3**                                 |\n| `[5]` (ICLR,2023)    | 8.975E-3                                     |\n\nFurthermore, traditional neuro-symbolic representation learning approaches searched for programs to generate the required symbols in a combinatorial search space `[8]`. The reviewer correctly points out that such search methods do not scale as the complexity of the environment increases. **We do not suffer from these scalability issues** as we use a foundation model to query symbols. The foundation model encodes the program in its parameter space, and inference has a constant time complexity. While we use CLIP for our experiments, in principle, we are agnostic to the choice of vision-language foundation model, allowing us to generalize to different domains.\n\n> *When the model will encounter with noisy or incomplete input, how will the model perform?*\n\nSymbols generally enable the model to be more robust to noisy and incomplete inputs. The CLEVRER domain referred in the previous response has noisy inputs in the form of partial occlusion. Preliminary results show that our model performs better than a comparable baseline.\n\n*(continued in next thread)*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699996152130,
                "cdate": 1699996152130,
                "tmdate": 1699996539034,
                "mdate": 1699996539034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UNP1HjLfY3",
                "forum": "4KZpDGD4Nh",
                "replyto": "XI5kSWc2v4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5cti (continued)"
                    },
                    "comment": {
                        "value": ">  *Combining neural and symbolic inputs might be computationally heavy, but there is no significant discussion about the computational complexity.*\n\nThe transition module has the highest computational complexity and dictates the complexity of the overall algorithm. We mention the computational complexity of our transition module in Section 3.3. Specifically, it has a $\\mathcal{O}(kl + k)$ time complexity, where $k$ is the number of slots and $l$ is the number of rules.\n\nUnlike previous work `[8]` (TMLR, 2022) which necessitates searching for programs in a combinatorial space, we query symbolic attributes using a foundation model. Since each symbolic attribute query is independent of other queries, we can batch all queries together. Consecutively, the symbolic labelling module has a constant time complexity and $\\mathcal{O}(kp)$ space complexity, where $p$ is the number of classes. Typically, $k$ is larger than $p$.\n\n> *Although the model showcases strong performance in the 2D block pushing domain with MSE but that\u2019s not the case for MRR and Eq.MRR always. More experimental results are required to establish this as the new state-of-the-art.*\n\nWe offer an explanation for the performance inconsistencies of Eq.MRR in (Section 4, Results). Specifically, we notice that models with high MRR tend to have underperforming autoencoders. For instance, in the five object scene composition case, the GNN exhibits a high Eq.MRR score yet simultaneously has the worst autoencoder reconstruction error. We notice this happens when the model suffers a partial representation collapse (overfitting to certain color, shapes combination seen during training). This maps many slot encodings to the same neighborhood in latent space; making it easier to retrieve similar encodings, boosting the MRR score.\n\nThe appendix section on the shortcomings of MRR and Eq.MRR further elucidates issues with using this metric for compositional world modeling (A.3, Evaluation Procedure). We are happy to discuss this point further if there are remaining questions.\n\n> *There are typos in introduction section.*\n\nThank you for pointing this out. I believe this is \u201cconsistsd\u201d which should be \u201cconsists.\u201d We will update the introduction accordingly.\n\n\n\n**References**\n\n- `[1]`: http://arxiv.org/abs/1911.12247 (ICLR 2019)\n- `[2]`: https://arxiv.org/abs/2103.01937 (NeurIPS 2021)\n- `[3]`: https://arxiv.org/abs/2204.13661 (ICML 2022)\n- `[4]`: https://arxiv.org/abs/2107.00848 (ICLR 2021)\n- `[5]`: https://arxiv.org/abs/2210.05861 (ICLR 2023).\n- `[6]`: https://arxiv.org/abs/2301.04104 (Preprint)\n- `[7]`: https://arxiv.org/abs/2303.05501 (NeurIPS 2022)\n- `[8]`: https://arxiv.org/abs/2107.13132 (TMLR, 2022)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699996207438,
                "cdate": 1699996207438,
                "tmdate": 1699996207438,
                "mdate": 1699996207438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UnIjncVAEW",
                "forum": "4KZpDGD4Nh",
                "replyto": "UNP1HjLfY3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4021/Reviewer_5cti"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4021/Reviewer_5cti"
                ],
                "content": {
                    "title": {
                        "value": "response to author's rebuttal"
                    },
                    "comment": {
                        "value": "I express gratitude to the authors and acknowledge their responses to my reviews. My concerns have been adequately addressed, and I am maintaining my initial decision."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634135607,
                "cdate": 1700634135607,
                "tmdate": 1700634135607,
                "mdate": 1700634135607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]