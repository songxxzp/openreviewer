[
    {
        "title": "DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization"
    },
    {
        "review": {
            "id": "Z1cF3gjDCZ",
            "forum": "koYsgfEwCQ",
            "replyto": "koYsgfEwCQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_p1WX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_p1WX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an unsupervised learning method for 3D dynamic scene capture and decomposition. The presented method, DynaVol, utilizes a canonical NeRF as well as a time-conditioned implicit warping field for modeling a 3D dynamic scene. Learning such a representation uses a monocular video together with multi-view observation of a static frame, and several other objectives designed for retrieving better geometry and enforcing temporal consistency. Comparisons with previous state-of-the-arts are shown on the tasks of novel view synthesis and 3D scene decomposition, and improved results are reported."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Experiment\n\n Extensive experimental results are shown on tasks like novel view synthesis and 3D scene decomposition. Improved results are achieved compared to many existing baselines and state-of-the-arts."
                },
                "weaknesses": {
                    "value": "* More insights from ablation study?\n\nIt is really good to see that the authors conduct a lot of ablation study on the work. However, some of them only consist of a chart with statistics, while lacking some insight analysis on them. It would further improve the quality of this work if more insights are discussed there.\n\n* Novelty?\n\nThe method proposed in the manuscript is pretty interesting, although it would be great to highlight the novelty and difference between the proposed method and previous literature. For example, on the dynamic modeling side, what is the relationship with [1, 2]. On the representation side, the design of volume slot attention is also not very well justified. It would be great if more insights and ablations are provided for the that part of design.\n\n* Introduction\n\nIn the introduction part, I feel the importance of the problem that the paper is working on is not well explained. Instead, the author directly starts to focus on what method is proposed and what experiments are conducted. It would better enhance the quality of the paper if more background and impact of solving this problem are provided and discussed.\n\n[1] Li, Zhengqi, et al. \"Neural scene flow fields for space-time view synthesis of dynamic scenes.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n[2] Li, Zhengqi, et al. \"Dynibar: Neural dynamic image-based rendering.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "* Supervision?\n\nAlthough not a very big concern, the number of objects N seems to serve as input to the pipeline. This semantic could serve as additional supervision with strong prior, especially to the task of scene decomposition. It would be interesting to see how to address the problem of automatically determining the number of objects in the scene."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698511697912,
            "cdate": 1698511697912,
            "tmdate": 1699636082042,
            "mdate": 1699636082042,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XiqeGGWE9R",
                "forum": "koYsgfEwCQ",
                "replyto": "Z1cF3gjDCZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. Please find our responses to each of them below. If you have any further concerns or questions, please feel free to inform us.\n\n> Q1. More insights from ablation study.\n\nWe appreciate your suggestion and have incorporated more discussion regarding the ablation study results in Section 4.5 of the revised paper. Specifically, we delve into the impact of different stages during the training process:\n- **Warmup Stages:** We conducted an additional ablation study performing 4D voxel optimization from scratch with randomly initialized $\\mathcal{V}{t=1}$ and $f_\\phi(\\cdot)$. This study clearly illustrates that excluding the warmup stages (which includes both the \"3D voxel warmup stage\" and the \"3D-to-4D voxel expansion stage\") has a substantial impact on the final performance, particularly for the scene decomposition results.\n- **4D Voxel Optimization Stage:** We emphasize that the performance of DynaVol significantly degrades when excluding the 4D voxel optimization stage. These results demonstrate the importance of refining the object-centric voxel representation with a slot-based renderer.\n\n> Q2. The method proposed in the manuscript is pretty interesting, although it would be great to highlight the novelty and difference between the proposed method and previous literature. \n\nWe have included more discussion of the differences and relationships between our model and the suggested existing work [1,2] in Section 5 in the revision. We here provide a more detailed comparison of these methods on the dynamic modeling side:\n- **DynaVol:** In addition to the rendering MLP, our model employs additional networks that learn $(\\mathbf{x},t) \\rightarrow \\Delta \\mathbf{x}\\in \\mathbb{R}^{3}$ to explicitly transport the density values in each voxel grid to the corresponding voxel grid at an arbitrary future time step.\n- **Method from [1]:** This approach extends the original rendering MLP in NeRF to incorporate the dynamics information. Basically, it learns a mapping function of (RayPoint, ViewDirection, Time) $\\rightarrow$ (Color, Density, Offset). Here, \"offset\" determines the 3D correspondence of the RayPoint at nearby time steps.\n- **Method from [2]:** This approach has achieved significant improvements on dynamic scene benchmarks by representing motion trajectories by finding 3D correspondences for sampling points in nearby views.\n\nFurthermore, while all of these methods leverage temporal-consistency constraints, unlike [1,2], which establish the constraint in pixel color space, we incorporate it in the canonical space to encourage coherent transportations of the density values across time."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654462498,
                "cdate": 1700654462498,
                "tmdate": 1700654462498,
                "mdate": 1700654462498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Di0WlsSd2A",
            "forum": "koYsgfEwCQ",
            "replyto": "koYsgfEwCQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_FQjp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_FQjp"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a method, DynaVol, that learns to represent dynamic scenes as decomposed into the different moving parts (objects). The input to the method is a sequence of frames and their pose.\nThis decomposition is learned in 3D by learning to deform a canonical volume into the 3d states at the different observation points. Slot attention on 3d voxel grids is used to learn time-invariant object representations that are deformed by the deformation field. A NERF rendering loss guides allows learning this from just the input image sequence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The research direction of learning to decompose and represent a dynamic scene in terms of the motion of individual constituent parts (objects / slots) is important. \n\nThe results on simulated data are encouraging (although there are only a few scenes shown) and show cases where the model is indeed able to nicely separate out the different moving parts of the scene.\nFigure 4 exactly captures the advantage of the proposed dynamic object-centric scene representation in comparison to 2d methods (SAM): being able to represent unobservable parts of an image.\n\nThe proposed approach to dynamic scene representation has fundamental advantages as demonstrated by the qualitative results on scene editing (removing objects; modifying dynamics).\n\nThe visuals and writing in the manuscript are of high quality (the clarity could be improved - see weaknesses)."
                },
                "weaknesses": {
                    "value": "The proposed model DynaVol is complex. The text and the architecture figures were not easy to follow. Especially the parts about the slot updating and training are a bit opaque to me still. There are various unexplained variables in Fig. 1 which means the figure doesnt really add much to explaining the concepts and architecture to me.\n\nOne weakness is that DynaVol has to be overfited to each individual single scene. As such this is similar to NERF and okay. But when comparing such an overfit model to a fully generalizing model like SAM caution has to be use to clarify very precisely. \n\nThe experiments do suggest to me that DynaVol works well on easily segmented objects (from the simulation) but on real sequences DynaVol does not much outperform other methods. The small set of sequences evaluated in sim and real weakens the claims."
                },
                "questions": {
                    "value": "Do I understand correctly that there is only one occupancy grid at t=0 and the other ts are reconstructed using the deformation field? A query at time t is warped back to t=0 and then interpolated into the feature volume?\n\nWhere is the GRU in Fig 2?\n\nFor the NERF rendering is it correct that the occupancy comes from the occupancy grid and the color comes from the slot features? Why this choice and not push occupancy into the slot features too?\n\nI couldnt find the per-point color loss in the DVGO paper? What am I missing?\n\nWhat happens without the warmup phase? \n\nHow are the occupancy grid and the deformation function represented? Just dense values on a grid? what resolution?\n\nHow is the feature graph computed after warmup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793970404,
            "cdate": 1698793970404,
            "tmdate": 1699636081964,
            "mdate": 1699636081964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HjJLpDZ777",
                "forum": "koYsgfEwCQ",
                "replyto": "Di0WlsSd2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your great efforts in reviewing our paper and hope that the following responses can address most of your concerns. \n\n> Q1. The proposed model DynaVol is complex. The text and the architectural figures were not easy to follow.\n\nWe apologize for any difficulty you experienced in reading our paper. We have tried our best to organize the writing but found it challenging due to the complexity of the proposed method, which involves multiple training stages and many details. To improve the overall reading experience, we have made the following changes:\n- We refined Section 3.1 and Figure 2 for a better overview of DynaVol.\n- We revised Section 3.4 to reorganize the entire training pipeline and separate it into three stages, including a \"3D voxel warmup stage\", a \"3D-to-4D voxel expansion stage\", and a \"4D voxel optimization stage\". We further clarified the key insight of each stage and the connections between these stages. \n- We introduced Algorithm 1 in Appendix.B to present the details of 3D-to-4D voxel expansion, outlining how to initialize the 4D voxel representation using the connected component algorithm.\n\nPlease refer to the updated paper for the details.\n\n> Q2. When comparing such an overfit model to a fully generalizing model like SAM caution has to be used to clarify very precisely.\n\nThanks for the suggestion! We have clarified the differences in the experimental setups of DynaVol and SAM in the caption of Table 3: \"*The compared models, including SAM, are fully generalizing models trained beyond the test scenes. In contrast, our model enables 3D scene decomposition by overfitting each individual single scene in an unsupervised manner.*\" Despite the distinct training setups, the comparison in Table 3 aims to showcase DynaVol's ability for 3D scene decomposition by leveraging knowledge of the segmented scene. \n\nNote: Both the quantitative results in Table 3 and the qualitative results in Figure 4 correspond to images of novel views outside the scope of DynaVol's training set.\n\n> Q3. DynaVol works well on easily segmented objects (from the simulation) but on real sequences, DynaVol does not much outperform other methods.\n\nWe have added new real-world experiments on the ENeRF-Outdoor dataset. Please refer to Figure 7 in Appendix.C for the qualitative results. Below, we present the quantitative comparisons in PSNR and SSIM:\n\n|  Real-world scenes  |       Actor1_4        |     Actor2_3      |\n| --------------------- |:-----------------------------------:|:-------------------------------:|\n| HyperNeRF  | 22.79 $\\mid$ 0.759 | 22.85 $\\mid$ 0.773 | 25.22 $\\mid$ 0.806|\n| Ours |    **26.44** $\\mid$ **0.871**      |  **26.24** $\\mid$ **0.864** |\n\n> Q4. Do I understand correctly that there is only one occupancy grid at t=0 and the other ts are reconstructed using the deformation field? A query at time t is warped back to t=0 and then interpolated into the feature volume?\n\nYes. \n\n> Q5. Where is the GRU in Fig 2?\n\nWe consider GRU as a component of slot attention, thereby placing it in $Z\\_\\omega$. We clarified this in the caption and the legend in Figure 2.\n\n> Q6. For the NERF rendering is it correct that the occupancy comes from the occupancy grid and the color comes from the slot features? Why this choice and not push occupancy into the slot features too?\n\nYes, as illustrated in the updated Figure 2, we determine the color $\\bar{\\mathbf{c}}$ at a ray sampling point through the following steps, all starting from the occupancy grids $\\mathcal{V}\\_t$:\n- $\\mathcal{V}\\_t$ $\\rightarrow$ [Volume Slot Attention] $\\rightarrow$ Slot features $s_t^n$ $\\rightarrow$ [MLP] $\\rightarrow$ Per-slot color $c\\_n$,\n- $\\mathcal{V}\\_t$ $\\rightarrow$ [Fetch and Trilinear Interpolation] $\\rightarrow$ Per-slot occupancy $\\sigma\\_n$,\n- $(c\\_n, \\sigma\\_n)$ $\\rightarrow$ [Accumulated as Eq. (3)] $\\rightarrow \\bar{\\mathbf{c}}$.\n\nRepresenting per-object volume density through explicit occupancy grids offers three advantages:\n- It speeds up the training process. \n- It improves the convenience of the downstream scene editing applications.\n- It allows seamless integration with off-the-shelf scene decomposition algorithms such as the connected components."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653670624,
                "cdate": 1700653670624,
                "tmdate": 1700653670624,
                "mdate": 1700653670624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UV7oSBT7AY",
                "forum": "koYsgfEwCQ",
                "replyto": "Di0WlsSd2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q7. Could not find the per-point color loss in the DVGO paper.\n\nYes, indeed, DVGO did not introduce the formulation of the per-point RGB loss in the main text, but examined its effectiveness in Table I6 in the supplementary material (please refer to its arxiv version).To improve clarity, we have presented the formulation of this loss term in Eq. (5) in our original manuscript.\n\n> Q8. What happens without the warmup phase?\n\nTo answer this question, we conducted an ablation study by training the entire model from scratch --- We directly performed \"4D voxel optimization\" with randomly initialized $\\mathcal{V}\\_{t=1}$ and $f\\_\\phi(\\cdot)$. The obtained results in terms of PSNR and FG-ARI (higher values are favorable) are presented below. It is evident that removing the warmup stage significantly impacts the final performance, especially for the scene decomposition results. We also include these comparisons in Table 4 (\"4D voxel optim. from scratch\") in the revised paper.\n\n|   |         3ObjsFall          |         6ObjsFall          |         8ObjsFall          |\n| ---------- |:--------------------------:|:--------------------------:|:--------------------------:|\n| w/o Warmup |     31.69 $\\mid$ 31.81     |     29.37 $\\mid$ 20.82     |     28.55 $\\mid$ 33.89     |\n| Full model | **32.11** $\\mid$ **96.95** | **29.98** $\\mid$ **94.73** | **29.78** $\\mid$ **95.10** |\n\nA side note: We have reorganized the entire training scheme for better clarity. Hence, the \"w/o Warmup\" results in the above table are obtained by excluding the first two stages below: \n- *3D voxel warmup stage*: Corresponds to the training phase in the original warmup stage.\n- *3D-to-4D voxel expansion stage*: Corresponds to the post-processing part of the original warmup stage (i.e., the computation of connected components).\n- *4D voxel optimization stage*: Corresponds to the dynamic scene optimization stage in the original manuscript.\n\n> Q9. How are the occupancy grid and the deformation function represented? Just dense values on a grid? what resolution?\n\nThe resolution of the occupancy grids is $\\mathcal{V}\\_{density}^{t=1} \\in \\mathbb{R}^{N \\times N\\_x \\times N\\_y \\times N\\_z}$, where $N$ is the number of slots, and $N\\_x$, $N\\_y$, and $N\\_z$ represent the spatial resolution along each axis. Specifically, we set the spatial resolution of the voxel grids to $110^3$.\n\nAs for the deformation function, we employ an MLP that takes $(\\mathbf{x},t)$ with positional encoding as inputs to predict the position movement $\\Delta \\mathbf{x} \\in \\mathbb{R}^{3}$.\n\n> Q10. How is the feature graph computed after warmup?\n\nWe have included **Algorithm 1** in Appendix B to provide a detailed explanation of the 3D-to-4D voxel expansion process, which consists of the following two steps: \n- **Feature Graph Generation:** We consider the voxel set $\\\\{X_k\\\\}\\_{k=1}^{N_x \\times N_y \\times N_z}$ from $\\mathcal{F}\\_{t=1}$. By filtering out invalid locations with density values below a pre-defined threshold (inspired by DVGO), we can get the node set $\\\\{X_k\\\\}\\_{k=1}^{K}$ to build the feature graph $G$. For any pair of nodes $(u,v)$ in $G$, an edge is defined between them if the following three conditions are satisfied:\n    - $u$ and $v$ correspond to spatially adjacent voxels.\n    - $u$ and $v$ result in similar emitted color $(c\\_u^r, c\\_v^r)$ produced by $N_{\\phi^\\prime}$ along an arbitrary ray $r$, such that $\\frac{1}{\\mathcal{R}}\\sum\\_r^\\mathcal{R} d_\\text{rgb}(c\\_u^r, c\\_v^r)<\\text{Threshold}\\_\\text{rgb}$, where $\\mathcal{R}$ is the number of rays randomly sampled with different directions. We take the Euclidean distance to calculate $d\\_\\text{rgb}(c\\_u^r, c\\_v^r)$.\n     - $u$ and $v$ lead to similar velocities across all time steps, which can be acquired through the forward deformation network $f^\\prime\\_\\xi$, such that $\\text{max}\\_t(d_\\text{vel}(u,v)) < \\text{Threshold}\\_\\text{vel}$, where we take the maximum Euclidean distance $||(f^\\prime\\_\\xi(u,t)-f^\\prime\\_\\xi(u,t-1))-(f^\\prime\\_\\xi(v,t)-f^\\prime\\_\\xi(v,t-1))||_2$ over all time steps ($t \\in [2,T]$).\n\n* **Connected Components Computation:** In this step, we first feed the feature graph $G$ into the connected components algorithm to obtain $M$ clusters. We then sort the clusters by size and merge the smallest $M-N+1$ clusters. For each cluster $p$ ($1 \\le p \\le N$), we assign the density value of voxels in $\\mathcal{F}\\_{t=1}$ to the corresponding channel at the same localation in $\\mathcal{V}\\_{t=1}$."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654276459,
                "cdate": 1700654276459,
                "tmdate": 1700656114643,
                "mdate": 1700656114643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EEHXNmuZUj",
                "forum": "koYsgfEwCQ",
                "replyto": "Di0WlsSd2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Reviewer_FQjp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Reviewer_FQjp"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. A lot of things are more clear now. I appreciate the additional ablations of no warmup. The real world experiment in the appendix is also nice and clean. \n\nRe Q1: 3D voxel warmup is also training the deformation field though it seems? That means a optimization in 4D is executed.\nBased on my understanding of the algorithm maybe something more like:\n- 4D voxel warmup\n- Slot Instantiation\n- 4D Slot optimization\n\nDoes that make sense?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681138315,
                "cdate": 1700681138315,
                "tmdate": 1700681310066,
                "mdate": 1700681310066,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HZscre9LDO",
            "forum": "koYsgfEwCQ",
            "replyto": "koYsgfEwCQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_B2an"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_B2an"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DynaVol, an attempt at unsupervised learning for object-centric representations in dynamic scenes. While most existing solutions focus on 2D image decomposition, DynaVol aims to offer a 3D perspective by leveraging object-centric voxelization within a volume rendering framework. It has a few steps to achieve this, including but not limited to canonical-space deformation optimization, and global representation learning with slo attention. The approach combines both local voxel and global features as conditions for a NeRF decoder. The proposed method achieves good results in a simulation environment and demonstrates its applicability in one real-world dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Good attempt. The proposed DynaVol approach introduces a shift from 2D image decomposition by bringing in a 3D perspective (via NeRF). NeRF as a 3D representation beyond view synthesis is an underexplored region and I think this work makes a good effort towards this direction by doing object-centric voxelization within a volume rendering framework. Although the canonical-space deformation operation is widely seen in dynamic NeRF works, it still brings value to object-centric learning.\n\n2. The paper is well articulated in the introduction section. I do capture the underlying motivation for the whole work, but I found a hard time understanding the two-stage training at the beginning. The authors are suggested to make the overview section and the overview figure flow better."
                },
                "weaknesses": {
                    "value": "Dataset Limitations: My primary concern is the robustness of DynaVol when applied to intricate real-world scenes. The paper predominantly employs what can be described as a 'toy' dataset to validate its model. The observations made from this kind of dataset often fail to generalize to real-world applications. The mention of the HyperNeRF dataset application does lend some credence to its real-world viability. However, a more thorough exploration, perhaps using more diverse and challenging datasets (one or two demos would be enough), would provide stronger evidence of the model's capability.\n\nAlso, I wonder how important the background loss is in the pipeline. Will this method work without this loss? Will the slot decompose the background?"
                },
                "questions": {
                    "value": "1. Based on the results presented in Table 1, it seems that models utilizing 5 views (V=5) don't consistently surpass the performance of those with just a single view. Could the authors shed light on the underlying reason for this unexpected behavior?\n\n2. Significance of the Warmup Stage: The paper doesn't delve deeply into the role of the warmup stage in the ablation studies. How crucial is this phase to the overall performance and efficiency of DynaVol?\n\n3. Performance of Warmup-Stage-Only: It's intriguing to observe that the 'warmup-stage-only' variant appears to perform quite competently even though it's restricted to initial timestep observations. It's even better than other baselines reported in Table 1. Could the authors explain the factors or mechanisms behind this seemingly robust performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1533/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1533/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1533/Reviewer_B2an"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816101319,
            "cdate": 1698816101319,
            "tmdate": 1700738057681,
            "mdate": 1700738057681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UNvleaHDPw",
                "forum": "koYsgfEwCQ",
                "replyto": "HZscre9LDO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments. We provide the responses below to each of the specific concerns. Please do not hesitate to let us know of any additional comments on the paper.\n\n> Q1. My primary concern is the robustness of DynaVol when applied to intricate real-world scenes.\n\nWe have added new real-world experiments on the ENeRF-Outdoor dataset. Please refer to Figure 7 in Appendix.C for the qualitative results. Below, we present the quantitative comparisons in PSNR and SSIM:\n\n| Real-world scenes |       Actor1_4        |     Actor2_3      |\n| --------------------- |:-----------------------------------:|:-------------------------------:|\n| HyperNeRF  | 22.79 $\\mid$ 0.759 | 22.85 $\\mid$ 0.773 | 25.22 $\\mid$ 0.806|\n| Ours |    **26.44** $\\mid$ **0.871**      |  **26.24** $\\mid$ **0.864** |\n\n> Q2. (1) How important the background entropy loss is? (2) Will the slot decompose the background?\n\n(1) On the background entropy loss:\n\nThe background entropy loss is borrowed from DVGO, which aims to encourage the renderer to concentrate on either the foreground or background information. We further analyzed the impact of the background entropy loss using different loss weights (metrics: PSNR/FG-ARI):\n\n | $\\alpha_e$ |     3ObjsFall     |     6ObjsFall     |      8ObjsFall      |\n | ---------- |:---------------------------:|:--------------------------:|:---------------------------:|\n | 0          | 31.38 $\\mid$ 94.98 |   **30.43** $\\mid$ 94.25  | **30.51** $\\mid$ 92.59 |\n | 0.01 (Ours)          | **32.11** $\\mid$ **96.95** |  29.98 $\\mid$ **94.73**     | 29.78 $\\mid$ **95.10** |\n  | 0.1          |18.20 $\\mid$ 4.65  |  19.68 $\\mid$ 31.63   |23.05 $\\mid$ 59.01  | \n\n**Findings:** Incorporating the background entropy loss with an appropriate weight is beneficial to the scene decomposition results as it penalizes ambiguous geometric estimations. However, a higher $\\alpha_e$ value can introduce instability to the training procedure.\n   \n(2) Will the slot decompose the background?\n\nYes, our proposed method is motion-aware and is capable of decomposing the background, as the background can be treated as a static component. Additionally, during the 3D-to-4D voxel expansion stage, voxel grids with significantly distinct velocities estimated by the dynamics modules tend to be initiated with different slot indices by the connected components algorithm (Please refer to Algorithm 1 in the appendix).\n\nAs illustrated in Figure 3(b), our model effectively decomposes backgrounds in real-world scenarios. In the synthetic scenes in Figure 4, our model successfully decomposes the background table with an individual slot. For additional visualization results of per-slot rendering, please refer to the newly added Figure 8 in Appendix.F.\n\n> Q3. Models utilizing 5 views don't consistently surpass the performance of those with just a single view.\n\nOur motivation for using multiple views at $t=1$ stems from empirical observations in particular challenging scenarios. For instance, in 3ObjMetal, the color of a moving object with a metal surface may change rapidly across different time steps due to the drastic changes in specular reflection, which presents difficulties in learning the geometries. Incorporating additional views at the initial time step can enrich the geometric information, thereby improving the quality of the learned 3D voxel grids $\\mathcal{F}\\_{t=1}$ and corresponding 4D object-occupancy grids $\\mathcal{V}\\_{t=1}$."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653402787,
                "cdate": 1700653402787,
                "tmdate": 1700653402787,
                "mdate": 1700653402787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8veBOaUTww",
                "forum": "koYsgfEwCQ",
                "replyto": "HZscre9LDO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Reviewer_B2an"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Reviewer_B2an"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttals."
                    },
                    "comment": {
                        "value": "I thank the authors for the time they have contributed to addressing my concerns. I have increased my rating accordingly."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738040935,
                "cdate": 1700738040935,
                "tmdate": 1700738040935,
                "mdate": 1700738040935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RNWsOLHhHW",
            "forum": "koYsgfEwCQ",
            "replyto": "koYsgfEwCQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_8F9K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1533/Reviewer_8F9K"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an optimization-based method for taking a monocular video as input and producing an object-centric 3D dynamic representation as output. The idea is to optimize a 3D voxel grid to learn the 3D occupancy/density field at each timestep, along with the warp that links each timestep to the zeroth timestep. Then, these occupancies are divided into a set of object representations using connected components, and then slot-based optimization proceeds, where  features associated with objects are averaged across the time axis, and per-timestep features are assigned to the objects via iterated slot attention. The representation is trained by rendering to the given views with a nerf-style setup, using a color loss, an entropy loss aiming to separate foreground/background, and a cycle-consistency objective on the scene flow fields. The results are impressive, both in simulation and in the two real videos."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I like this paper. It is well written, it is interesting, the method seems sensible (thought I have a few questions), and the results look good."
                },
                "weaknesses": {
                    "value": "I have a variety of fairly low-level questions, which I hope the authors can address. \n\nOverall I think the method would be easier to understand if the stages were presented in a more central way, instead of at the very end. After reading everything I am still a bit unclear on the meaning of an \"episode\". \n\nSection 2 emphasizes that \"we also consider scenarios where only one camera pose is available at the first timestamp\". What does it mean for only one camera pose to be available? Camera poses are relative to something. A single pose might as well be the identity matrix. \n\nThe beginning of Section 3.1 says the method will be trained \"without any further supervision\", without defining any initial supervision. So I suppose the question is: further to what?\n\nSection 3.1 introduces the number of objects N, as \"the assumed number of objects\". Is this manually chosen? Later on there is a connected components step, so I wonder if connected components could choose N instead.\n\n\nIt is unclear to me what exactly is happening in the warmup stage vs. the dynamics stage. From the names I might guess that the warmup stage does not involve any dynamics, but this is wrong, because it uses all timesteps and it also trains the forward and backward dynamics modules. The section also says \"To obtain the dynamics information, we train an additional module f\u2032\u03be\" -- but dynamics information is already obtained by f_\\psi, so there is some mixup here, unless the authors do not see f_\\psi as obtaining dynamics information. Finally, it's unclear how exactly the connected components and the \"feature graph\" connect with the rest of the method.\n\nL_point seems like it is never defined, and only a citation is given. It would be great to describe in English what is happening there.\n\nSection 3.3 seems to complain that prior compositional nerfs used an MLP to learn a mapping from position+direction+feature to color+density, and then proposes to do the exact same thing. Have I missed something here? \n\nOverall I like the paper and I hope it can be cleaned up. The video results show that the segmentations are a little messy, and it seems like only 2 real videos are used, but still I think the paper is interesting and useful to build on."
                },
                "questions": {
                    "value": "This is more of a personal curiosity, but: instead of learning two deformation networks (one forward, one backward) and then asking them to be cycle-consistent, would it work to learn an invertible mapping here instead? (Like with Real NVP.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1533/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826033381,
            "cdate": 1698826033381,
            "tmdate": 1699636081817,
            "mdate": 1699636081817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qBXBDRq4ZQ",
                "forum": "koYsgfEwCQ",
                "replyto": "RNWsOLHhHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. We hope that our responses below can adequately address your concerns regarding this paper.\n\n> Q1. The method would be easier to understand if the stages were presented in a more central way, instead of at the very end.\n\nThank you for the suggestion. We have included more discussions on the warmup stage and dynamic scene optimization stage in Section 3.1. Please refer to the revised paper for further details.\n\n> Q2. On the meaning of an \"episode\".\n\nThroughout the training stage, DynaVol iteratively processes each frame in the video in chronological order and repeats: $\\\\{I\\_1, I\\_2, \\ldots, I\\_T\\\\}\\_\\text{episode=1}$, $\\\\{I\\_1, I\\_2, \\ldots, I\\_T\\\\}\\_\\text{episode=2}$, $\\ldots$, where we use the term \"episode\" to denote one round spanning from the first frame to the last.\n\n> Q3. Section 2 emphasizes that \"we also consider scenarios where only one camera pose is available at the first timestamp\". What does it mean for only one camera pose to be available? \n\nThe term \"one camera pose\" means we only have a single-view image captured by the monocular camera at $t=1$, as opposed to having multiple images from different views. We apologize for any confusion and have rephrased this statement in the revised paper.\n\n> Q4. Section 3.1 says the method will be trained \"without any further supervision\", without defining any initial supervision.\n\nAs shown in Eq. (5), we use the pixel values of the observed video frames as the training targets for DynaVol. We here emphasize \"without any further supervision\" because some existing object-centric rendering approaches, such as those proposed by Stelzner et al. (2021) and Driess et al. (2022), incorporate additional information like depth maps or binary masks for each object to supervise the model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652323997,
                "cdate": 1700652323997,
                "tmdate": 1700654192010,
                "mdate": 1700654192010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B1G0ZWq41w",
                "forum": "koYsgfEwCQ",
                "replyto": "RNWsOLHhHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q6. It is unclear what is happening in the warmup stage vs. the dynamics stage. The section also says \"To obtain the dynamics information, we train an additional module $f^\\prime_\\xi$ -- but dynamics information is already obtained by $f_\\psi$, so there is some mixup here, unless the authors do not see $f_\\psi$ as obtaining dynamics information. \n\n(1) Reorganizing the training stages:\n\nYes indeed, the warmup stage also exploits the dynamics information to initialize the voxel grid features $\\mathcal{V}_{t=1}$. From this perspective, we acknowledge that the names of the training stages might be somewhat misleading. We have reorganized the entire training scheme and separated it into three stages for better clarity:\n- **3D voxel warmup stage** (*the training part of the original warmup stage*): This training stage does NOT incorporate object-centric features. Instead, we train the bi-directional deformation networks $(f^\\prime_\\xi, f_\\psi)$ and the neural renderer $N_{\\phi^\\prime}$ based on 3D voxel densities $\\mathcal{F}_t \\in \\mathbb R^{N_x \\times N_y \\times N_z}$.\n- **3D-to-4D voxel expansion stage** (*the post-processing part of the original warmup stage*): We extend the 3D voxel grids $\\mathcal{F}\\_{t=1}$ to 4D voxel grids $\\mathcal{V}\\_{t=1}$ using the connected components algorithm. The input features for the connected components algorithm involve the forward canonical-space transitions generated by $f^\\prime_\\xi$ and the emitted colors by $N_{\\phi^\\prime}$. We give more details in Appendix.B in the revision.\n- **4D voxel optimization stage** (*the original dynamic scene optimization stage*): In this stage, we train the entire model based on a set of 4D voxel features $\\mathcal{V}_t \\in \\mathbb R^{N \\times N_x \\times N_y \\times N_z}$. It's important to note that the additional dimension from 3D to 4D corresponds to the number of slots ($N$) rather than the time horizon ($T$).\n\nWe have made corresponding revisions in Section 3.1 (Overview of DynaVol), Section 3.4 (Descriptions of training stages), and Figure 2. Please refer to our revised paper!\n\n(2) The effect of $f^\\prime_\\xi$:\n\nAs pointed out by the reviewer, both $f^\\prime_\\xi$ and $f_\\psi$ are designed to capture the dynamics information, but in different time directions. We use $f^\\prime_\\xi$ for two purposes:\n- It allows for the computation of a cycle-consistency loss, which can enhance the coherence of the learned canonical-space transitions. \n- It provides useful input features to the subsequent connect components algorithm, representing the forward dynamics starting from the initial time step, which can improve the initialization of $\\mathcal{V}_{t=1}$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652480425,
                "cdate": 1700652480425,
                "tmdate": 1700654304238,
                "mdate": 1700654304238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AOY6KTnQ6o",
                "forum": "koYsgfEwCQ",
                "replyto": "RNWsOLHhHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1533/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q7. It's unclear how exactly the connected components and the \"feature graph\" connect with the rest of the method.\n\nTo highlight the relationship of the computation of connected components with the rest of the method, we have reorganized the entire training scheme into three stages. As clarified in our response to Q6, the corresponding \"**3D-to-4D voxel expansion stage**\" acts as a bridge between the previous \"3D voxel warmup stage\" and the subsequent \"4D voxel optimization stage\". It extends the 3D density values to 4D object-centric representation:\n$$\n\\text{3D-to-4D voxel expansion: } \\ \\mathcal{F}\\_{t=1} \\in \\mathbb R^{N_x \\times N_y \\times N_z} \\rightarrow \\mathcal{V}_{t=1} \\in \\mathbb R^{N \\times N_x \\times N_y \\times N_z}.\n$$\n\nThe 3D-to-4D voxel expansion stage includes the following two steps: \n- **Feature Graph Generation:** We consider the voxel set $\\\\{X_k\\\\}\\_{k=1}^{N_x \\times N_y \\times N_z}$ from $\\mathcal{F}\\_{t=1}$. By filtering out invalid locations with density values below a pre-defined threshold (inspired by DVGO), we can get the node set $\\\\{X_k\\\\}\\_{k=1}^{K}$ to build the feature graph $G$. For any pair of nodes $(u,v)$ in $G$, an edge is defined between them if the following three conditions are satisfied:\n    - $u$ and $v$ correspond to spatially adjacent voxels.\n    - $u$ and $v$ result in similar emitted color $(c_u^r, c_v^r)$ produced by $N_{\\phi^\\prime}$ along an arbitrary ray $r$, such that $\\frac{1}{\\mathcal{R}}\\sum\\_r^\\mathcal{R} d\\_\\text{rgb}(c_u^r, c_v^r)<\\text{Threshold}\\_\\text{rgb}$, where $\\mathcal{R}$ is the number of rays randomly sampled with different directions. We take the Euclidean distance to calculate $d\\_\\text{rgb}(c_u^r, c_v^r)$.\n    - $u$ and $v$ lead to similar velocities across all time steps, which can be acquired through the forward deformation network $f^\\prime\\_\\xi$, such that $\\text{max}\\_t(d_\\text{vel}(u,v)) < \\text{Threshold}\\_\\text{vel}$, where we take the maximum Euclidean distance $||(f^\\prime\\_\\xi(u,t)-f^\\prime\\_\\xi(u,t-1))-(f^\\prime\\_\\xi(v,t)-f^\\prime\\_\\xi(v,t-1))||_2$ over all time steps ($t \\in [2,T]$).\n\n* **Connected Components Computation:** In this step, we first feed the feature graph $G$ into the connected components algorithm to obtain $M$ clusters. We then sort the clusters by size and merge the smallest $M-N+1$ clusters. For each cluster $p$ ($1 \\le p \\le N$), we assign the density value of voxels in $\\mathcal{F}_{t=1}$ to the corresponding channel at the same localation in $\\mathcal{V}\\_{t=1}$.\n\nWe have included **Algorithm 1** in Appendix B to provide a detailed explanation of the 3D-to-4D voxel expansion process.\n\n> Q8. $\\mathcal{L}_\\text{point}$ seems like it is never defined.\n\nThe formulation of $\\mathcal{L}_\\text{point}$ is provided in Eq. (5). Intuitively, this regularization term encourages similar color radiance values for different sampling points along the same viewing direction. \n\nAdditionally, we acknowledge that it would to more reasonable to allow various sampling points on the same ray to have slightly different colors. Therefore, we tune the loss weight $\\alpha_p$ and discuss of the effect of $\\mathcal{L}_\\text{point}$ in the appendix. As demonstrated in Table 7:\n- $\\alpha_p=0.1$ (final DynaVol) outperforms $\\alpha_p=0$. This suggests that $\\mathcal{L}_\\text{point}$ helps the training process by moderately penalizing the discrepancy among nearby sampling points on the same ray.\n- $\\alpha_p=0$ (w/o $\\mathcal{L}\\_\\text{point}$) outperforms $\\alpha\\_p=1$. This indicates that a strong $\\mathcal{L}\\_\\text{point}$ may introduce bias to neural rendering and affect the final results.\n\n> Q9. Section 3.3 seems to complain that prior compositional NeRFs used an MLP to learn a mapping from position+direction+feature to color+density, and then proposes to do the exact same thing. \n\nMost prior compositional NeRFs learn a mapping function of *(Position, Direction, Feature) $\\rightarrow$ (Color, Density)*, where the object-centric density values are generated by the model.\n\nIn DynaVol, we slightly modify this approach by using the MLP to learn a mapping function like *(Position, Direction, Feature) $\\rightarrow$ Color*, while the density values are directly retrieved from the 4D voxel grids of $\\mathcal{V}^t_{\\text{density}}$. The value in each grid cell indicates the occupancy probabilities of each object. This method allows for easy manipulation of objects for scene editing."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1533/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652514923,
                "cdate": 1700652514923,
                "tmdate": 1700656034800,
                "mdate": 1700656034800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]