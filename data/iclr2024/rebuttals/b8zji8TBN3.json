[
    {
        "title": "A One-Step MSE Estimation of Models in Production"
    },
    {
        "review": {
            "id": "ZyQS8XA3vD",
            "forum": "b8zji8TBN3",
            "replyto": "b8zji8TBN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_ZbmE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_ZbmE"
            ],
            "content": {
                "summary": {
                    "value": "Model prediction evaluation metrics are important in evaluating the accuracy of the model. In particular, using models to evaluate other models is still a nascent field, and in particular have thus far been not well explored in context of regression as compared to classification. The authors suggest using a one-step method of estimating the MSE compared to other methods which evaluate both the mean and the variance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors prove good bounds as compared to a naive Jensen's approach for one-step MSE estimation. Then the authors demonstrate superior MSE estimation powers for a synthetic dataset and some real-world datasets as well."
                },
                "weaknesses": {
                    "value": "Unfortunately, the dataset choice is non-standard. Other baselines that the authors conduct their experiments against all choose to use the standard UCI datasets which are more commonly integrated, whereas the authors chose to use LIBSVM here. Some experiments on the more standard datasets would be appreciated.\n\nUse of \"tightly\" would suggest that the upper bound proposed is tight, but it seems like the authors just prove that it is tighter than that of a naive bound based off of Jensen's.\n\nOther \nMinor typos:\noverffiting"
                },
                "questions": {
                    "value": "Why 1000 samples for the datasets in particular? Seems like authors are trying to tackle the case for when it is not true that \"abundant numbers of samples are available\", but that doesn't seem to be part of the main points.\nIf that's the case, then also checking results on the full dataset would also be helpful to prove the point, especially since the paper claims that their objective is consistent against overfitting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4684/Reviewer_ZbmE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698255095966,
            "cdate": 1698255095966,
            "tmdate": 1699636449991,
            "mdate": 1699636449991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "qUCW1DtY9Z",
            "forum": "b8zji8TBN3",
            "replyto": "b8zji8TBN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_TPbn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_TPbn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new MSE estimation method. Instead of learning the mean and variance of the outputs, the proposed method learns a check model which gives pseudo labels for the MSE estimation. Generalization bounds and experiments have been provided to justify the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe paper provides both theoretical and empirical demonstration for the proposed method.\n-\tThe empirical improvements in real-world data seem to be significant."
                },
                "weaknesses": {
                    "value": "- The clarity needs improvement.\n- The comparison to existing work needs to be clearer.\n\nSee \"Questions\" for details."
                },
                "questions": {
                    "value": "-\tThe argument of Vapnik\u2019s principle is vague. Why is mean and variance estimation more complicated than MSE? Why is the proposed method less complicated than previous methods? The argument needs to be more rigorous.\n-\tThe authors claimed that the disadvantage of uncertainty estimation methods is the high uncertainty for small data regions. \u201cUncertainty in areas where p(x) is small is often very high, capturing epistemic uncertainty (a.k.a knowledge uncertainty), and this high uncertainty may be unnecessary and potentially worsen the MSE estimation error.\u201d I do not follow the argument here. Does it mean we should not consider epistemic uncertainty? \n-\tWhat data does Eq.4 compute over? Does the training of the check model require a validation set? If so, does the check model just try to fit the validation set?\n-\tWhy do existing estimation methods for classification not apply to regression? The paper only mentions they are not applicable since classification has discrete outputs and regression has continuous outputs. It is still unclear why they are not applicable. \n-\tIt is unclear what the differences are between the proposed check model method and existing check model methods.\n-\tWhat if the variance of the small data region (x in [2,3]) is indeed very large. Will the proposed method still be better than uncertainty estimation methods?\n-\tIs the proposed method affected by the performance of f(x)? In other words, will the MSE estimation be accurate for both good and bad models f?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699201866,
            "cdate": 1698699201866,
            "tmdate": 1699636449881,
            "mdate": 1699636449881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "2YpAYA1DIj",
            "forum": "b8zji8TBN3",
            "replyto": "b8zji8TBN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_q3T4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_q3T4"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel estimator for the MSE of a predictor, $E(f(x)-y)^2$, in a setting where we do not have access to test outputs and the input may exhibit covariate shift.  The method trains a \"check model\" $h(x)$ on training data to minimize the error $((E(f(x)-y)^2-E(f(x)-h(x))^2)^2$. The authors argued that the estimator has a non-degenerate behavior when $f$ is overfitted on training data, and the generalization error of $h$ can be controlled when its function class has a controlled Rademacher complexity.  Empirically, the proposed method outperform various instantiations of a naive estimator based on conditional mean and variance estimations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally well-written.\n- The proposed method demonstrates promising performance in experiments."
                },
                "weaknesses": {
                    "value": "I am concerned about the validity and relevance of various theoretical claims which are used to motivate the work.\n\n**1.** Section 3.1 claims that the proposed estimator is more robust against the overfitting of $f$ because when $f(x_i)\\equiv y_i$ for all training samples $(x_i,y_i)$, the proposed estimator has a more sensible behavior, whereas a standard MSE estimator based on estimating $E(f(x)-y\\mid x)$ will output 0.  However, \n- it is evident that any reasonable implementation of the latter should estimate $E(f(x)-y\\mid x)$ on a held-out set, in which case the pathology will not appear.\n- And the behavior of the proposed estimator is not necessarily more sensible without a similar sample splitting: as the authors note, the proposed objective reduces to $E((h(x)-y)^4)$.  This is not necessarily well-defined, for example when $y$ does not have a bounded 4th moment.  And even when it is, there is no guarantee that the minimizer is $h^*(x)=E(y|x)$, as the authors claimed (consider any skewed error distribution).\n\n**2.** Section 3.2 shows that the MSE estimate may enjoy a controlled generalization error if the hypothesis space for $h$ has a controlled Rademacher complexity.  It is unclear why such a hypothesis space cannot be adapted to estimate the mean and variance for the traditional (\"two-step\") MSE estimator.  It is true that strictly speaking, the estimation targets are different functions, so the approximation errors can be different.  But it appears to me that it is more reasonable to assume the approximation error for mean and variance estimation is at least not larger than that for $h$, as the former appears to be more natural targets."
                },
                "questions": {
                    "value": "Clarifications to point (1) above would be welcome.\n\nFor (2), I think a comparison of convergence rates for different estimators would be necessary if the authors want to claim theoretical benefits."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699099436061,
            "cdate": 1699099436061,
            "tmdate": 1699636449802,
            "mdate": 1699636449802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "RmWw7QEWib",
            "forum": "b8zji8TBN3",
            "replyto": "b8zji8TBN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_YTwt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_YTwt"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for estimating the MSE under the covariate shift assumption. The method relies on replacing the labels with a check model that is trained to minimize the mismatch with the true MSE."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing flows reasonable well in several parts of the paper and the technical sections are reasonably clear\n- The method is simple and easy to implement\n- The method comes with rigorous theoretical analysis"
                },
                "weaknesses": {
                    "value": "- First, I think some aspects of the presentation (especially the abstract and intro) can still be greatly improved. It took me a while to understand what problem the authors were trying to solve. The key statement of \"estimating MSE under covariate shift\" is buried in the preliminaries section inside a long paragraph. The words \"covariate shift\" are not found anywhere in the abstract or intro as far as I can tell.\n- Second of all, the experimental results are on the weak side. The datasets being used are simple (UCI datasets), which is potentially okay, but the results are not very strong. Out of five datasets, on at least two there seems to essentially no benefit from the method, and on two others there are for some reason massive errors bars around the metric, which makes it hard to understand what's going and if there is a true improvement.\n- Most importantly, I am confused by why this approach (and the baselines) are being used instead of a simpler approaches based on importance sampling. When seeing the problem, my first reaction is that if I have data (x,y) from distribution p_1 and unlabeled data x from distribution p_2, and I want to estimate error under p_2, than I can compute an importance sampled estimate of the MSE on data from p_1 using p_2/p_1 as my importance weights (these ratios of density can be estimated via supervised learning). I am confused by why this simple approach is not being discussed at all and why these other approaches (which to me appear less standard) are used.\n- A follow-up on the above, is that methods for domain adaptation like importance sampling (and there are many others) are not part of the baselines."
                },
                "questions": {
                    "value": "- Why are some error bars so large and can there be more experimental evidence that the method works?\n- Why would one use this method instead of importance sampling?\n- Can importance sampling be a baseline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699138891268,
            "cdate": 1699138891268,
            "tmdate": 1699636449736,
            "mdate": 1699636449736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "INApWXwtZs",
            "forum": "b8zji8TBN3",
            "replyto": "b8zji8TBN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_Mmfr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4684/Reviewer_Mmfr"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to measure and optimize models\ntrained for regression on p_tr for a downstream MSE\non heldout samples from an operational distribution p_op.\nThis setup is summarized in def 1.\nThe related work is on models that estimate the\nmean and variance, uncertainty estimation, and\naccuracy estimation for classification, e.g.,\nwith check models.\n\nThe proposed method on section 3 introduces\nanother model h to estimate the MSE that\nseeks to minimize eq (11).\nThis section also suggests that optimizing eq (11) with\nthe training distribution instead of the\noperational one is practically valid.\nSection 3.2 establishes the Rademacher complexity\nof models trained with this method,\nand section 3.3 proposes a regularization.\nThe experiments in section 4.1 investigate regressing onto\nthe synthetic functions in equation (19)\nand the experiments in section 4.2 look at some\nLIBSVM datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Understanding the generalization and downstream performance\n   of a model is an important open topic and methods such\n   as this have the potential to be impactful.\n2. The experimental results quantified in Tables 1 and 3\n   show that the proposed method best-optimizes for the\n   generalization MSE compared against many relevant baselines"
                },
                "weaknesses": {
                    "value": "I am giving this paper a low-confidence review as I am not\nan expert in this sub-area. I hope the following comments\nas an outsider will be useful for clarifying the contribution.\n\n1. The main weakness I see is in the experimental evaluation.\n  The paper jointly proposes new settings for measuring\n  the generalization MSE along with a new method.\n  It is difficult to understand if the improved performance\n  is because the method is better, or if the baselines\n  are not well-tuned or faithfully reproduced.\n  I would have found the experimental results significantly\n  clearer if they used the exact experimental settings from\n  the existing literature on related methods.\n2. One questionable part about the experimental results\n   is that their approach was tuned with a hyper-parameter\n   search resulting in the hyper-parameters in Table B.1\n   by selecting the hyper-parameters with the best MSE.\n   Then, if I understand correctly, the baseline methods were\n   evaluated using these hyper-parameters.\n   If this is true, it is unfair to the baselines as they \n   were never tuned for the MSE.\n3. On the method, I find it empirically surprising that training another\n   model h on the training set has any impact on\n   the generalization performance of the model.\n   The complexity bounds in section 3.2 explain this,\n   but I still have a difficult time understanding how to\n   interpret the complexity bounds and relating them\n   to other approaches.\n4. Despite section 2.1 presenting the training and\n   operational distributions as potentially different,\n   the theoretical results in section 3.2 and all of the\n   experiments take the training and\n   operational distributions to be the same.\n5. The readability of the paper could be improved:\n   + a) I found it confusing to read the problem setting in\n      Section 2.1 as \"preliminary\" information even though\n      it doesn't cite a reference for this problem setting.\n      The related works presented afterwards in Section 2.2\n      often do not look at exactly the formulation of Definition 1,\n      so it would be good to specifically give a reference \n      for other works solving exactly Definition 1.\n   + b) This problem statement appears to extend\n      the setting in [Chen 2021a](https://arxiv.org/pdf/2106.15728.pdf)\n      from measuring the heldout accuracy of a classifier\n      to the heldout MSE of a regression model.\n      It would be good to clearly say and cite this again\n      when defining the problem statement.\n    + c) I found it somewhat confusing at first the paper uses\n      the shorthand term \"MSE\" to refer to equation (1)\n      which measures the MSE on an operational distribution.\n      For example, this is not clear at all in the abstract,\n      where I originally thought \"MSE estimation\" was referring\n      to something else."
                },
                "questions": {
                    "value": "I have given the paper a low-confidence review where the weaknesses\nI see could be from my lack of understanding of this research sub-area.\nI am extremely open to re-evaluating my score and discussing the points\nI have brought up, especially on the experimental components."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699204631238,
            "cdate": 1699204631238,
            "tmdate": 1699636449652,
            "mdate": 1699636449652,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]