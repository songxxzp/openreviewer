[
    {
        "title": "Improved Variational Bayesian Phylogenetic Inference using Mixtures"
    },
    {
        "review": {
            "id": "nvmiGBQSyQ",
            "forum": "hVsiTj9aOO",
            "replyto": "hVsiTj9aOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_r9Xn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_r9Xn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces VBPI-Mixtures, an algorithm designed to enhance the accuracy of phylogenetic posterior distributions, particularly for tree-topology and branch-length approximations. The paper utilizesadvanced deep learning methodologies such as normalizing flows and graph neural networks to  a leading-edge black-box variational inference (BBVI) framework, Variational Bayesian Phylogenetic Inference (VBPI). The VBPI-Mixtures algorithm bridges this gap by harnessing the latest breakthroughs in mixture learning within the BBVI domain. As a result, VBPI-Mixtures algorithm is  capable of capturing distributions over tree-topologies that VBPI fails to model. On the experimental side, the paper empirically validates that a single-component approximation will struggle to properly model all parts\nof the target distribution when learned with black-bo. Additionally, the paper substantiates that the various mixture components cooperate to collectively encompass the target density. In addition, the paper demonstrates that the increased model flexibility and promotion of exploration translates into better marginal log-likelihood estimates and more accurate tree-topology posterior approximations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* VBPI-Mixtures is a new algorithm for Bayesian phylogenetics.\n* The paper shows that Mixtures of subsplit Bayesian nets (SBNs) can approximate distributions that a single SBN cannot, making a persuasive case for VBPI-Mixtures.\n* A VIMCO gradient estimator is derived for mixtures.\n* VBPI-Mixtures achieve a slightly better results than previous on eight popular real phylogenetics datasets,"
                },
                "weaknesses": {
                    "value": "* The mixture approximation for variational posterior has been already used in previous works on VAE. Therefore, the novelty is restricted to the application of Phylogenetic Inference.\n* Although the usage of Normalizing Flow could lead to an improvement in performance, they are not new in the VI context. Similar to the mixture approximation, they could be new in the application of Phylogenetic Inference.\n* The improvement in NLL compared to other baselines in Table 2 is not significant."
                },
                "questions": {
                    "value": "* It is not convincing to me to say that the proposed methods give a clear benefit in performance than other baselines. Could the paper include new experiments on new datasets or provide other benefits of the algorithm e.g., computational time, memory, etc?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5164/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5164/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5164/Reviewer_r9Xn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697745930633,
            "cdate": 1697745930633,
            "tmdate": 1700630775902,
            "mdate": 1700630775902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ljhtFyfa2m",
                "forum": "hVsiTj9aOO",
                "replyto": "nvmiGBQSyQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your valuable feedback and thoughtful analysis of our paper. We appreciate the opportunity to discuss the aspects of our work in more depth.\n\n- **On the Novelty and Application of VBPI-Mixtures:** We acknowledge that mixtures of variational approximations and normalizing flows have recently been studied in VI and machine learning. However, applying these techniques to Bayesian phylogenetic inference is non-trivial, and the efforts have been previously celebrated at NeurIPS and ICLR (NFs: Zhang, (2020); GNNs: Zhang, (2023)). Hence, our novel application of mixtures in BBVI represents a meaningful advancement in phylogenetic modeling, which will be appreciated by the ICLR community. \n\n- **Regarding Improvements in NLL and Other Metrics:** We understand your concerns about the incremental improvements in NLL as shown in Table 2. However, we emphasize that in the realm of VI, the sole focus should not be on achieving the lowest NLL. Instead, the consistency and robustness of the model in approximating the target distribution are equally crucial. Our findings show significant improvements in the variance of estimates for the marginal likelihood, especially with an increase in the 'S' parameter. This reduction in variance is a vital indicator of the model's improved capability in capturing the uncertainty of phylogenetic structures, which is a key contribution of our study.\n\n- **Suggestions for Future Work and Evaluation Metrics:** We appreciate your suggestion to include additional experiments or alternate evaluation metrics such as computational time and memory usage. While these aspects were not the primary focus of the current study, they certainly represent important dimensions for assessing the practical utility of VBPI-Mixtures. After running a set of tests based on your question, we found that the most expensive cost associated with learning the VBPI-Mixtures was the sum of the costs that scale linearly with $S$ (especially, the gradient updates of the parameters of the approximations, the feedforward operations and computing the likelihood function). There is a quadratic ($S^2$) complexity in computing the denominator of MISELBO, however, in practice the associated cost is small in comparison to the costs that scale linearly. The operations that grow linearly with $S$ can all naively be parallelized across multiple threads, which hence should considerably reduce the training time of VBPI-Mixtures (this was discussed in the Appendix E).\n\nIn summary, and to precisely answer your question, the superior, state-of-the-art results achieved by increasing the number of mixture components comes at a cost that scales linearly in \n in practice. This is a great feature, as these operations can be parallelized. We want to thank you, once again, for encouraging us to explore this aspect of our proposed method. We would gladly include a version of this analysis in our revised version of the paper.\n\nIn light of these clarifications, we hope you might reconsider your scoring. We are committed to further refining our work and would greatly value any additional specific feedback or actionable suggestions that could guide us towards acceptance.\n\nThank you once again for your insightful review and the opportunity to enhance our contribution to the field of phylogenetic modeling.\n\nSincerely,"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141538859,
                "cdate": 1700141538859,
                "tmdate": 1700141538859,
                "mdate": 1700141538859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CCQO3iVAnH",
                "forum": "hVsiTj9aOO",
                "replyto": "ljhtFyfa2m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5164/Reviewer_r9Xn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5164/Reviewer_r9Xn"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your detailed answer,\n\nI raised my score to 6. \n\nBest,"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630751473,
                "cdate": 1700630751473,
                "tmdate": 1700630751473,
                "mdate": 1700630751473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PlNKLKWmjo",
            "forum": "hVsiTj9aOO",
            "replyto": "hVsiTj9aOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_7sSs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_7sSs"
            ],
            "content": {
                "summary": {
                    "value": "This paper derives variational inference Monte Carlo objectives for fitting deep mixture models to phylogenetic tree data. This enables testing mixtures of subsplit Bayesian nets to approximate distributions over the tree topology space, and favorable comparison to existing methods that are unable to leverage the benefits of mixture distributions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental validation is thorough, and the careful derivation of the VIMCO objectives is sound. The relationship to existing work is also made clear."
                },
                "weaknesses": {
                    "value": "The clarity of the paper could be significantly improved. For example, figures refer to DS4, DS7, and DS8 whereas a specific and simpler example might help a reader better understand the method. Similarly, Figure 3 is difficult to read -- perhaps separating the target distribution into a separate plot from the learned approximate posteriors could help clarify this. Further, the motivation and examples (perhaps even Figure 1) could be expanded to use cases that could include e.g. syntax trees; programming languages; models of mathematics. This would ensure the work is of broader interest than just to the phylogenetic inference community."
                },
                "questions": {
                    "value": "I am curious how the variance of the gradients compares across different numbers of samples of the importance-sampled objective. Perhaps including a plot of this could help guide practitioners to understand the trade-offs of the difficulty of implementing this method versus a single-sample variational objective."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698350854082,
            "cdate": 1698350854082,
            "tmdate": 1699636511424,
            "mdate": 1699636511424,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qFhBs9DDMP",
                "forum": "hVsiTj9aOO",
                "replyto": "PlNKLKWmjo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your constructive feedback and the valuable insights provided on our paper. We appreciate the opportunity to address your comments and further clarify aspects of our work.\n\n- **Regarding the Figures:** We are open for suggestions on how to improve our figures. However, we did not understand the reviewer's provided suggestion/imperative. Would it be possible to ask for an alternative phrasing, in order for us to optimally understand the request?\n\n- **Improvements to Figure 3:** We will incorporate your recommendations as we improve the legibility of Figure 3. This is not a major revision.\n\n- **Expanding Motivations Beyond Phylogenetics:** Applying mixtures in BBVI to different problem settings is very interesting. Using e.g. the VIMCO estimator derived in our work, efficient learning of mixtures of tree models could be acheved in a variety of domains (such as the ones you mentioned). Indeed, applying mixtures to new domains requires care and consideration to the specific generative models at hand. In this work, we concentrated our efforts to the Bayesian phylogenetics domain, an application which is rapidly receiving plenty of attention in the machine learning community (all of the following were published in ICLR or NeurIPS: (Zhang and Matsen, 2018), (Zhang and Matsen, 2019), (Zhang, 2020), (Koptagel et al, 2022), and (Zhang, 2023)). We will make sure to inspire others to explore the mentioned domains by adding text in the future work-related part of our work.\n\n- **Variance of Gradients with Different Sample Sizes:** Thank you, this is an important aspect which we have addressed using our toy experiment. We show emprically in Figure 4 and 6 that the variance of the KL (and thus the gradients) reduces with $S$ even when we decrease the number of importance samples. That is, using fewer importance samples, we get decreased variance of the gradients by increasing $S$.\n\n    Additionally, concerning the choice of $K=10$ during training, the impact $K$ has on learning the variational parameters in VBPI has previously been studied in Matsen and Zhang (2022, Section 6.1.2) and in (Zhang and Matsen, 2019), where they have concluded, after ablation studies that $K=10$ is optimal for obtaining good ML estimates. As such, and given that VBPI methods have been primarily compared in terms of their ML estimates, we followed all prior VBPI works and used $K=10$.\n    \n    \nIn light of these discussions and our commitment to enhance the clarity and applicability of our work, we hope you might reconsider the scoring of our paper. We are open to further feedback and eager to refine our contribution to not just the phylogenetic community, but to broader areas where our methodology can be applied.\n\nThank you once again for your thorough review and the opportunity to improve our work.\n\nSincerely,"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130718299,
                "cdate": 1700130718299,
                "tmdate": 1700130718299,
                "mdate": 1700130718299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nM4VmdSB7a",
            "forum": "hVsiTj9aOO",
            "replyto": "hVsiTj9aOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_TkMa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_TkMa"
            ],
            "content": {
                "summary": {
                    "value": "A black-box variational inference algorithm with mixture variational posteriors is proposed to solve the Bayesian phylogenetic inference task. Motivation and detailed derivation of the method are presented and the proposed method achieves state of the art performance on real phylogenetics datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presentation of the paper is excellent. The appropriate amount of details are given to help the reader understand the method clearly.\n- The experimental sections are comprehensive, presenting good results with multiple reasonable baselines to compare against."
                },
                "weaknesses": {
                    "value": "- The method that the paper presents is a fairly straightforward application of preexisting methods (MISELBO, VIMCO) to a specific class of problems, which suggests a minor lack in the novelty of the work.\n- The authors put a lot of emphasis on the claim that \"the components [of the mixture] jointly explore the tree-topology space.\" I find that to be a weak statement. If the resulting ELBO is better, one would naturally expect the mixture components to be different, because otherwise it would not have any representational advantage over the single-component method. I think it would be more interesting and meaningful to explore more deeply what the individual components are trying to capture."
                },
                "questions": {
                    "value": "- In section 3.1.1, \"we conclude that extending the VIMCO estimator to S > 1 cannot be trivially achieved without our derivation provided above,\" what exactly do you mean by this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5164/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5164/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5164/Reviewer_TkMa"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698441703843,
            "cdate": 1698441703843,
            "tmdate": 1699636511289,
            "mdate": 1699636511289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xSoCTjyB8a",
                "forum": "hVsiTj9aOO",
                "replyto": "nM4VmdSB7a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your thorough evaluation and positive remarks on the presentation and experimental rigor of our paper. We value your insights and would like to address the points you've raised.\n\n- **Exploring the Function of Different Mixture Components:** We appreciate your interest in a deeper exploration of the individual components of the mixtures. As you correctly noted, in figures 5 and 8 (appendix), we demonstrate how these components capture different modes of the data. We agree that a detailed examination of how these modes differ would be intriguing. However, these differences are highly dataset-dependent. While our paper focuses on a novel methodology, presented in a similar manner as the preceding VI-based phylogenetic. The modes in more complex datasets can indeed represent dissimilar tree topologies, this analysis was done in Quantifying mcmc exploration of phylogenetic tree space by (Whidden and Matsen IV, 2015).\n\n\n- **Clarification on VIMCO Extension for Discrete Mixtures:**\n    We are happy to improve the clarity of this statement by the following alternative formulation: \"we emphasize that naively replacing all occurrences of variational approximations in the VIMCO estimator with mixtures of variational approximations will not result in the correct estimator (to see this, please see Appedix C). To this end, we provide the necessary derivations in the next section and in Appendix C.\"\n\n    Appendix C will then be extended with the following text, highlighting the importance of our derivations in our proposed work: \"We demonstrate how naively introducing mixtures without carefully performing the derivation gives the incorrect expression. We do this by inspecting Eq. (5) in (Mnih and Rezende, 2016) and employing their notation. In fact, it is sufficient to consider the first term in Eq. (5) in (Mnih and Rezende ,2016), i.e. \n    \n    $E_{Q(h^{1:K}|x)}\\left[\\sum_j \\hat{L}(h^{1:K})\\nabla_{\\theta}\\log Q(h^j|x)\\right],$\n    \n    to see that the naive introduction of a mixture $Q(h|x) = \\frac{1}{S}\\sum_{s=1}^S Q_s(h|x)$, where each component has parameters $\\theta_s,$ gives \n    \n    $\\frac{1}{S}\\sum_{s=1}^SE_{Q_s(h_s^{1:K}|x)}\\left[\\sum_j \\hat{L}(h_s^{1:K})\\nabla_{\\theta_s}\\log \\frac{1}{S}\\sum_{s'=1}^S Q_{s'}(h_s^j|x)\\right],$\n    \n    which is an incorrect expression for the first term of the VIMCO estimator for mixtures. Instead, the correct expression, as we have showed in our submission, is \n    \n    $\\frac{1}{S}E_{Q_s(h_s^{1:K}|x)}\\left[\\sum_j \\hat{L}(h_s^{1:K})\\nabla_{\\theta_s}\\log Q_{s}(h_s^j|x)\\right].$ \n    \n    As such, carefully performing the derivation is necessary to arrive at the correct expression for the objective.\"\n    \n    \n    As a final remark on the utility of our derivation of the VIMCO estimator for mixtures: In the original VBPI paper (Zhang and Matsen IV, 2019), the VIMCO estimator is derived for their specific choices of model and variational distributions. The derivation (given in their Appendix A) closely follows the original VIMCO paper (Mnih and Rezende, 2016). In a similar vein, we extend this VIMCO estimator for a new class of variational distributions, mixture distributions, which proved to be a less straightforward derivation than the one provided in (Zhang and Matsen IV, 2019). To our knowledge, this derivation is not available in the literature, and so our extension is an important contribution to the field of mixture learning in BBVI for discrete latent variable models.\n\nIn light of these clarifications, we hope that you might consider revising your score. We are open to further feedback and committed to enhancing the value of our work within the phylogenetic inference community and beyond.\n\nThank you once again for your comprehensive review and the opportunity to clarify these aspects of our study.\n\nSincerely,"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130587862,
                "cdate": 1700130587862,
                "tmdate": 1700130587862,
                "mdate": 1700130587862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T6MtlC1xgR",
            "forum": "hVsiTj9aOO",
            "replyto": "hVsiTj9aOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_Byae"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5164/Reviewer_Byae"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the problem of inferring the posterior over phylogenetic trees given a Bayesian model as well as nucleotide sequence data. The authors approach this from a Variational Inference perspective and seek to enhance existing VI techniques by combining advances in mixture-based Black Box VI methods with advances in modeling phylogenetic trees using subsplit Bayes Nets. Combining these two methods gives numerous advantages such as the ability to better model multimodal posteriors using mixtures as well as the ability to model correlations between different subsplits.\n\nThe authors have provided a mathematical derivation of the gradient update equation for their variational approximation and shown how this can be computed in a stable manner. Further, experimental results confirm that the new methods produces better marginal log likelihood on standard phylogenetic datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors have motivated the problem quite well in terms of why they chose to combine mixture-based VI methods with subsplit Bayesian networks for modeling phylogenetic data.\n\nThe mathematical derivations seem sound and the authors seem to be well aware of recent advances in VI for estimating stable gradients which tends to be very important.\n\nI fully understood the paper and all the key points made without any background in phylogenetics. Although I did have to read some primers on phylogenetics to fully understand what these trees represented and what things like a \"clade\" meant. However, the focus of this paper is not so much on phylogenetics and more on VI, so it would be of relevance to the audience of this conference."
                },
                "weaknesses": {
                    "value": "The work seems to lack in technical depth. The main gradient update equations for VI which are the crux of this paper follow quite naturally from prior work on mixtures in VI. Equation 6, for example follows directly from prior work. All of the observations made in this paper about the advantage of using a mixture in VI are from prior work.\n\nThe derivation in equation 9 does seem somewhat new and this is perhaps the only thing that I couldn't directly pin on a prior paper. However, I didn't see any technical issues in going from equation 6 to equation 9.\n\nThe analysis in the experiments fail to convince an outsider to phylogenetics. I see that there is a claim of better marginal log likelihood. First of all, it is not clarified whether this improvement is observed on held-out test data. More importantly, there is no demonstrated improvement in accuracy, say, on a downstream phylogenetic task. In other words, it is not possible to estimate how valuable this contribution is to the field of phylogenetics."
                },
                "questions": {
                    "value": "Is there a downstream task on which the better posteriors of the phylogenetic trees can be demonstrated to have an effect?\n\nHave the authors considered other approaches to model multimodal posteriors such as Stein Variational Inference? https://arxiv.org/abs/1608.04471\n\nWhy does the paper use the terminology KL (p || q) in the figures and in the text rather than KL (q || p)? The latter is more common in VI and the paper also seems to be using the latter since all the expectations are taken with respect to to q. In the toy example, I agree that KL(p||q) could be computed, but I would still suggest to report KL(q||p) as well.\n\nThe paper claims the following, \"Mixtures of SBNs allow for modeling correlations in the sampling of the partitions, and thus increase the flexibility of the approximation.\" Now, I understand the claim in terms of increasing the flexibility of the approximation, but I do not understand (lacking any phylogenetics background) as to why this flexibility is important in phylogenetics. A very native reader like myself might ask why two disjoint parts of the phylogenetics tree don't evolve independently?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699062785382,
            "cdate": 1699062785382,
            "tmdate": 1699636511199,
            "mdate": 1699636511199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kYq6pgV1zC",
                "forum": "hVsiTj9aOO",
                "replyto": "T6MtlC1xgR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5164/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your insightful comments and constructive feedback on our paper. We appreciate the opportunity to clarify and emphasize certain aspects of our work.\n\n- **Novelty of Applying Previous Works:** We wish to highlight that novel applications of existing methodologies also contribute significantly to scientific progress. In our paper, the combination of mixture-based Variational Inference (VI) methods with subsplit Bayesian networks represents an innovative approach in phylogenetic modeling. This synergy not only advances the field but also lays a foundation for future research.\n\n- **Modeling Approach and Data Analysis:** Regarding the absence of traditional test or train data in our models, it's important to clarify that our goal is to approximate a posterior distribution given the data at hand. The model itself serves as an analytical tool for downstream analysis. The relevance of analyzing disjoint parts of phylogenetic trees arises from the fact that each component captures different structures or modes of the topology-space of the posterior distribution. This independent analysis allows for a more nuanced understanding of the evolutionary process. Figures 1, 5, and 8 in our paper illustrate how different components identify distinct clusters within the topology space, highlighting the importance and utility of mixture components in phylogenetic studies.\n\n- **Clarification on KL Divergence:** We appreciate your observation regarding the use of KL divergence terminology in our work. To clarify, our Variational Bayesian Phylogenetic Inference (VBPI) structure is trained using KL(q\\||p) and evaluated using KL(p\\||q). This approach is consistent with earlier methodologies employed in the works [1]-[5] by Zhang and Matsen. This dual usage of KL divergence in training and evaluation phases is crucial for the effectiveness of our model. Any confusion arising from our presentation of these concepts will be addressed and made clearer in the revised manuscript.\n\n- **Phylogenetic Modeling Standards:** In response to your query about downstream tasks in phylogenetic modeling, it is indeed uncommon in this domain to focus on such tasks during model development. Our approach aligns with standards established in previous seminal papers (e.g., [1]-[5] by Zhang and Matsen). Our objective is to refine phylogenetic inference, contributing a tool of analytical significance to the field.\n\n- **Future Work and SVI:** Your suggestion regarding Stein Variational Inference (SVI) is intriguing. While we have not explored this in the current study, it certainly presents an exciting avenue for future research. But i far outside the scope of this paper. \n\nIn conclusion, we were surprised by the lower score assigned to our paper, considering the depth and potential impact of our work in the field of phylogenetic modeling. We hope our responses have addressed your concerns satisfactorily. We would be very grateful if you could reconsider your scoring, particularly in light of the clarifications and discussions presented. Alternatively, if there are specific areas you believe require further improvement, we would highly appreciate actionable suggestions that could guide us toward acceptance. We are open to further discussion and eager for the opportunity to enhance our work based on your expert feedback.\n\nSincerely,\n\n\n    [1] C. Zhang. Improved variational Bayesian phylogenetic inference with normalizing flows, 2020\n    [2] C. Zhang. Learnable topological features for phylogenetic inference via graph neural networks, 2023\n    [3] C. Zhang and F. A. Matsen IV. Generalizing tree probability estimation via Bayesian networks, 2018\n    [4] C. Zhang and F. A. Matsen IV. Variational Bayesian phylogenetic inference, 2019\n    [5] C. Zhang and F. A. Matsen IV. A variational approach to Bayesian phylogenetic inference, 2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037174694,
                "cdate": 1700037174694,
                "tmdate": 1700120335746,
                "mdate": 1700120335746,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]