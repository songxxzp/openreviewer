[
    {
        "title": "SWE-bench: Can Language Models Resolve Real-world Github Issues?"
    },
    {
        "review": {
            "id": "IetQFNhEPS",
            "forum": "VTF8yNQM66",
            "replyto": "VTF8yNQM66",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_BfZn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_BfZn"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new benchmark, SWE-bench, which collects code and issues from 12 Python repositories. This benchmark also considers the convenience of subsequent evaluation, and the test code for relevant issues is included. Moreover, this paper also finetunes Code Llama with SWE-bench training data. Experimental results show that there are still many challenges for existing LLM to solve real-world issues."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is generally well-written.\n2.\tThis paper introduced a new dataset SWE-bench that contains 2294 GitHub issues and related test scripts. The dataset can be used to evaluate the methods for resolving real-world GitHub issues."
                },
                "weaknesses": {
                    "value": "1.\tSome of the comparison is not very fair. As Claude 2 is trained on data up to early 2023, GPT's knowledge cutoff is September 2021 and there is no specific time for Code Llama\u2019s training data, evaluating these models on the dataset that contains instances before 2023 is not fair enough.\n2.\tThe contribution of SWE-Llama is not significant, especially for an AI conference. The paper could better target a software engineering/programming conference. \n3.\tThis method is mainly based on Code Llama while there is no comparison between Code Llama and SWE-Llama.\n4.\tSome of the experimental analysis is not solid enough. For example, in the \u201cDifficulty correlates with output length\u201d (Section 5), Table 8 only presents all successfully applied patches, and does not show the correlation between difficulty and output length. The length of other patches needs to be taken into account.\n5.\tThere are a lot of work on automated bug fixing, including LLM-based ones and traditional ones. The authors could discuss and compare. For example:\nJiang et al., Shaping Program Repair Space with Existing Patches and Similar Code, Proc. ISSTA 2018.\nD. Sobania, et al., An analysis of the automatic bug fixing performance of Chatgpt,arXiv:2301.08653, 2023."
                },
                "questions": {
                    "value": "1.\tAs the experimental results of GPT-4 are on a 20% random subset of SWE-bench while there is no comparison of other models on the same subset. If we only look at this part of the subset, are all the conclusions in the paper still valid/consistent?\n2.\tWhy are these 12 Python repositories chosen as the source of the benchmark? Does the selection of the programming language and repository influence the results of the comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563860044,
            "cdate": 1698563860044,
            "tmdate": 1699636725190,
            "mdate": 1699636725190,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R5k9nZpHcb",
                "forum": "VTF8yNQM66",
                "replyto": "IetQFNhEPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your thorough review of the paper and suggestions - the feedback has been helpful to clarify several details.\n\nPlease refer to the general response for a summary of our updates, which are also reflected in the updated paper. We also answer your specific concerns as follows:\n\n1. **Temporal comparisons to determine if knowledge cutoff provides unfair advantage**: We agree that comparing different models trained on data from different points in time could lead to unfair advantages, which is a shared concern among all LLM research.\nTo fully investigate this, we added an extended temporal analysis on our results (Appendix C.3). We calculate model performance on task for each calendar year from 2018-2023.\n\n| Year | Total | 25\\% | Claude 2 | GPT-3.5 | GPT-4 | SWE-Llama 13b | SWE-Llama 7b |\n|-|-|-|-|-|-|-|-|\n| 2023    | 244 | 61  | 4.51 | 1.56 | 0.00 | 4.07 | 3.50 |\n| 2022    | 395 | 117 | 4.05 | 0.85 | 3.42 | 2.80 | 2.46 |\n| 2021    | 383 | 102 | 4.18 | 0.00 | 2.94 | 4.45 | 2.56 |\n| 2020    | 427 | 109 | 5.15 | 0.71 | 0.00 | 3.96 | 3.43 |\n| 2019    | 437 | 112 | 5.72 | 1.49 | 1.79 | 4.55 | 2.21 |\n| 2018    | 165 | 37  | 5.45 | 0.00 | 0.00 | 3.57 | 2.94 |\n| < 2018 | 89  | 36  | 4.49 | 0.00 | 2.78 | 3.37 | 1.09 |\n\n_Our original observation holds_:\n* Earlier years\u2019 problems are not easier\n* Models trained on datasets w/ later cutoff dates != Better performance on more recent tasks (i.e. GPT-3.5 vs. GPT-4).\n\nThe fact that different models are trained on data from different dates is an important issue for anyone dealing with evaluating LMs, and we believe that our analysis efforts show that our results are not tainted by this issue. SWE-bench\u2019s ease of creating new issues via automatic collection also mitigate this concern for future users of this benchmark.\n\n2. **SWE-Llama is not a significant contribution**: SWE-Llama is not the main contribution (full list in General Response). SWE-Llama is important because it establishes a lower bound for future attempts at the benchmark. It also shows the promise of fine-tuning on SWE-bench-train, which would likely improve performance for models like GPT-4.\n\n3. **No comparison between CodeLlama and SWE-Llama**: As discussed in Section 4, we found that open source models like CodeLlama are completely unable to generate well formatted patches. CodeLlama\u2019s performance is 0% in any retrieval setting, and patch generations consistently fail to apply. This prevents any meaningful evaluation of CodeLlama, and is the primary motivation for fine-tuning with SWE-bench-train. We thank you for bringing this up and have clarified this in the new version of our paper.\n\n4. **Characterizing patches regardless of whether they applied or not**: Thanks for pointing this out. Based on your feedback, we recalculate the metrics (i.e. Total Lines, Added, Removed) for an average patch generation across all patches (not just successfully applied patches). Across all metrics + models, patch generations are much closer in size to gold edits. From this table, it\u2019s clear that models struggle with generating longer patches that are correctly formatted. Further inspection of these flawed generations, as shown in Appendix F, indicate that hallucinations, abiding to existing code style/structure, and referencing long range dependencies correctly are common formatting errors that surface more frequently in longer generations.\n\n| Model | Total Lines | Added | Removed | Functions | Files |\n| --- | --- | --- | --- | --- | --- |\n| Claude 2 | 27.2 | 6.6 | 3.3 | 1.2 | 1.1 |\n| Claude 2 (Gold) | 61.6 | 17.8 | 8.6 | 2.6 | 1.4 |\n| ChatGPT-3.5 | 42.0 | 6.1 | 3.9 | 1.7 | 1.0 |\n| ChatGPT-3.5 Gold | 44.5 | 12.7 | 5.5 | 2.1 | 1.2 |\n| GPT-4 | 22.4 | 4.4 | 1.8 | 0.8 | 0.9 |\n| GPT-4 Gold | 50.3 | 14.0 | 6.5 | 2.3 | 1.3 |\n| SWE-Llama 13b | 68.9 | 9.5 | 4.3 | 2.5 | 1.6 |\n| SWE-Llama 13b Gold | 61.5 | 17.8 | 8.6 | 2.6 | 1.4 |\n| SWE-Llama 7b | 78.9 | 10.1 | 7.6 | 2.5 | 1.5 |\n| SWE-Llama 7b Gold | 65.1 | 18.8 | 9.0 | 2.7 | 1.5 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091856394,
                "cdate": 1700091856394,
                "tmdate": 1700347644799,
                "mdate": 1700347644799,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ruNGsmX3Tx",
            "forum": "VTF8yNQM66",
            "replyto": "VTF8yNQM66",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_onWq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_onWq"
            ],
            "content": {
                "summary": {
                    "value": "Authors aim to determine if LLMs can resolve real world software issues (vs constructing or fixing toy programs). Authors propose SWE-bench, a benchmark based on GitHub issues. They apply LLMs to try and fix these real-world issues and discover very poor performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Authors present a good real-world problem benchmark based on real product sized GitHub repositories and real issues fixed in them.\n- Fine tune CodeLlama 7B and 13B models to get at least somewhat positive performance on repository-wide code edits\n- Propose retrieval methods to compose input for LLMs to fit into LLM context size.\n- Evaluate LLMs on the benchmark and present general lessons from the results."
                },
                "weaknesses": {
                    "value": "- Although benchmark and LLM evaluation on it are valuable, the paper does not present any novel solutions to the task in the benchmark. This limits the contribution.\n- Please reorganize the paper so tables and figures are collocated with the text. Currently, it is hard to read when tables referenced out of order and explained very far from their location in the paper."
                },
                "questions": {
                    "value": "This sentence, especially its last part, is unclear: \"We compare the BM25 retrieval results against the oracle retrieval setting in Table 3, where we see that BM25 retrieves a superset of the oracle files in about 40% of instances with the 27,000 token context limit but only also excludes all of the oracle files in over half of instances.\". I think this is trying to explain the results in Table 3 and trying to say that in around half cases BM25 does not retrieve any of oracle files. Is this what you are trying to say? Please explain or rephrase."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728477839,
            "cdate": 1698728477839,
            "tmdate": 1699636725056,
            "mdate": 1699636725056,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g4Wx5c5cAB",
                "forum": "VTF8yNQM66",
                "replyto": "ruNGsmX3Tx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks so much for your time and effort towards this review, and for pointing out that SWE-bench is a good, real-world benchmark.\n\nWe provide a list of updates in the general response comment above. Regarding your specific comments and suggestions:\n\n1. **Lack of novel solutions to the task in the benchmark**: Past precedents such as the GLUE benchmark for NLU, WMT benchmarks for machine translation, and WikiText103 for LM perplexity, have shown the importance of simply presenting a tough benchmark for driving progress. We believe that presenting work which focuses primarily on introducing a challenging, useful benchmark can be very useful for the ML and NLP communities. Through experiments and analysis we also present actionable insights for future improvement (See Section 5, Appendix C/D/F).\n\n2. **Reorganization of the tables/figures to be closer with text**: In the latest upload of our paper, we have incorporated this suggestion and put figures closer to text and made sure tables are mentioned in order. This should improve the readability. Thanks for this suggestion.\n\n3. **Unclear sentence discussing retrieval**: Yes, your understanding is correct. To make the paper clearer we\u2019ll be replacing the mentioned sentence with the following: \"We compare the BM25 retrieval results with those of the 'oracle' retrieval setting, as shown in Table 3. We observe that in approximately 40% of instances, BM25 retrieves a superset of the oracle files for the 27000-token context limit. However, in almost half of the instances with the 27000-token limit, it retrieves none of the files from the 'oracle' context.\"\n\nThank you again for your feedback and comments. If you have any remaining questions or concerns, we would be happy to address them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091829953,
                "cdate": 1700091829953,
                "tmdate": 1700347711232,
                "mdate": 1700347711232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z2gnQt6bhP",
                "forum": "VTF8yNQM66",
                "replyto": "g4Wx5c5cAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6476/Reviewer_onWq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6476/Reviewer_onWq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to the authors for comments and corrections"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their comments, clarifications and corrections"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346860899,
                "cdate": 1700346860899,
                "tmdate": 1700346860899,
                "mdate": 1700346860899,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "65Lqz2wzmr",
            "forum": "VTF8yNQM66",
            "replyto": "VTF8yNQM66",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_YqAB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_YqAB"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new benchmark and dataset for testing the abilities of LLMs to edit large code bases.  Previously existing test suites typically involve asking the LLM to generate a small self-contained function when given a natural language description.  In contrast, the new dataset requires the LLM to create a patch, which potentially affects many files across an entire repository, when given a bug report.\n\nBug reports and repositories were scraped from Github.  Ground truth is a human-written pull request, along with additional unit tests.  Success is determined by whether the patched repository passes additional unit tests that were supplied with the pull request.\n\nThe authors conduct numerous experiments with various LLMs, and discover that existing LLMs are (unsurprisingly) very bad at this task.  They analyze and discuss a number of issues as the cause of this failure, such as limited context length, difficulty in retrieving the relevant files from large datasets, poor test coverage, and the requirement that the model output a correctly-formatted patch, rather than ordinary code."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The primary contribution of this paper is the creation of a new dataset and methodology for evaluating the performance of LLMs on real-world software engineering tasks.  The benchmark is well-designed, and can be continually updated and expanded moving forward.  The experiments with existing models are interesting, but they mainly serve to illustrate that this is a difficult and unsolved problem.  \n\nI fully expect this to be a high-impact paper, because other practitioners working in this area can now measure the performance of their models against the new benchmark.  In addition, the analysis and discussion provided by the authors provides a good starting point for guiding future research in this area. \n\nThe qualitative analysis, which compares LLM-generated patches against human-generated patches was also quite insightful."
                },
                "weaknesses": {
                    "value": "Generating a patch file, and generating code, are two very different tasks.  Existing models are pretrained on code, not patch files, so at least some of the poor performance could simply be due to the fact that the models are operating out of distribution on this data set.  (The authors mention this issue in the paper.)"
                },
                "questions": {
                    "value": "There is an additional issue with the way pretraining for code LLMs is typically done.  Due to context length limitations, the LLM often does not even see a complete file, much less a complete repository.   Moreover, the code fragments that are used for pretraining do not indicate what file they come from.  \n\nIn contrast, in order to generate a good patch file, the model must be able to see the file and directory structure of the repository.  How do you handle file names and directory structure in your experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6476/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6476/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6476/Reviewer_YqAB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786410158,
            "cdate": 1698786410158,
            "tmdate": 1699636724932,
            "mdate": 1699636724932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1J6UZLRmFv",
                "forum": "VTF8yNQM66",
                "replyto": "65Lqz2wzmr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your encouraging comments and suggestions.\n\nPlease refer to the general response for a summary of our updates. Regarding your specific concerns and questions:\n1. **Patch generation is out of domain**: We agree that LMs trained on code may seem out-of-domain for generating patch files. We ultimately think this is why open source models, including CodeLlama instruct and Llama 2 chat, are currently incapable of solving any issue in SWE-bench; they\u2019re simply not on-par with proprietary models at instruction following yet. \n\n    We considered this problem as well in an ablation in Section 5, where we find for instance, when evaluated on the shortest half of inputs in the Oracle retrieval setting, Claude achieves 3.9% resolution rate when generating whole files compared to 7.8% when generating a patch. \n\n    _We therefore find that patch generation is both a practical and efficient modeling decision for our baselines, and that is why we decided to use it across the board_.\n\n2. **Pretraining code doesn\u2019t indicate the file names or paths**: You\u2019re correct that pre-training data typically omits things like filenames, directory structures, etc. In all of our experiments, when representing code, we always wrap files\u2019 contents with a start tag like [start of src/utils/readers.py] and an end tag [end of src/utils/readers.py], as well as prepend each line with its line number to help models create well formatted patch files. We\u2019ve made this clearer in the description of our representations in the paper.\n\nWe greatly appreciate your comments and suggestions. If you have any additional questions or concerns, please let us know. Thank you for your time and consideration."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091815854,
                "cdate": 1700091815854,
                "tmdate": 1700347419094,
                "mdate": 1700347419094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8HDIwrbvOj",
            "forum": "VTF8yNQM66",
            "replyto": "VTF8yNQM66",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_r43j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6476/Reviewer_r43j"
            ],
            "content": {
                "summary": {
                    "value": "The paper primarily describes a benchmark (Swe-Bench) for evaluating language models. The benchmark consists of issues reported in github python repositories. The authors give a detailed description of the criteria they used for constructing the benchmark. They also describe the inputs to the benchmark for evaluation. They finetune the CodeLlama model for the benchmark, and then evaluate this model and others using the benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses a practically relevant issue, that of a benchmark for evaluating language models. The paper is clearly written, and quite a lot of work seems to have been done to support the material in the paper."
                },
                "weaknesses": {
                    "value": "It seems that none of the models is doing well when the benchmark is used. It would be nice if the benchmark can be used to more clearly indicate where the problem in the language model lies. The results of the model evaluation e.g. difficulty correlates with context length or difficulty correlates with output length are expected and thus do not seem very interesting"
                },
                "questions": {
                    "value": "1) It would be nice if the exact contributions of the paper are stated more clearly.\n\n2) In section1, the authors point out that there is a need for a challenging benchmark that can be used to check the abilities of language models. Although the results have been reported, I am not sure how far they evaluate the specific abilities or weaknesses. The results are general, and seem to apply to all the models without discerning the strengths/abilities or weaknesses of a particular model\n\n3) At this stage, since all the models are performing poorly, perhaps there is a need for a benchmark that is neither too simple, but not as general as SWE-bench? Wouldn't this allow some aspects of the models to be better tested and reported?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698996308519,
            "cdate": 1698996308519,
            "tmdate": 1699636724814,
            "mdate": 1699636724814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mCbVZpgZhx",
                "forum": "VTF8yNQM66",
                "replyto": "8HDIwrbvOj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your review.\n\nPlease refer to the general response for a summary of our updates.\nTo address your specific concerns and questions:\n1. **More in-depth and insightful analysis**: Given the number and diversity of problems in SWE-bench, insightful analysis of generated solutions is certainly a challenge. To provide better insight, we\u2019ve expanded our qualitative analysis in the paper to analyze 11 full generations from both Claude and SWE-Llama, showing that:\nModels tend to propose shortcut solutions which do not consider all possible inputs to the code.\nModels have a simplistic coding style that ignores the style and utilities of the overall codebase, leading to less accurate and portable solutions.\nWe provide more insights and possible solutions, e.g., prompting with documentation, learning from interaction, and test generation, for solving these problems in the paper.\n\n2. **Our exact contributions** \u2014 We restate our exact contributions in the general response comment and clarify them in the paper. Please refer to the general response for more information on this concern.\n\n3. **Results are hard to interpret/distinguish between models**: We find that many of the challenges to solving long-context code problems are common to all models evaluated. While our results show that some models have better average success, overall performance on SWE-bench is so low across the board that it can be difficult to differentiate between models\u2019 behavior. We think that SWE-bench can provide an important benchmark in guiding LM progress on the continual refinement of long context understanding, high and low-level reasoning, and grounded generation.\n\n4. **Is there a need for a benchmark even simpler than SWE-bench?**: LM progress moves fast and evaluation benchmarks are quickly outdated. We believe that setting a bar as high as SWE-bench is of great interest to the community since it could serve as a more persistent aspirational achievement for LMs. Achieving high accuracy on SWE-bench would be both incredibly useful to actual software engineers and also indicate substantial technical progress in the development of LMs. \nAdditionally, provided the structure of SWE-bench, it is possible for users to evaluate their models in slightly simpler settings (e.g. with the Oracle and Oracle-collapsed baseline), which can provide researchers some improved insights in the development of their own models.\n\nThank you again for your feedback; we\u2019ve uploaded a new version of the paper which addresses your concerns. If you have any remaining concerns or questions, please let us know."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091805461,
                "cdate": 1700091805461,
                "tmdate": 1700347354262,
                "mdate": 1700347354262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]