[
    {
        "title": "Maximum Entropy Model Correction in Reinforcement Learning"
    },
    {
        "review": {
            "id": "sQvooqqduN",
            "forum": "kNpSUN0uCc",
            "replyto": "kNpSUN0uCc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8509/Reviewer_gS8c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8509/Reviewer_gS8c"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Value Iteration (VI) and Dyna-style sample-based version utilizing an approximate environmental model. Under the assumption that the approximate model $\\hat{\\mathcal{P}}$ is given, a corrected model $\\bar{\\mathcal{P}}$ is derived by minimizing the KL divergence $D_{KL} ( \\cdot \\parallel \\hat{\\mathcal{P}})$ with feature matching constraints. The proposed method, MoCoVI and MoCoDyna, employ the corrected dynamics to learn a state value function. Then, the value function trained with the corrected dynamics is used as a new basis function used in the feature matching constraints. MoCoVI and MoCoDyna are compared with OS-VI and OS-Dyna, respectively, on the modified Cliffwalk environment. The experimental results show that the proposed methods converge to the true value function faster than the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The motivation is well explained, and the paper is overall well-written and well-organized. The proofs given in the supplementary materials are complicated, but the authors provide a detailed explanation."
                },
                "weaknesses": {
                    "value": "The proposed method is evaluated on the modified Cliffwalk environment that is relatively small. I am unsure if the proposed method is scalable."
                },
                "questions": {
                    "value": "1. At the $k$-th iteration, the proposed method uses the basis functions $\\phi_{k+1:k+d}$ and the query results $\\psi_{k+1:k+d}$ to obtain $V^k$. I would like to know why $\\psi_{1:k}$ and $\\psi_{1:k}$ are discarded. Would you discuss this point?\n2. The function approximator for the value function is unclear. Is $V$ approximated by $\\sum_{i}^d w_i \\phi_i$? If so, adding $V$ as a new basis function at the next iteration is problematic because it is linearly dependent on the basis functions. \n3. On page 4, the authors mentioned that the number of basis functions, $d$, is usually small, but I am unsure whether it is true in general. I do not know the details of the modified Cliffwalk environment, but I think it is a toy problem. Is $d$ still small if the proposed method is applied to tasks with huge state space?\n\nThe following are minor comments.\n- The end of the second paragraph in Section 2.2: $\\phi(Z)$ should be $\\phi_i(Z)$. \n- The paragraph below equation (3.2): I do not understand the following sentence: $\\bar{\\mathcal{P}}$ is not constructed by the agent.\n- In the first paragraph of Section 3.2, the authors define the function $\\phi: \\mathcal{X} \\to \\mathbb{R}^d$, but $\\phi (x, a)$ is used to compute $\\epsilon_{\\mathrm{Model}}(x, a)$. Is it a typo?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767482714,
            "cdate": 1698767482714,
            "tmdate": 1699637063183,
            "mdate": 1699637063183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nm0KysOcLU",
                "forum": "kNpSUN0uCc",
                "replyto": "sQvooqqduN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gS8c"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive review of the paper.\n\n> I am unsure if the proposed method is scalable.\n\nIn terms of scalability and the cost of optimization procedure in MaxEnt MoCo, we point out three properties that we believe makes the procedure scalable:\n\n1. **Scale of the dual problem**: The dual problem of optimization problems (P1) and (P2) has only $d$ parameters. This means that it does not scale with the size of MDP. The value of $d$ will be kept rather small, since the agent needs to store $d$ functions for $\\phi_i$ and $\\psi_i$. Moreover, the dual problem of (P2), which is the version implemented in practice, is $\\beta^2/4$-strongly convex (See equation D.15). The convexity allows efficiently solving the problem.\n\n2. **Parallelization:** Sampling from the model is usually done in batches. Correcting each of these samples is an independent optimization problem that can be parallelized with careful implementation.\n\n3. **Adjusting the computation cost:** It is possible to adjust the amount of correction to meet the computational budget. Dual parameters equal to zero correspond to no correction and using the model itself. We expect that partially optimizing the dual parameters and making few updates on them still partially corrects the model. Another technique is to use stronger regularization of dual parameters by choosing a larger value for the hyper parameter $\\beta$, which makes the loss more strongly convex and easier to optimize. We provide an analysis of MaxEnt MoCo for the general choice of $\\beta$ in Theorem 5 and 6.\n\nWe have included the runtimes of our experiments in the new revision of the paper. We have also provided the computation time for the correction procedure, which is 0.2 - 1.5 seconds for all 144 state-actions depending on the setting.\n\n### **Question 1:**\n\nThe main reason for discarding $\\phi_{1:k}$ and $\\psi_{1:k}$ is solely to conserve memory. Otherwise, the required memory of the algorithm would grow by each iteration as $O(k)$. One other minor reason is that when the number of basis functions grows, the second term in the optimization problem (P2) will dominate the first term. This can be fixed by adjusting the parameter $\\beta$ accordingly, but it would make the algorithm more complicated.\n\n### **Question 2:**\n\nNo, we never construct any value function as a linear combination of the basis functions. The performance of the algorithm does depend on how well the true value function is approximated with a linear combination of the basis functions. This is only for the purpose of theoretical analysis. The distance between the true value function and the span of basis functions is not necessarily zero and appears in our bounds. More importantly, we add the new value function obtained from planning to the basis functions which is different from the true value function. Yet, such issues can be avoided with a properly designed BasisCreation function in line 9 of the MoCoDyna algorithm, for example by subtracting the part of the new value function that belongs to the span of previous basis functions.\n\n### **Question 3:**\n\nIf the queries are accurate, the correction of MaxEnt MoCo and the convergence rate of MoCoVI improve with a larger value of $d$.  However, the main benefits of our methods are achievable even with $d=1$. For example, our theory shows that if the model is accurate enough, MoCoVI can converge to the true value function and do so faster than VI. This holds even for $d=1$ and large/continuous MDPs. Also, a single constant basis function for $d=1$ might be effective in MaxEnt MoCo (See proposition 1 and lemma 1).\n\nIn large continuous MDPs used in deep RL, the query result $\\psi_i$ will probably be obtained by training a neural network to estimate $E[\\phi(X\u2019_i)]$ at next state $X_i\u2019$ from the input $(X_i, A_i)$ as a regression problem. This means that $d$ will determine the number of heads of a neural network (if a shared network is used for all $\\phi_i$ functions) or the number of networks (if a separate network is trained for each $\\psi_i$). The choice of $d$ is a design decision. It might be beneficial to use the computation resources to train a better/larger model instead of training a huge network for queries and doing a high amount of computation for the correction procedure. For these reasons, we expect the optimal choice of $d$ to be small even in large environments.\n\n> The paragraph below equation (3.2): I do not understand the following sentence: $\\bar P$ is not constructed by the agent.\n\nIt means that the agent does not store $\\bar P$ as a function of state-actions to next-state distributions the same way it does for $\\hat P$. For example if the model $\\hat P$ is a neural network, we do not create a new network for $\\bar P$. Instead, we compute the distribution $\\bar P(\\cdot | x,a)$ when needed by solving the optimization problem (P1) or (P2)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646300810,
                "cdate": 1700646300810,
                "tmdate": 1700723118405,
                "mdate": 1700723118405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L0jl3Xi5v6",
            "forum": "kNpSUN0uCc",
            "replyto": "kNpSUN0uCc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8509/Reviewer_sy6v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8509/Reviewer_sy6v"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for MBRL, where a correction of the model is learnt. The correction improves the distribution of the next state prediction. The work consists of a model correcting approach, and its application to value iteration and to a dyna-based approach. A benefit of the approach is the combination of a model with a faster convergence thanks to the use of a model. The model-correcting prediction helps to keep the model close to the true dynamics of the environment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The work is well motivated and the introduction / background are concise yet provide enough detail to set the scene. The paper is well, logically written. At times, it leads to a question (eg isn't computing this for all (x,a) expensive?, in the intro), to later answer the question. The approach is interesting and improves over conventional approaches. Good theoretical contributions.\n\nThe appendices are useful, equally logically structured, and contain additional analysis and proofs."
                },
                "weaknesses": {
                    "value": "The main weakness in my view is the limited number of experiments, moreover the experiments are relatively simple.  I would have liked to see how the approach is used and how its performs (also in terms of time) for large or continuous environments (which seems to be planned for future work).\n\nI'm not quite sure about the related work and comparison to approaches that use multiple models to improve predictions, which would be a more appropriate comparison (apart from OS-VI, eg residual models or even ensemble models), along with a comparison of computational costs (during training or inference). Even the additional empirical results in the appendix are not particularly large."
                },
                "questions": {
                    "value": "- In the introduction, you say MaxEnt MoCo first obtains $E[\\phi_i(X')]$ for all $(x,a)$. That sounds expensive and seems to be computed on demand; but what is the cost of solving the lazy approximation (P1).\n- Can you comment about computational cost and scalablity of the approach given that the experiments were limited?\n- I may have missed it but would it make sense to compare how close the dynamics of the real environment with the predicted and corrected one? If that's the case, evaluations of how well the environment can be predicted might be helpful even without additional results from RL experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825709377,
            "cdate": 1698825709377,
            "tmdate": 1699637063070,
            "mdate": 1699637063070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nh5XVpnjgO",
                "forum": "kNpSUN0uCc",
                "replyto": "L0jl3Xi5v6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sy6v"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive review of the paper.\n\n> The main weakness in my view is the limited number of experiments, moreover the experiments are relatively simple. I would have liked to see how the approach is used and how its performs (also in terms of time) for large or continuous environments (which seems to be planned for future work).\n\nWe agree with the reviewer that more extensive experiments can be very insightful for our proposed approach. \n\n> I'm not quite sure about the related work and comparison to approaches that use multiple models to improve predictions, which would be a more appropriate comparison (apart from OS-VI, eg residual models or even ensemble models), along with a comparison of computational costs (during training or inference). Even the additional empirical results in the appendix are not particularly large.\n\nSurely there\u2019s a large body of research on dealing with model error in MBRL. Many of these efforts can be considered ways to improve the approximate dynamics $\\hat P$. Consequently our method can be implemented along with them. The empirical comparison of the gains achieved with these techniques to our method is more reasonable in deep RL experiments and is to be studied in future work.\n\n> In the introduction, you say MaxEnt MoCo first obtains $E[\\phi(X\u2019)]$ for all $(x,a)$. That sounds expensive and seems to be computed on demand; but what is the cost of solving the lazy approximation (P1).\n\nIn large or continuous MDPs, we can use function approximation to estimate these expectations. It is equivalent of fitting a regression model $\\psi$ to a dataset $(X_i, A_i, \\phi(X_i\u2019))$ where $(X_i, A_i)$ is the input and $\\phi(X_i\u2019)$ is the output. Then an estimate of that expectation can be obtained at any $(x,a)$ by simply evaluating $\\psi$ on $(x,a)$.\n\n>Can you comment about computational cost and scalablity of the approach given that the experiments were limited?\n\nIn terms of scalability and the cost of optimization procedure in MaxEnt MoCo, we point out three properties that we believe makes the procedure scalable:\n\n1. **Scale of the dual problem**: The dual problem of optimization problems (P1) and (P2) has only $d$ parameters. This means that it does not scale with the size of MDP. The value of $d$ will be kept rather small, since the agent needs to store $d$ functions for $\\phi_i$ and $\\psi_i$. Moreover, the dual problem of (P2), which is the version implemented in practice, is $\\beta^2/4$-strongly convex (See equation D.15). The convexity allows efficiently solving the problem.\n\n2. **Parallelization:** Sampling from the model is usually done in batches. Correcting each of these samples is an independent optimization problem that can be parallelized with careful implementation.\n\n3. **Adjusting the computation cost:** It is possible to adjust the amount of correction to meet the computational budget. Dual parameters equal to zero correspond to no correction and using the model itself. We expect that partially optimizing the dual parameters and making few updates on them still partially corrects the model. Another technique is to use stronger regularization of dual parameters by choosing a larger value for the hyper parameter $\\beta$, which makes the loss more strongly convex and easier to optimize. We provide an analysis of MaxEnt MoCo for the general choice of $\\beta$ in Theorem 5 and 6.\n\nWe have included the runtimes of our experiments in the new revision of the paper. We have also provided the computation time for the correction procedure, which is 0.2 - 1.5 seconds for all 144 state-actions depending on the setting.\n\n> I may have missed it but would it make sense to compare how close the dynamics of the real environment with the predicted and corrected one? If that's the case, evaluations of how well the environment can be predicted might be helpful even without additional results from RL experiments.\n\nWe thank the reviewer for the suggestion. We have now included Figure 5 in the appendix to show the error of original and corrected dynamics in our experiments. It can be observed that the correction procedure successfully reduces the model error. The effect is stronger with larger values of $d$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645731027,
                "cdate": 1700645731027,
                "tmdate": 1700645731027,
                "mdate": 1700645731027,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0heVcVZaRd",
            "forum": "kNpSUN0uCc",
            "replyto": "kNpSUN0uCc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8509/Reviewer_aRvY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8509/Reviewer_aRvY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for planning with an imperfect model, called MaxEnt Model Correction (MaxEnt MoCo). It *corrects* the next-state distribution of the model such that its expected value aligns with the true environment. This is achieved through Maximum Entropy density estimation.\n\nBuilding on top of MaxEnt MoCo, they propose Model Correcting Value Iteration (MoCoVI) and the sample-based variant Model Correcting Dyna (MoCoDyna). Both methods iteratively update the basis function, using the value functions derived from MaxEnt MoCo.\nTheoretical analysis suggests that the MoCoVI may converge to the true value function at a faster rate than approximate VI, under specific conditions. The efficacy of the proposed methods is empirically validated in a 6x6 grid world environment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper delivers rigorous theoretical results.\n\n2. The approach is novel to the best of my knowledge.\n\n3. The paper is overall well organized, making it relatively easy to follow."
                },
                "weaknesses": {
                    "value": "1. I am not fully convinced by the potential faster convergence of MoCoVI than VI. \n\n    Let\u2019s say that the model is perfect, why should MoCoVI enjoy a faster convergence? Also, in Theorem 2, the comparison between the order of $\\gamma'$  and the order of $\\gamma$ could be oversimplified. For instance, the constant could matter: the big O hides the constant $(3\\sqrt{2})^K$ which could become significant.  Besides that, the $\\gamma'^K$ also hides $(\\frac{1}{1-\\gamma})^K$ but is not addressed in the paper.\n\n    Another concern is regarding the maximum of the ratio over K steps in $\\gamma'$. Can the authors comment on the implications of it on the robustness of the algorithm?\n\n2. The gap between theory and experiments: the theoretical analysis suggests applicability to both finite and continuous MDPs, yet experimental validation is confined to a small-scale tabular MDP. Broadening the experimental scope to include continuous MDPs would substantiate the theoretical findings more comprehensively.\n\n3. The requirement in MaxEnt MoCo to sample the dynamics `d` times for each state-action visited, seems to restrict its practical applicability.\n\n4. The paper could be improved with additional clarifications in certain sections. I would ask the authors to help address the following:\n\n    a. Is there any assumption on the action space? It does not appear to have been stated in the paper, except for MoCoDyna where Algorithms 1 assumes a finite MDP. It appears that there's an implicit assumption that MaxEnt MoCo and MoCoVI are applicable to both finite and infinite action spaces. However, the approximate VI method that MoCoVI is compared with, traditionally assumes a finite action space [Munos 2007]. Could the authors clarify why the finite action space assumption is not necessary for MoCoVI?\n\n    b. The introduction claims that the theoretical analysis is applicable to \u201cboth finite and continuous MDPs\u201d. Does this applicability refer to both the exact and approximate versions of the proposed methods? Considering the complexity often associated with analyzing continuous spaces in RL, a further explanation on how the proposed analysis overcomes these challenges would be beneficial.\n\n    c. The algorithm for MoCoDyna, as presented in Section 5, is specific to finite MDPs, yet prior analysis encompasses both finite and continuous MDPs. The paper suggests the possibility of extending MoCoDyna to incorporate function approximation without elaborating on the approach. Could the authors clarify if the focus on finite MDPs is solely for the sake of a simpler presentation, or are there inherent difficulties when adapting the algorithm to function approximations?\n\n    d. The significance of the additional `c` features in the `d+c` features in MoCoDyna is not clear. Could the authors provide clarifications, and guidance on how to select `c`?\n\n    e. The paper assumes that we can make exact queries to P in MoCoVI. Yet, the algorithm description only has approximation $\\psi$ and lacks details on how $\\psi$ is updated.\n\nMinor:\n\nIt would be beneficial if the algorithmic procedures for MaxEnt MoCo and MoCoVI were presented (perhaps in the appendix) in a structured algorithm format, similar to MoCoDyna. \n\nTypos:\n\n1. Is the $|| V^{\\pi_{PE}} ||_\\infty$ before Sec. 3.2 missing anything?\n\n2. Sec. 3.2: domain of $\\phi$ should be $\\mathcal{X}$ $\\times \\mathcal{A}$ instead of $\\mathcal{X}$.\n\n3. Sec. 3.1: in the equation at the end of paragraph 1, the expectation should be w.r.t. $\\bar{P}$."
                },
                "questions": {
                    "value": "Please see the weakness section for my questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8509/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8509/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8509/Reviewer_aRvY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699311258108,
            "cdate": 1699311258108,
            "tmdate": 1699637062946,
            "mdate": 1699637062946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JqbiceZ1I0",
                "forum": "kNpSUN0uCc",
                "replyto": "0heVcVZaRd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aRvY (Part 1)"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for their thorough review and feedback. Here are some clarifications about the points raised in the review.\n\n### **1)** \nThe reviewer has pointed out an insightful special case that shows why MoCoVI converges faster than VI. Assume that the model is perfect ($\\epsilon_\\text{Model} = 0$). Also for simplicity assume that both VI and MoCoVI obtain $PV$ for any value function $V$ perfectly ($\\epsilon_\\text{Query} = 0$). In this case, MoCoVI converges immediately and $V_0$ will be the solution ($V^{\\pi_\\text{PE}}$ for PE and $V^*$ for control). This is reflected in our bound in Theorem 2 as $\\gamma\u2019 = 0$.\n\nTo see this, observe that the correction procedure will not change $\\hat P$ at all because distribution $\\hat P(\\cdot | x,a)$ satisfies the constraints in optimization problem (P1) and achieves the optimal objective value of $0$. Consequently, the corrected dynamics $\\bar P$ used to calculate $V_0$ will be the model $\\hat P$ itself, which due to our assumption of perfect model is equal to $P$. Then as described in the second paragraph of Section 4, we have $V_0 = V^*(R, \\bar P) = V^*(R, \\hat P) = V^*(R, P) = V^*$.\n\nIn general and regarding Theorem 2, we want to clarify that the big O notation used in the discussion of Theorem does not hide the constants $3 \\sqrt{2}$ and $1/(1-\\gamma)$. In fact, they do impact the asymptotic rate $O(\\gamma\u2019^k)$. This notation hides any constant coefficient for the terms in the bound (e.g. $1 - 3c_1 \\lVert \\epsilon_\\text{Model} \\rVert$ in the second inequality) but constants inside $\\gamma\u2019$ change the asymptotic rate similar to how $O(2^x)$ and $O(3^x)$ are different. The key property of this rate regardless of these constants is that as $\\epsilon_\\text{Model}$ goes to zero, the rate $\\gamma\u2019$ goes to zero as well. Thus, if the model error is smaller than a threshold, which is at most $(1-\\gamma)/(3\\sqrt{2})$, we will have $\\gamma\u2019 < \\gamma$, and MoCoVI converges faster than VI. The aforementioned constants affect the value of this threshold, but not the existence of it.\n\nThe maximum term in $\\gamma\u2019$ captures how well the linear combination of the past calculated value functions in MoCoVI can approximate the true value function. This term is upper bounded by 1, so the theorem would still hold without it. However, it could be very small or potentially zero, if for example $V^*$ is well approximate with the initial basis functions. A tighter bound would be to use the geometric mean instead of the maximum, but we decided on this version for the sake of simplicity.\n\n\n### **2)**\n We agree with the reviewer on this point. Though, the experimental setup for a model-based RL algorithm on a continuous MDP requires the use of function approximators such as DNN. Implementations that work competitively in practice usually involve a lot of implementation-level fine-tuning. In this paper we focus on the fundamental understanding of MaxEnt MoCo through theory and do not get involved with these implementation nuances. Yet, we believe the study of our algorithms in continuous MDPs with function approximation is a very interesting future work.\n\n### **3)**\n  This might be a misunderstanding as MaxEnt MoCo does not require sampling the dynamics $d$ times. Having $d$ constraints in (P1) or terms in the summation in (P2) may incorrectly suggest that we need the number of samples to be proportional to $d$ to obtain $\\psi_i$ functions (which estimate $P\\phi_i$). But this is not needed as the same set of samples can be used to find all $\\psi_i$ for $i = 1, \\dotsc, d$. For example, if we have a single sample from $X\u2019 \\sim \\mathcal{P}(\\cdot|x,a)$, we can form $\\psi_i(x,a) = \\phi_i(X\u2019)$ for all $i = 1, \\dotsc, d$.\n\nAs we mentioned in the beginning of Section 5 and 3.2, finding functions $\\psi_i \\colon \\mathcal{X} \\times \\mathcal{A} \\to \\mathbb{R}$ is a regression problem. When using function approximation in continuous MDPs, this can be done by fitting a regression model to any dataset of real samples even though it doesn\u2019t cover all state-action pairs. In finite MDPs and without the generalization of function approximation, any RL algorithm requires knowledge of all state-action pairs relevant to the problem. Still, the requirement of samples does not depend on the order $d$ of the algorithm as the same samples can be used for finding all query result functions $\\psi_i$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634913034,
                "cdate": 1700634913034,
                "tmdate": 1700644817665,
                "mdate": 1700644817665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]