[
    {
        "title": "Matrix Information Theory for Self-Supervised Learning"
    },
    {
        "review": {
            "id": "U7hXuhxOhM",
            "forum": "e1IMBXiDhW",
            "replyto": "e1IMBXiDhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission524/Reviewer_m2RW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission524/Reviewer_m2RW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Matrix-SSL, a joint-embedding SSL method based on matrix information theory.\nSpecifically, the uniformity and alignment framework is implemented using principles from matrix information theory.\nThe results of this study demonstrate that Matrix-SSL surpasses prior state-of-the-art (SOTA) SSL methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces a matrix-based information-theoretic framework that provides a comprehensive explanation for self-supervised learning methods, including both contrastive learning and non-contrastive learning."
                },
                "weaknesses": {
                    "value": "- According to Propositions 3.1 and 5.2, it can be established that the Matrix-KL-uniformity loss is synonymous with the von Neumann entropy loss of I-VNE+ as proposed in [1]. This similarity diminishes the novelty of this paper. Therefore, it is imperative to substantiate, either through theoretical or empirical means, the superiority of Matrix-SSL in comparison to I-VNE+.\n- The results presented in this paper are not significant to substantiate the effectiveness of Matrix-SSL. Notably, Table 1 does not incorporate the official performance metrics of SwAV, as reported in [2], which report values of 71.99, 73.85, and 74.81 for 100, 200, and 400 training epochs, respectively. Additionally, Table 2 lacks the inclusion of performance data as reported in [1]. When both Table 1 and Table 2 are appropriately updated, it becomes evident that the performance of Matrix-SSL is not state-of-the-art.\nFurthermore, it is important to note that this paper does not provide comprehensive benchmark tables, including but not limited to \"Semi-supervised learning on ImageNet\" and \"Transfer learning: image classification,\" as elaborated in Table 2 and Table 3 of [3].\n- In Section 5, this paper demonstrates that enhancing uniformity leads to an increased effective rank through matrix entropy. However, this result is not groundbreaking. In [1], the authors have previously presented these mathematical findings and have empirically shown that von Neumann entropy regulates uniformity, thereby influencing the effective rank.\n\n[1] VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution, CVPR 2023.\n\n[2] https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md\n\n[3] Barlow Twins: Self-Supervised Learning via Redundancy Reduction, ICML 2021."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698316464492,
            "cdate": 1698316464492,
            "tmdate": 1699635979784,
            "mdate": 1699635979784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HK4fwadT48",
                "forum": "e1IMBXiDhW",
                "replyto": "U7hXuhxOhM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1 to reviewer m2RW"
                    },
                    "comment": {
                        "value": "> Q1: According to Propositions 3.1 and 5.2, it can be established that  the Matrix-KL-uniformity loss is synonymous with the von Neumann entropy loss of I-VNE+ as proposed in [1]. This similarity diminishes the novelty of this paper. Therefore, it is imperative to substantiate, either through theoretical or empirical means, the superiority of Matrix-SSL in comparison to I-VNE+.\n\nA1: \n\n- Thank you for your reminder. Our starting point is to use $MKL( \\frac{1}{d} I  + \\lambda I || C_{auto} + \\lambda I )$ to present **a unified framework to understand many SSL methods through uniformity pursuit,** where $C_{auto}$ is the auto-correlation matrix. Specifically, we demonstrate in this case that the difference between MCE and KL is constant, and we technically considered introducing MCE because using it can easily see the connections with previous pioneer works. Note that there is an asymmetry with matrix KL, i.e. In general $MKL(P || Q) \\neq MKL(Q || P)$. Interestingly, although generally asymmetric, when $P=Q$, the two are equal and equal to 0. This non-trivial result explains the phenomenon of the increase in \"Entropy\". **Regarding the KL uniformity you mentioned, we have shown its closed-form relationship with \"Entropy\" by noticing the quantity $MKL(C_{auto}||I)$, which is novel, because it established the connection between the two quantities and this is only a special case of $\\lambda=0$.**\n\n- In addition, In our revised paper, we **generalize** the definition of von Neumann entropy on density matrix (with unit trace) into any positive semi-definite matrices, which are differentiable, convex, and can even generalize to any complex-valued Hermitian matrices. The von Neumann entropy mainly arises from the Quantum Information realm, but we certainly generalize the concept of it, removing the constraints of the measurement axiom (the capstone of Quantum Mechanics) presented in the unit trace density matrix.\n\n- Besides, we generalize the quantum and kernel KL divergence defined in Bach's paper into positive semi-definite matrices as **matrix KL divergence**, and also introduce the definition of matrix cross-entropy, which forms the **exact mirror** of **Shannon information theory** and **Quantum Information theory**, into the PSD realm. And it has a nice expression as follows:\n$\\operatorname{MCE} (P, Q) =  \\operatorname{MKL} (P || Q) + \\operatorname{ME} (P)$"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672520784,
                "cdate": 1700672520784,
                "tmdate": 1700705239199,
                "mdate": 1700705239199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4POJfvDW0w",
                "forum": "e1IMBXiDhW",
                "replyto": "U7hXuhxOhM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2 to reviewer m2RW"
                    },
                    "comment": {
                        "value": "> Q2: The results presented in this paper are not significant enough to substantiate the effectiveness of Matrix-SSL. Notably, Table 1 does not incorporate the official performance metrics of SwAV, as reported in [2], which report values of 71.99, 73.85, and 74.81 for 100, 200, and 400 training epochs, respectively. Additionally, Table 2 lacks the inclusion of performance data as reported in [1]. When both Table 1 and Table 2 are appropriately updated, it becomes evident that the performance of Matrix-SSL is not state-of-the-art. Furthermore, it is important to note that this paper does not provide comprehensive benchmark tables, including but not limited to \"Semi-supervised learning on ImageNet\" and \"Transfer learning: image classification,\" as elaborated in Table 2 and Table 3 of [3].\n\nA2:\n\nThank you for your careful reading.\n\n- **The performance issue of SwAV and the related discussion on Multi-crop**. As you mentioned, the official results reported by SwAV are indeed significantly higher than those of other methods. The main reason is that **SwAV uses a Multi-crop setting when pretraining**, augmenting the images in a scheme of $2 \\times 224$ + $6 \\times 96$, totaling to 8 views. Our implementation, however, only considered augmenting the images twice (i.e., without the six smaller augmentations). One very important point is that in the ablation study done by SwAV (https://arxiv.org/pdf/2006.09882.pdf) on Multi-crop (Figure 3), it can be seen that **Multi-crop brings a very substantial improvement to self-supervised methods**. **Thus we reported the performance of SwAV without Multi-crop**. It should be noted that **our method can also use Multi-crop** as an add-on, but due to the lack of computational resources, we have not had the opportunity to test MCE + Multi-crop. We will promptly carry out the experiments of the multi-crop case once we have sufficient computational resources. And we believe that MCE + Multi-crop could achieve even better results than what we have now. \n\n- Thank you for pointing out that including I-VNE+(https://github.com/jaeill/CVPR23-VNE/) as baseline should be considered. We attempted to include I-VNE+ in the table, but we noticed that **the official open-source code implementation of I-VNE+ uses Multi-crop** (can be seen from lines 170 to 189 at https://github.com/jaeill/CVPR23-VNE/blob/main/examples/i-vne%2B/train_ivne_imagenet100.py ). Recall the discussion of multi-crop in (1), **we believe that adding I-VNE+ to the current table would not be a fair comparison at this stage**. We will include I-VNE+ in the table after conducting experiments of MCE + multi-crop. However, we note that even so, the result of I-VNE+'s Linear evaluation on Imagenet-1K is 72.1 (as presented in Table 10 in Section F of the paper), which is significantly below the results of our method in linear evaluation, indicating from another aspect that there is a considerable difference between our work and I-VNE+.\n\n- Thank you for your suggestion to add more downstream tasks. In this short period of time, we have conducted experiments on 'Semi-supervised Learning' and 'Transfer learning: Image classification'. For experiment results, see General Response.\n\n> Q3: In Section 5, this paper demonstrates that enhancing uniformity leads to an increased effective rank through matrix entropy. However, this result is not groundbreaking. In [1], the authors have previously presented these mathematical findings and have empirically shown that von Neumann entropy regulates uniformity, thereby influencing the effective rank.\n\nA3\uff1a \n\nThank you for your careful reading of our paper. \n\nWe want to clarify that [1] didn't previously present the **general** mathematical findings of \"enhancing uniformity leads to an increased effective rank through matrix entropy\". Firstly, [1] shows that under some conditions, when the autocorrelation matrix $C_{auto}$ 's von Neumann entropy is **maximized**, the representation achieves **Isotropy**. Note **isotropy** is not **uniformity**, they do not prove anything about uniformity. Another important **missing** point is that they **do not** show that many self-supervised learning methods can be seen as minimizing $\\operatorname{MKL}( (\\lambda + \\frac{1}{d}) I || C_{auto} + \\lambda I )$, this is what we have shown in section 4. Thus, from our derivation, at the optimal point of loss, uniformity is achieved. Secondly, [1] **does not introduce the concept of effective rank.** So we think [1] **does not** show \"have empirically shown that von Neumann entropy regulates uniformity, thereby influencing the effective rank.\" But we give a **closed-form** relationship among effective rank, matrix entropy, and matrix KL divergence. Our derivation can give an understanding of the rank increasing, due to the the optimal point of SSL loss and the closed-form relationship of effective rank and matrix entropy."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672590631,
                "cdate": 1700672590631,
                "tmdate": 1700711290922,
                "mdate": 1700711290922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wxsREwohua",
            "forum": "e1IMBXiDhW",
            "replyto": "e1IMBXiDhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission524/Reviewer_CuLc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission524/Reviewer_CuLc"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate self-supervised learning through the lens of matrix information theory. They present a unified theoretical framework for analyzing both contrastive and non-contrastive learning methods. Specifically, they employ matrix cross-entropy as the training objective to enhance uniformity and alignment, thereby improving self-supervised learning. Experiments conducted on ImageNet and COCO datasets demonstrate that the proposed method outperforms existing classical approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies self-supervised learning through a matrix information-theoretic framework. The analysis presented in this paper is particularly intriguing and I find it quite appealing. \n\n2. The authors further introduce a Matrix-SSL scheme based on matrix cross-entropy, which consists of matrix uniformity and matrix alignment. \n\n3. The experiments on the ImageNet and COCO datasets not only show that the proposed method surpasses state-of-the-art methods but also highlight its robustness in transfer learning tasks."
                },
                "weaknesses": {
                    "value": "1. There are some issues with the mathematical symbol definitions in this paper, such as inconsistency in the usage of symbols, missing definitions for certain symbols, and incorrect usage of mathematical symbols. For example, on the second page, the lowercase letter \"z\" represents features, and in subsequent chapters, the bolded lowercase letter \"**z**\" also represents features. In the part of the definition of matrix entropy, the definition of the lowercase letter $\\lambda$ is missing. In the proof of proposition 3.3, there is something wrong with the infoNCE loss. I suggest that the authors follow the definitions provided by the original authors in their arXiv paper.\n2. How was Lemma 3.4 obtained? I understand the purpose of this Lemma, but it's better to give the proof or the corresponding reference. On the other hand, Lemma 3.4 shows that minimizing matrix cross-entropy between the Identity diagonal matrix and the covariance matrix can achieve a uniformity target. However, starting from the fourth page, the zero-mean assumption is disregarded. Does this have any impact on the theoretical analysis results? \n3. Starting from the third page, the authors consistently assume that the feature matrix is positive semi-definite. However, can this constraint be maintained in practice\uff1f\n4. In section 3, the authors analyze that matrix information theory could provide a unified framework for many existing SSL methods. Then, according to Theorem 3.5, Uniformity-MCE loss is equal to MEC loss. The experiments in Table 3 can verify this, where the result of Matrix-SSL (when $\\gamma=0$ ) is equal to that of MEC (70.6%). With an increase in $\\gamma$, the results will improve. This means that matrix alignment is indeed helpful for final performance improvements. Therefore, if we consider the alignment term along with the MEC loss, what will be the results? I suggest the authors conduct a detailed analysis of the differences between the MEC loss and the Uniformity-MCE loss, especially from an experimental perspective. I wonder if the gradient computation for the Uniformity-MCE loss is easier compared to the MEC loss.\n5. Although matrix-KL and matrix-CE share similar optimization properties and theoretical results, are they consistent in practical experiments? I recommend that the authors conduct a set of experiments to validate this."
                },
                "questions": {
                    "value": "Please check the questions in the Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652226588,
            "cdate": 1698652226588,
            "tmdate": 1699635979709,
            "mdate": 1699635979709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5jvVY5G21l",
                "forum": "e1IMBXiDhW",
                "replyto": "wxsREwohua",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Dear Reviewer CuLc"
                    },
                    "comment": {
                        "value": "Dear Reviewer CuLc,\n\nThank you for your comprehensive review and insightful comments on our paper. We have carefully considered your feedback and made appropriate revisions to our manuscript. Below, we address each of your concerns:\n\n> Q1: Inconsistency and missing definitions in the usage of mathematical symbols.\n\n**A1**: We apologize for the oversight regarding the consistency and clarity of mathematical symbols. We have revised the manuscript to ensure uniformity in symbol usage throughout. Specifically, we now consistently use the bolded lowercase letter \"z\" to represent features. Additionally, we have clarified the definition of matrix entropy and made corrections to the usage of mathematical symbols, adhering closely to the definitions provided in the original arXiv papers.\n\n> Q2: Clarification and proof of Lemma 3.4.\n\n**A2**: We appreciate your request for clarity on Lemma 3.4. We have updated our paper with the proofs presented in Appendix A. Regarding the zero-mean assumption, it's important to note that Lemma 3.4 (now renumbered as Lemma 4.4 following a restructuring of the related work section) is an analysis of the optimal point of uniformity pursuit. Our theoretical analysis in subsequent sections does not rely on this assumption, allowing for broader applications derived from the matrix KL divergence perspective.\n\n> Q3: The constraint of the feature matrix being positive semi-definite in practice.\n\n**A3**: The feature matrix in our framework is defined as $\\frac{1}{B}\\mathbf{Z}\\mathbf{Z}^{\\top}$, which is inherently positive semi-definite. This is a fundamental property of matrices of this form, as \n$\\frac{1}{B}\\mathbf{Z}\\mathbf{Z}^{\\top}$ results in a symmetric matrix where all eigenvalues are non-negative. We will add a brief proof and discussion in our revised manuscript to clarify this aspect and ensure its practical applicability.\n\n[proof]\nGiven a feature matrix $Z \\in \\mathbb{R}^{d \\times n}$ where $d$ represents the dimensionality of the features and $B$ the number of samples, the product $ZZ^\\top$ results in a square matrix of size $d \\times d$.\n\nTo prove that $ZZ^\\top$ is positive semi-definite, we need to show that for any non-zero vector $v \\in \\mathbb{R}^d$, the following condition holds: $v^\\top (ZZ^\\top) v \\geq 0$.\n\nExpanding $v^\\top (ZZ^\\top) v$:\n\n$$\n\\begin{aligned}\nv^\\top (ZZ^\\top) v & = v^\\top Z (Z^\\top v) \\\\\\\\\n& = (Z^\\top v)^\\top (Z^\\top v) \\\\\\\\\n& = \\| Z^\\top v \\|^2\n\\end{aligned}\n$$\n\nThe expression $\\| Z^\\top v \\|^2$ represents the square of the Euclidean norm of the vector $Z^\\top v$, which is always non-negative. Therefore:\n\n$$v^\\top (ZZ^\\top) v = \\| Z^\\top v \\|^2 \\geq 0$$\n\nSince this inequality holds for any non-zero vector $v$, it follows that $ZZ^\\top$ is a positive semi-definite matrix.\n[/proof]\n\n> Q4: Differences between the MEC loss and the Uniformity-MCE loss, especially from an experimental perspective.\n\n**A4**: Empirically, the primary difference between our method and methods using MEC loss is that Matrix-SSL requires fewer hyperparameters to be tuned and demonstrates more robust training. The introduction of matrix alignment in our framework contributes to performance improvements, as evidenced in our experiments. While MEC loss focuses on maximal entropy coding, our Uniformity-MCE loss, augmented with matrix alignment, provides a more comprehensive approach, enhancing the performance, particularly in transfer learning tasks. We plan to include a more detailed experimental comparison between these two losses in our future work to further elucidate their differences.\n\n> Q5: Are matrix-KL and matrix-CE consistent in practical experiments?\n\n**A5**: Your question raises an important point about the empirical validation of theoretical properties. While matrix-KL and matrix-CE indeed share similar optimization properties and theoretical results, their practical performance can vary due to different characteristics in optimization landscapes and convergence behaviors. To address this, we plan to conduct a comprehensive set of experiments comparing these two approaches in various settings.\n\nIn these experiments, we will assess the performance of matrix-KL and matrix-CE on standard benchmarks, focusing on metrics such as accuracy, convergence speed, and robustness to hyperparameter variations. This will allow us to determine not only their theoretical consistency but also their practical applicability and effectiveness in self-supervised learning tasks.\n\nWe believe that these additional experiments will provide valuable insights into the nuances of matrix-KL and matrix-CE, further enriching our understanding of their roles in self-supervised learning frameworks.\n\n---\n\nWe hope these responses adequately address your concerns. We remain committed to refining our work and are grateful for the opportunity to improve it based on your valuable feedback."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699764909219,
                "cdate": 1699764909219,
                "tmdate": 1699764993906,
                "mdate": 1699764993906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7qXJqZja82",
                "forum": "e1IMBXiDhW",
                "replyto": "5jvVY5G21l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Reviewer_CuLc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Reviewer_CuLc"
                ],
                "content": {
                    "title": {
                        "value": "Comments on the authors' response"
                    },
                    "comment": {
                        "value": "Thank you for addressing all of my concerns in your response. Since my concerns have been resolved, I will maintain my positive rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738106995,
                "cdate": 1700738106995,
                "tmdate": 1700738106995,
                "mdate": 1700738106995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QsTweZK4hp",
            "forum": "e1IMBXiDhW",
            "replyto": "e1IMBXiDhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission524/Reviewer_zEzQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission524/Reviewer_zEzQ"
            ],
            "content": {
                "summary": {
                    "value": "The article aims to introduce a unifying information-theoretic framework for self-supervised learning (SSL). For this purpose, the article\n -  Surveys some established SSL methodologies.,\n - Introduces matrix entropy, matrix KL divergence and matrix cross entropy measures,\n - Expresses certain existing SSL loss functions using the matrix cross entropy measure,\n - Proposes a new SSL loss function derived from matrix cross-entropy,\n - Conducts numerical experiments, demonstrating the enhanced performance of the proposed method compared to select existing approaches,\n - Draws a connection of matrix cross entropy with the effective rank."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The article's pursuit of a unifying framework offers a commendable approach. Strategy to employ  matrix information measures to achieve this is intriguing. Moreover, the numerical examples showcase marked enhancements over certain existing methods, underscoring the efficacy of the algorithm derived from this framework."
                },
                "weaknesses": {
                    "value": "The article lacks a clear organizational structure and consistent notation, making it challenging to follow. Concepts are introduced without adequate explanation or clarity. Additionally, the matrix information measures employed are not innovative; similar methods have been previously applied in the SSL context. The attempt to frame existing methods as special cases within this framework falls short of being convincing and satisfactory. Please see Questions section for details."
                },
                "questions": {
                    "value": "## INTRODUCTION\n\n- The following reference,\n\n[a] Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53,\n\nproposes utilizing \"correlative information\" maximization for self-supervised learning. Analyzing this paper within the context of the proposed matrix information framework would be interesting, especially since the authors of [a] assert that maximizing correlative information between the representations of augmentations establishes a linear dependence rather than an arbitrary nonlinear one.\n\n- Figure 1: The citation for Coco is absent. It would be beneficial to compare the performances of  Vicreg, [a] (Bardes et. al, 2021), and (Tong et. al, 2023).\n\n\n### 2.1 CONTRASTIVE AND NON-CONTRASTIVE SELF-SUPERVISED LEARNING\n\n- First paragraph: The discussion here is based on the SimCLR and SimSiam, however, the authors introduce a generic SSL architecture. Furthermore, what is meant by dual networks is not clear at this point.\n\n- Would categorizing this section into subheadings like \"Contrastive SSL Approaches\" and \"Non-Contrastive SSL Approaches\" enhance clarity?\n\n- There seems to be inconsistency in the conventions used for sample and augmentation indices in Equations (2) and (3)?\n\n- For Equation (3), is there an underlying assumption about the normalization of the encoder,  such as  $\\||z_i^{(k)}\\||_2=1$ ?\n\n\n### 2.2 MATRIX INFORMATION-THEORETIC QUANTITIES\n- Could you provide a citation detailing Matrix Entropy. Furthermore, could you discuss its interpretation, and perhaps its existing applications especially within the context of machine learning/SSL?\n\n- Regarding the Matrix Entropy definition, is there a specific assumption about the trace of $\\mathbf{A}$ ensuring its eigenvalues form a probability mass function?\n\n- Can you provide interpretations and potential applications of Matrix KL Divergence and MCE?\n\n- Bach 2022's KL divergence doesn't seem to incorporate the  $-\\mathbf{P}+\\mathbf{Q}$ terms within trace? Could this discrepancy be addressed?\n\n- From the presented definitions, it appears that, unlike Shannon Entropy, MCE does not equate to the sum of Matrix KL and Matrix Entropy. Should there be a $\\text{Tr}(\\mathbf{P})$ term included in the matrix entropy definition?\n\n- A brief discussion explaining the relevance of these definitions to the SSL problem would be insightful.\n\n\n### 3 MATRIX INFORMATION-THEORETIC PERSPECTIVES OF SELF-SUPERVISED LEARNING\n\n- The assertion about proofs should be positioned adjacent to the first proposition, i.e., Proposition 3.1.\n\n- Proposition 3.3: The statement of the proposition is  ambigious:  InfoNCE cost in (1) is the MCE of which matrices? This should be clearly stated. Are InfNCE cost and SimCLR cost identical? The cost function obtained in the proof in terms of MCE does not match (1)?\n- The paragraph after Proposition 3.3: This part appears convoluted: Is $p_{data}=p_{\\mathbf{x}}$ ? and $\\mathcal{X}$ is the support set of $p_{\\mathbf{x}}$? I guess there is no clear definition of $f$ before. (there was $f_\\Theta$ and $f_\\phi$ without clear definitions before). $f$ is sometimes unbold and sometimes bold? Since $f$ is not prespecified, the assumption that there is a layer normalization $\\||f(\\mathbf{x})\\||_2^2=1$ is also not clear. Instead of stating \"straightforward calculation\", it is better to provide a proof of Lemma 3.4 with proper notation in Appendix A. In my opinion, both this paragraph and Lemma 3.4 is not properly motivated.\n\n- Lemma 3.4: Suggestion \"Let $\\sigma$ represent the uniform distribution on $S^{d-1}$. ...\".\n\n- The paragraph after Lemma 3.4: Change of variables formula? (Inverse image rule?). Why \"auto-correlation\" matrix for $q$ but \"covariance\" for $p_{data}$?  It is better for the authors to clearly state the uniformity principle. Can't we just say that we would like features $\\mathbf{z}$ to be uncorrelated? do we need the notation for $p_{data}$, $f^{-1}$.\n\n- Quoting the sentence: \"From Proposition 3.3, we find that SimCLR (InfoNCE) loss is not canonical for achieving matrix information-theoretic uniformity unless the covariance matrix is diagonal\". What do we mean by \"loss being not canonical\"? Has matrix information-theoretic uniformity been defined yet? Is this statement simply saying that  SimCLR or InfoNCE does not enforce feature whitening?\n\n- MCE-based decorrelation objective: why do we have a $\\lambda \\mathbf{I}_d$  perturbation for the desired $\\mathbf{I}_d$ matrix? it is already perfectly conditioned. This perturbation on the first argument of the MCE does not reflect on the right side of \n\n$$\\operatorname{MCE}\\left(\\frac{1}{d} \\mathbf{I}_d+\\lambda \\mathbf{I}_d, \\frac{1}{B} \\mathbf{Z} \\mathbf{Z}^{\\top}+\\lambda \\mathbf{I}_d\\right)=-\\operatorname{tr}\\left(\\log \\left(\\frac{1}{B} \\mathbf{Z Z}^{\\top}+\\lambda \\mathbf{I}_d\\right)\\right)+1+d \\lambda$$\n\nShouldn't there be a multiplier $\\frac{1}{d}$ or $\\frac{1}{d}+\\lambda$ in front of the trace term? I suggest that $\\mathcal{L}_{UMCE}$ should be defined at this stage.\n\n- The paragraph before Theorem 3.5: Suggestion \"This MCE based uniformity loss definition (or $\\mathcal{L}_{UMCE}$ ) and its Matrix-KL divergence based counterpart are closely related.... as outlined by the following theorem:\"\n\n- Theorem 3.6: Suggestion: .... under the constraint $\\||\\mathbf{z}_i\\||_2^2=1$, for $i=1, \\ldots, n$. The proof of Theorem 3.6 better be provided in Appendix.\n\n- Suggestion: The sentence \"Our formulation interestingly recovers the Maximal Entropy Coding (MEC) loss...\" can be written as \"The Maximal Entropy Coding (MEC) loss in ... can be formulated in terms of Matrix MCE..\" as\n\n$$ \\mathcal{L}_{MEC}=-\\mu \\log\\det(\\mathbf{I}_d+\\frac{d}{B\\epsilon^2}\\mathbf{Z}_1\\mathbf{Z}_2^T)$$\n\n$$ =MCE(...., .....)$$\n\n- $\\mathcal{L}_{EMP-TCR}$: $\\bar{\\mathbf{Z}}$ is not defined. Only $\\bar{\\mathbf{Z}}_i$ is defined. Again there is a confusion of index representations relative to  Equation (1). It is understood from this statement that $\\mathbf{z}_k^{i}$ vectors were defined as row vectors. The article should set up the proper data model and notation at the beginning an should stick with that throughout the article.\n\n- Can we also have MCE based representation for the Corinfomax SSL provided in [a] above?\n\n- Overall suggestion: I suggest that the article defines all SSL-related loss functions in Section 2.1, instead of introducing some in Section 2.1 and some in  Section 3.1. Furthermore,  In Section  3.1, the article can clearly write each SSL loss function in the form \nMCE(... , ...) to show that they can be put in the form of matrix cross entropies.\n\n#### 4 MATRIX INFORMATION THEORETIC UNIFORMITY AND ALIGNMENT FOR SELF-SUPERVISED LEARNING\n\n- First sentence: .... we would like embeddings to have zero mean and covariance ....\n\n- Sentence before Theorem 4.1: optimizing covariance matrix uniformity: is this maximizing $\\mathcal{L}_{UMCE}$ or $\\mathcal{L}_{UKL}$. This should be clarified.\n\n- Theorem 4.1. This needs to be clearly reworded with proper references to the objective function and constraints. What is \"effective rank\", how is it different than rank? If this is a constraint how do you pose it?  Is the argument of the MCE in the uniformity-MCE loss sample correlation or sample covariance? Is this theorem about  the following optimization?:\n\n$$ \\text{maximize } \\mathcal{L}_{UMCE}(\\frac{1}{B}\\mathbf{ZZ}^T)$$\n$$ \\text{ subject to } \\text{tr}(\\frac{1}{B}\\mathbf{ZZ}^T)=1$$\n\nThe proof of Theorem 4.1 in the appendix requires a rewrite: Dote (Typo?)  Denote? $\\mathbf{Z}$ can be confused as a matrix due to earlier notation. I guess the first sentence states that Let $\\mathbf{x}$ be a random vector, whose distribution has support $S^{d-1}$. Again what is effective rank? This proof needs to be in the form of a series of explicit mathematical assertions referring to a clearly stated optimization problem.\n\n- Lemma 4.2 is typically well known.\n\n- For $\\mathcal{L}_{Matrix-KL-uniformity}$,  $MCE$ is used not Matrix-KL measure. Why is it called this way?\n\n5 MATRIX-SSL: UNIFORMITY AND ALIGNMENT\n\n- Regarding the alignment cost based on Matrix: \n\n1. Again it is based on MCE rather than Matrix-KL. In fact after (11), it is stated that KL versions can also be considered. So why do you call it $\\mathcal{L}_{Matrix-KL-allignment}$ ?\n\n2. The fact that covariance matrices of two matrices are aligned with respect to MCE or KL does not necessarily imply that representations for the same image are aligned in the direction, where as euclidian distance based or cosine angle based approaches try to ensure that they are sample wise aligned. So why should $\\mathcal{L}_{Matrix-KL-allignment}$ be a better choice?\n\n### 5 EFFECTIVE RANK AND RANK INCREASING PHENOMENON\n\n- It is indeed surprising that effective rank is properly defined and connected to the framework of the article much later than it is already referred. \n\n### 6 EXPERIMENTS\n\n- It would be interesting to include Tong et.al, 2023 and [a] in the experiments for comparison.\n\n- Interestingly, the proposed Matrix-SSL method provides superior performance in experimental results. A natural question to ask if the authors reproduced the accuracy of other algorithms to calibrate their simulation and evaluation models.\n\n### 7 RELATED WORK\n\nThis section typically follows  the Introduction section. Furthermore, it should not be only stating the summary of literature but it should state the contributions of the article relative to these works."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission524/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission524/Reviewer_zEzQ",
                        "ICLR.cc/2024/Conference/Submission524/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690319445,
            "cdate": 1698690319445,
            "tmdate": 1700817445189,
            "mdate": 1700817445189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cy1MvsxvdP",
                "forum": "e1IMBXiDhW",
                "replyto": "QsTweZK4hp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1 to reviewer zEzQ"
                    },
                    "comment": {
                        "value": "> Q1: The following reference,\n>\n> [a] Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in \nNeural Information Processing Systems. 2022 Dec 6;35:35240-53,\n>\n> proposes utilizing \"correlative information\" maximization for self-supervised learning. Analyzing this paper within the context of the proposed matrix information framework would be interesting, especially since the authors of [a] assert that maximizing correlative information between the representations of augmentations establishes a linear dependence rather than an arbitrary nonlinear one.\n\nA1: Thanks to your advice, we are glad to share our new finding with you that the ColorInfomax loss function can also incorporated within our framework. For the linear dependence part, according to \"Information Geometry and Its Applications\" by Amari and [2], only under Gaussian distribution, it is precise. However, we do not think the representation space should obey Gaussian distribution, this assumption is not mild. \n\n[1] https://arxiv.org/pdf/2209.07999v1.pdf, page 5: we apply the first-order Taylor series approximation.\n\n[2] https://arxiv.org/pdf/2102.05485.pdf, On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions, page 2, Equation (3).\n\n> Q2: Figure 1: The citation for Coco is absent. It would be beneficial to compare the performances of Vicreg, [a] (Bardes et. al, 2021), and (Tong et. al, 2023).\n\nA2: we have revised our paper. We moved the plot to the appendix, since there was not enough space, and we added the official result of VICReg to the Table. For the EMP-SSL (Tong et. al, 2023), they haven't reported the results of ImageNet-1k. In addition, their setting uses 100 views, but our table aims for a fair comparison, using 2-view as default.\n\n> Q3: First paragraph: The discussion here is based on the SimCLR and SimSiam, ...\n\nA3: Thanks to your advice, we have revised our paper.\n\n> Q4: Would categorizing this section into subheadings like \"Contrastive SSL Approaches\" ...\n\nA4: Thanks to your advice, we have revised our paper. \n\n> Q5: There seems to be inconsistency in the conventions used for sample...\n\nA5: Thanks to your advice, we have revised our paper. \n\n> Q6: For Equation (3), is there an underlying assumption about the normalization of the encoder, ...\n\nA6: Yes, we have emphasized it in our revised paper with \\textcolor{blue}.\n\n> Q7: Could you provide a citation detailing Matrix Entropy. Furthermore, could you discuss its interpretation, and perhaps its existing applications especially within the context of machine learning/SSL?\n\nA7: \n- We have added references to the von Neumann Entropy definition (defined on density matrices with unit trace): Edward Witten. A mini-introduction to information theory, Springer, 2020.\n\n- It seems that we are the first (as far as we know), to generalize the von Neumann Entropy definition with density matrices constraints (with unit trace) into positive semi-definite (PSD) matrices realm, and the trio of matrix entropy, matrix KL divergence, and matrix cross-entropy exactly mirror the classical Shannon information-theoretic quantities, respectively.\n\n- Its existing applications: I think the VNE (https://arxiv.org/pdf/2304.01434.pdf) paper would be the first attempt, but they mainly focus on the von Neumann Entropy part, they haven't introduced the matrix entropy defined on PSD matrices, nor the **effective rank**. In addition, the matrix cross-entropy, and the matrix KL divergence based alignment loss are proposed by us first.\n\n- From Theorem 4.5, we discovered that the renowned work Total Coding Rate, EMP-SSL, etc. by Yi Ma, Yann Lecun, et. al, is essentially equivalent to matrix cross entropy between $\\frac{d}{B \\epsilon^2} \\mathbf{Z} \\mathbf{Z}^{\\top}$ and $\\frac{1}{d}\\mathbf{I}_d$, So we think that the effectiveness of matrix entropy regularization should be attributed to them.\n\n  [1] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via lossy data coding and compression. IEEE transactions on pattern analysis and machine intelligence, 29  (9):1546\u20131562, 2007.\n\n  [2] Zengyi Li, Yubei Chen, Yann LeCun, and Friedrich T Sommer. Neural manifold clustering and embedding. arXiv preprint arXiv:2201.10000, 2022.\n\n  [3] Shengbang Tong, Yubei Chen, Yi Ma, and Yann Lecun. Emp-ssl: Towards self-supervised learning in one training epoch. arXiv preprint arXiv:2304.03977, 2023.\n\n> Q8: Regarding the Matrix Entropy definition, is there a specific assumption about the trace ensuring its eigenvalues form a probability mass function?\n\nA8: In our revised paper, we generalize the definition of von Neumann entropy, without such assumption.\n\n> Q9: Can you provide interpretations and potential applications of Matrix KL Divergence and MCE?\n\nA9: We have conducted experiments on MCE loss for fine-tuning large language models, with results presented in Appendix A and also the General Response:"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670802891,
                "cdate": 1700670802891,
                "tmdate": 1700717364751,
                "mdate": 1700717364751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KStzkCcmVF",
                "forum": "e1IMBXiDhW",
                "replyto": "QsTweZK4hp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2 to reviewer zEzQ"
                    },
                    "comment": {
                        "value": "> Q10: Bach 2022's KL divergence doesn't seem to incorporate the $-\\mathbf{P} + \\mathbf{Q}$ terms within trace? Could this discrepancy be addressed?\n\nA10: It seems that we have addressed this problem in the revised paper, we certainly generalize Bach's KL divergence onto PSD matrices, and the answers A7 and A8 are relevant to this problem.\n\n> Q11: From the presented definitions, it appears that, unlike Shannon Entropy, MCE does not equate to the sum of Matrix KL and Matrix Entropy. Should there be a $\\text{Tr}(\\mathbf{P})$ term included in the matrix entropy definition?\n\nA11: Thanks for your suggestion, we have revised our paper.\n\n> Q12: A brief discussion explaining the relevance of these definitions to the SSL problem would be insightful.\n\nA12: \n- We appreciate your suggestions, we have moved the effective rank part into the background section, and we think the section **DIMENSIONAL COLLAPSE, EFFECTIVE RANK, AND MATRIX ENTROPY** presents the closed-form relevance among effective rank, matrix KL divergence, and matrix entropy. \n\n- In addition, inspired by your suggestion, our new findings on interpreting ColorInfoMax loss within our framework (presented in Section 5) are pretty interesting. \n\n- Besides, the original version contains the matrix information-theoretic interpretation on uniformity pursuit, including several renowned methods.\n\n- We also revised our paper to further clarify the SimCLR loss part presented in Proposition 4.3.\n\n> Q13: The assertion about proofs should be positioned adjacent to the first proposition, i.e., Proposition 3.1.\n\nA13: We appreciate your advice, we have revised our paper.\n\n> Q14: Proposition 3.3: The statement of the proposition is ambigious: InfoNCE cost in (1) is the MCE of which matrices? This should be clearly stated. Are InfNCE cost and SimCLR cost identical? The cost function obtained in the proof in terms of MCE does not match (1)?\n\nA14: We have revised our formula, to make it aligned with the original InfoNCE loss.\n\n> Q15: \n> - The paragraph after Proposition 3.3: ...\n> - Lemma 3.4: Suggestion ...\n> - The paragraph after Lemma 3.4: Change of variables formula? ...\n\nA15: We have revised our paper according to your suggestion.\n\n> Q16: MCE-based decorrelation objective: why do we have a $\\lambda$ perturbation for the desired $\\mathbf{I}_d$ matrix?\n\n- We have revised our paper to fix this typo. The main reason why we introduce the $\\lambda$regularization is because we want to interpret existing SSL approaches, $\\lambda$ in our MCE formulation can be set as 0, but in other methods the result would be not satisfying. In addition, we conducted ablation studies, and find that $\\lambda = 1$ has exactly the same performance, which make our formulation more robust to hyperparamters, or even simply without such a hyperparameter.\n\n> Q17:  The paragraph before Theorem 3.5: Suggestion \"This MCE based uniformity loss definition (or $\\mathcal{L}_{\\text{UMCE}}$ ) and its Matrix-KL divergence based counterpart are closely related.... as outlined by the following theorem:\"\n\nA17: Thanks for your suggestion, we have revised our paper based on your insightful comments.\n\n> Q18:  Theorem 3.6: Suggestion: .... under the constraints... The proof of Theorem 3.6 better be provided in Appendix.\n\nA18: Thanks for your suggestion, we have revised this Theorem, and added a self-contained proof of it to the appendix.\n\n> Q19: Suggestion: The sentence \"Our formulation interestingly recovers the Maximal Entropy Coding (MEC) loss...\" can be written as \"The Maximal Entropy Coding (MEC) loss in ... can be formulated in terms of Matrix MCE..\" as\n\nA19: Thanks for your suggestion, we have incorporated this part in our revised paper.\n\n> Q20: $\\mathcal{L}_{\\text{EMP-TCR}}$: ...\n\nA20: Thanks for your suggestion, we have fixed the notation inconsistency.\n\n> Q21: Can we also have MCE based representation for the Corinfomax SSL provided in [a] above?\n\nA21: Yes, we have incorporated this part in our revised paper.\n\n> Q22: Overall suggestion: I suggest that the article defines all SSL-related loss functions in Section 2.1, instead of introducing some in Section 2.1 and some in Section 3.1. Furthermore, In Section 3.1, the article can clearly write each SSL loss function in the form MCE(... , ...) to show that they can be put in the form of matrix cross entropies.\n\nA22: Thanks for your suggestion, we have re-arranged the content."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671231121,
                "cdate": 1700671231121,
                "tmdate": 1700672698785,
                "mdate": 1700672698785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W1SmHJImIo",
                "forum": "e1IMBXiDhW",
                "replyto": "QsTweZK4hp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 3 to reviewer zEzQ"
                    },
                    "comment": {
                        "value": "> Q23: First sentence: .... we would like embeddings to have zero mean and covariance ....\n\nA23:  Thanks for your suggestion, we have revised our paper.\n\n> Q24: Theorem 4.1. This needs to be clearly reworded with proper references to the objective function and constraints. What is \"effective rank\", how is it different than rank? If this is a constraint how do you pose it? Is the argument of the MCE in the uniformity-MCE loss sample correlation or sample covariance? Is this theorem about the following optimization?:\n\nA24: Our revised content addresses the issues by clarifying the notation, explaining the concept of effective rank, and presenting the theorem in the context of an optimization problem. The theorem is now more explicitly connected to the matrix uniformity loss, and the relationship between this optimization and other similar methods is clearly outlined.\n\n> Q25: Lemma 4.2 is typically well known.\n\nA25: Thanks for your suggestion, we moved it to the appendix.\n\n> Q26: MCE is used not Matrix-KL measure. Why is it called this way?\n\nA26: We have added supplementary content to address this problem:\n$$\\begin{aligned}\n\\mathcal{L}_{\\text{Matrix-KL-Uniformity}} = \\operatorname{MCE}(\\frac{1}{d}\\mathbf{I}_d, \\mathbf{C}\\left(\\mathbf{Z}_1, \\mathbf{Z}_2\\right)) &= \\operatorname{MKL}(\\frac{1}{d}\\mathbf{I}_d || \\mathbf{Z}\\left(\\mathbf{Z}_1, \\mathbf{Z}_2\\right)) + \\operatorname{ME}(\\frac{1}{d}\\mathbf{I}_d)\\\\\n&= \\operatorname{MKL}(\\frac{1}{d}\\mathbf{I}_d || \\mathbf{Z}\\left(\\mathbf{Z}_1, \\mathbf{Z}_2\\right)) + \\text{Const.}\\\\\n\\end{aligned}$$\n\n> Q27: Again it is based on MCE rather than Matrix-KL. In fact after (11), it is stated that KL versions can also be considered. \n\nA27: We have added supplementary content to address this problem:\n- When the stop gradient is used on the first branch $\\mathbf{Z}_1$ following the standard optimization technique introduced in SimSiam, the second term $\\operatorname{ME}(\\operatorname{C}(\\mathbf{Z}_1, \\mathbf{Z}_1))$ in Matrix-KL-Alignment loss is a constant. Even when not using the stop gradient, the second term can be absorbed into the Matrix-KL-Uniformity loss.\n\n> Q28: The fact that covariance matrices of two matrices are aligned with respect to MCE or KL does not necessarily imply that representations for the same image are aligned in the direction, where as euclidian distance based or cosine angle based approaches try to ensure that they are sample wise aligned. \n\nA28: We have updated our paper. Matrix-KL-Alignment loss has a large feasible solution, such that the alignment part and the uniformity part may not have inherent contradiction, since element-wise euclidean alignment has large possibility leads to collapsed solution (SimSiam w/o stop-gradient, presented in Figure 2 in Appendix)\n\nIn addition, we have conducted ablation studies, and found that previous SOTA method MEC with our newly added matrix-KL-alignment loss can have 0.3\\% improvement on 100-epoch linear probing tasks and 0.8\\% improvement on 400-epoch COCO transfer learning tasks.\n\n> Q29: It is indeed surprising that effective rank is properly defined and connected to the framework of the article much later than it is already referred.\n\nA29: We appreciate your insights, and move the effective rank part to the background section.\n\n> Q30: It would be interesting to include Tong et.al, 2023, and [a] in the experiments for comparison.\n\nA30: Thank you for your suggestion. We will include the performance of [a](CorInfoMax) on ImageNet-1k in the table in future version. As for Tong et al., 2023 (Emp-SSL), we have noticed that they do not report linear evaluation performance on ImageNet-1k or the other benchmarks we use. Unfortunately, we currently cannot include their results in our paper. However, if we add more experiments on additional benchmarks in the future, we will include Emp-SSL in the Baselines which Emp-SSL reports performance.\n\n> Q31: Interestingly, the proposed Matrix-SSL method provides superior performance in experimental results. A natural question to ask is if the authors reproduced the accuracy of other algorithms to calibrate their simulation and evaluation models.\n\nA31: Thank you for your question. We primarily replicated the 100-epoch experiment of MEC to ensure that our code is correct. Currently, all baseline data in the table come from MEC as well as those baselines' own papers. If sufficient computational resources become available later on, we will consider replicating more baselines in our experimental environment.\n\n> Q32: This section typically follows the Introduction section. Furthermore, it should not be only stating the summary of literature but it should state the contributions of the article relative to these works.\n\nA32: Thanks for your suggestion, we have revised our paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674334852,
                "cdate": 1700674334852,
                "tmdate": 1700674334852,
                "mdate": 1700674334852,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AYp3V8kFh9",
            "forum": "e1IMBXiDhW",
            "replyto": "e1IMBXiDhW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission524/Reviewer_CjGu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission524/Reviewer_CjGu"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce Matrix-SSL, an approach grounded in matrix\ninformation theory, to improve current SSL methods.  The approach is\nmotivated through theory and the experiments show improved accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Casting various contrastive methods in a unifying notation and\nframework is helpful and shows the similarity.\n\nThe related work and cited literature is extensive and I could not\nmake out any significant missing literature.\n\nThe findings are clearly presented."
                },
                "weaknesses": {
                    "value": "Table 1 only reports the accuracy of up to 400 epochs.  It would be\ninteresting to see the dynamics of all approaches after 800 epochs,\nare they closer to Matrix-SSL?  It also does not report any mean +-\nstd over multiple runs.\n\nWhile I find the experiments convincing, it could reproduce\nstate-of-the-art better with other methods.  E.g. SimCLR is usually\ntrained for 1000 epochs, but this is not done in this paper."
                },
                "questions": {
                    "value": "Why do you think that the method works best for gamma = 1?  \n\nPerhaps the authors could comment on the computational aspect of the method?  Does it slow down training?  If yes, why?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission524/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission524/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission524/Reviewer_CjGu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761000444,
            "cdate": 1698761000444,
            "tmdate": 1699635979517,
            "mdate": 1699635979517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WOBZOqLXP9",
                "forum": "e1IMBXiDhW",
                "replyto": "AYp3V8kFh9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Dear Reviewer CjGu"
                    },
                    "comment": {
                        "value": "Dear Reviewer CjGu,\n\nThank you for your insightful feedback and positive evaluation of our work. We appreciate your recognition of our approach and the thoroughness of your review. Below, we address your questions and concerns:\n\n> Q1: Table 1 only reports the accuracy of up to 400 epochs. It would be interesting to see the dynamics of all approaches after 800 epochs. Are they closer to Matrix-SSL? It also does not report any mean \u00b1 std over multiple runs.\n\n**A1**: We value your interest in the long-term dynamics and robustness of our approach. We are currently conducting experiments for 800 epochs and plan to update our paper with these results shortly. Regarding the mean \u00b1 std, we recognize the importance of this statistical measure. Conducting multiple experiments is costly (up to tens of thousands dollars), but we are committed to performing additional runs to ensure robustness. Preliminary results from three trials show a standard deviation of less than 0.1% for our method, indicating consistency in performance.\n\n> Q2: While the experiments are convincing, could you reproduce state-of-the-art better with other methods? E.g., SimCLR is usually trained for 1000 epochs, but this is not done in this paper.\n\n**A2**: We aim for fair comparisons and acknowledge the importance of aligning our training epochs with those typically used in the literature. In line with your suggestion, we will consider extending our experiments to include 1000-epoch training for all baselines, within the limits of our resources.\n\n> Q3: Why do you think that the method works best for gamma = 1?\n\n**A3**: Our choice of gamma = 1 was empirically driven. We conducted ablation studies on the 100-epoch pre-training task, which indicated that setting gamma = 1 achieved the best linear probe performance [8]. This suggests an optimal balance in our framework between uniformity and alignment loss components, leading to superior performance.\n\n> Q4: Could you comment on the computational aspect of the method? Does it slow down training? If yes, why?\n\n**A4**: The additional computational complexity introduced by Matrix-SSL is relatively small (less than 5%) and can be controlled by adjusting the Taylor expansion order. In addition, our method need much less pre-training epochs compared to previous baselines. Furthermore, this complexity can be mitigated through parallel multi-GPU training, particularly under multi-patch augmentation settings, similar to those used in EMP-SSL within one epoch (https://arxiv.org/abs/2304.03977). We are exploring these settings in our ongoing experiments and anticipate uncovering more interesting results.\n\nWe hope these responses address your concerns satisfactorily. We are committed to improving our work based on your valuable feedback and look forward to further discussions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699759457357,
                "cdate": 1699759457357,
                "tmdate": 1699761913579,
                "mdate": 1699761913579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C02KInXc6A",
                "forum": "e1IMBXiDhW",
                "replyto": "WOBZOqLXP9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission524/Reviewer_CjGu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission524/Reviewer_CjGu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I hope that the experiments can be finished in time.  I will stand by my score (since it is already positive)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736910988,
                "cdate": 1700736910988,
                "tmdate": 1700736910988,
                "mdate": 1700736910988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]