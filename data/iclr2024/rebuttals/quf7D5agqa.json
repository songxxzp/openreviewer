[
    {
        "title": "Deep Reinforcement Learning from Weak Hierarchical Preference Feedback"
    },
    {
        "review": {
            "id": "66CkTxRD2K",
            "forum": "quf7D5agqa",
            "replyto": "quf7D5agqa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_roR3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_roR3"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method for reducing the cost of human annotations when trying to avoid manual reward engineering for reinforcement learning. Their method uses several easy to compute reward factors and it compares trajectories based on these factors by going from more important to a less important factor. These comparisons are then used to fit a reward function which is used in RL optimisation. The authors present results on a set of diverse benchmarks ranging from control to traffic simulation and coding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper attempts to design an alternative approach to reward engineering, which could potentially lead to more principle and stable performance.\n- The method also avoids costly human annotations.\n- The authors conduct a number of ablations and analysis targeted at understanding the strengths and weaknesses of the proposed method.\n- The authors study a diverse set of applications from different domains."
                },
                "weaknesses": {
                    "value": "- The paper is not very clearly written and it left me with a few unclear points which are important for the proposed problem setting. The authors start by talking about human feedback in reinforcement learning, but it is not clear from the method description what feedback is provided by humans. Is it the order of factors? Or is it feedback in the form of comparisons? What exactly is the problem setting? For example, what are the assumptions about the relationship between the reward factors and the true reward? \n- Assuming that humans provide the comparisons in the form of weak preferences, it is not clear to me how it can be ensured that comparisons are in the form described on page 4 (fixed precise threshold, always correct). Also, usually in RLHF the humans would provide comparisons of full trajectories because this is a more reliable feedback compared to other types of feedback. Assuming that the comparisons are coming from the algorithm itself, it seems to me that the method is just a form of reward engineering that allows one to find the coefficients by ensuring certain rules (the importance order of factors) are respected. \n- One limitation of the method is that it requires reward factors which are hand crafted. In many real world applications of RL there has been a tendency of moving away from the hand crafted rewards and learning reward functions directly from the observations which are represented in the raw form (e.g., images or text). I would like to hear a discussion on the applicability of the method to the various realistic domains. \n- I have some questions and concerns about the experiments. For example, in all three environments the authors say that the ground truth reward is a linear combination of the factors. In this case it is not clear to me why one would need a non-linear function (MLP) for the learnt reward. Then, regarding the baselines, a classical baseline would be a (potentially linear) reward learnt as a function of factors on the basis of trajectory comparisons from human annotators. This is the most classical RLHF setting and it would be informative to compare the proposed method to it.\n\nMinor points:\n- I didn't find Algorithm 1 and Figure 2 necessary for understanding that part of the methodology, I think it can be skipped in favor of a more clear description of the problem setting and assumptions.\n- I didn't understand why GT line is worse than other methods in Figure 3(c).\n- Section 4.3 argues that the proposed method is quite flexible. It would be nice to hear a discussion of how this compares to the studied baselines such a reward engineering."
                },
                "questions": {
                    "value": "I would like the authors to elaborate on the problem setting and assumptions (in particular relationship between the factors and true rewards). Also, I would like to understand better the principal difference of the proposed method to reward engineering and to see comparison to a more traditional reward learning scenario from RLHF."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424978147,
            "cdate": 1698424978147,
            "tmdate": 1699636445971,
            "mdate": 1699636445971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ogABPb0g2",
                "forum": "quf7D5agqa",
                "replyto": "66CkTxRD2K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the helpful review!"
                    },
                    "comment": {
                        "value": "Dear Reviewer roR3:\n\nThank you for the insightful and critical review of our work. We believe that your first two cited weaknesses stem from a misunderstanding, and that by enhancing the clarity of our paper, they can be addressed. We also include discussions on your second two points, which we believe only require minor adjustment to be addressed in our paper. Please let us know if you have any further questions.\n\n**Weakness 1: Lack of clarity**\n- The general setting we consider here is one where at each time step, we receive a state observation, take an action, and then receive the next state as well as a list of reward factors. This setting is the same as that commonly used for reward engineering. Our setting is described in detail in Section 3.1 of the submitted manuscript.\n- Given this setting, HERON asks the human overseer to assign rankings over the order of the set of reward functions, $z_1, \u2026, z_t.$ A concrete example would be (wait time > queue length > number of emergency stops) in traffic light control. This is described in the weak preference elicitation paragraph. These rankings are then used for learning.\n- We make the assumption that a decision tree over the reward factors will be able to approximate the true reward. Using this assumption has two main benefits: (1) constructing the decision tree requires less human input than the typical linear combination that reward engineering employs and (2) decision trees are highly flexible, and can therefore provide a good approximation of the true reward in most practical scenarios. Our experimental results back such an assumption.\n\n**Weakness 2: reliable comparisons**\n- In HERON, the comparisons are coming from the algorithm itself, and there is no human involved in the preference elicitation stage (see Algorithm 1). \n- Indeed, HERON is a new reward design algorithm, but we believe that it is a novel and significant contribution: no algorithms have been proposed to use the type of rankings that HERON uses, and HERON is a principled method that can outperform classical reward engineering approaches, only utilizing the rank of the reward factors and gaining robustness.\n\n**Weakness 3: Reliance on reward factors**\n- Thank you for bringing this point up. Indeed, HERON is designed for the classical setting where a set of reward factors is available. We believe that such a setting can still be useful, as demonstrated by our experiments in code generation, robotics, and traffic light control, all of which are practical environments where learning directly from human feedback would be extremely costly.\n- Moreover, we believe that using reward factors could boost emerging applications which require reward learning directly from the input, such as LLM finetuning. For example, in LLM finetuning, we could use tools such as grammar checkers and unit tests as the reward factors to allow HERON to be used. Such tools could even be used in conjunction with human feedback.\n\n**Weakness 4: Experiments and comparison to RLHF**\n- A key advantage of including the trajectory as the input of the reward model is that doing so allows us to take the trajectory context into consideration, which reward engineering does not allow for. For example, in the code generation task, we would like the reward model to give a higher score to a program if it is almost correct compared to when it is completely incorrect. This can only be done by using the trajectory as the reward model input. Also, for code generation we fine tune a pre-trained model to obtain our reward model, allowing us to transfer domain knowledge obtained through pre-training into the reward model. \n- However, we believe that the main novelty and contribution of our work comes from how we use the reward factors to elicit preference labels. Once we have these labels, any technique can be used to learn the reward function, including linear regression with the weak reward factors as inputs. \n- To address your second point, we remark that our algorithm is not meant for the setting where a human annotator is used to compare each trajectory. Therefore we do not include RLHF in our main experiments. However, we do include a comparison of HERON to RLHF in Appendix F.\n\n**Question 1: Problem setting description**\n- We include a description of our problem in Section 3.1 of the submitted manuscript.\n- We do not make any strong assumption on the ground truth reward, but we do expect that the hierarchical comparison rule created by HERON will be aligned with the comparison rule induced by the ground truth reward. We believe that this assumption is reasonable, as the human overseer has ranked the reward factors by importance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505496855,
                "cdate": 1700505496855,
                "tmdate": 1700505496855,
                "mdate": 1700505496855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ugdMrzrt4S",
            "forum": "quf7D5agqa",
            "replyto": "quf7D5agqa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_yRzv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_yRzv"
            ],
            "content": {
                "summary": {
                    "value": "This paper addressed the reward design problem in RL. This paper proposed HERON, a novel RL framework that utilizes rankings of reward factors to design reward functions by employing a hierarchical decision tree for trajectory comparisons and preference-based reward modeling. This paper assumed weak preference supervision where there are three labels in comparison: A is better, B is better, and tie. If the comparison result is \"tie\", then, this method moves to the next stage and does the same thing. This decision tree is a cascade of binary classifiers. In experiments, HERON outperforms reward engineering approaches, reduces tuning costs, improves accessibility, and achieves robustness in various RL environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- HERON introduces an approach to distill rewards from relative feature rankings, filling a crucial gap in RLHF.\n- The paper provides some evaluation, spanning a wide range of tasks.\n- Addressing the challenge of reducing the human labeling burden in RLHF, HERON offers a solution that achieves parity with RL in various tasks, making it a valuable contribution to the field."
                },
                "weaknesses": {
                    "value": "- Marginal Improvement: HERON demonstrates only marginal or no substantial improvement over straightforward heuristic baselines in many environments, reducing confidence in its efficacy.\n- Unconvincing Comparison: Directly comparing HERON to standard RLHF methods, which often operate in settings with unavailable or human-annotated reward factors, may not provide a fair or meaningful benchmark."
                },
                "questions": {
                    "value": "1. Performance Comparison: How does the proposed method compare to existing state-of-the-art approaches in terms of performance, and what are the key differentiators? Comparative analysis is crucial to demonstrate the novel contributions of the proposed method. Evaluating its performance against existing methods (such as PEBBLE or Meta-RewardNet in control domain) highlights its strengths and areas where it excels. I think that comparison with existing RLFH methods is important since this paper targets the same problem in RL. (If this paper targeted only NLP domain or focused on the contribution in code generation, I think it is not essential.)\n\n2. Addressing Limitations: Have the authors considered potential limitations and challenges of their method? Are there opportunities for further exploration or improvements in addressing these limitations? \n\n3. Practical Significance: Is there a clear motivation and real-world application for the proposed research? How does the paper demonstrate the practical significance of the findings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813826414,
            "cdate": 1698813826414,
            "tmdate": 1699636445895,
            "mdate": 1699636445895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tt5PZC9aVD",
                "forum": "quf7D5agqa",
                "replyto": "ugdMrzrt4S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed review!"
                    },
                    "comment": {
                        "value": "Dear Reviewer yRzv:\n\nThank you for the detailed review of our work. We believe that your two main concerns on marginal improvements are addressable. As for the concerns about unconvincing comparisons, we believe there is a major misunderstanding. Our method is inspired by RLHF, but is not an RLHF method. We attempt to clarify in our response and in the updated paper. Please let us know if you have any further questions.\n\n**Weakness 1: Marginal Improvements**\n- We would like to point out that the improvements over comparable baselines are not marginal. In robotics, HERON results in a 7.8% improvement compared to reward engineering. For traffic light control, HERON beats all other baselines by 33%. For Coding, HERON can beat the CodeRL method by 3.9% as measured by Pass@K and by 5.2% as measured by filtered Pass@K, while greatly reducing the tuning cost. These improvements are displayed in Tables 1, 2, and 3 and Figure 3c of the submitted manuscript.\n- Moreover, we would like to point out that our method is designed to learn from rankings, greatly reducing the tuning cost of reward design. Therefore the goal of our experiments is not necessarily to beat carefully tuned rewards, but to show that we can match them while only using rankings over reward factors.\n\n**Weakness 2: Unconvincing Comparison**\n- We believe there is a misunderstanding here. In our main experiments, we do not compare our method to RLHF techniques as RLHF considers a completely different setting than our paper considers (see section 3.1 for our problem description). Instead, RLHF serves as the motivation for our HERON algorithm, as mentioned in the introduction. HERON is designed as an alternative to reward engineering.\n- We primarily compare our method with (1) policies trained with the ground truth reward (2) heuristic algorithms that receive the same amount of information of HERON (see the description of baselines on page 6) and (3) State-of-the-art reward engineering approaches for the Coding task. All of these algorithms either receive more or an equal amount of information in the reward design process than HERON, and are therefore meaningful baselines.\n\n**Question 1: Comparison with RLHF Baselines**\n- As we previously mentioned, our work is inspired by RLHF, but is not designed for the RLHF setting. Therefore it would not be meaningful to compare it to methods like PEBBLE and MetaReward-Net, which we consider to be orthogonal directions of work. However, we do include a comparison to RLHF based learning in Appendix F, where we further examine the difference between HERON and RLHF.\n\n**Question 2: Addressing Limitations** \n- HERON essentially proposes a new RL setting, where only weak feedback is given. The main limitation of HERON is that not every RL environment will contain an obvious ranking over the reward factors, as some factors may be equally important. We have added more discussion in Appendix L of the updated paper.\n\n**Question 3: Practical Significance**\n- Our main motivating problem is code generation, where reward engineering may suffer from approximation bias and require a large tuning effort. In contrast, ranking the reward factors will be easy. Concretely, CodeRL introduces 4 tunable reward parameters, while PPOCoder has 6. In this task, we show that HERON can achieve significantly better performance than state-of-the-art baselines, therefore demonstrating HERON\u2019s practicality in the very useful task of code generation. Similarly, we achieve great results in traffic light control, which is a practical and real world application."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505167407,
                "cdate": 1700505167407,
                "tmdate": 1700505167407,
                "mdate": 1700505167407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FNcTcPC0Zx",
            "forum": "quf7D5agqa",
            "replyto": "quf7D5agqa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_rUJV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_rUJV"
            ],
            "content": {
                "summary": {
                    "value": "In reinforcement learning, it is crucial to design a reward function that reflects the objective of the task. It can be complicated or error-prone to define the exact reward function. In practice, we can define some reward factors and the reward function is a weighted combination of the reward factors.\n\nInstead of using a weighted sum of reward factors, this work proposes a ranking approach, where the reward function is determined by the ranking of the reward factors. The HERON framework is evaluated on various domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Compared to weights, ranking can be more interpretable and easier to design. Empirically, RL trained using the ranking-based reward model outperforms other baselines."
                },
                "weaknesses": {
                    "value": "Rankings of reward factors can be less expressive than weighted combinations of reward factors. In other words, some reward functions can be expressed as weighted sums, but not as a ranking of reward factors. It would be helpful to have discussions on this.\n\nClarity: It would be helpful to make the setting clearer. My understanding is that the input to the reward model training process is no longer preference data ($ \\{ (\\tau_w, \\tau_l), \\dots \\} $), but an ordered list of reward factors. In other words, the reward model training part is not changed, but only the input to the reward model training is changed.\n\nMinor:\n* In Alg 1, the condition in while is not syntactically correct ($l \\leq n \\ \\mu = 0$).\n* You may move the legend in Fig. 3 out of the plot so readers can see the plot clearly."
                },
                "questions": {
                    "value": "In the experiments, if the ground-truth reward function is a linear combination of reward factors, using the rank of significance of the reward factors is enough to learn a good policy? Empirically, this seems to be the case for robotic control tasks. Can authors provide an intuition behind this?\n\nCan reward factors serve as a reward model directly without using the method proposed in this paper? For example, we can define a reward function like 10^4 * first_reward_factor + 10^3 * second_reward_factor + ..., where 10^i can be adjusted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4655/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4655/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4655/Reviewer_rUJV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829707424,
            "cdate": 1698829707424,
            "tmdate": 1699636445823,
            "mdate": 1699636445823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9DZismSUmY",
                "forum": "quf7D5agqa",
                "replyto": "FNcTcPC0Zx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the helpful review!"
                    },
                    "comment": {
                        "value": "Dear Reviewer rUJV,\n\nThank you for the thoughtful review and suggestions. We provide our response to your concerns and questions below.\n\n**Weakness 1: Lack of expressivity.**\n- Thank you for pointing this out. We remark that HERON and classical reward engineering are two different design principles, and which one can better capture the reward may be problem specific. However, decision trees can be highly non-linear, therefore allowing HERON to approximate a wide variety of reward functions.\n- In practice, we typically find that the HERON framework does provide enough expressivity, as shown by Section 4.3 where switching the order of secondary and tertiary rewards correctly changes how the agent acts. We can further increase the flexibility of the HERON framework by tuning the margin parameters. For example, if we would like to increase the emphasis of the reward on a given reward factor, we can decrease the size of the margin for that reward factor. In this way, a greater percentage of the decisions would be taken according to the reward factor we wish to emphasize. We have added discussion about how to increase flexibility in section 4.3 of the updated paper.\n- Finally we remark that HERON enhances the capability of reward modeling in two ways: (1) it allows us to take advantage of the contextual information contained in the trajectories (which reward engineering does not do) by predicting the reward based on the trajectory and (2) it allows us to use pre-trained models as the initialization of the reward, therefore vast amounts of domain knowledge (e.g., semantic and syntactic knowledge of code) can be transferred to the learnt reward.\n\n**Weakness 2: Lack of clarity on the setting.**\n- We believe there is a misunderstanding here. The general setting we consider is one where at each time step, we receive a state observation, take an action, and then receive the next state as well as a list of reward factors. This setting is the same as that commonly used for reward engineering. Our setting is described in section 3.1 of the submitted manuscript.\n- With regards to the reward modeling setting, the only difference with RLHF comes from how we obtain the preference labels. The input to the reward model is still the trajectory segments, but now the preference label is obtained from HERON\u2019s decision tree over the reward factors. This forms the key novelty of our algorithm. Otherwise all aspects remain unchanged from standard reward learning in RLHF. These details can be found on page 4 of the submitted manuscript .\n\n**Minor comments.**\nThank you for pointing out the error in Algorithm 1 and suggesting improvements for Figure 3a. We have fixed the algorithm in the updated paper and edited Figure 3a to make the curves more visible."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504751967,
                "cdate": 1700504751967,
                "tmdate": 1700504751967,
                "mdate": 1700504751967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ij9pa6HiB6",
            "forum": "quf7D5agqa",
            "replyto": "quf7D5agqa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_9kSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4655/Reviewer_9kSM"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces HERON, which leverages the rankings of multiple reward factors (objectives) to derive reward functions using a hierarchical decision tree. This paper is focused on scenarios where numerous reward factors are available for every state-action pair, and it assumes that human experts have ranked these factors to establish their relative importance (weak preferences). When comparing two trajectories, HERON systematically evaluates the reward factors in sequence to determine their preferences. To elaborate, HERON initiates the comparison with the first reward factor, and the labeling procedure (i.e., assigning a binary label) concludes if the disparity exceeds a predefined threshold. If the dissimilarity falls below the threshold, HERON proceeds to compare the next reward factor, iteratively following this process until all the reward factors have been assessed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Paper is well structured and easy to follow.\n\n* Extensive evaluation: The authors have extensively validated their method across a wide selection of tasks, including classic control, robotic control, multi-agent traffic light control, and large language model fine-tuning for code generation."
                },
                "weaknesses": {
                    "value": "* Lack of justification in reward learning from preferences. In this work, based on ranking between multiple objectives, the authors first generate preferences and train a reward function using cross-entropy loss, which stems from the Bradley\u2013Terry model. Here, the motivation for utilizing a preference-based reward learning framework is unclear. In the original preference-based RL framework, human preferences are generated from the Bradley\u2013Terry model under an unknown utility function and the goal of reward learning is approximating this function based on preference datasets. However, when we have a ranking between multiple objectives, why we also use this framework is unclear. Basically, what is the target of the reward learning? The authors need to clarify this part.\n\n* Lack of ability in personalization. Unlike the standard preference-based learning framework, the ability to obtain personalized rewards is limited. Humans only can specify the ranking between objectives and it is hard to control the tradeoff between objectives. For example, let's consider a case where there are two objectives. Assume that there are two human annotators with the same ranking but the first human wanna get a reward function that emphasizes the first objective more. In this case, this framework can't provide a different reward for two human annotators (because the ranking is still the same). It would be nice if the authors could discuss this limitation."
                },
                "questions": {
                    "value": "* Learning curves on robotics environments. It would be nice to include the learning curves on robotics environments in the main draft or appendix. \n\n* Standard deviation across different trials is quite high in Figure 3a and Figure 3c. It would be nice to tune some hyper-parameters for DDPG or Q-learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841278981,
            "cdate": 1698841278981,
            "tmdate": 1699636445751,
            "mdate": 1699636445751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VeJTCmquv6",
                "forum": "quf7D5agqa",
                "replyto": "ij9pa6HiB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed review!"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9kSM,\n\nThank you for the insightful and clear review! We provide our responses to your questions and concerns below and look forward to further discussion.\n\n\n**Weakness 1: Lack of clarity regarding motivation of reward modeling.**\n- Note that we have explained our motivation of employing reward modeling in paragraph 5 of the introduction, where we state that we use a reward model to leverage human rankings over reward factors. Here we would like to summarize our motivation again: \n\n- The main motivation of HERON is to replace reward engineering, which requires handcrafting the reward. HERON uses human rankings over reward factors to automatically generate preferences between trajectories. These comparisons are then used to train a reward model. The motivation of using a reward model is to convert the preferences between trajectories into a numerical reward that can then be used to train the policy.\n- In the updated version we have added a clarifying sentence to paragraph 5, stating \u201cThe reward model provides us a principled manner to convert rankings over the reward factors into a numerical reward.\u201d\n\n**Weakness 2: Lack of ability in personalization.**\n- This is an interesting point. Beyond changing the ranking, we can increase the flexibility of HERON by tuning the margin parameters in the HERON framework. For example, if we would like to increase the emphasis on a given reward factor, we can decrease the size of the margin for that reward factor. In this way, a greater percentage of the decisions would be taken according to the reward factor we wish to emphasize. In the updated version, we have added discussion about how to increase flexibility in section 4.3, writing \u201cTo further increase the flexibility of HERON, we could tune the margin parameter of each reward factor, therefore impacting the fraction of decisions made at each level of the decision tree.\u201d\n- In addition, in the case where we have two factors, we can randomly drop the comparison based on the second factor is we would like to increase the dependence on the first factor.\n- Finally we remark that HERON has two additional benefits for enhancing the capability of reward modeling, which the linear combination of the reward factors does not have:\n(1) HERON allows us to take advantage of the contextual/state information contained in the trajectories by predicting the reward based on the trajectory and (2) for certain applications, e.g. code generation, it allows us to use pre-trained models as the initialization of the reward, and therefore vast amounts of domain knowledge (e.g., semantic and syntactic knowledge of code) can be transferred to the learnt reward.\n\n**Question 1: Learning curves in robotics environments**\n- We have included the curves in Appendix K of the updated paper. From these curves, we observe that HERON matches or beats the performance of reward engineering while being much more stable.\n\n**Question 2: Hyperparameters in Figures 3a and 3c.**\n- We have sufficiently tuned the baselines in both of these experiments, selecting the best policy (over 5 random seeds) from the learning rates {1e-6, 1e-5, 1e-4, 1e-3}. We believe that these two baselines have high variance due to the poor shaping of the reward, which can lead to instability in training."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504472851,
                "cdate": 1700504472851,
                "tmdate": 1700504472851,
                "mdate": 1700504472851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iLcYoGODJI",
                "forum": "quf7D5agqa",
                "replyto": "VeJTCmquv6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4655/Reviewer_9kSM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4655/Reviewer_9kSM"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers but my concerns on problem setup are not fully addressed. \n\nFor personalization, tuning margin parameters makes sense but why not adjust weights (over multiple objectives) directly instead? \n\nThis is also relevant to my question about clarity: why do we need to learn reward function from preferences based on ranking between objectives? \n\nThis setup is closely related to multi-object RL setup, and there are many works in that direction. \n\nHowever, I feel like the authors didn't discuss the connection (and differences) with multi-object RL and this makes it hard to understand the necessary (or practicality) of the problem setup.\n\nAfter reading the rebuttal and other reviewers' comments, my impression is that other reviewers also have a similar concern on the problem setup and the motivation of this work is not clear. \n\nDue to this, I'd like to keep my score as borderline reject."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566722238,
                "cdate": 1700566722238,
                "tmdate": 1700566722238,
                "mdate": 1700566722238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]