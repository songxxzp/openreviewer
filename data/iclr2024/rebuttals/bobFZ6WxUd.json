[
    {
        "title": "Non-Autoregressive Machine Translation as Constrained HMM"
    },
    {
        "review": {
            "id": "tugxdU9QS0",
            "forum": "bobFZ6WxUd",
            "replyto": "bobFZ6WxUd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes that DAT can be considered a special case of HMM, and then utilizes this perspective to identify that DAT exits the label bias problem. To address this problem, the authors present two solutions, namely 1) adaptive window HMM and 2) bi-directional HMM. Experimental results on WMT'14 English to German and WMT'17 Chinese to English demonstrate that our methods can achieve better or comparable performance to the original DAT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Viewing DAT as a variant of HMM is correct and very helpful. As a broader and high-level perspective, HMM can provide more opportunities for improving NAT (DAT).\n2) Label bias is indeed an issue with DAT, and the two solutions proposed by the authors are simple but effective. The intuition behind them is also easy to understand.\n3) The experiments are very thorough, verifying not only the improvement in performance but also analyzing whether the label bias issue has been resolved in the analysis section.\n4) This paper is clear and easy-to-follow."
                },
                "weaknesses": {
                    "value": "I cannot point obvious shortcomings, but if pressed, I would argue that label bias is not the most critical issue within DAT. In other words, this paper is not a game changer for NAT. From the experiments, it appears that addressing label bias offers only limited enhancement to DAT's performance. However, this cannot be considered a very strong point of criticism, as I think the authors' perspective of viewing DAT through the lens of HMM to be very useful and improtant."
                },
                "questions": {
                    "value": "I've also entertained the idea of viewing DAT as an HMM and have conducted some preliminary experiments. For instance, I removed the lower triangular mask matrix in DAT, transforming the model into a globally normalized general HMM. However, the model did not converge. If you could contrast this unsuccessful method in your paper, we might gain a deeper understanding of HMM-DAT.\n\nAdditionally, in DAT experiments, glancing training significantly aids DAT. Do you think this training method can be generalized to all HMM algorithms, such as those used in speech recognition, etc.?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9009/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9009/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698398446510,
            "cdate": 1698398446510,
            "tmdate": 1699637134916,
            "mdate": 1699637134916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2ZFTJGdsI3",
                "forum": "bobFZ6WxUd",
                "replyto": "luKbqltNME",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9009/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9009/Authors",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_D7ED",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_bRfo",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_fXmy",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj",
                    "ICLR.cc/2024/Conference/Submission9009/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
                ],
                "content": {
                    "title": {
                        "value": "Strong Disagreement"
                    },
                    "comment": {
                        "value": "In my view, the first two weaknesses you mentioned are based on a bias against the NAT field rather than inherent issues with the paper itself. When evaluating a paper, we should focus more on the content of the paper itself, rather than imposing the problems of the entire field on a single paper. Moreover, I believe the issues with BLEU are not intentionally caused by the authors, as previous works have used BLEU for EN-DE and DE-EN, so it is acceptable to use BLEU for fairness. The authors also reported SACREBLEU results for WMT 17 EN-ZH.\n\nI think this paper has a deep understanding of DAT (in the view of HMM). Beyond the  acceleration advantages of NAT this model is also a variant different from the existing autoregressive models. I believe the academic community must encourage such exploration, otherwise, neural machine translation (NMT) will never replace SMT. To my knowledge, the performance of early NMT models (e.g., RNN Search) was not significantly better than SMT."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699688428577,
                "cdate": 1699688428577,
                "tmdate": 1699688428577,
                "mdate": 1699688428577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xDdE39HdQH",
                "forum": "bobFZ6WxUd",
                "replyto": "tugxdU9QS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your thorough and thoughtful review and your recognition of the significance of our work. It's truly encouraging to know that our efforts to view DAT as a variant of HMM and address the label bias issue have been understood and valued. \n\n### **About the Lower Triangular Mask in DAT**\nWe understand that you attempt to treat DAT as a conventional HMM, where the transition probability between any two states can be greater than zero, instead of a left-to-right HMM where we only allow left-to-right transitions.\n \nTo illustrate the importance of the lower triangular mask (LTM) for optimizting the objective function in DAT, let's start with a toy example. This example is similar to but slightly more complex than the one in Figure 3(a) of our paper.\nAssume we have 6 positions (or latent states, 0-5) and 4 target tokens `[BOS, y1, y2, EOS]`. We force the start and end of the target sequence to be the first and the last positions, i.e., 0 and 5. Thus, position 0 and 5 are responsible for predicting the special BOS and EOS tokens.\n\n**Removing the mask confuses training**\n\nWith the LTM, the latent paths that can yield the target sequence `[BOS, y1, y2, EOS]` are `[0,1,2,5], [0,1,3,5], [0,1,4,5], [0,2,3,5], [0,2,4,5], [0,3,4,5]`.\nHowever, without the LTM, we would have additional latent paths, namely, `[0,2,1,5], [0,3,1,5], [0,4,1,5], [0,3,2,5], [0,4,2,5],[0,4,3,5]`. Note that an additional path like `[0,2,1,5]` corresponds to tokens `[BOS, y1, y2, EOS]`, respectively.\n\nAccording to the Eq. (5) in the original DAT paper [1], only one latent path will stand out during the training process for each training instance, and this standing-out path can be any valid path. For example, with LTM, `[0,1,2,5]` might be the most probable path for the target sentence `[BOS, the, world, EOS]` while `[0,1,3,5]` is the most likely one for `[BOS, the, earth, EOS]`. \nWithout LTM, `[0,1,2,5]` might still be chosen for `[BOS, the, world, EOS]`, but `[0,2,1,5]`, which has a right-to-left transition 2->1, might be selected for `[BOS, the, earth, EOS]`.\nThus, when we keep the LTM, the model is more likely to share some learned patterns (e.g., placing \"the\" at position 1) between different training examples. \nNevertheless, when we remove the LTM, half of the training instances will likely to have a winning left-to-right latent path whereas another half choose right-to-left. This is because the DAT model parameters are randomly initialized at the start of training. Consequenttly, this random L2R-R2L choices will make it hard for the model to share learned patterns among the training examples. Hence, the model cannot compress the training data effectively and struggles to converge.\n\n\n**Removing the mask creates loops during inference** \n\nAnother reason for keeping the LTM is that we can avoid loops in the decoding path, which is important for generating a complete sequence. For example, if we remove the LTM, and we happen to have the transition probability `P(1->2)=0.99` and `P(2->1)=0.99` during greedy inference, arriving at position 1 would make position 2 the most likely next position. However, once at position 2, the most likely candidate becomes position 1, which leads to a loop. We will never reach the EOS token (the last position) and simply generate a repetitive and meaningless sequence.\n\nThis LTM is important for both training and inference, and we sincerely thank you for asking this question so that more readers can understand.\n\n### **About Glancing Training in other HMMs**\n\nThis is an interesting question! Even though we are not experts in the speech domain, we'd like to share our understanding of glancing training in HMM-DAT.\n\nIdeally, the DAT decoder inputs should be only the learnable positional embeddings plus the `unk` token embeddings, but with glancing training, we replace some `unk` tokens with the ground truth target tokens, which provides richer training signals in the forward pass as well as rich gradient info in the backward pass, and thus leads to better convergence. By richer, we refer to the fact that different training examples will provide different glancing input tokens with glancing training. Without glancing, the inputs are always the same, consisting only of the unk token plus positional embedding. \nIf you are using a non-autoregressive decoder in speech recognition, you can similarly adopt the glancing training strategy. \nWe are happy to discuss more on this topic if you don't mind providing more background information or related papers.\n\nThank you once again for your insightful review. We hope our response has addressed your queries satisfactorily. We look forward to further discussions and exchanges our views.\n\n\n[1]: Directed Acyclic Transformer for Non-Autoregressive Machine Translation (Huang Fei et al., ICML 2022)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592290849,
                "cdate": 1700592290849,
                "tmdate": 1700592290849,
                "mdate": 1700592290849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3EiwIBDtWK",
                "forum": "bobFZ6WxUd",
                "replyto": "xDdE39HdQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed reply! Your paper is really good."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620908236,
                "cdate": 1700620908236,
                "tmdate": 1700620908236,
                "mdate": 1700620908236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U45ECSTaND",
            "forum": "bobFZ6WxUd",
            "replyto": "bobFZ6WxUd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9009/Reviewer_bRfo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9009/Reviewer_bRfo"
            ],
            "content": {
                "summary": {
                    "value": "Based on the directed acyclic Transformers (DAT) for non-autoregressive translation (NAT), this paper first shows that NAT is a fully connected left-to-right Hidden Markov Model (HMM) model. Then, the authors propose two constrained HMM strategies to address label bias issues in DAT, including adaptive window HMM and bidirectional HMM. The former adaptively balances the number of outgoing transitions at different latent states. And the latter uses bidirectional components to regularize each other's label bias.\n\nThe experiments are conducted on WMT14 en-de and WMT17 zh-en. Results demonstrate that both proposed strategies can obtain comparable or better performance compared to previous DAT models, and reduce the influence label bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes two methods, namely adaptive window HMM and bidirectional HMM to alleviate the challenges of label bias.\n\n2. Experimental results and analysis demonstrate the effectiveness of the proposed methods, which can achieve comparable or better BLUE scores than the original DAT models, and mitigate the effect of label bias."
                },
                "weaknesses": {
                    "value": "Compared to original DAT methods, the proposed strategies are incremental innovation, and only achieve improvements on the part of translation directions. For example, it does not seem to work for WMT zh-en, the reasons also need explaining."
                },
                "questions": {
                    "value": "1. Can the proposed two strategies be applied to the DAT model at the same time?\n\n2. Why does the proposed method behave differently in different translation directions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674259006,
            "cdate": 1698674259006,
            "tmdate": 1699637134767,
            "mdate": 1699637134767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6HfEOTcAsu",
                "forum": "bobFZ6WxUd",
                "replyto": "U45ECSTaND",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and constructive feedback. We appreciate your time and effort, which has helped us improve our paper.\n\n**Response to Question 1**\n\nWe attempted to integrate two strategies, but we did not see an improvement in performance compared to using each approach separately. We believe this is due to the following reasons:\n- In AW-HMM, as discussed in Section 5.2 (see Figure 4), there exists a delicate balance between mitigating label bias and preserving the expressiveness offered by a larger window size in AW-HMM. \n- In the context of Bi-HMM, L2R and R2L HMMs attempt to counteract the label bias by sharing all the parameters except for the transition matrix (which contains only 0.5M paramters). Consequently, the expressive capabilities of both L2R and R2L HMMs are somewhat restrained.\n- If we impose the window size constraint of AW-HMM on both L2R and R2L HMMs, the expressiveness of both HMMs is further reduced, which consequently did not lead to improved performance.\n\nWe appreciate this insightful question and have incorporated a detailed discussion on this phenomenon in our revised manuscript (see `the first paragraph in Section 6`). \n\n**Response to Question 2**\n\nThe performance improvements our methods can bring to Directed Acyclic Transformer (DAT) across different translation directions are closely correlated with the performance gap between DAT and the Autoregressive (AR) Transformer.\nAs confirmed both theoretically and empirically in [2], Non-Autoregressive (NAR) models are not as expressive as AR models.\nHence, we can consider the performance of the AR Transformer as a theoretical upper bound for all NAR models.\nWe have tabulated the best NAR model performances for all translation directions from the Table 1 of our paper, and compare them with the AR Transformer.\n\n|             | En-De | De-En | Zh-En | En-Zh |\n|-------------|-------|-------| ------ | ----- |\n| Transformer | 27.3  | 31.6 | 24.2 | 34.9 |\n| DAT         | 26.9 (-0.4)| 30.6 (-1.0)|24.0 (-0.2) | 33.8 (-1.1) |\n| AW-HMM/Bi-HMM | 27.2 (-0.1)| 31.4 (-0.2)| 24.1 (-0.1)|34.3 (-0.6)|\n|\n\nAs can be seen, the DAT model can perform similarly to AR Transformer on En-De and Zh-En, with only a `0.4` and `0.2` BLEU gap. However, for De-En and En-Zh, the BLEU score differences are `1.0` and `1.1`, respectively.\nGiven that our AW-HMM/Bi-HMM are also NAR models and DAT is already pretty close to the upper bound (AR Transformer) on En-De and Zh-En, the performance gains improvements appear to be relatively modest.\n\nThis point has been clarified and added to our revised paper (see `the last paragraph in Section 5.1`). We sincerely thank you for bringing up this important issue.\n\n\nWe hope that our response has satisfactorily addressed your questions. If there are any additional questions or points of clarification you require, we respectfully invite you to share them.\n\n\n**References**\n\n[1]: Directed Acyclic Transformer for Non-Autoregressive Machine Translation (Huang Fei et al., ICML 2022)\n\n[2]: An EM Approach to Non-autoregressive Conditional Sequence Generation (Zhiqing Sun, Yiming Yang, ICML 2020)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592907413,
                "cdate": 1700592907413,
                "tmdate": 1700623565190,
                "mdate": 1700623565190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "luKbqltNME",
            "forum": "bobFZ6WxUd",
            "replyto": "bobFZ6WxUd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9009/Reviewer_fXmy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9009/Reviewer_fXmy"
            ],
            "content": {
                "summary": {
                    "value": "The presented work extends the understanding of DAT as a left-ro-right Hidden Markov Model and proposes two approaches to mitigate the inherent label bias problem, namely an Adaptive Window HMM and a combination with a right-to-left HMM."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Extends the understanding of DAT as a HMM and solves the label bias problem by incorporating an R2L HMM and adding a hyper parameter to balance the outgoing transitions. \n- Experiments to back up the claim that the label bias problem is mitigated using the proposed approach.\n- NAT papers should follow the broader machine translation standard to report multiple metrics and metrics that correlate better with human judgement besides only relying on tokenized BLEU as that doesn't show the full picture, see **[1]**, **[2]**, **[3]**. I'm glad to see that BLEURT was additionally reported in the presented work and we do see nice gains there as well."
                },
                "weaknesses": {
                    "value": "### Weaknesses\n\n- **[major]**: Despite WMT'14 and WMT'17 being commonly used in the NAT literature, they are now way overhauled in the broader machine translation literature and should be replaced by more recent test sets to put the results into the context of recent research, see **[1]**.\n- **[major]**: NAT papers should follow the broader machine translation standard to report their evaluation scores using `sacrebleu` and provide the corresponding hash that was used for generating the scores. This will ensure that scores are reproducible and do not vary across papers by up to 1.8 BLEU points due to varying tokenization and normalization, see **[2]**, **[4]**. Mixing `sacrebleu` and tokenized BLEU as done in Table 1 shouldn't be done and needs to be fixed.\n- **[major]**: While it is nice to see that the paper attempts to provide GPU benchmarking numbers, the speed multipliers are heavily inflated since the baseline is a non-optimized autoregressive Transformer. There are many de-facto standard ways in practice to construct are more competitive autoregressive inference speed baseline with negligible translation quality drop using e.g. shallow decoders, shortlisting, or quantization (see **[2]**, **[5]**, **[6]**) which should be adopted here.\n- **[major]**: Table 1 doesn't include parameter counts or inference speed numbers which makes it hard to compare the different approaches and understand if the improvement comes from the better algorithm or, simply, the increased parameter count capacity. For example, Bi-HMM uses two different parameter sets to model L2R and R2L and as a result they should have more parameters. Bigger baselines, potentially in parallel branches through e.g. MoE, or scaling up previous approaches might be needed.\n- **[minor]**: It is unclear how well the proposed approach extends to the multilingual setting.\n- **[minor]**: Figure 4 doesn't show a clear trend in the window size, making it hard to extrapolate the findings to other language pairs or datasets without additional analysis for the dataset at hand. This will require additional grid search tuning trials to adopt and no guidance on how to tune this parameter is given.\n\n---\n### References\n\n- **[1]**: [Non-Autoregressive Machine Translation: It\u2019s Not as Fast as it Seems](https://aclanthology.org/2022.naacl-main.129) (Helcl et al., NAACL 2022)\n- **[2]**: [Non-Autoregressive Neural Machine Translation: A Call for Clarity](https://aclanthology.org/2022.emnlp-main.179) (Schmidt et al., EMNLP 2022)\n- **[3]**: [Results of WMT22 Metrics Shared Task: Stop Using BLEU \u2013 Neural Metrics Are Better and More Robust](https://aclanthology.org/2022.wmt-1.2) (Freitag et al., WMT 2022)\n- **[4]**: [A Call for Clarity in Reporting BLEU Scores](https://aclanthology.org/W18-6319) (Post, WMT 2018)\n- **[5]**: [Findings of the WMT 2022 Shared Task on Efficient Translation](https://aclanthology.org/2022.wmt-1.4) (Heafield et al., WMT 2022)\n- **[6]**: [Edinburgh\u2019s Submission to the WMT 2022 Efficiency Task](https://aclanthology.org/2022.wmt-1.63) (Bogoychev et al., WMT 2022)"
                },
                "questions": {
                    "value": "- How were the hyperparameters tuned for the proposed method and the previous works? If defaults were used for previous methods, the comparison needs to potentially be adjusted to also allow hyper parameter tuning for those methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696288130,
            "cdate": 1698696288130,
            "tmdate": 1699637134618,
            "mdate": 1699637134618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pp6Becjobz",
                "forum": "bobFZ6WxUd",
                "replyto": "2ZFTJGdsI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_fXmy"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9009/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9009/Authors",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_D7ED",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_bRfo",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_fXmy",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_z6kj",
                    "ICLR.cc/2024/Conference/Submission9009/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Reviewer_fXmy"
                ],
                "content": {
                    "comment": {
                        "value": "> When evaluating a paper, we should focus more on the content of the paper itself, rather than imposing the problems of the entire field on a single paper.\n\nThere are multiple works now that mention the above points as a big problem in the NAT literature and inherently issues within a subfield fall back to the individual papers. If obvious issues in the evaluation are not used as gating criteria for paper acceptance, the subfield will never change and remain its problematic state. Frankly, it is not much effort to actually have a sound evaluation procedure and multiple papers have now demonstrated how to do it properly. I firmly stand by my points and would like to see a revised version of the paper.\n\n> Moreover, I believe the issues with BLEU are not intentionally caused by the authors, as previous works have used BLEU for EN-DE and DE-EN, so it is acceptable to use BLEU for fairness. The authors also reported SACREBLEU results for WMT 17 EN-ZH\n\nI do acknowledge in my previous review that BLEURT was additionally used and I'm quite happy to see that. Still, the issue I mentioned in my original review stands where mixing scores from `sacrebleu` and tokenized BLEU is confusing and shouldn't be done. Again, it's not much effort to switch to a sound evaluation procedure and multiple papers have demonstrated existing issues with the current approach.\n\n> I believe the academic community must encourage such exploration\n\nI agree and am very much in favour for encouraging progress in this subfield. However, common best practices from the broader machine translation community should not be disregarded to do so such that meaningful progress can actually be identified. In its current state, neither the translation quality nor the inference speedup are properly evaluated."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699694841469,
                "cdate": 1699694841469,
                "tmdate": 1699694841469,
                "mdate": 1699694841469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DHFrWaoFi4",
                "forum": "bobFZ6WxUd",
                "replyto": "luKbqltNME",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking time to review our paper. \n\nFirstly, we would like to clarify that **we aim to improve the best NAR model from a novel theorectical perspective, not to push the translation frontier**.\n\nThe primary focus of our study is on the extension of the understanding of the Directed Acyclic Transformer (DAT) as a left-to-right Hidden Markov Model (HMM) and to address the inherent **label bias** problem with our AW-HMM/Bi-HMM approaches. Our objective was to delve deeper into the theoretical aspects of this model and propose improvements in the same, rather than advancing the translation landscape. We believe our findings are original and can add new knowledge to our community.\n\nThe concerns you raise about the broader Non-Autoregressive Translation (NAT) community, while relevant to the field as a whole, fall outside the scope of this particular study.\nWe must highlight that even after the publication of DAT, many research papers continue to use WMT'14, WMT'16, WMT'17 translation tasks as the testbed for NAR models, e.g., `[2],[3],[4],[5],[6],[7],[8],[9],[10]`. Even the authors of `[11]`, whom you cited, used WMT'13, 14, 16 for their evaluations, as their goal was also not to push the translation frontier.\n\nAs `Reviewer z6kj` accurately highlighted, the potential limitations existing within the broader NAT community should not overshadow the specific contributions made in individual studies, such as ours, which aims to enhance the state-of-the-art NAR architecture from a novel HMM perspective.\n\n\n**About BLEU and SacreBLEU**\n\nWe choose to report BLEU for EN-DE and SacreBLEU for ZH-EN for the sake of a consistent and fair comparison with previous work, as it was hard to accurately reproduce all the previous baselines and report their SacreBLEU scores.\n\n**About GPU speedup numbers**\n\nComparing a non-optimized NAR model with an optimized AR model would be unfair. The AR model field has various optimization techniques for acceleration, while the NAR field, being at its early stage, lacks specially-designed speedup techniques.\nHowever, it is crucial to note that the time complexity of NAR/AR model is $O(N)/O(N^2)$ [12], respectively. Theoretically, as indicated in [12], NAR models can surpass AR models in terms of decoding speed under any test conditions, provided that they are properly optimized. We look forward to such advancements in NAR accerleration, but again, it falls outside the scope of this paper.\n\n\n**About Parameter Count**\n\nWe list our parameter count below:\n|             | En-De | En-Zh |\n|-------------|-------|-------|\n| Transformer | 65M   | 86M   |\n| DAT         | 67M   | 88M   |\n| AW-HMM      | 67M   | 88M   |\n| Bi-HMM      | 68M   | 89M   |\n|\n\nThe table indicates that our AW-HMM/Bi-HMM and DAT models only require a `3%-5%` increase in parameter count compared to the baseline models. This suggests that our performance improvements are due primarily to our algorithm, rather than an increase in parameters.\n\nThis information has been added to our revised paper (see Appendix D).\n\n\nIn response to your minor queries:\n\n**Response to [minor 1]**\n\nOur models/methods are not language-specific, and should thus be applicable to other language pairs. We chose En-De and En-Zh to maintain consistency with previous work, as in [1].\n\n**Response to [minor 2]**\n\nRegarding the window size trend experiment depicted in Figure 4, we confined our investigation to the WMT'14 En-De dataset. This decision was made due to the significant computational cost associated with examining 8 different window sizes (ranging from 1 to 8) for each of the 4 translation directions (En-De, De-En, Zh-En, En-Zh).\n\nHowever, please note that we have conducted experiments with window sizes of 7 and 8 for all language pairs, the results of which are documented in Appendix B.\n\nIn relation to the tuning of the window size $\\beta$ hyperparameter, we followed standard hyperparameter tuning practices. The hyperparameter was tuned on the development set, a process that is mentioned in the caption of Figure 4.\n\n**Response to Questions**\n\nOur training hyperparamters follow previous work [1]. For the hyperparmeters in our AW-HMM and Bi-HMM, we tune them on the dev set.\n\nWe welcome further questions regarding the DAT-as-an-HMM perspective and our Bi-/AW-HMM models and would be glad to elaborate on these points."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593079985,
                "cdate": 1700593079985,
                "tmdate": 1700621424305,
                "mdate": 1700621424305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cXB4Yk0xEZ",
                "forum": "bobFZ6WxUd",
                "replyto": "luKbqltNME",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[1]: Directed Acyclic Transformer for Non-Autoregressive Machine Translation (Huang Fei et al., ICML 2022)\n\n[2]: Rephrasing the Reference for Non-autoregressive Machine Translation (Chenze Shao et al., AAAI 2023)\n\n[3]: Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation (Min Liu et al, AAAI 2023)\n\n[4]: Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning (Lihua Qian et al., Dec 2022)\n\n[5]: DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder (Jiaao Zhan et al., IWSLT 2023)\n\n[6]: Deep Equilibrium Non-Autoregressive Sequence Learning (Zaixiang Zheng et al., ACL 2023)\n\n[7]: Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation (Chenze Shao et al., EMNLP 2022)\n\n[8]: Non-autoregressive Streaming Transformer for Simultaneous Translation (Zhengrui Ma et al., Oct 2023)\n\n[9]: DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises (Jiasheng Ye et al., 2023)\n\n[10]: AMOM: Adaptive Masking over Masking for Conditional Masked Language Model (Yisheng Xiao et al., AAAI 2023)\n\n[11]: Non-Autoregressive Neural Machine Translation: A Call for Clarity (Schmidt et al., EMNLP 2022)\n\n[12]: Deep Encoder, Shallow Decoder: Reevaluating Non-Autoregressive Machine Translation (Jungo Kasai et al., ICLR 2021)"
                    },
                    "title": {
                        "value": "add references due to char limit"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593096185,
                "cdate": 1700593096185,
                "tmdate": 1700593119806,
                "mdate": 1700593119806,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]