[
    {
        "title": "Regularized KL-Divergence for well-defined function space variational inference in BNNs"
    },
    {
        "review": {
            "id": "nEv1acxp0m",
            "forum": "rmFtICfcd8",
            "replyto": "rmFtICfcd8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2545/Reviewer_izSH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2545/Reviewer_izSH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new function space variational inference for BNNs based on generalized KL divergence. The framework follows the linearized functional BNNs (Rudner et al. 2022). The new objective function is claimed to be a well-defined objective compared with the original possible ill-defined objective function. The experimental results show good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well presented with good organization. The issues of existing functional VI for BNNs are well described, and the problem of this paper is well-motivated. Sufficient background is given to understand the problem and the proposed idea. The proposed method is simple and looks good from the experiments."
                },
                "weaknesses": {
                    "value": "The authors claim that \u2018VI is too restrictive for BNNs with informative function space priors\u2019 is one of the contributions of this paper. However, the discussions in Section 2.2 just follow the existing works and the proofs in Appendix 1.1 are also a repeat of existing works. This point as a contribution is too weak. \n\nAlthough the idea of introducing generalized KL divergence may be reasonable, the final objective is without much difference from the previous one. During the practical implementation of previous functional BNNs, we usually add a diagonal identify matrix to ensure the non-singular the covariance matrix in (2) or (11). Hence, the technique contribution is weak. In the experiments, there are no results to show the cases where the non-singular covariance matrix impacts greatly the results."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549528267,
            "cdate": 1698549528267,
            "tmdate": 1699636190942,
            "mdate": 1699636190942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ByHkCG4gNF",
                "forum": "rmFtICfcd8",
                "replyto": "nEv1acxp0m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2545/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and informative feedback. We are encouraged that they found our method well motivated, simple and that it provides good experimental results. We are also glad they thought the background and issues with function-space BNNs were well presented.\n\nThe review has led us to realize the need for some clarifications in the paper, and we will upload a revised version in the coming days. In the meantime, please find our answers and comments below.\n\n> Although the idea of introducing generalized KL divergence may be reasonable, the final objective is without much difference from the previous one.\n\nWe realize that the differences to previous work require some clarification in our paper. We invite the reviewer to read items 1 and 3 in the overall response (\u201cImportance of function-space priors and fundamental differences to TFSVI and FVI\u201c) which highlights how our method differs from its closest baselines TFSVI (Rudner et al., 2022) and FVI (Sun et al., 2019).\n\nIn brief, our method allows for informative priors in *function space* whereas TFSVI requires priors to be specified in weight space. This may not be obvious since TFSVI pushes the weight-space prior forward into function space, but the choice of priors in TFSVI is still limited to distributions in weight space where meaningful belief specification is difficult e.g. see Figure 4 in the appendix. For details, see item 1 in the overall response.\n\nCompared to FVI, our method uses the linearized BNN and avoids score function estimators (see Issue (i) in Section 2.2), which have been reported to make FVI difficult to use in practice (Ma et Hern\u00e1ndez-Lobato 2021). From a more theoretical point of view, we show that, within the linearized BNN, the KL-divergence estimator of FVI (which introduces noise in a somewhat ad-hoc way) can be well-motivated and interpreted as an estimator of a well-defined regularized KL-divergence. See item 3 in our overall response for more details.\n\n> During the practical implementation of previous functional BNNs, we usually add a diagonal identity matrix to ensure the non-singular the covariance matrix in (2) or (11)\n\nWe agree that existing methods also introduce such jitter, and that it is somewhat subtle how the role of jitter differs across methods. We invite the reviewer to read items 2 and 3 in our overall response, which discusses these differences between our method, TFSVI, and FVI.\n\nIn summary, the jitter introduced in the implementation of TFSVI only addresses numerical issues that would not arise in the absence of rounding errors (see Issue (iii) in Section 2.2). By contrast, the jitter in our method does not solve numerical issues but instead is fundamentally necessary as we calculate a divergence between measures with different support, which would be infinite without the jitter.\n\nFVI also adds jitter to the variational posterior and prior when estimating the KL divergence. This jitter serves a similar purpose as in our method. However, since FVI does not linearize the BNN, it does not have access to an explicit variational measure in function space. Therefore, in order to take jitter into account, FVI has to draw Gaussian noise. By contrast in the linearized BNN, the marginalization over jitter can be done analytically as the variational posterior is a Gaussian measure.\n\n\n> In the experiments, there are no results to show the cases where the non-singular covariance matrix impacts greatly the results.\n\nWe are not sure to understand the reviewer\u2019s question. If the reviewer asks why we present results for TFSVI and FVI despite explaining that these methods do not work without jitter (see Section 2.2 issue (iii)), it is because our implementations are based on the code provided by their respective authors which add jitter in the KL divergence estimators. If the reviewer asks for additional experiments where we use our method, TFSVI or FVI without jitter, then we would like to point out that training such models would be impossible as we would immediately run into singularities.\n\nOur experiments show that our method is better than FVI and TFSVI at capturing prior beliefs (see, e.g., Figure 2, and Figure 4 in the appendix), and that it leads to better predictive performance and out-of-distribution detection (Tables 1 and 3).\n \n> The authors claim that \u201cVI is too restrictive for BNNs with informative function space priors\u201d\n\nAfter consideration, we agree with the reviewer that this point is a bit weak as a contribution. Indeed, our argument for this claim builds entirely on the results by Burt et al. (2020). We will remove this claim for the paper. The main contribution of our paper is not theoretical, but instead of a practical and well-defined method that empirically performs well.\n\nWe hope that our response clarifies how our contribution differs from prior work, and that we were able to convince you that our simple method could be helpful to members of the community."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245578480,
                "cdate": 1700245578480,
                "tmdate": 1700245578480,
                "mdate": 1700245578480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TiGCYa4qF0",
                "forum": "rmFtICfcd8",
                "replyto": "ByHkCG4gNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2545/Reviewer_izSH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2545/Reviewer_izSH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses! \n\nI am still concerned about the difference between the TFSVI and the proposed method. The objective function of the proposed method is just with a \"jitter\" term compared with previous work and such a term is usually added for any implementation of GP-related algorithms. Hence, I will keep my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649919110,
                "cdate": 1700649919110,
                "tmdate": 1700649919110,
                "mdate": 1700649919110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zywOToVTV4",
            "forum": "rmFtICfcd8",
            "replyto": "rmFtICfcd8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2545/Reviewer_PriF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2545/Reviewer_PriF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a generalized function space variational inference (GFSVI) for Bayesian neural networks (BNNs), where the intractable and often ill-defined function-space KL term is replaced with a regularized KL divergence. While the original KL divergence in function space can in principle blow up to infinity, and this actually happens for many practical applications, the regularized KL divergence does not suffer from the same issue, thus providing more stable results. The proposed method has been demonstrated on synthetic and real-world regression tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n- Bringing regularized KL to function space variational inference is a good contribution that might benefit the community.\n- Good empirical results on UCI regression benchmarks."
                },
                "weaknesses": {
                    "value": "- No significant contribution. As far as I could understand, the only contribution of this paper is to bring the regularized KL divergence which was well developed by (Quang 2019), and use it as a substitute for the ordinary KL divergence in the framework of tractable function space VI (TFVI, Rudner et al., 2022). Other than this, I fail to see any contribution, and even the combination of those two methods is implemented quite straightforwardly, without any issue to consider during that process.\n- Limited experiments. While I appreciate the experiments on the regression tasks, they are relatively small-scale tasks, and only the small BNNs (MLPs mostly) are tested. It is hard to judge the effectiveness of the proposed method without scaling, for instance, the image classification task (CIFAR-10 or CIFAR-100, at least) solved with ResNet, as typically done in the literature."
                },
                "questions": {
                    "value": "- Once we replace the KL divergence with regularized KL divergence, the resulting objective becomes something that is different from ELBO, so we end up optimizing an objective that is not particularly a lower bound on the marginal likelihood. There is a class of inference algorithms (generalized Bayesian inference) generalizing the standard Bayesian inference procedure and thus variational inference by extending the likelihood or KL regularization term with more flexible functions, and for such cases the utility of the extended objective can be described in the perspective of generalizaton error. However, in the current form, the alternative objective with the regularized KL does not explain anything about its optimum. I think there should be some intuitive justification for this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824529062,
            "cdate": 1698824529062,
            "tmdate": 1699636190839,
            "mdate": 1699636190839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "znFxJxdPLp",
                "forum": "rmFtICfcd8",
                "replyto": "zywOToVTV4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2545/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and informative feedback. We are encouraged that they thought that the regularized KL divergence was a good contribution for the community and that they found our method to perform well on regression tasks. We are also glad they thought the paper to be well written and easy to follow.\n\nThe review has led us to realize the need for some clarifications in the paper that we will make in the coming days. In the meantime, please find our answers and comments below.\n \n> the only contribution [...] is to bring the regularized KL divergence [...] as a substitute for the [...] KL divergence in the framework of [...] TFVI (Rudner et al., 2022).\n\nWe kindly refer to the reviewer to item 1 and 3 which highlights how our method differs from TFSVI and FVI (Sun et al., 2019) in the overall response (\u201cImportance of function-space priors and fundamental differences to TFSVI and FVI\u201c). \n\nIn summary, our main contribution is a simple method for inference in BNNs with *function-space priors*. Unlike our method, TFSVI defines prior beliefs in weight space using standard BNN priors, but pushes it to function-space to compute the KL divergence. Thus priors in TFSVI are still limited to distributions in weight space where meaningful belief specification is difficult e.g. see Figure 4 in the appendix. For details, see item 1 in the overall response.\n\nCompared to FVI, our method uses the linearized BNN and avoids score function estimators (see Issue (i) in Section 2.2), which have been reported to make FVI difficult to use (Ma et Hern\u00e1ndez-Lobato 2021). From a more theoretical point of view, we show that, within the linearized BNN, the KL-divergence estimator of FVI (which introduces noise in a somewhat ad-hoc way) can be well-motivated and interpreted as an estimator of a well-defined regularized KL-divergence. See item 3 in our overall response for more details. \n\n> While I appreciate the experiments on the regression tasks, they are relatively small-scale tasks, and only the small BNNs (MLPs mostly) are tested.\n \nIndeed, the BNNs we consider are relatively small as is typical in the literature on function space VI for BNNs\u00b9. Computing the regularized KL divergence can be expensive for large models. Thus, in its current state, we believe that our method is best suited for models in a regime in-between analytically solvable and deep learning models, which find many practical applications involving decision making, such as Bayesian optimization or bandits. At the same time, we believe that one of the strengths of our method is its simplicity, which would make it a solid starting point for future research, including ways to make it more scalable.\n\n\u00b9 note that Rudner et al. (2022) reports results using a Resnet-18 on a Nvidia V-100 GPU with 32GB of memory. It should be possible to fit this model with our method using similar resources. However our method offers little advantages for classification tasks, as discussed in the next answer below.\n\n> It is hard to judge the effectiveness of the proposed method without [...] the image classification task (CIFAR-10 or CIFAR-100, at least) solved with ResNet, as typically done in the literature.\n\nWe thank the reviewer for raising this point on which we did not expand on in the paper. Our main contribution is the use of GP priors for BNNs to incorporate *informative knowledge* about the functions generated by the BNN. Prior beliefs in the classification setting are defined in logit-space before the output activation which are much harder to specify and puts into doubt our original motivation. This is why we preferred to restrict our method to regression tasks. However, automatic prior selection methods with our method might be beneficial for classification and would be an interesting direction for future work.\n\n> There is a class of inference algorithms (generalized Bayesian inference) [...] can be described in the perspective of generalization error. However, [...] the [...] objective with the regularized KL does not explain anything about its optimum. I think there should be some intuitive justification for this.\n \nThank you for raising this point. We are not sure if we understand the question. As discussed at the beginning of Section 3, our method builds on generalized VI (Kloblauch et al., 2018), which views Bayesian inference as regularized empirical risk minimization by considering the ELBO as a regularized expected log-likelihood objective. While we summarize the main idea behind generalized VI at the beginning of Section 3, we think that adding more details about generalized VI would be beyond the scope of our paper, and we instead refer readers to the original paper. We hope this answers the question and we are happy to follow up if we misunderstood it.\n\nWe hope that our response clarifies how our contribution differs from prior work, and that we were able to convince you that our simple method could be helpful to members of the community."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247647014,
                "cdate": 1700247647014,
                "tmdate": 1700247647014,
                "mdate": 1700247647014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IEuWvWHYdC",
                "forum": "rmFtICfcd8",
                "replyto": "znFxJxdPLp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2545/Reviewer_PriF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2545/Reviewer_PriF"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' effort to clarify the issues raised in my reviews. I apologize for my misunderstanding on generalized VI; I missed that the paper justified the objective function using that.\n\nHowever, I'm still not convinced by the claim on the novelty and the lack of larger-scale experiments; my main point (and I think the other reviewers are with me on this point) is that the presented paper is a combination of existing methods without significant technical innovation. Also, while I agree that some aspects of the proposed method can be highlighted through the experiments presented in this paper, without demonstration for practicality (at least an image classification benchmark with moderate-sized deep neural nets that do not require large memories), it is hard to value the contribution of the paper to the community (especially considering the lack of novelty claim). So I decided to keep my original score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576254629,
                "cdate": 1700576254629,
                "tmdate": 1700576254629,
                "mdate": 1700576254629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yIuINktSLU",
            "forum": "rmFtICfcd8",
            "replyto": "rmFtICfcd8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2545/Reviewer_sJLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2545/Reviewer_sJLb"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on a challenge in function-space variational inference, where the KL divergence between two stochastic processes, evaluated on a finite number of inputs, could have an infinite value. This issue raises the numerical issue when training the Bayesian Neural Networks (BNNs) by function-space variational inference (VI). To address this problem, the authors employ the regularized KL divergence, which is defined to have a finite value and can be used to resolve the mentioned issue. Empirically, the authors demonstrate that function-space VI using the regularized KL divergence leads to better uncertainty estimation on synthetic and UCI regression datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Justification for jitter addition\n> It seems that this work justifies the addition of jitter to each covariance term to address the numerical issue, i.e., the infinite value of function-space KL divergence by introducing the well-established regularized KL divergence into the framework of the function space VI."
                },
                "weaknesses": {
                    "value": "### Incremental contribution\n\n> Compared to the tractable function space VI of [1], it seems that the objective in Eq. (9) exhibits only minor differences, i.e., jitter term ($\\gamma M I_M$) in each covariance term in Eq. (11). Based on my understanding, adding the jitter term has been commonly used in implementation to handle the numerical issue when training the model with the Gaussian KL divergence as KL objective. Therefore, the proposed objective itself does not seem novel in sense of training objective for VI.\n\n\n### Experiment results are limited to regression setting.  \n\n> While the tractable function space VI of [1] has been demonstrated on both classification and regression tasks, this work has been demonstrated only for regression experiment setting.\n\n[1] Tractable Function-Space Variational Inference in Bayesian Neural Networks - NeurIPS 22"
                },
                "questions": {
                    "value": "*  I could not identify significant differences between the tractable function space VI of [1] and the proposed method. What is the primary distinction in comparison to [1]?\n\n* In comparison to [1], what specific difference in the proposed method leads to the improved performance in Table 1?\n\n* Regarding Table 1, why is TFSVI categorized under weight space priors? As far as I understand, the KL divergence of TFSVI is evaluated in the function space using the push-forward distribution of the weight parameter distribution, which is defined in the function space.\n\n\n[1] Tractable Function-Space Variational Inference in Bayesian Neural Networks - NeurIPS 22"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2545/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2545/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2545/Reviewer_sJLb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2545/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849084666,
            "cdate": 1698849084666,
            "tmdate": 1699636190751,
            "mdate": 1699636190751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HzAMnfjMUH",
                "forum": "rmFtICfcd8",
                "replyto": "yIuINktSLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2545/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2545/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their useful and informative feedback on our paper. Please find our responses below.\n\n> - \u201cI could not identify significant differences between the tractable function space VI of [1] and the proposed method. What is the primary distinction in comparison to [1]?\u201d\n> - \u201cWhy is TFSVI categorized under weight space priors?\u201d\n> - \u201cIn comparison to [1], what specific difference in the proposed method leads to the improved performance in Table 1?\u201d\n\nWe kindly refer the reviewer to point 1 in our overall response (\u201cImportance of function-space priors and fundamental differences to TFSVI and FVI\u201d). In brief, our method allows specifying prior beliefs in function space whereas TFSVI specifies the prior in weight space, but pushes it forward to function space to calculate the KL-divergence. However, such a push-forward only changes the *representation* of the prior and does not lift the limitations of expressing meaningful prior beliefs in weight-space since neural networks weights are not interpretable. \nThese differences explain the improved performance of our method in Tables 1 and 3, and the qualitative improvements in Figure 2, and Figure 4 in the appendix. In these experiments, we chose the best weight-space prior that we could find for TFSVI by cross validation.\n> \u201cjitter term has been commonly used in implementation to handle the numerical issue when training the model with the Gaussian KL divergence as KL objective\u201d\n\nThank you for bringing this up. The different ways in which TFSVI, FVI (Sun et al., 2019), and our method introduce jitter is indeed somewhat subtle, but these differences are crucial. We discuss the differences in paragraph issue (iii) of Section 2.2, but we realize based on reviewer feedback that this issue requires further explanation. We kindly refer to points 2 and 3 in our overall response above, which we hope clarify these differences (we will add a similar clarification to the paper over the next few days). We provide a synopsis below.\n\nTFSVI introduces jitter to avoid numerical issues. Since TFSVI defines both prior and variational distributions in weight space, their respective push-forwards into function space have the same support, and the KL-divergence between them would be technically finite if it wasn\u2019t for numerical errors. By contrast, the jitter in our method does not resolve numerical issues but instead is fundamental to obtaining a finite objective. Indeed, in our method the prior and variational posterior have different support since the prior is specified in function space while the variational posterior is still limited to the push-forward of a variational distribution in weight space.\n\nCompared to our method, FVI does not linearize the BNN and only *implicitly* defines the variational measure in function-space which makes the method substantially more complicated to use in practice (Ma et Hern\u00e1ndez-Lobato 2021). Thus, \u201cadding jitter\u201d in FVI means drawing actual realizations of Gaussian noise. By contrast, we use the linearized BNN which yields a Gaussian variational posterior for which we can marginalize over the jitter analytically. It turns out that applying the estimator of FVI to our situation is equivalent to calculating the estimator of the regularized KL-divergence, which has an *explicit* form that we can directly optimize.\n\n> \u201cCompared to the tractable function space VI of [1] (TFSVI), it seems that the objective in Eq. (9) exhibits only minor differences\u201d\n\n\nWe agree that the *implementation* of our method is similar to TFSVI. But we see this as a strength rather than a shortcoming. In fact, we believe that our method has the potential of becoming a go-to solution for function-space BNNs precisely because it can be implemented easily and yet evidently offers improvements over TFSVI and FVI.\n\n> \u201cWhile the tractable function space VI of [1] has been demonstrated on both classification and regression tasks, this work has been demonstrated only for regression experiment setting.\u201d\n\nWe thank the reviewer for raising this point that we did not clarify in the paper. As mentioned above, the main difference between our method and the one in [1] is the use of GP priors in function space, which allow us to incorporate knowledge about the properties of the functions generated by the BNN. Prior beliefs in the classification setting are specified in the space of logits before the output activation, which is less interpretable than for regression and puts into question our initial motivation for informative priors. For this reason, we prefer to concentrate on the regression setting. However, using automatic prior selection along with our method might provide benefits over standard BNNs in classification tasks and would be an interesting direction for future work.\n\nWe hope that our response clarifies how our contribution differs from prior work, and that we were able to convince you that our simple method could be helpful to members of the community."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245380324,
                "cdate": 1700245380324,
                "tmdate": 1700245380324,
                "mdate": 1700245380324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gQAM09Ydrx",
                "forum": "rmFtICfcd8",
                "replyto": "yIuINktSLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2545/Reviewer_sJLb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2545/Reviewer_sJLb"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer sJLb"
                    },
                    "comment": {
                        "value": "Thank you for responding to my questions. Your responses have helped me better understand the distinction between the proposed method and TFSVI, as claimed by the authors. However, I am somewhat skeptical that using Gaussian processes to construct a function-space prior can result in an interpretable function-space prior for a given dataset. Based on my understanding, the inductive bias of the GP prior could be interpretable for certain types of covariance functions like the RBF kernel, Matern, and Periodic kernel. Choosing the proper kernel function for GP seems non-trivial and depends on the dataset. Thus, I believe that relying on a GP prior to build an interpretable function-space prior might still have some limitations.\n\nPersonally, I think that providing a specific method for forming the interpretable function-space prior would further enhance the novelty of this work, especially considering the distinction between the proposed method and TFSVI. \n\nWith these reasons, I will maintain my score for the current content of the manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2545/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716289891,
                "cdate": 1700716289891,
                "tmdate": 1700725058610,
                "mdate": 1700725058610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]