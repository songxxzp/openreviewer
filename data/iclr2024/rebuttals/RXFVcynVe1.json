[
    {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning"
    },
    {
        "review": {
            "id": "cHYscOybi9",
            "forum": "RXFVcynVe1",
            "replyto": "RXFVcynVe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of leveraging Large Language Models (LLMs) such as GPT to enhance the performance of Graph Neural Networks (GNNs) on Text-Attributed Graphs (TAGs). The authors propose an innovative approach called GRAPHTEXT, which combines LLMs' textual modeling abilities with GNNs' structural learning capabilities. The key innovation lies in using explanations generated by LLMs as features to boost GNN performance on downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method presents a novel and effective way to integrate the power of LLMs, like GPT, with GNNs to handle text-attributed graphs. By using LLMs to generate explanations and converting them into informative features for GNNs, it bridges the gap between textual information and graph structure, enabling more sophisticated graph reasoning.\n\n2. The experimental results demonstrate that the proposed method achieves state-of-the-art performance on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, and a newly introduced dataset, arXiv-2023. This underscores the effectiveness of the proposed approach in improving the accuracy of downstream tasks on TAGs.\n\n3.The proposed method not only enhances performance but also significantly speeds up training. It achieves a 2.88 times improvement over the closest baseline on the ogbn-arxiv dataset. This computational efficiency is crucial for practical applications and scalability."
                },
                "weaknesses": {
                    "value": "1. The time analysis and money estimation are lacking. The paper utilizes the chatgpt API, nonetheless, there is a large restraint on the word limitation per day, and the cost of one dataset should also be taken into consideration. \n\n2. The robustness of the prompt is lacking. The paper proposed a single prompt for the node classification. Nonetheless, is another similar prompt can achieve a similar performance. \n\n3. There may lack of ablation study on the LM. Current LM focuses on BERT, while other LM are ignored. Recent paper demonstrates that they can achieve satisfying performance with only SentenceBert Embedding\n\n4. There lack some experimental results in table 1, especially Giant. \n\n[1] Duan, Keyu, et al. \"Simteg: A frustratingly simple approach improves textual graph learning.\" arXiv preprint arXiv:2308.02565 (2023)."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8930/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45",
                        "ICLR.cc/2024/Conference/Submission8930/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697761869194,
            "cdate": 1697761869194,
            "tmdate": 1700492331306,
            "mdate": 1700492331306,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jNNO9nhwOd",
                "forum": "RXFVcynVe1",
                "replyto": "cHYscOybi9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8930/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Dm45 (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments on our work. Please, find our answers to your comments and questions below.\n\n> **Reviewer:** The time analysis and money estimation are lacking. The paper utilizes the chatgpt API, nonetheless, there is a large restraint on the word limitation per day, and the cost of one dataset should also be taken into consideration.\n\n**Authors:** We acknowledge the importance of a comprehensive analysis of time and cost, given the constraints of the ChatGPT API.  We incorporated it in appendix D.10 (in blue) in our revised manuscript.\nTime and Cost Implications: Our primary dataset, ogbn-arxiv, with 169,343 nodes and 1,166,243 edges, serves as a representative case for our approach. On average, our input sequences consist of approximately 285 tokens, while the output sequences comprise around 164 tokens. For the ChatGPT-3.5 Turbo API, priced at $\\textdollar$0.0015 per 1,000 input tokens and $\\textdollar$0.002 per 1,000 output tokens, with a token per minute rate limit of 90,000, the monetary estimation for ogbn-arxiv is as follows:\n\nCost = ((285 * 0.0015) / 1000 + (164 * 0.002) / 1000) * 169,343 \u2248 128 USD\n\nConsidering the token rate limit, we estimate the deployment time as follows:\n\nTime = 169,343/ (90,000/285) \u2248 536 min \u2248 9h\n\n**Cost-Effective Alternatives**: Additionally, we have explored cost-effective alternatives, such as leveraging open-source LLMs like llama2. The use of llama2 is entirely free, and the querying process to llama2-13b-chat takes approximately 16 hours when utilizing 4 A5000 GPUs (24GB). \n\n**Efficiency through Single Query and Reuse**: Our method requires only one query to the LLM, with predictions and explanations stored for subsequent use. This not only enhances efficiency but also minimizes the number of API calls, contributing to cost-effectiveness. \n\n**Dataset Sharing**: We also make the GPT responses available for public use.\n\n> **Reviewer:** The robustness of the prompt is lacking. The paper proposed a single prompt for the node classification. Nonetheless, is another similar prompt can achieve a similar performance.\n\n**Authors:** We have extensively explored the influence of various prompts, as outlined in Table 8 and Table 9 (in blue) in the appendix. Our findings reveal a positive correlation between the zero-shot accuracy of the LLM and the overall accuracy of our method, i.e. the higher the zero-shot prediction score, the better the accuracy of TAPE. Besides, despite employing different prompt designs, our approach consistently maintains similar accuracy, ranging from 0.7660 to 0.7750, demonstrating the robustness of the prompt.\n\n> **Reviewer:** There may lack of ablation study on the LM. Current LM focuses on BERT, while other LM are ignored. Recent paper demonstrates that they can achieve satisfying performance with only SentenceBert Embedding\n\n**Authors:** We thank the referee for pointing out the necessity of a more comprehensive ablation study regarding the LM. In response, we ran additional experiments involving different LMs to assess their impact on TAPE.\n\nBefore revealing our findings, we would like to address a recent paper, SimTeG [1], which uses SentenceBert and might have been referenced by the reviewer. SimTeG indeed demonstrates satisfactory results with SentenceBert Embedding. However, it is crucial to acknowledge that, similar to our model, their approach entails fine-tuning the LM-SentenceBert, with raw text and node labels. Additionally, SimTeG highlights the seamless integration of our method with theirs. A claim substantiated through experimental testing, showing that SimTeG+TAPE with Ensembling establishes a new SOTA performance on the ogbn-arxiv dataset.\n\nMoving forward, we extended our investigation beyond the deberta-base model by studying the effect of different LMs on ogbn-arxiv. In alignment with the approach taken in SimTeG [1], we incorporated two additional widely-used LMs from the MTEB leaderboard: all-roberta-large-v1 and e5-large. The detailed outcomes of our study are presented in Table 13 (in blue). Notably, our model demonstrates insensitivity to the choice of a specific LM, emphasizing its robustness to variations in LM selection."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406750381,
                "cdate": 1700406750381,
                "tmdate": 1700407390135,
                "mdate": 1700407390135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0NxgwuUvUy",
                "forum": "RXFVcynVe1",
                "replyto": "rjt7XIpoUf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response. I have updated the score accordingly."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492372339,
                "cdate": 1700492372339,
                "tmdate": 1700492372339,
                "mdate": 1700492372339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bqSDLBvRGL",
            "forum": "RXFVcynVe1",
            "replyto": "RXFVcynVe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework for representation learning on text-attributed networks. The authors first prompt the LLM to obtain explanations and predictions for each node based on its textual information. Then they finetune language models on different textual attributes and obtain feature embeddings. Finally, the graph neural network adopts the learned embeddings from the second step to conduct the final prediction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author proposes a method to augment LM with LLM-generated features.\n2. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. Lack of baselines. Many important baselines that conduct learning with language models on TAGs including GraphFormers [1] and Patton [2] are missing in the experimental section.\n2. The theorem is not specific to this problem. The theorem in 4.4 is not particular for LLM and LM for TAGs, and needs strong assumptions.\n3. Lack of evaluation tasks. The paper claims to do representation learning on TAGs but only evaluates the node classification task. It would be better to add experiments on other tasks such as link prediction to evaluate the quality of the representations.\n4. Lack of ablation studies. 1) If we need to finetune the LM in step 2 or not?; 2) Why do we need to have different LM for original text encoding and explanation encoding?\n5. Limit novelty. While I appreciate the introduction of the new arxiv-2023 dataset, the technique contribution of this work is very limited, which basically introduces LLM to conduct data augmentation for node classification on TAGs.\n\n[1] J. Yang, et al. GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. NeurIPs 2021. \n\n[2] B. Jin, et al. Patton: Language Model Pretraining on Text-Rich Networks. ACL 2023."
                },
                "questions": {
                    "value": "Please refer to the comments raised in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8930/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8930/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698079186890,
            "cdate": 1698079186890,
            "tmdate": 1700494533129,
            "mdate": 1700494533129,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wWvDk4i5aO",
                "forum": "RXFVcynVe1",
                "replyto": "bqSDLBvRGL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8930/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HEW1 (Part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the examination of our work and the thoughtful comments provided. Kindly find our responses to the raised comments and questions below.\n\n> **Reviewer:** Lack of baselines. Many important baselines that conduct learning with language models on TAGs including GraphFormers [1] and Patton [2] are missing in the experimental section.\n\n**Authors:** We appreciate the reviewer\u2019s valuable feedback and acknowledge the concern regarding the absence of certain baselines in our experimental section, particularly GraphFormers [1] and Patton [2]. We would like to clarify the reason for this absence.\n\nOur primary focus in this work is the node-prediction task in TAG. For this specific task, we concentrated on the most widely used datasets, i.e. ogbn-arxiv and ogbn-products. Additionally, recognizing the high popularity of Cora and Pubmed as node-classification datasets, we augmented these datasets by collecting text information, title and abstract, to create two additional TAGs (also made publicly available). We further introduced a new dataset, arxiv-2023, ensuring it is not part of the training set of GPT-3.5.\n\nIn the context of ogbn-arxiv and ogbn-products, we chose to compare our method against GLEM [3] and GIANT [5], recognized as the strongest baselines for these datasets. While we understand the concern about the absence of GraphFormers and Patton, we believe our study offers a robust comparison by evaluating against the strongest baselines within the constraints of the available datasets.\n\nSpecifically addressing GraphFormers, it was primarily designed for link prediction tasks, making a direct comparison less suitable due to differences in task objectives. Similarly, Patton involves pre-training on Microsoft Academic Graph (MAG) or Amazon e-commerce networks, followed by fine-tuning in a few-shot manner, making it also challenging for a direct comparison.\n\nMoreover, it is important to note that the datasets used in the GraphFormers and Patton papers do not overlap with the datasets in our study. Consequently, we are unable to report from their papers the performance on our datasets.\n\nNevertheless, to address the referee\u2019s concern, we conducted additional experiments within the rebuttal period. We extended our experiment to include MAG-Economics, a 40-way node classification task, to facilitate a comparison with Patton. The results, reported in Table 15, demonstrate that our technique TAPE outperforms GraphFormers and Patton by a significant margin of at least +13% accuracy on the MAG-Economics dataset.\n\nIn the revised version, we will commit to extending the comparative analysis to other datasets such as MAG-Mathematics, MAG-Geology, Amazon-Clothes, and Amazon-Sports. We believe this comprehensive evaluation across multiple datasets will address the gap mentioned by the reviewer and provide a more holistic comparison with existing baselines.\n\nWe thank the reviewer for the insightful comment, which contributes to the enhancement of the experimental section of our work.\n\n\n> **Reviewer:** The theorem is not specific to this problem. The theorem in 4.4 is not particular for LLM and LM for TAGs, and needs strong assumptions.\n\n**Authors:** While it is true that Theorem 4.4 could be applied in other contexts, we found it valuable to provide a rigorous foundation for the intuitions guiding the proposed technique.\n\nOur intention was to reinforce the understanding of why the technique is effective, beyond solely relying on experimental results. Specifically, the theorem underscores the importance of fidelity in explanations with LLM's reasoning, and emphasizes the non-redundancy between LLM and LM. These aspects constitute the key assumptions under which the technique was designed to operate successfully."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406599631,
                "cdate": 1700406599631,
                "tmdate": 1700406904186,
                "mdate": 1700406904186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VeMQGnXHbC",
                "forum": "RXFVcynVe1",
                "replyto": "bqSDLBvRGL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed response and their added experiments. I'm still not fully convinced by the novelty of this work, but I'm happy to raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494515590,
                "cdate": 1700494515590,
                "tmdate": 1700494515590,
                "mdate": 1700494515590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2iJJkcKwnO",
            "forum": "RXFVcynVe1",
            "replyto": "RXFVcynVe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8930/Reviewer_NoEV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8930/Reviewer_NoEV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to solve TAG problems. The method uses an LLM to generate category prediction and explanation of the target node, then the explanation is used as an enhancing part of the node feature. An LM is used to encode the raw text feature and the additional explanation into hidden space. A GNN predictor receives the hidden features, the prediction of LLM, and the shallow embeddings to give the final classification results. Experiments on several TAGs show the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly written and easy to follow.\n2. Bagging LLM prediction, LM features and shallow features is reasonable for the node classification task.\n3. The experiments show the method can achieve SOTA performance on several benchmark datasets."
                },
                "weaknesses": {
                    "value": "1. The main concern is that the method has little to do with graph. It seems like an application attempt of LLMs on natural language tasks, and TAG is just a scenario. Thus the contribution of the method is limited.\n2. Since the Debera need fully fine-tuning, training the method cost much more memory than pure GNN methods."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838844252,
            "cdate": 1698838844252,
            "tmdate": 1699637124331,
            "mdate": 1699637124331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VpzyLVrLKp",
                "forum": "RXFVcynVe1",
                "replyto": "2iJJkcKwnO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8930/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NoEV"
                    },
                    "comment": {
                        "value": "We thank the reviewer very much for the careful reading and comments regarding our work. Please, see below our answer to the raised comments/questions.\n\n> **Reviewer:** The main concern is that the method has little to do with graph. It seems like an application attempt of LLMs on natural language tasks, and TAG is just a scenario. Thus the contribution of the method is limited.\n\n**Authors:** While it is true that the first part of our technique has no direct connection to graph topology, and focuses on node features, the overall methodology is strongly motivated towards addressing the challenges inherent in the TAG problem, an active research area propelled by the advent of language models. \n\nAs advocated in our global response, our approach is deeply rooted in addressing the challenges associated with the joint training of GNNs and LMs. The task of representation learning on TAGs involves integrating the raw text with graphs in a robust and scalable way. Existing literature can be broadly categorized into two paradigms: 1) End-to-end or joint training of LM and GNN, exemplified by approaches like GLEM [1] and DRAGON [2]; and 2) Two-stage training, where LM is initially trained followed by GNN training. Our approach belongs to the second paradigm, inheriting its efficiency and ease of plugging into existing GNN pipelines, but incorporates an LLM-to-LM interpreter to extract and interpret the LLM's useful prior knowledge. \n\nThe first approach is generally considered more attractive because the LM is trained to directly leverage graph structure. However, it often involves complex language and graph architectures, encounters training instabilities, and demands significant GPU resources, particularly challenging with LLMs. In this context, an important contribution of our work is the finding that with the help of our LLM-to-LM interpreter, the second paradigm of two-stage training can achieve equal or better performance than end-to-end or joint training, with much greater efficiency. In particular, our LLM-to-LM interpreter allows the valuable prior knowledge from LLMs to be embedded into GNN features, and our work shows that this does not sacrifice performance but greatly improves efficiency, compared to joint training.\n\nTherefore, our work should be viewed as a strategic response aimed at overcoming these challenges. It goes beyond being a mere application of LLMs to TAG; rather, it represents a novel, efficient, and robust approach to addressing tasks that involve both text and graphs.\n\nWe acknowledge that we may not have sufficiently conveyed the important message that our work aims to resolve the intricate issues tied to LM+GNN training for TAG. We are committed to amending the paper to provide greater clarity on this aspect for the reader.\n\n\n> **Reviewer:** Since the Deberta need fully fine-tuning, training the method cost much more memory than pure GNN methods.\n\n**Authors:** We appreciate the reviewer's observation regarding the increased memory usage in our approach compared to pure GNN methods, as illustrated in Figure 2. It is important to note that learning on TAG inherently demands the integration of LMs and GNNs. Established methods like GIANT [5], GLEM [1], GraphFormers [3], and Patton [4], among others, all involve LM in their processes. Therefore, a meaningful comparison of memory cost should be drawn against these baselines rather than pure GNNs.\n\nRecognizing the inherent tradeoff between cost and accuracy, we highlight that despite the increased resource requirements, our method TAPE significantly boosts the accuracy of the pure GNN method from 70.83% to 77.50%. We firmly believe that the substantial performance improvement justifies the accompanying rise in memory costs. Notably, these elevated resource demands are manageable, even with relatively cost-effective academic GPU resources like our A5000 GPU (24GB).\nWe greatly appreciate the reviewer's observation regarding memory usage, which prompted us to conduct a more in-depth analysis, as outlined in Table 14 (in blue), showing that TAPE provides both best performance, and comparable or better running time than other LM-based methods.\n\n\n\n\n**Reference**\n\n[1] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference. ICLR, 2023\n\n[2] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, and Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. NeurIPS, 2022.\n\n[3] J. Yang, et al. GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. NeurIPS 2021.\n\n[4] B. Jin, et al. Patton: Language Model Pretraining on Text-Rich Networks. ACL 2023.\n\n[5] Chien, Eli, et al. Node feature extraction by self-supervised multi-scale neighborhood prediction."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406347320,
                "cdate": 1700406347320,
                "tmdate": 1700406676754,
                "mdate": 1700406676754,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]