[
    {
        "title": "Reinforcement Learning for Node Selection in Branch-and-Bound"
    },
    {
        "review": {
            "id": "EKLMZYrARW",
            "forum": "0ez68a5UqI",
            "replyto": "0ez68a5UqI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_Lp54"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_Lp54"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a reinforcement learning algorithm for node selection problem in Branch-and-Bound (B&B) for Mixed Integer Programming. While prior work mostly focuses on ranking a pair of nodes, authors propose to use GNN to leverage information across the B&B tree. The policy induces a distribution across all open nodes in B&B tree. Authors propose Policy network and Value network architecture based on GNN. The proposed architecture is trained on TSP problems, and evaluated on both TSP and MIPLIB benchmark. The proposed method outperforms SCIP's default node selector across benchmarks considered."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Significance: Node Selection is instrumental for successful B&B. Prior research has been focused on Imitation Learning. This is limiting because when \"expert\" policy doesn't work well on problem at hand, ML-based method similarly struggles. Therefore, authors' Reinforcement Learning-based method has the potential of allowing ML-based node selection methods to be applied to a broader range of mixed integer programming problems with bigger, more practical improvements. Hence, I consider the potential significance to be high.\n\nOriginality: Given a related problem of Variable Selection has gone through a similar transition from Imitation Learning to Reinforcement Learning, the proposal of reinforcement learning method is not entirely unforeseen. However, authors make original contribution by proposing how to represent states of Markov Decision Process with GNNs."
                },
                "weaknesses": {
                    "value": "Quality: Experimental setup of the paper could be improved more directly test the paper's key hypothesis: that 1) Reinforcement Learning provides an advantage over Imitation Learning, 2) considering the entire tree state is better than just considering isolated nodes. These are points which distinguish authors' work from prior work. Unfortunately, authors compare against only SCIP's default node selector, and previously proposed algorithms are not considered.\n\nAlso, authors use metrics and benchmark datasets not used in previous papers in this area of research. This makes difficult to interpret experimental results within the context of current research. In fact, many of the issues with metrics authors run into could be addressed with Primal/Dual/Gap Integral metrics https://www.ecole.ai/2021/ml4co-competition/ (see Metrics page), as these metrics would still be sensible when one algorithm can reduce the gap to be zero; since authors' metrics are not very well-defined when zero gap can be (nearly) reached, authors had to employ nontrivial preprocessing of data.\n\nThese two are major concerns. The contribution of the paper is mostly the proposal of an empirical method that improves upon prior work, and therefore it is important for experiments to be designed to measure the advantage of the proposed method upon prior art.\n\nClarity: The main ideas of the paper is clearly described and easy to follow. Some technical statements did not provide sufficient reasoning to justify, however. For example, in Section 4.3, it's argued: Assuming $P \\neq NP$, it is unreasonable for the proposed algorithm to tackle provably hard instances. In practice, MIP solvers are often applied to problems which don't allow even good approximation guarantee, and therefore I wasn't sure why $P \\neq NP$ would imply these problems to be not tractable."
                },
                "questions": {
                    "value": "Is the reward (equation 5) only received at the end of the \"episode\" (end of the MIP solve)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646900744,
            "cdate": 1698646900744,
            "tmdate": 1699636037404,
            "mdate": 1699636037404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kvM6FGqehu",
                "forum": "0ez68a5UqI",
                "replyto": "EKLMZYrARW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First and foremost, we thank the reviewer for his time. We thank the reviewer for noting our improvements on one of the core aspects of the branch-and-bound algorithm.\n\n> 1) Reinforcement Learning provides an advantage over Imitation Learning,\n\nIt is not really possible for our methods to use an off-the-shelf imitation learning approach due to one of our (implicit) core assumptions: The reinforcement learner is going to \u201cdiscover\u201d novel problems during search which were not originally specified, i.e. while we only generate TSP instances, due to the branching and cutting inside the algorithm, the model will still observe sub-trees which have significantly different structure. This is why we can get away with training only on 200 synthetic instances, while prior art often uses 100k across different problems. However, this also means that imitation learning algorithms are going to get stuck (if they work well) as they are not going to explore as much. Further, using off-the-shelf imitation learning models is not possible due to how the model represents the environment:\nFor instance, which future actions a model has fully depends on which past actions were performed, i.e. you have #leaves \u201cactions\u201d, and which actions you have available depends on the prior actions. This means behavioral cloning by e.g. minimizing the KL-divergence between agent and expert distributions is simply not possible, because after two or three steps the distributions between them will be almost fully disjoint: \nEven the domain of the expert\u2019s probability density function (i.e. set of leaves) might be different from what the agent explored (meaning that e.g. the KL-divergence would always be infinite). \nThis does not mean imitation learning is fully impossible, but it does mean that imitation learning would need specific algorithmic adaptations to work in this setting.\nIn that sense the advantage of Reinforcement Learning over Imitation Learning is that this policy is trainable _at all_ using standard techniques.\n\nWe have clarified this in our methods section.\nWere we able to clarify your concerns?\n\n\n> 2) considering the entire tree state is better than just considering isolated nodes. These are points which distinguish authors' work from prior work. Unfortunately, authors compare against only SCIP's default node selector, and previously proposed algorithms are not considered.\n\nJust considering isolated nodes it is not possible to design a node selection policy as node selection specifically relies on the relative difference between the quality of two nodes. This is why Labassi et al had to phrase their \"node local\" representation as a comparison between two nodes as individual nodes are fundamentally not capable of selecting their own priority in isolation.\nThe closest thing we can try is setting our message-passing iterations to zero, which means that only the pathwise sums are used to build the tree representations. While these are still global representations, this would allow one to test whether the quality of the non-local embeddings has significant effects on the overall policy's quality.\nThank you for discussing this point, we will run these experiments and update the paper as soon as the results are ready (soon).\n\nYou further note that a comparison against Labassi et al would have been useful:\nThis is true, however benchmarking other learned node selectors proved challenging: The major piece of prior art is Labassi et al\u2019s \u201cLearning to Compare Nodes in Branch and Bound with Graph Neural Networks\u201d. Unfortunately this method is hard-coded on SCIP version 7, while we rely on SCIP version 8. Considering that \u201cThe SCIP Optimization Suite 8.0\u201d (https://or.rwth-aachen.de/files/research/repORt/scipopt-80.pdf) estimates the difference between SCIP 7 and SCIP 8 at up-to 50%, meaning that labassi\u2019s method loses simply due to being implemented in a 2 year old MILP framework (as a side note: Labassi also only works for Linear Problems, so would not even apply to our nonlinear benchmarks).\nConsidering that SCIP has been the most dominant open-source MILP solver for over a decade (see e.g. https://plato.asu.edu/bench.html), we think that benchmarking against it is a fair representation of the overall performance (i.e. we consider the SOTA MILP solver, rather than the SOTA under the Deep Learning heuristics specifically)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699710255647,
                "cdate": 1699710255647,
                "tmdate": 1699710255647,
                "mdate": 1699710255647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qn16qQJTtz",
                "forum": "0ez68a5UqI",
                "replyto": "EKLMZYrARW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Once again, thank you for your remarks regarding local vs global representations!\nWe conducted additional experiments by training an additional agent without the GNN (i.e. zero message passing steps), to test the performance in a \u201cminimally global\u201d setting.\nNote that completely independent node predictors do not work, as without correlating nodes at least pairwise, no node can determine whether it is \u201cthe most optimal\u201d simply because the node does not know which other options exist.\n\nTesting the model without message passing, gave some interesting insights.\nFirstly, the model without the GNN is considerably more noisy; so much so that benchmarking on our tests sets does not really make sense: Running the same model 3 times on TSPLIB essentially yields 3 different results. \nConsidering only the cases where the GNN-free model reaches low optimality gaps, the model needed considerably more nodes than the baseline or our model with GNN to reach comparable performance (roughly 30%, but that number is prone to noise).\nSecondly, models trained without a GNN diverge on the training set: Starting off at a reward of roughly zero, it diverges to a reward of -0.2 (i.e. 20% worse than SCIP) by the end of training.\nWe have added these additional details in appendix F.\n\nThank you once again for your pointer and we would be excited to answer additional questions!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048042090,
                "cdate": 1700048042090,
                "tmdate": 1700048042090,
                "mdate": 1700048042090,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qz2UJu2Qif",
            "forum": "0ez68a5UqI",
            "replyto": "0ez68a5UqI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_rRw3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_rRw3"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the brand-and-bound problem, for which the authors propose a new method of simulation using RL for getting a more global view of the tree, augmented with heuristic node selection methods: tree encoding by GNN, features are learned by message passing and node selection is done by PPO. Experiments show positive results on many benchmarks despite the training being on TSP simulations. Another good thing is that code is provided (although I haven\u2019t tested myself)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The global view of trees is a strong motivation given the limitation of current methods in BnB. \n- I personally like the \u201cgreedy\u201d aspect in reasoning (in introduction) that theory vs. practice has a gap, especially for many cases like the BnB and in practice, oftentimes we should favor a shorter-term choice over long-term ones if it\u2019s good enough for many reasons. I think that is correct to the large spectrum of deep learning applications nowadays. \n- Positive results on many benchmarks. \n- Helpful supplemental contents."
                },
                "weaknesses": {
                    "value": "- The strong motivation leads to a much larger cost in carrying out the algorithm, especially when it involves recursion.. However, it\u2019s not clear from the paper as to why the authors only choose the upper bound as a factor of choosing. Would be interesting if they have a study \u2013of maybe a comparison\u2013leading to that choice. \n- To solve this complex problem, the proposed method has to be broken down into many phases as shown in Section 2. That raises a question about the practicality: can the method be integrated as one to make it end-to-end. If not yet, what are the factors needed or what changes to enable that. \n- Another unclear aspect is the design of the reward method, e.g. why that formula in terms of motivation and explanation, and why not replace the term (\u201c-1\u201d) in Equation 5 with a constant C and study different values of it? \n- After the reward function, yet another unexplained technique of \u201cshifting\u201d, and another heuristics of clipping the reward. Is there any other better way of normalizing that or better design of the reward function to make sure that range complies while having a nice curve to the problem?\n- Why PPO? Would also be nice if comparing PPO to alternatives such as maybe TRPO, SAC, \u2026\n- Yet another heuristics is to remove problems >100% or 0 gap. That begs a question on the quality of design including the reward function. \n- Overall, the paper gives an impression that despite a good motivation and a complex problem, it\u2019s a collection of heuristic choices without substantiated evidence/studies supporting them. Such heuristics I think undermine the main motivation (i.e. ones might question how much contribution of RL in yielding the results you are getting?) It would be much more convincing if the authors address this aspect now or later. \n\n======\n\nAdditionally: some monor typos: \n- Maybe should not write TSP in the abstract as abbreviation (abbr), which is inconsistent, since the abbr term RL was defined before that. \n- Just in case, please use the newest ICLR 2024 template \n- Appendix E citation format is not consistent with other parts  \n- Section3, first line of 2nd paragraph, define the abbr \u201cIL\u201d first."
                },
                "questions": {
                    "value": "- As also stated and shown, the problems are hard to handle computationally due to numerical instability. It is however not clear what problems they run into, and how the authors handle them. Those are very important for the community in terms of insights and reproducibility. \n- Table 3: The \u201cGap Ours\u201d and \u201cGap Base\u201d column have all normal values but the mean is NaN. Why? \n- See other questions in the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795195966,
            "cdate": 1698795195966,
            "tmdate": 1699636037334,
            "mdate": 1699636037334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mZ2Vpj8xv1",
                "forum": "0ez68a5UqI",
                "replyto": "qz2UJu2Qif",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We first and foremost want to thank the reviewer for his time. Thank you for highlighting the strengths of our paper in the form of global tree representation, our thorough benchmarking and supplemental information.\n\n> The strong motivation leads to a much larger cost in carrying out the algorithm, especially when it involves recursion.. However, it\u2019s not clear from the paper as to why the authors only choose the upper bound as a factor of choosing. Would be interesting if they have a study \u2013of maybe a comparison\u2013leading to that choice.\n \nDue to the fact we do not train our model during inference (solving the benchmarking instances), the actual overhead is relatively small and mostly exists due to python being slower than optimized C code.\nWe will clarify this in our revision.\nWe could also parameterize the selection of leaves as a comparison, but that would ultimately yield a very similar objective. Ultimately we select max(leaves) one could also rephrase this as selecting the leaf such that it is the most optimal one wrt node comparison, but ultimately this would produce the objective \u201cchoose x such that x >= y for all leaves y\u201d, which in turn amounts to maximization.\nWe mostly chose to select based on maximum probability as it aligns well with prior research into reinforcement learning (i.e. the same as action selection in other environments as well).\n\n> To solve this complex problem, the proposed method has to be broken down into many phases as shown in Section 2. That raises a question about the practicality: can the method be integrated as one to make it end-to-end. If not yet, what are the factors needed or what changes to enable that.\n\nThe graphic may be a little misleading here: The model is trained end-to-end. One has to think of SCIP as the RL \u201cenvironment\u201d which, just like all other RL environments , is thought of as a black box of input actions and output states. The actual selection policy is still trained end-to-end, just like in all other RL problems.\n\n> Another unclear aspect is the design of the reward method, e.g. why that formula in terms of motivation and explanation, and why not replace the term (\u201c-1\u201d) in Equation 5 with a constant C and study different values of it?\n\nThe movement by -1 is simply mean-shifting the reward, just like one would do in a normal regression problem. Clipping has been a common technique for reward stabilization for a number of years (see e.g. Human-level control through deep reinforcement learning https://www.nature.com/articles/nature14236), especially in environments with a high possible range of rewards (such as Atari). We will make sure to reference this.\n\n> Why PPO? Would also be nice if comparing PPO to alternatives such as maybe TRPO, SAC, \u2026\nWe chose PPO due to its relative robustness and easy hyperparameter tuning. Off-policy methods like SAC are theoretically very appealing, but end up being hard to tune properly.\nAs our method relies on a novel network design in a relatively under researched field, we opted to err on the side of robustness.\n\n> Yet another heuristics is to remove problems >100% or 0 gap. That begs a question on the quality of design including the reward function.\n\nThis is simply a restriction one has when working with mixed-integer solvers such as SCIP: For some instances, SCIP\u2019s solver and simplification routines are simply so advanced that the instance becomes trivial. At that point, one could keep this instance in the training set, but that would give misleading rewards to the model since even horrible selections could solve these instances simply due to the power of SCIP itself. On the other hand, there are problems that are so hard one can never make any progress on them, even if you had oracle node-selections. To improve the signal-to-noise ratio of our reward, we decided to filter those out proactively. This is presumably also one of the reasons why we can train a highly generalizing agent with only 200 instances, compared to e.g. Labassi which needed over 100k training instances across different problems.\n\n> Additionally: some monor typos [...]\nThank you for the heads up! \nWe will fix this as soon as we update the paper with the additional results mentioned for Reviewr hSJj."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699706656673,
                "cdate": 1699706656673,
                "tmdate": 1699709064933,
                "mdate": 1699709064933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mRkoI43d10",
                "forum": "0ez68a5UqI",
                "replyto": "zMA8unxTFL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Reviewer_rRw3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Reviewer_rRw3"
                ],
                "content": {
                    "comment": {
                        "value": "Post rebuttal: after reading others' reviews and authors' responses, I appreciate the authors for the great effort in responding and making revisions. I think the revised version is a lot more clear. Yet I hope the paper will have another round of revision to highlight more an underscored theme of RL with a global tree view, rather than a mixture of heuristics to pull the main method off. \nFor now, I am keeping my scores, thinking it's almost there."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328439481,
                "cdate": 1700328439481,
                "tmdate": 1700328439481,
                "mdate": 1700328439481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YDtwWjqKCu",
            "forum": "0ez68a5UqI",
            "replyto": "0ez68a5UqI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_KqYv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_KqYv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for node selection in branch-and-bound using reinforcement learning. The proposed method uses a graph neural network to model the node selection as a probability distribution considering the entire tree. Based on this, reinforcement learning is applied to perform node selection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper clearly states the issue (node selection in branch-and-bound) trying to address, and the limitation of the conventional methods on that issue.\n2. The paper provides the simulation results in a variety of problem instances."
                },
                "weaknesses": {
                    "value": "1. There are existing related works that use graph neural networks for node selection in the branch-and-bound algorithm. The proposed method in this paper uses graph neural networks for tree representation, but the difference from the existing works is not clearly stated.\n2. The structure of RL such as states, actions, and reward function is not rigorously defined in the paper. This makes it harder to understand how the RL method works in the proposed method.\n3. As the branch-and-bound algorithm proceeds, the number of nodes and the tree structure keep changing. Then, should the RL agent be trained from the scratch for every step of the branch-and-bound algorithm?\n4. The discussion on the learning cost of the RL algorithm is required. How is the cost due to collecting the enough experiences for the convergence of the RL policy?\n5. It seems that the RL agent should be trained for each problem instance. Is this training of RL agent for each problem instance is mandatory to use the proposed method? If additional RL agent training is always required, it may not be practical to use it.\n6. In the similar context, is there a possibility of using a pretrained RL agent that can be applied to a variety of problems? It would be helpful to demonstrate the generalization capabilities of the RL agent for more insight on the proposed method.\n7. Comparison with related works using graph neural networks, in particular, Labassi et al. (2022) which is one of the state-of-the-art methods, seems to be necessary in experiments. The authors stated that it was unable to conduct experiments due to the version compatibility issue, but a comparison between the similar state-of-the-art methods is essential."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810336509,
            "cdate": 1698810336509,
            "tmdate": 1699636037248,
            "mdate": 1699636037248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bHC3QoBfAI",
                "forum": "0ez68a5UqI",
                "replyto": "YDtwWjqKCu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First and foremost, we thank the reviewer for his time and effort and thank you for recognizing our main contributions.\n> There are existing related works that use graph neural networks for node selection in the branch-and-bound algorithm. The proposed method in this paper uses graph neural networks for tree representation, but the difference from the existing works is not clearly stated.\n\nWe are not aware of any method that utilizes Graph Neural Networks to represent the branch and bound tree.\nDo you have references for relevant works?\nWe are only aware of Graph representations of individual nodes in the tree, i.e. every node is one constraint graph between the node\u2019s variables and constraints. One contemporary work published during this review cycle also uses an implicit tree representation, where instead of a timeseries the authors consider a \u201cTree series\u201d where the MDP is modelled as a tree rather than a linear timeseries. They also consider the variable selection problem, rather than the node selection problem.\n\nBoth of these approaches are completely orthogonal to ours: Our approach allows for TreeMDPs and nodes being represented as Graphs just the same.\n\nHowever, in some sense our approach can be seen as a transpose of the constraint-graph representation: Constraint graphs consider one node and anchor their graph to the instance, while our approach considers the entire graph and anchors its graph to the branch and bound structure.\nOur approach does not need this implicit correspondence between a nodes constraint matrix and graph, which allows us to tackle nonlinear problems in which no constraint matrix exists.\n\nDoes the reviewer think it would be useful for us to make this connection as part of the paper?\n\n> The structure of RL such as states, actions, and reward function is not rigorously defined in the paper. This makes it harder to understand how the RL method works in the proposed method.\n\nWe amended our paper to define this more thoroughly by giving the tuple of (State, Action, Reward) explicitly.\n\n> As the branch-and-bound algorithm proceeds, the number of nodes and the tree structure keep changing. Then, should the RL agent be trained from the scratch for every step of the branch-and-bound algorithm?\n\nThis is not a problem due to how we design our model: Each state is an entire Tree that is parameterized as a graph neural network. After each state transition we still have a graph, which can be processed just the same using the same network and the same weights. The trick behind our algorithm is precisely that we can obtain a global view on the entire problem _without_ needing any modifications to the network (similar to how an LSTM works regardless of sequence length, our GNN works regardless of tree size and topology).\n\nBundling the next remarks into one:\n> The discussion on the learning cost of the RL algorithm is required. How is the cost due to collecting the enough experiences for the convergence of the RL policy? It seems that the RL agent should be trained for each problem instance. Is this training of RL agent for each problem instance is mandatory to use the proposed method? [...]\n\nAll experiments in this paper were performed using the same agent trained only on synthetic samples. This means that during benchmarking no training takes place at all: It just infers the correct next node by using the existing fixed weights. Overall our method was only ever trained on a small dataset of 200 instances of the traveling salesmen problem for what amounts to 16 epochs (generation details are in section 4.3).\nSpecifically, the benchmarks are all done using a _single pretrained_ agent: The agent was trained once on the synthetic instances, the weights are frozen and then the agent is run on the different benchmark datasets.\nThese datasets are substantially more complex, larger, and of completely different problems than what can be found in the training dataset.\nIn practice, this would amount to e.g. SCIP being delivered with a pretrained policy which will be automatically used without additional training on novel problems.\n\nOther reviewers also noted this confusion, hence we updated the paper to make this more clear (see abstract)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699706810435,
                "cdate": 1699706810435,
                "tmdate": 1699709199594,
                "mdate": 1699709199594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P3gGwUe2px",
                "forum": "0ez68a5UqI",
                "replyto": "qVQAO3LtUD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Reviewer_KqYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Reviewer_KqYv"
                ],
                "content": {
                    "comment": {
                        "value": "After reading the responses, some of the concerns have been addressed and the revised paper is improved. However, two primary concerns are still not addressed.\n\n1) I still don't feel that the RL structure is rigorously defined. The revision is careless and the MDP is still not well defined mathematically. The additional descriptions are not clear and make it even more confusing.\n2) Comparison with other SOTA algorithms is essential, and the authors' response cannot be an excuse for not doing experiments. SCIP is just one of the solvers, and the existing algorithms typically present the general idea of node selection that can be used in branch-and-bound, and they are not an add-on of SCIP. Therefore, performance improvements may still be expected by implementing and applying the existing algorithms to SCIP 8. If not, the rationale should be clearly provided.\n\nBecause of the above concerns, I keep my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482961357,
                "cdate": 1700482961357,
                "tmdate": 1700482961357,
                "mdate": 1700482961357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qNJn2tKNFE",
            "forum": "0ez68a5UqI",
            "replyto": "0ez68a5UqI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_hSJj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1112/Reviewer_hSJj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a reinforcement learning framework to learning the node selection policy in branch-and-bound algorithm. In particular, the paper considers an environment based on SCIP. The considers a graph neural network on the branch-and-bound tree with root-to-leaf path aggregated scores as the policy net and employs policy gradient algorithms for training. The paper carefully generates TSP problems with moderate difficulty for training and evaluate the learned node selection policy on TSPLIB, UFLP, MINLPLib, MIPLIB. The results show that the learned node selection policy outperforms the default policy in SCIP in terms of Reward and Utility/Node."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Most of the paper is well written and easy to understand for readers with basic knowledge in reinforcement learning and branch-and-bound.\n* The root-to-leaf path aggregated score is a clever design. It avoids the computation challenge from the growing of the branch-and-bound tree by an intuitive assumption: if a node is good, so should be its ancestors."
                },
                "weaknesses": {
                    "value": "* The definition of the reward is not rigorously defined. Specifically, the paper does not disclose how are the gap(node selector) and gap(scip) are calibrated. It could be\n    * The gap when reaches the time budget.\n    * The gap at the same number of nodes n, with $\\text{traj}(\\text{node selector})[:n]$ rolled out with node selector, $\\text{traj}(\\text{scip})[:n]$ rolled out with scip, \n    * The gap at the same number of nodes n, with $\\text{traj}(\\text{node selector})[:n]$ and $\\text{traj}(\\text{scip})[:n-1]$ rolled out with node selector, $\\text{traj}(\\text{scip})[n-1:n]$ rolled out with scip.\n\n* The score in evaluation need more justification. From my perspective, the most important goal of learning a node selection policy is finding good primal solutions. With this aim, none of the scores are a good choice. The second priority for a node selection policy is to close the duality gap. In this sense, \"Utility/Nodes\" is not good choices. \n\n* The paper consider a small threshold 45 seconds. This is a relatively small time budget for solvers to solve MIP problems. To demonstrate the learned policy is practical, the results with a longer running time should be reported."
                },
                "questions": {
                    "value": "* Only the results on benchmarks are provided in the paper. I am curious about the performance on the training data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698894898347,
            "cdate": 1698894898347,
            "tmdate": 1699636037175,
            "mdate": 1699636037175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6A4JntDl6Z",
                "forum": "0ez68a5UqI",
                "replyto": "qNJn2tKNFE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First and foremost we want to thank the reviewer for their time and effort.\n\n> The definition of the reward is not rigorously defined. Specifically, the paper does not disclose how are the gap(node selector) and gap(scip) are calibrated. It could be\n\nThank you, we did indeed not define this thoroughly enough.\nYour first variant is the correct one:\nWe define the gap(node selector) and gap(scip) as the optimality gap reached by our selector or SCIP after the time budget is reached. This accounts for the fact that SCIP node-selection may be faster than our node selection, which wouldn\u2019t be accounted for when measuring relative to the number of nodes solved. We updated the paper accordingly.\n\n> The score in evaluation need more justification. From my perspective, the most important goal of learning a node selection policy is finding good primal solutions. With this aim, none of the scores are a good choice. The second priority for a node selection policy is to close the duality gap. In this sense, \"Utility/Nodes\" is not good choices.\n\nThe objective of our metrics was mostly to preserve the goal of minimizing the duality gap, while also accounting for the fact that the duality gap may have very different scales from problem to problem, hence the normalization using the SCIP baseline and zero-centering of the \u201cour gap\u201d / \u201cscip gap\u201d loss.\nWe agree with you and the other reviewers, therefore we decide to also publish the shifted geometric mean of the duality gap for all instances in each dataset to give more of a global view of our methods performance. The reason we did not do so initially is due to the fact that shifted geomean does not work if infinite duality gaps are in the results. In the new results we model infinities as a duality gap of 10^20, just like SCIP would.\n\n> The paper consider a small threshold 45 seconds. This is a relatively small time budget for solvers to solve MIP problems. To demonstrate the learned policy is practical, the results with a longer running time should be reported.\n\n\nWe also share your concern of our short runtime: We initially chose this runtime since we were mostly interested in the initial (most impactful) phase of node selection, and other works (such as Labassi et al.) only having an average solver time of less than thirty seconds, even on their largest instances.\nHowever, we do concede that evaluating with larger time budgets is very important: We have added additional runs on our real-world datasets with a time budget of 5 minutes for TSP, MINLPlib, and MIPLIB. For these tests we increase the number of nodes processed by our method from 250 to 650, as the longer overall runtime justifies running our selector for a longer time as well.\n\nAs you can see, we still outperform the SCIP baseline despite only being trained on instances with a 45s time limit, usually even increasing the relative gap between our method and scip.\n\n> Only the results on benchmarks are provided in the paper. I am curious about the performance on the training data.\nWe also added information on our training results in the appendix.\n\nWe hope we managed to address all your concerns and are happy to discuss further."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699706094992,
                "cdate": 1699706094992,
                "tmdate": 1699707645171,
                "mdate": 1699707645171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]