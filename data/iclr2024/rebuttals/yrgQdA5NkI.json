[
    {
        "title": "Equivariant Matrix Function Neural Networks"
    },
    {
        "review": {
            "id": "87wgrmi0xN",
            "forum": "yrgQdA5NkI",
            "replyto": "yrgQdA5NkI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2444/Reviewer_Qi1r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2444/Reviewer_Qi1r"
            ],
            "content": {
                "summary": {
                    "value": "GNNs need to balance the contributions from local (nearby nodes) data and more non-local data. The authors here present an approach based on spectral graph convolutional methods which can capture local and non-local features through a parameterization via resolvent expansions. The authors propose various means of implementing their method to both improve performance and speed up the technique. Their results are backed by experiments on a few datasets which show relatively good performance of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The most important strength is that the paper presents a method which performs well on numerical experiments. Though I have some concerns about the experiments, if the results are confirmed and these concerns are appropriately addressed, I think this method should be documented and tested further.\n\nSeparate from the experiments, the proposed method is an extension of spectral convolutional approaches which have enjoyed great success. The particular strategy that the authors propose, via resolvent expansions, seems like a decent approach to take when including long range interactions. It is tied together relatively nicely with geometric data as well. Implementation can be tricky in certain parts (e.g. implementing complex numbers) but the architecture and overall idea is relatively sound."
                },
                "weaknesses": {
                    "value": "The method is laid out in section 4 and to be honest, I had quite a bit of trouble following along. A number of notational concerns are pointed out below. But let me just start at the beginning and make sure I have the right picture. As I understand, the authors are choosing $b$ basis operators which are functions taking inputs consisting of a set of $\\mathcal{X} \\times V$ pairs where $V$ is some space for the hidden state. Then, they apply linear maps on top of these outputs from the bases which respect the graph permutation equivariance, hence a $bn \\times bn$ matrix. I still am not sure this is the right interpretation, but the authors need to state this more formally and cleanly. They should specify what the functions $\\phi_m$ do. I.e. specify input and output space. Indexing of eq. (7) is confusing. I believe $ij$ refers to the node indexing, but the matrix is $bn \\times bn$ so $H_{cij, m_1, m_2}$ is not consistent with how the entries of the matrix work. Also, eq. (7) seems to indicate the entries are outputs of fixed basis functions, but these can be learnable functions as well? The authors should also give examples of $\\phi_m$ as promised (is (21) an example of such?). I also believe that eq. (7) should be indexed over $k$ in the intersection of the neighbors, not the union. This is just a sampling of the confusions that arise and eventually required me to make decisions as to what is actually happening.\n\n\n\n\nMoving on, I have concerns about the experiments. First, there is no code shared so I cannot verify numbers. What is shared in the zip file appears to be a dataset and some details on how to train a model for that dataset. Sharing code is obviously not a requirement, but it means I have to judge the result solely on what is written. Here, the methods of comparison to other results are potentially in question. For example, it appears complex numbers are being counted as a single parameter per number whereas it should be two per number (real and complex parts are both variables); see question below asking to clarify this. Runtime comparisons are also not provided, and this can be a concern here, since the method requires spectral decompositions (or approximations thereof). Finally, see questions below, but I am not sure how certain features are being extracted. E.g., the authors say that they take descriptors from the PDF model in the appendix for the ZINC data, but this would be unfair if they take features from a trained model on the same dataset. Nonetheless, if these issues are resolved, results look good as stated and if confirmed, this would add support to the method.\n\nI simply could not follow section 4.3. What is the Combe-Thomas theorem? There is no citation for this. Also, doesn\u2019t going from the bound stated in the paragraph before Eq (16) to the bound in Eq. (16) require some dependence on the norm of the weights? Later on, the geometric expressiveness statements are stated without proof and missing too many details for me to follow. I have many questions. First, what exactly are the authors saying is equivalent to the infinite layer MPNN (does the matrix channel need an infinitely sized pole expansion)? Where are the formal proofs of these statements? Does expressiveness here imply approximating a given function and if so, in what norm? What are two-body entries?\n\nNotation comments:\n- Eq (3): $\\mathcal{G}$ in the notation was defined as a specific graph; however, here it is used to denote the space of graphs.\n- Eq (4): $\\Phi \\odot g$ is confusing. Why is there a representation for the output space but not the input space? \n- Below Eq (8): $\\rho$ is bold in the equation but not bold there. Also, $\\rho$ is unitary in general, not orthogonal as far as I understand. If it is orthogonal, then why is there a complex conjugate transpose instead of just a transpose?\n- ... and many more. I would ask the authors to go through the paper and significantly revise and clean things up. \n\n\nAltogether, I cannot recommend acceptance for this paper given the writing quality and concerns laid out above. Nonetheless, I welcome the authors' responses to these points."
                },
                "questions": {
                    "value": "Other questions/concerns:\n- When reporting parameter counts, the authors have some parameters which are complex numbers. Are these complex numbers counted as two parameters per number or one? A fair comparison should count each complex number as two parameters since there is a trainable real and complex part.\n- It seems this parameterization is a nonlocal form of message passing and the resulting layers manipulate the eigenvalues of the Laplacian or adjacency matrix. We know from expressiveness results (WL or otherwise) that this is limited because the eigenvalues are insufficient to encode all the information in a graph. Is my understanding here correct? If so, what are the practical limitations that arise from this implementation?\n- What are the $\\phi_m$ basis functions for pure graph inputs? \n- If the $\\phi_m$ are being learned, did you include this in the parameter count?\n- When you say \"The initial edge features ... are obtained by concatenating PDF [Yang et al. (2023)] descriptors used for ZINC\u201d, are these descriptors the outputs of the trained model? That would be unfair for comparison if so."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2444/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2444/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2444/Reviewer_Qi1r"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697736701490,
            "cdate": 1697736701490,
            "tmdate": 1700533929123,
            "mdate": 1700533929123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lVCy19FJaG",
                "forum": "yrgQdA5NkI",
                "replyto": "87wgrmi0xN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. We appreciate that you found that our method \"performs well on numerical experiments\" and works \"nicely with geometric data\". In our revision, we made a significant effort to make the method more understandable. In the following, we respond to your questions and suggestions to further improve the paper. We respectfully hope that our responses will be satisfactory and increase your score.\n\n### Relationship to spectral convolutions\n\n> The proposed method is an extension of spectral convolutional approaches which have enjoyed great success. The particular strategy proposed by the authors, via resolvent expansions, seems like a decent approach.\n\nWe want to emphasize here that there are key differences between Matrix Function Networks and previous spectral convolution approaches. \n- In all spectral convolution approaches known to us, the convolution operators were used as filters. We believe that the step of parameterizing features in a neural network architecture to be learnable graph operators and updating the node features based on matrix functions of these operators is new. To be precise, previous spectral convolution approaches have an update rule of \\$X^{t+1} = f(L)X^{t}\\$ where \\$L\\$ is fixed (the Laplacian), while MFN acts as \\$X^{t+1} = f(H(X^{t}))\\$ where both \\$H\\$ and \\$f\\$ are learnable. In particular, each entry in the learnable matrices is a many-body function of the local environment of each edge. In subsequent layers, new operators will be learned based on these extract features.\n- Furthermore, no spectral convolution methods known to us included equivariance to Lie groups, and therefore these methods were not competitive for geometric point clouds. Our method to parametrize the operators naturally expend to geometric point clouds.\n\n### Clarification on the matrix construction step\n\n> The method is laid out in section 4 and to be honest, I had quite a bit of trouble following along.\n\nWe understand that we attempted to make Section 4 the most general possible. However, we now realize that this leads to a very abstract and abstruse definition. We have rewritten the matrix construction step to simplify the language, and we added a new section that gives examples of concrete implementation of the matrix construction in the case of the \\$O(3)\\$-group. We also have largely expanded Figure 1 to provide a better illustration of the method, including a visual representation of each step of the matrix function network (see https://ibb.co/2NhsmRb for a preview). We hope that these edits will enable a better understanding of the key contributions of our method. Here is a more condensed version of the key part of our approach, simplifying a bit the notation:\n- First, the method takes a graph with node labels and edge labels as entries and constructs **local** many-body node features using a graph neural network layer (for example, a convolution graph neural network for pure graphs, or an equivariant graph neural network for geometric graphs), that we call here \\$V_i^{(t)}\\$, \\$V_j^{(t)}\\$.\n- Second, a graph matrix \\$H_{ij}\\$ is learned by parameterizing its entries with a learnable function of the node features \\$H_{ij} = \\phi(V_i^{(t)}, V_j^{(t)})\\$. The notion of the size of the basis is relevant only for geometric graphs, where a Lie group is acting, because this function \\$\\phi\\$ needs to be equivariant. \n- The node features are updated using the output of a learnable matrix function of these matrices. We want to stress that this is a **key** novelty in the context of GNNs. The output features are now **non-local** features.\n\n### On the experiments\n\n> First, there is no code shared [...] For example, it appears complex numbers are being counted as a single parameter per number whereas it should be two per number, see question below asking to clarify this. [...] I am not sure how certain features are being extracted. For example, the authors say that they take descriptors from the PDF model in the appendix for the ZINC data.\n\nWe have attached a copy of the two codes used to run the experiments, both for the Cumulene case and the pure graph case. The parameter count is correct; see the response below for more details, but only the poles are complex parameters. The PDF descriptors are **not** extracted from a trained model; we just use the Laplacian and fixed powers of the Laplacian of the graph to extract initial edge labels to enrich the information in pure graphs as initial embedding. We want to stress that this does not affect the number of parameters, and this extraction was not trained in any way. They are not related to **anything** learnable, just a fixed precomputable initial embedding. As our MFN learns custom operators beyond the Laplacian, we wanted to include this operator as a baseline. Sorry for the confusion this has generated."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243344523,
                "cdate": 1700243344523,
                "tmdate": 1700322547468,
                "mdate": 1700322547468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vRO0542YvD",
                "forum": "yrgQdA5NkI",
                "replyto": "87wgrmi0xN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### On the Combes Thomas Theorems\n\n> I simply could not follow section 4.3. What is the Combe-Thomas theorem?  Also, doesn\u2019t going from the bound stated in the paragraph before Eq (16) to the bound in Eq. (16) require some dependence on the norm of the weights?\n\nWe understand that this section was very technical and we moved most of this to the appendix and used the available space to clarify Section 4.1 on the core of the architecture. We now cite the Combes-Thomas theorem paper [1]. \n\nThank you for highlighting the dependence on the norm of the weights, this is an important point that we forgot to mention. We now added a brief explanation that the bounds can be taken uniform for suitable bounds on the operator which are guaranteed by our normalization procedure.\n\n\n### Runtime comparison\n\n> Runtime comparisons are also not provided, and this can be a concern here, since the method requires spectral decompositions (or approximations thereof).\n\nWe agree that computational cost is a key parameter in any method. As explained in Section 4.2 in the Linear Scaling Paragraph, the prototype implementation has a cubic scaling and is therefore quite slow compared to other approaches (a factor of x2 to x5 compared to local models). However, fast inversion methods are well established, and we explain how this scaling can be reduced significantly to less than quadratic scaling, all the way to linear scaling, by exploiting the sparsity of the matrices we construct and selected inversion methods. \n\nAlthough these methods are known, they require significant effort to implement and integrate them into machine learning libraries like Pytorch due to the need for specialized CUDA kernels. We intend to make this effort and release open-source code in future work, but we believe that this is beyond the scope of this paper, where we focus on the novelty of the architecture and its expressivity. \n\nFinally, we stress that one of the key results of this paper is that MFN can model systems that no other architectures can model, including Transformers. Due to the unstructured non-locality of global attention, Transformers cannot extrapolate to large systems in the Cumulene example. This means that regardless of speed, there are applications in which MFNs are clearly the only solution to date. We want to stress that this solves an important open problem in the community of machine learning force fields (see [3]). We made changes to the contributions to better highlight this key point.\n\n### On relationship with infinite layer MPNN\n\n> Later on, the geometric expressiveness statements are stated without proof and missing too many details for me to follow. First, what exactly are the authors saying is equivalent to the infinite layer MPNN (does the matrix channel need an infinitely sized pole expansion)? Where are the formal proofs of these statements? Does expressiveness here imply approximating a given function and if so, in what norm? What are two-body entries?\n\nAlthough we agree that a full proof of this statement would be interesting, we believe that it is beyond the scope of this paper due to the very limited length. That is why we did not claim any proof but intended to present that as a remark for future investigation. We moved this part of the paper to the appendix and used the gain space to clarify other sections. We also have toned down the statement to show that it is a well-motivated remark rather than a theorem. Here, we give an intuition of the relationship between the two approaches:\nAny analytic matrix function \\$f\\$ admits a formal power series expansion, valid in its radius of analyticity,\n\\begin{equation}\n    f(H) = \\sum_{k=0}^{\\infty} c_{k} H^{k}\n\\end{equation}\nwhere \\$c_{k}\\$ are the complex (or real) coefficients of the expansion. Lets look at the diagonal elements of each of these powers,\n\n$(H)^{2}_ {ii, c00} = \\sum_ {j \\in \\mathcal{N}(i)} H_ {ij,c0m}  H_ {ji,cm0}, \\quad (H)^{3}_ {ii, c00} = \\sum_ {j \\in \\mathcal{N}(i)}  \\sum_ {k \\in \\mathcal{N}(j)} H_ {ij,c0m} H_ {jk,cmm'} H_ {ji,cm'0}$\n\nThe sum over \\mathcal{N}(i) is forced due to the sparsity of the constructed matrix \\$H\\$ in our method. Therefore, we observe a convolutional structure similar to message-passing methods. The power \\$2\\$, corresponds to a single convolution, the power \\$3\\$ to two convolutions, all the way to infinity. Each coefficient of the matrix function that is **learnable** can act like weights in the linear update of the analogous layer. Because the power series expansion is infinite, one sees a clear analogy between the two.\n\n### On the parameters count\n\n> Are these complex numbers counted as two parameters per number or one? \n\nThank you for your inquiry; all parameters were appropriately counted. We want to stress that only the poles of the resolvent expansion are complex parameters, the rest being real. The poles usually represent at most 256 parameters (times two for real and complex), which is negligible in parameter counts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243964437,
                "cdate": 1700243964437,
                "tmdate": 1700243964437,
                "mdate": 1700243964437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rGMT4CV3Ev",
                "forum": "yrgQdA5NkI",
                "replyto": "87wgrmi0xN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### On the expressiveness and WL test\n\n> It seems this parameterization is a nonlocal form of message passing and the resulting layers manipulate the eigenvalues of the Laplacian or adjacency matrix. We know from expressiveness results (WL or otherwise) that this is limited because the eigenvalues are insufficient to encode all the information in a graph. Is my understanding here correct?\n\nThank you for this interesting question. First, we emphasize that MFN layers do not manipulate the eigenvalues of the Laplacian or adjacency matrix. We learn operators that are arbitrary operators. In particular, when we apply the MFN for geometric graphs, the learned operators are group equivariant, while the Laplacian or adjacency would just be invariant. \nFor expressiveness, there are two separate interesting cases:\n\n- The first case is about expressiveness on pure graphs (not embedded in a vector space). In this case, it is true that many graphs share the same spectrum for the Laplacian or for the adjacency. However, the MFN learns **multiple** graph operators. Therefore, a well-converged MFN layer can learn a set of operators whose spectrum maximally discriminates graphs relevant to the learning task. Moreover, the fact that multiple operators are learned, even if some graphs are cospectral for one of the operators, means that they need not be cospectral for the union of the spectrums of all the operators. Therefore, an MFN layer should be more expressive than simply using the Laplacian spectrum.\n- The case for geometric graphs is different. The question is related to the geometric WL-test [1]. We know that if the matrix entries are learnt using a $G$-MACE [2] layer with correlation order equal to the number of points of the graph, then the spectrum of this operator is complete, in the sense that it will be able to discriminate any graph that are not related by permutations or symmetries of $G$.\n\n### Basis functions\n\n> What are the basis functions for pure graph inputs? If they are being learned, did you include this in the parameter count?\n\nWe have added a more detailed explanation of the matrix construction in pure graph that was previously in the appendix. Here is a short summary, using the new notation introduced in Section 4 to clarify our architecture. We use a convolutional graph neural network to learn the node features $V_{i}^{(t)}$ for each node $i$. We then use a simple multilayer perceptron $H_{ij,c} = \\text{MLP}(V_{i}^{(t)}, V_{j}^{(t)}, e_{ij}^{(t)})$, where $e_{ij}^{(t)}$ are a concatenation of fixed edge attributes (see the PDF explanation) and learnable edge features extracted from the off-diagonal blocks of the matrix function. This was counted in the parameter count. We hope that the code will help clarify these points.\n\n[1] On the Expressive Power of Geometric Graph Neural Networks, Joshi, et al  \n[2] A General Framework for Equivariant Neural Networks on Reductive Lie Groups, Batatia, et al  \n[3] Machine learning force fields, Oliver T Unke, et al"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244040393,
                "cdate": 1700244040393,
                "tmdate": 1700244040393,
                "mdate": 1700244040393,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yvvwt58tX0",
                "forum": "yrgQdA5NkI",
                "replyto": "rGMT4CV3Ev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Reviewer_Qi1r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Reviewer_Qi1r"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\n\nThank you for the detailed responses to my questions and feedback. It seems that many of my concerns are directly addressed:\n- Parameter counts being undercounted due to having complex numbers is largely negligible. This appears to add at most about a thousand parameters. Nonetheless, I would still ask that the authors report the correct parameter count in the paper rather than saying certain parameters are negligible in count. \n- The inputs are not trained outputs of PDF.\n- Some code has been shared so I thank the authors for finally sharing it. I tried running it and personally had issues. This is likely due to my lack of time in going through the code, and wish I had more time to go through it.\n- Many of the confusing notation and incorrect equations have been cleaned up. Nonetheless, I still have struggles to follow at times. For example, before equation $7$ we are told that the matrix $H$ is $MN \\times MN$ but then the indexing $H_{ij,cm_1m_2}$ which adds a channel dimension and also makes the indexing in some sort of tensor notation where $ij$ comes before $m_1, m_2$ (i.e. this is clearly not the actual entry locations). This again leaves me having to make decisions about how this method works.\n\nSome things that are still concerns and I feel can be addressed better:\n- Runtime comparisons are important here especially given the expensive matrix operations. The authors are clear that this slows things down, but still, a comparison should be done to see the trade-offs. After all, other networks also use expensive steps so the added time may or may not be as bad as stated. We can only tell once it's reported. \n- Many parts of the discussion are still rather ad hoc and imprecise mathematically. For example, the *linear scaling cost* subsection makes a number of claims that may or may not hold in practice or are not cited well enough to make it clear that they are true. E.g. topological dimension often does not correlate with the actual sparseness and even if improvements in scaling can be made, this often comes with sources of error that are not accounted for. Similar concerns arise elsewhere too with the technical statements.\n- As mentioned in my initial review, the experimental results seem promising. One drawback that is even more reinforced is the many complications that are added with this methodology: choice of feature constructors, integration of complex numbers, LDL factorizations, etc. I wonder how much of this is really needed. E.g. the authors could compare their resolvent method with simpler low order matrix functions (e.g. Chebyshev polynomials as they state) that do not need many of these complications. \n\n\nOn reflection, my perspective is that I still like the basic idea of the method. Though I am more convinced by the experiments, I think more detailed comparisons and additional analysis would help as mentioned earlier. The largest concern that still remains though is the writing quality. Many changes were made and though I could not review again the paper in full detail, I still was confused by parts and believe the technical quality is still missing at points. If this were a journal setting, I would ask the authors to revise and resubmit, but since that is not an option, I will improve my score to a weak accept and leave the ultimate decision up to the AC."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533911749,
                "cdate": 1700533911749,
                "tmdate": 1700533911749,
                "mdate": 1700533911749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wn8Sfnz0Mb",
                "forum": "yrgQdA5NkI",
                "replyto": "87wgrmi0xN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for iterating on the feedback and helping us improve the quality of the submission.\n\nBased on this feedback, we have further revised the manuscript to use improved notation for the matrix indices. Additionally, we have added a section in the appendix on runtime comparison, which includes timings for the MFN models and the MACE model on the cumulene example. We have also added the following reference in the 'Linear Scaling' paragraph [1], providing a more formal exposition of the statement presented in the text.\n\n[1] T. A. Davis, 'Direct Methods for Sparse Linear Systems'"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656409276,
                "cdate": 1700656409276,
                "tmdate": 1700656409276,
                "mdate": 1700656409276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y9ziJLJwjM",
            "forum": "yrgQdA5NkI",
            "replyto": "yrgQdA5NkI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2444/Reviewer_2GgR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2444/Reviewer_2GgR"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel architecture for processing graph data. The goal of the proposed method is to model non-local interactions between nodes in graphs without the shortcomings of alternate approaches. Namely, over-smoothing and excessive compute requirements for message-passing GNNs and the need for hand-designed features with spectral GNNs.\n\nMatrix function neural networks learn equivariant representations over graphs. The approach consists of three stages: 1) building a matrix that corresponds to a group-equivariant matrix operator 2) applying a learned matrix function to this matrix 3) updating the matrix. To my understanding, the major contributions appear in the second part. The authors make use of resolvent calculus to determine an efficient parameterization of the matrix function. Given that the underlying matrix is sparse, this parameterization admits an efficient (potentially linear) algorithm for computing the function.\n\nThe method is evaluated on three different graph learning datasets, where it generally outperforms the baseline methods (or performs at least as well)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To my knowledge, this is an original and novel approach to designing GNNs. The authors describe theoretical and practical differences between their approach and existing methods. The proposed approach pulls on learnings from several other areas of study (resolvent calculus, electronic structure methods, and more) to design an expressive and efficient class of graph neural networks. The resulting method is elegant and performs well in the empirical evaluation.\n\nOverall, I found the paper to be clear and well-written. I am not familiar with resolvent calculus or some of the other technical background introduced in section 4, but I was able to understand and follow the details. Furthermore, full details of the experiments are provided in the supplementary.\n\nI feel that the contributions in this work are significant. GNNs are a busy area of research and it is no small feat to propose a novel and practical architecture. Empirically, the proposed method performs well. While the margin of improvement is often narrow, MFNs consistently achieve performance at least on par with leading methods in comparison."
                },
                "weaknesses": {
                    "value": "It is unclear whether MFNs can be used as a drop-in replacement for GNNs. In two of the experiments, the models used are not end-to-end matrix function networks, due to model size considerations. Instead, a small number of MFN layers are used with more standard layers following. In all experiments, initial edge features are also extracted from other architectures --- I haven't quite understood the implication of this in terms of comparison to baselines.\n\nThe method is fairly complex with several design decisions. For example, the choice of basis, choice of update method, number of learnable weights and poles, etc. Moreover, making the method computationally tractable requires some techniques that are unfamiliar to the learning community at large. Therefore, I expect that the method may be adopted quite slowly by the community. This could be alleviated with an open-source implementation."
                },
                "questions": {
                    "value": "One area of the paper that I feel I couldn't completely understand was on the introduction of a choice of basis (Section 4.1). It is stated that this choice should be made with consideration of the application domain. On the surface, this seems to align with spectral GNNs that lean on hand-designed features for effective learning. However, the authors point out that this can be approximated well in some cases. I'm curious to know what the limitations here are. When are we unable to automatically learn these functions? And are there any practical considerations for learning these?\n\n\nDo we need to ensure that the learnable weights and poles satisfy any constraints for the resolvent calculus definition (Section 4.2)? From reading the explanation in the paper, I was under the impression that the choice of weights and poles determine the curve implicitly via the quadrature rule. Is it possible that we could learn weights and poles that lead to invalid curves, or otherwise undesirable behaviour?\n\nAs I mentioned above, the MFN models used in experiments always contain only a small number of layers. Was this a computational consideration?\n\nMinor comments:\n\n- There are a couple of symbols that are not defined in the main text. These can be mostly be inferred from context, but should ideally be defined.\n    - Above equation (1), the domain and codomain of $s$ should be defined.\n    - I could not find a definition of $\\mathfrak{Z} z$ and related symbols used in Section 4.3.\n- You cite transformers as having a problematic quadratic scaling. While this is true in the original formulation, it is quite common nowadays to use methods with far more efficient scaling. For example, PerceiverIO can reduce the computational cost to linear.\n- Shaw et al. 2018 are cited for positional encodings, but these are also used in the original Vaswani et al. transformer paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787475476,
            "cdate": 1698787475476,
            "tmdate": 1699636180206,
            "mdate": 1699636180206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ogGZ0a3JQ2",
                "forum": "yrgQdA5NkI",
                "replyto": "y9ziJLJwjM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. We appreciate that you found our work \"novel\" and \"strong empirical evidence\". In the following, we respond to your questions and suggestions to further improve the paper.\n\n### On the basis\n\n> It is stated that this choice should be made with consideration of the application domain. I'm curious to know what the limitations here are. When are we unable to automatically learn these functions? And are there any practical considerations for learning these?\n\nThank you for an excellent question. In the case of pure graphs, enumerating all possible graph operators is NP-hard. The approach we take to do the parameterization in a data-driven way takes the assumption that the model would converge to learn the optimal set of operators for the specific task. There are no limitations in the parameterization except that the matrix needs to be permutation invariant. However, for a graph with N nodes, the entry of the matrix might be an N-body function of all the other nodes. In our experiments, we simply used a multilayer perceptron to learn these entries, but it is likely that other solutions perform better. There are many theoretical open questions related to learning these operators, linked to the cospectrality to a set of spectrums of operators. In the case of geometric graphs (which are embedded in a vector space), the answer can be made more complete. In this case, the entries are \\$G\\$-equivariant functions of the graph. The proof of completeness of equivariant architectures like ACE or MACE for general Lie groups shows that for an infinitely large basis, any of these functions can be uniformly approximated with arbitrary accuracy. \\[1,2\\]\n\n### Conditions on the poles\n\n> Is it possible that we could learn weights and poles that lead to invalid curves, or otherwise undesirable behaviour?\n\nYes, poles that are too close to being purely real lead to divergences. Normalization of the matrix was fundamental to ensure that the optimizer did not explore these points. In practice, finding a good parameterization of the poles was important to making the method work. We parametrize the poles as \\$z_{s} = w_{1}\\exp(w_{2}) + w_{3}\\$, with \\$w_{1}, w_{2}, w_{3}\\$ learnable weights initialized from a standard Gaussian. For the weights, conditions on the norm of the weights would mean targeting a specific space of functions. We are just initializing these weights with a Gaussian centered on zero and a variance one, and let the optimizer move them freely. However, we believe that more clever initialization based on some inductive bias might improve the convergence in applications where one knows the kind of matrix function that gives rise to non-locality (e.g. Fermi Dirac in electronic systems).\n\n### Number of layers\n\n> As I mentioned above, the MFN models used in experiments always contain only a small number of layers. Was this a computational consideration?\n\nFor the pure graph case, it was mostly for restricting the number of parameters and for computation consideration. We intend to implement specialized CUDA kernels for fast inversion as outlined is Section 4.2, that would enable us to scale to larger number of matrices. For the molecular case, we do not see any improvement when increasing the number of layers past 2. This is expected from previous work with many-body methods \\[3\\]. \n\n### Minor comments\n\nThank you for pointing out these small notation issues. We changed to the more conventional $\\text{Imag}$ to denote the imaginary part in Section 4.3. We also have added a reference to PerceiverIO, mentioning that better scaling can be achieved with newer implementations.\n\n\\[1\\] Atomic Cluster Expansion: Completeness, Efficiency and Stability, Dusson, et al\n\n\\[2\\] A General Framework for Equivariant Neural Networks on Reductive Lie Groups, Batatia, et al\n\n\\[3\\] MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields, Batatia, et al"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242020982,
                "cdate": 1700242020982,
                "tmdate": 1700242020982,
                "mdate": 1700242020982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jVBsN92S2d",
                "forum": "yrgQdA5NkI",
                "replyto": "ogGZ0a3JQ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Reviewer_2GgR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Reviewer_2GgR"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's thorough rebuttal. I have read through the other reviews, and your responses there too.\n\nI feel that all of my questions have been adequately answered. I am pleased that several improvements to clarity have been made to the paper.\n\nI am keeping my score, as I believe this to be a meaningful piece of work that provides a novel approach to graph learning and gives sufficient empirical evidence of the effectiveness of the proposed method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477921288,
                "cdate": 1700477921288,
                "tmdate": 1700477921288,
                "mdate": 1700477921288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8vurddXaP7",
            "forum": "yrgQdA5NkI",
            "replyto": "yrgQdA5NkI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2444/Reviewer_qFqk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2444/Reviewer_qFqk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MFN, a graph neural network where the intermediate representation takes a matrix form. The matrix form makes the neural network equivariant for a group action where the representation is unitary. An efficient computation based on the power expansion of the matrix is also proposed. The experiments on the molecule datasets show the competitive performance of MFN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strong empirical results.\n\nMatrix variate formulation and its efficient approximation by the resolvent expansion are novel."
                },
                "weaknesses": {
                    "value": "There is room for improvement in the presentation. I found several incomplete descriptions. For example,\n- Given a matrix $A$, $A^*$ is not defined (I guess it's adjoint of A)\n- $f_\\theta(H)$ is defined as \"scalar to scalar,\" but it seems \"matrix to matrix\". \n- In Eq. (10), since $H$ is a matrix, it's not obvious how $g$ acts on $H$. I guess H is the function of $\\sigma$ and $g$ acts on it, but it's not clearly described. \n- I couldn't find the proof of Eq. (10).\nAlso, see the \"Questions\" area below. \n\nIt is not easy to grasp the computational cost of MFN since it depends on the graph dimension d, the choice of the update equations (11)--(13), etc. A clear comparison is needed. For example, you can evaluate the actual wall clock time and memory usage, which would be clear evidence that MFN is better than other methods, including Transformer."
                },
                "questions": {
                    "value": "In experiments:\n- What is L in the caption of Fig 2?\n- It seems H is constructed with Kronecker products as in Eq. (21). What is the actual size of H?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2444/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837636211,
            "cdate": 1698837636211,
            "tmdate": 1699636180115,
            "mdate": 1699636180115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NbUFEmZHab",
                "forum": "yrgQdA5NkI",
                "replyto": "8vurddXaP7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. We appreciate that you found our work \"novel\" and with \"strong empirical evidence\". In the following, we respond to your questions and suggestions to further improve the paper. We respectfully hope that our responses will be satisfactory and will increase your mark.\n\n### General remark\n> There is room for improvement in the presentation.\n\nWe tried to make the architecture as general as possible. However, we now see that this abstraction made the method quite hard to grasp. We have now added several sections, including examples of the MFN architecture for specific groups, to exemplify the architecture in a concrete setting. We also have rewritten part of Section 4 to clarify the matrix construction step. Finally, we have improved Figure 1 to give a better illustration of the method. We hope that has significantly improved the overall presentation of the method.\n\n### Some details\n\n> Given a matrix \\$A\\$, \\$A^{*}\\$ is not defined (I guess it's adjoint of A).  \n> \\$f_{\\theta}(H)\\$ is defined as \"scalar-to-scalar,\" but it seems \"matrix-to-matrix\".\n\n\\$A^{*}\\$ is indeed the adjoint of \\$A\\$ in the paper. Thank you for spotting these; we have added clearer introductions to all the notation in the paper, including the adjoint. The ambiguous notation of the function \\$f\\$ is to highlight that any real value function can be \"overloaded\" to a symmetric matrix value function (matrix to matrix) by simply applying it point-wise to its eigenvalues. An alternative view if \\$f\\$ is analytic is to consider the formal power series expansion of the function \\$f\\$, and formally replace the scalar powers to matrix powers. In this context, it is widely accepted to use the same symbol for the real value map and the matrix value map (see, for example, the Wikipedia page on matrix functions [https://en.wikipedia.org/wiki/Analytic_function_of_a_matrix]()).\n\n### Action of \\$G\\$ on the matrix \\$\\textbf{H}\\$\n> In Eq. (10), since \\$\\textbf{H}\\$ is a matrix, it's not obvious how \\$g\\$ acts on. I guess H is the function of and acts on it, but it's not clearly described.\n\nYes, \\$g\\$ is a group action that acts on the state of each of the nodes. At the matrix level, the action is given by Eq. 9, since the functions \\$\\phi\\$ are assumed to be equivariant. The action of \\$G\\$ propagates from the actions in the states to the action in the matrix. A good example is for 3D graphs. In the case \\$G=O(3)\\$ the group of 3D rotations and the element \\$g\\$ are rotation matrices \\$R\\$. The state of each node would be a 3D vector of positions \\$(x, y, z)\\$. Rotations can be applied to the graph by rotating each position \\$(x, y, z) \\to R (x, y, z)\\$. The function \\$\\phi\\$ being equivariant will result in matrix entries that are also equivariant. Therefore, if a rotation is applied to the whole graph, transforming by a rotation \\$R\\$, then there exists an orthogonal matrix \\$D(R)\\$ such that the graph operator \\$H_{1}\\$ of \\$G_{1}\\$ is transformed into \\$H_{2} = D(R) H_{1} D(R)^{T}\\$.\n\n### Proof of equivariance of matrix function \\$H\\$\n> I couldn't find the proof of Eq. (10). \n\nWe have written a detailed proof of the equivariance of generic continuous matrix functions in the appendix and reference it in the main text. Thank you for noticing that.\n\n### Computation cost of MFN\n> It is not easy to grasp the computational cost of MFN since it depends on the graph dimension [...] For example, you can evaluate the actual wall clock time and memory usage, which would be clear evidence that MFN is better than other methods, including Transformer.\n\nWe agree that computational cost is a key parameter in any method. As explained in Section 4.2 in the Linear Scaling Paragraph, the prototype implementation has a cubic scaling and is therefore quite slow compared to other approaches (a factor of x2 to x5 compared to local models). However, fast inversion methods are well established, and we explain how this scaling can be reduced significantly to less than quadratic scaling, all the way to linear scaling, by exploiting the sparsity of the matrices we construct and selected inversion methods. \n\nAlthough these methods are known, they require significant effort to implement and integrate them into machine learning libraries like Pytorch due to the need for specialized CUDA kernels. We intend to make this effort and release open-source code in future work, but we believe that this is beyond the scope of this paper, where we focus on the novelty of the architecture and its expressivity. \n\nFinally, we stress that one of the key results of this paper is that MFN can model systems that Transformers cannot model. Due to the unstructed non-locality of global attention, Transformers can not extrapolate to large systems in the Cumulene example. This means that regardless of speed, there are applications in which MFNs are clearly the only solution to date. We made changes to the contributions to better highlight this key point."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241124999,
                "cdate": 1700241124999,
                "tmdate": 1700242934954,
                "mdate": 1700242934954,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HzLIzeg757",
                "forum": "yrgQdA5NkI",
                "replyto": "8vurddXaP7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2444/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Construction of \\$H\\$ in the \\$O(3)\\$ group\n> What is L in the caption of Fig 2?  \n> It seems H is constructed with Kronecker products as in Eq. (21). What is the actual size of H?\n\nThe \\$L\\$ index corresponds to the maximal spherical order in the basis expansion of the matrix for the spherical case. In order to make this notation clearer and improve the presentation of the paper, we have added a section to detail the implementation of the matrix construction case in the \\$O(3)\\$ group. We hope that this will significantly help the readers understand the method. \\$H\\$ is a tensor of size \\[\\$N_{\\text{nodes}} \\times (L+1)^{2}, N_{\\text{nodes}}\\times(L+1)^{2}, N_{\\text{channels}}\\$], where \\$N_{\\text{nodes}}\\$ is the number of nodes in the graph, and \\$N_{\\text{channels}}\\$ is a hyper-parameter corresponding to the number of operators we are learning."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2444/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241271704,
                "cdate": 1700241271704,
                "tmdate": 1700241271704,
                "mdate": 1700241271704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]