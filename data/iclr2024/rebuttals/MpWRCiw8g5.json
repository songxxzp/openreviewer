[
    {
        "title": "JOSENet: A Joint Stream Embedding Network for Violence Detection in Surveillance Videos"
    },
    {
        "review": {
            "id": "00HPAwJYlu",
            "forum": "MpWRCiw8g5",
            "replyto": "MpWRCiw8g5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_sND9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_sND9"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces JOSENet, a novel framework designed for violence detection in surveillance videos. It aims to tackle the challenges of real-world surveillance, such as varying scenes, actors, and the need for real-time detection. The framework consists of a primary target model and an auxiliary self-supervised learning (SSL) model. It uses multiple datasets for training and validation, applies various preprocessing and data augmentation strategies, and evaluates the model using a comprehensive set of metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "**Originality**\n\nThe paper is innovative in proposing a dual-model architecture, involving a primary target model and an auxiliary SSL model. It also introduces a new SSL algorithm based on VICReg and a novel data augmentation strategy called \"zoom crop.\"\n\n**Quality**\n\nThe research is thorough, with detailed experimental settings, multiple datasets, and a diverse set of evaluation metrics. The use of an auxiliary SSL model to achieve a balance between performance and computational resources is commendable.\n\n**Clarity**\n\nThe paper is well-structured and clear, with each section contributing to the reader's understanding of the proposed framework.\n\n**Significance**\n\nThe work addresses a vital real-world problem, that of violence detection in surveillance videos, and proposes a framework that seems both effective and efficient."
                },
                "weaknesses": {
                    "value": "Lack of Details: Some sections could provide more implementation details, especially on how the VICReg loss and weight optimization between the two models are implemented.\n\nDataset Limitations: While multiple datasets are used, they are mostly centered around violence detection, which could limit the model's generalizability across domains.\n\nRobustness: The paper does not address how the model handles potential issues like occlusion, varying light conditions, or camera angles, which are common in real-world surveillance.\n\nHyperparameter Tuning: The paper doesn't discuss the process or criteria for hyperparameter selection, which could affect the model's performance."
                },
                "questions": {
                    "value": "1.\tCould you provide more details on the \"zoom crop\" data augmentation strategy, specifically its effectiveness and efficiency?\n\n2.\tWhy were these particular datasets chosen, and have you considered using more diverse datasets to improve the model's generalizability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Reviewer_sND9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697794896805,
            "cdate": 1697794896805,
            "tmdate": 1699636591071,
            "mdate": 1699636591071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yx0X7yCDI8",
                "forum": "MpWRCiw8g5",
                "replyto": "00HPAwJYlu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the lack of details."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for recognizing the value of the paper, we did our best also to address the remaining concerns.\nWe checked the paper carefully, all the details related to VICReg and optimization are included, and specifically we have also added some further implementation details in the appendix (section A.3.4 \u201cVICReg pretraining additional details\u201d)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639715937,
                "cdate": 1700639715937,
                "tmdate": 1700639715937,
                "mdate": 1700639715937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MmGaHM9OLs",
                "forum": "MpWRCiw8g5",
                "replyto": "00HPAwJYlu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the hyperparameter tuning."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for highlighting this point. We added a new section A.2.2 in the appendix called \u201cHyperparameters Tuning\u201d together with Table 10 which offers an overview of the hyperparameter selection."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639773830,
                "cdate": 1700639773830,
                "tmdate": 1700639811051,
                "mdate": 1700639811051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7GWbfzAnc7",
            "forum": "MpWRCiw8g5",
            "replyto": "MpWRCiw8g5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes an approach for performing the video task of violence detection in surveillance videos by employing a self-supervised learning network to help improve the primary supervised model. The core network to perform the primary task is based on flow gated network (FGN), by Cheng et al (2021). The semi-supervised learning block applies VICReg approach, by Bardes et al. (2021), to the two streams of RGB and optical flow. The results are reported on three datasets related to activity recognition with the comparison with multiple SOTA approaches and an ablation study."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper describes an interesting idea that can leverage the strengths of semi-supervised learning in the domain of violence detection in surveillance videos where the rarity of the events poses challenges for obtaining a large volume of positive training samples and the need for a low false alarm rate. The proposed approach also has some interesting nuggets related to computational efficiency and reduced memory footprint. They have also studied the tradeoff between the size of the temporal window, framerate, and quality of results."
                },
                "weaknesses": {
                    "value": "The problem, application, and the core part of the solution (FGN) is not new. However, the addition of SSL \n\nThe baseline model from `Sec. 4.2` should have been reported in the tabular form for a more effective presentation of material and instead of explaining the numerical differences in the narrative form as done in `Sec. 4.2` and other sections. It should be clear from ONE table the various variants, baselines, and the final version. Additionally, it is hard to follow this paper at times because the different tables are reporting results on different datasets. Are the results in this section reported on the exact same test data as that in Table 3? If so, then should we be comparing $F_1$ of $85.87$ (baseline) with $86.5$ (JOSENet)? i.e. improvement of $0.63$? Is it also fair to say that the baseline approach is very close to FGN, by Cheng et al (2021)? \n\nThe main result comparing JOSENet with SOTA in Table 3 has aspects that are not clear. I assumed this statement\n`We decide to take as reference the results obtained on the 15% subset of UCF-101 with JOSENet.` \nmeant that Table 3 results are on UCF101 but then `a pretraining obtained on a random 15% subset of UCF-101` suggests that it was used for pretraining. Is it a different 15%? More importantly, UCF101 does NOT have violent activities in surveillance scenes (to the best of my knowledge) in the way it has been portrayed in the motivation described in the paper. There are activities like Punch or Boxing Punching Bag, but not much else. Additionally, why stick with some *random* 15% split of UCF101 instead of using the standard test split that could be compared with the SOTA. \n\nThe writing quality of the paper can be improved significantly. There are several grammatical mistakes, a few long run-on sentences, unusual usage of some phrases, and confusing or inconsistent usage of citations that break the flow."
                },
                "questions": {
                    "value": "1. It was surprising that results were not reported explicitly on the RWF-2000 dataset in the `4. Experimental Results`, as far as I could tell. In my opinion, it is unusual to make statements like this:\n`pg 8: We have noticed that we do not reach the state-of-the-art performances for RWF-2000.`\nand not provide the quantified numbers. The other statement (`To train and validate the model during supervised learning we use the RWF-2000 dataset`) was also noted. \n\n2. Is there a reason why Table 3 does not have a row with a comparison with FGN, by Cheng et al (2021)?\n\n3. Table 3, AUC column has numbers in [0,100] and [0,1.0] ranges. Are those just typos? \n\n4. Table 5, the use of temporal pooling is not clear as it makes things worse as reported by the scores. The explanation in `Sec. 5` is unclear. The table does not support this claim (if I am following it as intended):\n`To find a confirmation of this approach, using the zoom crop strategy, we apply the temporal pooling\nin the merging block, obtaining on the target task a very low value for most of the evaluation metrics\nused.`\n\n5. pg: 2, FGN was not defined or cited until pg 3 so it was confusing.\n\n6. pg: 2, should `contrastive learning (CT)` be `contrastive learning (CL)` ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625056499,
            "cdate": 1698625056499,
            "tmdate": 1699636590966,
            "mdate": 1699636590966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1QHZ1vp0B4",
                "forum": "MpWRCiw8g5",
                "replyto": "7GWbfzAnc7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the novelty."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for this comment, although probably the sentence is not complete. \n\nThe fact that the problem and the application are not new is clear from the existing literature. Indeed, violence detection is one of the most critical and challenging action recognition problems in surveillance videos and it is still a matter of research, as also highlighted in the related work section. However, we firmly believe that an open problem is not a problem for research, nor even a weakness of a single paper.\n\n\nWith respect to the core part of the solution, notably, our primary model draws inspiration from the two-stream flow gated network (FGN) proposed by Cheng et al. (2021), as detailed in Sec. 3.1. However, although we maintain the original name \u201cFGN\u201d to denote that network, our scheme shows some significant changes with respect to the original model, specifically geared towards enhancing the efficiency and effectiveness of neural network design in practical scenarios. \n\nBesides proposing a new and efficient version of the FGN, the paper also includes a modified VICReg strategy tailored to violence detection for mitigating performance degradation while improving efficiency. Even for this part, it is essential to highlight that, despite employing the same VICReg loss function, our approach diverges significantly from the original paper in terms of input type, augmentation strategies, and architectural modifications. These distinctions were deliberately introduced to align with the overarching objective of our work: minimizing both memory and computational costs while maintaining high results on the violence detection task.\n\nTo summarize, the methodological contribution of the papers is characterized by:\n1. A new efficient version of the two-stream flow gated network specifically designed for action recognition tasks.\n2. A VICReg approach for video streams relying on the joint information of two different video modalities, specifically the RGB and flow batches that are augmented using a novel \u201czoom crop\u201d strategy.\n3. A novel self-supervised learning architecture, involving the networks described in the above points, for violence detection.\n\nWe have modified the paper to highlight the methodological contributions of the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639366991,
                "cdate": 1700639366991,
                "tmdate": 1700639366991,
                "mdate": 1700639366991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pIoc4tOsxD",
                "forum": "MpWRCiw8g5",
                "replyto": "7GWbfzAnc7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the use of UCF101."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for this comment. We explain our point and we have also clarified it in the paper.\nAs explained in the paper, all reported results are derived from evaluating the target task on the RWF-2000 validation set, with the 15% subset consistently employed across all experiments.\nThe decision to utilize the 15% subset for UCF101 in the pretraining phase was motivated by two key considerations:\n1) Firstly, it was implemented to expedite the pretraining phase, thereby achieving the optimal configuration in a more time-efficient manner. \n2) Secondly, the choice aimed at maintaining a balanced number of data points across all pretraining datasets (HMDB51, UCF101, and UCF-Crime). \n\nTo ensure a fair comparison, especially between HMDB51 and the other datasets, while still adhering to practical training time constraints (approximately 12 hours for each pretraining on the HMDB51 dataset), a portion of UCF101 and UCF-Crime was pretrained. This involved random sampling of 15% of each of these datasets, resulting in a more equitable and feasible training process."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639423113,
                "cdate": 1700639423113,
                "tmdate": 1700639438902,
                "mdate": 1700639438902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iYoQL8zvc9",
                "forum": "MpWRCiw8g5",
                "replyto": "7GWbfzAnc7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the comparison with FGN."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for this comment. Actually we included this comparison but in a fair way.\nWe refrained from conducting a direct comparison between JOSENet (our framework) and the FGN (Cheng et al. 2021) because they are based on different learning strategies. The reason lies in the utilization of the SSL pretraining phase with additional data in JOSENet, introducing a significant divergence in the training methodology between the two approaches. This distinction in training strategies makes a direct comparison challenging and may not accurately reflect the intrinsic capabilities of each model on a level playing field. \n\nHowever, we have included a comparison with the baseline model, which is actually nothing but the FGN with the same learning conditions considered for JOSENet in order to have a fair comparison."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639570806,
                "cdate": 1700639570806,
                "tmdate": 1700639927177,
                "mdate": 1700639927177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7eGriZBB9N",
                "forum": "MpWRCiw8g5",
                "replyto": "1QHZ1vp0B4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Reviewer_tKHs"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response and the paper update. Given the late rebuttal close to the end of the rebuttal period, I have not been able to read the revised paper. However, I have reviewed the rebuttal and I am not convinced that the changes are sufficient to warrant for me to change the rating to accept."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718531841,
                "cdate": 1700718531841,
                "tmdate": 1700718531841,
                "mdate": 1700718531841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EhGA6Vt3Op",
            "forum": "MpWRCiw8g5",
            "replyto": "MpWRCiw8g5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_o7vs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_o7vs"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces JOSENet, a network for video violence detection. It contains a pretraining part and a detection part. Given the RGB and Flow inputs, a two-stream flow gated network (FGN) is firstly pretrained on UCF-101, HMDB-51 and UCF-Crime datasets using VICReg method. Then, the pretrained FGN weights are used to initialize the FGN in the detection part. In this way, the model requires less training data and generalizes better. In addition, some optimization of the network improves the efficiency of the model in terms of memory consumption and computation load. The proposed method is evaluated on RWF-2000 dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The overall idea is easy to understand and makes sense.\n2) By efficient implementation, the model requires less memory and less frames for each segment.\n3) The model leverages self-supervised learning to improve the generalization of the model."
                },
                "weaknesses": {
                    "value": "1) The goal of the paper is violence detection, but there is no related contents in the method part. Necessary components such as loss function of violence detection should be included. \n2) The proposed \u201ccomputational enhancement\u201d is just hyper-parameter tuning. N_s=7.5s is found to be the optimal. However, different datasets may have different optimal parameters. More justification are needed to demonstrate the generalization performance. \n3) The theoretical contribution is limited. The pretraining part is borrowed from VICReg and the detector is borrowed from FGN. \n4) The proposed method is only evaluated on RWF-2000 dataset, which is not enough. I suggest authors to include results on more datasets since you claim the proposed method generalizes better. \n5) Missing comparison with recent methods such as:\n[1] Islam, Zahidul, et al. \"Efficient two-stream network for violence detection using separable convolutional lstm.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021.\n[2] Garcia-Cobo, Guillermo, and Juan C. SanMiguel. \"Human skeletons and change detection for efficient violence detection in surveillance videos.\" Computer Vision and Image Understanding 233 (2023): 103739.\n6) Compared with other methods, the proposed method uses addition training data (UCF-101, HMDB-51, and UCF-Crime). This may be a concern the comparison is not fair.   \n7) The related work of violence detection is incomplete, it should contains more recent methods and discussion. \n8) To demonstrate the efficiency, a comparison with other methods should be included."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5668/Reviewer_o7vs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631030270,
            "cdate": 1698631030270,
            "tmdate": 1699636590872,
            "mdate": 1699636590872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "glci4BQJKR",
                "forum": "MpWRCiw8g5",
                "replyto": "EhGA6Vt3Op",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the content related to the violence detection task."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for this comment. We probably did not highlight this point too much, but actually the whole framework has been designed for violent action detection. The choice of the two-stream network is motivated by its effectiveness in violence detection tasks, as proved in Cheng et al. (2021). The spatial and temporal blocks are fundamental to detect such kind of actions. Then, as the model relies on the two different modalities of the video stream, we decided to apply a modified VICReg self-supervised learning method to further regularize the solution and improve the performance. \n\nWith respect to the loss function we adopt a binary cross-entropy loss which is the state-of-the-art in violent detection tasks (see Cheng et al. (2021)), as outlined in Section 4.2, \"Experimental Results Without Pretraining\". We have further highlighted the methodological relation with the application task in the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638712501,
                "cdate": 1700638712501,
                "tmdate": 1700640067701,
                "mdate": 1700640067701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "otjv9zS72Q",
                "forum": "MpWRCiw8g5",
                "replyto": "EhGA6Vt3Op",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the performance generalization."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for highlighting this point. The goal of the paper is not to achieve state-of-the-art results for violence detection, but rather to propose a novel approach for efficient detection with reduced fps and data and involving self-supervised learning. Our experiments were focused on proving this goal.\nComparing our paper with the above mentioned methods can be done of course as shown in the table below.\n\n\n| **Method** | **Number of Frames** | **FPS** | **Temporal Footprint** | **Parameters** | **Accuracy** | **TNR** | **TPR** | **F1-Score** |  **AUC**  |\n|:----------:|:--------------------:|:-------:|:----------------------:|:--------------:|:------------:|:-------:|:-------:|:------------:|:---------:|\n|     [1]    |          32          |   **6.4**   |           5 s          |     333,057    |    89.75%    |    -    |    -    |       -      |     -     |\n|     [2]    |          50          |    10   |           5 s          |   **62,583**   |  **90.25%**  |    -    |    -    |       -      |     -     |\n|    Ours    |        **16**        |  7.5  |       **2.13 s**       |     272,690    |     86.5%    | **88%** | **85%** |   **86.5%**  | **0.924** |\n\n\nHowever, the comparison cannot be considered as fair as the models are designed, trained and assessed on different conditions (fps, temporal footprints, percentage of trained data, etc.). Moreover, the other methods cited in the comparison did not disclose additional metrics beyond accuracy. We posit that a comprehensive evaluation of a violence detection method necessitates the incorporation of additional metrics such as True Negative Rate (TNR), True Positive Rate (TPR), Area Under the Curve (AUC), and F1-Score. These metrics contribute to a more nuanced and holistic assessment of the method's performance, providing insights beyond accuracy alone. In order to assess a fair comparison, the referenced methods should be modified according to the proposed framework and then tested under the same conditions, but their original valence should be of course modified from the original papers. This point can be definitely investigated in future work, as we have pointed out in the revised paper.\n\nGiven that other violence detection methods should be modified to be fairly compared with our method, we already included some comparisons with state-of-the-art methods that fairly evaluate the goodness of our self-supervised learning strategy. In future work, some state-of-the-art methods for violence detection could be redefined according to the proposed framework and then fairly compared. However, the redefinition would push the methods far from their original versions. \n\nWe added some comments all along the paper, to clarify these points in the text."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639292410,
                "cdate": 1700639292410,
                "tmdate": 1700643221935,
                "mdate": 1700643221935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V8I3j2tdsk",
            "forum": "MpWRCiw8g5",
            "replyto": "MpWRCiw8g5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_94ds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5668/Reviewer_94ds"
            ],
            "content": {
                "summary": {
                    "value": "Paper proposes a novel violence detection framework which combines 2 features, 2 spatiotemporal streams (RBG + optical flow) and self-supervised learning (SSL).\n\nThe design is more efficient in memory usage (75%) and inference speed (2-fold). For the SSL, the paper adopts the VICReg which is more memory efficient.\n\nEmpirical experiments were done with RWF-2000, HMDB51, UCF101 and UCFCrime. The proposed framework was compared against the SOTA SSL methods: InfoNCE, UberNCE and CoCLR for the UCF101 dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Paper's proposed method is more efficient in memory and inference speed compared to the original baseline methods.\n2. The motivation for the design is well explained."
                },
                "weaknesses": {
                    "value": "1. Novelty is highly limited. The combination of optical flow with RGB has been used in multiple prior work. See references.\nThe novelty of SSL is also limited as it is a direct implementation of VICReg.\n\n2. Experimental design is confusing and does not directly support the core claim of the paper. Only SSL-based SOTA algorithms were directly compared with the proposed method for one single dataset (UCF101). There were several experiments on JoseNet based methods. But these experiments are not relevant to demonstrate the core claim of the paper \"outstanding performance for violence detection\" against other SOTA methods.\n\n3. (minor) Writing style is informal and not well-structured. This is especially for the experiment section. E.g. \"We\nhave noticed that we do not reach the state-of-the-art performances for RWF-2000. However, this is not a big deal in a deployment application.\". There is no reference to which experiment this statement refers to (which Table).\n\nReferences\n\nDiba, A., Pazandeh, A. M., & Van Gool, L. (2016). Efficient two-stream motion and appearance 3d cnns for video classification. arXiv preprint arXiv:1608.08851.\n\nWang, G., Muhammad, A., Liu, C., Du, L., & Li, D. (2021). Automatic recognition of fish behavior with a fusion of RGB and optical flow data based on deep learning. Animals, 11(10), 2774.\n\nLi, S., Zhang, L., & Diao, X. (2020). Deep-learning-based human intention prediction using RGB images and optical flow. Journal of Intelligent & Robotic Systems, 97, 95-107."
                },
                "questions": {
                    "value": "Why is the comparison against SOTA limited to SSL methods for a single UCF101? This is insufficient to show the generalization of the claim of superior performance of the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834663110,
            "cdate": 1698834663110,
            "tmdate": 1699636590782,
            "mdate": 1699636590782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbMEkZUSLL",
                "forum": "MpWRCiw8g5",
                "replyto": "V8I3j2tdsk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the novelty."
                    },
                    "comment": {
                        "value": "We thank the Reviewer for her/his comment. \nOur intention is not to assert novelty in the combination of optical flow with RGB nor in the VICReg as well.\n\nWe are completely aware that the combination of optical flow with RGB has been largely used in prior work. Indeed, in Sec. 2 \"Related Work - Violence Detection,\" we have explored various methodologies incorporating both optical flow and RGB. Notably, our primary model draws inspiration from the two-stream flow gated network (FGN) proposed by Cheng et al. (2021), as detailed in Sec. 3.1. However, although we maintain the original name \u201cFGN\u201d to denote that combination, our scheme shows some significant changes with respect to the original model, specifically geared towards enhancing the efficiency and effectiveness of neural network design in practical scenarios. \n\nThe adoption of VICReg in our implementation serves a crucial purpose: mitigating performance degradation while improving efficiency. It is essential to highlight that, despite employing the same VICReg loss function, our approach diverges significantly from the original paper in terms of input type, augmentation strategies, and architectural modifications. These distinctions were deliberately introduced to align with the overarching objective of our work: minimizing both memory and computational costs while maintaining high results on the violence detection task.\n\nTo summarize, the methodological contribution of the papers is characterized by:\n1. A new efficient version of the two-stream flow gated network specifically designed for action recognition tasks.\n2. A VICReg approach for video streams relying on the joint information of two different video modalities, specifically the RGB and flow batches that are augmented using a novel \u201czoom crop\u201d strategy.\n3. A novel self-supervised learning architecture, involving the networks described in the above points, for violence detection.\n\nWe have modified the paper to highlight the methodological contributions of the paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638445130,
                "cdate": 1700638445130,
                "tmdate": 1700638445130,
                "mdate": 1700638445130,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F3OcrwOcGq",
                "forum": "MpWRCiw8g5",
                "replyto": "V8I3j2tdsk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the goal of the experimental validation."
                    },
                    "comment": {
                        "value": "We would like to thank the Reviewer for her/his comment on the experimental part. \nIt is essential to clarify that our primary objective was not to achieve state-of-the-art performance on violence detection; rather, our focus centered on proposing an approach that can be versatile, robust, and suitable for real-world video surveillance applications. Consequently, our emphasis was on enhancing speed, and reducing memory consumption, while maintaining results comparable to the main methodology. \n\nFor instance, the original FGN architecture proposed by Cheng et al. (2021) attained an accuracy of 87.25%, whereas our model achieved an accuracy of 86.5%. However, it is important to note that a fair comparison between the two methods is challenging. As stated in Sec. 4.3 \u201cFinal Results for JOSENet\u201d, in contrast to Cheng et al. (2021), we have developed a more efficient and faster solution through a four-time reduction in segment length (16 instead of 64) and a lower required frames per second (7.5 instead of 12.8). Consequently, we firmly believe that our model strikes an excellent compromise between performance and efficiency."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638489364,
                "cdate": 1700638489364,
                "tmdate": 1700640087573,
                "mdate": 1700640087573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]