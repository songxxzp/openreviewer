[
    {
        "title": "End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon"
    },
    {
        "review": {
            "id": "UuabeStU7O",
            "forum": "cphhnHjCvC",
            "replyto": "cphhnHjCvC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission326/Reviewer_1KiF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission326/Reviewer_1KiF"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors introduce pretext tasks and a dual visual encoder for ImageNav and Instance-ImageNav navigation, offering rich geometric information and enabling the resolution of the challenging mono-view scenario with end-to-end trained techniques. \n\nThe method breaks down the problem into multiple training stages, demonstrating the emergence of solutions to the correspondence problem without explicit supervision. \n\nThrough experiments, the manuscript highlights the effectiveness of the proposed pretext tasks and a dedicated dual encoder architecture, surpassing competing methods and achieving  a novel state-of-the-art performance on both benchmarks. Additionally, the authors demonstrate seamless integration into a modular navigation pipeline of the proposed approach."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- In the state of the art (Section 2), a detailed review of all the literature related to the problem investigated in this work is conducted. Furthermore, the main differences of the proposed model with respect to previous works are outlined.\n\n- The system proposed in Section 2 is novel. It describes an integration of subsystems/submodules that allows for an approximation to the problem, yielding highly promising results. Additionally, the proposed design can be considered innovative. The integration of Cross-View Completion (CroCo) into a navigation system as described has not been explored before, although it's true that this contribution is not the strongest of all. The second pretext task, relative pose estimation and visibility (RPEV) for navigation settings, is interesting and provides significant results. Overall, the originality of the work is considerable.\n\n- The experimental evaluation provides very interesting results in which the proposed model outperforms the state of the art (see tables 4 and 5). An ablation study is provided where the real impact of each of the contributions or parts of the model can be interpreted. Finally, qualitative evidence is presented on how the proposed pre-training process results in the emergence of correspondences between images by analyzing the attention of the last layers."
                },
                "weaknesses": {
                    "value": "The experimental evaluation included in the paper has some minor limitations that should be addressed:\n- It is not clear on which databases or environments the system has been tested. For example, on what environments are the results reflected in Table 5 obtained? The interested reader has to review (Krantz et al., 2023) to know these details. Section 4, \"Experimental setup\" subsection needs to be improved.\n\n- I believe one of the most interesting contributions of the proposed model is that it generates correspondences between images due to how the pre-training is designed. Figure 4 shows some results or qualitative evidence in this regard. However, the paper does not explain in detail how this image is generated or how these correspondences are analyzed. The manuscript simply mentions that they \"visualize averaged attention of the last cross-attention layer of a DEBiT-L model\", but more details should be provided."
                },
                "questions": {
                    "value": "Overall, I see an strong paper here.\nI would simply suggest to the authors that they provide explanations for the limitations I have described in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Authors already provide a convincing ethics discussion."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission326/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698660822890,
            "cdate": 1698660822890,
            "tmdate": 1699635959552,
            "mdate": 1699635959552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uZ6nciaUWJ",
                "forum": "cphhnHjCvC",
                "replyto": "UuabeStU7O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review.\n\n> It is not clear on which databases or environments the system has been tested. For example, on what environments are the results reflected in Table 5 obtained? The interested reader has to review (Krantz et al., 2023) to know these details. \n\nInstance Imagenav task has been trained and tested on the Habitat-Matterport3D (HM3D) Semantics v0.2 2 dataset that was provided for the Habitat Navigation Challenge 2023. The dataset contains 216 different scenes; the train/val/test split is 145/36/35. Our model was trained on the 145 training scenes, we reported results for the 36 validation scenes. The remaining 35 test scenes are used by organizers internally; they are not publicly available. \n\nThis information has been added to the appendix of the paper.\n\n> I believe one of the most interesting contributions of the proposed model is that it generates correspondences between images due to how the pre-training is designed. Figure 4 shows some results or qualitative evidence in this regard. However, the paper does not explain in detail how this image is generated or how these correspondences are analyzed. The manuscript simply mentions that they \"visualize averaged attention of the last cross-attention layer of a DEBiT-L model\", but more details should be provided.\n\nThe visualization is simple:  we have examined the last cross-attention layers, with attention summed over all heads, and looked at individual attention values. We have then picked the highest N attention values in this matrix and displayed their corresponding query-key pairs, i.e., drawing a line between the corresponding patches.\n\nThis has been added to the paper and to the video."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission326/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580475907,
                "cdate": 1700580475907,
                "tmdate": 1700580475907,
                "mdate": 1700580475907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rmqWVdwpNn",
            "forum": "cphhnHjCvC",
            "replyto": "cphhnHjCvC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission326/Reviewer_isxL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission326/Reviewer_isxL"
            ],
            "content": {
                "summary": {
                    "value": "This paper improves the view correspondence encoding in image-goal navigation by pre-training a large binocular ViT model (DEBiT) with cross-view completion (CroCo) and relative pose estimation and visibility prediction (RPEV) from images in photorealistic indoor scenes. Specifically, the dual encoder in DEBiT takes binocular images as input, and the encoded features are merged with a decoder to model the correspondence between the two images. DEBiT is pre-trained with CroCo, followed by RPEV to estimate the relative distance, relative rotation, and overlap between two images. When addressing downstream Instance-ImageNav and ImageNav tasks, DEBiT takes the goal image and agent's observation as inputs, its network is adapted in tuning, and the output representations will be passed to the policy network for decision-making. Results show substantial improvement compared to previous approaches."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies an important problem in visual navigation: learning high-capacity perception modules for modeling and reasoning view correspondence. It proposes DEBiT, which implements a dual encoder to enable early fusion of the images. It introduces CroCo and RPEV pre-training, which are highly relevant and have shown to be effective in addressing the problem. \n- Significant improvement is achieved compared to previous approaches, boosting the ImageNav and Instance-ImageNav results to 94% SR and 59.3% SR, respectively. The methods introduced in this paper and the resulting pre-trained models are very likely to inspire/to be used by future research in relevant fields.\n- The paper is technically sound; many important arguments and design choices are justified by experiments. Besides the key results, there are many highly constructive analyses/findings, such as \"directly training binocular encoder from scratch\", \"tuning visual encoder with downstream policy network\", \"early fusion vs. late fusion\", \"attention with scale changes\", etc., that are very valuable to future research.\n- Overall, this paper is nicely written; it is very compact, informative, and clear. All methods (and most of the implementation), visualizations, and discussions are clearly presented."
                },
                "weaknesses": {
                    "value": "- The proposed DEBiT model seems to be limited in addressing image-goal navigation tasks (the image-nav task itself needs more justification), and it is unclear how it might benefit other visual navigation problems.\n    - However, I do believe that CroCo and RPEV can help in learning a general (and better) navigation-specific perception model (e.g., for obj-Nav, language-guided-Nav, Audio-Nav, etc.), while I am concerned that it might be much less effective compared to ImageNav which is defined to take two images as input.\n    \n- I am aware that ImageNav is an interesting visual navigation problem and has gained some research attention (publications), but I am still not convinced by its setting, especially how much practical value it might bring to real-world applications. [This question might be more appropriate for researchers who proposed ImageNav, but since this paper is devoted to ImageNav, I believe it must have a very strong reason and motivation behind.]\n    - The two ImageNav tasks consider indoor short-range navigation for finding static objects (Instance-ImageNav has paths with an avg. geodesic length of 12.41m); from the user's perspective (e.g., a household robot), giving instruction in the form of a specific image is unnatural.\n    - From the ImageNav data statistics and the visualization provided in this paper (supp. video), it seems that most of the paths have very few intersections, and the agent doesn't really need to explore the environment but keeps moving to new regions until the target is in its sight. This could be the reason why a very simple policy network, without methods like SLAM, can still lead to amazingly high performance (94% SR). I am concerned that the ImageNav task itself oversimplifies the problem, and this paper might overclaim the contribution to visual navigation research.\n\n- Some experiments are missing to justify arguments (see Questions below).\n\n- The limitation and future extension of this work are not discussed in this paper."
                },
                "questions": {
                    "value": "Questions without a star (*) are not critical to my evaluation of this paper, but I still hope the authors can kindly and briefly respond to them. Please also respond to my concerns in Weaknesses.\n\n- (*) This paper mentioned depth inputs, but I wonder why depth images are not applied in the model since it might greatly facilitate learning view correspondence and identifying space and obstacles. (I might have overlooked some details; please correct me if I did.)\n\n- (*) How does the choice of $\\tau$ influence the pre-training and the downstream results? If two images are too distant away and have little overlap, will it be too noisy to learn? Or it might help the visual encoder to learn distant image correspondence and benefit exploration in navigation? Any numerical analysis on this?\n\n- (*) The paper claims the benefit of having visibility estimation but does not quantify its impact on downstream tasks. Any results for this?\n\n- Just curious, can CroCo and RPEV be trained simultaneously? \n\n- In Table 2, from Tiny to Large, the model size increases drastically, but the difference in navigation results is quite small; what might be the reason? Does it mean that a very large perception model might not be necessary? What would be the result of DEBiT-Tiny + Adapters?\n\n- Instance-ImageNav depicts targets viewed from a different camera; if this is the main reason why the proposed method gets much less improvement compared to ImageNav (an OOD situation as claimed), I wonder what if the images for pre-training are augmented with different camera parameters, (1) Is it still feasible to learn CroCo and RPEV? (2) Will it reduce the visual domain gap in downstream?\n\n- Are the visual encoders in OVRLs fine-tuned with the policy networks in downstream tasks? From their papers and Table 4, it seems Yes. But in the section Related Work, \"Once pre-trained, the encoder is often frozen before passing into a policy learning module.\"\n\n- About comparison experiments.\n    - Using adapters in the perception model largely improves the results. I wonder how much improvement it might bring to the previous approaches.\n    - In Table 3, for a fair comparison (in terms of pre-training tasks and #params), I think DEBiT-Tiny and RPEV-only should be listed, which I believe is more rigorous and can prove the same argument.\n    - (*) In Table 4, an important point to mention is the size of the visual encoders (#params) applied in each work. After scanning some papers in the Table, I believe there is a clear trend of larger-models-better-results. But it seems that DEBiT is relatively efficient, especially compared to OVRL-v2. I hope the authors can clarify this point, which I believe will also strengthen the argument.\n\nOther Suggestions:\n- Remove \"(Instance)-\" from the title.\n- I found a paper, \"Learning navigational visual representations with semantic map supervision (Hong et al., ICCV2023)\", which also focuses on learning better navigation visual encoder with view correspondence and uses two images for pre-training. It seems relevant and could be added to the references."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission326/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721781721,
            "cdate": 1698721781721,
            "tmdate": 1699635959477,
            "mdate": 1699635959477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9WCS8qrLxx",
                "forum": "cphhnHjCvC",
                "replyto": "rmqWVdwpNn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review.\n\n> The proposed DEBiT model seems to be limited in addressing image-goal navigation tasks \n\nWe plan to use DEBiT also for visual odometry, in the lines of: \n\nPartsey et al. Is mapping necessary for realistic pointgoal navigation? CVPR 2022.\n\n> ImageNav (...) [This question might be more appropriate for researchers who proposed ImageNav, but since this paper is devoted to ImageNav, I believe it must have a very strong reason and motivation behind.]\n\nWe indeed cannot answer for the original authors introducing the two benchmarks, but we do agree on the observations. However, this task was among the most difficult among the EAI navigation tasks until recently, much more than ObjectNav for instance, and we think we have contributed to making significant steps forward. We actually also do agree on your point which, if we have understood correctly, states that visual search can be separated from classical observation tasks like exploration and detecting (and respecting) navigable space. This is the exact reason why we proposed a specialized architecture with tailored pre-text losses, which addresses exactly this issue. We argue that this will still hold if the length of the episodes increases significantly, as this will require more exploration skills, not more competence in visual comparison.\n\nWe do not claim that DEBiT will contribute to the exploitation of visual priors as other work does, but which would be complementary: if, for instance, our goal image shows a shampoo bottle which DEBiT does not find in the current observation, the exploration component could exploit the semantic information in the goal and guide the robot towards bathrooms. \n\n> The limitation and future extension of this work are not discussed in this paper.\n\nLimitations: \n* further improvements could be done on the Instance-ImageNav benchmark by changing the pre-training tasks such that image pairs are also sampled with different camera intrinsics, as currently this difference is only learned through the RL loss and with adapters.\n* increasing the image resolution might be beneficial.\n\nFuture and ongoing work will address the following aspects:\n* using DEBiT for visual odometry, \n* Pre-training on frame pairs which correspond to the Instance-ImageNav setting \n* Training the model on pairs where the background of the goal image is segmented out by a model like \u201cSegment Anything\u201d\n* Porting the model to our navigation software on our real robotics platform\n\nThis has been added to the paper.\n\n> (*) This paper mentioned depth inputs, but I wonder why depth images are not applied in the model\n\nWe did not use depth for multiple reasons: \n* we do not want to rely on capturing depth for the goal image, as this would significantly restrict the use case of ImageNav.\n* we think that depth is extremely helpful in navigation but perhaps less so when calculating visual correspondences. The boost it could probably get in simulation might quickly get offset when the model is run on a real robot, as the sim2real gap is high.\n* the two benchmarks we also targeted for evaluation do not use depth images.\n\n> (*) How does the choice of tau influence the pre-training and the downstream results?\n\nWe have added an ablation by training the RPEV with tau=0.1 and tau=0.4, compared to tau=0.2 in the paper. We report the results in the table below and added it to the appendix of the revised manuscript (Table 7). More precisely, we report the visibility accuracy (predicted visibility within 0.05 of the ground-truth one) as well as the percentages of correct poses within 1m&10\u00b0 and 2m&20\u00b0, considering all pairs with visibility above a threshold tau\u2019, for tau\u2019 = 0.2, 0.3 and 0.4. \n\nOverall, in terms of RPEV performance, the impact is extremely limited. We are training the navigation model with these new visual encoder variants, but this will not be ready during the rebuttal phase. We will add them to the camera ready version of the paper.\n\n| tau        |   vis-acc | % correct poses tau\u2019=0.2 | % correct poses tau\u2019=0.3 | % correct poses tau\u2019=0.4 |\n|-----------|-------------|-----------------------------------|-----------------------------------|----------------------------------------|\n**0.2** | 89.3 | 92.4 / 96.8 | **93.6** / **97.4** |    **93.7** / **97.6**  |\n0.1       | **90.0** | **92.5** / **96.9** |  93.3 / 97.3  |   92.9 / 97.4 |\n0.4       |  87.7 | 89.6 / 95.6 | 92.7 / 97.1 |    93.5 / 97.5 |\n\nThis table has been added to the appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission326/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580079293,
                "cdate": 1700580079293,
                "tmdate": 1700580079293,
                "mdate": 1700580079293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eqS95Luxk2",
                "forum": "cphhnHjCvC",
                "replyto": "rmqWVdwpNn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission326/Reviewer_isxL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission326/Reviewer_isxL"
                ],
                "content": {
                    "title": {
                        "value": "Final Rating"
                    },
                    "comment": {
                        "value": "I'm satisfied with most of the responses and appreciate the additional analyses and discussions from the authors. After reading all the other reviews and responses, I decided to keep my rating as Weak Accept (6)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission326/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627766006,
                "cdate": 1700627766006,
                "tmdate": 1700627766006,
                "mdate": 1700627766006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PZ4FmsOEmE",
            "forum": "cphhnHjCvC",
            "replyto": "cphhnHjCvC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission326/Reviewer_Sgek"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission326/Reviewer_Sgek"
            ],
            "content": {
                "summary": {
                    "value": "This paper is concerned with image-goal navigation and proposes a new end-to-end method for this task. The main contribution of the work is the inclusion of a pre-training stage of two auxiliary tasks that help in learning relevant visual features to be used as a state representation to a recurrent policy. The method achieves state-of-the-art performance on publicy available datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and easy to follow. The authors show that they understand the underlying challenges of this problem well and clearly explain their thought process behind the proposed approach.\n\nI agree with the authors that the bottleneck for these navigation problems is perception. I think the pretext tasks being proposed help in learning relevant representations for this task which would be otherwise very difficult to learn in typical end-to-end methods. Overall I think the paper proposes  a novel and sensible approach and moves the needle forward on this subject and improves the sample efficiency of these methods."
                },
                "weaknesses": {
                    "value": "The title is somewhat misleading as the emergence of correspondences is really not the focus of the work, but more of an afterthought. The content of the paper might be mistaken as an investigation into this phenomenon that is carried out in this paper: \n[A] Tang et al, Emergent of Correspondences from Image Diffusion, arXiv, 2023.\nIn fact the emergence of correspondences from the learned representation is not really that surprising, given that the pre-training task is relative pose estimation. Even before the introduction of transformers, monocular pose estimation methods have shown that the representation learns to identify meaningful keypoints on objects. One example:\n[B] Mousavian et al, 3D Bounding box estimation using deep learning and geometry, CVPR 2017\n\nI appreciate the inclusion of the experiment where DEBiT was integrated with ANS for a direct comparison to a modular approach. However, I am surprised by the large performance gap to the proposed method. It seems unintuitive that a method that tries to map pixels directly to actions would outperform an approach that uses a map and de-couples the planning from the control. I think this should be looked more carefully to ensure fair comparison. Was the global policy of ANS finetuned with the frozen binocular encoder b and using the AdaptFormers? ANS was trained for object-goal so probably the Neural SLAM and global policy components need to be re-trained for the ImageGoal task. As it stands, the experiment is not convincing that end-to-end methods are better than modular approaches."
                },
                "questions": {
                    "value": "Why is using panoramas treated as unrealistic with regards to robotic applications? RGB sensors (and even RGB-D) are relatively cheap and can be easily mounted on robots to cover a 360 degree field-of-view.\n\nThe authors cover some of the literature on pre-text tasks for improving sample efficienty but I think these are also worth discussing:\n[C] Ye et al, Auxiliary Tasks and Exploration Enable ObjectGoal Navigation, ICCV 2021\n[D] Sax et al, Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies, CoRL 2019\nEspecially regarding [D], shouldn't a representation trained on a multitude of vision tasks be more task-generalizable to just pre-training on relative pose estimation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission326/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission326/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission326/Reviewer_Sgek"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission326/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728725185,
            "cdate": 1698728725185,
            "tmdate": 1699635959399,
            "mdate": 1699635959399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vyOA4jzhoz",
                "forum": "cphhnHjCvC",
                "replyto": "PZ4FmsOEmE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review.\n\n>The title is somewhat misleading as the emergence of correspondences is really not the focus of the work, but more of an afterthought. The content of the paper might be mistaken as an investigation into this phenomenon that is carried out in this paper: [A] Tang et al, Emergent of Correspondences from Image Diffusion, arXiv, 2023. In fact the emergence of correspondences from the learned representation is not really that surprising, given that the pre-training task is relative pose estimation. Even before the introduction of transformers, monocular pose estimation methods have shown that the representation learns to identify meaningful keypoints on objects. One example: [B] Mousavian et al, 3D Bounding box estimation using deep learning and geometry, CVPR 2017\n\nWhile the emergence of correspondence is not the focus of this work, we nevertheless think that it is an interesting finding, in particular when compared to competing methods for this problem, which either calculate correspondences explicitly (Krantz et al, 2023), or are based on late fusion, where correspondences is hard or impossible to emerge. We therefore argue that this sets our work apart from the literature on this topic.\n\nThank you for the references, we have added them to the paper.\n\n> (...) ANS (...). However, I am surprised by the large performance gap to the proposed method. It seems unintuitive that a method that tries to map pixels directly to actions would outperform an approach that uses a map and de-couples the planning from the control. I think this should be looked more carefully to ensure fair comparison. Was the global policy of ANS finetuned with the frozen binocular encoder b and using the AdaptFormers? ANS was trained for object-goal so probably the Neural SLAM and global policy components need to be re-trained for the ImageGoal task. As it stands, the experiment is not convincing that end-to-end methods are better than modular approaches.\n\nANS was not finetuned, we took the weights from the model provided by the authors. This will also explain a drop in performance. As said in the paper, this experiment was proof-of-concept of the possible integration of DEBiT into a modular method. It should not be seen as a direct comparison. We have toned down the language on the comparison in the paper.\n\nWe conjecture that a large gain is obtained by the fact that the end-to-end trained policy benefits from the richer latent embeddings passed from the pen-ultimate layers of the visual encoders, whereas ANS only receives pose and visibility directly. We have run an experiment to confirm this where  the end-to-end trained agent only receives pose and visibility. It reached only 20\\% SR in training, which provides evidence that the full embedding is quite important for the end-to-end trained agent. We conjecture that the performance could be further increased by longer training, but we made it comparable and trained the agent for 200M steps.\n\nWe have added some elements of this to the paper.\n\n> Why is using panoramas treated as unrealistic with regards to robotic applications? RGB sensors (and even RGB-D) are relatively cheap and can be easily mounted on robots to cover a 360 degree field-of-view.\n\nIn the panoramic setting, both observed and goal images are supposed to be panoramic, and this is why the setting is not realistic. While 4 cameras can be simply put on a robot, the panoramic setting also requires that 4 different *goal* images are captured by user, which is often done with a hand-held camera or a cell phone, and it is required to do this with the same optical center and with exact angles of 90\u00b0. This is very complicated to do for an end user.\n\n> The authors cover some of the literature on pre-text tasks for improving sample efficienty but I think these are also worth discussing: [C] Ye et al,, ICCV 2021 [D] Sax et al,, CoRL 2019 Especially regarding [D], shouldn't a representation trained on a multitude of vision tasks be more task-generalizable to just pre-training on relative pose estimation?\n\nThank you for these references, we will add them to the paper. We indeed think that certain / several pre-text tasks are complementary to ours, and it would also certainly be worth exploring whether these tasks should be used to train the monocular encoder m, or whether the binocular encoder should take over parts of the reasoning they add."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission326/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579285457,
                "cdate": 1700579285457,
                "tmdate": 1700579285457,
                "mdate": 1700579285457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J2jCiz0znJ",
            "forum": "cphhnHjCvC",
            "replyto": "cphhnHjCvC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission326/Reviewer_Q8jf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission326/Reviewer_Q8jf"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduced pretext tasks and a dual visual encoder for ImageNav and Instance-ImageNav navigation in 3D environments, which provide rich geometric information and make it possible to address the challenging mono-view setting with end-to-end trained methods. Via experiments it is shown that peformance on competing methods and SOTA on both benchmarks is better.  The idea of cross-view completion and goal direction computation as pre-text for ImageGoal in contrast to ObjectGoal is the philosophical novelty in the work. Apart from that, the pipeline and architecture presented will aid future scope of research in the benchmark challenges."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Fig. 3 is well described to give the view of the problem scope being breaked into cross view completion, relative pose estimation and visual navigation.\nThis is a well written paper with clear explainations and technical soundess.\nThe results sections and ablation studies uphold the claims.\nThis paper will help the ImageGoal community - hence recommended."
                },
                "weaknesses": {
                    "value": "The last portion of supplementary video in terms of correspondence needs better representation and also that is the core focus.\nThe analyis of time complexity for doing correspondances in the benchmark and the distribution of work load should have helped understand the bottlenecks for a near real time system like robotic agents, even in embodied setups.\nA practical deployment in robotic setup should have confirmed the real world transfer applicability.\nI think Fig. 1 image you ahve search to related the chair with big picture - can any other image or better reoslution be used?\nSame with panaromic and mono view - please help in making sense of the image if at all included in body. If space contsraint, appendix referral is there for later sections, but introduction setup has to be clear.\nI think related work can only focus on the core related work, getting rid of object goal and general visual nav in such detail - this space can be used elsewhere in explaination later.\nNo limitations of the work is presented. Future gaps should be explained well."
                },
                "questions": {
                    "value": "\"we split this path into 5 parts cor\" - any logic regarding the discreet steps evenly spaced?\nInstead of Active Neural SLAM, anything else has been tried out in the pipeline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission326/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699267775114,
            "cdate": 1699267775114,
            "tmdate": 1699635959326,
            "mdate": 1699635959326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XuP3xy5JR0",
                "forum": "cphhnHjCvC",
                "replyto": "J2jCiz0znJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission326/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review.\n\n> The last portion of supplementary video in terms of correspondence needs better representation and also that is the core focus. \n\nWe have added an explanation to the video.\n\nIn essence, the visualization is simple: we have examined the last cross-attention layers, with attention summed over all heads, and looked at individual attention values. We have then picked the highest N attention values in this matrix and displayed their corresponding query-key pairs, i.e., drawing a line between the corresponding patches.\n\n>The analysis of time complexity for doing correspondances in the benchmark and the distribution of work load should have helped understand the bottlenecks for a near real time system like robotic agents, even in embodied setups. \n\nComputational complexity is dominated by the visual encoder, and our training frame-rates indeed dropped when adding the attention based binocular encoder. However, we still get high frames rates in training on an A100:\n\n* DEBiT-L : 92 fps\n* DEBiT-B : 156 fps\n* DEBiT-S : 192 fps\n* DEBiT-T : 225 fps\n\nThis includes all forward passes over visual encoders and policy, and also things not needed in inference on a robot: backward passes and the simulation of 12 different Habitat environments in parallel. We did not yet test DEBiT on our own robots, but we have tested similar policies running on an embedded Nvidia Jetson and we can achieve decisions with delays of well under 100ms, which allows the robot to move quickly with 1m/s if dynamics if the delay is handled correctly in simulation as well.\n\nIn terms of complexity of the correspondences themselves, it is the complexity of attention, which is quadratic in terms of tokens (which are patches) and linear in the embedding dimension, number of heads and number of layers. We have input images of size 112x112  and patch sizes of 16x16 which gives 7x7 = 49 patches per image.\n\nThis has been added to the appendix of the revised paper.\n\n> A practical deployment in robotic setup should have confirmed the real world transfer applicability.\n\nWe leave this for future work, integration in our real robotics platform will actually start very soon.\n\n> I think Fig. 1 image you ahve search to related the chair with big picture - can any other image or better reoslution be used? Same with panaromic and mono view - please help in making sense of the image if at all included in body. \n\nWe have zoomed into the observation in the revised paper, this should make the chair better visible."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission326/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579071830,
                "cdate": 1700579071830,
                "tmdate": 1700579071830,
                "mdate": 1700579071830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]