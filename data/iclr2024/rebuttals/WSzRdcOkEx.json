[
    {
        "title": "GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models"
    },
    {
        "review": {
            "id": "kmJ138abEY",
            "forum": "WSzRdcOkEx",
            "replyto": "WSzRdcOkEx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data-independent metric called GREAT Score for evaluating adversarial robustness. To be specific, GREAT Score is calculated as the mean of the gaps in the likelihood of model prediction between the ground-truth class and the most likely class other than the ground-truth class achieved by the samples generated from a generative model. The empirical results seem to validate the effectiveness of GREAT Score in evaluating robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The GREAT Score is data-independent, which is applicable to privacy-sensitive black-box evaluation. \n\n2. The GREAT Score is less sensitive to data points compared to AutoAttack."
                },
                "weaknesses": {
                    "value": "1. The definition of robustness in Eq. (1) and Eq.(4) seems to be confusing and possibly wrong. In Eq. (1), $\\Delta_{\\min}$ is defined as the minimal perturbation of a sample-label pair causing the change of the top-1 class prediction. Then, I understand $g(x)$ is a lower bound of $\\Delta_{\\min}$. However, in Eq. (3), $g(x)$ is defined as the gap between two probabilities. Therefore, I am confused about how the gap between two probabilities measures the minimal perturbation.\n\n2. The rank of the model in terms of adversarial robustness is unclear and confusing. As shown in Table 1, the rank of a model based on GREAT Score could be different from the rank based on AutoAttack.  Besides, the rank of a model based on GREAT Score could also be different from the rank based on the calibrated GREAT Score.  Therefore, the research would be confused about what is the genuine rank of a model in terms of its robustness.\n\n3. How can you guarantee the gap between the two probabilities achieved by the generated data is achieved in the worst case? As far as I can see, the authors directly used the generated samples from the generative models, which are supposed to be natural samples instead of adversarial samples.  \n\n4. The table and figure should be resized to be larger, which makes it easier to read."
                },
                "questions": {
                    "value": "Please refer to my comments in \u201cWeaknesses\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698399063571,
            "cdate": 1698399063571,
            "tmdate": 1699636337102,
            "mdate": 1699636337102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EZXHfwoUzm",
                "forum": "WSzRdcOkEx",
                "replyto": "kmJ138abEY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We express sincere gratitude for your valuable feedback and constructive comments.\n\n## Question 1: The definition of robustness in Eq. (1) and Eq.(4) seems to be confusing and possibly wrong, how does the gap between two probabilities measure the minimal perturbation?\n\nThe reviewer's question is exactly one of the major contributions of our work - establishing the connection between the defined metric in Eq. (1) and the derived certified lower bound in Appendix A.2.3. The novelty lies in the use of a generative model taking random Gaussian noise as input and adapting the Stein\u2019s Lemma to derive a global certified lower bound on the minimal perturbation defined in Eq. (1). Please see the proof of Theorem 1 in Appendix 6.4 for the detailed derivation. If the reviewer still feels uncertain about this connection, we are willing to engage in further discussion to address the concern.\n\n\n\n## Question 2\uff1aDetermine which rank should be used.\n\n\nThe criterion depends on whether or not additional information is available to a user at the time of evaluating global robustness. We would like to emphasize that the calibrated GREAT score ranking is expected to be better than the uncalibrated one since calibration improves the rankings by providing additional information (e.g. calibrated with CW attack distortion). In principle, if no additional information is provided, then the uncalibrated score is the default choice. If some auxiliary information (e.g., attack perturbations) is available, then we suggest the use of a calibrated score.\n\n\n\n\n## Question 3: How can you guarantee the gap between the two probabilities achieved by the generated data is achieved in the worst case?\n\nBased on the reviewer's comment, we believe there may be some misunderstandings about our guarantee. We would like to use the following points to clarify the reviewer's concern.\n1. For a generated sample $G(z)$, the gap in our context refers to the difference in the prediction probability between the correct class $c$ and the most likely (top-1) predicted class, where the gap is defined as $f_c(G(z)) - \\max_{k \\in \\{1,\\ldots,K\\},k\\neq c} f_k(G(z))$. We note that $f_k(\\cdot)$ is the prediction probability of class $k$, instead of a probability distribution.\n2. Our goal is not to generate data that achieves the worst-case \"gap\" (we don't actually know what gap refers to here in the reviewer's comment). Instead, we are using our defined gap to derive a certified lower bound on the true global robustness, as proved in Sec. 6.4.\n\nWe hope our explanation clarifies the reviewer's concern. We are at the reviewer's disposal to answer any further questions the reviewer may have.\n\n\n\n\n## Question 4: The table and figure should be resized to be larger, which makes it easier to read.\n\nIn the updated version, we have included new changes requested by reviewers and shortened some parts to remain in 9 pages."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585591638,
                "cdate": 1700585591638,
                "tmdate": 1700585591638,
                "mdate": 1700585591638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VXa7SVq4zW",
                "forum": "WSzRdcOkEx",
                "replyto": "kmJ138abEY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's replies. My questions have been partially resolved. However, I still have the following concerns:\n\n1. Could you provide any high-level explanations of how the gap between two probabilities (i.e., $g(G(z))$) measures the lower bound of minimal perturbation ($\\Delta_{\\min}$)? The scale of the two probabilities' gap is $ g(G(z)) \\in [0,(\\pi/2)^{1/2}]$, right? If your theorem holds, the lower bound of the minimal perturbation belongs to $[0,(\\pi/2)^{1/2}]$, right? It is weird that the minimal distance $\\Delta_{\\min}$ should be related to the dimensionality whereas the gap between two probabilities is irrelevant to the dimensionality. \n\n2. You mean that '''uncalibrated score is the default choice''. However, it is still confusing for me to judge which model is more robust based on your metric. For example, in terms of uncalibrated score, Gowal_extra (0.534) is more robust than Rebuffi_extra (0.507); in terms of AutoAttack accuracy, Gowal_extra (80.53) is less robust than Rebuffi_extra (82.32). Further, in terms of calibrated score, Rebuffi_28_ddpm (1.214) is more robust than Rebuffi_70_ddpm (1.208); in terms of AutoAttack accuracy, Rebuffi_28_ddpm (78.80) is less robust than Rebuffi_70_ddpm (80.42). I believe this inconsistency makes the rank of the model unclear in terms of robustness, which degrades the soundness and contribution."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640601187,
                "cdate": 1700640601187,
                "tmdate": 1700640716868,
                "mdate": 1700640716868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BOgS89Amwj",
                "forum": "WSzRdcOkEx",
                "replyto": "aOPxFsHVWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reviewer's responses. While I appreciate the author's dedicated efforts, I have the following two major concerns, thus I decided to keep my score.\n\n1. I appreciate the detailed explanation of the proof. However, I am still not fully convinced of the correctness of the lower bound since it is irrelevant to the dimensionality. It seems that when GREAT Score is applied to datasets of different resolutions, the margin-based perturbation has a consistent lower bound. Intuitively, I find it difficult to believe the correctness.\n\n2. My major concern is that the GREAT Score and AutoAttack accuracy make the robustness rank ambiguous. If we have the assumption of the adversarial budget, AutoAttack accuracy provides a deterministic rank, and we can say that the model is good against a particular adversary. Uncalibrated and calibrated GREAT Score provide other two different deterministic ranks, which are irrelevant to the adversary's adversarial budget. However, when there exist some contradictions between these different ranks of GREAT Score and the ranks of AutoAttack, it makes the comparison more difficult/ambiguous and makes the researcher more difficult to obtain a conclusion. Therefore, I am doubt of the value of the proposed method."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704377234,
                "cdate": 1700704377234,
                "tmdate": 1700704377234,
                "mdate": 1700704377234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tfw36rloRR",
            "forum": "WSzRdcOkEx",
            "replyto": "WSzRdcOkEx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_npik"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_npik"
            ],
            "content": {
                "summary": {
                    "value": "To solve the problems of lack of proper global adversarial robustness evaluation, limitation to white-box settings and computational inefficiency, this paper proposes GREAT Score for global robustness evaluation of adversarial perturbation using generative models. The authors have introduced and compared the proposed method, but the novelty of this paper is limited. Here are some of my suggestions for improving the quality of this paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well writen, with better typesetting, the research content is also of certain practical value."
                },
                "weaknesses": {
                    "value": "(1) There are obvious spelling and coincidence errors in the paper.\n(2) In the background and related works section, it is suggested to simplify the introduction of generative models and add the description of global robustness evaluation rather than Appendix 6.3.\n(3)  It is recommended to draw flow charts of the algorithm proposed by the author and other algorithms to highlight the innovation of the algorithm.\n(4)  It is recommended to give a more detailed time complexity calculation.\n(5)  In the experimental results section, the layout of the experimental results graphs and tables is confusing, and it is recommended to rearrange them.\n(6)  Appendix chapters need to be arranged according to the order in which they appear in the main text.\n(7)  Some of the examples in the article need to be replaced to adhere to the previous arguments."
                },
                "questions": {
                    "value": "(1)  For other competitive methods, it would be better to use all the data in the dataset to measure robustness rather than the strategy proposed by the author?\n(2)  Does it still make sense to use a lower bound if a true global robustness evaluation can be obtained with high computational efficiency ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805139136,
            "cdate": 1698805139136,
            "tmdate": 1699636336997,
            "mdate": 1699636336997,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qvm8escYhq",
                "forum": "WSzRdcOkEx",
                "replyto": "tfw36rloRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors(1/2)"
                    },
                    "comment": {
                        "value": "We express sincere gratitude for your valuable feedback and constructive comments.\n\n## Question 1: There are obvious spelling and coincidence errors in the paper.\nWe are sorry about the spelling errors and typos. We have run a careful check in the revised version.\n\n## Question 2: Simplify the introduction of generative models and add the description of global robustness evaluation.\n\nIn the updated version, we have included new changes requested by reviewers and shortened some parts to remain in 9 pages.\n\n\n## Question 3: Draw flow charts of the algorithm.\n\nThis is a great suggestion! We have added a flowchart right after the algorithm in Figure 5 of the Appendix.\n\n\n## Question 4: Give a more detailed time complexity calculation.\n\n\nHere we analyze the time complexity of our algorithm. The time complexity of the GREAT Score algorithm is determined by the number of iterations (generated samples) in the loop, which is denoted as $N_S$. Within each iteration, the algorithm performs operations such as random selection, sampling from a Gaussian distribution, generating samples, and predicting class labels using the classifier. We assume these operations have constant time complexity $I$ and absorb them in the big $O$ notation.\n\nAdditionally, the algorithm computes the sample mean of the recorded statistics, which involves summing and dividing the values. As there are $N_S$ values to sum and divide, this step has a time complexity of $O(N_S)$.\n\nTherefore, the overall time complexity of the algorithm can be approximated as $O(N_S \\cdot I)$.\n\nFor CW attack, the optimization process iteratively uses gradients to find the adversarial perturbation. The number of iterations required depends on factors such as the desired level of attack success and the convergence criteria. Each iteration involves computing gradients, updating variables, and evaluating the objective function. It also involves a hyperparameter search stage to adjust the weighted loss function.\n\nLet $B$ be the complexity of backpropagation, $T_g$ be the number of iterative optimizations, and $T_b$ be the number of binary search steps for the hyperparaneter. The dominant computation complexity of CW attack for $N_S$ samples is in the order of $O(N_S \\cdot T_g \\cdot T_b \\cdot B)$. Normally, $T_g$ is set to 1000, and $T_b$ is set to 9. Therefore, the CW attack algorithm is much more time-consuming than ours. In the table below, we include a table of run-time comparison of GREAT Score and CW Attack on the same 500 examples for Rebuffi\\_extra and Gowal\\_extra models.\n\n\n\n\n\n| Model Name     | GREAT Score (seconds per sample) | CW Attack (seconds per sample) |\n| -------------- | -------------------------- | ------------------------ |\n| Rebuffi\\_extra |       0.038                     |                    11.376       | \n| Gowal\\_extra   |        0.034             | 11.544                   |\n\n\n\n\n\n\n## Question 5: The layout of the experimental results graphs and tables is confusing.\n\nWe understand the reviewer's concern, but the space of our revision is constrained by the ICLR submission guideline that does not allow for an extra page in the revised version. Nonetheless, we have made adjustments to our revised version to include the requested changes by all reviewers and optimize the graph and table presentations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586232716,
                "cdate": 1700586232716,
                "tmdate": 1700586232716,
                "mdate": 1700586232716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YXAxfvMpBu",
                "forum": "WSzRdcOkEx",
                "replyto": "tfw36rloRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (2/2)"
                    },
                    "comment": {
                        "value": "## Question 6: Appendix chapters need to be arranged according to the order in which they appear in the main text.\n\nWe have updated our paper to rearrange the order.\n\n## Question 7: Some of the examples in the article need to be replaced to adhere to the previous arguments.\n\nWe have rearranged the graphs and appendix.\n\n\n## Question 8: For other competitive methods, it would be better to use all the data in the dataset to measure robustness rather than the strategy proposed by the author?\n\nFor a fair comparison, we indeed compared our metric with other methods on the same set of generated samples. For baseline methods, we also reported their robust accuracy on the entire test set. For example, as demonstrated in Table 2, the utilization of AutoAttack on both synthetic and original data resulted in a low correlation coefficient of 0.7296, suggesting the instability of using AutoAttack for model ranking. Moreover, scalability is the unique strength of our method, as our method only requires forward inferences with the predictions made on the generated data samples, whereas attack algorithms are computationally intense, as discussed in the run-time analysis in Sec. 4.4.\n\n\n\n\n## Question 9: Does it still make sense to use a lower bound if a true global robustness evaluation can be obtained with high computational efficiency?\n\n\n\nBased on the findings presented in following research, it has been established that the computation of the local minimum perturbation is an NP-hard/NP-complete problem, which means the true global robustness is computationally inefficient to obtain. Therefore, computationally efficient lower bounds such as GREAT Score become a viable alternative for global robustness evaluation.\n\n\nDaskalakis, Constantinos, Stratis Skoulakis, and Manolis Zampetakis. \"The complexity of constrained min-max optimization.\" Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021.\n\nKatz, Guy, et al. \"Reluplex: An efficient SMT solver for verifying deep neural networks.\" Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30. Springer International Publishing, 2017."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587616522,
                "cdate": 1700587616522,
                "tmdate": 1700587616522,
                "mdate": 1700587616522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tRyTgSoaYz",
                "forum": "WSzRdcOkEx",
                "replyto": "qvm8escYhq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_npik"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_npik"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's reply. After considering the comments of other reviewers and the content of the reply, I decided to maintain my original score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708075345,
                "cdate": 1700708075345,
                "tmdate": 1700708075345,
                "mdate": 1700708075345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lgqQsnjDKp",
            "forum": "WSzRdcOkEx",
            "replyto": "WSzRdcOkEx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_UUBS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_UUBS"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose GREAT score, a privacy measure that may be used to quickly evaluate the robustness of black-box models.  Towards this end, GREAT score utilizes generative models (e.g., GANs and DDPMs) to generate (potentially label-conditioned) samples which are fed into the black-box model of interest.  In this manner, the black-models outputs (over the input classes) are gathered and used in the calculation of GREAT score.  The GREAT score itself is interesting; it leverages the overlapping input-noise characteristics utilized by both GANs and DDPMs (i.e., zero-mean isotropic Gaussian noise) to calculate a certified lower-bound on the true global robustness (wrt to the underlying generative model used).  GREAT score may thus be used either as a standalone auditing strategy for black-box models, or in conjunction with computationally intensive benchmarks which directly test robustness via adversarial attacks (e.g., RobustBench or specific attacks, such as AutoAttack, FGSM, PGD, etc.).  One limitation of the presented work is GREAT score's theoretical guarantees only apply to L2-based attacks, the effects of which on other widely-used distance metrics (L0 and L_{\\inf}) requires understanding through further follow-up work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The overall writing and derivation of the GREAT score itself are easy to follow and intuitive.  Furthermore, the authors do a good job of extensively testing GREAT score, empirically contrasting its results to RobustBench, and demonstrating its use across a suite of face recognition APIs.  While the theoretical results are interesting, exhaustive experiments are impressive and convincing of the utility of this score for measuring robustness of black-box models."
                },
                "weaknesses": {
                    "value": "Given the extensive research concerning robustness wrt L0/L2/L_{\\inf} based adversarial attacks, GREAT score's limitation to L2-based attacks is a major weakness.   As only L2-based attacks are considered in the paper, either GREAT score must be extended to cover L0/L_{\\inf} based attacks (which, the authors note, is unlikely under the current derivation) or follow up work must explore how effective GREAT score is at measuring robustness for L0 and L_{\\inf} based attacks.\n\nThe papers description of some relevant work is either terse or lacking.  For instance:\n- \"using the Rebuffi_extra model (Rebuffi et al., 2021)\" <- please give a short inline description of this model\n\n- \"successful adversarial perturbations (an upper bound on the minimal perturbation of each sample) returned by any norm-minimization adversarial attack method such as the CW attack (Carlini & Wagner, 2017)\" <- Please give a brief description of the CW attack\n\n- In Table 1, please add a column showing which generative model is being evaluated.\n\nFurthermore, GREAT score's role as a lower bound for CW distortion is noted throughout the paper.  However, this is not entirely clear given the current version of the paper.  Please explain that the CW (L2) attack solves an optimization objective which results in low L2 distortion.  Also, it is important to note why GREAT score serves as a lower bound for the CW attack distortion; Equation 3 is equivalent to the non-L2 term of the CW (L2) attack, with c = sqrt{ \\pi / 2} (in practice, CW determines c via a grid search).  Thus, assuming c >= sqrt{\\pi /2), GREAT score trivially lower bounds the CW L2 attack."
                },
                "questions": {
                    "value": "- \"Moreover, as a byproduct of using generative models, our adversarial robustness evaluation procedure is executed with only synthetically generated data instead of real data, which is particularly appealing to privacy-aware robustness assessment schemes, e.g., remote robustness evaluation or auditing by a third party with restricted access to data and model.\" <- Please note that this is not a panacea, i.e., see:  \n-Carlini, Nicolas, et al. \"Extracting training data from diffusion models.\" 32nd USENIX Security Symposium (USENIX Security 23). 2023.\n-Duan, Jinhao, et al. \"Are diffusion models vulnerable to membership inference attacks?.\" arXiv preprint arXiv:2302.01316 (2023).\n\n- \"In Figure 2, we compare the cumulative robust accuracy (RA)\" <- RA is used before its definition on page 7; please define on the first use of acronym RA\n\n- \"using the Rebuffi_extra model (Rebuffi et al., 2021)\" <- please give a short inline description of this model\n\n- \"DMs reverse the forward process and implement a sampling process from Gaussian noises to reconstruct the true\nsamples\" <- Please mention that this is done by solving a stochastic differential equation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822214776,
            "cdate": 1698822214776,
            "tmdate": 1699636336915,
            "mdate": 1699636336915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GbeJHtfq8M",
                "forum": "WSzRdcOkEx",
                "replyto": "lgqQsnjDKp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We express sincere gratitude for your valuable feedback and constructive comments.\n\n\n\n## Question 1: limitation to L2-based attacks.\nAs stated and explained in Sec. 5, we admitted that our GREAT Score focused on $L_2$ norm, due to the restriction of extending Stein's Lemma to obtain a closed-form global Lipschitz constant in other $L_p$ norms. Please see Lemma 4 in Appendix A.3 for details about the derivation.\n\nNonetheless, we hope the new insights and contributions from this paper, such as global certified robustness using generative models and its use in evaluating the robustness of online black-box APIs can outweigh this limitation. \n\n\n## Question 2: The paper's description of some relevant work is either terse or lacking.\n\nWe understand the reviewer's concern, but our revision is constrained by the ICLR submission guideline that does not allow for an extra page in the revised version. Nonetheless, we have made adjustments to our revised version to expand the descriptions of related work in the main paper. We also note that we have provided extended discussion in the supplementary material.\n\n\n\n\n\n## Question 3: Please explain that the CW (L2) attack solves an optimization objective which results in low L2 distortion. Also, it is important to note why the GREAT score serves as a lower bound for the CW attack distortion.\n\n\n### Question 3.1: Explain CW (L2) attack solves an optimization objective that results in low L2 distortion.\n\nUsing our nation, consider a $K$-way classifier $f$. Let $x$ be a data sample and $y$ be its top-1 classification label. Denote $\\delta$ as the adversarial perturbation. The untargeted CW Attack ($L_2$ norm)  solves the following optimization objective:\n\n$$\\delta^* = \\underset{\\delta}{\\arg \\min }  ( \\Vert \\delta \\Vert_2^2  + \\alpha \\cdot \\max \\{f_y(x + \\delta) - \\max_{k \\in \\{1,\\ldots,K\\},k\\neq y} f_k(x + \\delta),0 \\}  ),$$ where $f_k(\\cdot)$ is the prediction of the $k$-th class, and $\\alpha > 0$ is a hyperparameter. \n\n\n\n\nIf the second term of the objective function is zero, then by definition $x+\\delta$ is a perturbed sample of $x$ causing label flipping. In CW attack implementation, given $\\alpha$, it uses a gradient-based optimization solver and runs it for $T_g$ iterations to solve for the smallest $\\delta$ that makes the second term zero. Then, it performs a binary search on $\\alpha$ for a $T_b$ number of trials (in each trial, another round of multiple iterations is executed) to ensure the second term remains zero while aiming to minimize the first term (the square of the $L_2$ performance level). Finally, the attack algorithm returns the smallest perturbation in the entire process such that $f_y(x + \\delta) < \\max_{k \\in \\{1,\\ldots,K\\},k\\neq y} f_k(x + \\delta)$ (equivalently, when the second term is 0). If no solution is found, one should increase either $T_b$ and $T_g$. \n\nThe dominant computation complexity of CW attack for $N_S$ samples is in the order of $O(N_S \\cdot T_g \\cdot T_b \\cdot B)$, where $B$ is the complexity of backpropagation. On the other hand, the complexity of GREAT Score is $O(N_S \\cdot I)$, where $N_S$ is the number of generated samples, and $I$ is the forward inference cost of the generative model and the classifier. A run-time comparison is shown in Sec. 4.4. In the table below, we include a table of run-time comparisons of GREAT Score and CW Attack on the same 500 examples for Rebuffi\\_extra and Gowal\\_extra models.\n\n\n\n\n\n| Model Name     | GREAT Score (seconds per sample) | CW Attack(seconds per sample) |\n| -------------- | -------------------------- | ------------------------ |\n| Rebuffi\\_extra |       0.038                     |                    11.376       | \n| Gowal\\_extra   |        0.034             | 11.544                   |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587170173,
                "cdate": 1700587170173,
                "tmdate": 1700587170173,
                "mdate": 1700587170173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4uN5rVmFLV",
                "forum": "WSzRdcOkEx",
                "replyto": "lgqQsnjDKp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (2/2)"
                    },
                    "comment": {
                        "value": "### Question 3.2: Explain why the GREAT score serves as a lower bound for the CW attack distortion.\n\nWe would like to clarify that the derivation of GREAT Score is irrelevant to CW attack, and it is not an attack algorithm that finds an adversarial perturbation. In fact, we proved in Sec. 3.1 that GREAT Score is a lower bound of the *minimal perturbation*, making it a natural lower bound for all successful adversarial perturbations, including CW attack.\n\nWe further emphasize their differences using the following points:\n1. Although our equation (3) may look similar to the second term of the CW attack objective, the only similarity is that they are the evaluation of the classification criterion for misclassification. Note that our metric does not involve any $L_2$ norm loss term as did in the CW attack.\n2. Another notable difference is that the CW attack aims to find a perturbation $\\delta^*$, whereas our metric is to use a generated sample $G(z)$ from a generative model for global robustness evaluation. Our metric shows a certified perturbation range and does not involve the design of attack perturbation.\n3. In Theorem 1 we showed that GREAT Score serves as a certified lower bound of the true robustness (minimal perturbation), whereas CW attack (and any other attack methods) is an upper bound of the minimal perturbation.\n\n\n\n\n\n\n\n\n\n## Question 4: Privacy-aware question for training data. \n\nThank you for your advice. To our understanding, the mentioned paper discloses the risk of leaking private *training* data through model queries at inference time. However, our statement is about preventing the use of private and sensitive data to evaluate/audit a model. The scope is different from the privacy of training data. For example, when evaluating online facial recognition APIs (as discussed in Sec. 4.5), using our generated samples is more privacy-preserving than using real private and sensitive facial images. On the other hand, in this context, the mentioned paper is interested in finding private facial images used to train those APIs, which is beyond our scope.\n\n\n\n## Question 5: \"In Figure 2, we compare the cumulative robust accuracy (RA)\" <- RA is used before its definition on page 7; please define on the first use of the acronym RA. \n\nWe have defined it in the summary of Classifiers on the RobustBench part.\n\n## Question 6: \"using the Rebuffi_extra model (Rebuffi et al., 2021)\" <- please give a short inline description of this model.\n\nWe have added it to the updated version.\n\n## Question 7: \"DMs reverse the forward process and implement a sampling process from Gaussian noises to reconstruct the true samples\" <- Please mention that this is done by solving a stochastic differential equation. \n\nWe have added the description in the updated version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587540620,
                "cdate": 1700587540620,
                "tmdate": 1700587540620,
                "mdate": 1700587540620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PSNgqmglPH",
                "forum": "WSzRdcOkEx",
                "replyto": "4uN5rVmFLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_UUBS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Reviewer_UUBS"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors' rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their time and detailed responses, I have read all other reviews and rebuttals."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600700729,
                "cdate": 1700600700729,
                "tmdate": 1700600700729,
                "mdate": 1700600700729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3r3jTOkWQv",
            "forum": "WSzRdcOkEx",
            "replyto": "WSzRdcOkEx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_kzkL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3797/Reviewer_kzkL"
            ],
            "content": {
                "summary": {
                    "value": "The paper is focused on an important problem which is \"global robustness\" that aims to evaluate the robustness of classifiers with respect to the underlying, unknown data distribution. This work can be classified in the family of robustness evaluation algorithms. The authors propose GREAT Score for evaluating the global robustness of classifiers by leveraging generative models. Specifically, the GREAT Score utilizes a generative model to approximate the data distribution and then calculates a certified lower bound on the minimal adversarial perturbation level, averaged over the sampling distribution of the generative model. This allows estimating the distribution-wide robustness without needing the true data distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This problem setting is important. The overwhelming majority of work in the space of robustness evaluation has considered aggregated local robustness statistics over test samples. However, one might argue that these test samples might be biased and not able to represent the true data distribution. The global robustness of models w.r.t. the entire data space is still under exploring. In this sense, the problem the authors seek to address is relevant and will likely continue to grow in relevance.\n+ The paper is very well-written and organised. The main insights are well explained, and it is easy to follow. The use of generative models is well-motivated."
                },
                "weaknesses": {
                    "value": "- The authors utilise the generative models to estimate models' global robustness and try to provide statistical guarantees which is claimed as one of the main theorems. My biggest concern is from this point. Since generative models are not perfect, there exists a notable gap between the generative and true data distribution. We are still worrying about the error bound induced by the difference between the underlying data distribution and generative data distribution.\u00a0 The paper would greatly benefit from including probabilistic guarantees or error bounds on the estimated global robustness concerning the true data distribution, as opposed to solely with respect to the generative data distribution. Such bounds would significantly enhance the appeal and credibility of the results.\n- Table 1 illustrates the comparison between (Calibrated) GREAT Score v.s. minimal distortion found by CW attack on CIFAR-10. However, the table does not explicitly show the advantages of GREAT score compared to other metrics. For example, the CW distortion score between Rebuffi_extra and Gowal_extra is pretty large, but the GREAT score only differs 0.003. \n- Is there any other options besides generative models or GANs, which can approximate the unknown data distribution?\n- Figure 1 and 2 should be larger. Similarly, it is hard to get a close look at Table 1 and 2. The figures and tables on Page 7 and 8 are not well-constructed."
                },
                "questions": {
                    "value": "Pls see the Section Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699080620680,
            "cdate": 1699080620680,
            "tmdate": 1699636336810,
            "mdate": 1699636336810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ekt6v3tLE6",
                "forum": "WSzRdcOkEx",
                "replyto": "3r3jTOkWQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We express sincere gratitude for your valuable feedback and constructive comments.\n## Question 1: Error bound induced by the difference between the underlying data distribution and generative data distribution.\n\nThe reviewer's intuition is correct. In our ablation study of generative models (GMs), we do find that better GMs give higher ranking. In Figure 3, we show that increasing the Inception Score of GMs can significantly increase the Spearman's rank correlation. Intuitively, a higher inception score means better learning of GMs to the underlying data distribution, resulting in improved ranking efficiency in our case. We also had a discussion on the approximation guarantee of some GMs to the true data distribution in Sec. 6.2.\n\n## Question 2: Table 1 lacks clear advantages of GREAT score over other metrics despite significant distortion differences.\n\nThe main message we wanted to deliver for Table 1 is to show the complete statistics of different methods on all CIFAR-10 models. The results also verify our main theoretical contribution (Theorem 1) that GREAT Scores are indeed certified lower bounds of minimal perturbation, as the perturbations found by CW Attack (an upper bound) are all numerically larger than that of GREAT Score.\n\nThe \"Advantages\" of GREAT Scores are demonstrated in other figures and tables in the same section. For example, high model ranking correlation (Sec. 4.3, Table 2 & Table 3), fast run time (Sec. 4.4 & Fig. 4), and its utility in the evaluation of online black-box APIs (Sec. 4.5, Table 4 & Table 5).\n\n## Question 3: Are there any other options besides generative models or GANs, which can approximate the unknown data distribution?\n\n\n\nThis is a great question! Beyond our discussion in Sec. 6.2, in addition to generative models such as GANs or Diffusion models, another approach commonly used to approximate unknown data distributions is Kernel Density Estimation (KDE). KDE is a non-parametric statistical tool that can provide an estimation of the underlying data distribution based on observed data samples.\n\nSeveral research papers have explored the application of KDE for approximating hidden data distributions, such as\n\nS. A. Bigdeli, G. Lin, L. A. Dunbar, T. Portenier and M. Zwicker, \"Learning Generative Models Using Denoising Density Estimators,\" in IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3308191.\n\n\n\n\nM. Rosenblatt, Remarks on some nonparametric estimates of a density function.Ann.Math. Stat.27, 832\u2013837 (1956).\n\nE. Parzen, On estimation of a probability density function and mode. Ann. Math. Stat.33, 1065\u20131076 (1962).\n\nHowever, it is important to note that KDEs typically perform well in low-dimensional settings and they are known to suffer from the curse of dimensionality. For common datasets like CIFAR-10 and ImageNet, KDE may not be the most effective approach to accurately approximate the underlying data distribution given the limited size of their training data.\n\n\n \n\n\n\n## Question 4: Figures 1 and 2 should be larger. Similarly, it is hard to get a close look at Tables 1 and 2. The figures and tables on Pages 7 and 8 are not well-constructed.\n\nThanks for your advice. Although the ICLR submission guideline does not allow for an extra page in the revised version, we have rearranged the tables and figures to improve the clarity of figures. Please see the updated version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585403898,
                "cdate": 1700585403898,
                "tmdate": 1700585403898,
                "mdate": 1700585403898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]