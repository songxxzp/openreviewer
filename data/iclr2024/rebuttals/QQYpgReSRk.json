[
    {
        "title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images"
    },
    {
        "review": {
            "id": "3p0iVARIxZ",
            "forum": "QQYpgReSRk",
            "replyto": "QQYpgReSRk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_NPAD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_NPAD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new vision foundation model, uses three different training strategies, and verifies the performance on the image retrieval task on the constructed dataset. Experimental results on the constructed large-scale data set verify the effectiveness of the proposed new model and training strategy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It looks novel and promising to leverage  Image-to-Entities data to improve image retrieval performance using multi-task pre-training.\n\nAblation studies are pretty solid and comprehensive to reveal characteristics of the proposed method.\n\nThe analysis of different learning strategies is interesting. The paper provides a nice insight into exploiting image representation learning strategies for a particular task."
                },
                "weaknesses": {
                    "value": "In the experiment section, my major concern is that it lacks comparisons with SOTA methods. Ideally, it is encouraged to have comparisons with SOTA methods from both traditional methods and methods based on foundation models. So it will show improvement and advances of the proposed method in this area.\n\nA lack of analyses on efficiencies. In particular, the performance gains from the method should be evaluated along with its time and memory complexity. Does the complex the model, the higher the performance?\n\nThe authors observe a similar performance gain on the ImageNet image retrieval task. Why does model performance improve roughly the same on both ImageNet and Image-to-Entities ? The author may want to clarify rationale behind their observations."
                },
                "questions": {
                    "value": "I am wondering why using entity filtering images for multi-task pre-training helps the retrieval problem. This paper could benefit from illustrating more rationale behind using Image-to-Entities dataset.\n\nWhat are the major difference between multi-task pre-training methods employed in this paper and the contrastive learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698371684424,
            "cdate": 1698371684424,
            "tmdate": 1699636448462,
            "mdate": 1699636448462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tfhl64nhk5",
                "forum": "QQYpgReSRk",
                "replyto": "3p0iVARIxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NPAD"
                    },
                    "comment": {
                        "value": "We thank to reviewer for the positive feedback and useful suggestions. Please see responses as follows.\n\n**Baselines**\n\nThanks for the suggestion, and we will add more baselines as suggested. If you have any particular models you think we should add, please list them and we are happy to include them.\n\nWe use CLIP as the major baseline because it is the most commonly vision foundation model now and achieved state-of-the-art performance on various of tasks. It is hard to make fair comparison for other models because of the differences like training data, model size, et.c. For example, CoCa and LiT achieved stronger imageNet zero-shot but with private JFT dataset. Similar to the traditional models. \n\nBesides CLIP, we also considered strong baselines from GPR1200 paper and DINOv2 which is  the state-of-the-art model on image retrieval tasks.\n\n**Performance v.s. Efficiency**\n\nWe clarify that the model backbone are the same for MOFI model and baseline CLIP model, the inference latency should be the same. We also keep the seen examples during training for all our models to make the fair comparison. \n\nAnd we don\u2019t think it is more complex the model, the higher the performance in our case. It is all about the training data, and the right training objective. As reviewer fBZj also commented, *MoFI's classification objective could be seen as pulling similar images together by ensuring they have a similar object distribution*. And many classification tasks and most image retrieval tasks are usually object / entity based, which will benefit more from the entity centroid training objective.\n\n**Performance gain on ImageNet**\n\nWe do not fully understand the question here. Could you please elaborate on what you mean by \"the performance improves roughly the same on both ImageNet and Image-to-Entities\"? Specifically, could you identify the table in question and explain why it poses a problem? We are happy to offer additional clarification once we have a more detailed understanding of your concerns.\n\n**Re Questiones**\n\nWe think it is the same as we addressed in the performance v.s. efficiency section above., MoFI's classification objective could be seen as pulling similar images together by ensuring they have a similar object distribution.  As image retrieval tasks are usually object / entity based, they are benefited from the entity centroid training objective. We will add more discussion in the final version of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700015296903,
                "cdate": 1700015296903,
                "tmdate": 1700015296903,
                "mdate": 1700015296903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i6XSC3uHlp",
                "forum": "QQYpgReSRk",
                "replyto": "Tfhl64nhk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_NPAD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_NPAD"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding performance improvements on imagenet, this refers to the penultimate sentence in the second paragraph on the second page of the submission where the author says \"We observe a similar performance gain on the ImageNet image retrieval task.\" I hope the authors provide more analysis of the reasons behind the roughly equal performance improvements on the two datasets."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203412305,
                "cdate": 1700203412305,
                "tmdate": 1700203412305,
                "mdate": 1700203412305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Do4QJTlabv",
                "forum": "QQYpgReSRk",
                "replyto": "3p0iVARIxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer NPAD \n\nThank you again for your insightful reviews of our submission and help to clarify your question after the initial response. Following your feedback, we have provided a detailed response trying to address the concerns you raised. Please let us know if these answered your questions / concerns. We are happy to continue discussing if you have any remaining concerns.\n\nYour effort and time in reviewing our submission are sincerely appreciated.\n\nWarm regards, \n\nAuthor(s)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638749430,
                "cdate": 1700638749430,
                "tmdate": 1700638777769,
                "mdate": 1700638777769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZD3EFSmRV7",
            "forum": "QQYpgReSRk",
            "replyto": "QQYpgReSRk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_4Nss"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_4Nss"
            ],
            "content": {
                "summary": {
                    "value": "This article primarily presents a billion-scale Image-to-Entities dataset, which includes 1 billion images and 2 million distinct entities. Based on this dataset, the authors have attempted a series of model training algorithms, including supervised learning, contrastive learning, and a combination of both in the form of multi-task learning. The constructed models have achieved state-of-the-art performance on the GPR1200 and VTAB datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ A large-scale image-to-entity dataset is provided, which has the potential to be reused by other pre-training models for vision-language related tasks.\n+ Based on this data, a pre-training model has been trained, which can serve as a general-purpose tool for tasks such as image classification and image retrieval."
                },
                "weaknesses": {
                    "value": "+ There are a few unfair comparisons between MOFI and CLIP: (1) CLIP can handle text at the sentence level, serving as the base model for MLLM image inputs. It can be used in tasks like stable diffusion in text-guided image generation. (2) MOFI only utilizes the proposed datasets, which definitely excels at mapping images to entities. The selected dataset for the experiment also leans towards examining the correspondence between images and entities.\n\n+ The training of the MOFI model is limited to image-level correspondence to entities. However, in reality, a single image may contain multiple entities, necessitating the inclusion of region-level correspondence in the model's training. Unfortunately, MOFI does not take this aspect into consideration."
                },
                "questions": {
                    "value": "I am more concerned about how MOFI performs on existing hot tasks, such as whether it is more advantageous to combine with LLM than CLIP, or whether MOFI can be used for image generation tasks. Translate the above into English."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635515380,
            "cdate": 1698635515380,
            "tmdate": 1699636448392,
            "mdate": 1699636448392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0uB5QnnW4j",
                "forum": "QQYpgReSRk",
                "replyto": "ZD3EFSmRV7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Nss"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for providing valuable feedback and constructive suggestions. While acknowledging the merit of the points raised in the weakness section, we view them more as insightful suggestions rather than inherent weaknesses. Implementing these suggestions has the potential to enhance the MOFI model further. The current results show MOFI advanced the image retrieval SoTA on the GPR1200 dataset by a significant margin, and has strong image representations, with improved zero-shot performance on ImageNet and VTAB benchmarks. We believe it is worth sharing them as the current form, the intuition and results of the paper could benefit the community already. \n\n\n**Regarding the unfair comparison** \n\nAgain, we believe it is more like the difference between the two models, not a weakness. And we do not claim that MOFI will replace CLIP in all cases. Table 3 and 4 indeed show the CLIP model and MOFI model can do a better job on different tasks. Users can choose the most suitable model for their use cases. \n\nIn addition,  MOFI also has a contrastive task. It is possible to extend the contrastive part to include all CLIP data, which we believe will address the long text understanding concern from the reviewer, but may make the image representation more like CLIP. \n\n**Regarding multiple entities** \n\nBecause we extracted the entity from the alt-text, and the nature of short text length of the alt-text, we observed that it usually only describes the primary object / entity in the image. Taking the multi-entity annotation with region (bounding box) is a good direction and we believe it will be the future of MOFI model as well, we leave it as a future work. \n\n\n**Re Questions**\n\nThanks for the suggestion. We agree combining MOFI with a LLM or an image generation task is interesting and can be useful. However, we believe it is beyond scope of the paper, as it will require a significant amount of space to fully explore the impact when connecting with a LLM or a image generation task. It can be explored as one of the follow-up works."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699945440306,
                "cdate": 1699945440306,
                "tmdate": 1699945440306,
                "mdate": 1699945440306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0kAHud5HYp",
            "forum": "QQYpgReSRk",
            "replyto": "QQYpgReSRk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_FYZT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_FYZT"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript presents two main contributions. Firstly, it constructs a large-scale I2E dataset in which the correspondences between images and entities are collected. The dataset is derived from a large-scale web corpus, and the authors enhance its data quality by cleaning and filtering. Secondly, the manuscript explores various training recipes and ultimately adopts a multi-task learning strategy that combines supervised learning and contrastive learning. The multi-task model is trained on the constructed I2E dataset to perform image retrieval and classification tasks. Through experiments, the manuscript demonstrates that both the dataset and the training method contribute to achieving better results across multiple downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe manuscript constructs a large-scale and high-quality I2E dataset, which can be utilized for model training across various downstream tasks such as image classification and image retrieval. Furthermore, through a comparative analysis between the original CLIP, CLIP trained on I2T, and CLIP trained on I2E, the manuscript demonstrates that the constructed I2E dataset improves model performance.\n2.\tThe experiment is comprehensive, conducting thorough comparisons to assess the impact of different datasets on model performance, thereby validating the high quality of the dataset. Additionally, through comparisons between different models, the manuscript verifies the effectiveness of the proposed training approach.\n3.\tThe paper is well-organized and clearly written."
                },
                "weaknesses": {
                    "value": "1.\tThe number of training data can be shown in Table 2 for more clear comparison.\n2.\tIncomplete experimental comparison in Tables 3 and 4. Why is there only CLIP for comparison? Other methods for solving zero-shot/linear probe classification should be discussed and compared."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4674/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4674/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4674/Reviewer_FYZT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763118122,
            "cdate": 1698763118122,
            "tmdate": 1699636448196,
            "mdate": 1699636448196,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f4UIRkozGI",
                "forum": "QQYpgReSRk",
                "replyto": "0kAHud5HYp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FYZT"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s positive feedback and recognition of our contribution. We will add the number of training examples in table 2, and more baselines for table 3 and 4, as suggested."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944185183,
                "cdate": 1699944185183,
                "tmdate": 1699944185183,
                "mdate": 1699944185183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xpe0CAT5gM",
            "forum": "QQYpgReSRk",
            "replyto": "QQYpgReSRk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a vision-language model akin to CLIP, called MOFI. However, unlike CLIP, which primarily focuses on pure contrastive learning with image and text captions, the authors suggest supplementing this by directing the model's attention to specific entities within the captions. They achieve this by integrating an auxiliary task into the standard CLIP training objectives. To accomplish this, the authors introduce a new large-scale dataset, Image-to-Entities (I2E), containing 1 billion images and 2 million distinct entities.\n\nTo construct their dataset, the authors initially utilized a large crawled web corpus comprising 8.4 billion image-text pairs. They then applied a named entity recognition model to the text associated with the images, which could be derived from the captions or titles. However, due to the potential for multiple words to refer to the same actual entity, the authors implemented entity linking to connect entities to specific identifiers. To do so, they followed previous methodologies by learning entity embeddings based on graph embeddings from Wikipedia data. To determine the specific identifier linked with the entity, they utilized other entities within the same text to disambiguate. They combined other entity embeddings and iteratively computed the probability of the identifier based on the distance to the full text embedding. To eliminate noisy entities unrelated to the image, the authors used the image-entity distance computed by CLIP to filter out those with low similarity.\n\nThe authors explored three strategies for training models on their new dataset. The first strategy involves a straightforward classification-based objective where the aim is to classify an image as one of a fixed number of entities using a standard, fixed-size classifier. The second approach is contrastive pre-training, employing a cross-entropy loss for contrastive training, similar to CLIP. Finally, the authors proposed MOFI, which integrates both a standard contrastive learning objective and the classification-based objective by combining the two losses.\n\nThe authors conducted experimental evaluations of their approach across various tasks. Initially, they evaluated their model on the image-to-image retrieval task and exhibited significant gains compared to standard CLIP, controlling for the model backbone architecture. The authors demonstrated that, in many settings, their approach achieved state-of-the-art performance and outperformed other models overall. Next, they evaluated zero-shot classification across ImageNet and VTAB. The authors showcased that their approach achieved state-of-the-art performance on ImageNet and the VTAB Benchmark for their largest model size. Lastly, the authors evaluated the performance of linear probing with their model, illustrating improved performance over CLIP. Additionally, the authors included qualitative results showcasing examples of retrieved images from various benchmarks and a comparison of CLIP and MOFI models on image-to-text and image-to-entity tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In terms of strengths, before delving into the technical details of the method, the writing of the paper is very good and appears highly polished. The figures are well-made, and overall the presentation is of very high quality.\n\nIn terms of the substance of the paper, the idea of using entities as an additional type of supervision is an interesting one. By explicitly factoring out the entities from the text and then forcing the model to perform a task on those directly, the authors essentially force the model to learn an entity-centric visual representation. In particular, rather than allowing the model to get by with some rough semantic similarity of entities which can be encoded in a feature representation for image-text matching, by explicitly forcing the model to understand millions of specific entities and to disambiguate them, this explicitly forces the model to learn representations for differentiating these very fine-grained entities. Thus, the concept of using entities as an auxiliary task is an interesting one and well motivated.\n\nThe authors create a new image-to-entities dataset, which consists of a billion images and 2 million distinct entities. As the authors point out, the dataset has the largest number of distinct class labels than any other dataset, 66 times more than the next one cited. Thus, the dataset created by the authors poses a true challenge, as classifying images into 2 million categories is a daunting task even for the most powerful models. Thus, the image-to-entity dataset could serve as a new benchmark or task for large-scale visual recognition.\n\nIn terms of experimental results, the authors demonstrate that MOFI outperforms a number of recent state-of-the-art baselines and a number of image-text foundation models, setting a new state-of-the-art for CLIP-style models on a number of different benchmarks, as they note."
                },
                "weaknesses": {
                    "value": "Despite being an impressive and large-scale model that the authors have trained, in my view, the approach has several significant weaknesses.\n\nPrimarily, in terms of technical novelty, there seems to be minimal innovation in the proposed approach. The authors employ a standard contrastive learning approach alongside a standard classification objective, combined through a linear combination. In the context of entity extraction, the authors utilize a named entity recognizer, following an existing approach for learning entity embeddings from Wikipedia. Therefore, in terms of actual technical contribution, the paper seems rather limited. The primary contribution appears to be the extensive scale at which the model operates.\n\nRegarding entity linking, I find the proposed approach weak. It seems the authors don't conduct any form of multimodal entity linking. The process involves initial entity linking using entity embeddings on the text side, followed by a filtering step using a pre-trained CLIP model. A stronger approach for entity linking could potentially be achieved by integrating visual features into the knowledge embeddings. Since this component is central to the proposed approach, such a modification could potentially reduce the number of spurious entity linkings. I'm also curious if the authors experimented with off-the-shelf entity linkers, like BLINK.\n\nFundamentally, the authors emphasize the significance of the entity prediction task over standard contrastive learning. They argue that the experimental results demonstrate the advantage of predicting these entities. However, this claim is not entirely apparent. For instance, in table 2, the authors report better performance using their approach compared to CLIP and other baselines. Yet, this comparison might be somewhat unfair due to the substantially larger size of the author's dataset. For instance, CLIP's dataset comprises 400 million, whereas the author's dataset is 1 billion. Therefore, a comparison controlling for dataset size would be more appropriate to demonstrate the actual improvement using the author's approach rather than a mere performance increase due to a larger dataset. It would be beneficial to see a comparison with MOFI using 400 million images to understand how it compares to CLIP. Table 3 presents a more precarious situation. For zero-shot classification, we observe that the authors' multitask model doesn't show a clear advantage over CLIP on the ImageNet and VTAB benchmarks. It's not evident that the multitask formulation actually enhances performance compared to CLIP, especially considering the larger dataset size accessible to the authors. The MOFI model slightly outperforms CLIP in linear probing but only marginally.\n\nAdditionally, I'd like to ask the authors why they believe the training setting proposed in MOFI is the best approach. Since the advent of CLIP, a considerable amount of work has explored contrastive model training. For example, the LIT paper (Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., & Beyer, L. (2022). Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18123-18133).) demonstrated significant performance gains by locking the image branch of the model and tuning the text model using a private dataset on which LIT was trained. There are also other large foundation contrastive models like the open-source LAION-H-14 model (https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K) that the authors could have compared against. These models, not using the described objectives, seem to perform similarly to the author's approach, making this comparison more equitable since the other models the authors compare to seem to have substantially less data.\n\nRegarding the approach itself, it remains unclear if the concept of contrastive learning with a classification objective on entities is necessary. Other works like Coca (Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., & Wu, Y. (2022). CoCa: Contrastive Captioners are Image-Text Foundation Models.) , demonstrate that a similar objective could be achieved by performing contrastive learning + captioning. For example, on ImageNet, the authors obtained 86.3% zero shot performance using this contrastive captioning-based approach, which significantly outperforms the authors. The contrastive captioning approach actually seems to encompass what the authors are doing in the MOFI paper, as a caption needs to be generated, not just the entity. The authors focus solely on entities, yet other crucial visual relations like events, actions, and visual properties are omitted in their approach. These could be captured under the contrastive captioning setting. Therefore, it's uncertain whether MOFI's approach is superior to training vision-language models as opposed to the contrastive captioning-based approach.\n\nIn summary, it's unclear whether the multitask learning approach presented is the optimal way to utilize this data. There's a lack of clear performance analysis against contrastive captioning-based models, making the experimental results challenging to interpret due to the differing dataset sizes.\n\nAdditionally, there's a concern regarding the technical approach itself. Filtering out entities using CLIP could potentially result in error amplification. If an entity is poorly captured by CLIP, using it to filter the dataset might exclude that entity.\n\nIt seems that the paper would have been better positioned as a very large scale visual entity linking paper, than a vision-language foundation model paper, since the key novelty the authors seem to be addressing is the ability to disambiguate these various entities. However, that itself raises a lot of questions. For example, how do we ensure that more common entities don't overwhelm the long tail of entities? This does not seem to be explicitly handled by the authors' approach and currently authors do not perform an evaluation sufficient to consider the paper for the visual entity linking task."
                },
                "questions": {
                    "value": "1. What is the key benefit of MOFI's technical approach vs contrastive captioners like Coca? It seems that Coca significantly outperforms MoFi on several benchmarks significantly (e.g. Imagenet). Conceptually, the contrastive captioner seems to be more straightforward as well and doesn't require the complex entity linking that MOFI does. It seems that the entity prediction task is a subset of the captioning task that Coca addresses. Also, Coca is capturing entities AND verbs (and other words).\n\n2. Have the authors explored the statistics on how often the model predicts entities from the long tail? Is it mainly focusing on more common entities? It seems the core use of the model would be to classify an image as an entity from Wikipedia - this is an important task for knowledge graph construction and coreferencing, but it is not clear that the authors have evaluated the accuracy on that task - which seems to be the most unique part of the work.\n\n3. Will the dataset and model be released? If not, the contribution would be the technical approach, but as stated above, it is not at all clear that it is significant compared to other ways of training contrastive models (e.g. LiT) on equivalent size datasets AND it is not clear that this multi-task formulation is superior to existing methods for contrastive captioning (Coca).\n\n4. Table 2 - why evaluate on image-image retrieval when CLIP and other baselines were not trained explicitly for image retrieval. These methods have been trained for image-text retrieval, not intramodal retrieval. In contrast, MoFI's classification objective could be seen as pulling similar images together by ensuring they have a similar object distribution. It seems that if we are going to be comparing against VL models like CLIP, LiT, etc. we should focus on those types of tasks (zero shot classification, linear probing, etc.)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4674/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4674/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4674/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809264436,
            "cdate": 1698809264436,
            "tmdate": 1700713146521,
            "mdate": 1700713146521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E6poSVUVde",
                "forum": "QQYpgReSRk",
                "replyto": "Xpe0CAT5gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fBZj [1/2]"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s detailed feedback and constructive suggestions. Please find our responses below:\n\n**Contribution**\n\nFirst of all, we want to clarify that our main contributions are: 1) the way we curate the large scale classification dataset,  2) explore the different training objectives of the constructed dataset,  and 3) demonstrate the effectiveness of the resulting model. \nWe advocate for an expansive view of novelty beyond methodology alone. Valuable contributions can stem from innovative experimental designs, the unveiling of new findings or results, or the introduction of novel training paradigms that have the potential to inspire and benefit the community.\n    \nSecondly, we agree that both contrastive and classification objectives have been well studied in the research community. However, we want to point out several differences of our work: \n\n1. Studying learned generic image representation using a noisy large scale classification dataset is new. When combined with the contrastive objective in a multitask setup, we demonstrate that the MOFI model achieves significantly better results on both zero-shot classification and image retrieval. Especially on the image-to-image retrieval benchmark GPR1200, the new model is 14% (absolute) better than OpenAI\u2019s CLIP model.\n\n2. The existing largest classification datasets people have used are JFT-3B and IG-3.6B, both of them are private and hard to reproduce. Our dataset is curated from public data, can be easily reproduced by others, and 66x larger in terms of number of classes. \n\n3. We need to adjust the normal classification objective (e.g. sampled negatives and add margin) to effectively learn from such a big number of classes, as detailed in section 3.1.\n        \nWe will add more discussion about the contributions in the final version. \n\n**Entity Linking**\n\nWe appreciate the reviewer\u2019s suggestion on the entity linking, and will be happy to try BLINK as an alternative to our approach. Note that the main purpose of the entity annotation is to provide supervision to learn strong image representation. We find the results from the current noisy annotation have already shown strong performance on multiple benchmarks. Apart from the precision of entity annotation, it's crucial to factor in efficiency, enabling the processing of billions of texts in a feasible time frame. Our current approach can run 8.4B texts within 10 hours. While we think a better entity annotation approach is likely to further improve the performance, we leave it as a future work.\n\n**Dataset size**\n\nThanks for pointing out the dataset issue. We realized that we didn\u2019t make it clear in the paper. The row 4 (CLIP-B/16_Ours trained on I2T) is an in-house CLIP model trained on the original 5.5B image-to-text data. We train the CLIP model to see the same number of examples to make it a fair comparison. So in table 3 and 4 we also compare the in house CLIP model instead. Hope this addresses your concern. We will clarify it in the paper.\n        \nSimilarly, table 3 and 4 reports CLIP model trained on several different datasets. It is clear that the CLIP model trained on I2E dataset outperforms the CLIP trained on original image-text pairs. Switching to multitask improves the performance for those entity centroid tasks, while is worse on those tasks that are less entity specific. Users can decide to use which model depends on their tasks. We put more detailed discussion about it in section 4.3.\n \n**Comparing with other CLIP models**\n\nIt is hard to compare with other CLIP models as they are trained on different datasets. Some of them are private, e.g. LIT uses JFT-3B. And LAION is prohibited to use by our institution. OpenAI\u2019s CLIP model has been widely adopted so we list it as a baseline model. In addition, we trained our in-house CLIP model using the image-text pairs from the same data source, and a similar number of examples as addressed in the previous paragraph. Given that, we believe our comparison is fair. \n        \nWe also note that the image representation of LiT model is learned from a classification objective, which is similar to our MOFI_sup-B/16 setting in Table 2, Row 7. MOFI_sup-B/16 has comparable performance on GPR1200, but worse performance on ImageNet retrieval metric.\n\n**Compared to CoCa**\n\nCoCa is trained on private JFT-3B + LAION-1.8B data. It is not a fair comparison to compare our model v.s. original CoCa. We indeed implemented the contrastive captioning model and trained it on our data. However, in our preliminary experiments we didn\u2019t observe the same performance gain compared to CLIP only as it is described in its original paper. We suspect it is because of the private training data (e.g. JFT-3B) they used.\n        \nNote, JFT 3B is also a classification dataset with 30k unique labels."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699943976831,
                "cdate": 1699943976831,
                "tmdate": 1699943976831,
                "mdate": 1699943976831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3yOcZBlpoe",
                "forum": "QQYpgReSRk",
                "replyto": "Xpe0CAT5gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "CoCa Experiments"
                    },
                    "comment": {
                        "value": "For your consideration, we also list some preliminary results of CoCa v.s. CLIP in our early exploration. They were trained on the same configurations for fair comparison.  \n\n\n|         | ImageNet-zeroshot | GPR1200 |\n| :--- | --------------: | ---------------: |\n| CLIP-B/16 |       68.97         |      61.21 |\n| CoCa-B/16 |    68.08            |  59.93 |\n\nFurther, in the CoCa paper, under 4K batch size, CoCa performs much better than CLIP. We indeed also observed this performance improvement. However, as the batch size becomes larger, such as 32k, the performance gain diminishes. We empirically observe that under large batch size setting, CoCa actually performs slightly worse than CLIP. \n\nWe did non trivial work to tune CoCa, and also inspected the caption outputs. They all look good and generated captions are very reasonable. So we believe our implementation is correct, but the difference is the training data.\n\n\\* *The configurations are different than the models reported in the paper for fast iteration, e.g. using a smaller image-text datasets and were trained with smaller number steps (~300k))*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116086108,
                "cdate": 1700116086108,
                "tmdate": 1700116086108,
                "mdate": 1700116086108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nxeBUHi551",
                "forum": "QQYpgReSRk",
                "replyto": "Xpe0CAT5gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the detailed reply comments and apologies for delayed response. I have taken a look at all the reviews and re-read the paper. I also appreciate the authors doing non-trivial experiments to reply to comments.\n\nTo be clear, the reason for my weaker initial rating was primarily due to three key factors:\n1. My perception of the low technical contribution of the paper. In a nutshell, the paper is training like CLIP, with an additional classification objective on top of the vision model. I felt that more advanced types of objectives have been explored before in prior work, e.g. CoCA, Align before Fuse, etc. Seemed like the approach of this paper was quite straightforward - add an object classifier on top of the image branch. It also seemed to go against a line of recent work, e.g. LiT (Locked Image Tuning) that argued that we should in fact *not train* the image encoder, and instead focus on adapting the text encoder. I also felt that the approach could have been executed better, e.g. how to ensure that long-tail objects are predicted? What about some sort of focal loss, etc.? As it stands, my takeaway of the paper was, \"they added a classifier to CLIP's image model and got some better/mixed results\". Again, LiT and others achieved strong results by locking the image branch. \n\n2. I felt that some of the claims the paper were making were not experimentally justified for various reasons that I pointed out (e.g. differing dataset sizes that made comparison difficult). In some cases, the results were quite mixed and the story the reviewers were making from the results was not clear-cut from the results. The authors of course point to some successful cases where it outperforms, but others were much more mixed as I pointed out. I think it was equally possible to interpret the results differently, as I mentioned, so I do thinking some of the language making claims about the technique needed to be tempered.\n\n3. I was concerned about claiming a dataset or model contribution, without a guarantee that the dataset would be released. I also disagree with the authors that \"Our dataset is curated from public data, can be **easily** reproduced by others, and 66x larger in terms of number of classes\" (emphasis added). I don't think that the dataset can be easily produced by others, or else others would have done something at this scale. As the authors mention in the paper, they use an enormous amount of resources to do the entity linking part from wikipedia. I hope that those models for entity embeddings, etc. that they trained are all released. While of course the authors don't have to release a dataset, we understand that there are various legal and commercial reasons for datasets not to be able to be released, it makes it difficult to then claim this is a major strength of the paper. I think this argument about the dataset would be much stronger if the dataset had some new technique that was used to curate it or process the text, but as the authors mention, the strategy for entity linking largely follows other work on that topic. Thus there isn't really anything novel about the harvesting process or curation process, it is more the scale at which it was done. Even in the comments offered by authors, there is no guarantee that the dataset or models will ever be released. Of course, this may be due to factors beyond authors control, but then it is difficult to place a large emphasis on a dataset contribution from this paper, when the dataset might never be forthcoming. \n\nRegarding new CoCa results:\n\nThank you very much for providing this. Encourage the authors to include comparable results in main text or supplementary. However, I do have a question.\n\nYou state, \"Further, in the CoCa paper, under 4K batch size, CoCa performs much better than CLIP. We indeed also observed this performance improvement. However, as the batch size becomes larger, such as 32k, the performance gain diminishes. We empirically observe that under large batch size setting, CoCa actually performs slightly worse than CLIP.\"\n\nDoes this mean you trained CoCa with 4K batch size or the 32K batch size, i.e. are these results from 4K or 32K batched size CoCa? If CoCa works better with a 4K batch size, it would make sense to train it with a 4K batch size, since that is the setting that the original paper claimed worked best. Even though for your CLIP, maybe you use larger batch size, it could be that CLIP does better on larger batch size, but CoCa does not.\n\nCan you clarify this point?\n\nOne final comment, I was also looking at papers like OVEN: Open-domain Visual Entity Recognition Towards Recognizing Millions of Wikipedia Entities. In this paper, the authors argue that the same image could reasonably be linked to multiple possible knowledge based IDs, so predict based on a prompt (photo of a horse could be linked to horse, or to the bridle the horse is wearing). So, similar types of image prediction tasks using wiki have been explored."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682869814,
                "cdate": 1700682869814,
                "tmdate": 1700683201294,
                "mdate": 1700683201294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GEg8AWSh9L",
                "forum": "QQYpgReSRk",
                "replyto": "Xpe0CAT5gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "2nd response"
                    },
                    "comment": {
                        "value": "Thanks reviewer fBZj for taking a look at our responses and follow up question. Really appreciate your time and effort. Please see our latest response below:  \n\n1. As we stated in the original response, we advocate for an expansive view of novelty beyond methodology alone. Valuable contributions can stem from innovative experimental designs, the unveiling of new findings or results, or the introduction of novel training paradigms that have the potential to inspire and benefit the community. To our best knowledge, we believe our findings are novel, and the strong results of training on a large scale entity supervised data could inspire the community for next generation of vision foundation model.\n\nIn addition, we also explored the CoCa style captioning training as suggested, by we didn't find the captioning objective helpful in our benchmarks from the preliminary experiments, thus we didn't include it in our main paper. We can include more fair comparison in the final version.\nRegarding LiT, note LiT needs to start from a pre-trained image encoder. Focusing on the text encoder doesn't mean we don't need to train a image encoder. I don't think our results is against it. One possible way is to train a entity supervised classification model first (MOFI_sup) and then train CLIP model with the frozen image encoder. We also did preliminary experiments, and found the results are almost identical. Thus, we kept the multitask training as it doesn't require multi-stage training.\n\nAnd also note that both of CoCa and LiT are trained using a private JFT dataset from Google, which we cannot access. We believe it could be the reason why we couldn't reach the same level performance as reported by their papers.\n\n2. We are happy to carefully review the claims again and rewrote to make it clear. We believe our experiments are carefully designed, and make sure all comparisons are fair. In terms of results, we didn't claim people should always use MOFI, e.g. in table 3 and 4, we suggested for tasks are not entity specify, CLIP could be better. We are happy to provide more clarification if reviewer can specify which places are still confusing / concerning. \n\n\n3. Regarding the dataset contribution. The approach should be robust to the entity embedding model as it is gated by a CLIP model after the entity linking step. e.g. It should be easy to replace it by an open sourced entity embedding model as suggested. We can add an ablation study in the final version.  Once the entity embedding is ready, it only takes 10 hours using 256 machines to extract the entities from the text. \n\nWe are also working hard with our institution to release dataset. While cannot promise yet, we are also exploring different ways to make it useful to the community. \n\nOne direction we are actively working on reproducing the I2E dataset using open-source datacomp[1] data in lieu of our internal one. By now, we have generated one data version which has similar data scale as the one presented in the paper. It contains 1.19B images and 2.12M entities, which is comparable to the 1.1B images and 2M entities presented in the paper.\n\nWith the new I2E data derived from datacomp, we have conducted preliminary model experiments to compare its performance with that of the model trained on internal version. For both training datasets, we maintain the same setup as specified in the paper, except for training the model for shorter time, i.e. 300,000 steps . **We observe comparable performance on GPR1200 MAP@all (a light decrease of -0.7%), ImageNet retrieval Acc@1 (a marginal increase of +0.1%) and ImageNet Zeroshot (a marginal increase of +0.26%).**\n\nWe will try our best to release our work to make it useful to the community. \n\n\n\n**Questions for CoCa:**\n\nWe experimented training CoCa from 4k batch size to 32k batch size. The numbers we reported above uses 32k batch size for fair comparison.  And  `under 4K batch size, CoCa performs much better than CLIP` is from its original paper. We validated the point in our implementation as well. And here it only means CoCa perform much better than CLIP when they both trained using 4k batch size.\n\n\nAt last, thanks for pointing the paper `OVEN: Open-domain Visual Entity Recognition Towards Recognizing Millions of Wikipedia Entities.`  We will take a careful look and comparing them. Note this paper is just published last month at ICCV. We believe it can be treated as a concurrent work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690897135,
                "cdate": 1700690897135,
                "tmdate": 1700691462765,
                "mdate": 1700691462765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xB6m5gZF0b",
                "forum": "QQYpgReSRk",
                "replyto": "Xpe0CAT5gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ],
                "content": {
                    "title": {
                        "value": "Reply in Response to Reply"
                    },
                    "comment": {
                        "value": "Thank you for your comments. \n\nI believe that if the authors are intending to replicate their approach on a public dataset (datacomp), have models trained on that, and are planning to release those deliverables, that is a meaningful attempt to contribute something as a dataset, in the event that the original dataset can't be released. Hopefully the models on the datacomp can also be released. \nThis will alleviate concerns about data release of the original dataset.\n\nTo be clear regarding CoCa result shown, I don't think it is unfair to train each model using the batch size that works best for that particular approach. In fact, I think that it does make it more fair, since original CoCa paper mentioned that 4K batch size works best. I don't think it's incorrect then, to compare CoCa (with 4K batch) against CLIP (with 32K batch), since those are two different models, with different behaviors. In the same way, your method works best with 32K batch, so any comparison should use that, not 4K.\n\nDo you have the number for CoCa (with 4K) vs Your menthod (with 32K)? If so, that is the number that should be reported, presuming that CoCa 4K > CoCa 32K. You seemed to suggest that you did have this number. I am interested in seeing CoCa-B/16 on the Imagenet-zero shot, trained with 4k batch size. \nI ask because your method is less than 1% better than this.\nI am not saying that your paper should be rejected if you don't outperform CoCa with 4K batch size, but for completeness, we should have this to place work in context. \nIf accepted, in your final version, I think you should report the version of CoCa that worked best (4K) not the 32k, especially since authors of CoCa mention 4K works better.\n\nIf you can share that, that would address all my questions to the authors at this time.\nThanks"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692835801,
                "cdate": 1700692835801,
                "tmdate": 1700692861938,
                "mdate": 1700692861938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MmUT6tgbjV",
                "forum": "QQYpgReSRk",
                "replyto": "Xpe0CAT5gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More clarification about CoCa"
                    },
                    "comment": {
                        "value": "We are sincerely grateful to the reviewer fBZj for engaging in a thoughtful and active discussion!\n\nRegarding CoCa, we apologies for any confusion in our communication about it.  We experimented for both CoCa and CLIP with different training batch size from 4k to 32k.  Under 4K batch size, CoCa performs much better than CLIP relatively (in our experiments, we saw +2 points (+8% relative) improvement on ImageNet zero-shot). But 32k batch size trained CoCa still perform better than the CoCa model trained under 4K batch size. See the table below. _Note they are trained using the same number of training steps, so technically the 32K models have seen 8x more training examples than the 4k models._\n\n|         | Batch size | ImageNet-zeroshot | \n| :--- | ---: |--------------: | \n| CLIP-B/16  | 4k | 33.69 |\n| CoCa-B/16 | 4k | 35.67 |\n| CLIP-B/16  | 32k |      68.97         |      \n| CoCa-B/16 |  32k |   68.08            | \n\n\nWe emphasize that despite putting considerable effort into training the captioning objective, we have not observed any improvements upon its addition.  It is often challenging to claim something does not work, because there are so many things to explore, including those we do not have control, e.g. JFT dataset. However, based on our experiments, the inclusion of the captioning objective has not proven beneficial so far.\n\n\\*_We also note in CoCa paper, the final model is trained using 65k batch size. In our practice, we found models trained with >=16k batch size can get very similar performance as long as they have seen the same number of the training examples. Larger batch size with more resources can make the model interate faster._"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697031830,
                "cdate": 1700697031830,
                "tmdate": 1700698529402,
                "mdate": 1700698529402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JAvZvPq9vy",
                "forum": "QQYpgReSRk",
                "replyto": "MmUT6tgbjV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4674/Reviewer_fBZj"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thanks so much. This has addressed my questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4674/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697219721,
                "cdate": 1700697219721,
                "tmdate": 1700697219721,
                "mdate": 1700697219721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]