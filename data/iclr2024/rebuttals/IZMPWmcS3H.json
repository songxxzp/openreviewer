[
    {
        "title": "HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance"
    },
    {
        "review": {
            "id": "5Ql08fTQIe",
            "forum": "IZMPWmcS3H",
            "replyto": "IZMPWmcS3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1424/Reviewer_CJyW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1424/Reviewer_CJyW"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes four techniques on the task of 2D diffusion-guided text-to-3D generation, to enhance the generation quality. In particular, the authors 1) propose score distillation in both the latent and image space of the pre-trained text-to-image diffusion models; 2) introduce a timestep annealing strategy to achieve photo-realistic and highly-detailed generation; 3) present a regularization method on the variance of z-coordinates along NeRF rays to encourage crisper surfaces; 4) they also propose a kernel smoothing technique to address flickering issues in the optimized NeRFs. They conduct qualitatively ablation studies and the experimental results demonstrate the effectness of the proposed techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* From the presented experimental results (mainly qualitative results), the proposed techniques are effective and improve the performance over prior methods; \n* The ablation studies also demonstrate the effectiveness of individual technique;"
                },
                "weaknesses": {
                    "value": "* The experimental results are all qualitative results. It is good to have a metric/metrics to compare quantitatively against prior methods; For example, to measure the CLIP similarity between the text prompts and the generated contents; Otherwise, it is difficult to evaluate the performance since we can deliberately select good performing prompts over prior methods for comparisons. \n\n*  The results from Fantasia3D are also very impressive (i.e. in terms of texture quality and geometry) from Figure 4 and Figure 14. Can you provide more results to show that yours is better? I provide following text prompts from DreamFusion: 1) an orangutan making a clay bowl on a throwing wheel; 2) a raccoon astronaut holding his helmet; 3) a blue jay standing on a large basket of rainbow macarons; 4) a corgi taking a selfi; 5) a table with dim sum on it; 6) a lion reading the newspaper; 7) a tiger dressed as a doctor; 8) a chimpanzee dressed like Henry VIII king of England; 9) an all-utility vehicle driving across a stream; 10) a squirrel gesturing in front of an easel showing colorful pie charts. Can you do the comparisons with those prompts?\n\n* For the kernel smoothing, you only choose [1, 1, 1] as the sliding window kernel, have you tried other choices?"
                },
                "questions": {
                    "value": "* How is the kernel smoothing conducted for coarse-to-fine importance sampling? Could the authors provide more details? In fact, I do not understand \"kernel smoothing\". Use an equation to explain it would be very helpful. Figure 3 seems only presents the results with/without kernel smoothing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1424/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1424/Reviewer_CJyW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636009134,
            "cdate": 1698636009134,
            "tmdate": 1699636070726,
            "mdate": 1699636070726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ahe3uP45fl",
                "forum": "IZMPWmcS3H",
                "replyto": "5Ql08fTQIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to R3"
                    },
                    "comment": {
                        "value": "**Q: \u201cCan you provide more results to Fantasia3D with my given prompts from DreamFusion.\u201d**\n\nFor a comprehensive evaluation, both qualitative and quantitative, we obtained 30 text prompts from DreamFusion and conducted experiments using our method and Fantasia3D. We followed the training instructions from the official repository of Fantasia3D.\n\n**Qualitative comparison**: Additional visual comparisons to Fantasia3D were included in Fig.14-15 (10 samples) of the Appendix. We also added a comparison of the rendered videos (``fantasia3d.mp4\u201d, 30 samples) in the supplementary material. Notably, we observed a low success rate in the geometry generation stage using Fantasia3D, even with a higher number of training iterations. We attribute this to the increased difficulty of learning DMTet with the SDS loss, especially when compared to other implicit representations such as NeRFs.\n\n**Quantitative comparison**: We added a user study and computed the CLIP similarity for evaluation:\n\n**(1) User study**: We conducted a user study for our method and Fantasia3D. In the survey, we present rendered 2D images of the 30 generated 3D objects. Users are asked to choose the result that, in their opinion, exhibits overall better quality. Below, we report the macro-average rate of preference for each method across the 30 objects. The results indicate that our method achieves a higher preference than Fantasia3D in terms of overall quality.\n\n| Method 1 | Preference 2 | \n|----------|----------|\n| Fantasia3D | 9.7% | \n| Ours | 90.3% |\n\n\n\n**(2) CLIP similarity**: We also compare our method to Fantasia3D using CLIP-Similarity (\u2191). In this evaluation, we render 100 images for each generated object and compute the averaged CLIP similarity for the text-image pairs. We use the model \"openai/clip-vit-base-patch16\u201d for this evaluation. The scores show that our method achieves slightly better clip similarity than Fantasia3D.\n\n| Method 1 | CLIP-Similarity(\u2191) | \n|----------|----------|\n| Fantasia3D | 0.302 | \n| Ours | 0.344 |\n\nWe have incorporated these evaluations into the manuscript, highlighted in the red color.\n\n**Q: \u201cIt is good to have a metric/metrics to compare quantitatively against prior methods; For example, to measure the CLIP similarity\u201d**\n\nPlease refer to the answer to the above question.\n\n\n**Q:\u201cFor the kernel smoothing, you only choose [1, 1, 1] as the sliding window kernel, have you tried other choices?\u201d**\n\nIn addition to using the proposed kernel [1, 1, 1], we also experimented with alternative weighted kernels. For example, we set the kernel K = scale_k * [1, 6, 1], where scale_k represents a scale parameter, and [1, 6, 1] can be interpreted as the importance weights assigned to the current signal and its neighbors within the kernel window. We also varied the weights [1, 6, 1] to explore different proportions. Our experiments showed no significant differences. Notably, as the PDF estimated from the coarse stage has broad coverage in non-zero density regions, and the number of sampling points is fair (e.g., 36 points per ray, compared to 64-128 points used in prior works) during the refined stage, the rendered output does not contain flickering. \n\n\n**Q: \u201cHow is the kernel smoothing conducted for coarse-to-fine importance sampling? Could the authors provide more details?\u201d**\n\n**Motivation:**\n\nRecall that NeRFs usually use a hierarchical sampling procedure with two distinct MLPs, one \u201ccoarse\u201d and one \u201cfine.\u201d The coarse MLP captures the broad information of the scene, and the fine MLP captures detailed features. This is necessary for NeRF because the MLPs were only able to learn a model of the scene for one single scale (either the \"coarse\u201d or the \"fine\u201d scale) [1]. However, using two MLPs can be expensive. \n\n*Thus, how to use a single MLP that can learn multiscale representations of the scene?* To address this, we propose a kernel smoothing (KS) approach. The KS approach involves flattening the estimated density in the coarse stage, enabling it to capture a broad signal range of the scene.\n\n**Details:**\n\nSpecifically, the KS approach is a weighted moving average of neighboring signals. The weight is defined by a kernel. In practice, for each estimated weight point v_i in the coarse stage, we choose the kernel window as N and compute a weighted average for all signals within the kernel window:\n\n>v_i = \\frac{\\sum_{j=1}^{N} K_j \\cdot v_{i+j- \\lfloor \\frac{N}{2} \\rfloor}}{\\sum_{j=1}^{N} K_j}\n\nIn practice, we set K = [1,1,1].\n\n**Paper modification and code release**:\n\nWe have included a detailed explanation of the KS approach in Sec.4.2 of the manuscript. Additionally, we will provide code implementation with the final version.\n\n[1] Jon Barron et al., Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields, ICCV 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547141081,
                "cdate": 1700547141081,
                "tmdate": 1700547426886,
                "mdate": 1700547426886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kmkF6XQuCm",
            "forum": "IZMPWmcS3H",
            "replyto": "IZMPWmcS3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1424/Reviewer_WWao"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1424/Reviewer_WWao"
            ],
            "content": {
                "summary": {
                    "value": "- The paper proposes holistic sampling and smoothing approaches for high-quality text-to-3D generation in a single-stage optimization. \n- The method introduces a timestep annealing approach and regularization for the variance of z-coordinates along NeRF rays.\n- The paper also addresses texture flickering issues in NeRFs with a kernel smoothing technique.\n- Experiments show the method's superiority over previous approaches."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The single-stage optimization is useful to the generation of highly detailed and view-consistent 3D assets and the proposed solution to it is impressive.\n- Compared to baseline methods like Dreamfusion, Magic3D, and Fantasia3D, the rendered images from this approach exhibit enhanced photo-realism, improved texture details of the 3D assets, and more natural lighting effects."
                },
                "weaknesses": {
                    "value": "- The paper could benefit from a more explicit explanation in the introduction regarding why previous works were unable to achieve single-stage optimization.\n- The contributions presented in the paper seem fragmented, lacking a cohesive thread or central theme. It would enhance the paper's clarity and impact if the authors could refine the structure."
                },
                "questions": {
                    "value": "I believe the technical aspects are articulated clearly and technically sound. Thus, I have no further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698665963396,
            "cdate": 1698665963396,
            "tmdate": 1699636070641,
            "mdate": 1699636070641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VH7zSxQXOY",
                "forum": "IZMPWmcS3H",
                "replyto": "kmkF6XQuCm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to R2"
                    },
                    "comment": {
                        "value": "**Q: \u201cThe paper could benefit from a more explicit explanation in the introduction regarding why previous works were unable to achieve single-stage optimization.\u201d**\n\nThanks for the useful feedback. We modified the Sec.1 of the manuscript according to the reviewer\u2019s suggestion. Specifically, we added\n\n>``Moreover, generating a detailed 3D asset through single-stage optimization is challenging. Specifically, explicit 3D representations, such as meshes, struggle to capture intricate topology, such as objects with holes. Implicit 3D representations, such as NeRF, may lead to cloudy geometry and flickering textures.\u201c\n\n\n**Q: \u201cThe contributions presented in the paper seem fragmented, lacking a cohesive thread or central theme. It would enhance the paper's clarity and impact if the authors could refine the structure.\u201d**\n\nWe thank the reviewer for the useful feedback. We believe that the innovative idea of the paper is well-organized when viewed at a systemic level, aiming to enhance the entire text-to-3D generation system by refining its two essential components: **(1) representation** and **(2) supervision**. These improvements result from a synergistic fusion of various methods. As a result, our contributions stand as comprehensive improvements across every component of the system, incorporating nuanced ``look-like\u201d discrete details.\n\nSpecifically, within the framework of text-to-3D generation, we  enhance generation quality for representation and supervision:\n- For representation, we introduce effective regularizations and an advanced importance sampling approach in NeRFs.  \n- For supervision, we propose a novel training schedule and an advanced loss function for score distillation.\n\nWe reorganized our manuscript to enhance its structure and emphasize the contributions for the entire text-to-3D framework."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546805514,
                "cdate": 1700546805514,
                "tmdate": 1700546805514,
                "mdate": 1700546805514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t4PhQ3gHWK",
            "forum": "IZMPWmcS3H",
            "replyto": "IZMPWmcS3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1424/Reviewer_iKQ3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1424/Reviewer_iKQ3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an improved version of Score Distillation Sampling by introducing several strategies. The authors proposed to perform denoising in both image and latent space for better performance. A novel timestep annealing strategy is provided to reduce the sampling space. Besides, the authors also provide a z-coordinates regularization term to achieve high-quality rendering in a single-stage optimization. The paper is well-organized and easy to follow. The proposed strategies are effective to improve the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The strategy by denoising in both image and latent space is useful to improve the details.\n2. The proposed z-variance loss alleviates cloudy artifacts and shows better performance than distortion loss.\n3. The proposed method achieves high-quality text-to-3D generation."
                },
                "weaknesses": {
                    "value": "Basically the paper is good and I have several concerns:\n1. The contributions of the paper are discrete, which comprises several small contribution points.\n2. Maybe a user-study should be conducted to quantitatively evaluate the method.\n3. Basically the proposed strategies can be generalized to other baseline methods, for example, ProlificDreamer [1]. I\u2019m curious to see the performance with other baseline methods.\n\n[1] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., & Zhu, J. (2023). ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. arXiv preprint arXiv:2305.16213."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1424/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1424/Reviewer_iKQ3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744632779,
            "cdate": 1698744632779,
            "tmdate": 1699636070566,
            "mdate": 1699636070566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h2OUKBfqd5",
                "forum": "IZMPWmcS3H",
                "replyto": "t4PhQ3gHWK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to R1"
                    },
                    "comment": {
                        "value": "**Q: \u201cThe contributions of the paper are discrete, which comprises several small contribution points.\u201d**\n\nWe thank the reviewer for the useful feedback. We believe that the innovative idea of the paper is well-organized when viewed at a systemic level,: aiming to enhance the entire text-to-3D generation system by refining its two essential components: **(1) 3D representation** and **(2) optimization**. These improvements result from a synergistic fusion of various methods. As a result, our contributions stand as comprehensive improvements across every component of the system, incorporating nuanced ``look-like\u201d discrete details under a solid base insight.\n\nSpecifically, within the framework of text-to-3D generation, we  enhance generation quality for both representation and optimization:\n- For 3D representation, we introduce effective regularizations and an advanced importance sampling approach in NeRFs.  \n- For optimization, we propose a novel training schedule and an advanced loss function for score distillation.\n\nWe reorganized our manuscript to enhance its structure and emphasize the contributions for the entire text-to-3D framework.\n\n**Q: \u201cMaybe a user-study should be conducted to quantitatively evaluate the method.\u201d**\n\nFor a more comprehensive evaluation, both qualitative and quantitative, we obtained 30 text prompts from DreamFusion and conducted experiments using our method and Fantasia3D. We followed the training instructions from the official repository of Fantasia3D.\n\n**Qualitative comparison**: Additional visual comparisons to Fantasia3D were included in Fig.14-15 (10 samples) of the Appendix. We also added a comparison of the rendered videos (``fantasia3d.mp4\u201d, 30 samples) in the supplementary material. Notably, we observed a low success rate in the geometry generation stage using Fantasia3D, even with a higher number of training iterations. We attribute this to the increased difficulty of learning DMTet with the SDS loss, especially when compared to other implicit representations such as NeRFs.\n\n**Quantitative comparison**: We added a user study and computed the CLIP similarity for evaluation:\n\n**(1) User study**: We conduct a user study for our method and Fantasia3D. In the survey, we present rendered 2D images of the 30 generated 3D objects. Users are asked to choose the result that, in their opinion, exhibits overall better quality. Below, we report the macro-average rate of preference for each method across the 30 objects. The results indicate that our method achieves a higher preference than Fantasia3D in terms of overall quality.\n\n| Method | Preference | \n|----------|----------|\n| Fantasia3D | 9.7% |\n| Ours | 90.3% | \n\n\n**(2) CLIP similarity**: We also compare our method to Fantasia3D using CLIP-Similarity(\u2191). In this evaluation, we render 100 images for each generated object and compute the averaged CLIP similarity for the text-image pairs. We use the model \"openai/clip-vit-base-patch16\u201d for this evaluation. The scores show that our method achieves slightly better clip similarity than Fantasia3D.\n\n| Method | CLIP-Similarity (\u2191)| \n|----------|----------|\n| Fantasia3D | 0.302 |\n| Ours | 0.344 | \n\n\nWe have incorporated these evaluations into the manuscript, highlighted in the red color.\n\n\n**Q: \u201cThe proposed strategies can be generalized to other baseline methods, for example, ProlificDreamer.\u201d**\n\nWe have integrated the z-variance loss into ProlificDreamer, and the results are presented in Fig.19 in the appendix. Our observations indicate that adding the z-variance loss leads to sharper textures, highlighting the effectiveness and generalizability of the approach.\n\nAdditionally, our proposed latent- and image-space SDS loss has been incorporated in various image-to-3D [1] works, particularly in cases where image-space supervision is necessary. Our SDS loss has proven effective in addressing over-saturation issues in these contexts.\n\n[1] Huang et al., HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546697854,
                "cdate": 1700546697854,
                "tmdate": 1700546738461,
                "mdate": 1700546738461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]