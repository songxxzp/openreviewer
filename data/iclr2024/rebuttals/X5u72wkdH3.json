[
    {
        "title": "SYRAC: Synthesize, Rank, and Count"
    },
    {
        "review": {
            "id": "C1sQzhiFiy",
            "forum": "X5u72wkdH3",
            "replyto": "X5u72wkdH3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4186/Reviewer_QJht"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4186/Reviewer_QJht"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the unsupervised crowd counting task, a critical yet challenging task. To achieve this goal, the authors use latent diffusion models to create two types of synthetic data and then utilize the ranking image pairs for pre-training and fit a linear layer to the noisy synthetic images using these crowd quantity features. Experiments conducted on five datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(a) Using stable diffusion to generate the crowd dataset is a good idea, providing a new perspective for this area. \n(b) This paper is written well and easy to follow"
                },
                "weaknesses": {
                    "value": "1. For the fully supervised part, the authors only discuss the density-based crowd counting methods. In other words, many localization-based methods should be discussed, making the related work more comprehensive.\n\n2. The authors have pointed out that the prompt count is not reliable but using it as the GT count directly during the training phase. It makes me confused. I think it would be better to rank the generated 60 images using the pre-trained backbone first. Secondly, fine-tune the GT count according to the ranking results. Specifically, image A and image B are generated using the same prompt count 20. However, ranking results present that image A contains fewer persons than image B, so the GT count of image A could be fine-tuned to be smaller than the GT count of image B.\n\n3. I understand that the input of the generation process is complete images without cropping, but the inference process uses image patches as input. There may be resolution gaps. How about cropping the original images into patches in the generation process instead?\n\n4. There is a lack of quantitative analysis about the reliability of the generation process. Specifically, the authors can sample n source images for generation and statistics on the percentage of images where the objects were successfully removed.\n\n5. The authors think the ranking information is reliable, and the prompt count is relatively unreliable. Thus, the authors pre-train the backbone using the ranking information and freeze the backbone during the training phase to resist the prompt count noise. I agree that the ranking information is more reliable. However, I am not sure it is necessary to fix the backbone as only fine-tuning the linear layer may limit the learning potential on the prompt count, which is considered ground truth. There could be an ablation study on fine-tuning the backbone during the training phase.\n\n6. Since the current method is still significantly lower than CrowdCLIP, the authors think the early stop used in CrowdCLIP might be unfair. So I would like to know the performance under early stop.\n\n7. The motivation to synthesize the ranking crowd image is still unclear since one can utilize the existing datasets to generate the ranking image pairs, like CrowdCLIP."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571355081,
            "cdate": 1698571355081,
            "tmdate": 1699636384891,
            "mdate": 1699636384891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xintbCFS34",
                "forum": "X5u72wkdH3",
                "replyto": "C1sQzhiFiy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer QJht"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valued insights. We would like to address some of their concerns:\n1. **Counting vs. Detection:** We appreciate this insight. We would be happy to expand our discussion to emphasize how crowd counting is a different task than detection and thus can rely on different features, for two reasons: 1) Counting is a more relaxed task than detection in that the former does not require the localization of objects like the latter, 2) counting may leverage features that respond to multiple quantities of objects (not only to one object) in different spatial constellations, e.g. clusters of pedestrians, whereas in object detection, the features delineate a single object. Additionally, typical object detection datasets will have around 1 to 20 objects in an image. Crowd counting datasets tend to have an average of more than 100 pedestrians per image, and density based methods tend to outperform detection based methods in these scenarios.\n2. **Ranking Noisy Synthetic Counts:** The goal of our synthetic ranking pre-training strategy is to mitigate label noise by narrowing down the feature set to those specifically linked to crowd quantity before initiating fine-tuning with the noisy synthetic counting data. While we could pseudo-label the noisy synthetic data using the ranking network, the ranking network may itself be unreliable due to a domain shift between the synthetic ranking dataset and the noisy synthetic counting dataset.\n3. **Cropping before Synthesis:** We did run experiments where we cropped a portion of a source image and generated a synthetic ranking pair that way, similar to your idea. However, this performed poorly in practice. We suspect that was due to the way that pedestrians are distributed in crowd counting images, resulting in many crops that had no crowds in them. As an example, the perspective of an image might cause the top half of the image to contain only sky or buildings or other unrelated features while the bottom half contains the crowd. This setup would result in comparing a real image with no crowds to a synthetic image with no crowds, this potentially impacts feature learning.\n4. **Reliability of Ranking Labels:** We agree that it would be good to provide a more concrete analysis of the reliability of the synthetic ranking data. We manually inspected 50 ranking pairs from each dataset to estimate the accuracy. For ShanghaiTechB, there were no incorrectly ranked examples. For ShanghaiTechA there was 1 incorrectly ranked example. For QNRF there were 2 incorrectly ranked examples. And, for JHU++, there were 2 incorrectly ranked examples. Additionally, we use a pre-trained crowd counting model (DM-COUNT [1]) to estimate the crowd count for 500 real and synthetic ranking pairs to provide an additional estimate of the reliability of the synthetic ranking data. The estimated accuracy is 99.6% for ShanghaiTechB, 99.2% for ShanghaiTechA, 97.2% for QNRF, and 90.2% for JHU++. These additional experiments corroborate the idea that the synthetic ranking data is highly reliable.\n5. **Fine-tuning Backbone:** We agree that it would be useful to have an ablation study examining different strategies for tuning the pre-trained network with the noisy counting data. We did run experiments where we included the noisy synthetic counting data during pre-training with the synthetic ranking data. However, that did not achieve good results.\n6. **Comparison to CrowdCLIP:** Our method outperforms CrowdCLIP on 2 of the 4 datasets. We had considered using early stopping in a similar fashion to CrowdCLIP. However, during our discussion with the authors of CrowdCLIP (which we have documented for reference), we had found that they used the **test** set for early stopping. We could not justify this as an experiment, as it would seriously contaminate the results. \n7. **Motivation for Synthetic Ranking:** We do utilize existing datasets to generate the ranking pairs, as those images are the source of the synthetic ranking data. Methods like CrowdCLIP generate ranking pairs by performing \u201cintra-image ranking\u201d. In Table 3 we demonstrate that intra-image ranking underperforms compared to our synthetic ranking strategy.\n\n[1]. Wang, Boyu, et al. \"Distribution matching for crowd counting.\" Advances in neural information processing systems 33 (2020): 1595-1607."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245270580,
                "cdate": 1700245270580,
                "tmdate": 1700245270580,
                "mdate": 1700245270580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iOcqDWI7vq",
                "forum": "X5u72wkdH3",
                "replyto": "C1sQzhiFiy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Reviewer_QJht"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Reviewer_QJht"
                ],
                "content": {
                    "comment": {
                        "value": "1. Are the results chosen from the latest epoch?\n\n2. See the log from CrowdCLIP, and we can find that CrowdCLIP might not utilize early stops since all models are trained on 100 epochs."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636481384,
                "cdate": 1700636481384,
                "tmdate": 1700636502256,
                "mdate": 1700636502256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B870DbXlB2",
                "forum": "X5u72wkdH3",
                "replyto": "C1sQzhiFiy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Reviewer_QJht"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Reviewer_QJht"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\nI would like to know the following information:\n\n(1) Are the results chosen from the latest epoch in the SYRAC?\n\n(2) I have checked the log of CrowdCLIP. However, if we also choose a fixed epoch (e.g., 50) and then utilize the final epoch as the final result, their results are still higher than SYRAC. For example, the 50th epoch's result is 150 MAE of CrowdCLIP on Shanghai PartA, significantly better than the proposed method. This is the reason why I ask the problem (1).\n\n(3) The training epoch is a hyperparameter. I think that the authors utilize 40 epochs for training, which might also based on experiential tuning."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700762483,
                "cdate": 1700700762483,
                "tmdate": 1700700778351,
                "mdate": 1700700778351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tzaPPFH5CY",
            "forum": "X5u72wkdH3",
            "replyto": "X5u72wkdH3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4186/Reviewer_rbvA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4186/Reviewer_rbvA"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an unsupervised counting method that utilizes latent diffusion models to create synthetic data. The approach involves two unsupervised techniques: first, removing pedestrians from actual images, resulting in ranked image pairs that provide a ranking loss of object quantity. Second, generating synthetic images with a predetermined number of objects, which gives a noisy but related counting label."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of utilizing a stable model to generate synthetic images seems feasible.\n- The paper introduces two strategies: a weak but reliable object quantity signal and a strong but noisy counting signal. This approach seems quite reasonable, as it can potentially complement and enhance the model's performance."
                },
                "weaknesses": {
                    "value": "- What is the rationale behind the setting of N, which is the crowd count to generate synthetic images? What is the quality of the generated images? Is it possible to provide a measure of variance to assess the feasibility of this method? \n- There are only six categories for N. Why not train the model by a classification task? In situations where the labels are not stable, the classification task seems to be able to maintain a relatively high level of accuracy.\n- The synthetic images do not include images with 0 crowd count. Does this method have the capability to handle datasets that consist of a large portion of (background) images with no people, such as NWPU?\n- How does the computational cost of generating synthetic images using the diffusion model compare to that of other unsupervised counting models?\n- There are some repetitions in the references."
                },
                "questions": {
                    "value": "-  Figure 6 illustrates that the features exhibit an underlying crowd-count-based ordering. However, it would be more convincing if features from supervised counting models could be provided for comparison.\n- In Table 3, the methods proposed in the paper actually include ImageNet pretraining. What is the performance when combining ImageNet pretraining with intra-image ranking?\n- Does the ranking loss merely train the model to distinguish between real and synthetic images?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793825814,
            "cdate": 1698793825814,
            "tmdate": 1699636384816,
            "mdate": 1699636384816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VO4NqfiOQ5",
                "forum": "X5u72wkdH3",
                "replyto": "tzaPPFH5CY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer rbvA"
                    },
                    "comment": {
                        "value": "We are grateful for your comprehensive feedback on our paper. We would like to address the points that you have raised.\n1. **Quality & Settings for Noisy Synth Images:** To assess the quality + variance of our synthetic crowd counting image dataset, we've included a table below. The table reports the quality by using a fully-supervised model to estimate, for each synthetic image, the count and compares it to the desired count dictated by the prompt. The table reveals noise across count predictions for each count category. Despite this noise, the average predicted count closely aligns with the given count in the prompt.  \nWith regards to the count category selections in N, we used the following rationales for setting N =  {1, 5, 10, 20, 50, 100, 200}\n    - **Exponential:** We select values from small to large in an approximately exponential fashion. We chose 200 as a maximum because we use patching to split the image into regions with smaller counts (<200) during inference.\n    - **Computational Cost:** We avoid generating images from a more granular count range (i.e. 1, 2, 3, 4, \u2026, 200) due to the increased computational cost this would incur. \n   - **Simplicity:** Our aim wasn't to optimize for specific prompts. While there might exist combinations of N that yield better performance, determining these without introducing validation data, thereby adding supervision, is challenging. We see this as a strength of our method, as the selection of N is based on simple heuristics, and yet it still works. \n2. **Regression vs. Classification:** The count categories chosen for N = {1, 5, 10, 20, 50, 100, 200} represent a subset of counts among positive unbounded integers. Given the nature of our task, predicting counts intuitively aligns with regression rather than classification. We did consider different methods for fitting the pre-trained features to the noisy crowd counting data, including classification. We found that nothing performed better than linear regression. However, it\u2019s possible that we did not explore this sufficiently and there may be a formulation where classification outperforms regression. \n3. **Zero Count Images:** The synthetic images do include images with 0 crowd count. We didn\u2019t explicitly show this in Figure 1. However, we do explain our strategy for generating images with 0 crowd count in the last two sentences of section 3.2.\n4. **Computational Cost:** The question of computational cost is an important one, as generating synthetic data does incur an additional computational cost beyond the burden of training the network. We generated all synthetic data on a single GPU, and we also trained our models on a single GPU.\n5. **Repititions:** Thank you, noted.\n6. **Comparison to Full-Supervised Features:** This is a great point, thank you! We included Figure 6 to corroborate the idea that the pre-training features capture a quantity based distribution. However, it would be useful to further explore that distribution and compare it to features learned by different methods with different supervision budgets. We expect that more supervision would lead to similar but better organization of the latent space with regards to object count.\n7. **ImageNet Pre-training:** In fact, we already leverage ImageNet pre-trained features for intra-image ranking. In Table 3, the performance for intra-image ranking is achieved by starting from ImageNet pre-trained features.\n8. **Real vs. Synth Spurious Correlations:** This is a critical question. Deep learning performance normally suffers due to spurious correlations, and so we expect that our method may be prone to these spurious correlations, as well. However, we rely on the following evidence to suggest that our method produces a network which learns something coherent beyond the separation of real from synthetic:\n    - Our method outperforms other unsupervised methods, and our pre-training strategy outperforms other pre-training strategies.\n    - Figure 6 demonstrates a quantity based distribution.\n    - There is significant domain overlap between the synthetic and real distributions, as the synthetic images are generated using real images as a source. We provide a measure of this domain overlap in the supp. materials.\n    - In sec 5, we mention the use of a non-negativity constraint in the final layer of the network (+ ReLU activation), which prevents the weights and output from becoming negative. Thus, we expect that it will be harder for the network to exploit spurious correlations, as it cannot set the output of the network to be negative when synthetic features are present.\n\nThat said, we could explore ways to further ensure that we mitigate learning features that distinguish real from synthetic images by producing ranked pairs of images where both images in each pair are synthetic (e.g. remove objects then remove some more; or by using SD to synthesize images then removing objects from those synthetic image), or by using domain adversarial learning."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244536353,
                "cdate": 1700244536353,
                "tmdate": 1700244536353,
                "mdate": 1700244536353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AI7IvW0KS3",
            "forum": "X5u72wkdH3",
            "replyto": "X5u72wkdH3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4186/Reviewer_98cL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4186/Reviewer_98cL"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an unsupervised crowd counting approach based on synthetic data. Specifically, it generates synthetic data through stable diffusion with a selected prompt and then employs the rank loss and a count loss for prediction. The excellent experimental results demonstrate the advantages of the proposed unsupervised method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea is novel and the experimental results demonstrate the advantages of the proposed unsupervised method."
                },
                "weaknesses": {
                    "value": "A deep analysis of the experimental results is not provided."
                },
                "questions": {
                    "value": "1. Could the author provide a more comprehensive explanation for Figure 6? Both small and large counts are distributed throughout the entire space in the QNRF dataset. It is difficult to interpret the UMAP results without any explanation.\n2. What is the advantage of generating synthetic data via stable diffusion, especially when compared with the large synthetic dataset GCC[1]? Both approaches are label-free, but GCC contains more detailed count and localization information. Additionally, [1] achieves better counting performance than the proposed method when there are no human-labeled annotations. It would be helpful to clarify the specific advantages of this paper.\n3. Although this is an unsupervised method, it would be valuable to understand whether the pre-training phase performs as expected. The authors could randomly select pairs of images from SHA/SHB/QNRF to determine accuracy or probability and analyze cases in which it failed. Furthermore, the accuracy should be compared with a similar method presented in Liu et al[2].\n4. The impact of patch size is only presented in the table. Could the authors provide a deeper analysis and discussion on the reasons for the observation that different patch sizes lead to different performance?\"\n\n[1] Wang, Qi, et al. \"Learning from synthetic data for crowd counting in the wild,\" CVPR, 2019.\n[2] Liu, Xialei, et al. \"Leveraging unlabeled data for crowd counting by learning to rank,\" CVPR, 2018"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4186/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4186/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4186/Reviewer_98cL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819466004,
            "cdate": 1698819466004,
            "tmdate": 1699636384742,
            "mdate": 1699636384742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1cMi1MCXrj",
                "forum": "X5u72wkdH3",
                "replyto": "AI7IvW0KS3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer 98cL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their useful feedback and helpful questions. We would like to address the points that the reviewer has raised.\n1. **Explanation for Figure 6.:** Figure 6 illustrates the UMAP representation of the training images based on the pre-trained synthetic ranking features, with points color-coded according to the logarithm of the ground truth count. While interpreting dimensionality reduction results can be complex, we included this figure to demonstrate that there is a coherent quantity-based structure to the latent space learned during the pre-training step. We can interpret this qualitatively by noticing clustering of similarly colored points, which suggests that images with similar counts tend to have similar features.\n2. **GCC vs. SYRAC:** There are several advantages to using synthetic data generated by Stable Diffusion even when GCC already exists.\n    - **Labor Burden:** producing 3D synthetic counting data is not labor free, despite not requiring manual annotations. It requires acquiring new 3D assets for each new object category whereas our method can extend to any novel category that is recognized by Stable Diffusion. And, it requires designing complex 3D scenes as well as environmental conditions such as rain or limited light. Our approach does not carry this burden.\n     - **Domain Shift:** GCC exhibits a significant domain shift when compared to real crowd counting datasets, given that it is produced from 3D assets. Our method maintains a smaller domain shift by using real images as a seed for the synthetic ranking images and also due to the fact that Stable Diffusion is trained using natural images. We have added details to the supplementary material where we provide a measure of domain overlap between GCC and the synthetic ranking datasets, which demonstrates that our synthetic ranking data is closer to the real distribution than the GCC dataset.\n     - **Different Requirements:** While we currently underperform when compared to GCC based methods, we are also working on a fundamentally different problem. GCC requires human labor to produce, whereas our method does not. GCC provides density map annotations, whereas ours does not. Finally, methods that utilize GCC typically focus on domain adaptation due to the large domain gap between GCC and real datasets, whereas our method produces data that is already close to the true distribution.    \nGiven these reasons, we believe our paper serves as a valuable contribution to the research community\n3. **Comparison with Liu et al.:** We do include intra-image ranking as a pre-training strategy in Table 3, which is the self-supervised component of their work. It is worth clarifying that Liu et al. do not perform self-supervised crowd counting, as their method relies on fully-supervised crowd counting data as the primary training signal. In row 3 of Table 3, we only include the results from the self-supervised component of Liu et al.\u2019s method, which demonstrates that our method consistently outperforms intra-image ranking as a pre-training signal for each dataset.\n4. **Patch Size:** To understand the impact of patch size, we consider two points:\n     - **Label Noise:** There is significant noise in the true underlying count for synthetic crowd counting data produced by Stable Diffusion. This noise increases as the expected count embedded within the prompt increases (see table below). Thus, we expect that prediction noise will be concentrated around images with larger underlying counts. To counter this, we adopt patching during inference, aiming to reduce the object count within each patch. Our findings (Table 2) indicate that increased patching benefits higher count datasets, improving performance by mitigating noise. However, this strategy may compromise images with smaller crowd counts, as larger objects get fragmented across patches. This was apparent in a performance drop among those images.\n     - **Limited Representation for Dense Crowds:** The synthetic ranking generation process often removes many objects in dense crowds, resulting in limited ranking examples for densely crowded images. Consequently, our model might struggle to learn features specific to highly dense crowds. Splitting these images into smaller patches during inference may align them with a distribution where the model performs more effectively."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242893350,
                "cdate": 1700242893350,
                "tmdate": 1700242893350,
                "mdate": 1700242893350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZKH9TXmvdm",
                "forum": "X5u72wkdH3",
                "replyto": "AI7IvW0KS3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Table for Noisy Synthetic Crowd Counting statistics"
                    },
                    "comment": {
                        "value": "The table below is produced by using DM-Count [1], which is trained on the NWPU dataset (with full supervision using GT counts), to estimate the count in each noisy synthetic counting image. We report the mean, StD, max, and min count across all synth images. The predicted count can differ significantly from the prompt count for images with higher prompt counts. This suggests that more label noise in larger crowd counts. \n| Prompt Count | Mean   | StD    | Min | Max |\n|--------------|--------|--------|-----|-----|\n| 1            | 0.5   | 1.0   | 0   | 6   |\n| 5            | 1.9   | 1.4   | 0   | 5   |\n| 20           | 18.8  | 11.3  | 2   | 78  |\n| 50           | 71.9  | 59.3  | 10  | 353 |\n| 100          | 150.1 | 118.5 | 19  | 763 |\n| 200          | 245.8 | 93.6  | 37  | 484 |\n\n[1]. Wang, Boyu, et al. \"Distribution matching for crowd counting.\" NeurIPS 2020: 1595-1607."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242917629,
                "cdate": 1700242917629,
                "tmdate": 1700242917629,
                "mdate": 1700242917629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "87WzoilwkZ",
                "forum": "X5u72wkdH3",
                "replyto": "1cMi1MCXrj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4186/Reviewer_98cL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4186/Reviewer_98cL"
                ],
                "content": {
                    "title": {
                        "value": "GCC vs. SYRAC"
                    },
                    "comment": {
                        "value": "1. SYRAC can extend to any novel category, but this paper does not show it. This paper is only for crowd counting.\n2. GCC contains domain shift, but it can provide detailed localization annotation. The domain shift is addressed well in many publications following GCC. SYRAC is close to the real domain. However, it cannot provide localization information. I also noticed that SYRAC cannot provide images containing dense crowds, i.e, thousands of humans, which is the major scene in crowd counting.\n3. I did not see any human labor that GCC requires. Could the author give me some details about human labor to avoid that I ignore some details?\n4. About the performance. The author should notice that the counting model trained on GCC performs better than yours even if the domain adaptation method is not applied:\n|pre-train dataset| SHA (MAE/MSE)| SHB(MAE/MSE) | QNRF(MAE/MSE)|\n|:-:|:-:|:-:|:-:|\n| SYRAC | 196.0 / 295.2 | 49.0 / 60.3 | 390.0 / 697.5 |\n| GCC (No-Adpt) | 160.0 / 216.5 | 22.8 / 30.6 | 275.5 / 458.5 |\n| GCC (GAN-Adpt) | 123.4 / 193.4 | 19.9 / 28.3 | 230.4 / 384.5 |\nexperimental results come from [1].\n -----\n\nBased on the above, I think SYRAC has the following parts that should be improved:\n1. Extending to any novel category. If you say this is an advantage, you should show it in the paper;\n2. SYRAC can provide localization information and generate images containing thousands of humans freely;\n3. The performance is better than GCC without adaptation.\n\nI think the response will be recognized if any of the above points is achieved.\n\n----\n[1] Wang, Qi, et al. \"Learning from synthetic data for crowd counting in the wild,\" CVPR, 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375561675,
                "cdate": 1700375561675,
                "tmdate": 1700375561675,
                "mdate": 1700375561675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]