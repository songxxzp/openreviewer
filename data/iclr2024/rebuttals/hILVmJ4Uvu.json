[
    {
        "title": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning"
    },
    {
        "review": {
            "id": "MsrafnBNRd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU"
            ],
            "forum": "hILVmJ4Uvu",
            "replyto": "hILVmJ4Uvu",
            "content": {
                "summary": {
                    "value": "The paper introduces TWOSOME, a novel finetuning method designed to ground Large Language Models (LLMs) in embodied environments. TWOSOME comprises three main components: action-selection, where the LLM identifies the best action from a list of actions on criteria such as likelihood, token normalization, and word normalization, LoRA updates and prompt design. The authors conducted empirical experiments using TWOSOME on four fine-tuning tasks within Overcooked/VirtualHome environments. Their results demonstrate that TWOSOME's word normalization component outperforms others. Moreover, the fine-tuned policies exhibit superior generalization to unseen tasks in zero-shot scenarios. Remarkably, TWOSOME's capabilities in NLP benchmarks remain unaffected after the fine-tuning process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors noticed the limitation of considering plain action likelihood (a problem known in NLP when selecting different texts based on likelihood of the full sentences). To my knowledge, the word normalization is new and I really liked the idea when noticing that dividing by the sequence length is not enough to mitigate the end of words being more likely.\n- The authors worked in a compute budgeted setting so the work can be reproduced easily.\n- I really liked the task generalization section.\n- Results on showing little to no catastrophic forgetting on NLP tasks (possibly coming from LoRA?) are really promising."
                },
                "weaknesses": {
                    "value": "- As for now it is hard to consider LoRA to be part of the proposed method: hard to see the benefits of the LoRA contribution to sample efficiency and no catastrophic forgetting without an ablation on with and without LoRA. \nThe contribution of the method seems to be the action normalization and some prompt design (qualitatively assessed by the authors in 4.4).  Could you add experiments on these points? \n\n- Paper mentions that (Carta et al, 2023) \u201cfocus on primitive actions in toy environments without rich semantics\u201c. I am not sure actions are primitive in (Carta et al, 2023) but rather semantically described actions, e.g. \u201cgo forward\u201d, \u201cturn left\u201d, \u201cdrop and toggle\u201d. What makes them more \u201cprimitive\u201d seems to be the environment they chose for experimenting (BabyAI vs Overcooked/VirtualHome). \n\n- Regarding the choice of environments. I am afraid there are too few fine-tuning tasks. I might be wrong, but the task generalization seems almost too-good from 4 tasks only. BabyAI was procedurally generated which would have enabled experimentation on more finetuning tasks."
                },
                "questions": {
                    "value": "- Can you elaborate on the difference between (Carta et al, 2019) and unnormalized action selection.\n- Can you explain the PPO baseline: state/action space and architecture and initialization here (and in the main paper)? This would help in understanding the contribution of pretrained initialization.\n- Discussion question (not required to increase my score): \n     - Do you have insights on the use of embodied environments to **improve** LLMs, e.g. dealing with safety concerns?  How to design such environments?\n    - It would be nice to see how multi-modal text generation could be used to remove the assumption that the agent can extract a textual description of its state."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3575/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU",
                        "ICLR.cc/2024/Conference/Submission3575/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697457060726,
            "cdate": 1697457060726,
            "tmdate": 1700401414243,
            "mdate": 1700401414243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b6PffQzuE1",
                "forum": "hILVmJ4Uvu",
                "replyto": "MsrafnBNRd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for their time and comments. Please see the above official comment to all the reviewers first and then our answers to your questions. We hope our answers will clear up the doubts about our work, and please let us know if there is any other clarification we can provide.\n\n*** \n\nQ1: As for now it is hard to consider LoRA to be part of the proposed method: hard to see the benefits of the LoRA contribution to sample efficiency and no catastrophic forgetting without an ablation on with and without LoRA. The contribution of the method seems to be the action normalization and some prompt design (qualitatively assessed by the authors in 4.4). Could you add experiments on these points?\n\nA1: Thanks for your valuable comments. **We want to point out that we use LoRA mainly for parameter efficiency instead of sample efficiency.** Even using LoRA, TWOSOME still has about 500M parameters to update. In contrast, the PPO network only has about 10K parameters to update, which is 50,000X less than TWOSOME. But TWOSOME still exhibits much better sample efficiency than PPO with the same amount of data, we attribute this to the prior knowledge of LLM, instead of LoRA. \n\nAs mentioned in the first point of the above official comment, in the section of **Parameter-efficient Fine-tuning Architecture**, one of our main baselines, GLAM requires 8 NVIDIA A100 80GB GPUs to train their framework with a much smaller LLM, Flan-T5 780M without using LoRA. They report using 18880 GPU hours on Nvidia A100 80GB in total. Our model is 9X larger than theirs. **We estimate that at least 32 NVIDIA A100 80GB GPUs are required to conduct the experiments to train LLaMA-7B without using LoRA, otherwise, there is even no way to load the model and do a complete backpropagation.** Training LLaMA-7B without using LoRA is significantly far beyond most research labs\u2019 ability, including us. It also shows the importance of our non-trivial parameter-efficient architecture, which enables the whole framework to run in a single A100 40G GPU. It greatly lowers the threshold for researchers to conduct the following research.\n\n*** \n\nQ2: Paper mentions that (Carta et al, 2023) \u201cfocus on primitive actions in toy environments without rich semantics\u201c. I am not sure actions are primitive in (Carta et al, 2023) but rather semantically described actions, e.g. \u201cgo forward\u201d, \u201cturn left\u201d, \u201cdrop and toggle\u201d. What makes them more \u201cprimitive\u201d seems to be the environment they chose for experimenting (BabyAI vs Overcooked/VirtualHome).\n\nA2: Yes, you are correct. That is one of the main reasons why we do not think BabyAI is a suitable environment to exhibit the performance of LLM-based agents. **We argue that LLM-based agents should be deployed in environments as close to daily life or their learned specific domains as possible, like SayCan with high-level daily skills (household robotics).**  Then they can make full use of the prior knowledge learned in the training process. If LLMs are not familiar with the observation and actions, not only will they fail to apply their learned prior knowledge but also dramatically slow down the training process for their tremendous parameters. It also emphasizes the importance of prompt design. \n\nAs we mentioned in the above official comment, both the observation and action in BabyAI have relatively poor semantics. Here is an example prompt used by GLAM in BabyAI, \u201cYou carry a blue ball, You see a wall 4 steps forward, You see a wall 2 steps left, You see a grey key 1 step right and 1 step forward, You see a red box 2 steps forward\u201d. It is rare for LLMs to see a corpus like this in the training process and we all know that even GPT-4 struggles with simple navigation tasks. It might partially explain why GLAM spent as much as 18880 GPU hours on Nvidia A100 80GB to get the results. \nLLM-based agents should not be applied to solve tasks in these environments under primitive settings. They are exactly the strong areas of RL agents. This is the reason why we insist on deploying our framework in daily environments with rich semantics for observation and actions.\n\n***"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074360634,
                "cdate": 1700074360634,
                "tmdate": 1700074360634,
                "mdate": 1700074360634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rfT3JSHWRc",
                "forum": "hILVmJ4Uvu",
                "replyto": "7KlpkWZ7Uz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU"
                ],
                "content": {
                    "title": {
                        "value": "Final remarks."
                    },
                    "comment": {
                        "value": "**Framework.** I have a hard time convincing myself that the proposed method is a \"framework\". I consider the method contribution to be only the action normalization (and prompt design). The LoRA part cannot be considered part of the method as its contribution is not evaluated.  That said, I totally understand that LoRA was the only solution to finetune larger models given limited compute budgets. I won't ask for extra experiments and am happy with the current ones but probably be careful with the formulation of a \"framework\". I also believe the model/critic architecture is not really ablated so hard to consider it part of the method. As for now, without more ablations on both LoRA and mode/critic, I think a \"framework\" is not a good formulation and the authors could improve their writing to make LoRA an important experimental detail that is required to finetune larger pre-trained models.\n\n**Environments.** I understand that BabyAI is a poorer environment in terms of semantics compared to overcooked/virtualhome, however my main point was that the choice of overcooked/virtualhome comes at the expense of the number of finetuning tasks (which could easily be procedurally generated). \"BabyAI is a relatively easy environment\" is that true for all scenarios? Couldn't you come with hard scenarios? Once again I understand that this would become more compute intensive (for which I ask a justification later). \n\n**Computational consumption of experiments.** Could you please describe why finetuning on downstream tasks is compute intensive? What is the main bottleneck (slowness of the environment, computing actions with a large model...)? I don't know how overcooked/virtualhome are implemented but I would expect the inference of a 7B model not be too long compared to the environment steps."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235864112,
                "cdate": 1700235864112,
                "tmdate": 1700235864112,
                "mdate": 1700235864112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Thrw6TnKVU",
                "forum": "hILVmJ4Uvu",
                "replyto": "JudR2NsECP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Reviewer_HnZU"
                ],
                "content": {
                    "title": {
                        "value": "Final comment"
                    },
                    "comment": {
                        "value": "Thanks for the last answers. It seems sample inefficiency on decision-making tasks (as compared to classical RLHF) prevents doing larger experiments under reasonable budgets. I increased my score to 6: I am in favor of this work to presented at a conference but think the experimental results are not enough (due to budget constraints) to make it a ground-breaking paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401672624,
                "cdate": 1700401672624,
                "tmdate": 1700401672624,
                "mdate": 1700401672624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tf3kjrWanD",
            "forum": "hILVmJ4Uvu",
            "replyto": "hILVmJ4Uvu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_fSyb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_fSyb"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides TWOSOME (True knoWledge cOmeS frOM practicE), a Large Language Model (LLM)-based policy learning method that fine-tunes a LLM agent with a Reinforcement Learning (RL) algorithm. To address problems incurred when adopting text actions sampled from a LLM policy, this paper proposes a normalization method that divides the log probability of a text action by the number of words (or tokens). This paper demonstrates the effectiveness of TWOSOME by employing Llama-2-7B as a LLM agent and fine-tuning the LLM agent with Proximal Policy Optimization (PPO). This paper provides experiment results in Overcooked and VirtualHome, and shows that the LLM agent fine-tuned with PPO can learn better policy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- S1. The main approach of fine-tuning a LLM agent (e.g., Llama-2-7B) with a RL algorithm (e.g., PPO) proposed in this paper seems interesting and promising. This approach can be seen as applying recent prevalent reinforcement learning with human feedback (RLHF) into decision-making tasks.\n\n- S2. This paper provides promising initial results toward fine-tuning LLM agents with RL methods."
                },
                "weaknesses": {
                    "value": "- W1. The overall approach of TWOSOME is to fine-tune a LLM agent (e.g., Llama-2-7B) with a RL algorithm (e.g., PPO). And, the unique contribution of TWOSOME can be seen as providing a normalization method for the probability distribution over text-based actions sampled from a LLM policy. This paper proposes two normalization methods: token normalization (Eq. 3) and word normalization (Eq. 4). Even though this paper proposes a promising direction towards fine-tuning a LLM agent with a RL algorithm, the main contribution seems rather marginal. The normalization method is about dividing the log probability of a varying length text-based action by the number of words (or the number of tokens). It seems to compensate a text action with larger number of words, but I am not sure that this kind of simple division is a proper way to normalize varying length text actions sampled from a LLM policy. \n \n- W2. This paper shows that TWOSOME can learn the optimal policy in a simple environment like Overcooked. However, I am not sure that TWOSOME can properly learn optimal policy in a complex environment like VirtualHome. When showing the performance of TWOSOME in VirtualHome, the authors masks out unrelated actions to reduce the complexity of the action space."
                },
                "questions": {
                    "value": "- Q1. Regarding W1, how does the word normalization help LLM agents to learn better policy? What is the intuition of the word normalization?\n\n- Q2. Regarding W2, without manual action masking in VirtualHome, how much scores can TWOSOME achieve?\n\n- Q3. Recently, in RLHF, some enhanced RL algorithms such as Direct Policy Optimization (DPO) and Pair-wise Proximal Policy Optimizaition (P3O) have been proposed. Can these enhance RL algorithms be applied to solving decision making tasks? And then, can we expect some performance improvements of TWOSOME?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3575/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3575/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3575/Reviewer_fSyb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698993246546,
            "cdate": 1698993246546,
            "tmdate": 1699636312230,
            "mdate": 1699636312230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IJw3TtiMVu",
                "forum": "hILVmJ4Uvu",
                "replyto": "Tf3kjrWanD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for their time and comments. Please see the above official comment to all the reviewers first and then our answers to your questions. We hope our answers will clear up the doubts about our work, and please let us know if there is any other clarification we can provide.\n\n\n*** \n\nQ1. How does the word normalization help LLM agents to learn better policy? What is the intuition of the word normalization?\n\nA1: For the intuition of the word normalization, we provided a detailed explanation in Section 4.1 in the main paper. Briefly speaking, **word normalization can provide a more accurate and reasonable initialized policy for the LLM agents to interact with environments**. So reasonable good actions, no matter whether they are long or short, always have higher probabilities to be sampled at the beginning of the training process, which would greatly accelerate the exploration process.\n\nWe observe that the probability of each token is always less than 1. Therefore, longer action prompts tend to have lower token-level probabilities, even though the longer action prompts may be more reasonable in the current situation. So we propose the token normalization to normalize the joint probability by the number of tokens (length). \n\nFor example, if we have two actions, the first action has 20 tokens. To to simple, let the probability of each token in this action be as same as 0.8, which means this action is well-written and highly related to the task and the current observation. However, the joint probability of the action is 0.8^20 = 0.011. In contrast, the second action has only two tokens, with a probability of 0.2 for each token,  which means this action is much less relative to the task and the observation, and even the relation between these two tokens is quite weak. The joint probability of the second action is 0.2 * 0.2 = 0.04. Though each token in the first action is much higher than the second action, the first action finally has much fewer chances to be chosen, compared to the second action, which is not a desirable result. With the use of the token/word normalization by taking the lengths of the action into consideration, the joint probability of the action is still 0.8 and the joint probability of the second action is still 0.2 The distribution is much more reasonable. And the first action has a higher probability of being sampled.\n\nHowever, we find the token normalization is slightly excessive. We observe that if a word is divided into several tokens, the first token usually has a relatively low probability, while the probabilities of the rest of the tokens tend to be remarkably high, which are often almost close to 100%. Therefore, it is more reasonable to regard the several tokens made up of one word as an integrated symbol. Then we propose our word normalization method to normalize the joint probabilities of actions by the number of words. Our experiments also demonstrate the same conclusion that TWOSOME without normalization is very unstable and fails to learn the optimal policy. Both TWOSOME with token normalization and word normalization have much better performance. And TWOSOME with word normalization converges much faster than token normalization since token normalization is slightly excessive.\n\nIn Appendix F, we also provide a detailed comparison and analysis of the probabilities generated by TWOSOME without normalization and with token/word normalization across all the tasks. \n\n\n***"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074108935,
                "cdate": 1700074108935,
                "tmdate": 1700074108935,
                "mdate": 1700074108935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0hYlUB74UI",
                "forum": "hILVmJ4Uvu",
                "replyto": "Tf3kjrWanD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q2. Even though this paper proposes a promising direction towards fine-tuning a LLM agent with a RL algorithm, the main contribution seems rather marginal. It seems to compensate a text action with larger number of words, but I am not sure that this kind of simple division is a proper way to normalize varying length text actions sampled from a LLM policy.\n\nA2: Thanks for your valuable comments.  We kindly remind the reviewer that the contribution of the whole framework may be underestimated. There are some previous similar works like SayCan and GLAM, but we managed to polish and optimize every module in the framework, resulting in high parameter-efficiency and better generalization and sample-efficiency. Please refer to the first point of the above official comment, **About Technical Contribution**, for a clear explanation.  \n\nFor the word normalization, we have introduced it in the previous question, Q1. We would not claim that our solution is the ultimate solution to the issue, but it turns out to be an easy-to-implement and feasible way to alleviate the issue, which is well acknowledged by the other three reviewers. Our method has shown significant improvement including both performance and stability according to the corresponding experiments and ablation studies. \n\nNot like the traditional natural language generation task where the output can be as long as several hundred tokens. The actions in traditional decision-making tasks are typically composed of a verb-object structure (e.g. pick up the apple), or Verb + Object + Preposition structure (e.g. put the apple on the table). They are usually within 20 tokens. Our experiments have covered the most common cases, which show that even in this range, the length of the prompts still has a significant influence on the joint probability of the actions and introduces much instability to the training process. \n\n*** \n\nQ3. I am not sure that TWOSOME can properly learn optimal policy in a complex environment like VirtualHome. When showing the performance of TWOSOME in VirtualHome, the authors masks out unrelated actions to reduce the complexity of the action space.\n\nA3: Sorry for the confusion. We intend to use action masking to prove that **compared to traditional RL methods, like PPO, it is more convenient for our method to mask actions and solve the tasks with a large action space**. When doing action masking, the network of PPO still needs to calculate the values of the masked actions, but we just do not use their values to form the policy in the final step. However, for our method, we just directly delete the masked actions from the action prompt list in the input, which also saves the computation for the values of these masked actions. Furthermore, it is also worth noting that our framework is also very flexible in extending the action space, by just directly add new actions to the action prompt list as the input, which is impossible for traditional RL methods. They need to train new agents from scratch with the new extended action space.\n\nMoreover, we argue that struggling with large action space has been one of the main challenges for all decision-making methods, which is not a unique weakness of our method. Action masking is a very common trick to deal with large action space in decision-making tasks. The action space of all the environments is also a subset of the action space of reality, where these environments also mask out unrelated actions to reduce the complexity. It is fairly reasonable to mask invalid actions and unrelated items in the action space. \n\nFinally, we want to kindly remind the reviewer that even with the use of action masking, the difficulty of VirtualHome might be underestimated. Because we adopt a sparse reward setting, only when the agent successfully finishes the task will it receive +1 reward, as shown in Figure 5, even PPO with action masking, still fails to solve the task of Entertainment. In contrast, our method, TWOSOME with word normalization manages to learn the optimal policy, which already shows that our method has the ability in a complex environment. \n\n*** \n\nQ4. Recently, in RLHF, some enhanced RL algorithms such as Direct Policy Optimization (DPO) and Pair-wise Proximal Policy Optimizaition (P3O) have been proposed. Can these enhance RL algorithms be applied to solving decision making tasks? And then, can we expect some performance improvements of TWOSOME?\n\nA4: To our understanding, compared to RLHF, both DPO and P3O attempt to directly use the preference data to train LLMs, saving the effort of training an accurate and explicit reward model or value function.  However, in the traditional decision-making setting, the reward function is already contained in the environment. We can directly use rewards to train our models instead of using preference pairs to infer the reward/value distribution. Therefore, DPO and P3O do not help in our standard RL decision-making settings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074259390,
                "cdate": 1700074259390,
                "tmdate": 1700706642515,
                "mdate": 1700706642515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VHNkQFlPDy",
                "forum": "hILVmJ4Uvu",
                "replyto": "Tf3kjrWanD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer fSyb:\n\nWe just added some new results in Appendix G.2 (on the last page of the paper), to show that **even without action masking, our method, TWOSOME with word normalization, can still manage to learn the optimal policy**, which shows the superior robustness and performance of our method and demonstrates that our method has the ability to deal with large action space.   \n\nWe hope these new results can solve the reviewer's doubts. As the deadline for the rebuttal period is approaching, is there any other clarification we can provide?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553580793,
                "cdate": 1700553580793,
                "tmdate": 1700557919692,
                "mdate": 1700557919692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L3xLXyuAmy",
                "forum": "hILVmJ4Uvu",
                "replyto": "RR5E5RcOH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Reviewer_fSyb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Reviewer_fSyb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing thoughtful responses to my comments. It helped me to understand this work more concretely. I will reiterate my review once again."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741876880,
                "cdate": 1700741876880,
                "tmdate": 1700741876880,
                "mdate": 1700741876880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6IF891np6q",
            "forum": "hILVmJ4Uvu",
            "replyto": "hILVmJ4Uvu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_Xize"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_Xize"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies using a large language model (LLM) as a decision-making agent to interact with an embodied environment. Specifically, the authors proposed to train the policy with reinforcement learning (RL). First, the policy is formed by querying the LLM with each feasible action in a given state. Then, token normalization and word normalization approaches are proposed to stable the training. The proposed approach is evaluated on Overcooked and VirtualHome. The experiments show that the proposed approach outperforms baselines in terms of sample efficiency and return."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality and Significance:   \nThe reviewer appreciates the motivation of the paper: Classical RL agents align well with a given environment but fail to leverage prior knowledge. On the other hand,  LLM has numerous prior knowledge while often fails to align with the given environment. This paper proposed a method to align the LLM policy with the environment and show improvement over classic RL methods.  \n\nQuality:  \nThe paper is technically sound and the claims are supported by experiments. \n\nClarity:    \nThis paper is generally well-organized and easy to follow. Some minor improvement may be required. Please see the Question section for details."
                },
                "weaknesses": {
                    "value": "1. The idea of querying joint probabilities of each valid action with LLM to form a policy is explored in previous works such as GLAM (Carta et al., 2023). The reviewer found, in the current version, the credit is not clearly given to authors of previous works. \n\n2. The reviewer has some concerns on the baselines used in the experiments. In Figure 5, aside from the ablations, the authors only consider classic PPO as a baseline. It seems unfair to directly compare an agent equipped with LLM with a classic trial-and-error PPO agent which doesn\u2019t have access to the prior knowledge in LLM. There are many existing works that leverage LLM to form RL policies [1, 2, 3]. Particularly,  Li [1] also uses virtualHome as test environments. Comparing approaches that also leverage knowledge in LLM could make the experimental section more convincing. \n\n3. The technical contribution of the paper is somewhat limited. Specifically, the idea of forming valid policy is proposed by previous works and LoRA for fine-tuning is a standard method. The review found the main new method is the proposed normalization approach. That to be said, the reviewer still considers the combination of existing methods might be valuable to our embodied AI community. \n\n[1] Pre-Trained Language Models for Interactive Decision-Making, Li et al., NeurIPS 2022\n\n[2] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, Ahn, CoRL 2022. \n\n[3] Grounding large language models in interactive environments with online reinforcement learning, Cartra et al, 2023"
                },
                "questions": {
                    "value": "1. As discussed in Weakness, comparing with methods that also leverage LLM for RL agent could make the experimental section more convincing.  \n\n2. In the experiments, only two tasks from virtualHome and Overcooked are considered. Reporting results on more tasks would be helpful. \n\n3. In Section 4.3, it reads \u201cThe critic\u2019s MLPs use the last token of the observation prompt as input \u2026\u201d. Could you elaborate why only the last token of the observation prompt is used? Shouldn\u2019t the MLP use the output of the frozen LLM?\n\nMinor:    \n  4. In Eq (2), using $a_k$ in the summation is confusing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699158426886,
            "cdate": 1699158426886,
            "tmdate": 1699636312152,
            "mdate": 1699636312152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ne4NFpV8TU",
                "forum": "hILVmJ4Uvu",
                "replyto": "6IF891np6q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for their time and comments. Please see the above official comment to all the reviewers first and then our answers to your questions. We hope our answers will clear up the doubts about our work, and please let us know if there is any other clarification we can provide.\n\n\n*** \n\nQ1: The idea of querying joint probabilities of each valid action with LLM to form a policy is explored in previous works such as GLAM (Carta et al., 2023). The reviewer found, in the current version, the credit is not clearly given to authors of previous works.\n\nA1: Thanks for pointing this out. **To our best knowledge, it is SayCan that first proposed to use joint probabilities of each valid action to form a policy**. And GLAM also acknowledges this point. Here is an excerpt from GLAM in Page 5, \u201cFinally, Ahn et al. [2022] proposed to directly use the LLM to compute the (log) probability of each action by computing the conditional probability of each token in action given the prompt p\u201d, where Ahn et al. [2022] refers to SayCan. We also mention this in our 4.1 Section that the way we used to form the policy is very similar to SayCan without the affordance function. We will add GLAM behind SayCan, since it also uses the same way to form the policy.\n\n*** \n\nQ2: The reviewer has some concerns on the baselines used in the experiments. There are many existing works that leverage LLM to form RL policies [1, 2, 3]. Comparing approaches that also leverage knowledge in LLM could make the experimental section more convincing.\n\n[1] Pre-Trained Language Models for Interactive Decision-Making, Li et al., NeurIPS 2022  \n[2] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, Ahn, CoRL 2022.  \n[3] Grounding large language models in interactive environments with online reinforcement learning, Cartra et al, 2023  \n\nA2: Thanks for your pointing out. Please refer to the third point of the above official comment, **About Baselines Selection**, for a clear explanation. **Actually, we do compare our method with SayCan[2] and GLAM[3]**. Because we need to make some modifications and apply them to our tasks and domains, and we found that they also served as our ablation studies, for consistency, we call SayCan \u201cTWOSOME without finetuning\u201d and GLAM \u201cTWOSOME without action prompt normalization\u201d or directly \u201cTWOSOME without normalization\u201d. \n\n**As for [1], it is actually a purely supervised-learning-based method**. Instead of explicitly calculating the probabilities of each action to form standard RL policies, they directly generate actions using LLMs.  The loss they show in Equation 1, is exactly the same as the autoregressive loss. They just formulate the loss into a policy-like format. There is no difference between autoregressive supervised learning and imitation learning if actions are the only output of LLMs. And the authors also do not claim they are an RL-based method. In their paper, they also rely on pre-collected high-quality expert datasets. We do not think that they directly compete with our method. We can first apply their methods to do offline training if provided with a high-quality dataset, and then use our method to do online training by interacting with the environments. \n\n\n*** \n\nQ3: The technical contribution of the paper is somewhat limited. Specifically, the idea of forming valid policy is proposed by previous works and LoRA for fine-tuning is a standard method. The review found the main new method is the proposed normalization approach. That to be said, the reviewer still considers the combination of existing methods might be valuable to our embodied AI community.\n\nA3: We appreciate that the reviewer acknowledges our contribution to the embodied AI community. Please refer to the first point of the above official comment, **About Technical Contribution**, for a clear explanation.  \n\n\n*** \n\n\nMinor Q4:  In Eq (2), using ak in the summation is confusing.\n\nA4: We greatly thank the reviewer for catching the typo. We have corrected it in the latest version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073999364,
                "cdate": 1700073999364,
                "tmdate": 1700705712087,
                "mdate": 1700705712087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1oHS0eiDAb",
            "forum": "hILVmJ4Uvu",
            "replyto": "hILVmJ4Uvu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_FDRX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3575/Reviewer_FDRX"
            ],
            "content": {
                "summary": {
                    "value": "- TWOSOME is a general online framework that allows LLMs to be used as decision-making agents in embodied environments.\n\n- TWOSOME uses a two-stage approach to align LLMs with environments: first, it queries the joint probabilities of each valid action with LLMs to form behaviour policies; and second, it enhances the stability and robustness of the policies using normalisation methods and prompt design principles.\n\n- TWOSOME exhibits significantly better sample efficiency and performance compared to other methods in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome.\n\n- TWOSOME shows superior generalization ability to unseen tasks and does not result in a significant loss of the LLMs' original ability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n\n- Proposes a novel framework, TWOSOME, for aligning LLMs with embodied environments.\n\n- TWOSOME is a general framework that can be applied to a wide range of decision-making tasks.\n\n- TWOSOME exhibits significantly better sample efficiency and performance compared to other methods.\n\n- TWOSOME shows superior generalization ability to unseen tasks.\n\n- TWOSOME does not result in a significant loss of the LLMs' original ability.\n\n\nOverall, the paper proposes a promising new framework for aligning LLMs with embodied environments. The framework is well-motivated and the experimental results are good."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n- The paper does not provide a theoretical analysis of the proposed framework.\n\n- The paper does not evaluate the performance of TWOSOME on a wider range of tasks and environments.\n\n- The paper does not discuss the potential limitations of the proposed framework.\n\n- The paper does not introduce anything novel, rather just combines existing components together to generalise to new tasks (apart from word normalisation which is fairly trivial). \n\n- The paper does not compare to other LLM fine tuning baselines which generalise to new unseen tasks."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699535667256,
            "cdate": 1699535667256,
            "tmdate": 1699636312078,
            "mdate": 1699636312078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "25W83LMg5J",
                "forum": "hILVmJ4Uvu",
                "replyto": "1oHS0eiDAb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for their time and comments. Please see the above official comment to all the reviewers first and then our answers to your questions. We hope our answers will clear up the doubts about our work, and please let us know if there is any other clarification we can provide.\n\n*** \n\nQ1: The paper does not provide a theoretical analysis of the proposed framework.\n\nA1: Thanks for pointing it out. We indeed do not provide much theoretical analysis of the proposed framework because the whole framework is still basically under the traditional online actor-critic (PPO) framework, where the convergence of PPO should also apply. Though LLMs are usually transformer-based architecture, the last layer of LLMs is still an MLP layer to map the values generated by Transformer blocks to the probabilities of each token in the vocabulary.  We use the values to form the policies, which have no essential difference from the traditional RL agents, which usually have some CNN/MLP layers and use the output of the last MLP layer to form the policies. Finally, LoRA only updates part of the parameters, which may indeed prevent RL agents from learning the optimal policies. Empirically, LoRA shows superior performance not only in our work but also in tremendous CV/NLP works. We agree that it is non-trivial to provide a theoretical analysis of LoRA, especially in RL settings, but it is beyond the scope of this work.\n\n*** \n\nQ2: The paper does not evaluate the performance of TWOSOME on a wider range of tasks and environments.\n\nA2: Thanks for your valuable comments. We fully understand the reviewers\u2019 wishes to see more results in different scenarios. Please refer to the second point of the above official comment, **About Evaluations on More Tasks and Environments** , for a clear explanation.  \n\n*** \n\nQ3: The paper does not discuss the potential limitations of the proposed framework.\n\nA3: Thanks for pointing it out. Due to the limitation of pages, we do not provide a detailed analysis of the potential limitations. However, **we do mention our limitations in the final Discussion and Conclusion Section**, \u2018However, our method suffers a major limitation that training a PPO agent from scratch is far faster and cheaper than finetuning an LLM. TWOSOME needs to feed all the valid actions to the LLMs for every action sampling, resulting in multiple times the amount of computation and a small batch size.\u2019  Another limitation is that because we use an LLM as the backbone of our framework, we need to convert the observation into text and feed it into LLM, which inevitably causes information loss. The future work will be replacing the LLM with a VLM to directly use the image as input. We will put them in a separate section called Limitations and provide a more detailed discussion in the camera-ready version as there is more space provided.\n\n\n*** \n\nQ4: The paper does not introduce anything novel, rather just combines existing components together to generalise to new tasks (apart from word normalisation which is fairly trivial).\n\nA4: Thanks for your valuable comments. We kindly remind the reviewer that the contribution of the whole framework may be underestimated. There are some previous similar works like SayCan and GLAM, but we managed to polish and optimize every module in the framework, resulting in high parameter-efficiency and better generalization and sample-efficiency. Please refer to the first point of the above official comment, **About Technical Contribution**, for a clear explanation.  \n\n\n*** \n\nQ5: The paper does not compare to other LLM fine tuning baselines which generalise to new unseen tasks.\n\nA5: Thanks for your valuable comments. To our best knowledge, only GLAM uses RL to train LLMs in embodied environments, which also serves as one of our main baselines. For the other supervised-learning-based methods, we do not think that they directly compete with our method. We can first apply their methods to do offline training if provided with a high-quality dataset, and then use our method to do online training by interacting with the environments. Please refer to the third point of the above official comment, **About Baselines Selection**, for a clear explanation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073908120,
                "cdate": 1700073908120,
                "tmdate": 1700073908120,
                "mdate": 1700073908120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]