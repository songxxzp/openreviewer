[
    {
        "title": "Learning Graph Representation for Model Ensemble"
    },
    {
        "review": {
            "id": "TWQ8ZWZxKl",
            "forum": "3t57X1Fvaf",
            "replyto": "3t57X1Fvaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_3xiQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_3xiQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores an ensemble strategy for neural networks, which exploits a graph NN to perform the final prediction. The key steps of the algorithm are: (1) they build a graph where the nodes are the models and the edges depend either on the overlap across predictions of the networks, or on their architectural similarity; (2) they apply a GNN on top of this graph; (3) they optimize a loss composed of three terms (a cross-entropy, a KL divergence for \"diversity\", and a Laplacian smoothness term). They show some results on some simple tabular datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The only \"strength\" I can think of is that the topic is interesting, and I like the idea of introducing a graph relation across models to improve the ensemble. However, the way the idea is executed in the paper is not acceptable for any scientific conference. Even addressing all questions below, I still feel the scientific novelty would not warrant presentation at ICML."
                },
                "weaknesses": {
                    "value": "There are many weaknesses and questions so I will focus on the key ones.\n\n1. Claims made in the paper are excessive and the paper should be significantly rewritten, e.g., \"groundbreaking approach\", \"general-purpose learning systems\", \"foundational framework\", \"self-adaptation\", \"a foundational paradigm for creating adaptable machine learning systems\", etc. The method is an ensemble technique for neural networks, and it does not deviate too much from other ensemble / merging / blending strategies. Abstract and introduction should reflect this.\n\n2. Always from the introduction, \"The foremost challenge emanates from their resource-intensive nature\", but the model does not address this since running the graph requires running the entire ensemble. \"by selecting the optimal subset\" is another sentence that is not justified since the graph is constructed deterministically and not trained. \"overcome limitations found in both dedicated single-purpose models and multipurpose models tailored for specific tasks\" is yet another example of how claims are excessive, since the model must be retrained for each task.\n\n3. Many key parts of the method are unclear or completely left out of the paper. These include:\n\n(a) \"Characteristic Connectivity Function\" should measure the similarity between models but it seems too restrictive (e.g. a model with 2 conv layers and 1 dense layer against a model with 1 conv layer and 2 dense layers would have a CCF of 0?). \n\n(b) The authors are never stating what is the input of the GCN (predictions? embeddings from the last layer?).\n\n(c) Are the original models fine-tuned in the last stage? Are they frozen?\n\n(d) \"Kullback-Leibler Divergence\" in (4) should guarantee the \"diversity\" of the models, but it is the KL divergence computed on what? The predictions of the models?\n\n(e) All the components are never benchmarked in isolation (e.g., do you need the spanning tree? Does the Laplacian smoothness term improves the results? Etc.)\n\n(f) Algorithms for the comparison are very standard ML models. There is a huge literature on ensembling / merging neural networks which is only briefly touched in the paper and never benchmarked.\n\n(g) The results are questionable. They only use toy datasets, and in some cases the results are very strange (e.g., Titanic score can easily get > 80% with any model, see https://www.kaggle.com/code/alexisbcook/titanic-tutorial). Their models have sometimes > 20% absolute improvement against strong baselines such as random forest.\n\n(h) Theorems 1 and 2: these prove convexity of the loss (with respect to the predictions). But this is just a combination of cross-entropy and KL, and the overall model would be non-convex anyway since we are using neural networks. The entire analysis here is lacking value."
                },
                "questions": {
                    "value": "The questions follow more or less the drawbacks:\n\n3a: provide more details on how the connectivity is built.\n\n3b: provide the precise equations or structure of the model.\n\n3c: clarify the training process in pseudo-code.\n\n3d-e: provide clear ablation studies on all components of the model.\n\n3f-g: add more baselines and datasets to the experimental comparison, and carefully check the results and hyper-parameters of all models.\n\n3h: remove the \"theoretical analysis\" if unneeded."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697544493222,
            "cdate": 1697544493222,
            "tmdate": 1699636180098,
            "mdate": 1699636180098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "OTS9C5xBAD",
            "forum": "3t57X1Fvaf",
            "replyto": "3t57X1Fvaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_oYx9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_oYx9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a learning strategy called LGR-ME for model ensemble. The authors define a characteristic connectivity function and a performance connectivity function to calculate the degree of connection between models. Then, a graph is constructed by the calculation and maximum spanning tree (MST) is extracted from the graph. MST is used to derive the Laplacian loss based on the connectivity between models. The total loss is defined as the sum of the cross-entropy loss, KL divergence loss, and Laplacian loss."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "It is hard to find strentghs of this paper."
                },
                "weaknesses": {
                    "value": "1. The citation format is incorrect. When the authors or the publication are not included in the sentence, \"\\citep\" should be used.\n\n2. The detailed explanation is severely lacking. For example, how is the similarity between two model specifications calculated? How is the similarity between two model outputs calculated?\n\n3. The proposed method is only compared with outdated techniques (e.g., Random Forest, SVC, and MLP). Moreover, it should be compared with ensemble strategies, not classifiers.\n\n4. The authors state, \"We introduce, LGR-ME (Learning Graph Representation for Model Ensemble), a groundbreaking approach within the domain of general-purpose learning systems.\" However, the authors only evaluate their methods for classification on simple datasets."
                },
                "questions": {
                    "value": "Q1. How is the similarity between two model specifications calculated?\n\nQ2. How is the similarity between two model outputs calculated?\n\nQ3. Could the authors compare their method with other ensemble learning methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2442/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2442/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2442/Reviewer_oYx9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769119596,
            "cdate": 1698769119596,
            "tmdate": 1699636179995,
            "mdate": 1699636179995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "zB9xXuREiw",
            "forum": "3t57X1Fvaf",
            "replyto": "3t57X1Fvaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_WLfx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_WLfx"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the model ensemble using Graph Neural Networks (GNNs). Specifically, they regard each model (specification) in the pool as node. Also, they define two connectivity functions to construct the edges. And then, they employ a graph neural network (GCNN) to  learn complex dependencies among various models, and ultimately utilizing a fully connected layer to yield the final learning output aligned with the user\u2019s desired task. Experiments verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "[+] The idea of formulating model ensemble is interesting.\n[+] Extensive experiments are performed to verify the effectiveness of the proposed method.\n[+] The codes are provided for reproducing the results."
                },
                "weaknesses": {
                    "value": "[-] Are the model specifications adequate to describe the models? For instance, the authors claimed that they use Si = {conv = 1, pool = 1, att = 0, bn = 0, dr = 1} to represent that the model Mi has a convolution layer, a pooling layers, no attention layer, no batch normalization layer, and has a dropout layer. However, a plenty of important information (the dimension of each layer, the dropout ratio, and etc.) of the model is ignored.         \n          \n[-] In the experiments, how does the  model specifications of other models (Random Forest, Gradient Boosting, and etc.) look like? It is extremely important to provide these experimental details.            \n \n[-] The authors only compare with single model. But, can the proposed method perform better than advanced ensemble methods [1]? Especially, some recent works on model fusion [2,3] exhit overwhelming superiority over previous methods.   \n\n[-] I notice that the authors only adopt the Performance Connectivity Function (PCF) to construct the edges. So, is it necssary to define the Characteristic Connectivity Function (CCF) in the paper?  Actually, CCF would be an empty set when the two models (e.g., Random forest and MLP) are completely different.             \n\n[-] The authors adopt 3-layer GNNs in the experiments. However, there are only 7 nodes (models) in the experiments. The well-known oversmoothing issue of GNNs would degrade the performance given that the deep GNN layer and few nodes.                  \n   \n[-] The presentation id not clear. For example, I suggest that the authors to provide detailed formulations of $C(x)$ , $D(x)$, and $L_{laplace}$ after Eq. (4).              \n\n[-] Please use the symbols consistently. For example, in Figure 1, is Model1 the same as M1? The $\\mathcal{Y}$ in Eq. (1) should be same as $Y$ in Eq. (3).                     \n\n[-] The citation style of the paper is wrong. I expect the authors can use \\citep{} and \\citet{} correctly.             \n                                    \n[-] The quotation marks (e.g., \u2019best of both worlds.\u2019) in this manuscript are wrong.        \n\n\n[1] A Survey on Ensemble Learning under the Era of Deep Learning (arXiv:2101.08387)                         \n[2] Model Fusion via Optimal Transport (NeurIPS 2020)                      \n[3]  Git Re-Basin: Merging Models modulo Permutation Symmetries (ICLR 2023)"
                },
                "questions": {
                    "value": "1. Are the model specifications adequate to describe the models? For instance, the authors claimed that they use Si = {conv = 1, pool = 1, att = 0, bn = 0, dr = 1} to represent that the model Mi has a convolution layer, a pooling layers, no attention layer, no batch normalization layer, and has a dropout layer. However, a plenty of important information (the dimension of each layer, the dropout ratio, and etc.) of the model is ignored.              \n     \n2. In the experiments, how does the  model specifications of other models (Random Forest, Gradient Boosting, and etc.) look like? It is extremely important to provide these experimental details.             \n\n3. he authors only compare with single model. But, can the proposed method perform better than advanced ensemble methods?     \n\n4. Is it necssary to define the Characteristic Connectivity Function (CCF) in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820161265,
            "cdate": 1698820161265,
            "tmdate": 1699636179913,
            "mdate": 1699636179913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "rYln8pVBBS",
            "forum": "3t57X1Fvaf",
            "replyto": "3t57X1Fvaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_eLAm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2442/Reviewer_eLAm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an ensembling method where a number of models are trained for a specific task and then a graph of these models is created based on similarity in model specification and output. Then a GCN is trained on a maximum spanning tree of this graph to produce the final model."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Ensembling can be important for improving the performance of machine learning models and some regularizations like dropout implicitly perform ensembling."
                },
                "weaknesses": {
                    "value": "There are several things that are vague and confusing about this submission.  First the loss function does not make sense to me. If the loss is  L(x) = C(x) +\\lambda D(x) + \\gamma L_laplace\nThen the D(x) KL divergence would be minimized rather than maximized which is what the authors state they want for diversity. The Laplacian of the graph would not change unless the underlying graph changes so it is unclear what adding this to the loss function does. \n\nThe writing of this manuscript is also very unusual with the formating of Section 3.4 being very unclear. \n\nThe results are also confusing because most of them are not trying to ensemble generic ML models. For example random forest only contains a set of decision trees, and KNN classifiers are not an ensemble method. I don't see any comparisons to an ensemble of deep learning or other machine learning models."
                },
                "questions": {
                    "value": "Can you explain the loss functions and experimental setup better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2442/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835394423,
            "cdate": 1698835394423,
            "tmdate": 1699636179838,
            "mdate": 1699636179838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]