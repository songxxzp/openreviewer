[
    {
        "title": "Taming AI Bots: Controllability of Neural States in Large Language Models"
    },
    {
        "review": {
            "id": "kdJvKihxjy",
            "forum": "X2gjYmy77l",
            "replyto": "X2gjYmy77l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_vfen"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_vfen"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of controlling LLMs. In general, the authors seek to find whether an arbitrary well-formed output can be generated from a well-trained LLM.\n\nThe authors characterize LLMs as discrete-time dynamical systems, moving in the token or embedding space. On top of this low-level representation space, the authors consider \"meaning\" spaces based on discriminators.\n\nOverall, given some assumptions about the surjective nature of LLMs within the set of meaningful sentences, the authors conclude that well trained LLMs are controllable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "## Originality\nThis work appears relatively novel, considering the important problem of LLM controllability and bringing insight from dynamical systems literature to characterize controllability.\n\n## Quality\nFor the most part, this is a very carefully written theory paper that lays out important assumptions and characteristics for notions of controllability.\n\n## Clarity\nThe paper was largely clear. I tend to favor figures or diagrams, but the first figure (which I found helpful) only appears in the appendix. I confess that I am typically more of an empiricist than a theorist, but I am familiar with LLMs, dynamical systems, and the math presented in the paper, which I was able to follow.\n\n## Significance\nI am undecided about the significance of this work for one particular reason: Postulate 1. The authors are considering a very important problem that many people care about - if a paper can establish that real-world LLMs that we use are controllable, that would be an extremely important paper. The problem, in my mind, is that to reach this conclusion, the authors make a very big assumption about well-trained LLMs in Postulate 1. To their credit, the authors note this limitation, both in the main text and Appendix 1. Overall, I fear that the strength of the assumption in Postulate 1 could significantly limit the significance of this work."
                },
                "weaknesses": {
                    "value": "## Postulate 1: \nAs noted earlier, the biggest weakness of this work is the strong assumption in Postulate 1 about the bijective/invertible nature of the map $F_w$. To me, this assumption almost gives away the controllability conclusion: if there is an invertible map, then it at least intuitively follows very clearly how such a system would be controllable.\n\n## Presentation of results:\nI recognize that the primary contribution of this work is theoretical, rather than empirical, in nature. However, the authors do conduct experiments and present results in Appendix A. Even while keeping results in appendices, I would encourage the authors to at least provide a sentence or two in the main paper highlighting high-level trends of the experiment results.\n\nI wish to emphasize again that I traditional study more empirical AI methods and analysis. Thus, if other reviewers find that this work would contribute to the more theory-focused community, I am happy to change my mind."
                },
                "questions": {
                    "value": "1. In Section 3, the authors describe how, \"For a sufficiently high temperature parameter $T$...\" random sequences can be generated from any start token. This is true for any $T > 0$, right (barring numerical precision errors from very small numbers)? I do not see why one would need a larger value of $T$ for this theoretical claim (although I recognize that in practice, higher $T$ would \"spread out\" faster to cover more random sequences).\n\n2. As stated earlier, my main concern about this paper is Postulate 1. What evidence or arguments do the authors have supporting this assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6458/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421423314,
            "cdate": 1698421423314,
            "tmdate": 1699636722250,
            "mdate": 1699636722250,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hMIOpwq45u",
                "forum": "X2gjYmy77l",
                "replyto": "kdJvKihxjy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q:** *I am undecided about the significance of this work for one particular reason: Postulate 1. The authors are considering a very important problem that many people care about - if a paper can establish that real-world LLMs that we use are controllable, that would be an extremely important paper. The problem, in my mind, is that to reach this conclusion, the authors make a very big assumption about well-trained LLMs in Postulate 1. To their credit, the authors note this limitation, both in the main text and Appendix 1. Overall, I fear that the strength of the assumption in Postulate 1 could significantly limit the significance of this work.*\n\n**A:** Indeed, we acknowledge that Postulate 1 is a limitation of this paper. However, the assumption are actually not that strong: that current models are well-trained according to our definition can be tested by measuring  perplexity, and qualitatively one would be hard-pressed to argue that at least some current LLMs generate sentences that are statistically distinguishable from human-generated training data, which presumably was written to convey some meaning. We could have made the claim seemingly rigorous by enumeration arguments in the style of old theory papers, so the conclusion would be formally proven. However, we are unsatisfied with this kind of argument which is why we state the proposition as a postulate. By doing so, we  we hope to open a discussion, rather than close it, on this topic that we concur is an important one.\n\n**Q:** *I recognize that the primary contribution of this work is theoretical, rather than empirical, in nature. However, the authors do conduct experiments and present results in Appendix A. Even while keeping results in appendices, I would encourage the authors to at least provide a sentence or two in the main paper highlighting high-level trends of the experiment results.*\n\n**A:** Thank you, will do.\n\n**Q:** *I wish to emphasize again that I traditional study more empirical AI methods and analysis. Thus, if other reviewers find that this work would contribute to the more theory-focused community, I am happy to change my mind.*\n\n**A:** We view ours as more of a position paper, since it opens -- rather than closing -- a key question. We view its contribution as framing the problem in a specific technical language, that of control theory,  which has a rich history that may yield insights for further analysis.\n\n**Q:** *In Section 3, the authors describe how, \"For a sufficiently high temperature parameter ...\" random sequences can be generated from any start token. This is true for any , right (barring numerical precision errors from very small numbers)? I do not see why one would need a larger value of  for this theoretical claim (although I recognize that in practice, higher  would \"spread out\" faster to cover more random sequences).*\n\n**A:** Yes, we could just say $T>0$. Thanks for pointing that out.\n\n**Q:** *As stated earlier, my main concern about this paper is Postulate 1. What evidence or arguments do the authors have supporting this assumption?*\n\n**A:** The fact that there exist LLMs that are well-trained according to our definition should not be very controversial, as one could be convinced by simply interacting with some of the best models, or measuring their perplexity on any benchmark validation set. There are also quantitative empirical studies establishing that the best models generate sentences that are indistinguishable from human-generated, with high-frequency/probability. As for the rest of the claim, that follows from the complexity of the trained map coupled with the finiteness of the context. Also empirical evidence is the practice of jailbreaking and adversarial prompting, as well as the massive effort that goes into preventing such in the area of Responsible AI.  However, we recognize that even taking all this into account still does not make a proof.  In our view, though, the framing of the problem in the language of controls still would be beneficial to the community."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6458/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692092704,
                "cdate": 1700692092704,
                "tmdate": 1700692092704,
                "mdate": 1700692092704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m1zIjTo4mC",
            "forum": "X2gjYmy77l",
            "replyto": "X2gjYmy77l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_gqJX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_gqJX"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to define \"meaning\" in the context of LLMs, use this definition to characterize well-trained models, and establish the conditions for controllability of LLMs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper sets out to formally characterize controllability of LLMs which is an important issue in preventing adversarial attacks on language models and preventing LLMs from producing undesirable content."
                },
                "weaknesses": {
                    "value": "It's unclear what the contribution of the paper is. The related work section does not connect this paper to specific prior work (only citing two survey papers). Then, most of the paper is spent discussing preliminaries and introducing notation and definitions. The first use of the proposed definitions to make a non-trivial claim is in Section 5 with Postulate 1, but this claim is not justified. Theorem 1 in Section 5 seems to follow directly from the proposed definition of controllability. Theorem 2 is presented with no intuition and the proof in the appendix is only for a special case. It's also not explained how Theorem 2 justifies the main conclusion: that \"a prompt engineer aided by enough time and memory can force an LLM to output an arbitrary sequence of \u2113 tokens.\""
                },
                "questions": {
                    "value": "See weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6458/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6458/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6458/Reviewer_gqJX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6458/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742313581,
            "cdate": 1698742313581,
            "tmdate": 1699636722133,
            "mdate": 1699636722133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cM9XJBejbJ",
                "forum": "X2gjYmy77l",
                "replyto": "m1zIjTo4mC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer gqJX"
                    },
                    "comment": {
                        "value": "**Q:** *It's unclear what the contribution of the paper is. The related work section does not connect this paper to specific prior work (only citing two survey papers)*\n\n**A:** The contribution is to have framed the question of controllability of LLMs in the language of control systems: Controllability of dynamical models operating in the quotient space of trajectories determined by the model itself. As for prior work, we are unaware of any work that formalized the problem of controllability of LLMs. If the reviewer is, we would appreciate pointers so we can expand the discussion of relevant related work. We are also unaware of anyone having formalized meanings as pre-images of the output of trained LLMs, which is another contribution of our paper.\n\n**Q:** *Then, most of the paper is spent discussing preliminaries and introducing notation and definitions.*\n\n**A:** Indeed. We believe it is important to spend some effort on the definitions given the level of confusion and disagreement surrounding these concepts. For example, some in the community claim that LLMs are \"uncontrollable\", without giving any definition, and there is often quoted literature claiming that LLMS \"cannot in principle represent meanings\", based on a definition of meaning in terms of \"intentionality\" that is, however, left undefined, leading to tautological arguments. So, we think that within the plethora of empirical papers, a few papers focusing on definitions may not be such a bad thing.\n\n**Q:** *The first use of the proposed definitions to make a non-trivial claim is in Section 5 with Postulate 1, but this claim is not justified.*\n\n**A:** If the reviewer means it is not proven, that is indeed the case. We could have \"proven\" it by arguing that the context is finite so one can always find the prompt by enumeration, but this would not be practical, so we do not frame the claim as a theorem but rather a postulate. If by \"justified\" the reviewer means that it not a worthwhile effort to try to study the controllability of LLMs analytically, that would be a judgment call we would disagree with, and stress that of course our analysis is not an alternative but a complement to the necessary empirical assessments, which are currently dominating the literature.\n\n**Q:** *Theorem 1 in Section 5 seems to follow directly from the proposed definition of controllability. Theorem 2 is presented with no intuition and the proof in the appendix is only for a special case. It's also not explained how Theorem 2 justifies the main conclusion: that \"a prompt engineer aided by enough time and memory can force an LLM to output an arbitrary sequence of \u2113 tokens.\"*\n\n**A:** The notion of controllability requires verifying the existence of a sequence of inputs for the dynamics (11)-(13) which is a non-trivial endeavor. We further note that inputs appear in equation (13) whereas the assumptions in Theorem 1 relate to the function in equation (12).\n\nIn the revised version we will allocate more space in the appendix to provide an intuitive description of Theorems 1 and 2.\n\nAs we wrote in the proof of Theorem 2, a general proof would require heavy notation thus rendering it even less accessible to a general audience. Anyone versed in nonlinear controls can verify that the arguments applies in general, whereas readers not familiar with it can more easily follow the special case to gain insight."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6458/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691973099,
                "cdate": 1700691973099,
                "tmdate": 1700691973099,
                "mdate": 1700691973099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FSyiyTpPn3",
            "forum": "X2gjYmy77l",
            "replyto": "X2gjYmy77l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_jHbh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_jHbh"
            ],
            "content": {
                "summary": {
                    "value": "The paper hypothesize that the state space of a language model is sentences understandable to human (rather than sheer tokens or sequence of tokens). It then tries to provide a theory for controllability of language systems in the meaning space."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I appreciate the effort to formalize controllability of LLMs as a generic question. This is an interesting topic that can spur further research and help predictability and understand the LLM's behaviours in general. However, the presented theory suffers from various core issues."
                },
                "weaknesses": {
                    "value": "- Definition of equivalence seems to be insufficient and lacks important components. \n\n- Definition of meaning seems to defeat the whole purpose of this paper, as it allows for any gibberish/random sequence of tokens to still induce a meaning and possibly a set of many other gibberish sequences to be in their equivalent class. \n\n- How to find a discriminant for meaning is left out as the authors explicitly assume that \u201cthe mechanism is provided by human annotators and other providers of training data.\u201d While the authors emphasize in the introduction that such information can be used in the LLM training without external reward model: \u201cThis observation shows that sentence-level annotations can be incorporated directly into the trained model without the need for any external reward model nor external policy model, simply by sentence-level feedback,\u201d I do not see the advantage of this approach over using the very same data to train a reward model and use that either during the training (as in RLHF) or as an augmentation (as in Rectification method), the latter indeed provides quite strong theoretical guarantees. If this secondary goal is valid, the paper requires sufficient reasoning for why the presented approach is superior. If not, the presentation requires to change and reflect only the controllability analysis."
                },
                "questions": {
                    "value": "- Last paragraph of introduction: \u201cwe hope to encourage the design of actual new control mechanism\u201d --> It looks like the authors are not aware of the [*Rectification* method](https://arxiv.org/abs/2302.14003), which is indeed a formal control paradigm for avoidance any definable \u201cmeaning,\u201d as in the terminology of this paper. See the point bellow.\n\n- Let me first emphasize that I am aware of the differences between these two works (no need to tell me they say X while we say Y). My main point is to help you broaden your research and make the current paper be more comprehensive and useful for the readers, also this is aligned with your last paragraph of the introduction: The concept of probabilistic avoidance, which is akin to *stochastic* controllability but in preventing a meaning from being reached is [studied before](https://arxiv.org/abs/2302.14003). Even though the authors of that paper did not call it controllability (they call it security condition which also comes [from an older paper](http://proceedings.mlr.press/v97/fatemi19a/fatemi19a.pdf)), but it is indeed stochastic controllability in the context of avoidance. To use the terminology of this submission, what that work shows mathematically is that the LLM\u2019s output probabilities can be minimally corrected and that will guarantee the avoidance of any prescribed meaning of interest (the one under their study was toxicity, but essentially the same analysis is applicable to any other meaning). Importantly, such results show that a certain optimal value function corresponding to reaching the prescribed meaning (with reward of -1 and no discount) is key at least in principle for the avoidance, which also highlights the sufficiency of RL as the core learning paradigm for closed-loop controlling/steering an LLM. \n\n- Section Preliminaries, first part --> An LLM in its standard form provides a simplex over token space, not an embedding + a metric. How can an LLM be a discriminant?\n\n- Section Preliminaries, second part --> The definition of equivalence is quite confusing. It looks like that equivalence requires both discriminant and a set of classes, no matter how they are defined (does it?). However, the presented definition only uses discrimanent, which seems incomplete. In that case, $x^1 \\stackrel{\\phi}{\\sim} x^2$ is ambiguous.\n\n- Definition 1 --> [related to the point above] without a set of classes (meanings), the definition is incomplete. Indeed, $\\phi$ only maps a given sentence to a metric space, that is a vector $\\in \\mathbb{R}^K$ (plus a given metric), nothing more. One needs to have (i) a set of classes in addition to (ii) a criteria (like argmax in your example) to define equivalence. \n\n- By extension, $[x]$ does not provide a comprehensible set by just using $\\phi$. Intuitively, one can ask, given another vector $y$, then $<\\phi(x), \\phi(y)>$ must be what in order for $y$ to be part of $[x]$?\n\n- Even if the above issue is resolved, the provided definition of meaning lacks any connection to linguistic meanings, as any random sequence of tokens will induce its own $[x]$. This conflicts with the authors initial claim that they would like to distinguish between a meaningful sentence and a random sequence (first line of page 2). Importantly, this is totally separate from the question of what a good $\\phi$ is, and it hold from *any* given $\\phi$.\n\n- As for selecting $\\phi$, the authors say \u201cHere, we assume that the mechanism is provided by human annotators and other providers of training data.\u201d How is this different from an external reward model, as the authors named as a disadvantage of RL-based methods? (Note: the reward is defined for a complete text not a given point at the middle, so these seem to be exactly the same.)\n\n- Eq 1 --> This is confusing. The range of $\\phi$ is $\\mathbb{R}^K$, where $K$ was the dimension of matric space, yet the output of $\\phi$ is a probability distribution (which is a simplex)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6458/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777680385,
            "cdate": 1698777680385,
            "tmdate": 1699636722014,
            "mdate": 1699636722014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZsT8hSSHI7",
                "forum": "X2gjYmy77l",
                "replyto": "FSyiyTpPn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer jHbh"
                    },
                    "comment": {
                        "value": "**Q:** *Definition of equivalence seems to be insufficient and lacks important components. Definition of meaning seems to defeat the whole purpose of this paper, as it allows for any gibberish/random sequence of tokens to still induce a meaning and possibly a set of many other gibberish sequences to be in their equivalent class.*\n\n**A:** Any model trained on natural sentences determines a partition of the space of input sentences, including gibberish, indeed. This is why we restrict the model to operate in the quotient space (10), so input gibberish is excluded from the analysis. In theory, one can of course feed gibberish to a model, but that does not have any meaning in our definition since it is outside $\\sigma({\\cal I})$. In practice, one can easily exclude gibberish sequences by filtering the input, as done to avoid jailbreaking.\n\n**Q:** *How to find a discriminant for meaning is left out as the authors explicitly assume that \u201cthe mechanism is provided by human annotators and other providers of training data.\u201d*\n\n**A:** It is described in in multiple passages (although we can make it clearer) that the discriminant is the trained LLM itself: That means that it is found by training, which is based on human annotations and human-generated utterances. For example, starting from the abstract: \nPage 1: *\"The LLM maps complete sequences to a vector space [...] that vector space can be coopted to represent meanings during fine-tuning\"*\nPage 2: *\"an LLM is a map [...] from complete sentences to meanings\"*\nPage 4: Note that the notation used for the discriminant, $\\phi$ (Definition 1), is on purpose the same used for the (parametrized) logit vector $\\phi_w$ of the trained model (1)\nPage 5: *\"by feeding back complete sentences to the input [of the model] and training [...] the same $\\phi_w$ as a sentence-level discriminant to attribute meaning to synthesized sentences\"*\nPage 6: Claim 1 states that a well-trained model $\\phi_{\\hat w}$ generates meaningful sentences (and therefore meanings).\nPage 8: establishes that the domain of meaningful sentences is the range of the model $F_w$, again emphasizing that meanings are imposed by the trained model.\n\nHaving said that, *we agree* that this point could be expressed more clearly at the beginning: the trained model depends on human-generated content (during pre-training) and human annotation (during fine-tuning), so the origin of meanings is latent in the human content providers and annotators, and the mechanism by which they are transferred to the trained model is the training process. We will try to simplify this explanation and place it at the top of the introduction to avoid any confusion.\n\n**Q:** *While the authors emphasize in the introduction that such information can be used in the LLM training without external reward model: \u201cThis observation shows that sentence-level annotations can be incorporated directly into the trained model without the need for any external reward model nor external policy model, simply by sentence-level feedback,\u201d I do not see the advantage of this approach over using the very same data to train a reward model and use that either during the training (as in RLHF) or as an augmentation (as in Rectification method), the latter indeed provides quite strong theoretical guarantees. If this secondary goal is valid, the paper requires sufficient reasoning for why the presented approach is superior. If not, the presentation requires to change and reflect only the controllability analysis.*\n\n**A:** We do not claim that forgoing an external reward model is superior, just that it is sufficient. Indeed the reviewer is correct that, if one can afford it, direct augmentation or RLHF with an externally-trained reward model (which then becomes the origin of meanings) is also possible, and possibly better. All we are saying is that an external reward model is not necessary, because one can use the model itself -- as has been established in various alternatives to RLHF, such as Direct Preference Optimization (DPO). We were not aware of the \"rectification method\", as discussed below, since that appeared after our paper was completed, but we will include it as an alternative way of endowing the trained model with meanings."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6458/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691504034,
                "cdate": 1700691504034,
                "tmdate": 1700691504034,
                "mdate": 1700691504034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hG5y0NrPnA",
                "forum": "X2gjYmy77l",
                "replyto": "FSyiyTpPn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer jHbh (continued)"
                    },
                    "comment": {
                        "value": "**Q:** *Last paragraph of introduction: \u201cwe hope to encourage the design of actual new control mechanism\u201d --> It looks like the authors are not aware of the Rectification method, which is indeed a formal control paradigm for avoidance any definable \u201cmeaning,\u201d as in the terminology of this paper. See the point bellow. Let me first emphasize that I am aware of the differences between these two works (no need to tell me they say X while we say Y). My main point is to help you broaden your research and make the current paper be more comprehensive and useful for the readers, also this is aligned with your last paragraph of the introduction\u2026*\n\n**A:** Thanks for the useful reference and the thorough description of that work, which we were not aware of since it appeared on ArXiv after our work was completed. We will add discussion as it certainly sounds pertinent, thank you for the pointer and the explanation. \n\n**Q:** *Section Preliminaries, first part --> An LLM in its standard form provides a simplex over token space, not an embedding + a metric. How can an LLM be a discriminant?*\n\n**A:** An LLM is trained so that the vector of logits, or the corresponding soft-max, approximates the (log-)posterior of the next token, which is the optimal (Bayesian) discriminant. So, if you view the LLM as a map from an input sequence of discrete tokens to the softmax vector, it is indeed a discriminant, and since it is trained with the (exponential of the) inner product of the embedding with the one-hot encoding of the next token, the inner product (metric) is built into the embedding space by design of the training process.\n\n**Q:** *Section Preliminaries, second part --> The definition of equivalence is quite confusing. It looks like that equivalence requires both discriminant and a set of classes, no matter how they are defined (does it?). However, the presented definition only uses discriminant, which seems incomplete. In that case, x^1 phi ~ x^2 is ambiguous.*\n\n**A:** The definition requires either a discriminant or a set of classes, since one determines the other. If given a discriminant, one can define partitions using simple order relations or rules (e.g. thresholds), and given a set of classes one can define a discriminant as a (piecewise constant) function."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6458/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691734411,
                "cdate": 1700691734411,
                "tmdate": 1700695998892,
                "mdate": 1700695998892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oQNHMmOuRz",
            "forum": "X2gjYmy77l",
            "replyto": "X2gjYmy77l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_ecwe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6458/Reviewer_ecwe"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies whether an agent can use prompts to steer LLMs to generate any sentences as wanted. A dynamical system perspective is adopted, and LLMs are viewed as discrete-time systems evolving in the embedding space of tokens. The authors first describe that  ''meanings'' in trained LLMs can be viewed as equivalence classes of complete trajectories of tokens. Based on this viewpoint, they end up with a question of determining the controllability of a dynamical system evolving in the quotient space of discrete trajectories induced by the model itself. Several conditions for controllability are then provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is very original, and presents some unique idea on how to determine the controllability of \"well-trained\" LLMs. Connecting LLMs with control is definitely interesting. A new theoretical perspective is developed. This paper may also inspire more researchers to think about the fundamental theory of LLMs."
                },
                "weaknesses": {
                    "value": "1.  The characterization of meanings seems quite subjective. The authors mentioned that their characterization of meanings is compatible with the deflationary theories in epistemology. It seems that this explanation itself does not justify why such a characterization is meaningful for studying the controllability of LLMs in general.\n\n2. The authors claim that their conditions are largely met by today\u2019s LLMs. More justifications are needed. This also seems a hand-waving statement. \"Largely met\" means \"not always met\"?\n\n3. Many of the equations are quite difficult to understand. I have tried very hard to follow the theoretical arguments in this paper. However, I still feel very confused in the end. I will ask some questions in the \"Question\" section."
                },
                "questions": {
                    "value": "1. Does Definition 1 assume some sort of underlying classifier such that the equivalent classes can be defined? What is that specific classifier? The authors mentioned \"the mechanism is provided by human annotators and other providers of training data.\" This is very confusing. I don't understand what exactly this phi is.\n\n2. Equation (6) is very confusing. I mean, in Equation (4), y is sampled from the softmax operation. Then all of a sudden, it becomes an additive noise? It seems that the noise n_t depends on x_{1:t}?\n\n3. Regarding Definition 3, an LLM is well-trained if theta is any small positive number?\n\n4. What does Claim 1 mean? Things that happen before C do not matter? Then how large does this C need to be for practical LLMs? I mean, suppose for a task of writing a book, C has to be really large?\n\n5. What does Postulate 1 really mean? How to justify this?\n\n6. How can we justify the assumptions in Theorem 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6458/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699237214994,
            "cdate": 1699237214994,
            "tmdate": 1699636721887,
            "mdate": 1699636721887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ymHupP2ujJ",
                "forum": "X2gjYmy77l",
                "replyto": "oQNHMmOuRz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6458/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ecwe"
                    },
                    "comment": {
                        "value": "**Q:** *The characterization of meanings seems quite subjective.*\n\n**A:** Indeed, our definition of meaning is specific to a discriminant function $\\phi(\\cdot)$, which in the case of an LLM is implemented by the trained model $\\phi_w(\\cdot)$. Different models partition the data differently, although models trained on the same or similar data will likely yield similar partitions. We do not believe in a universal notion of meaning, and in any case our scope is limited to how meanings are represented by LLMs.\n\n**Q:** *The authors mentioned that their characterization of meanings is compatible with the deflationary theories in epistemology. It seems that this explanation itself does not justify why such a characterization is meaningful for studying the controllability of LLMs in general.*\n\n**A:** It is not essential that our definition aligns with any particular epistemological theory, but we note that, once mapped to LLMs,  some theories boil down to representing meanings as equivalence classes of expressions, and then differ on their genesis. In our case, such classes are determined by the state of the trained model, which is what we want to control.\n\n**Q:** *The authors claim that their conditions are largely met by today\u2019s LLMs. More justifications are needed. This also seems a hand-waving statement. \"Largely met\" means \"not always met\"?*\n\n**A:** Met by some, but not all, trained LLMs. LLMs by design represent a joint distribution on a sequence of tokens, but how well that approximates the empirical distribution of natural sentences is a matter of approximation rather than constraint satisfaction. In this context, by \"largely\" we mean that perplexity of trained models is low enough that generated sentences are indistinguishable from human generated ones. This may not be true of all models, so we use \"largely\" to indicate that the conditions are valid only for some models.\n\n**Q:** *Does Definition 1 assume some sort of underlying classifier such that the equivalent classes can be defined? What is that specific classifier? The authors mentioned \"the mechanism is provided by human annotators and other providers of training data.\" This is very confusing. I don't understand what exactly this phi is.*\n\n**A:** In the definition, $\\phi$ can be any discriminant vector. In a trained model, $\\phi = \\phi_w$ is the soft-max vector after supervised fine-tuning (and optionally RLHF). Since that vector is a function of the training data, both in pre-training and fine-tuning, and the training data is provided by human annotators and authors, ultimately the source of meanings is latent in the expressions, ranking, or scoring humans have provided for training.\n\n**Q:** *Equation (6) is very confusing. I mean, in Equation (4), y is sampled from the softmax operation. Then all of a sudden, it becomes an additive noise? It seems that the noise $n_t$ depends on $x_{1:t}$?*\n\n**A:** Indeed the fact that the same token can be represented in different ways can be confusing at first:  Tokens can be represented as elements of a discrete dictionary after sampling, as in (4), or as discriminant vectors before sampling, as in (5). In the latter case, the effect of different selections in the (previous round of) sampling results in a perturbation of the embedding vector of the next token, which is represented in (6) as an additive perturbation since embedding vectors (logits) are  elements of a vector space. In general, it is true that the ``noise'' $n_t$ is dependent on the previous history, $x_{1:t-1}$, but in complex ways that can be modeled statistically. \n\n**Q:** *Regarding Definition 3, an LLM is well-trained if theta is any small positive number?*\n\n**A:** Yes, although different users, or use cases, may have different standards on what constitutes a well-trained model, reflected in the perplexity thresholds that are considered acceptable."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6458/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691004792,
                "cdate": 1700691004792,
                "tmdate": 1700691004792,
                "mdate": 1700691004792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]