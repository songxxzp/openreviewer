[
    {
        "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning"
    },
    {
        "review": {
            "id": "Eoakc2sjo4",
            "forum": "vNiI3aGcE6",
            "replyto": "vNiI3aGcE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies provably efficient reinforcement learning in two-player zero-sum Markov games, an important special case of multi-agent RL. This paper improves existing results in the following directions: sample complexity, memory efficiency, Markov output policy, and burn-in cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper studies an important topic in MARL theory. The proposed algorithm simultaneously achieved state-of-the-art results in all the aspects it considers: It matches the best sample complexity bounds, reduces the burn-in cost, and improves the space complexity while still outputting a Markov policy. The theoretical analysis looks solid."
                },
                "weaknesses": {
                    "value": "I reviewed this paper at NeurIPS 2023. My concern was about the technical novelty of the paper because the proposed algorithm follows the mature framework of Nash Q-learning. The improved sample complexity is achieved by also following an existing reference-advantage decomposition technique. \n\nIn terms of the bounds, the biggest improvements that this paper makes over existing works are regarding the space complexity and burn-in cost. In my opinion, these are less important metrics compared to sample complexity or time complexity, yet this work has to optimize these metrics at the cost of a much more complicated algorithm and proof procedure. \nWhile I still hold most of my previous opinions, I appreciate the authors\u2019 effort in improving their work and would like to increase my score compared to my NeurIPS evaluation. \n\nCompared to the NeurIPS submission, the new major results are Theorems 2 and 3. I found that the extension to multi-player general-sum games (Theorem 3) particularly interesting, but I was not able to find any algorithm or proof for this theorem. What is the learning target for general-sum games, Nash or correlated equilibria?"
                },
                "questions": {
                    "value": "1.\tCould you please point me to the proofs of Theorem 3? Also what is the algorithm for this theorem (as I assume that Algorithm 1 only applies to two-player zero-sum games)? I do not think the extension from zero-sum to multi-player general-sum is straightforward and would hope to see a more detailed discussion.\n\n2.\tSince you now also consider multi-player general-sum games, it is probably helpful to include related works for learning in general-sum games, especially those using Nash V-learning (to name a few):\n\na. Song, Ziang, Song Mei, and Yu Bai. \"When can we learn general-sum Markov games with a large number of players sample-efficiently?.\" arXiv preprint arXiv:2110.04184 (2021).\n\nb. Mao, Weichao, and Tamer Ba\u015far. \"Provably Efficient Reinforcement Learning in Decentralized General-Sum Markov Games.\" arXiv preprint arXiv:2110.05682 (2021).\n\nc. Daskalakis, Constantinos, Noah Golowich, and Kaiqing Zhang. \"The complexity of markov equilibrium in stochastic games.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023.\n\n3.\tFrom what I understsand, the new major results compared to the NeurIPS submission are Theorems 2 and 3. Could you please let me know if there are any other new results that I am missing?\n\n4.\tIn your future work, you mentioned the possibility of achieving A+B sample complexity instead of AB. Does the Nash V-Learning algorithm help with this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697920188962,
            "cdate": 1697920188962,
            "tmdate": 1700243270588,
            "mdate": 1700243270588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xwUlooWBBi",
                "forum": "vNiI3aGcE6",
                "replyto": "Eoakc2sjo4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness:** Thank you for providing your insightful feedback once again. We greatly respect your opinions and appreciate your openness to various research directions. We would like to take this opportunity to provide a more detailed explanation of our ideas.\n\nWe first make use of the reference-advantage decomposition (also known as a variant of variance reduction) for Markov games to achieve sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, space complexity $O(SABH)$, computational complexity $O(T \\mathrm{poly}(SAB))$, and burn-in cost $O(SAB\\mathrm{poly}(H))$, which is never shown in previous works. Specifically, we design a pair of optimistic and pessimistic value functions and an early-settlement method to get such results. This is the first time to show the effectiveness of these ideas in Markov games, which is of significant importance in our opinion. We have stated these at the beginning of Section 3.1 in the revised version.\n\nIn terms of performance metrics, we have several noteworthy points to highlight. On the one hand, as per your previous suggestions, we have provided a comprehensive comparison of time complexity as shown in Table 1. It demonstrates that our algorithm achieves the best time complexity. On the other hand, the complex algorithm and proof process may not necessarily pose practical challenges for the algorithm implementation. On the contrary, we address a current research gap, which we think is more important.\n\nFinally, we apologize for not presenting the implications of Theorem 3 in our initial submission. In the revised version, we have included additional explanations in Appendix F to address this concern.\n\n**Q1:** Thank you for your comments and sorry for the confusion. We have included the algorithm and proof for multi-player general-sum games in Appendix F to address your comment in the revised version.\n\n**Q2:** Thank you for your comments. In the revised version, we have included a paragraph in Section 1.2 to introduce the related works on multi-player general-sum Markov games.\n\n**Q3:** Thank you for your question. The new major results compared with the NeurIPS version are indeed Theorems 2 and 3. Additionally, we have made several improvements. On the one hand, we have improved the presentation of Table 1 which compares various algorithms in terms of several performance metrics. Specifically, We have added a comparison of whether the output policy is a Markov/Nash-VI policy and a comparison of computational complexity. On the other hand, according to the feedback from the NeurIPS reviewers, we have provided a more detailed explanation of our reference-advantage decomposition and early-settlement methods in Section 3.1 on page 7.\n\n**Q4:** Thank you for your question. Currently, we do not have a clear plan for achieving a sample complexity of $O(A+B)$. Algorithms such as Nash V-learning, or those based on V-learning, achieve a sample complexity of $O(A+B)$ because they do not need to store the Q-function and only require the Value function. It seems that they may help in future work. However, the policies learned by V-learning are non-Markov and history-dependent in general. It remains to study how to achieve a sample complexity of $O(A+B)$ without compromising the performance of existing metrics. We have discussed this in the conclusion of the revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209982617,
                "cdate": 1700209982617,
                "tmdate": 1700210005520,
                "mdate": 1700210005520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pmeEiHB0fZ",
                "forum": "vNiI3aGcE6",
                "replyto": "xwUlooWBBi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response and for including the new results & proofs in Appendix F. They look good to me and I am happy to increase my rating to \"8: accept, good paper\"."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243517621,
                "cdate": 1700243517621,
                "tmdate": 1700243517621,
                "mdate": 1700243517621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kaULO911yx",
            "forum": "vNiI3aGcE6",
            "replyto": "vNiI3aGcE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies two-player zero-sum Markov games (TZMG). It proposes the model-free algorithm Memory-Efficient Nash Q-Learning (ME-Nash-QL), which achieves state-of-the-art space and computational complexity, nearly optimal sample complexity, and the best burn-in cost compared to previous results with the same sample complexity. Moreover, the proposed algorithm generates a single Markov and Nash policy rather than a nested mixture of Markov policies, by computing a relaxation of the Nash equilibrium instead, i.e. Coarse Correlated Equilibrium (CCE)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "# Originality\n- The related works are covered in detail.\n# Quality\n- The theoretical proofs seem to be rigorous. \n# Clarity\n- This paper is in general well-written and easy to follow. The design idea of the algorithm is clearly explained.\n# Significance\n- The theoretical results of this work are strong. It achieves state-of-the-art space and computational complexity, nearly optimal sample complexity, and the best burn-in cost compared to previous results with the same sample complexity.\n- TZMG is foundational and critically significant for MARL. This research has the potential to establish a new benchmark, providing a foundation for further studies in the related literature."
                },
                "weaknesses": {
                    "value": "- Although the proposed algorithm is compared to Nash-VI (Liu et al., July 2021) and V-learning (Jin et al., 2022) in detail, the design idea of the proposed algorithm seems to share certain similarities with those from the two works. For example, they all compute a CCE policy and take the marginal policies; the choice of learning rate $\\frac{H+1}{H+N}$, the form of bonus terms, and the update of lower and upper bounds for Q-functions are similar. The originality of this paper could be significantly enhanced if the authors could discuss thoroughly the fundamental distinctions between the ideas of the proposed algorithm and the aforementioned Nash-VI and V-learning.\n- The theoretical findings are limited to the TZMG and CCE setting, which somewhat diminishes the overall contribution of this paper.\n- The auxiliary functions in Algorithm 2 are too nested, making it hard to read.\n### Minor:\n- There seems to be a blank section A.3.1 on page 14."
                },
                "questions": {
                    "value": "- How is $\\operatorname{CCE}(\\bar{Q}, Q)$ compuated? I was anticipating a detailed introduction to its calculation to ensure the paper's comprehensiveness. An explicit explanation would greatly contribute to the paper's self-containment.\n- Is the achievement of the space complexity independent of $T$ attributed to the fact that the output policy is a single Markov policy? In this context, do the authors consider the CCE as an essential relaxation for realizing such space complexity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733640933,
            "cdate": 1698733640933,
            "tmdate": 1700612827715,
            "mdate": 1700612827715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8egu9fclPY",
                "forum": "vNiI3aGcE6",
                "replyto": "kaULO911yx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 1:** We would like to express our sincere gratitude for your valuable comments. The fundamental distinctions between the proposed algorithm and reference algorithms (Nash-VI and V-learning) are the application of the ideas of variance reduction and the early-settlement method. Moreover, we first show the effectiveness of these ideas in TZMG by proving sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, space complexity $O(SABH)$, computational complexity $O(T \\mathrm{poly}(SAB))$, and burn-in cost $O(SAB\\mathrm{poly}(H))$.\n\nNash-VI uses the model-based idea and designs a different style of bonus term, which achieves the sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$ with burn-in cost $O(S^3ABH^4)$. However, Nash-VI needs space complexity $O(S^2ABH)$ to store the empirical transition matrix. Moreover, it computes the output policy for all states $s\\in \\mathcal{S}$ in each step, which leads to a computational complexity of $S$ times higher than that of our algorithm.\n\nV-learning uses the FTRL idea (from adversarial bandit, i.e., $H = 1$), which overcomes the challenges posed by the curse of multi-agent, and achieves the sample complexity $\\widetilde{O}(H^6S(A+B)/\\varepsilon^2)$. However, this sample complexity depends greatly on $H$. Besides, the output policy of V-learning is neither **Nash** nor Markov policy due to the incremental updates of V-learning and causes the space complexity $O(S(A+B)T)$ to increase with the number of samples $T$.\n\nTo address the aforementioned issues, we have designed the ME-Nash-QL. First, we propose a model-free algorithm to shave the $S$ factor in space complexity compared with Nash-VI. Secondly, we have successfully obtained a lower sample complexity and a lower burn-in cost by designing a pair of optimistic and pessimistic value functions along with an early-settlement method based on the reference-advantage decomposition technique, which is a variant of variance reduction. Specifically, as detailed in Section 3.1, the standard deviation of $\\widehat{P} _{h,s,a,b}\\big(\\overline{V} _{h+1}-\\overline{V}^\\mathrm{R} _{h+1}\\big)$ might be $O(H)$ times smaller than the stochastic term used in Nash Q-learning. This is a key observation to weaken the dependency of the regret bound on $H$, and obtain the burn-in cost $\\widetilde{O}(SABH^{10})$. Thirdly, our algorithm computes marginal policies using a CCE subroutine for a specific state in each step, which leads to a lower space complexity $(SABH)$ and a lower computational complexity $T{\\rm poly}(AB)$ compared with Nash VI, and meanwhile a Markov and Nash output policy. We have clarified these in Section 1 in the revised version.\n\n**Weakness 2:** Thanks for your insightful feedback. Regarding the TZMG setting, we have extended it to multi-player general-sum Markov games in Theorem 3. We have also included a detailed explanation of this in Appendix F in the revised version.\n\nFor the CCE setting, we would like to discuss it in two cases, i.e., TZMG and multi-player general-sum Markov games. In the former case, CCE, CE, and NE are equivalent, meaning that CCE in zero-sum games is guaranteed to be Nash. Thus our algorithm achieves NE in this case. For the latter case, we can achieve CCE, CE, and NE by replacing the subroutine in line 15 in Algorithm 3. However, it's important to note that NE leads to a much higher computational complexity, such as PPAD-complete (a complexity class widely believed to be computationally challenging). We have clarified these in Appendix F in the revised version.\n\n**Weakness 3:** Thanks for your comments. We have reorganized Algorithm 2 in the revised version to enhance its readability.\n\n**Minor:** Thanks for your comments, and we apologize for this typo. We have corrected this in the revised version.\n\n**Q1:** Thank you for your question. There are many well-established computational methods to calculate CCE, as demonstrated by Xie et al. in June 2022. We have supplemented relevant explanations on page 8 in the revised version of our paper to make it self-contained.\n\nQiaomin Xie, et al., Learning zero-sum simultaneous-move Markov games using function approximation and correlated equilibrium.\n\n**Q2:** Thanks for your questions. For the first question, the fact that space complexity remains independent of $T$ is attributed to the fact that the output policy is a single Markov policy. \n\nFor the second question, our decision to opt for CCE over NE is primarily motivated by the goal of reducing computational complexity, rather than space complexity. This choice is driven by the understanding that NE methods often come with high computational complexity, involving PPAD-complete problems (widely believed to be computationally challenging). In other words, we could achieve NE with the same space complexity by replacing CCE with NE in the algorithm, but the computational complexity is prohibitive.\n\nWe have clarified these at the end of Section 3.1 and Appendix F.2 in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209478025,
                "cdate": 1700209478025,
                "tmdate": 1700209478025,
                "mdate": 1700209478025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i0gDmyDDgl",
                "forum": "vNiI3aGcE6",
                "replyto": "8egu9fclPY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing detailed explanations for my concerns, and for the extended results in Appendix F. They all look good to me so I increased my rating to 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612803855,
                "cdate": 1700612803855,
                "tmdate": 1700612803855,
                "mdate": 1700612803855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9Oiq9kvNwl",
            "forum": "vNiI3aGcE6",
            "replyto": "vNiI3aGcE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a model-free algorithm for two-player zero-sum Markov game, which enjoys low sample complexity and computational/space complexity. The resulting algorithm has optimal dependency on S and H but sub-optimal dependence on the number of actions. The algorithm design features the early-settlement method and the reference-advantage decomposition technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well written and easy to follow. \n+ The proposed algorithm outperforms existing algorithms in terms of space complexity and computational complexity."
                },
                "weaknesses": {
                    "value": "- My main concern is the technical novelty. The reference-advantage decomposition technique has already been incorporated in two-player zero-sum Markov game by Feng el al (2023) (not cited by this work), which achieves a regret in \\tilde{O}(\\sqrt{H^2SABT}) and matches with the regret bound in this work. The main novelty of the algorithm design thus lies in the early-settlement design in order to reduce the burn-in cost, which is not new in the literature.\n\nFeng, S., Yin, M., Wang, Y. X., Yang, J., & Liang, Y. (2023). Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. arXiv preprint arXiv:2308.08858."
                },
                "questions": {
                    "value": "+ Regarding my point in weakness section, is there any other technical contributions besides reference-advantage decomposition and early-settlement design?\n\n+ Is it possible to obtain similar result for learning CCE in multi-agent general-sum Markov games?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771476977,
            "cdate": 1698771476977,
            "tmdate": 1700664665978,
            "mdate": 1700664665978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ek07GzyKip",
                "forum": "vNiI3aGcE6",
                "replyto": "9Oiq9kvNwl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for the reviewer guideline"
                    },
                    "comment": {
                        "value": "We would like to express our cordial thanks to the reviewer for your precious time and feedback on our paper. However, it appears that there might be some misunderstanding regarding the reviewer guideline (https://iclr.cc/Conferences/2024/ReviewerGuide).\n\nAccording to the guideline, it states: \"*We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was **published** (i.e., at a **peer-reviewed venue**) on or after **May 28, 2023**, authors are not required to compare their own work to that paper.*\"\n\nThe work by Feng et al. was available on **arXiv** on **August 17, 2023**, which is contemporaneous with our work.  We appreciate the reviewer for bringing this reference to our attention, and we will ensure to cite it in the final revised version of our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885722250,
                "cdate": 1699885722250,
                "tmdate": 1699885722250,
                "mdate": 1699885722250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xW5zXYHqRM",
                "forum": "vNiI3aGcE6",
                "replyto": "9Oiq9kvNwl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:** Thank you for your question. Besides applying the reference-advantage decomposition and early-settlement techniques, our main technical contribution is to show their effectiveness by proving sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, space complexity $O(SABH)$, computational complexity $O(T \\mathrm{poly}(SAB))$, and burn-in cost of $O(SAB\\mathrm{poly}(H))$. In addition, we extend our algorithm from the TZMG setting to multi-player general-sum Markov games in Theorem 3, which is also one of our contributions. We have stated these in Section 1.1 in the revised version.\n\n**Q2:** Thank you for your question. Similar conclusions can be obtained in multi-player general-sum Markov games, as stated in Theorem 3 in the main text. We have added Appendix F in the revised version to provide detailed supplements for Theorem 3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209852370,
                "cdate": 1700209852370,
                "tmdate": 1700209852370,
                "mdate": 1700209852370,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5jdMW1fGY",
                "forum": "vNiI3aGcE6",
                "replyto": "Ek07GzyKip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ],
                "content": {
                    "comment": {
                        "value": "I understand the reviewer guideline and I acknowledge that those works are concurrent. I was wondering whether you can provide a more detailed comparison between those two works in terms of algorithm design and analysis, and whether such comparison will be reflected in the revised paper. Usually researchers compare concurrent works in detail, especially the potential overlapping parts."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492059503,
                "cdate": 1700492059503,
                "tmdate": 1700492059503,
                "mdate": 1700492059503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YFZJlgzIKq",
                "forum": "vNiI3aGcE6",
                "replyto": "UVqutesERD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. It would better if Feng et al (2023) is included in Table 1 as well."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621638409,
                "cdate": 1700621638409,
                "tmdate": 1700621638409,
                "mdate": 1700621638409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3VpeKjGwnQ",
                "forum": "vNiI3aGcE6",
                "replyto": "yEG71IFvTf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have increased my rating based on the revision."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664631288,
                "cdate": 1700664631288,
                "tmdate": 1700664631288,
                "mdate": 1700664631288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5vLCXCTyJC",
            "forum": "vNiI3aGcE6",
            "replyto": "vNiI3aGcE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-free algorithm for learning Nash policy in Two-player Zero-sum Markov Game. The authors prove that this algorithm enjoy many benign properties, including outputting Markov policy, low computational/sample/space complexity in certain regime and low burn-in cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed algorithm enjoys several benign properties, as mentioned in the summary. In particular, the algorithm perform well when the horizon is very long while retaining other nice properties such as Markov output policy and low burn-in cost."
                },
                "weaknesses": {
                    "value": "1. The proposed algorithm does not break the curse of multi-agent. Although the authors argue that there are many scenarios where horizon length is very long, I still feel that this is not general enough. I personally would still be more interested in algorithms that have $O(A+B)$ dependence in complexity.\n2. The algorithmic novelty is a bit unclear to me."
                },
                "questions": {
                    "value": "1. There are many elements mentioned in the paper, such as complexity, burn-in cost, Nash policy, Markov policy etc. While I understand that no prior algorithm surpassing this algorithm in every aspect, I wonder what do the authors think is the most important aspect/what is the main focus?\n2. Can the authors explain what is the most salient algorithmic novelty to the newly proposed algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821785700,
            "cdate": 1698821785700,
            "tmdate": 1700481881121,
            "mdate": 1700481881121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "midoBXbTQ4",
                "forum": "vNiI3aGcE6",
                "replyto": "5vLCXCTyJC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 1:** Thank you for your feedback and concerns regarding the issue of the curse of multi-agent. We do agree that some research works focus on the curse of multi-agent, but there indeed exists a lot of attention on the dependency of long-horizon and size of state space, such as Nash Q-learning (Bai et al., 2020), OMNI-VI (Xie et al., Jun. 2022), Nash-UCRL (Chen et al., 2022), Optimistic PO (Qiu et al., 2021), VI-Explore/VI-UCLB (Bai \\& Jin, 2020), and Nash-VI (Liu et al., July 2021). Compared with them, we design a model-free self-play algorithm for TZMG that improves performance from three aspects simultaneously, including sample complexity ($\\widetilde{O}(H^4SAB/\\varepsilon^2)$), space complexity ($O(SABH)$), and computational complexity ($O(T \\mathrm{poly}(SAB))$). Furthermore, our algorithm achieves a $\\sqrt{T}$-regret bound with the best burn-in cost of $O(SAB\\mathrm{poly}(H))$ as long as $\\min\\{A, B\\}\\ll H^2$. In addition, our algorithm produces a Markov and Nash policy as output. To the best of our knowledge, these algorithms with $O(A+B)$ sample complexity do not output Nash policies. We consider our work to be of significant research value and hope that ME-Nash-QL can inspire other researchers to achieve a $O(A+B)$ dependency in their work without compromising the performance of existing metrics. To address the reviewer's concern, we have discussed the curse of multi-agent in the conclusion of the revised version.\n\nYu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play.\n\nQiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move Markov games using function approximation and correlated equilibrium.\n\nZixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player zero-sum linear mixture Markov games.\n\nShuang Qiu, Xiaohan Wei, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. Provably efficient fictitious play policy optimization for zero-sum Markov games with structured transitions.\n\nYu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning.\n\nQinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play.\n\n**Weakness 2:** We express our heartfelt gratitude for your feedback. The algorithmic novelty lies in two aspects. First, we apply the reference-advantage decomposition technique to the design of Markov game algorithms and achieve the $\\tilde{O}(H^4SAB/\\varepsilon^2)$ sample complexity. Second, we use an innovative early-settlement approach, substantially reducing burn-in costs. Notably, we are the first to integrate the reference-advantage decomposition and early-settlement technique into the domain of two-player zero-sum Markov games, and successfully prove the improvement of the performance. To make it clear, we have stated this at the beginning of Section 3.1.\n\n**Q1:** Thank you for your question. While we believe that each performance metric plays a crucial role in assessing algorithm quality, if we were to choose one as the most important, we would consider **sample complexity** and **burn-in cost** to be paramount. These two metrics are closely tied to the amount of data required and have a significant impact on data-driven learning algorithms, especially in tasks that involve expensive or hazardous data collection, such as autonomous driving. Our work focuses on the context of online learning, which means that algorithms need to excel in terms of real-time adaptability to the environment or task. This underscores the significance of sample complexity and burn-in cost. In addition, we think the most important role in multi-player general-sum Markov games may output the **Nash policy**, which is deficient in the existing research.\n\n**Q2:** Thank you for your question. As we outlined in our response to Weakness 2, our main contribution lies in the innovative adaptation and application of the reference-advantage decomposition technique and the early-settlement method to two-player zero-sum Markov games. We first show the effectiveness of these ideas by proving the $\\tilde{O}(H^4SAB/\\varepsilon^2)$ sample complexity and the $O(SAB\\mathrm{poly}(H))$ burn-in cost for the proposed model-free TZMG algorithms. We have stated this at the beginning of Section 3.1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209771737,
                "cdate": 1700209771737,
                "tmdate": 1700211990670,
                "mdate": 1700211990670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rn0V71r488",
                "forum": "vNiI3aGcE6",
                "replyto": "midoBXbTQ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your explanations! After confirming that sample complexity/burn-in cost are the most important elements and reference-advantage/early settlement are designed to improve the result, this paper is now clearer to me. Thus I choose to increase my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481889302,
                "cdate": 1700481889302,
                "tmdate": 1700481889302,
                "mdate": 1700481889302,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]