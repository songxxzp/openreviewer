[
    {
        "title": "Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning"
    },
    {
        "review": {
            "id": "BGy7Ynu6Ed",
            "forum": "WNxlJJIEVj",
            "replyto": "WNxlJJIEVj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of learning state-action trajectory generation with diffusion models. To better leverage the high-return states in the offline dataset, the authors proposed a contrastive learning mechanism to drive the generated trajectory toward the high-return states. Experiments are performed on D4RL benchmarks to validate the idea."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation for leveraging contrastive learning to guide the generation process is interesting and reasonable; \n\nThe paper is well-written and easily read."
                },
                "weaknesses": {
                    "value": "The method seems on par or slightly worse than the baseline approaches; \n\nThe experiments were only conducted on a few simple periodic tasks, which could not sufficiently demonstrate the effectiveness of the method; \n\nThere is no analysis of failure modes and limitations."
                },
                "questions": {
                    "value": "What is the task shown in Fig. 5? For visualizing the task, it would be better to align some of the high-return and low-return states to the visual observations of the environment, which may help readers better understand the task. \n\nFor Fig. 6, could the authors provide the annotations for the x-axis and y-axis? \n\nIt would greatly enhance the paper if the authors could offer a more in-depth analysis of failure cases. Additionally, aside from relatively straightforward periodic tasks, it would be beneficial if the authors explored more complex tasks. Demonstrating the applicability of their approach in scenarios like robot navigation or manipulation would significantly bolster the paper's overall impact and practical relevance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698161215347,
            "cdate": 1698161215347,
            "tmdate": 1699637386401,
            "mdate": 1699637386401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CdgynayF2a",
                "forum": "WNxlJJIEVj",
                "replyto": "BGy7Ynu6Ed",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer ogtz"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments! We have conducted more experiments and revised our paper according to your suggestions. The responses to your questions are listed below: \\\n**Q1**: The method seems on par or slightly worse than the baseline approaches;\\\n**A1**: Thank you for your comment. CDiffuser achieves impressive performance in most cases, although it has slightly worse performance in some cases. Compared with our backbone method Diffuser, CDiffuser demonstrates clear advantages across all the environments, which indicates the effectiveness of contrast in boosting diffusion-based RL methods.\n\n\n**Q2**: The experiments were only conducted on a few simple periodic tasks, which could not sufficiently demonstrate the effectiveness of the method;\\\n**A2**: Thank you for your comment.  Following Diffuser, to evaluate the performance of CDiffuser on tasks beyond simple periodic tasks such as hopper, walker2d and halfcheetah, we conduct additional experiments on another three RL environments maze2d-umaze-v1, maze2d-medium-v1, maze2d-large-v1 in Section 4.1. which is not periodic  and tests the model's ability for long-horizon planning. Same to Diffuser, we compare CDiffuser with CQL, IQL and Diffuser. The results are available in Table 1, and are summarized as follows:\n|Environment|CQL|IQL|Diffuser|CDiffuser|\n|:----|:----|:----|:----|:----|\n|Maze2D-UMaze|5.7|47.4|113.9|**142.9\u00b12.2**|\n|Maze2D-Medium|5.0|34.9|121.5|**140.0\u00b10.7**|\n|Maze2D-Large|12.5|58.6|123.0|**131.5\u00b13.2**|\n\nAs can be observed, CDiffuser achieves better results than baselines across all three environments, especially on maze2d-umaze-v1, where CDiffuser demonstrated a clear advantage of 25.4% over Diffuser. The results in the Maze2d environments indicate that CDiffuser is also effective in scenarios that require future planning.\n \n**Q3**: There is no analysis of failure modes and limitations.\\\n**A3**: Thanks for your suggestion! We have discussed the limitations and future works of CDiffuser in **Conclusion and Discussion**: *the CDiffuser is limited in the case in which a certain state corresponds with both high and low return, as CDiffuser relies on the return on the state to contrast. Nevertheless, the contrast based on the return of state is just the beginning, the contrast on actions also deserves to be explored. We will leave it to future works.*\n \n**Q4**: What is the task shown in Fig. 5? For visualizing the task, it would be better to align some of the high-return and low-return states to the visual observations of the environment, which may help readers better understand the task.\\\n**A4**: Thank you for your suggestion. The task shown in Figure 5 is Walker2d-Med-Replay, and we have visualized the high-return and low-return states of the agent in Figure 9 in Appendix A.4.\n \n**Q5**: For Fig. 6, could the authors provide the annotations for the x-axis and y-axis?\\\n**A5**: Thanks for you suggestion! We have replotted Figure 6 and provided the annotations for the x-axis and y-axis, please refer to our revised paper for details.\n \n**Q6**: It would greatly enhance the paper if the authors could offer a more in-depth analysis of failure cases. Additionally, aside from relatively straightforward periodic tasks, it would be beneficial if the authors explored more complex tasks. Demonstrating the applicability of their approach in scenarios like robot navigation or manipulation would significantly bolster the paper's overall impact and practical relevance.\\\n**A6**: Please refer to **Q3** for analysis of failure cases, **Q2** for more complex tasks like navigation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460365618,
                "cdate": 1700460365618,
                "tmdate": 1700460365618,
                "mdate": 1700460365618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kr0KlE37zH",
                "forum": "WNxlJJIEVj",
                "replyto": "CdgynayF2a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_ogtz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 response, which clarifies the majority of my inquiries."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618217359,
                "cdate": 1700618217359,
                "tmdate": 1700618217359,
                "mdate": 1700618217359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NWTSdeeTpT",
            "forum": "WNxlJJIEVj",
            "replyto": "WNxlJJIEVj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel contrastive diffusion probabilistic planning approach to tackle offline reinforcement learning (RL) tasks. It expands upon the foundational Diffuser model, leveraging contrastive learning to enhance the quality of samples by generating high-return trajectories. This focus on sequence modeling within offline RL is both interesting and important. The paper is well-written, though it lacks some specifics, and the visual representations, particularly Figure 1, are insightful."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The focus on sequence modeling in offline-RL is both innovative and significant, addressing a crucial aspect of this field.\n- This paper proposed a novel method of integrating contrastive learning that showed improvement over the baseline Diffusers.\n- Figure 1 is quite illustrative and intuitive."
                },
                "weaknesses": {
                    "value": "### Missing Details in Methodology:\n\n- The training process of the model remains unclear. While Equation 14 suggests end-to-end training, it is unclear where the contrastive loss is integrated. If added to the diffusion probabilistic model, which aims to reconstruct the un-corrupted trajectories, will this added loss diverge the learning, making the training unstable?\n- Is the contrastive loss involved during guidance sampling? \n\n### Insufficient Experiments:\n\n- The claim of 'significant improvements in medium and medium-replay datasets' seems overstated. The improvements are noticeable in only one task from each dataset compared to DD.\n- Extending experiments to more complex control tasks or scenarios with high-dimensional state/action spaces would substantiate the method's effectiveness.\n- A comparative test incorporating DD + Contrastive Learning would add effectiveness to the proposed method.\n- Figure 6 requires more explanation, particularly regarding the methodology for generating and comparing states in each showcased scenario.\n\n### The method introduces several additional hyperparameters, as depicted in Figure 7, indicating a significant sensitivity to these parameters, which could complicate the tuning process."
                },
                "questions": {
                    "value": "- The distinctions among the three models presented in Figure 5 are not very clear to me.\n- For the experiments depicted in Figure 6, how are the generated states and actual states obtained in each case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722217399,
            "cdate": 1698722217399,
            "tmdate": 1699637386303,
            "mdate": 1699637386303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mT5Av4Djte",
                "forum": "WNxlJJIEVj",
                "replyto": "NWTSdeeTpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer FaMZ (Q1-Q6)"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments! We have conducted more experiments and revised our paper according to your suggestions. The responses to your questions (Q1-Q6) are listed below: \\\n**Q1**: The training process of the model remains unclear. While Equation 14 suggests end-to-end training, it is unclear where the contrastive loss is integrated. If added to the diffusion probabilistic model, which aims to reconstruct the un-corrupted trajectories, will this added loss diverge the learning, making the training unstable?\\\n**A1**: The pseudo code of CDiffuser's training process is available in Appendix A.1, and the hyper parameters of  each environment are shown in  Appendix A.3. The contrastive loss is added to the diffusion probabilistic model.  To evaluate how contrastive loss impacts the training process, we visualized the loss during the training process with various hyperparameter values, as is shown in Figure 8 (Appendix A.2). As Figure 8 illustrates,  the added contrastive loss actually has no significant impact on the stability of training.\n\n\n**Q2**\uff1aIs the contrastive loss involved during guidance sampling?\\\n**A2**\uff1aNo, the contrastive loss is not involved during guidance sampling.\nThe contrastive loss is limited to the training phase. After being guided by the contrastive loss during the training phase, the diffusion probabilistic model performs guided sampling from the distribution of states with higher rewards, leading to better results.\n \n**Q3**\uff1aThe claim of significant improvements in medium and medium-replay datasets' seems overstated. The improvements are noticeable in only one task from each dataset compared to DD.\\\n**A3**\uff1aThank you for your comment. Here, the significant improvements refers to the comparison of CDiffuser and Diffuser, as CDiffuser takes Diffuser as the backbone. As is shown in Table 1, compared with Diffuser, our approach CDiffuser demonstrates clear advantages across all the environments. Nevertheless, we have modified the description of significant improvements in Section 4.2 to avoid any ambiguous expressions.\n \n**Q4**\uff1aExtending experiments to more complex control tasks or scenarios with high-dimensional state/action spaces would substantiate the method's effectiveness.\\\n**A4**\uff1aThank you for your comment.  Following Diffuser, to evaluate the performance of CDiffuser on tasks beyond simple periodic tasks such as hopper, walker2d and halfcheetah, we conduct additional experiments on another three RL environments maze2d-umaze-v1, maze2d-medium-v1, maze2d-large-v1 in Section 4.1. which is not periodic  and tests the model's ability for long-horizon planning. Same to Diffuser, we compare CDiffuser with CQL, IQL, and Diffuser. The results are available in Table 1, and are summarized as follows:\n|Environment|CQL|IQL|Diffuser|CDiffuser|\n|:----|:----|:----|:----|:----|\n|Maze2D-UMaze|5.7|47.4|113.9|**142.9\u00b12.2**|\n|Maze2D-Medium|5.0|34.9|121.5|**140.0\u00b10.7**|\n|Maze2D-Large|12.5|58.6|123.0|**131.5\u00b13.2**|\n\nAs can be observed, CDiffuser achieves better results than baselines across all three environments, especially on maze2d-umaze-v1, where CDiffuser demonstrated a clear advantage of 25.4% over Diffuser. The results in the Maze2d environments indicate that CDiffuser is also effective in scenarios that require future planning.\n \n**Q5**\uff1aA comparative test incorporating DD + Contrastive Learning would add effectiveness to the proposed method.\\\n**A5**: Thank you for your suggestion! Actually, we once had intentions of applying the contrast mechanism on DD, however, we encountered the same problem with other researchers: we are unable to reproduce the results reported in DD (https://github.com/anuragajay/decision-diffuser/issues/1) . Nevertheless, here are the results of our early experiments:\n\n|Environment|DD|DD + Contrastive Learning|\n|:----|:----|:----|\n|Hopper-Med-Expert|66.7\u00b11.4|**83.0 \u00b1 0.9**|\n\nThe results show that incorporating contrastive learning into DD also brings much improvement.\n \n**Q6**\uff1aFigure 6 requires more explanation, particularly regarding the methodology for generating and comparing states in each showcased scenario.\\\n**A6**: Thank you for your suggestion. We have adjusted Figure 6 and the descriptions corresponding to it in Section 4.4."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459948087,
                "cdate": 1700459948087,
                "tmdate": 1700462675876,
                "mdate": 1700462675876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ymXcBExbl0",
                "forum": "WNxlJJIEVj",
                "replyto": "NWTSdeeTpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer FaMZ (Q7 and Q8)"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments! We have conducted more experiments and revised our paper according to your suggestions. The responses to your questions Q7 and Q8 are listed below: \\\n**Q7**: The method introduces several additional hyperparameters, as depicted in Figure 7, indicating a significant sensitivity to these parameters, which could complicate the tuning process.\\\n**A7**: Thank you for your comment. **Firstly**, every hyperparameter is necessary to perform contrastive learning, since we need them to define the positive/negative indicator, as is introduced in Section 3.2.1.  **Secondly**, Figure 7 shows how the performance of our method varies with the hyperparameter. It can be observed that CDiffuser exhibits a smooth and regular change in performance with hyperparameters various, so it is actually easier for us to tune the parameters. \n\n**Q8**: The distinctions among the three models presented in Figure 5 are not very clear to me.\\\n**A8**: Thank you for your suggestion. We have replotted Figure 5. Colored scatters are the states collected during the models' interaction with the environment, while the gray scatters are the states in the dataset.  As can be observed, compared with Decision Diffuser and Diffuser, CDiffuser achieves higher rewards in both in-distribution areas(circled with blue) and out-of-distribution areas(circled with red)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460013961,
                "cdate": 1700460013961,
                "tmdate": 1700460458374,
                "mdate": 1700460458374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZSSIDAWlrY",
                "forum": "WNxlJJIEVj",
                "replyto": "ymXcBExbl0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed explanation.\n\n**Q1-Q3**: Thank you for your clarification.\n\n**Q4**: Maze2D is a simple, low-dimensional state/action space task. The results are not convincing enough to demonstrate the proposed method's effectiveness and robustness. \n\n**Q5**: The result on only one task is also not convincing, I can not make any concrete conclusion based on this.\n\n**Q6**: Thanks for the revision. It now looks better to me.\n\n**Q7**: Tuning so many sensitive hyperparameters will make extending the proposed method to other tasks challenging. The authors only tested their method on Gym-MuJoCo and simple Maze2D tasks. It is still not clear if the proposed method will be effective on other complex tasks, i.e., Kitchen, AntMaze, etc.\n\n**Q8**: Thank you! It's much clearer now."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716635448,
                "cdate": 1700716635448,
                "tmdate": 1700716635448,
                "mdate": 1700716635448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RGqoXHToUv",
                "forum": "WNxlJJIEVj",
                "replyto": "NWTSdeeTpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer FaMZ (Q1-Q4)"
                    },
                    "comment": {
                        "value": "Thank you for your response and comments! The responses to your questions Q1-Q4 are listed below:\\\n**Q1-Q3**: Thank you for your clarification.\\\n**A1-A3**: We are pleased that our response has addressed your confusion. \n\n**Q4**: Maze2D is a simple, low-dimensional state/action space task. The results are not convincing enough to demonstrate the proposed method's effectiveness and robustness. \\\n**A4**: Thank you for your  comments. We agree that it is worth exploring the performance of CDiffuser in more complex high-dimensional tasks, and we are evaluating CDiffuser on complex tasks  such as Kuka Block Stacking, Antmaze, and the results will be updated soon on our anonymous repo https://anonymous.4open.science/r/ContrastiveDiffuser.\nWe believe CDiffuser has great potential to achieve outstanding performance in those tasks for two reasons:  Firstly,  CDiffuser has the potential to gain better performance than Diffuser, which has demonstrated outstanding performance in some high-dimensional tasks like Kuka Block Stacking according to the results of [7,8]. Since the only difference between CDiffuser and Diffuser is the contrastive module, and the effectiveness of contrastive learning in processing high-dimensional information has been proven in the area like CV and RL[1,2,3,4,5,6], CDiffuser is supposed to have outstanding performance on complex tasks with high dimension.\nSecondly, the keys for an RL model to achieve outstanding performance on complex tasks are the state encoder and the policy network: (1) The state encoder is designed to encode the high-dimensional information (i.e., visual information) to a low-dimensional vector, and the state encoders are easily deployed to many RL approaches. Therefore, it is possible to equip CDiffuser with an efficient state encoder;  (2) The policy network is designed to make decisions based on the low-dimensional vectors of states and the rewards. Since CDiffuser's policy has demonstrated excellent performance  on  locomotion tasks and navigation tasks (see Table 1), and CDiffuser can share the same encoder with other methods, it is supposed to achieve outstanding performance in high-dimensional tasks. \n\n[1]Laskin, Michael, Aravind Srinivas, and Pieter Abbeel. \"Curl: Contrastive unsupervised representations for reinforcement learning.\" International Conference on Machine Learning. PMLR, 2020.\\\n[2]Ma, Xiao, et al. \"Contrastive variational reinforcement learning for complex observations.\" Conference on Robot Learning. PMLR, 2021.\\\n[3] Laskin, Michael, et al. \"CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery.\" Deep RL Workshop NeurIPS 2021. 2021.\\\n[4] Sun, W., Zhang, J., Wang, J., Liu, Z., Zhong, Y., Feng, T., ... & Barnes, N. (2023). Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6420-6429).\\\n[5]Yang, J., Li, C., Zhang, P., Xiao, B., Liu, C., Yuan, L., & Gao, J. (2022). Unified contrastive learning in image-text-label space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19163-19173).\\\n[6] Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., ... & Jitsev, J. (2023). Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2818-2829).\\\n[7] Ajay, Anurag, et al. \"Is conditional generative modeling all you need for decision-making?.\" arXiv preprint arXiv:2211.15657 (2022).\\\n[8] Janner, Michael, et al. \"Planning with diffusion for flexible behavior synthesis.\" arXiv preprint arXiv:2205.09991 (2022).\\"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737154865,
                "cdate": 1700737154865,
                "tmdate": 1700737410343,
                "mdate": 1700737410343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cNJR8uoCcX",
                "forum": "WNxlJJIEVj",
                "replyto": "NWTSdeeTpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer FaMZ (Q5-Q8)"
                    },
                    "comment": {
                        "value": "Thank you for your response and comments! The responses to your questions Q5-Q8 are listed below:\\\n**Q5**: The result on only one task is also not convincing, I can not make any concrete conclusion based on this.\\\n**A5**: Thank you for your  comments. There are two reasons that we did not report the results of  DD + Contrastive Learning in our paper.  Firstly, as we mentioned previously,  we are unable to reproduce the results reported in the paper of DD, although we directly use the public code in the GitHub repo of DD (https://github.com/anuragajay/decision-diffuser). Here are the results. \n\n\n|Environment            |           DD(Reproduced) |  DD(Reported) |\n|  ----  | ----  | ----  |\n|Hopper-Med-Expert      |     66.7\u00b11.4            |     111.8\u00b11.8|\n|Hopper-Med             |          16.7\u00b10.53      |         79.3\u00b13.6|\n|Hopper-Med-Replay       |   25.1\u00b10.8             |    100\u00b10.7|\n|Halfcheetah-Med-Expert  | 39.1\u00b11.3              |    90.6\u00b11.3|\n\nAs we can observe,  there are many differences between the results of reproducing and the results reported by DD.\n\nSecondly, we did conduct some experiments on DD + Contrastive Learning, and here are the results.\n|Environment     |                  DD              |        DD + Contrastive Learning|\n|  ----  | ----  | ----  |\n|Hopper-Med-Expert     |    66.7\u00b11.4           |     83.0 \u00b1 0.9|\n|Hopper-Med            |         16.7\u00b10.53      |        26.8 \u00b1 0.8|\n\nAs can be observed, DD + Contrastive Learning brings significant improvement (33.4% better than DD on hopper-med-expert, 60.5% better than DD on hopper-med). It can be inferred that Contrastive+DD should exhibit clear improvements on the other tasks. \n\n**Q6**:  Thanks for the revision. It now looks better to me. \\\n**A6**:  We are pleased that our response has addressed your confusion.\n\n**Q7**: Tuning so many sensitive hyperparameters will make extending the proposed method to other tasks challenging. The authors only tested their method on Gym-MuJoCo and simple Maze2D tasks. It is still not clear if the proposed method will be effective on other complex tasks, i.e., Kitchen, AntMaze, etc. \\\n**A7**: Thanks for your clarification. For the tuning of hyperparameters,  we have offered the suggested hyperparameters in Appendix A. 3. Also, readers can try an easier version of CDiffuser with fewer hyperparameters by directly designating the positive and negative samples to conduct contrast (CDiffuser-Easy). Here is the performance of CDiffuser of the easier version:\n\n|Variants             |       halfcheetah-med-exp |  hopper-med-exp |  walker2d-med-exp |   halfcheetah-med |  hopper-med |  walker2d-med | halfcheetah-med-replay |  hopper-med-replay |  walker2d-med-replay |\n|  ----               |             ----             | ----                  | ----      | ----  |                ----         | ----          | ----                           | ----  |----  |\n|CDiffuser-Easy       |    90.8           |    112.1|  108.2 | 43.8 | 85.4 | 82.9 | 40.0 | 95.4 | 83.2 |\n|CDiffuser            |        92.0   |     112.4| 108.2 | 43.9 | 92.8 | 82.9 | 40.0 | 96.4 | 84.2 |\n|Diffuser            |     88.9   |     103.3 | 106.9 | 42.8 | 74.3 | 79.6 | 37.7  | 93.6 | 70.6 |\n\nThe experiment results demonstrate that CDiffuser with fewer hyperparameters also has better performance.\n\nFor the evaluation of complex tasks, please refer to **Q4**.\n\n**Q8**: Thank you! It's much clearer now.\\\n**A8**: We are pleased that our response has addressed your confusion."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737410742,
                "cdate": 1700737410742,
                "tmdate": 1700740490260,
                "mdate": 1700740490260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7zjpJm2hj5",
                "forum": "WNxlJJIEVj",
                "replyto": "NWTSdeeTpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_FaMZ"
                ],
                "content": {
                    "comment": {
                        "value": "**Q5**: The result on only one task is also not convincing, I can not make any concrete conclusion based on this\n\nI happened to run DD (officially released code) on Hopper-Med-Expert from my side; though I couldn't reproduce the reported result, I got a score of around 103 (raw rewards 3345.3, averaged over 10 random samples), much higher than the results obtained from the authors. I encourage authors to communicate with DD's author on the reproduction issues directly and try to seek some help from DD's author."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737879861,
                "cdate": 1700737879861,
                "tmdate": 1700738319396,
                "mdate": 1700738319396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j0qRS6iJIx",
                "forum": "WNxlJJIEVj",
                "replyto": "NWTSdeeTpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer FaMZ"
                    },
                    "comment": {
                        "value": "Thanks for your suggestion. We conducted experiments by leveraging the same parameters of DD repo (averaged with 10 random seeds). These results of DD and DD + contrastive learning are obtained under the same hyper-parameters and seeds, therefore the results reported above are comparable to some extent. Nevertheless, we will try to communicate with DD's author on the repo's issues to check the training and evaluating settings."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740221352,
                "cdate": 1700740221352,
                "tmdate": 1700740246735,
                "mdate": 1700740246735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "olpRA5MBDa",
            "forum": "WNxlJJIEVj",
            "replyto": "WNxlJJIEVj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission513/Reviewer_F1CC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission513/Reviewer_F1CC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a diffusion-based trajectory generation based on contrasting high-return and low-return training samples, called CDiffuser. The core approach in the method is performing a contrastive learning between generated trajectory and samples in the dataset. The contrastive learning serves as a guidence to the diffusion process, and pushes the generated trajectories towards high return states and away from low return states. Experimental results on Gym show the improvements of the proposed algorithm."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of combining contrastive learning with trajectory generation is somehow novel.\n\n- The analysis on the similary of generated states are informative.\n\n- The ablation study on different loss terms are appreciated. The study on using high-reward samples only is great."
                },
                "weaknesses": {
                    "value": "1. The performance improvement may not be significant. According to Table, 1 the performance is highly comparable to DD. \n\n2. The benchmark only uses Gym.\n\n3. The method is using one step generation from EDP. However, Table 1 and ablation study do not include the comparison against this method. \n\n4. The method highly relies on EDP to make the contrastive loss differentiable through the generated states, from my understanding. However, this could be hard to generalize to other diffusion-based methods. \n\n5. The guidence on return is confusing. The return is predicted from the very **first state** of the **noisy trajectory**, according to the third line after Equation (6). How can the prediction and the learned model be accurate, when solely from a noisy state?  And clarifications on its backpropogation is needed, since it only takes the noisy trajectory input, and the denoising process only takes one step.\n\n6. The original diffuser seperately trains the auxiliary return prediciton model on all data. This modification is not discussed and experimentally validated.\n\n7. Can the authors explain the reasons of intriguing properties presented in 4.4?\n\n8. The contrastive loss Equation (9) seems to not be a common form. Usually the denominator considers all the samples, for example in [2,3]. This is a concern on the correctness of this implementation and a justification is needed.\n\nBased on the points above, I am not convinced the proposed method is sound and could actually work in terms of training.\n\n[1] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. arXiv preprint arXiv:2305.20081, 2023.\n\n[2] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\n\n[3] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727917242,
            "cdate": 1698727917242,
            "tmdate": 1699635977776,
            "mdate": 1699635977776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0mC9bZX7eq",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer F1CC (Q1-Q4)"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments! We have conducted more experiments and revised our paper according to your suggestions. The responses to your questions 1-4 are listed below:\\\n**Q1**: The performance improvement may not be significant. According to Table 1 the performance is highly comparable to DD.\\\n**A1**: Thank you for your comment. Here, the significant improvements refers to the comparison of CDiffuser and Diffuser, as CDiffuser takes Diffuser as the backbone. As is shown in Table 1, compared with Diffuser, our approach CDiffuser demonstrates clear advantages across all the environments. Nevertheless, we have modified the description of significant improvements in Section 4.2 to avoid any ambiguous expressions.\n\n**Q2**: The benchmark only uses Gym. \\\n**A2**: Thank you for your comment.  Following Diffuser, to evaluate the performance of CDiffuser on tasks beyond simple periodic tasks such as hopper, walker2d and halfcheetah, we conduct additional experiments on other three RL environments maze2d-umaze-v1, maze2d-medium-v1, maze2d-large-v1 in Section 4.1. which is not periodic and tests the model's ability for long-horizon planning. Same to Diffuser, we compare CDiffuser with CQL, IQL and Diffuser. The results are available at Table 1, and are summarized as follows:\n\n|Environment|CQL|IQL|Diffuser|CDiffuser|\n|:----|:----|:----|:----|:----|\n|Maze2D-UMaze|5.7|47.4|113.9|**142.9\u00b12.2**|\n|Maze2D-Medium|5.0|34.9|121.5|**140.0\u00b10.7**|\n|Maze2D-Large|12.5|58.6|123.0|**131.5\u00b13.2**|\n\n\nWe can observe, CDiffuser achieves better results than baselines across all three environments, especially on maze2d-umaze-v1, where CDiffuser demonstrated a clear advantage of 25.4% over Diffuser. The results in the Maze2d environments indicate that CDiffuser is also effective in scenarios that require future planning.\n \n\n**Q3**: The method uses one-step generation from EDP. However, Table 1 and the ablation study do not include the comparison against this method. \\\n**A3**: Thank you for your comment. CDiffuser and EDP leverage diffusion models differently. Specifically, following Diffuser and Decision Diffuser, CDiffuser models RL problems as sequence generation problems and leverages diffusion models to generate subsequent  trajectories at each time step. However, methods like EDP and Diffusion-QL leverage diffusion models to act as the policy network, which only predicts action for the current time. In conclusion,  CDiffuser and EDP adopt completely different pipelines.  Considering these, we compare CDiffuser with sequence generation models such as Diffuser and Decision Diffuser rather than EDP.\n \n**Q4**\uff1aThe method highly relies on EDP to make the contrastive loss differentiable through the generated states, from my understanding. However, this could be hard to generalize to other diffusion-based methods. \\\n**A4**:  Thank you for noticing the extension of CDiffuser on other RL methods. We would like to clearify that Firstly, CDiffuser is independent of EDP. As is mentioned in the reply for Q3, CDiffuser and EDP adopted completely different pipelines. Secondly, transplanting the CDiffuser framework to other RL methods is quite convenient. For methods that predict the future states such as Diffuser and Decision Diffuser, the CDiffuser framework can be naturally applied to them as  our contrastive loss $\\mathcal{L}_{c}$ in Equation (13) only requires states to contrast. For other methods that only predict actions,  such as Diffusion-QL and EDP, we just need to make little modifications to enable them to predict states while predicting actions, and then our framework can be applied to these methods.  Moreover, the CDiffuser framework is not limited to contrast over states. For example, we can conduct contrast over actions. We leave that as our future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458752998,
                "cdate": 1700458752998,
                "tmdate": 1700458972702,
                "mdate": 1700458972702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1vgjo83qa5",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer F1CC (Q5-Q7)"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments! We have conducted more experiments and revised our paper according to your suggestions. The responses to your questions 5-7 are listed below:\\\n**Q5**\uff1aThe guidance on return is confusing. The return is predicted from the very first state of the noisy trajectory, according to the third line after Equation (6). How can the prediction and the learned model be accurate, when solely from a noisy state? And clarifications on its backpropagation is needed, since it only takes the noisy trajectory input, and the denoising process only takes one step. \\\n**A5**: Thank you for your question. Firstly, we would like to clarify that the return is predicted from the whole trajectory, rather than the very first state of it. Secondly, the design of guiding over return is borrowed from Diffuser[1] and Classifier Guided Diffusion[2], in which the return predictor $\\mathcal{J}$ is designed to be able to take noisy samples and denoising step as input and output the true returns. Please refer to [1,2] for more details. According to the performance of the Diffuser and CDiffuser, providing guidance by predicting return over noisy trajectory is feasible. \n\n[1] Janner, Michael, et al. \"Planning with Diffusion for Flexible Behavior Synthesis.\" International Conference on Machine Learning. PMLR, 2022. \\\n[2] Dhariwal, Prafulla, and Alexander Nichol. \"Diffusion models beat gans on image synthesis.\" Advances in neural information processing systems 34 (2021): 8780-8794.\n \n**Q6**: The original diffuser seperately trains the auxiliary return prediciton model on all data. This modification is not discussed and experimentally validated. \\\n**A6**: We separately train the auxiliary return prediction model on all data as is described in Equation (12) and Equation (14), just the same as Diffuser does. The parameters of return predictor $\\mathcal{J}$ and diffusion model are independent, therefore updating the parameters of $\\mathcal{J}$ separately is equal to updating $\\mathcal{J}$'s parameters with $\\mathcal{L}$.\n\n**Q7**: Can the authors explain the reasons of the intriguing properties presented in 4.4? \\\n**A7**: Thank you for your suggestion. We have discussed the properties of Figure 5 and Figure 6 in Section 4.4. Please refer to our revised paper for details."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459435791,
                "cdate": 1700459435791,
                "tmdate": 1700459435791,
                "mdate": 1700459435791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1wikkOrj0P",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer F1CC (Q8)"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments! We have conducted more experiments and revised our paper according to your suggestions. The response to your question 8 is listed below:\\\n**Q8**: The contrastive loss Equation (9) seems to not be a common form. Usually, the denominator considers all the samples, for example in [2,3]. This is a concern about the correctness of this implementation and a justification is needed. \\\n**A8**: Thank you for your question. **Firstly**, sampling a batch as negative samples is commonly adopted in many previous works[1, 4, 5]. The underlying reason is that Equation (9) is iterated for a huge number of steps (1e6 for CDiffuser), which makes Equation (9) equivalent to considering all the samples in the denominator.\n**Secondly**, the contrastive loss in Equation (9) is derived from InfoNCE in [2,3]. We made a few modifications to adapt it to the scenario of a large number of samples. For the negative samples, we randomly sample a batch of size $\\kappa$ for calculating the distance of the sample $\\hat{s}$ from the original sample space. The reason for this modification is that even if we use the anchor function to classify a small portion of samples as negative (For example, 30%), calculating the distance for all samples is impractical due to the extremely large number of samples (190,000 in the case of med-expert). However, this challenge can be approximately addressed by randomly sampling a batch of size $\\kappa$ and iterating a huge number of steps externally (1e6 for CDiffuser), as is the denominator of Equation (9). For the positive samples, we take multiple positive samples for each sample and average the loss values. This extends InfoNCE in [2,3] to the case of multiple positive samples.  The underlying reason is that we want the model learn a distribution close to the distribution of positive samples, not just reproducing a single positive sample. It is worth noting that the numerator of Equation (9) should have a coefficient of $\n\\frac{1}{\\kappa}$, however, this will transform into a coefficient during the process of gradient descent. Therefore, we merge it into the weight of the contrastive learning loss, $\\lambda_c$, and omit it from equation (9). \n\n[1]Wang, Hao, et al. \"Knowledge-Adaptive Contrastive Learning for Recommendation.\" Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 2023.\\\n[2] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\\\n[3] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673.\\\n[4] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\\\n[5] Wonsung Lee, Jaeyoon Chun, Youngmin Lee, Kyoungsoo Park, and Sungrae Park. 2022. Contrastive Learning for Knowledge Tracing. In Proceedings of the ACM Web Conference 2022 (WWW '22). Association for Computing Machinery, New York, NY, USA, 2330\u20132338. https://doi.org/10.1145/3485447.3512105"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459509943,
                "cdate": 1700459509943,
                "tmdate": 1700459509943,
                "mdate": 1700459509943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tlmYRlmxbP",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_F1CC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_F1CC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. \n\n**1 & 2:**\n\nThanks for the clarification and additional experiments. In the revised paper Section 4.4 the authors mentioned Diffuser, DD and proposed method share a similar framework of trajectory generation. So it is a bit contradictory and confusing. Further clarifications are appreciated.\n\n**3 & 4:**\n\nIn the submission Section 3.2.2 the authors describe that the proposed method is following Kang et al. 2023 to do one-step denoising. This is not addressed in the response. This paper is using this one-step denoising but not addressing how this one-step denoising affects the performance. Kang et al. definitely focus on a different usage, but the one step-denoising should be well ablated in this submission. The authods did not address the result of removing this one-step denoising. \n\n**5:**\n\nThanks for the clarification! I think the sentence is already fixed in the revised version.\n\n**6:**\n\nThanks. I highly suggest that the author could revise the paper to clearly describe it. But this point is still confusing. Diffuser first trains a diffusion model then train J (Page 4, section 3.2), while this paper somehow trains them together according to the response and Equation (14). Could you please further clarify?\n\n**7:**\n\nI appreciate those explanations.\n\n**8:**\n\nWant et al., also consider the positive pairs, i.e. the $\\mathbb{N} \\cup \\\\{{v\\\\}} $ in their equation (8).\nLee et al. also consider the positive paris in their equations (7) and (8).\nChen et al. (SimCLR) rely on large batch and do not have explicit negative pairs which is not applicable to CDiffusor. CDiffusor explicitly samples negative samples. \n\nI want to clarify that \"all samples\" mean both positive and negative pairs in each batch. I understand the difficulty of calculating on all data samples in the dataset. The authors should further clarify this point.\n\nSince the conefficient of contrastive loss is merged into hyper-parameters, and different environment uses different sets of them according to Appendix A.3. The weight of contrastive loss is very small compared to others. The significance of the contrastive loss is questionable."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609764233,
                "cdate": 1700609764233,
                "tmdate": 1700610142743,
                "mdate": 1700610142743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x0LH1JHQBn",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer F1CC (Q1-Q5)"
                    },
                    "comment": {
                        "value": "Thank you for your response and comments! The response to your questions Q1-Q5 are listed below:\n\n**Q1 & 2**:  In the revised paper Section 4.4 the authors mentioned Diffuser, DD and proposed method share a similar framework of trajectory generation. So it is a bit contradictory and confusing. \\\n**A1 & 2**: Thank you for your response. The similar framework means that Diffuser, DD and CDiffuser apply diffusion to model the RL as a sequence generation problem. Nevertheless, we have revised our paper to avoid ambiguity.\n\n**Q3 & 4**: In the submission Section 3.2.2 the authors describe that the proposed method is following Kang et al. 2023 to do one-step denoising. This is not addressed in the response. This paper is using this one-step denoising but not addressing how this one-step denoising affects the performance. Kang et al. definitely focus on a different usage, but the one step-denoising should be well ablated in this submission. The authods did not address the result of removing this one-step denoising.\\\n**A3 & 4**: Thank you for your clarification! There are three reasons that we did not conduct ablation studies on ont-step denoising. Firstly, we focus on the contrast of states rather than the one-step denoising, and one-step denoising is the contribution of EDP rather than ours. Therefore, we only conduct the ablation studies on contrast. Secondly, multi-step denoising increases the computational requirements significantly, which is beyond the resources we can afford. Thirdly, the ablation studies in EDP demonstrate that one-step generation slightly decreases the performance. Therefore, intuitively speaking, CDiffuser is supposed to achieve better performance with multi-step denoising, which has no negative impact on our current comparison with the baselines.\n\n\n**Q5**: Thanks for the clarification! I think the sentence is already fixed in the revised version.\\\n**A5**: We are pleased that our response has addressed your confusion."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666366993,
                "cdate": 1700666366993,
                "tmdate": 1700706348346,
                "mdate": 1700706348346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X4XORu4DYM",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer F1CC (Q6 and Q7)"
                    },
                    "comment": {
                        "value": "Thank you for your response and comments! The response to your questions Q6 and Q7 are listed below:\n\n\n**Q6**: Thanks. I highly suggest that the author could revise the paper to clearly describe it. But this point is still confusing. Diffuser first trains a diffusion model then train J (Page 4, section 3.2), while this paper somehow trains them together according to the response and Equation (14). Could you please further clarify? \\\n**A6**: Of course, we would love to. We have also added the explaination in Appendix A.5 as you suggested. Here are the details of our explaination. Firstly, Diffuser trains $\\mathcal{J}$ and diffusion model $\\psi$ separately.  Secondly, although the objective of $\\mathcal{J}$ and $\\psi$ are put into one equation (Equation 14), $\\mathcal{J}$ and $\\psi$ share no parameters, and their inputs are independent with each other. \n\nTherefore, during the training process, the gradient backpropagation for the parameters of $\\mathcal{J}$ and $\\psi$ is also independent. The only thing common in $\\mathcal{J}$ and $\\psi$ is that, they're sharing the same Dataloader, which is actually the same if we set a seed for the training of $\\mathcal{J}$ and $\\psi$ in Diffuser. In other words, training them 'together' with Equation (14) is equal to training them in the manner of Diffuser.\n\nHere is an example to explain why:\\\nSuppose we have the diffuison model $\\psi_{\\theta}(\\cdot)$ parameterized by $\\theta$, and the return predictor $\\mathcal{J}_{\\phi}$ parameterized by $\\phi$. At each training iteration, Dataloader provides a batch contains as $\\\\{s_t, \\tau_t, v_t\\\\}$.  \nFollowing Equation(14), \n$$\n\\begin{equation}\n    \\mathcal{L} = \\lambda_d \\mathcal{L_d} + \\lambda_v \\mathcal{L_v} + \\lambda_c \\mathcal{L_c}.\n\\end{equation}\n$$\nFurther, \n$$\n\\begin{equation}\n    \\mathcal{L}_d = \\mathbb{E}\\_{{\\tau}\\_t \\in \\mathcal{D}, t>0, i \\sim [1, N]} [ || {\\tau}_t - \\psi\\_\\theta ({\\tau}^i_t,i) ||^2 ],\n\\end{equation}\n$$\n\n$$\n\\begin{equation}\n    \\mathcal{L}_v = \\mathbb{E}\\_{{\\tau}\\_t\\in \\mathcal{D}, t>0, i \\sim [1, N]}[|| \\mathcal{J}\\_\\phi({\\tau}\\_t^i, i) - v_t||^2].\n\\end{equation}\n$$\n\nThe training process can be viewed as a procedure of calculating gradients of all the parameters and updating them, specifically,\n$$\n\\begin{align}\n    \\nabla\\theta \n    &= \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\\\\\\\\n    &= \\lambda_d\\frac{\\partial \\mathcal{L_d}}{\\partial \\theta} + \\lambda_v\\frac{\\partial \\mathcal{L_v}}{\\partial \\theta} + \\lambda_c\\frac{\\partial \\mathcal{L_c}}{\\partial \\theta} \\\\\\\\\n    &= \\lambda_d\\frac{\\partial \\mathcal{L_d}}{\\partial \\theta} + \\lambda_c\\frac{\\partial \\mathcal{L_c}}{\\partial \\theta},\n\\end{align}\n$$\n$$\n\\begin{align}\n    \\nabla\\phi \n    &= \\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\\\\\\\\n    &= \\lambda_d\\frac{\\partial \\mathcal{L_d}}{\\partial \\phi} + \\lambda_v\\frac{\\partial \\mathcal{L_v}}{\\partial \\phi} + \\lambda_c\\frac{\\partial \\mathcal{L_c}}{\\partial \\phi}  \\\\\\\\\n    &= \\lambda_v\\frac{\\partial \\mathcal{L_v}}{\\partial \\phi}.\n\\end{align}\n$$\n\nThus, calculating the gradients of $\\theta$ with $\\mathcal{L}$ is equal to calculating $\\theta$ with $\\mathcal{L}_d$ and $\\mathcal{L}_c$, calculating the gradients of $\\phi$ with $\\mathcal{L}$ is equal to calculating $\\phi$ with $\\mathcal{L}_v$, $i.e.$,  training them 'together' with Equation (14) is equal to training them in the manner of Diffuser. \n\n**Q7**: I appreciate those explanations. \\\n**A7**: We are pleased that our response has addressed your confusion."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667442233,
                "cdate": 1700667442233,
                "tmdate": 1700709878133,
                "mdate": 1700709878133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dnof2ZUkGl",
                "forum": "WNxlJJIEVj",
                "replyto": "olpRA5MBDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer F1CC (Q8)"
                    },
                    "comment": {
                        "value": "Thank you for your response and comments! The response to your question Q8 are listed below:\n\n**Q8(1)**: Want et al., also consider the positive pairs, i.e. the $\\mathbb{N} \\cup \\{{v\\}} $ in their equation (8). Lee et al. also consider the positive paris in their equations (7) and (8). Chen et al. (SimCLR) rely on large batch and do not have explicit negative pairs which is not applicable to CDiffusor. CDiffusor explicitly samples negative samples.I want to clarify that \"all samples\" mean both positive and negative pairs in each batch. I understand the difficulty of calculating on all data samples in the dataset. The authors should further clarify this point. \\\n**A8(1)**: Thank you for your comment. Firstly, different from the works mentiond in the references[1,2,3] which consider one positive sample for each sample, we consider $\\kappa$ positive samples to guide the learned distribution. Inspired by [4,5], we adopt Equation (9) as the learning target to pull the states in the generated trajectory toward the high-return states and away from the low-return states.\nSecondly, Equation (9) is similar to the objectives in [1,2,3]. Most of the works mentioned above is based on the InfoNCE:\n$$\n\\begin{align}\n    \\theta^*_{InfoNCE} \n    & = \\underset{\\theta}{\\arg \\min} -\\mathbb{E} \\left[  log\\frac{exp(sim(f(s),s^+)/T)} {exp(sim(f(s),s^+)/T) + \\sum_{s^- \\in S^-} exp(sim(f(s),s^-)/T) }   \\right] \\\\\\\\\n    & = \\underset{\\theta}{\\arg \\min} \\mathbb{E} \\left[  log\\frac{exp(sim(f(s),s^+)/T) + \\sum_{s^- \\in S^-} exp(sim(f(s),s^-)/T) }{exp(sim(f(s),s^+)/T)}  \\right] \\\\\\\\\n    & = \\underset{\\theta}{\\arg \\min} \\mathbb{E}  \\left[  log(1+ \\frac{\\sum_{s^- \\in S^-} exp(sim(f(s),s^-)/T)}{exp(sim(f(s),s^+)/T)} )        \\right],\n\\end{align}\n$$\nand optimizing this equation is similar to optimizing (though the gradients are different):\n$$\n\\begin{align}\n    & \\underset{\\theta}{\\arg \\min} \\mathbb{E}  \\left[  log(\\frac{\\sum_{s^- \\in S^-} exp(sim(f(s),s^-)/T)}{exp(sim(f(s),s^+)/T)} )        \\right] \\\\\\\\\n    &= \\underset{\\theta}{\\arg \\min} - \\mathbb{E}  \\left[  log(\\frac{exp(sim(f(s),s^+)/T)}{\\sum_{s^- \\in S^-} exp(sim(f(s),s^-)/T)} )        \\right].\n\\end{align}\n$$\nThus, Equation (9) is similar to the InfoNCE mentioned in [1,2,3].\n\n[1]Wang, Hao, et al. \"Knowledge-Adaptive Contrastive Learning for Recommendation.\" Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 2023.\\\n[2] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\\\n[3] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673.\\\n[4] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815\u2013823, 2015\\\n[5]Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. Advances in neural information processing systems, 29, 2016.\n\n**Q8(2)**: Since the conefficient of contrastive loss is merged into hyper-parameters, and different environment uses different sets of them according to Appendix A.3. The weight of contrastive loss is very small compared to others. The significance of the contrastive loss is questionable. \\\n**A8(2)**: Thank you for your comment. Firstly, the weight of contrastive loss is used to balance the objectives  in $\\mathcal{L}$, and the values of each weight in Equation (14) is adjusted based on the performance.  Secondly, the expiremental results demonstrate the effectiveness of contrastive loss: **(1)** The hyper-parameters analysis in Figure 7 (d) show that, the weight of contrastive loss has significant impact on the performance, although its value is small. **(2)** The ablation study demonstrates that removing the contrastive loss (Diffuser-C) decreases the performance on all of the 9 locomotion tasks, which shows the significance of the contrastive loss."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667754606,
                "cdate": 1700667754606,
                "tmdate": 1700708755070,
                "mdate": 1700708755070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SKwNKGbmIx",
            "forum": "WNxlJJIEVj",
            "replyto": "WNxlJJIEVj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
            ],
            "content": {
                "summary": {
                    "value": "This paper combines a trajectory planning method based on a diffusion model and contrastive learning to select states with higher returns. States are grouped into fuzzy sets of low and high reward and then used to constrain the trajectory planning by pulling states towards regions of higher return. An extensive ablation study, comparison with state of the art and hyperparameter search shows good results and the importance of all components."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The method is suprisingly simple yet effective. It can be probably easily adopted for a wide range planning problems or even tasks without explicit trajectories beyond the typical RL tasks in table 1."
                },
                "weaknesses": {
                    "value": "It is hard to find weaknesses in this paper. Sometimes the sentences are a bit long and convey a lot of concepts at the same time which is not necessarily bad but harder to understand. One example is the sentence around equation 13. The impact of predictions in the future of planning could be elaborated a bit more, but this is just an example to illustrate my point.\n\nWhile being best and second best in the med-replay datasets, it could be argued if being so close to the other results can be called significant improvements and highlighting the second best is potentially done to have the results in the best possible light. However, the authors put their results in ample perspective and give reasonable hypothesis about the impact of expert examples."
                },
                "questions": {
                    "value": "- More a suggestion, Figure captions like Figure 2 could provide more information. The general function of both modules as take away message for the reader could improve the figure understanding even though it appears in the text pointing to this figure\n\n- In figure 5 I find it hard to see what is supposed to be in and out of distribution. Maybe some circles could help making the points from section 4.4. All three figures also look very alike. I get the idea of comparison here but not sure about the overall value of this. The nuanced color changes are also hard to see and some people are color blind."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790774629,
            "cdate": 1698790774629,
            "tmdate": 1699635977699,
            "mdate": 1699635977699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FXO4tOkXVW",
                "forum": "WNxlJJIEVj",
                "replyto": "SKwNKGbmIx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer VS91"
                    },
                    "comment": {
                        "value": "Thank you very much for your admiration of our work!  As you commented, CDiffuser can be further applied in many other RL tasks. Moreover, CDiffuser is not limited to performing contrastive learning over states but also can be extended to perform contrastive learning over actions. Also, thank you for your constructive comments! We have made some changes to the paper based on your suggestions. The responses to your questions and comments are listed below:\n\n**Q1**: While being the best and second best in the med-replay datasets, it could be argued if being so close to the other results can be called significant improvements, and highlighting the second best is potentially done to have the results in the best possible light.\\\n**A1**: Thank you for your comment. Here, the significant improvements refers to the comparison of CDiffuser and Diffuser, as CDiffuser takes Diffuser as the backbone. As is shown in Table 1, compared with Diffuser, our approach CDiffuser demonstrates clear advantages across all the 12 settings. Nevertheless, we have modified the description of significant improvements in Section 4.2 to avoid any ambiguous expressions.\n \n**Q2**: More a suggestion, Figure captions like Figure 2 could provide more information. \\\n**A2**: Thank you for your suggestions. We have added a brief description of the pipeline in the caption of Figure 2 to improve the figure understanding.\n \n**Q3**: In figure 5 I find it hard to see what is supposed to be in and out of distribution. Maybe some circles could help making the points from section 4.4. All three figures also look very alike. I get the idea of comparison here but not sure about the overall value of this. The nuanced color changes are also hard to see and some people are color blind. \\\n**A3**: Thank you for your suggestion. We have replotted Figure 5. Colored scatters are the states collected during the models' interaction with the environment, while the gray scatters are the states in the dataset.  As can be observed, compared with Decision Diffuser and Diffuser, CDiffuser achieves higher rewards in both in-distribution areas(circled with blue) and out-of-distribution areas(circled with red)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457372581,
                "cdate": 1700457372581,
                "tmdate": 1700458934099,
                "mdate": 1700458934099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yNM4XgjMt9",
                "forum": "WNxlJJIEVj",
                "replyto": "FXO4tOkXVW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission513/Reviewer_VS91"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. I have read all the discussion of all other reviewers. From the perspective of trajectory generation and contrastive learning I can see the value of this approach and would be happy if it were to be discussed in the larger research community at the conference. I think the method is interesting for application in my domain and the results show decently honest that it is needed to further evaluate and improve it because it does not beat the state of the art by a large margin. I still see the large methodical contribution of the method. However, I understand the critique of my dear colleagues from the perspective of generalization to other diffusion based methods and while I still think this paper warrants acceptance I would not be upset if I were to be overruled."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650988800,
                "cdate": 1700650988800,
                "tmdate": 1700650988800,
                "mdate": 1700650988800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]