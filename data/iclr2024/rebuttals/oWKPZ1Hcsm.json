[
    {
        "title": "Efficient Offline Reinforcement Learning: The Critic is Critical"
    },
    {
        "review": {
            "id": "7wGVlxLX11",
            "forum": "oWKPZ1Hcsm",
            "replyto": "oWKPZ1Hcsm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8212/Reviewer_p7js"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8212/Reviewer_p7js"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach that combines supervised learning and off-policy reinforcement learning to enhance efficiency and stability. This is achieved through pre-training the critic using a supervised Monte-Carlo value-error and applying regularization to both the actor and the critic. The results demonstrate a reduction in training time and improved learning efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Utilizing Monte-Carlo estimation as the initialization for offline RL is reasonable, yet it is ignored in prior works.\n* The efficacy of the proposed method is demonstrated through experiments conducted on MuJoCo and Adroit tasks.\n* Implementation details are provided in the Appendix."
                },
                "weaknesses": {
                    "value": "* First of all, the overall structure and writing of this paper necessitate meticulous reorganization and refinement. Some paragraph is confusing and hard to follow due to poor organization. Especially in Section 4 and Section 5, the important conclusion in these paragraphs need to be highlighted and summarized. In Section 5, the transition from Monte Carlo (MC) pretraining to emphasizing both actor and critic regularization is perplexing, especially since these regularization are not introduced in the methods section. And the title \"Application to Adroit Environments\" is incongruous as the methodology differs from the prior parts.\n\n* The two parts of pretraining and regularization that the authors want to underscore appear to be incremental additions rather than naturally integrated components. This disjointed presentation detracts from the coherence of the paper and needs to be addressed.\n\n* Regarding the methodology, the paper's primary emphasis appears to be on the use of Monte Carlo (MC) estimates as pretraining targets. However, the results in Appendix A.3 indicate that the efficiency and performance during pretraining stem from the Behavioral Cloning (BC) loss rather than MC. MC only contributes to stability during subsequent fine-tuning. Furthermore, in Section 5, the authors assert that pretraining is less critical than both regularization techniques. Consequently, I am unconvinced about the significance of this work.\n\n*  The experiments are also limited in variety of dataset types and domains. For instance, BC pretraining may depend on data quality; therefore, additional dataset types such as \"medium-replay\" and \"random\" datasets are necessary to substantiate the importance of this work. Moreover, further ablation studies are required to validate the paper's claims. Current results in Figures 3, 5, and 6, which only base on a single dataset, are unconvincing. Additionally, the inclusion of more domains, such as AntMaze, would be beneficial."
                },
                "questions": {
                    "value": "* Revise the structure and refine the writing to make the conclusions and emphasized information more evident.\n\n*  The two parts of the methods seem to be added incrementally rather than integrated as natural components. The authors need to improve the presentation to address this problem.\n\n* The reviewer finds the significance of the MC pretraining unconvincing based on the results in Appendix A.3 and Section 5.\n\n* How is the performance of the pretraining approach on \"medium-replay\" and \"random\" datasets?\n\n* It is necessary to conduct more ablation experiments with additional environments.\n\n* How does pretraining and the regularizations perform on the AntMaze domain?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8212/Reviewer_p7js",
                        "ICLR.cc/2024/Conference/Submission8212/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698134526827,
            "cdate": 1698134526827,
            "tmdate": 1700732463255,
            "mdate": 1700732463255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xyxrz07uUC",
                "forum": "oWKPZ1Hcsm",
                "replyto": "7wGVlxLX11",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer p7js (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your honest review and helpful feedback, which we have now incorporated to improve the coherence and completeness of our paper. In particular, we have attempted to address each of your concerns/weaknesses as follows:\n\n> Revise the structure and refine the writing to make the conclusions and emphasized information more evident.\n\nWe have restructured and re-written Sections 4 and 5 following your advice to better separate our experimental setup and implementation details from the results and discussion in each section. Each paragraph now corresponds to a separate point of analysis, and we have highlighted the key conclusions in each section. We have also renamed Section 5 from \u2018Application to Adroit Environments\u2019 to \u2018Extension to Data-Limited Adroit Environments\u2019, and added a Section \u2018Motivation for Actor and Critic Regularisation with Pre-Training\u2019 to explain the necessary extension of our methodology to handle the limited-data setting considered in this section.\n\n> The two parts of the methods seem to be added incrementally rather than integrated as natural components. The authors need to improve the presentation to address this problem.\n\nAs mentioned above, we have now added an additional Section (5.1) to better justify our addition of combined actor and critic regularisation with pre-training. To summarise here, pre-training trains the actor and critic networks to first match the behaviour policy distributions, which improves the efficiency of subsequent off-policy RL, as we show in Section 4. However, in data-limited domains, subsequent off-policy RL can lead to policy collapse, where the performance drops to close to zero after pre-training. To prevent this, and to try to maintain at least the pre-training performance, we can add additional regularisation to both the actor and the critic to prevent chain extrapolation away from the initialised behaviour distributions. Therefore, our proposal is to pre-train both the actor and critic to match the behaviour policy, and then regularise them to prevent extrapolation significantly out-of-distribution of the behaviour distributions.\n\nWhile we provide this additional justification of additional regularisation in Section 5.1, we also anticipate and highlight this idea throughout the paper, with the addition of regularisation being mentioned in the last paragraph of the introduction (Section 1), the overview of our approach in Section 3 (including Algorithms 1 and 2), and the abstract and conclusion.\n\n> The reviewer finds the significance of the MC pretraining unconvincing based on the results in Appendix A.3 and Section 5.\n\nAppendix A.3 provides an ablation which isolates the significance of the MC pre-training, so we are confused about why the reviewer is unconvinced of the significance based on these results. \n\nIn Figure 2, we see that pre-training both the actor with imitation learning and the critic with a Monte-Carlo value error (blue) provides an efficiency benefit over randomly initialising these networks (black, still using additional LayerNorm). However, one might wonder if pre-training the actor is enough, since this is what leads to non-zero performance during pre-training (and does not depend on the critic values at all, since it is only trying to imitate and not optimise behaviour during this stage). \n\nTherefore, in Appendix A.3, we perform actor only pre-training with imitation learning. We see that during pre-training performance is equivalent as expected, but after pre-training performance drops off and takes a long time to recover, generally performing similarly to training from scratch with TD-learning as in Figure 2. Therefore, since the blue line reaches maximal performance more quickly than both the black line in Figure 2 and the red line in Figure 4 (Appendix A.3) we have isolated that the significant efficiency gain is due to our proposed MC pre-training.\n\nIn Section 5, we agree that pre-training does not help in these environments (as we mention) since performance collapses with or without pre-training. Therefore it is the combined regularisation approach that we propose that is responsible for the performance gains over the baselines in these data-limited environments. However, the pre-training stage is still beneficial for obtaining a non-zero initial baseline performance that allows for easier tuning of the regularisation parameters to prevent subsequent performance collapse."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651361835,
                "cdate": 1700651361835,
                "tmdate": 1700651361835,
                "mdate": 1700651361835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T7F3RU6gjR",
                "forum": "oWKPZ1Hcsm",
                "replyto": "uHbSozmilV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8212/Reviewer_p7js"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8212/Reviewer_p7js"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thank you for your response. I appreciate the inclusion of additional results and modifications, which have undoubtedly improved this work. As a result, I am inclined to increase my score to 5. The paper requires further meticulous refinement in its writing to better structure the methodology. Specifically, the regularization part seems to be specifically tailored for adroit environments, which makes its organization stand apart."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732427335,
                "cdate": 1700732427335,
                "tmdate": 1700732427335,
                "mdate": 1700732427335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bI6nfCEksv",
            "forum": "oWKPZ1Hcsm",
            "replyto": "oWKPZ1Hcsm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8212/Reviewer_KvJB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8212/Reviewer_KvJB"
            ],
            "content": {
                "summary": {
                    "value": "Off-policy reinforcement learning is able to further improve the offline RL performance while suffering from instability and inefficiency. This works propose to bridge the supervised approach and the off-policy approach aiming for a more stable offline RL. The key innovation is the pre-training of the critic using a supervised Monte-Carlo value-error, which leverages information from offline data. This step provides a consistent actor and critic for off-policy TD-learning. The experiments on D4RL MuJoCo benchmark show that the proposed method is more stable and efficient during the offline training comparing with other method, such as behavior cloning and TD3.  Meanwhile the results in Adroit shows the proposed method can achieve good performance for most of the tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-motivated and focuses on an important problem in offline RL.\n2. The proposed method is easy to understand and shown to perform well comparing with previous methods."
                },
                "weaknesses": {
                    "value": "1.  The motivation example is rather unnecessary due to its simplicity\n2.  The consistence of the actor and critic networks play critical roles in the proposed method, while it is unclear how much degree of consistence is needed in order to make it work well for off-policy training? If it is possible to derive any explicit criteria on this matter?"
                },
                "questions": {
                    "value": "1. How do you choose the pretraining phase steps, e.g. different environments choose different pretraining steps in Figure 2. What is the impact of the pretraining steps on the final performance?\n2. What is the main reason for the huge performance drop in Walker2d-edium-EDAC?\n3. What is the computation complexity for the pretraining phase? Does it exceed a lot comparing with the offline training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728708180,
            "cdate": 1698728708180,
            "tmdate": 1699637019442,
            "mdate": 1699637019442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y51s8Wvd0g",
                "forum": "oWKPZ1Hcsm",
                "replyto": "bI6nfCEksv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer KvJB (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review of our work. We\u2019re pleased that you recognise the importance of our motivation and the simplicity and effectiveness of our method for improving the efficiency of offline RL.\n\nTo address your questions first:\n\n> How do you choose the pretraining phase steps, e.g. different environments choose different pretraining steps in Figure 2. What is the impact of the pretraining steps on the final performance?\n\nThe number of pre-training steps can be considered a hyperparameter which we set to between 10 and 50 epochs of the data depending on the environment in Figure 2. However, since both the actor and critic objectives in this pre-training phase are supervised objectives fit with a mean squared error, the loss is smooth and convex, so it is straightforward to see when these losses have converged to move onto the subsequent offline reinforcement learning objective. If the number of pre-training steps is fewer than required for convergence, this reduces the subsequent training efficiency to somewhere between the red (no critic pre-training) and the blue (pre-training steps required for convergence) performance lines in Figure 4 in Appendix A.3 (i.e. the performance would drop after pre-training). If the number of pre-training steps is greater than required for convergence, this just leads to unnecessary updates (effectively extending the width of the blue region with no benefit).\n\n> What is the main reason for the huge performance drop in Walker2d-edium-EDAC?\n\nFundamentally, this arises because at the end of pre-training we change the objective of the actor from trying to choose the action that would have been chosen in the dataset (imitation learning), to trying to choose the action that will maximise the return i.e. critic prediction (off-policy reinforcement learning). If the values predicted by the critic are sufficiently accurate for the behaviour policy as a result of our proposed critic pre-training, then the performance should smoothly improve as we see in the Hopper environment. However, if the values are incorrect then the performance will drop, as we see in the extreme case for the red lines in Figure 4 in Appendix A.3 where the critic is not pre-trained at all.\n\nIn the Walker2d environment, we have an intermediate case where the values have been pre-trained, but are high enough variance that sometimes an action is selected which is erroneously expected to have a high value but it does not, leading to a performance drop before the critic is updated to correct its value prediction. This is an example of the bias-variance tradeoff, as we discuss in Section 3.2. The values are generally higher variance for EDAC than for TD3+BC because the critic must predict soft Q values, which also incorporate an entropy bonus as defined by Equation 5. We believe the HalfCheetah and Walker environments are higher variance because the medium trajectories end with timeouts rather than termination (as for all but one trajectories in Hopper-medium) which are more difficult to  predict without access to the current timestep [1], as we mention in Section 4. The Walker2d-medium-EDAC case therefore has the highest variance because it uses soft Q-values incorporating entropy, the episodes end with a difficult to predict timeout condition, but the episodes can still end in termination (leading to high variance on evaluation). \n\nTo mitigate this performance drop, we can add in a temporal-difference component as in Equation 3, which reduces the variance at the cost of bias - in our case reducing the performance drop at the cost of efficiency. While we use a small temporal difference component for these environments ($\\lambda=0.1$), for increasingly large datasets used for offline RL, we generally expect that the variance will be lower and a temporal difference component will not be required for pre-training.\n\n> What is the computation complexity for the pretraining phase? Does it exceed a lot comparing with the offline training?\n\nThe pretraining phase has a lower computational complexity than the offline RL training (for the case that $\\lambda=0$). This is because each pre-training update requires one forward and backward pass of the actor and critic networks to regress them towards their static supervised target. However, each offline RL training update requires an additional forward pass of the actor network for the subsequent timestep and then the target critic on the subsequent timestep in order to compute the temporal difference target ($Q(s_{t+1}, \\pi(s_{t+1})$) for the critic to regress towards. This means each pre-training update is also generally faster than each offline RL update. If $\\lambda \\neq 0$, then the computational complexity is effectively equivalent.\n\nReferences:\n\n[1] Time Limits in Reinforcement Learning, https://arxiv.org/abs/1712.00378"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649803994,
                "cdate": 1700649803994,
                "tmdate": 1700649803994,
                "mdate": 1700649803994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UrcqMic4QY",
                "forum": "oWKPZ1Hcsm",
                "replyto": "bI6nfCEksv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8212/Reviewer_KvJB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8212/Reviewer_KvJB"
                ],
                "content": {
                    "title": {
                        "value": "Thank authors for the response"
                    },
                    "comment": {
                        "value": "I appreciate the authors effort on addressing my questions. Based on your response, my major concerns remain, in particular: 1) since this work is mainly focusing on the empirical study, I find many details are lacking (or not explained well). For instance, the details on the \"consistence\" of the A and C networks. Many further questions can be asked based on the paper, e.g., does inconsistent A C necessary will contribute to bad performance? 2) The pre-training step plays an important role on the learning performance and efficiency, which is treated as a hyper-parameter. It can be tricky for other people to utilize the results in practice. Overall I will keep my original evaluation on this work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700876511,
                "cdate": 1700700876511,
                "tmdate": 1700700983749,
                "mdate": 1700700983749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BVmmEUMfCa",
            "forum": "oWKPZ1Hcsm",
            "replyto": "oWKPZ1Hcsm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8212/Reviewer_NdGK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8212/Reviewer_NdGK"
            ],
            "content": {
                "summary": {
                    "value": "The authors in this paper consider pretraining the critic function using a mix of objectives of Monte-Carlo estimation and TD estimation of Q values from offline data and then train both the critic function and with policy with standard offline algorithms such as  TD3+BC. The empirical results demonstrate that such a training pipeline would help the latter training and make the policy learning converges faster."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors show a toy example at the beginning of the paper, which demonstrates the intuition why the pertaining of the critic function might help for later offline policy and critic learning.\n- The idea is simple to follow and not hard to implement. \n- The authors evaluate the idea on both simple offline benchmark environments such as mujoco, and also hard ones, such as Adroit environments. Demonstrate that the proposed method can be helpful for both simple and complex scenarios. \n- A detailed ablation study has been done in the appendix to make sure the proposed idea is valid and indeed helps the latter offline training."
                },
                "weaknesses": {
                    "value": "- The pretraining stage increases the complexity of the overall training pipeline. From the training curve, we can see that the training converges faster than that without the pertaining, but almost for each environment, the performance would first drop and then begin to improve, which is quite weird in terms of robustness for the training. \n- The authors did not provide the training curves of policy learning on hard environments, which makes me wonder if the performance drop would be even larger than that of standard mujoco environments."
                },
                "questions": {
                    "value": "- Please explain the performance drop phenomenon in detail. \n- Please provide the training curves on hard environments such as Adroit. \n- Please explain why the pertaining steps are different for different environments, is that a hyperparameter? \n- I wonder if the pertaining loss can be a regularization loss in additional to previous regularization loss, maybe in this way we can make sure the whole training is more robust and the training curve would be more smooth."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698901603016,
            "cdate": 1698901603016,
            "tmdate": 1699637019337,
            "mdate": 1699637019337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NxAPpGbqxf",
                "forum": "oWKPZ1Hcsm",
                "replyto": "BVmmEUMfCa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer NdGK"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your thorough and helpful review. We\u2019re pleased that you appreciated the simplicity and intuitiveness of the idea, the range of evaluation environments, and our ablations.\n\nTo address your concerns and questions:\n\n> Please explain the performance drop phenomenon in detail. \n\nThis is indeed an interesting phenomenon. Fundamentally, this arises because at the end of pre-training we change the objective of the actor from trying to choose the action that would have been chosen in the dataset (imitation learning), to trying to choose the action that will maximise the return i.e. critic prediction (off-policy reinforcement learning). If the values predicted by the critic are sufficiently accurate for the behaviour policy as a result of our proposed critic pre-training, then we expect the performance to smoothly improve as we see in the Hopper environment. However, if the values are inaccurate then the performance may drop, as we see in the extreme case for the red lines in Figure 4 in Appendix A.3 where the critic is not pre-trained at all.\n\nIn the HalfCheetah and Walker environments, we have an intermediate case where the values have been pre-trained, but are high enough variance that sometimes an action is selected which is erroneously expected to have a high value but it does not, leading to a performance drop before the critic is updated to correct its value prediction. We believe the HalfCheetah and Walker (medium) environments are higher variance because the medium trajectories end with timeouts rather than termination (for all but one trajectory in Hopper-medium) which is more difficult to predict without access to the current timestep [1], as we mention in Section 4. This is an example of the bias-variance tradeoff, as we discuss in Section 3.2. To mitigate this performance drop, we can add in a temporal-difference component as in Equation 3, which reduces the variance at the cost of bias - in our case reducing the performance drop at the cost of efficiency. While we use a small temporal difference component for these environments ($\\lambda=0.1$), for increasingly large datasets used for offline RL, we generally expect that the variance will be lower and a temporal difference component will not be required for pre-training.\n\n> Please provide the training curves on hard environments such as Adroit. \n\nWe have now provided these training curves in Appendix A.7. There is no noticeable drop-off in performance in these environments. However, as we discuss at the beginning of Section 5, we initially found that the pre-training performance collapsed entirely on these environments due to the fact that the datasets are very limited, so the critic values were leading to actions that were very different to those found in the dataset and therefore collapsing performance as we discuss in Section 2.2. As a result of this initial performance collapse, we added additional regularisation to ensure both the actor and critic remained close to the behaviour policy and prevent the actor capitalising on the extrapolated critic variance. This prevents the actor from taking actions that are substantially different to the behaviour policy and therefore mitigates any performance drop after pre-training.\n\n> Please explain why the pertaining steps are different for different environments, is that a hyperparameter? \n\nYes, the number of steps of pre-trainining can be considered a hyperparameter. However, since both the actor and critic objectives in this pre-training phase are supervised objectives fit with a mean squared error, the loss is smooth and convex, so it is straightforward to see when these losses have converged to move onto the subsequent offline reinforcement learning objective.\n\n> I wonder if the pertaining loss can be a regularization loss in additional to previous regularization loss, maybe in this way we can make sure the whole training is more robust and the training curve would be more smooth.\n\nYes, we agree that this could be sensible and we tried using the Monte-Carlo values as regularisation ourselves at one point during our research. However, we found that the performance can still drop using these values, and the standard CQL loss regularisation was more effective at preventing performance drop due to large OOD Q values, so we applied this to TD3+BC to regularise the critic instead. This also makes the training curves more smooth, as now provided in Appendix A.7.\n\nThank you again for your helpful review. We hope we have addressed your questions and you can appreciate the value of our contribution towards understanding and improving the efficiency of offline reinforcement learning.\n\nReferences:\n\n[1] Time Limits in Reinforcement Learning, https://arxiv.org/abs/1712.00378"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649283268,
                "cdate": 1700649283268,
                "tmdate": 1700649283268,
                "mdate": 1700649283268,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]