[
    {
        "title": "Efficient Modulation for Vision Networks"
    },
    {
        "review": {
            "id": "2mZjNUj7w1",
            "forum": "ip5LHJs6QX",
            "replyto": "ip5LHJs6QX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
            ],
            "content": {
                "summary": {
                    "value": "UPDATE: the rebuttal has answered many of the issues and I have reflected this in the score.\n\n\nThis paper describes an optimized deep learning architecture for vision tasks. It is related to a line of work that utilizes mixtures of transformers and CNNs or adds modulation to CNNs in order to come up with an architecture with high accuracy and low computational complexity and low latency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S: One strength of the paper is the relative simplicity of the architecture compared to some related works.\n\nS: Another strength seems to be good empirical results on selected vision tasks.\n\nS: Even though on the surface the work seems to be incremental improvement over VAN and FocalNets, this paper generalizes them nicely and provides a simpler alternative, which also seems to perform better. Also, any improvement in efficient models for vision tasks is naturally important."
                },
                "weaknesses": {
                    "value": "W: The paper is mainly constructive and experimental in nature. In the appendix there is a tentative explanation that describes that the modulation might lose effectiveness in larger networks. Expanding on this and at the same time showing this in larger networks would make the contribution stronger.\n\nW: One of the most curious things about the paper is the ablation results in Table 5. From there it seems that replacing the modulation with just a regular residual connection (sum) has quite modest performance drop (abs perf drop 1%). Without the multiplication, unless I am mistaken, the architecture reduces to a ResNet with specific hyperparameters and two-path construction. Could the authors discuss this. I think both contributions are valuable, but I am wondering whether it is correct to attribute the performance of the architecture to the modulation since other aspects seem to play even larger role than abs. perf. of 1%. \n\nW: Continuation of the above (without mult). Would the architecture without mult be second best of the architectures compared in Table 2. If so, please add it there.\n\nW: Without mult cont: What would be the performance of the ResNet (no mult) version with the VIT-style attention layers on top?\n\nW: Since this work is most closely related to VAN and FocalNets those works should be at least briefly mentioned in the related work section as well. \n\nW: In Fig 1 and corresponding text, would it make sense to mention where nonlinearities are applied?"
                },
                "questions": {
                    "value": "Q: Is Figure 1a) missing the softmax part of the transformer architecture? I think this is one of the key differences to the modulation designs.\n\nQ: In section 3.1, the FC layer seems the same as a 1x1 pointwise convolutional layer. If this is correct, it might be beneficial to mention this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498147079,
            "cdate": 1698498147079,
            "tmdate": 1700223429124,
            "mdate": 1700223429124,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YU8hcriBDG",
                "forum": "ip5LHJs6QX",
                "replyto": "2mZjNUj7w1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6eCe,\n\nWe are grateful for your insightful comments and constructive suggestions. We address your concerns and questions as below:\n\n-----\n\n\n**1. Tentative explanation that describes that the modulation might lose effectiveness in larger networks. Expanding on this and at the same time showing this in larger networks would make the contribution stronger.**\n\n**Response**:\n\n**Firstly, we clarify that while modulation is more suitable for efficient networks, but it is still effective in larger networks as well.** \n\n\nAs suggested, we developed a larger variant of EfficientMod by increasing its width and adding more blocks. This variant, named EfficientMod-Large, has approximately 25M parameters. We compare it with related baselines:\n\n| Model | Publisher | Parameters | FLOPs | Accuracy |\n|------|---|----|----|----|\n| Swin Trans | ICCV 2021 | 28M | 4.5G | 81.3 |\n| ConvNeXt | CVPR 2022 | 29M | 4.5G | 82.1 |\n| FocalNet-T [3,5] | NeurIPS 2022 | 28.4M | 4.4G | 82.1 |\n| FocalNet-T [3,5,7] | NeurIPS 2022 | 28.6M | 4.5G | 82.3 |\n| VAN-B2 | CVM 2023 | 26.6 | 5.0G | 82.8 |\n| EfficientMod-Large | - | **24.8M** | **3.1G** | **82.9** |\n\nAs evident from the table, EfficientMod-Large, while utilizing fewer parameters and significantly fewer FLOPs, achieves superior performance. This outcome, achieved without extensive network design optimization or a fine-tuned training recipe due to limited time, underscores the scalability and efficiency of EfficientMod in larger networks.\n\n\n\n**Secondly, we provide additional details on the tentative explanation that why modulation mechanism can implicitly achieve high dimensions.**\n\n\nConsidering the modulation formula as $y=x^2$ (for simplicity, excluding the transformation function in two branches), where $x \\in R^d$. This results in:\n\n$y = [x_1^2, x_2^2, x_3^2, ..., x_d^2] \\in R^d$. Here, $y = [x_1^2, x_2^2, x_3^2, ..., x_d^2]$ represents a new feature space that is linearly independent of the original input $x= [x_1, x_2, x_3, ..., x_d]$. \n\nWith a skip connection, we effectively combine the original $d$-dimensional features with the new $d$-dimensional features, doubling the feature dimension. This is distinct from simply widening the network's width. Through the modulation mechanism, each layer potentially doubles the feature dimension, leading to a significant increase in dimensionality when multiple layers are stacked.\n\nNotice that this is a tentative explanation and hence we placed it in the suppelemntary. A more rigorous analysis and proof would be an exciting avenue for future research, although it may extend beyond the scope of this current work.\n\n\n--------------------\n\n**2. One of the most curious things about the paper is the ablation results in Table 5 ... since other aspects seem to play even larger role than abs. perf. of 1%.**\n\n**Response**:\n\n**We apologize for any confusion caused by Table 5 and appreciate your curiosity regarding its results**. To clarify, the performance variations observed in the table are attributed to different factors. The reduction in performance in top rows is due to a decrease in parameters and FLOPs. However, the change from multiplication to summation in our model, which does not alter the parameters or FLOPs, results in a significant accuracy drop of 1%.\n\nTo provide a clearer understanding, we have revised Table 5 to include FLOPs and parameters, and have explicitly indicated which components were removed or altered:\n\n\n\n\n| f(.) | Conv | g(.) | Params. | FLOPs | Acc | Acc drop | Comment |\n|---|---|---|------|----|----|-------|-------|\n| \u2713 | | | 11.3M | 1.2G | 72.7 | -7.8 | Remove second FC layer, no sptial interction |\n| | \u2713 | | 10.3M | 1.1G | 78.6 | -0.9 | Remove two FC layers |\n| | | \u2713 | 11.3M | 1.2G | 72.3 | -8.2 | Remove first FC layer, no sptail interaction |\n| \u2713 | \u2713 | | 11.6M | 1.3G | 79.8 | -0.7 | Remove second FC layer |\n| | \u2713 | \u2713 | 11.6M | 1.3G | 79.6 | -0.9 | Remove first FC layer |\n| \u2713 | \u2713 | \u2713 | 12.9M | 1.4G | 80.5 | - | - |\n| mul. | to | sum | 12.9M | 1.4G | 79.5 | -1.0 | Change multiplication to summation |\n\n\nThis updated table elucidates the critical role of the multiplication operation in our method. Changing from multiplication to summation, while keeping the parameters and FLOPs constant, leads to a 1.0% decrease in accuracy, a notable gap for efficient models. In fact, this performance gap is larger than that caused by reducing parameters and FLOPs (see row 2, 4 and row 5).  Row 1 and Row 2 means no convlutional layer in the network, hence no spatial interactions, resulting to significant performance drop. The primary objective of this ablation study was to assess the impact of each component in our design individually. Replacing multiplication with summation does transform our network into a ResNet-like architecture, ignoring detailed designs.\n\n-----\n\nWe will post rest responses later."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180834100,
                "cdate": 1700180834100,
                "tmdate": 1700180834100,
                "mdate": 1700180834100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0fb3vaFyR4",
                "forum": "ip5LHJs6QX",
                "replyto": "LfHMQn977V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal. It addresses many of the concerns I have raised and I have reflected this in the score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700223466152,
                "cdate": 1700223466152,
                "tmdate": 1700223466152,
                "mdate": 1700223466152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Io9VBb5CpQ",
            "forum": "ip5LHJs6QX",
            "replyto": "ip5LHJs6QX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a unified convolutional-based building block, EfficientMod, which incorporates favorable properties from both convolution and attention mechanisms. Comparing the prior work shown in Figure 1(b), EfficientMod firstly fuses the FC layers from the MLP and the modulation block to achieve better efficiency, resulting in a unified block. Secondly, EfficientMod includes simplified context modeling, which employs one large-kernel depth-wise convolution layer between two linear projection layers. Extensive experiments and comparisons demonstrate the effectiveness of the proposed method across a range of different tasks (classification, detection, and segmentation)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method, EfficientMod, is remarkably simple, which could exert a significant influence when deploying a deep model on a resource-limited device.\n- The experimental results clearly showcase the effectiveness of EfficientMod in outperforming existing state-of-the-art methods across various tasks (classification, detection, and segmentation)."
                },
                "weaknesses": {
                    "value": "- Examining Figure 1, and comparing (b) and (c), the proposed EfficientMod block fuses the MLP on the top and the modulation block as one unified block to improve efficiency. It is conceivable that this might limit performance when using the same number of parameters. The authors should elucidate the principles behind this design, not only from the perspective of efficiency but also in terms of representational ability.\n-  Building on the first point, it is imperative to present a comparison between (b) and (c) with the same number of parameters, both in terms of performance and efficiency, and under the same training settings. For example, a comparison between (b), only with fused MLP, and (c).\n- Could the authors discuss whether the transformer block can also benefit from the proposed method for efficiency (at least it is feasible to fuse the MLP into one unified block)?\n- In Figure 8 and Figure 9, one of the notations in the legend should be 'EfficientMod'."
                },
                "questions": {
                    "value": "My main concern lies with points 1 and 2 in the weaknesses. I look forward to the authors\u2019 response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t",
                        "ICLR.cc/2024/Conference/Submission8518/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775703016,
            "cdate": 1698775703016,
            "tmdate": 1700691653980,
            "mdate": 1700691653980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "czgl0DPmgp",
                "forum": "ip5LHJs6QX",
                "replyto": "Io9VBb5CpQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Q55t,\n\nThank you for your constructive comments, which we have found immensely valuable. Although the MLP fusion is only a small part of EfficientMod, we appreciate your concerns regarding it (weakness 1,2, and 3) and are glad to discuss it in more detail.\n\n------\n\n**Weakness 1: The authors should elucidate MLP fusion not only from efficiency but also representational ability.**\n\n**Response:** We appreciate your thoughtful recommendations. We examined the effectiveness of slimming the modulation architecture by integrating an extra MLP layer with our EfficientMod block and modifying expansion ratios to retain comparable parameters and FLOPs. That is, the EfficientMod is divided into an EfficientMod block (having an expansion ratio of 1) and an MLP block (with an expansion ratio of (r-1)). Here are the results based on EfficientMod-s-Conv:\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|----|-----|-----|------|-----|----|\n| EfficientMod-s-Conv (sperate MLP) | 12.9M | 1.5G | 80.6 | 6.2 ms | 26.2 ms |\n| EfficientMod-s-Conv | 12.9M | 1.5G | 80.5 (-0.1) | 5.8 ms (-0.4) | 25.0 ms (-1.2) |\n\n\nAs shown in the table, the sperating MLP block just have a marginally better representational ability (80.6 vs. 80.5), which is negligible. \nBy fusing MLP with the EfficientMod block, we are able to increase the inference speed on the CPU and GPU a lot.\n\n\n-----\n\n**Weakness 2:  It is imperative to present a comparison between (b) and (c) with the same number of parameters, both in terms of performance and efficiency, and under the same training settings. For example, a comparison between (b), only with fused MLP, and (c)**\n\n**Response:**\nThanks for your suggestions. Fig.1 (b) is a general concept that describes our summarized modulation mechanism, and the inner implementation of context modeling is not limited. We take FocalNet as an example and compare it with our method. Table 4 presented a comparison between FocalNet@4M and our EfficientMod-xxs. Here, we customized a FocalNet variant at 12.7M parameters and compared it with our EfficientMod-s-Conv for a more detailed comparison.  The results are presented here:\n\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|-----|-----|----|---|----|-----|\n| FocalNet@12M (sperate MLP) | 12.7M | 2.1G | 79.4 | 8.6 ms | 45.1 ms |\n| EfficientMod-s-Conv (sperate MLP) | 12.9M | 1.5G | 80.6 | 6.2 ms | 26.2 ms |\n| EfficientMod-s-Conv | 12.9M | 1.5G | 80.5 (-0.1) | 5.8 ms (-0.4) | 25.0 ms (-1.2) |\n\nFocalNet (an instance for Fig.1 (b)) performs much worse than ours EfficientMod-s-Conv, in terms of both latency and performance. When fusing the MLP block to the focal modulation block, we reduce the latency of FocalNet to 7.9 ms and 44.0 ms on GPU and CPU, respectively. However, we didn't train the model successfully, maybe a detailed modification is required.\n\n-----\n\n**Weakness 3: Discuss whether the transformer block can also benefit from the proposed method for efficiency (at least it is feasible to fuse the MLP into one unified block)** \n**Response**: It is feasiable to Transformer block to fuse the MLP into one unified block. We thank the reviewer for the insightful questions, which has a great impact to Transformer-based architectures, not only in efficient networks, but also in large models. \n\nIndeed, there are some endeavors that tried to fuse MLP block to a unified block for faster inference. For example, ViT-22B [1] applies the Attention and MLP blocks in\nparallel for faster inference, instead of sequentially as in the standard Transformer. In detail, the Q, K, V, and first layer in the original MLP block are fused into one layer, the attention-out layer and the second layer in the original MLP block are fused. Also, PaLM [2] also considers this strategy to speed up inference, rewriting the standard formulation of Transformer from $y=x + MLP(LN(x+Attention(LN(x))))$ to $y=x + MLP(LN(x) + Attention(LN(x)))$. As indicated, the unified block results in roughly 15% faster training speed at large scales. \n\nHence, it is feasible to fuse the MLP block to a unified block for faster inference. Our EfficientMod represents an early exploration of this approach in the realm of efficient networks.\n\n\n\n[1] Dehghani, Mostafa, et al. \"Scaling vision transformers to 22 billion parameters.\" ICML, 2023.\n\n[2] Brown, Tom, et al. \"Language models are few-shot learners.\" NeurIPS, 2020.\n\n\n-------------------\n\n**Weakness 4:  In Fig. 8 and 9, one of the notations should be EfficientMod**\n\n**Response**: Thank you for pointing out this. We will amend the legend to correctly represent 'EfficientMod' (not 'MobileMod') in our revised manuscript.\n\n-----------------\nAgain, we extend our deepest gratitude for your insightful questions.\nWe hope our responses can address your concerns. Please feel free to share any further questions or concerns if any. We are eager to engage in further discussions and happy to refining our work based on your valuable insights."
                    },
                    "title": {
                        "value": "Rebuttal"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517673100,
                "cdate": 1700517673100,
                "tmdate": 1700517688099,
                "mdate": 1700517688099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KkrCKofHpE",
                "forum": "ip5LHJs6QX",
                "replyto": "czgl0DPmgp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' efforts in the rebuttal. Their response has addressed my concerns, and I am going to increase my rating."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691513133,
                "cdate": 1700691513133,
                "tmdate": 1700691513133,
                "mdate": 1700691513133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "okwV8G9sDS",
            "forum": "ip5LHJs6QX",
            "replyto": "ip5LHJs6QX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_fvW3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_fvW3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient modulation block to build efficient vision networks. The proposed EfficientMod block mainly consists of a simple context modeling design (CTX) with FC and Conv layers. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that the proposed method achieves strong performance compared with prior methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe proposed model is simple yet effective.\n2.\tThe proposed model shows strong performance on several benchmarks, including ImageNet, COCO, and ADE20K."
                },
                "weaknesses": {
                    "value": "1.\tIn Table 2, there are no latency reported for state-of-the-art efficient models.\n2.\tThe proposed method seems simple and more analysis and motivations for the design are needed to understand the principal of the design choice.\n3.\tImportant baselines such as ConvNeXt and Swin Transformer are not included in the comparisons."
                },
                "questions": {
                    "value": "See the weakness part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840158638,
            "cdate": 1698840158638,
            "tmdate": 1699637064433,
            "mdate": 1699637064433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q0biLWwFb8",
                "forum": "ip5LHJs6QX",
                "replyto": "okwV8G9sDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer fvW3,\n\nThanks a lot for your insightful feedback and the time you have dedicated to reviewing our work! We address your concerns as follows:\n\n-----\n**Weakness 1: In Table 2, there are no latency reported for state-of-the-art efficient models.**\n\n**Response:** \nLatency (for both CPU and GPU) is primarily reported in Table 1, comparing state-of-the-art efficient models. Table 2 focuses on demonstrating the empirical improvements of introducing attention to our method. \n\nHowever, to address your concern, we have benchmarked all models in Table 2 (incorporating results from Table 1) and present the updated results here:\n\n\n| Arch.|Model | Params | FLOPs | Acc. | Epoch | Latency GPU | Latency CPU |\n|:--------:|:----:|:------:|:-----:|:----:|:-----:|-----|--------|\n| Conv |RSB-ResNet-18 |12.0M |1.8G | 70.6 |300| 2.0 | 11.5|\n| Conv |RepVGG-A1 |12.8M |2.4G | 74.5 |120| 2.0 | 15.0|\n| Conv | PoolFormer-s12 |11.9M |1.8G | 77.2 |300| 5.0 | 22.3|\n| Conv | GhostNetv2x1.6 |12.3M |0.4G | 77.8 |450| 5.7 | 19.1|\n| Conv |RegNetX-3.2GF |15.3M |3.2G | 78.3 |100| 6.1 | 22.7|\n| Conv |FasterNet-T2|15.0M |1.9G | 78.9 |300| 4.4 | 18.4|\n| Conv |ConvMLP-M |17.4M |3.9G | 79.0 |300| 4.1 | 32.1|\n| Conv | GhostNet-A |11.9M |0.6G | 79.4 |450| - | - |\n| Conv |MobileOne-S4|14.8M |3.0G | 79.4 |300| 4.8 | 26.6|\n| Conv | EfficientMod-s |12.9M |1.5G | **80.5** |300| 5.8 | 25.0|\n| + Atten. | EfficientMod-s |12.9M |1.4G | **81.0** |300| 5.5 | 23.5|\n\n\nAs illustrated, our method demonstrates both high speed and promising performance. While a single scale of models may not fully represent the performance-latency trade-off across different models, we invite readers to refer to Figure 3 in our submission for a comprehensive efficiency analysis of our method.\n\n\n\n--------\n**Weakness 2: The proposed method seems simple and more analysis and motivations for the design are needed to understand the principal of the design choice.**\n\n**Response:** \nWe appreciate your perspective on the simplicity of our method. We believe that its simplicity, coupled with effectiveness, could be an advantage\n. The primary motivation for our design is the suitability of the modulation mechanism for efficient networks, given its computational efficiency and dynamic nature, blending the benefits of convolution and attention. This motivation of modulation mechanism enables EfficientMod to be straightforward yet impactful.\n\nIn detail, we further elaborate on our motivations and the analysis behind our design choices to clarify our approach:\n\n**Principles and Contributions:** At the core of our work, we have summarized the modulation mechanism and further developed the EfficientMod block. This design effectively harnesses the strengths of both convolution (i.e., local representation) and attention mechanisms (dynamics).\n\n**Customization of the EfficientMod Block:** We have tailored the EfficientMod block to enhance efficiency and effectiveness. This customization involved streamlining the modulation design and simplifying the context modeling process. These refinements are also important in achieving best balance between performance and computational efficiency.\n\n**Integration with Attention Blocks:** In integrating attention blocks, we strategically placed attention blocks only in the last two stages of the network for computational efficiency.\n\n\n\n\n--------------------\n**Weakness 3: Important baselines such as ConvNeXt and Swin Transformer are not included in the comparisons.**\n\n**Response:** \nWe excluded ConvNeXt and Swin Transformer because they were not designed for efficient networks. However, based on your suggestion, we have carefully designed efficient versions of these models and benchmarked them as follows:\n\n\n| Model| Top-1 | Param. | FLOPs| GPU latency| CPU latency (ms) |\n|---------|-------|--------|--------|--------------|------------------|\n| ConvNeXt @4M |75.5 | 4.3M | 0.7G | 3.1 ms| 11.8 ms |\n| Swin-T @4M |71.1 | 4.8M | 0.8G | -| -|\n| EfficientMod-xxs |**76.0** | 4.7M | 0.6G | **3.0 ms** | **10.2 ms** |\n\nOur EfficientMod-xxs model consistenly outperforms both ConvNeXt and Swin Transformer in terms of latency and accuracy. For ConvNeXt, we reduced the embedding dimension from 96 to 36. For Swin Transformer, we adjusted the configuration to embed_dim=32, depths=[3, 3, 9, 3], and num_heads=[4, 4, 8, 8], resulting in ConvNeXt @4M and Swin-T @4M with computational complexities comparable to our EfficientMod-xxs. These models were trained using the EfficientMod training recipe for a fair comparison. Swin-Transformer cannot be easily exported to ONNX format, but it empirically infers much slow then ConvNeXt and our method.\n\n------\nIn conclusion, we hope that our responses and the additional results address your concerns effectively. We are grateful for the opportunity to improve our work based on your valuable feedback! We are eager to make any additional revisions that might be necessary and are open to further discussion.\n\nBest,\nAuthors of Submission 8518"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699853791896,
                "cdate": 1699853791896,
                "tmdate": 1699853791896,
                "mdate": 1699853791896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "waufftvIv1",
                "forum": "ip5LHJs6QX",
                "replyto": "okwV8G9sDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further experimental studies for Weakness 2"
                    },
                    "comment": {
                        "value": "We greatly appreciate the insightful feedback from the reviewers.  As Weakness 1 and Weakness 3 are responsed by expiermental results, we have also conducted additional experiments to strengthen our arguments and address the identified weaknesses comprehensively for Weakness 2.\n\nIn previous rebuttal, we discussed about the motivation for our design. Here we experimentally show the detailed analysis for our designs.\n\n\n\n\n1. **Efficiency of Slimming Modulation Design (Sec. 3.2)**. \nIn addressing the concerns regarding our slimming modulation design, we have integrated an additional MLP layer into the EfficientMod block. This modification was aimed at assessing the impact of slimming on both performance and computational efficiency. Remarkably, this resulted in a notable reduction in both GPU and CPU latency, with a negligible impact on accuracy. This underlines the effectiveness of our slimming approach in enhancing model efficiency without compromising accuracy.\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|-----------|------|-------|------|-------|-------|\n| EfficientMod-s-Conv (sperate MLP) | 12.9M | 1.5G | 80.6 | 6.2 ms | 26.2 ms |\n| EfficientMod-s-Conv | 12.9M | 1.5G | 80.5 | 5.8 ms (-0.4) | 25.0 ms (-1.2) |\n\n\n2. **Efficiency of simplifying Context Modeling (Sec. 3.2)**. \nTo further validate the efficiency of our approach in simplifying context modeling, we compared our single kernel size (7x7) implementation against multiple convolutional layers with varying kernel sizes, as the implementation of FocalNet. Our findings reinforce the superiority of using a single, optimized kernel size. This strategy not only simplifies the model but also achieves a better accuracy-latency trade-off, demonstrating the practicality and effectiveness of our design choice.\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|----------|-------|-------|------|-------|-------|\n| EfficientMod-s-Conv (kernel: [3, 3]) | 12.7M | 1.4G | 79.7 | 5.8 ms | 28.5 ms |\n| EfficientMod-s-Conv (kernel: [3, 5]) | 12.8M | 1.5G | 80.1 | 6.1 ms | 29.0 ms |\n| EfficientMod-s-Conv (kernel: [3, 7]) | 12.9M | 1.5G | 80.2 | 6.4 ms | 29.7 ms |\n| EfficientMod-s-Conv (kernel: [5,5]) | 12.9M | 1.5G | 80.2 | 6.3 ms | 29.2 ms |\n| EfficientMod-s-Conv (kernel: [5,7]) | 13.0M | 1.5G | 80.3 | 6.6 ms | 29.8 ms |\n| EfficientMod-s-Conv (kernel: [3, 5,7]) | 13.1M | 1.5G | 80.5 | 7.2 ms | 32.4 ms |\n| EfficientMod-s-Conv (kernel: [7], ours) | 12.9M | 1.5G | 80.5 | 5.8 ms | 25.0 ms |\n\n\n\n3. **Integrating Attention in EfficientMod (Sec. 3.3)**: \nThe introduction of vanilla attention in the last two stages of EfficientMod aimed to improve global representation. We adjusted the block and channel numbers to ensure the parameter count remained comparable between EfficientMod-s-Conv and EfficientMod-s. The results highlight that EfficientMod-s not only shows improved performance but also reduced latency, thereby validating our approach in integrating attention for enhanced efficiency.\n\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|----------|--------|-------|----------|-----|--------|\n| EfficientMod-s-Conv | 12.9M | 1.5G | 80.5 | 5.8 ms | 25.0 ms |\n| EfficientMod-s | 12.9M | 1.4G | 81.0 | 5.5 ms | 23.5 ms |\n\nIn conclusion, the additional experiments and analyses affirm the distinct contributions and efficacy of each design element in our model. We believe these results comprehensively address the concerns raised and further underscore the novelty and significance of our work.\n\nWe thank you for your constructive feedback, which has been instrumental in refining our paper and clarifying our contributions to the field."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548403275,
                "cdate": 1700548403275,
                "tmdate": 1700607734054,
                "mdate": 1700607734054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AsR2C3rqEQ",
                "forum": "ip5LHJs6QX",
                "replyto": "okwV8G9sDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer  fvW3,\n\nThank you for the time and effort you have devoted to reviewing our work. We have carefully considered and responded in detail to the questions and concerns you raised.\n\nAs the author-reviewer discussion deadline is drawing near, we would like to know if our responses have effectively addressed your concerns. Your insights are invaluable to us, and we aim to ensure that our paper meets the highest standards based on your feedback.\n\nWe would like to again appreciate for your time for helping strengthen the paper with your suggestions.\n\nLooking forward to your feedback."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664351805,
                "cdate": 1700664351805,
                "tmdate": 1700664412329,
                "mdate": 1700664412329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PtysnKkxO3",
                "forum": "ip5LHJs6QX",
                "replyto": "mBPz6azd7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_fvW3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_fvW3"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. Overall some of my previous concerns have been addressed, but I tend to keep my rating. In the above rebuttal, the authors compare the proposed method with ConvNeXt @4M\tmodel with 75.5\\% top1 accuracy and 0.7G FLOPs. However, the official paper actually releases ConvNeXt/ConvNeXtv2 models with small scales, as listed in https://github.com/facebookresearch/ConvNeXt-V2#imagenet-1k-fine-tuned-models. Also, as shown in Fig. 1 of ConvNeXt-V2 paper, ConvNeXt-Atto and ConvNeXt-Femto has 75.7\\% and 77.5\\% top1 accuracy with 0.55G and 0.78G FLOPs, which might be better than the proposed method."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664507812,
                "cdate": 1700664507812,
                "tmdate": 1700664507812,
                "mdate": 1700664507812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4zya1XQK6o",
            "forum": "ip5LHJs6QX",
            "replyto": "ip5LHJs6QX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a new model structure based on previous modulation designs to further improve the efficiency (especially inference latency) and performance. The paper revisited previous modulation designs and improved the efficiency by reducing fragmented operations and simplifying the structure. The proposed method shows better performance than previous efficient networks on ImageNet with lower latency. The improvements also transfer to detection and segmentation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper had a clear introduction to previous works and how is the proposed method motivated from these works. This makes it easier to follow the work and understand how it works.\n2.\tThere are extensive experiments on multiple tasks. And the proposed method achieves better performance and latency than previous efficient models."
                },
                "weaknesses": {
                    "value": "1.\tThere are limited technical contributions in the work. This paper focuses on improving the latency of previous works. The improvements/changes from previous works are mainly engineering designs, for example, fuse multiple FC layers together, fuse multiple DWConv into a larger one, replace reshape operation with repeat. The guidance is mainly from previous works such as ShuffleNet v2, which is to reduce fragmented operations for improved latency. There are limited new insights.\n2.\tIt is not clear how much efficiency improvement does each design contribute. I suggest the author to conduct a thorough ablation study to show the impact of each structure change, and explain why it could achieve improvement.\n3.\tFig 1 is good to illustrate the difference between the proposed method and previous works. But it could be better to expand Fig 1 (c) in details when explaining the method. This makes it easier to understand the proposed structure and details.\n4.\tIn Table 1, why VAN and FocalNet results are not included? They seem to be the most relevant works.\n5.\tIn Table 2, why adding Attention even reduced the FLOPs?\n6.\tIn Table 1, are the GPU and CPU latency of different models measured on the same device?"
                },
                "questions": {
                    "value": "Please see the weakness part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899519754,
            "cdate": 1698899519754,
            "tmdate": 1699637064296,
            "mdate": 1699637064296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U7Fw2kr7Jz",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal  (part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer xsDm,\n\nWe greatly appreciate your insightful feedback and the time you have dedicated to reviewing our work! Your comments have been instrumental in enhancing the quality of our research. We address your concerns as follows:\n\n--------\n**Q6: In Table 1, are the GPU and CPU latency of different models measured on the same device?**\n\n**Response:** Yes, all models were benchmarked on the same device to ensure consistency in our measurements. \n\nAdditionally, we conducted benchmarks across various GPU types, as detailed in Supplementary Table 10. This approach was taken to maintain uniformity in the testing environment and minimize latency variances across different GPU models.\n\n--------\n**Q5: In Table 2, why adding Attention even reduced the FLOPs?**\n\n**Response:** The reason is that we replaced some EfficientMod blocks with Attention blocks to keep almost the same number of parameters, for a fair comparison. \n\nThis involved adjusting the expansion ratio in Attention blocks and other hyper-parameters. We believe that the number of parameters plays a crucial role in performance, and maintaining a consistent parameter number is essential for a fair comparison.\n\n\n-----\n**Q4: In Table 1, why VAN and FocalNet results are not included? They seem to be the most relevant works.**\n\n**Response:**  The comparison (EfficientMod, VAN, and FocalNet) is detailed in our Ablation Study Table 4, rather than Table 1. This is because VAN and FocalNet models do not fall under the category of 'efficient models' and lack small variants. \n\nFor the study, we developed a FocalNet variant with 4M parameters (FocalNet@4M), selecting the best-performing variant among several tested. Our analysis in Table 4 demonstrates that EfficientMod not only outperforms FocalNet and VAN in terms of efficiency but also exhibits significantly lower latency on both CPU and GPU.\n\nFor your convenience\uff0cwe included Table 4 in our submission here:\n\n| Model | Top-1(%)\u2191| GPU (ms)\u2193 | CPU (ms)\u2193   | Param. | FLOPs |\n|:------:|---------|------|-------|--------|-------|\n|VAN-B0 | 75.4  | 4.5  | 16.3 | 4.1M   | 0.9G  |\n|FocalNet@4M  | 74.5 | 4.2 | 16.8 | 4.6M   | 0.7G  |\n| EfficientMod-xxs  | **76.0**     | **3.0** | **10.2**  | 4.7M   | 0.6G  |\n\n---------\nWe will be posting our responses to the remaining questions shortly. We look forward to receiving further feedback and engaging in more detailed discussions. Thank you once again for your valuable contributions to improving our research."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699739668259,
                "cdate": 1699739668259,
                "tmdate": 1699739668259,
                "mdate": 1699739668259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nfvhs3jvQj",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
                ],
                "content": {
                    "title": {
                        "value": "Didn't see Part 2"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. But I didn't see the part 2 (if there is)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356667590,
                "cdate": 1700356667590,
                "tmdate": 1700356667590,
                "mdate": 1700356667590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z63Fvjgoq1",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer  xsDm,\n\nThanks for your patience since we are working for the experimental results for rebuttal Q2. Here is our rebuttal part 2.\n\n-------\n\n**Q3: It could be better to expand Fig 1 (c) in details when explaining the method.**\n\n**Response**: Thank you for your kind suggestions,  we would revise the Fig 1 (c) and add more details in it, ensuring that our method is communicated more effectively and comprehensibly. \n\n----------\n\n**Q2: It is not clear how much efficiency improvement does each design contribute.**\n\n**Response**:  \n\nThank you for prompting further exploration into the efficiency improvements contributed by each design aspect. We have conducted additional experiments to elucidate this:\n\n\n1. **About the efficiency of Sliming Modulation Design as described in Sec. 3.2**. We investigated the efficiency of slimming the modulation design by integrating an additional MLP layer with our EfficientMod block, adjusting expansion ratios to maintain similar parameters and FLOPs. The results indicate a notable reduction in latency with minimal impact on accuracy, demonstrating the effectiveness of this slimming approach:\n\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|-----------|------|-------|------|-------|-------|\n| EfficientMod-s-Conv (sperate MLP) | 12.9M | 1.5G | 80.6 | 6.2 ms | 26.2 ms |\n| EfficientMod-s-Conv | 12.9M | 1.5G | 80.5 | 5.8 ms (-0.4) | 25.0 ms (-1.2) |\n\n\n2. **About the efficiency of Simplifying Context Modeling as described in Sec. 3.2**. We compared our implementation using a single kernel size (7x7) with multiple convolutional layers of varying kernel sizes, following FocalNet's context modeling approach. Our results show that adopting a single constructive kernel size optimizes the accuracy-latency trade-off, as detailed in the following table:\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|----------|-------|-------|------|-------|-------|\n| EfficientMod-s-Conv (kernel: [3, 3]) | 12.7M | 1.4G | 79.7 | 5.8 ms | 28.5 ms |\n| EfficientMod-s-Conv (kernel: [3, 5]) | 12.8M | 1.5G | 80.1 | 6.1 ms | 29.0 ms |\n| EfficientMod-s-Conv (kernel: [3, 7]) | 12.9M | 1.5G | 80.2 | 6.4 ms | 29.7 ms |\n| EfficientMod-s-Conv (kernel: [5,5]) | 12.9M | 1.5G | 80.2 | 6.3 ms | 29.2 ms |\n| EfficientMod-s-Conv (kernel: [5,7]) | 13.0M | 1.5G | 80.3 | 6.6 ms | 29.8 ms |\n| EfficientMod-s-Conv (kernel: [3, 5,7]) | 13.1M | 1.5G | 80.5 | 7.2 ms | 32.4 ms |\n| EfficientMod-s-Conv (kernel: [7], ours) | 12.9M | 1.5G | 80.5 | 5.8 ms | 25.0 ms |\n\n\n\n3. **Integrating Attention in EfficientMod as described in Sec. 3.3**: To enhance global representation, we introduced vanilla attention in the last two stages of EfficientMod. The adjustments of block and channel number ensured comparable parameters between EfficientMod-s-Conv and EfficientMod-s, with the latter showing improved performance and reduced latency:\n\n\n| Method | Parameters | FLOPs | Accuracy | GPU Latency | CPU Latency |\n|----------|--------|-------|----------|-----|--------|\n| EfficientMod-s-Conv | 12.9M | 1.5G | 80.5 | 5.8 ms | 25.0 ms |\n| EfficientMod-s | 12.9M | 1.4G | 81.0 | 5.5 ms | 23.5 ms |\n\nThese findings affirm the distinct contributions of each design element to the model's overall efficiency.\n\n--------\n\n**Q1: Limited technical contributions in the work.**\n\n**Response**: We understand your concerns regarding the perceived technical contributions. While certain implementation aspects like layer fusion and reshaping operations are not our main focus, we emphasize that our primary contributions lie in:\n\n**Summarization of Modulation Mechanism**: We provide a comprehensive overview of the modulation mechanism in Section 3.1, highlighting its unique position in the landscape of efficient network designs.\n\n**Efficient Modulation Design**: As detailed in Section 3.2, we introduce the EfficientMod block, a novel building block for efficient networks, leveraging both convolutional and attention mechanisms.\n\n\nCompared with other efficient network designs, we hold the idea that modulation is unique and distinguished. It can take advantage of both convolution (local representation, efficiency) and attention mechanism (dynamics). Here we list a taxonomy of the representative efficient networks in Table 1 based on their main technical contributions:\n\n| Technical Contributions | Networks |\n|-------|----------|\n| DW-Convolution | MobileNet, MobileNetv2 |\n| Channel Shuffle | ShuffleNet, ShuffleNetv2 |\n| Feature Re-use | FasterNet |\n| Re-parameterization | MobileOne |\n| Neural Architecure Searching | EfficientNet, EfficientFormerV2 |\n| Hybrid architecture | MobileViT, EfficientFormerV2, MobileFormer |\n| Modulation Mechanism | EfficientMod (ours) |\n\nThis table underscores the distinctiveness of our contribution, positioning the modulation mechanism as a key innovation in EfficientMod. Our work focuses on advancing this aspect, offering a significant step forward in the field of efficient network design."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502596462,
                "cdate": 1700502596462,
                "tmdate": 1700607759457,
                "mdate": 1700607759457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W8SDLnyGfA",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Dear Reviewer xsDm,\n\nThank you very much for your insightful review. Your feedback has been invaluable in helping us enhance the quality of our work. \n\nIn response to your concerns, we have provided detailed additional results, further expanded our analysis, and revised our manuscript.\n\nAs the rebuttal deadline is approaching, we would greatly appreciate any further comments or suggestions you might have. Could you please let us know if our responses and revisions effectively address your concerns? We are committed to continuous improvement and eagerly await your guidance to further refine our work.\n\nThank you once again for your constructive feedback and support.\n\nBest regards,\n\nAuthors of Submission 8518"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632519649,
                "cdate": 1700632519649,
                "tmdate": 1700632519649,
                "mdate": 1700632519649,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YbsfHCWwEk",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Dear Reviewer xsDm,\n\nWe greatly appreciate your constructive suggestions and insightful comments on our work. As the rebuttal deadline is drawing near, we are keen to know if our responses have adequately addressed your concerns.\n\nPlease feel free to share any further questions or insights you may have. Your feedback is invaluable to us."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675006315,
                "cdate": 1700675006315,
                "tmdate": 1700675006315,
                "mdate": 1700675006315,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dxOu7fdS7w",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer xsDm,\n\nWe sincerely appreciate your insightful comments on our submission (ID: 8518). As the review period is nearing its conclusion, we are eager to know if our rebuttal has effectively addressed your concerns and questions.\n\nShould you have any further questions, or if there are additional aspects of our work that you would like us to clarify, please do not hesitate to let us know. Your feedback is invaluable to us!\n\nBest regards,\n\nAuthors of Submission 8518"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695307314,
                "cdate": 1700695307314,
                "tmdate": 1700695307314,
                "mdate": 1700695307314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yjWIZBGDyq",
                "forum": "ip5LHJs6QX",
                "replyto": "dxOu7fdS7w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. After reading the rebuttal, I still think there are very limited insightful takeaways and analyses in terms of the design pricinples (Reviewer fvW3 also shared this concern). Thus, I would like to keep my score."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716672713,
                "cdate": 1700716672713,
                "tmdate": 1700716672713,
                "mdate": 1700716672713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OpthKhawrG",
                "forum": "ip5LHJs6QX",
                "replyto": "4zya1XQK6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback and our clarification"
                    },
                    "comment": {
                        "value": "Dear Reviewer xsDm,\n\nThanks for your feedback. We believe that in our paper and rebuttal, we have provided detailed analyses and enough insightful takeaways. To ease your understanding, we summarize as follows:\n\n--------\n**insightful takeaways:**\n\n1. **We summarized the modulation mechanism**, and we pointed out that the modulation mechanism could be the most important part other than detailed implementations in related work like VAN and FocalNet. The modulation mechanism brings advantages from bot convolution (local receptive field, efficiency, etc) and attention (dynamics).\n\n2. **We introduced the EfficientMod block based on the modulation mechanism**,  experimental results indicated that our EfficientMod block can serve as an essential building block for efficient networks, supported by Table 1 and Table 6.\n\nIn details of  the  EfficientMod design, we discuss two points:\n\n1. \"Sliming Modulation Design by fusing MLP block, generating a unified block\". By doing so, we can achieve comparable performance and speed up inference a lot, as supported by our rebuttal. Also, Reviewer Q55t is really interesting in this part and increases the score to 8.\n\n2. \"Simplifying Context Modeling by keeping the most constructive component in the design.\" By doing so, we show that we don't need sophisticated designs (which are also not good for efficiency), a block that inherits advantages from the modulation mechanism can achieve promising performance. \n\n-----------------------------------\n**analyses in terms of the design principles:**\n\n1. **We have detailed the analyses of design principles in our rebuttal 2.** This is also acknowledged by Reviewer Q55t, who raised the score to 8. \n\n2. **We have presented detailed ablation studies in Section 4.2 ablation studies, and in Table 5.** Table 5 is also questioned by Reviewer 6eCe; after our rebuttal, Reviewer 6eCe increased his score to 6.\n\n-----------------------------\nMost importantly, we believe that our main contribution \"summarization of modulation mechanism\" and \"EfficientMod block\" should not be overlooked.  **With modulation mechanism, we can improve the performance by 1.0% on ImageNet, WITHOUT ANY additional overhead,** as shown in Table 5 last row.\n\nThanks a lot for your time. We hope our clarification can help you better understand our work."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718734680,
                "cdate": 1700718734680,
                "tmdate": 1700718755357,
                "mdate": 1700718755357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]