[
    {
        "title": "Decoupling regularization from the action space"
    },
    {
        "review": {
            "id": "TEpsDLMiup",
            "forum": "UaMgmoKEBj",
            "replyto": "UaMgmoKEBj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2937/Reviewer_g8g2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2937/Reviewer_g8g2"
            ],
            "content": {
                "summary": {
                    "value": "This work reveals the problem that the regularization effect can be severely affected by the number of actions (especially when action spaces are state-dependent) in regularized reinforcement learning. Starting with two motivating examples, this paper introduces decoupled regularizers whose range is constant over action spaces. The authors further prove that the range of a general class of standard regularizers grows with the number of actions. Two solutions of temperature selection, static v.s., dynamic, are proposed based on which practical algorithms can be implemented. The proposed solutions are evaluated in toy examples, DMC and drug design environments with SQL, SAC and GFN as backbones and baselines. The results demonstrating the effectiveness of the proposed solutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is overall well organized and written. Illustrative examples and discussions are used for smooth reading.\n- The proposed solutions to decoupling regularization and temperature adjustment are of generality to a family of commonly seen regularizations.\n- The experiments cover toy examples, popular DMC environments and drug design problem. Noticeably, unprecedented results in the domain of drug design are achieved."
                },
                "weaknesses": {
                    "value": "- I think the key hyperparameter $\\alpha$ needs more discussion. It will be helpful to analyze the (empirical) effects of different choices of the value of $\\alpha$ and recommend the values or the strategy of value selection.\n- The content in the experiment part lacks of sufficient details, which is also missing in the appendix. I recommend the authors to add the key details of experiments."
                },
                "questions": {
                    "value": "1) Is it possible to have some case-analysis for the results of DMC (e.g., in Figure 3,4)? For example, the proposed method achieves significantly better performance in tasks like BallInCupCatch, FingerTurnHard, HopperStand while the baseline fails totally.\n\n2) Which is the type of temperature setting adopted for the experiments in Section 8.3, i.e., fixed temperature or dynamic temperature?\n\n&nbsp;\n\nMinor:\n- It seems that the coefficients $\\gamma$ and $\\tau$ are missing for the entropy term in Equation 3a."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699410477620,
            "cdate": 1699410477620,
            "tmdate": 1699636237390,
            "mdate": 1699636237390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "45xoHKfpRC",
                "forum": "UaMgmoKEBj",
                "replyto": "TEpsDLMiup",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2937/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your time on our work and the report. We provide answers the comments below.\n\n> I think the key hyperparameter $\\alpha$ needs more discussion. It will be helpful to analyze the (empirical) effects of different choices of the value of $\\alpha$ and recommend the values or the strategy of value selection.\n> \n\nWe will include an ablation for $\\alpha$. However, it might not be ready before the end of the discussion period. \n\nWe added a small note in the paper that we recommend our proposed value of $\\alpha$ and that it should be tuned the same way one would tune the temperature as the interpretation of $\\alpha$ and $\\tau$ is fairly similar, the higher $\\alpha$, the higher entropy the policy will be. As for a recommended default, we chose $\\alpha=0.77$ (obtained mathematically, not via hyperparameter tuning) to match SAC\u2019s rule at the default scale (it was noted in the last paragraph of 8.2). This is why the performance of 4.a is almost identical to SAC\u2019s.\n\n> The content in the experiment part lacks of sufficient details, which is also missing in the appendix. I recommend the authors to add the key details of experiments.\n> \n\nWe have included all details of the experiments, including the experiment files (see `rexp.py` in the rl codebase) in the appendix. We do not understand which details are missing.\n\n> It seems that the coefficients $\\gamma$ and $\\tau$ are missing for the entropy term in Equation 3a.\n\nWe added the missing $\\tau$; thank you for noting this omission.\n\n> Is it possible to have some case-analysis for the results of DMC (e.g., in Figure 3,4)? For example, the proposed method achieves significantly better performance in tasks like BallInCupCatch, FingerTurnHard, HopperStand while the baseline fails totally.\n> \n\nWe will include an ablation for the temperature in the static temperature. However, it might not be ready before the end of the discussion period.\n\nAs for Figure 4.b, the scale was chosen such that the heuristic of SAC becomes infeasible, as noted in the last paragraph of 8.2. \n\nDo you have something particular in mind?\n\n> Which is the type of temperature setting adopted for the experiments in Section 8.3, i.e., fixed temperature or dynamic temperature?\n> \n\nWe added a remark that the temperature is static. We assumed that the fact that we called it soft Q-learning (SQL) and not SAC would convey this information as the dynamic temperature was introduced with SAC, not SQL."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099057839,
                "cdate": 1700099057839,
                "tmdate": 1700099057839,
                "mdate": 1700099057839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V1vtouSzQr",
            "forum": "UaMgmoKEBj",
            "replyto": "UaMgmoKEBj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2937/Reviewer_aWwr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2937/Reviewer_aWwr"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on regularized reinforcement learning, where the objective is to find a policy in an MDP which maximizes the reward minus a weighted regularization term\u2014most commonly the negative entropy. The authors argue that the weight of the regularization term should vary depending on the environment and even sometimes based on the specific state, since the range of values the regularizer can take might depend heavily on the action space. To solve this problem, the authors introduce a method for dynamically setting the regularization coefficient based on the maximum and minimum values the regularizer can take at a particular state. They show that this can improve RL performance compared to using the same coefficient across environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "As the authors note, regularized RL is widely applicable in both control and IRL, so improving its sensitivity to hyperparameters could be helpful for a variety of applications. The experimental results show that the proposed method for setting the regularization coefficient seems to work quite well in practice, particularly when the action space does not have a standard scale."
                },
                "weaknesses": {
                    "value": "While the paper is promising, I worry that in it's current form it is not ready for publication at a top conference. First, the contribution is relatively small, since it is already known how to choose the entropy coefficient for many environments (e.g., via the SAC rule of using $\\bar{H}=-n$) and it often needs to be tuned regardless depending on the scale of the reward function. While I still think the ideas here are useful, the writing, theory, and experiments should be of very high quality to justify publication.\n\nHowever, I found the paper to be often unclear and imprecise, making it difficult to read. See the list of issues below for a partial list of the problems I noticed. The fact that many claims are unprecise and thus actually incorrect as stated makes it difficult to know which contributions to trust.\n\nOne possible extension to this work that could increase the contribution of the paper is to also include regularization to a base policy, as is often done in RLHF for LLMs via a KL divergence regularization term (e.g., see Bai et al., \"Training a Helpful and Harmless Assistant with\nReinforcement Learning from Human Feedback\"). There have been various papers attempting to find a systematic way of setting the regularization coefficient (e.g., the above paper and Gao et al., \"Scaling Laws for Reward Model Overparameterization\"), but none have considered possible setting the coefficient differently at different states, so this could be an interesting direction to apply the ideas presented in this paper.\n\nAlso, a relevant paper is \"LESS is More: Rethinking Probabilistic Models of Human Behavior\" by Bobu et al. They approach the problem from the motivation of modeling human behavior, which under the \"Boltzmann rational\" model is equivalent to solving a regularized MDP. They have a different approach to solving the issue of how the number of actions or number of trajectories that can achieve a particular reward affects the optimal regularized solution. It would be good to compare the approach in this paper with theirs.\n\n**Issues with clarity and precision:**\n * The first paragraph of the intro argues \"changing the action space should not change the optimal policy.\" This is true in most cases\u2014the issue is that \"changing the action space should not change the optimal *entropy-regularized* policy.\" I think it's important to make this distinction since otherwise it sounds wrong.\n * In Section 2, it is not specified that the MDPs considered have discrete actions, although it later appears this is an unspoken assumption. Then later, the paper switches to continuous MDPs without any explicit distinction.\n * In Section 2, according to the formalism in Geist et al., $\\Omega$ should really be a function of only $\\pi(s)$, not of $\\pi$ in general.\n * In Section 2, the first time regularization is introduced it's with $\\tau \\Omega(\\pi)$ subtracted from the value function; then immediately below it's instead added to the value function.\n * The definition of $\\Omega^*_\\tau$ is unclear.\n * Proof of Lemma 1: this proof seems insufficient; why exactly does convexity imply any other point must be smaller?\n * In SAC, the entropy is actually calculated before applying $\\tanh$ to squash the actions into the action space. This means that the entropy is unbounded, and thus the idea in this paper does not apply directly.\n\n**Typos:**\n * Proof of Lemma 1: \"supermum\" -> \"supremum\"\n * Section 3 title: \"graviation\" -> \"gravitation\""
                },
                "questions": {
                    "value": "* Why should the ideas in this paper apply to SAC when the entropy as measured in SAC is actually unbounded?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699562303060,
            "cdate": 1699562303060,
            "tmdate": 1699636237334,
            "mdate": 1699636237334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9kHZ3Drj53",
                "forum": "UaMgmoKEBj",
                "replyto": "V1vtouSzQr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2937/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your time on our work and the report. We provide answers to the comments and questions below.\n\n> First, the contribution is relatively small, since it is already known how to choose the entropy coefficient for many environments (e.g., via the SAC rule of using\n$\\bar{H}=-n$) and it often needs to be tuned regardless depending on the scale of the reward function.\n> \n\nThe contribution of our work goes beyond a practical method for choosing the coefficient: we show that in problems with state-dependent action sets, our proposed solution improves the convergence conditions. This important theoretical and empirical finding is unknown in the literature. It is key to extending the scope of regularized methods in real-life problems beyond such as OR and drug discovery. In particular, in the drug discovery MDP, our proposed method is key to getting SQL to function properly. This approach had been discarded and shown to be (wrongly) inapplicable by the Gflownet authors.  \n\nWe agree that in MDPs where the actions are **not** state-dependent, we can tune the temperature for the environment. However, works like [1] consider one temperature for the whole benchmark, whereas our approach seamlessly adapts to each problem without the need for expensive hyperparameter tuning. Furthermore, as we show, the SAC rule (a constraint) need not be feasible, in which case the temperature will grow to infinity if it cannot be satisfied. Our proposed method would change the results in both cases.\n\nMore importantly, we focus on a more general setting **with** state-dependent action sets where tuning the temperature will not be able to find our proposed temperature scheme, as the temperature proposed here depends on the number of actions. This was highlighted in the last experiment. \n\nOf course, one may argue that we can tune one temperature for every action size, but then the set of parameters to tune can become extremely large in MDPs like the drug design MDP (see last experiment).\n\nLastly, we note that we are arguing for invariance with the scale of the actions, not the rewards.\n\n> The first paragraph of the intro argues \"changing the action space \nshould not change the optimal policy.\" This is true in most cases\u2014the \nissue is that \"changing the action space should not change the optimal *entropy-regularized* policy.\" I think it's important to make this distinction since otherwise it sounds wrong.\n> \n\nWe changed it to \u201cChanging the action space should not change the optimal regularized policy under the same changes\u201d to rectify. Given that we work under the more general framework of regularized MDPs, the expression \u201c*entropy-regularized* policy\u201d is limiting.\n\n> In Section 2, it is not specified that the MDPs considered have discrete actions, although it later appears this is an unspoken assumption. Then later, the paper switches to continuous MDPs without any explicit distinction.\n> \n\nOur theory applies to both cases. In Section 4, we mention that \u201cWe look at differential entropy and continuous actions in Section 7.\u201d We moved the phrase up so it\u2019s more visible.\n\n> In Section 2, according to the formalism in Geist et al., $\\Omega$ should really be a function of only $\\pi(s)$, not of $\\pi$ in general.\n> \n\nWe denote the the regularized value as $\\max_{\\pi\\in\\Delta(\\mathcal{A}(s))} \\mathbb{E}_{a\\sim\\pi}[Q(s,a)] - \\tau\\Omega(\\pi)$ where we denote the policy **at the current state** by $\\pi$. This is accentuated by the fact that the domain of $\\pi$ depends on the current state. In the spirit of being aligned with the notation of [2] and the aforementioned comment, we renamed the optimization variable to $\\pi_s$ \n\n> In Section 2, the first time regularization is introduced it's with subtracted from the value function; then immediately below it's instead added to the value function.\n> \n\nWe fixed this typo; thank you for finding it.\n\n> The definition of $\\Omega^*_\\tau$is unclear.\n> \n\nOur definition follows [2]. See the equation\n\n$V(s) = \\max_{\\pi\\in\\Delta(\\mathcal{A}(s))} \\mathbb{E}_{a\\sim\\pi}[Q(s,a)] - \\tau\\Omega(\\pi) = \\Omega^\\star_\\tau(Q(s,\\cdot)),$\n\nin Section 2. [2] define $\\Omega^\\star$ as \n$\\Omega^*(q_s)=\\max_{\\pi_s\\in\\Delta_\\mathcal{A}}\\langle\\pi_s,q_s\\rangle-\\Omega(\\pi_s).$\n\n> Proof of Lemma 1: this proof seems insufficient; why exactly does convexity imply any other point must be smaller?\n> \n\nWe added a clarification that we are looking at points between 0 and 1 as we are working with discrete distributions, which follows the notice in Section 4. Hence, this follows directly from strict convexity: $f(p)<pf(1) + (1 - p) f(0)$ for $0\\leq p \\leq 1$(see Section 4 of [3] in particular Theorem 4.1). Since we assumed that $f(1)=f(0)=0$ (see Assumption 1), $f(p)<p0+(1-p)0=0$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098351007,
                "cdate": 1700098351007,
                "tmdate": 1700098351007,
                "mdate": 1700098351007,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1XJI0CBIk9",
            "forum": "UaMgmoKEBj",
            "replyto": "UaMgmoKEBj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2937/Reviewer_nhQG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2937/Reviewer_nhQG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method of regularization of common RL algorithms. First, the paper proves that standard regularization techniques used in RL algorithms such as SQL and SAC suffer from regularization that behaves differently with the size of the action space. The authors propose a new family of regularization techniques where state-dependent temperature parameters are learned by normalizing by the range of regularization function. The authors also show that such normalization can also be used on regularization techniques that dynamically learn the temperature parameter. Finally, the authors demonstrate their results on a variety of domains, including DM env and drug discovery."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper motivates the problem with standard regularization techniques well in Section 2-5. \n\n2. The empirical results are quite compelling. The proposed method, while simple, succeeds in various DM env tasks where standard methods fail. More convincingly, the proposed method allows SQL to succeed on a drug discovery task, whereas prior attempts were known to be too unstable."
                },
                "weaknesses": {
                    "value": "1. The method proposed in the paper is very simple, and involves normalizing the standard temperature by the range the regularization objective can take (e.g. minimum possible entropy). While I do not think that the simplicity of the approach should detract from the novelty of the paper, I am not convinced that the better experimental results are actually due to the proposed range-normalization, and not simply that the regularization can now be state-dependent. To my knowledge, state-dependent temperatures is not itself a novel concept, as other works have tried to dynamically set temperature based on state [1, 2]. The results would be more convincing if the authors showed that prior efforts in state-dependent regularization fail whereas theirs succeeds.\n\n2. The algorithm does not actually circumvent the extensive hyperparameter tuning required by existing RL algorithms. It is unclear how sensitive the proposed approach is to various settings of temperature and \\alpha. \n\n\n[1] https://proceedings.mlr.press/v70/asadi17a/asadi17a.pdf\n\n[2] https://arxiv.org/pdf/2111.14204.pdf"
                },
                "questions": {
                    "value": "1. The empirical results swap between dynamic and static learning of the temperature parameter. Is there a reason why one of the method works better for a particular domain. Specifically, will dynamic temperature learning at least reproduce the results of using a static temperature in the drug discovery experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2937/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2937/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2937/Reviewer_nhQG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2937/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700693263289,
            "cdate": 1700693263289,
            "tmdate": 1700693263289,
            "mdate": 1700693263289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]