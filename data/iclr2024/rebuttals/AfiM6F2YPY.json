[
    {
        "title": "Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula"
    },
    {
        "review": {
            "id": "GAIHjtCCc0",
            "forum": "AfiM6F2YPY",
            "replyto": "AfiM6F2YPY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2245/Reviewer_VtUd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2245/Reviewer_VtUd"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers using machine learning approaches to study a problem in algebraic topology, namely the problem of sampling elements from the numerator of Wu's formula. It does so by proposing several variants of natural language models to generate possible words which may be elements of this numerator set, after generating a synthetic dataset based on sampling from Dyck paths."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has an original premise in using tools from machine learning to study a specific problem in algebraic topology which requires generative modeling. This may be of  interest to NLP researchers who wish to understand the breadth of applications which are suitable for their architectures. Table 2 also clearly shows that the proposed methods outperform the baselines."
                },
                "weaknesses": {
                    "value": "My main concern with this paper is its relevance within the machine learning field. \n\nFrom a presentation perspective, the work emphasizes the mathematical contributions by framing the problem with Wu's formula, rather than focusing on the machine learning methodology. Because the setup of Wu's formula is quite complicated for anyone unfamiliar with algebraic topology, the paper may be difficult to parse for all but a few attendees of the conference. As written, it's hard to understand some of the notation (e.g., $[x_i, x_j]$ is defined for elements of $F$, but $[R_i, R_j]$ is not for the subgroups, which is a core component of the formulas). Further, because the paper spends so much time on these preliminaries, it is difficult to understand which machine learning methods are used and why. For example, the training procedure is difficult to understand. \n\nFrom a more substantive perspective, the paper doesn't provide convincing evidence that this use of machine learning can provide meaningful progress for the problem in the domain of mathematics. For example, scalability of the method is highlighted, but the experiments only go up to $n = 5$ due to computational constraints (p. 9). It's not immediately clear from the writing that this contribution can be built upon for further research.\n\nUltimately, the paper may be of more interest to a pure mathematics community.\n\nSome secondary concerns:\n\n1) The use of the completion/reduction ratio metrics could be better justified: can this distinguish whether the model is suggesting a wide variety of elements from that set or simply repeating the same element multiple times?\n\n2) The notation throughout the work is often difficult to understand: for example, the $\\pm$ notation in (1) is unclear, and it's not immediately clear how the $y_{k,i}$ terms in (5) are related to the notation $y_i$ often used to describe elements of the set $F$"
                },
                "questions": {
                    "value": "1) Related to secondary concern 1 above: how do we know that the model is generalizing well, i.e. finding words which are not from the training set?\n\n2) Are there simpler generative models which could work well for this task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2245/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2245/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2245/Reviewer_VtUd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583288718,
            "cdate": 1698583288718,
            "tmdate": 1699636157854,
            "mdate": 1699636157854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A8hLiQMfCy",
                "forum": "AfiM6F2YPY",
                "replyto": "GAIHjtCCc0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**relevance within the machine learning field**\n\nWe hope that our work will be of interest for people working not only in applications of machine learning in math, but to a machine learning researchers who are interested in conditional generation and generation of synthetic and algorithmic languages. A machine learning researcher may found interest in a fact that in our work the target distribution, namely the full intersection, is not directly accessible to us and is not employed in the training process in any way. The only exception is its symmetric commutator part, which is utilized exclusively in training the negative baseline model. We also compare various modes of ensembling multiple generators which we believe is not domain specific.\n\n\n**difficult to understand which machine learning methods are used and why**\n\nSorry for this. We attacked a problem first by constructing a series of deterministic and optimization-based approaches. When they became insufficient we turned our attention to deep learning models. We can definitely expand on a motivation for using attention based models for the camera-ready version (some of this motivation is related to a research on generating Dyck languages mentioned in Section 4.3). In short, we used a range of simpler neural network architectures first (described in Appendix D.1), and then moved to the attention-based models. Starting from ensembles we moved to prompting techniques that showed the best results.\n\n\n**the training procedure is difficult to understand**\n\nThank you for pointing it out! You are right, we should wrote it more carefully. The training procedure fully conicides with the standard training algoirthm for autoregressive language models employing cross-entropy as a loss function.\n\n\n**meaningful progress for the problem in the domain of mathematics? scalability?**\n\nOur main goal was to establish a new machine learning framework for attacking the problem of computing homotopy groups of spaces. In this context, the domain of machine learning offers a unique arsenal of tools, particulary flexible generative models. We are sorry that from the paper it is unclear that research can not be expanded, we think the opposite. Due to size constraints we exclude the \"future research\" section from this version of the paper, we will definitely include it in the final version. For example, now we are working on different tokenization (on the level of commutators, not the generators of free group), which will help with computational constrains. Our main direction for the research is to use the following regularization technique: we increase the loss of the model's prediction if it is very similar to the predictions of the 'negative baseline model'. \n\n\n**completion/reduction ratio can distinguish ... repeating the same element multiple times?**\n\n\nDuring the test time we sample a batch of distinct prefixes of certain length. Then for each prefix we say that the model \"completed\" this prefix if it was able to generate a word from the full intersection using this prefix as a prompt. Our metrics aggreate the result across these distinct prefixes. Taking this description into account, it is easy to construct a degenerate example with a high score. Having a prefix $p$ one can continue it with $wp^{-1}$, where $w \\in [R_0\\dots R_n]_S$ is a fixed word. Such model would score perfectly. We are now working on the additional metrics that would help us to detect such behaviour. We think of employing pairwise similarity coefficients across the batch sample.\n\n\n**how do we know that the model is generalizing well, i.e. finding words which are not from the training set?**\n\nWe agree that the question of generalization is very important. We guarantee that the training dataset does not contain words from the *full* intersection and as you can see our model is capable of generating elements from the *full* intersection. So, we believe that our model is able to generalize well. However we might need to investigate this question further. One way of doing so is the following. We can sample $m$ prefixes of length $l$ before training and put them away as the test dataset. During the training we would sample words that do not start with these test prefixes. Using this strategy we can further evaluate the generalization performance of our model.\n\n\n**Are there simpler generative models which could work well for this task?**\n\nAs mentioned in Appendix D.1, LSTM ensemble works relatively well, but is still outpermormed by our final model."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503277276,
                "cdate": 1700503277276,
                "tmdate": 1700503277276,
                "mdate": 1700503277276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5iNUTV030Z",
            "forum": "AfiM6F2YPY",
            "replyto": "AfiM6F2YPY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2245/Reviewer_LHhA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2245/Reviewer_LHhA"
            ],
            "content": {
                "summary": {
                    "value": "In this work the authors propose the use of language modeling techniques and architectures as a means for generating elements of the homotopy group of the sphere. First, the authors cite Wu's formula, which provides a connection between the homotopy groups of a sphere and a quotient group of particular combinations of free groups. Specifically, the $n+1$ homotopy group is given by a quotient group, the numerator of which is an intersection of subsets $R_i$, called *normal closures*, whose elements are strings comprised of characters $\\\\{x_i^{\\pm 1}\\\\}_{i=1}^n$.\n\nThe authors claim that generating words in the intersection $w\\in \\cap_{i=0}^n R_i$ is not straightforward, however any partial intersection (i.e. omitting at least one $i \\in \\\\{0,\\ldots, n\\\\}$) has an explicit description and can be sampled from. The authors propose a particular sampling method, and propose to use it to generate words with multi-labels as to which $R_i$ they belong. They then train a decoder-only transformer on these sampled words, leveraging the multi-labels via attention masking or prompting (eg. if $x_1 x_2^{-1} \\ldots x_t \\in R_0 \\cap R_1$, they train the model on $R_0 R_1: x_1 x_2^{-1} \\ldots x_t$). They then exploit these mechanisms at inference time to generate elements of the full intersection.\n\nThe authors propose a number of symbolic baselines (random search, an evolutionary method, and a greedy algorithm) which attempt to generate elements in the full intersection. They compare these methods to their approaches using language modeling and measure the *completion ratio*, which is essentially the percentage of generated words which are actually in the full intersection. In all but one case they find the language model is far better at generating words in the full intersection than any of the synthetic approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's greatest strength is in it's originality. To be fair, this is not a line of work I am intimately familiar with, however a literature review on the topic suggests to me that this work is novel in both application and method.\n\nThe authors are also operating in a highly technical application area (homotopy theory), and to their credit I feel they do an admirable job conveying the necessary information without getting bogged down in details, and include additional background material in the appendix."
                },
                "weaknesses": {
                    "value": "The largest weakness, in my opinion, is that the significance of this work is unclear. The authors motivate the work by a connection with homotopy theory, however their approach does not generate elements of the homotopy group itself but rather just the numerator (i.e. full intersection of normal closures). Moreover, there is no notion of the *coverage* of this generative procedure. More specifically, based on the coverage metric, a constant function which simply always returned a fixed element $w \\in \\cap_{i=0}^n R_i$ would score perfectly, but this would not be useful in any way.\n\nThe lack of any notion of \"coverage\" means that, even if their approach *did* generate elements of the homotopy group with some high probability, it is unclear to me exactly how those words could be used to gain any insight into the structure of the homotopy group itself, because we would not have any assurance that it was covering the full set of potential words in the homotopy group.\n\nThe paper also needs significant editing before publication. There are many mistakes and typos, some of which lead to unparsable text."
                },
                "questions": {
                    "value": "1. Can you explain more what you meant by \"thereby enforcing it not to 'learn' the pattern of the trivial words in the denominator\" at the end of Section 3? I don't understand this statement, because (as I understand it) you were not doing anything in particular to encourage it not to learn the trivial words, and indeed the results in Figure 4 suggests that the models very much did predominantly learn trivial words.\n2. In your discussion on random search (in section 4.2) it was mentioned that the difference between naive sampling and bracket-style sampling from $R_i$ was evident in the number of generated words from intersection per batch. Could you please expand on this - what does the number of generated words from intersection per batch of random search imply about naive vs. bracket-style sampling?\n3. Could you provide a simple explanation of why the bracket-style sampling is preferred to naive? (a) It is stated in section 4.1 that \"adjacent letters of different conjugators $y_k, y_{k+1}$ do not interact with each other, resulting in reduced variability within the generated dataset\" but this notion of \"variability\" has not been defined. Also, isn't this straightforward to solve by introducing dependencies between $y_k$ and $y_{k+1}$? (b) I understand the statistics calculated in Figure 1, but I don't understand why the distribution for the bracket-style sampling is \"better\" than naive. Also, if one wanted a more uniform distribution of valleys, couldn't we just always include the inverse of every word?\n4. Could you provide quantitative evidence that the LLM results are \"covering\" the space well? I realize this might be challenging, the metrics which come to mind often involve knowing the ground-truth $cap_{i=0}^n R_i$ in order to provide a percentage, but without this there's seemingly no way to know if the model is not simply exploiting some degenerate pattern to solve the task easily but in a useless way. Would it be possible to provide such a metric if we limit ourselves to words with length less than some threshold?\n5. You mention in the \"Datasets\" section that the training dataset is infinite and generated online, and that the validation dataset is also generated in an online fashion. Does this mean there is potential for train/test overlap?\n\n**Typos / Minor Suggestions** (small selection)\n* $y_i$ in equation (1) should probably be $y_j$ (I assume the subscript here has no relation to the subscript of $R_i$).\n* The set $[R_i, R_j]$ is not defined. I assume $[R_i, R_j] = \\\\{[u,v] \\mid u \\in R_i, v \\in R_j\\\\}$.\n* I wasn't able to parse the first sentence of Remark 5.1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815312953,
            "cdate": 1698815312953,
            "tmdate": 1699636157787,
            "mdate": 1699636157787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GZKambyDKe",
                "forum": "AfiM6F2YPY",
                "replyto": "5iNUTV030Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**the significance of this work is unclear**\n\nOur work aims to transition the problem of computing homotopy groups (with no general solution currently) into a domain of machine learning where a broader array of methods can be applied. In our opinion setting such framework is important.\n\n**a constant function ... would score perfectly**\n\nYou are right. In the answer to VtUd we described a construction of the degenerate example very close to a constant function that fools the metric. We are working on incorporating word similarity-based metrics to exlude such degenerate cases. \n\n**not doing anything in particular to encourage it not to learn the trivial words**\n\nBy \"enforcing it not to 'learn' the pattern of the trivial words\" we meant that we ensure that no words from $[R_0\\dots R_n]_S$ are in the training dataset. It is done by manually checking that no words in the training dataset are from the full intersection as well.\n\n**Figure 4 suggests that the models very much did predominantly learn trivial words**\n\nNon-trivial words (on which the loss in computed in Figure 4) are obtained by cyclic permutations of generators for the known non-trivial element (Table 1 of the paper). By multiplying this word by some other word from $[R_0\\dots R_n]_S$ we can obtain another representative with much lower loss. Our aim here was to highlight a highly non-typical structure of the distiguished representative found by hand. \n\n**naive sampling and bracket-style ... the number of generated words**\n\nPlease refer to Figure 1 in the new supplementary, which illustrate the benefits of using bracket-style sampling in terms of greater variability. It can be seen that for bigger batches bracket-style sampling provide better coverage of $R_i$. \n\n**why the bracket-style sampling is preferred to naive?**\n\nBecause the samples are more diverse. A side comment: your inquiry into the differences between these methods is greatly appreciated, as it prompted further investigation on our part. We realized that we can rewrite (non-reducible) words from $R_i$ as words from a context-free grammar and use the methods of https://linkinghub.elsevier.com/retrieve/pii/0020019094900337 to sample uniformly from it with a better control over the length.\n\n**this notion of \"variability\" has not been defined**\n\nTo provide a more precise definition, we can define variability as the ratio of the number of unique words generated per batch to the total number of words of a given length. This metric is detailed in (new) Table 1.\n\n**why the distribution (of valleys) for the bracket-style sampling is \"better\" than naive**\n\nThe distribution for bracket style is better because it is \"closer to uniform\". But this is necessary condition for the ideal sampler, not sufficient. Perhaps the better illustration to illustrate the benefits of bracket style sequencing would be a plot like Figure 1 in the attachment. In general, in the paper the number of valleys serves just as an auxilary statistic of the sampler and the whole discussion of Dyck paths and Dyck languages serves an additional purpose to tie our research with the research on generating Dyck languages, as stated in the paper.\n\n**Could you provide quantitative evidence that the LLM results are \"covering\" the space well?**\n\nWe could not provide such evidence now. For us the \"degenerate pattern\" you mention is defined as being an element of $[R_0\\dots R_n]_S$, hence this question is more suitable for us in a context of negative sampling, where we train the model to generate elements away from $[R_0\\dots R_n]_S$. Note that both $\\cap_iR_i$ and $[R_0\\dots R_n]_S$ are infinitely generated subgroups of free group, but their quotient (i.e. the homotopy group itself) is finite (provided $n > 2$). To our knowledge there is no known purely group-theoretical proof of this fact. \n\n\n**... if we limit ourselves to words with length less than some threshold?**\n\nBecause of the remark above, if we are studying the question of coverage in a context of generating from intersection (not the homotopy group), we should restrict ourself to some finite subset of the free group. The word length restriction may not be natural for the type of objects like $\\cap_iR_i$, in a sense that it is quite tricky to get a formula for number of words in $R_i$ of a given length, not to mention $\\cap_iR_i$. We can bruteforce such computation for one $R_i$ and smaller lengths (used in Table 1 in the attachment), but these are insufficient to draw conclusions about the full intersection. We are investigating other filtrations on the free group that may be more natural for the problem. \n\n**there is potential for train/test overlap?**\n\nAs mentioned above, we check that the words in the training datasets are not from the full intersection, hence train/test overlap is not possible. Additionally we can ensure the non-overlapping of the train and the test by separating a subset of prefixes, that would only be shown to the model during the test phase."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503153415,
                "cdate": 1700503153415,
                "tmdate": 1700503153415,
                "mdate": 1700503153415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oqptMql1Dp",
                "forum": "AfiM6F2YPY",
                "replyto": "GZKambyDKe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2245/Reviewer_LHhA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2245/Reviewer_LHhA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply, but my concerns remain"
                    },
                    "comment": {
                        "value": "Thank you for your reply.\n\nYour comments on the reasons for bracket over naive, as well as the associated supplementary PDF, made things quite a bit clearer on that front.\n\nUnfortunately, in my opinion, this work does not demonstrate enough significance in either a machine learning or mathematical setting. Your reply states that your goal was to transition the problem of computing homotopy groups into a domain of machine learning, however I do not see your current work providing a practical solution here, for the reasons I originally outlined in the section on weaknesses. In particular, the work does not generate elements from the homotopy group but rather just from the full intersection of normal closures, and it is unclear on how this procedure provides any insight into the structure of the homotopy group. In addition, there is the lack of any notion of \"coverage\", and so even in the context of generating elements from the full intersection it is not clear what conclusions could be drawn from such a process.\n\nOverall, my impression is that this work is a first attempt at the following idea: use generative models and prompt strategies to generate elements of free groups which are otherwise difficult to sample from. This idea is certainly novel, however for the reasons above it seems a bit too limited to be of practical use for the proposed task."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677068761,
                "cdate": 1700677068761,
                "tmdate": 1700677068761,
                "mdate": 1700677068761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b20qA3DPdF",
            "forum": "AfiM6F2YPY",
            "replyto": "AfiM6F2YPY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at studying the simplicial homotopy groups of the 2-dimensional sphere. Following Wu's formula, each homotopy group is isomorphic to the quotient of a free group. Each element in the homotopy group, as a reduced word, can be connected to a sentence in the language models. The decoder part of a transformer is used to generate approximate samples from the homotopy group, and a multi-label is used to denote whether the sample satisfies relations of the homotopy groups. Multiple approaches for handling the multi-label and training the transformer are designed. Adequate numerical experiments are provided, with comparison to multiple baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The problem originating from algebraic topology is important.\n2. The idea is very innovative.\n3. The paper is well-written and mostly clear.\n4. Adequate numerical experiments are provided."
                },
                "weaknesses": {
                    "value": "1. It would be helpful to discuss a bit of how the samples drawn from the homotopy group can be used to understand the topological properties of the 2-dimensional sphere, compared to the more obtainable homology groups.\n2. It would be nice to discuss the significance and potential applications of the proposed method to topological spaces other than the 2-dimensional sphere."
                },
                "questions": {
                    "value": "1. Does the samples drawn using the decoder transformer have a much wider diversity compared to the baseline methods? What are the proportions of repeated samples?\n2. Is it possible to intuitively understand the distributions over the homotopy group that the transformer or the baseline methods are drawing from?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2245/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2245/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826446164,
            "cdate": 1698826446164,
            "tmdate": 1699636157717,
            "mdate": 1699636157717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iyhAEmeWFh",
                "forum": "AfiM6F2YPY",
                "replyto": "b20qA3DPdF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The decoder part of a transformer is used to generate approximate samples from the homotopy group**\n\nIt is important to clarify that in this project, our focus has been exclusively on generating samples from the numerator of the Wu formula. The generation of non-trivial elements from the homotopy group itself is a significantly more challenging problem. Our current research is laying the groundwork for this, and we are actively working on it in our current project.\n\n\n**how the samples drawn from the homotopy group can be used to understand the topological properties**\n\nAlmost nothing is known about group-theoretic description of elements of the homotopy groups in terms of their topological properties. We can speculate that the commutator $[x_1, x_2]$ which generates $\\pi_3 S^2$ and corresponds to the Hopf fibration $S^1 \\to S^3 \\to S^2$ is related to the Whitehead product expression for it. Deeper connections can be given using the language of $\\Lambda$-algebra as well. \n\n\n**spaces other than the 2-dimensional sphere**\n\nOur results can be applied to spaces which have infinite cellular decompositions, but still fit into the framework of Wu's formula. Such spaces are usually unreachable by methods of the classical computational topology. Example os such space is $\\Sigma\\mathbb R \\mathrm{P}^\\infty$, i.e. a suspension over the infinite real projective plane. We are investigating this particular example currently.\n\n\n**proportions of repeated samples?**\n\nOur metrics count the numbers of unique generated words. We will count the repetitions as well and report them in the supplementary material if time permits.\n\n\n**distributions over the homotopy group ?**\n\nAs for now we do not know how to describe this distribution. This problem is quite challenging since the answer is unknown theoretically (in the group-theoretic language) and the full intersection that we are sampling from is infinitely generated (see also answer to LHhA)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503409395,
                "cdate": 1700503409395,
                "tmdate": 1700503409395,
                "mdate": 1700503409395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4cYTor3TXQ",
                "forum": "AfiM6F2YPY",
                "replyto": "iyhAEmeWFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the detailed reply and clarification. I myself have enjoyed the idea behind this paper although may not fully grasp certain intricate math details. Contrary to the other reviewer's comments, I find this paper to be quite motivating and is one of the good examples of using machine learning ideas in studing algebraic topology, and should qualify to be at a top machine learning conference. I am keeping my rating as is."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524597747,
                "cdate": 1700524597747,
                "tmdate": 1700524597747,
                "mdate": 1700524597747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]