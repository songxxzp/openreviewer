[
    {
        "title": "Your CLIP Model Might Be Undertrained"
    },
    {
        "review": {
            "id": "oOtckXZ2yG",
            "forum": "JEAlXPYSjC",
            "replyto": "JEAlXPYSjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_N6Pn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_N6Pn"
            ],
            "content": {
                "summary": {
                    "value": "This paper makes a simple but intriguing observation that vision-language models trained on even a small dataset such as CC12m can improve substantially (10% zero-shot accuracy), by restarting the training of the model on the *same* data. The paper starts by showing this for on CC12m by training for an extra 15 epochs (Figure 1), then it shows that similar improvements are reached with 2 other models for various number of extra epochs (Figure 3), then shows that the improvements are consistently significant on various OOD datasets (Table 2), then they confirm that restarting training even from early checkpoints can result in faster training (Figure 4), then they extend the idea by repeating the restart by revisiting the idea of cyclical LR schedules and show that a cyclical LR can reach a higher accuracy than a single cycle cosine LR schedule (Figure 5). Finally, they use a cyclical LR schedule to train on the larger LAION400M dataset and conclude that \u201cCLIP models trained on large datasets are less likely to be undertrained.\u201d"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The results are surprising and promising for future directions on alternative LR schedules for multi-modal training.\n- The paper is easy to read and asks natural questions sequentially that engages the reader. Although the reader is left with many unanswered questions by the end!"
                },
                "weaknesses": {
                    "value": "- While the paper makes interesting observations, the paper is missing a lot of discussion and potential for extending the observations given the unused page limit. A few unanswered, immediate and simple questions: How many cycles are optimal? How long should each cycle be? Is there a relation between the number of samples and the length of the LR cycles? Figure 5 only presents results with one schedule.\n- Results in Figure 4 and 5 are limited to only one architecture and are not shown to hold for other architectures. Would these results hold for ViT-L/14?\n- The final conclusion is that \u201cCLIP models trained on large datasets are less likely to be undertrained.\u201d. This is based on only one LR schedule and one model that does not provide definitive evidence for the conclusion."
                },
                "questions": {
                    "value": "- Where are Figure 2 and Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801013702,
            "cdate": 1698801013702,
            "tmdate": 1699636216320,
            "mdate": 1699636216320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e5aw6A24FQ",
                "forum": "JEAlXPYSjC",
                "replyto": "oOtckXZ2yG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. We respond below to the raised questions.\n\n1. *While the paper makes interesting observations, the paper is missing a lot of discussion and potential for extending the observations given the unused page limit...* \n\nWe thank the reviewer for their suggestions. We have indeed experimented with some of them.\n \n*Re- number of extra cycles:* We have found in our investigation that additional cycles do not provide any further performance boost.\n\n*Re- length of cycle:* We investigated in Figure 3 how changing the cycle duration affects performance. We can see that a small cycle of 3 epochs (on CC12M) was enough to get most of the gain.\n\n*Re- relation with dataset size:* We have observed that for small datasets, 10 epochs was enough for both CC3M and CC12M (a dataset with 4x more samples). In particular, by applying a cycle with 10 epochs, we improve the ImageNet zero-shor accuracy of a ResNet-50 CLIP model by 4% when trained on CC3M (Table 7), and by 11% when trained on CC12M (Table 2).\n\n2. *Results in Figure 4 and 5 are limited to only one architecture and are not shown to hold for other architectures. Would these results hold for ViT-L/14?*\n\nOur experiments on smaller datasets with several models suggest that the phenomenon is model-agnostic. For example, as we can see in Table 2, the ImageNet zero-shot accuracy of several CLIP models improves when applying our cycle: ResNet-50 by 11.3%, ViT-B-32 by 7.83% and ViT-B-16 by 8.4%. This improvement translates to other downstream tasks. We expect the same results with a ViT-L/14.\n\n3. *The final conclusion is that \u201cCLIP models trained on large datasets are less likely to be undertrained.\u201d. This is based on only one LR schedule and one model that does not provide definitive evidence for the conclusion.*\n\nWe agree with the reviewer that more analysis needs to be done at large scale to confirm. Unfortunately, such an analysis is beyond our compute capabilities, especially given the rebuttal period length. Our results from a single run on LAION-400M indicate only a little improvement by adding the extra training cycle, and as such, we think that applying our method to other large models would lead to the same results. We are happy to add more large-scale experiments for the final version if the reviewer thinks it would strengthen the paper.\n\n4. *Where are Figure 2 and Table 1?*\n\nWe thank the reviewer for pointing this out. We have a unified numbering system for figures and tables. We will fix that."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625081304,
                "cdate": 1700625081304,
                "tmdate": 1700625249354,
                "mdate": 1700625249354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iUOCA4o9PP",
                "forum": "JEAlXPYSjC",
                "replyto": "oOtckXZ2yG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2738/Reviewer_N6Pn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2738/Reviewer_N6Pn"
                ],
                "content": {
                    "title": {
                        "value": "I thank the authors for their response. My concerns remain unresolved."
                    },
                    "comment": {
                        "value": "I thank the authors for their response. My concerns remain unresolved.\n\n**W1** The paper is missing a lot of discussion and potential for extending the observations.\nI do not see a revision to the paper although the general response suggests. As such, my response is based on only the authors\u2019 latest comments and other reviews.\n\n**W1.1** How many cycles are optimal?\n> We have found in our investigation that additional cycles do not provide any further performance boost.\n\nThis does not answer my question about \u201coptimal number of cycles\u201d. Figure 4 and 5 suggest that the optimal number is more than 1 and possibly more than 8 (maximum in the plot) for ResNet50. My question is, a complete study on the optimal number of cycles even for ResNet50 is missing. More broadly, a study for various architectures is missing.\n\n**W1.2** How long should each cycle be?\nI agree Figure 4 partially answers this question. However, the following sentence is not concrete and the exact amount of drop needs to be reported and a plot that shows cost-vs-accuracy and connecting the final accuracy of each curve in Figure 4 would be important to include.\n> \u2026a small cycle of 3 epochs (on CC12M) was enough to get most of the gain.\n\n**W1.2** Is there a relation between the number of samples and the length of the LR cycles?\n> We have observed that for small datasets, 10 epochs was enough for both CC3M and CC12M\n\nAlthough these two datapoints are important, they do not answer my question. I particularly asked for a study on the relation between the number of samples and the length of the LR cycle. Such a study can be done by varying the number of samples for a single dataset and finding the optimal LR cycle length. The suggested numbers do not provide a complete picture. Adding to this picture, the reduced gap on the large LAION dataset shows that there will be a saturation point in the plot I\u2019m suggesting.\n\n**W2** Results in Figure 4 and 5 are limited to only one architecture and are not shown to hold for other architectures.\n> Our experiments on smaller datasets with several models suggest that the phenomenon is model-agnostic. For example, as we can see in Table 2 \u2026\n\nI agree that Table 2 provides a partial evidence for the potential of the alternative LR schedule. However, here I\u2019m particularly asking about Figure 4 and 5 which investigate shorter training and cyclical learning rates. This weakness also relates to W1 as the previously suggested analysis might provide different optimal values for different architectures.\n\n**W3** \u2026does not provide definitive evidence for the conclusion.\n> such an analysis is beyond our compute capabilities, especially given the rebuttal period length\n\nI understand the compute limitations in general. However, limited results would weaken the applicability of any method and the conclusions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678342468,
                "cdate": 1700678342468,
                "tmdate": 1700678376536,
                "mdate": 1700678376536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l3Xbij33yU",
            "forum": "JEAlXPYSjC",
            "replyto": "JEAlXPYSjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_B4x4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_B4x4"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a training technique using which the performance of CLIP like vision language models can be improved without undergoing any change in objective function or model architecture. Authors of this work shows that CLIP like models trained on small scale datasets like CC-3M or CC-12M might be under trained. To improve their performance, simply finetuning a pre-trained CLIP model again with additional few epochs with a restarted learning rate scheduler is enough. The paper additionally shows that this technique is less effective when tried on CLIP models trained on large scale datasets like LAION-400M. The paper shows additional ablations and result comparisons with previous methods to provide a broad perspective."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work shows that hyper-parameter and tweaking training strategies for large-scale pretraining of vision-language models plays significant roles in determining their performance on downstream tasks after training.\n2. Restarting the LR scheduler with few-additional epochs is a simple and elegant way to improve performance of CLIP models trained on small scale datasets.\n3. The resulting performance of the model is competitive to prior methods that bring additional objective function or model architecture changes to baseline CLIP model.\n3. Most importantly, this study underlines the crucial need of bench-marking at larger scales to truly reflect improvements due on proposed modifications. Otherwise, showing effects on small scale datasets could be sometimes misleading.\n4. The paper conducts fair comparison and additional ablation studies to provide a broad perspective about CLIP training.\n5. Paper is easy to read and well presented."
                },
                "weaknesses": {
                    "value": "1. In my view, this paper presents a effective but somewhat hyperparameter training technique which is more of a engineering trick rather than pure novel contribution. In other words, this paper says that one should use a altered version of multi-cycle LR scheduler instead of a single-cyle LR schedule to improve CLIP performance.\n2. There is little or no analysis on why the proposed trick helps improve CLIP performance. This work can be further supplemented by conducting a detailed analysis on the learned embeddings, for example analysis on modality gap [1], t-SNE visualizations etc.\n3. This work shows performance comparison on zero-shot tasks, but it will be great to see how the learned embeddings provide benefits for adaptation tasks like linear probing or its use on downstream tasks which uses CLIP features in their framework.\n4. Although competitive to prior methods trained on CC3M and CC12M, it is unclear how this technique performs when combined on pre-trained models of prior methods. For example, does this technique shows complementary effect when plugged on CyCLIP pretrained model? (using same CyCLIP objective functions even in the proposed technique)\n5. Similar results with-and without this technique on large scale CLIP models like CLIP-LAION400M shows that this trick is only valid for small scale models with less than 50% baseline accuracy.  \n6. There are missing tables in the paper. For example, I cannot see Table 1 anywhere in the paper."
                },
                "questions": {
                    "value": "My main concern is that this paper mainly shows a training trick, which poses questions for novelty of this work. Please refer to the weaknesses section for my additional concerns and queries. \n\nIn summary the paper is nice but it lacks solid technical contributions which can be an significant issue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2738/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2738/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2738/Reviewer_B4x4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806447918,
            "cdate": 1698806447918,
            "tmdate": 1699636216242,
            "mdate": 1699636216242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aq1aB6X6JP",
                "forum": "JEAlXPYSjC",
                "replyto": "l3Xbij33yU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. We respond below to the raised questions.\n\n1. *In my view, this paper presents a effective but somewhat hyperparameter training technique...* \n\nWe fully agree with the reviewer that the method is a simple and straightforward baseline for CLIP. In fact, we have re-named the paper and positioned the method accordingly. Given its competitive performance, we believe it provides a better (and fast to compute) baseline for CLIP on smaller datasets.\n\n2. *This work shows performance comparison on zero-shot tasks, but it will be great to see how the learned embeddings provide benefits for adaptation tasks like linear probing or its use on downstream tasks which uses CLIP features in their framework.*\n\nWe thank the reviewer for these suggestions, and will incorporate the suggested experiments in our final paper. Sadly, we were not able to finish running all these experiments by the time of the rebuttal deadline, but we have some preliminary results that we believe are quite promising. Please refer to **[GP4]** for more zero-shot results and **[GP5]** for linear probing results. \n\n3. *Although competitive to prior methods trained on CC3M and CC12M, it is unclear how this technique performs when combined on pre-trained models of prior methods...* \n\nWe thank the authors for their suggestion. We have indeed experimented a bit with this idea and found out that applying the extra training cycle on top of other approaches leads to the same results as applying the extra training cycle by itself. Although we were not able to run conclusive enough experiments to update the draft of our paper, we are happy to include these results in the final version of the paper.\n\n4. *Similar results with-and without this technique on large scale CLIP models like LAION400M shows that this trick is only valid for small scale models with less than 50% baseline accuracy.*\n\nWe agree with the reviewer that the finding does not translate to large datasets. However, we believe that the smaller data regime is definitely interesting. Training CLIP models on large datasets is expensive and only large companies and a very limited number of academic labs afford it. Most academic labs however cannot train CLIP models except on smaller datasets. As such, when such works propose methods to improve the performance of CLIP models, it is necessary to consider better baselines to validate the effectiveness of the proposed method, and to increase our confidence that it would work on large datasets.\n \nWe believe our approach can provide such a baseline given its simplicity. \n\n5. *There are missing tables in the paper. For example, I cannot see Table 1 anywhere in the paper.*\n\nWe thank the reviewer for pointing this out. We were using a unified numbering system for figures and tables (so Figure 1, Table 2, Figure 3, etc.), but will change it for the final version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624951690,
                "cdate": 1700624951690,
                "tmdate": 1700625233022,
                "mdate": 1700625233022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b02VWnMXXl",
            "forum": "JEAlXPYSjC",
            "replyto": "JEAlXPYSjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_1NTN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_1NTN"
            ],
            "content": {
                "summary": {
                    "value": "The paper revisits the training schedule of the CLIP, especially on those trained on smaller-scale datasets, and finds that a simple continuing training with LR rewinding can significantly improve the CLIP baselines. It demonstrates improvements on 6 ImageNet variants with R50/ViT-B-32/ViT-B-16 backbones and even outperforms some approaches that are designed to be data-efficient (e.g. DeCLIP) when trained on CC12M."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The finding of the paper is interesting\n- It establishes a stronger baseline for CLIP training, and questions whether we should solely evaluate data efficient approaches on smaller scale, and its transferability to large-scale datasets."
                },
                "weaknesses": {
                    "value": "- The finding does not necessarily transfer to models that are trained on large-scale datasets (e.g. 400M).\n- There is not much analysis / theory on why such behavior exists (or not) on different scales of datasets.\n- It would be interesting to see how the proposed schedule helps when we apply to other approaches in Table 7."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820254781,
            "cdate": 1698820254781,
            "tmdate": 1699636216173,
            "mdate": 1699636216173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e0RDt2Smaf",
                "forum": "JEAlXPYSjC",
                "replyto": "b02VWnMXXl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. We respond below to the raised questions.\n\n1. *The finding does not necessarily transfer to models that are trained on large-scale datasets (e.g. 400M).*\n\nWe agree with the reviewer that the finding does not translate to large datasets. However, we believe that the smaller data regime is still interesting (see general point **[GP1]**). Training CLIP models on large datasets is expensive and only large companies and a very limited number of academic labs afford it\u2014most academic labs cannot train CLIP models except on smaller datasets. As such, when such works propose methods to improve the performance of CLIP models, it is necessary to consider better baselines to validate the effectiveness of the proposed method, and to increase our confidence that it would work on large datasets. \n\nWe believe our approach can provide such a baseline given its simplicity. \n\n2. *There is not much analysis / theory on why such behavior exists (or not) on different scales of datasets.*\n\nWe refer the reviewer to **[GP2]** above\u2014although more theory CLIP models are known to require a lot of training data to achieve a good performance. We believe that when the training data is limited, it is easy for the model to be stuck in a local optimum. As the dataset size increases, the sheer number of training data points leads to a better generalization. \n\n3. *It would be interesting to see how the proposed schedule helps when we apply to other approaches in Table 7.* \n\nWe thank the authors for their suggestion. We have indeed experimented a bit with this idea and found out that applying the extra training cycle on top of other approaches leads to the same results as applying the extra training cycle by itself. Although we were not able to run conclusive enough experiments to update the draft of our paper, we are happy to include these results in the final version of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624608073,
                "cdate": 1700624608073,
                "tmdate": 1700625215965,
                "mdate": 1700625215965,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "656rzroyTG",
                "forum": "JEAlXPYSjC",
                "replyto": "e0RDt2Smaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2738/Reviewer_1NTN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2738/Reviewer_1NTN"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional clarification. My concerns are not fully addressed.\n\n*Given that the author's response was provided only a day before the end of the discussion period, it may be hard for the authors to provide additional feedback, but I will still provide my comments below.*\n\n> R3. We have indeed experimented a bit with this idea and found out that applying the extra training cycle on top of other approaches leads to the same results as applying the extra training cycle by itself.\n\nCan the authors please clarify what is \"the same results\" and what is \"by itself\"? It would be clearer and more direct if the authors can directly provide a table of the results: base method / original performance / performance after the additional schedule.\n\nAnd if the author's implication is that the other approaches do not benefit from the additional schedule, it would be more interesting to give an analysis of what is the reason for that -- what is fundamentally different between the original CLIP and the improved approaches, and why the additional schedule does not help."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719441383,
                "cdate": 1700719441383,
                "tmdate": 1700719441383,
                "mdate": 1700719441383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JfASBpQ7hF",
            "forum": "JEAlXPYSjC",
            "replyto": "JEAlXPYSjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_MMUs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2738/Reviewer_MMUs"
            ],
            "content": {
                "summary": {
                    "value": "This paper reports the observation that existing popular CLIP training recipe is suboptimal on smaller datasets (under training), and demonstrates clear improvement by resuming the training with a higher learning rate. Experimental results on ImageNet show the effectiveness of the proposed modified training recipe."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The finding of the paper is clear and simple. The experiments seem convincing. It is a meaningful contribution and the community should move away from the current suboptimal CLIP recipe especially on the CC3M data."
                },
                "weaknesses": {
                    "value": "1. Why is the proposed observation of under-training only present with the smaller dataset e.g. CC3M but not with the larger ones like LAION-400M? It'd be very helpful if the authors can provide some insights here, because this seems to be the core contribution of the paper. For example, does the noise level in the image-text dataset affect the fitting behavior in some way?\n\n2. Many existing CLIP recipes rely on high-resolution finetuning after the low-resolution pretraining (e.g.ALIGN, CoCa), which is essentially doing the same finetuning with the extra cycle as shown in Figure 1 (right). Would the proposed approach still benefit models that are already trained with high-res finetuning? \n\n3. For ablation, I think it'd be cleaner to compare with a baseline that is trained on the same number of epochs so that the only difference is the learning schedule (instead of adding additional training epochs). \n\n4. The paper writing can use some improvement to expand on the introduction, method, analysis, and related work (still plenty of space left)."
                },
                "questions": {
                    "value": "See weaknesses in order."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2738/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698883874773,
            "cdate": 1698883874773,
            "tmdate": 1699636216094,
            "mdate": 1699636216094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "thrK7wdViP",
                "forum": "JEAlXPYSjC",
                "replyto": "JfASBpQ7hF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2738/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. We respond below to the raised questions.\n\n1. *Why is the proposed observation of under-training only present with the smaller dataset e.g. CC3M but not with the larger ones like LAION-400M?...* \n\nWe refer the reviewer to the general points **[GP2]** above. In short, additional experimentation rules out factors such as label noise or model capacity as the underlying causes of undertraining. These results suggest that the root of undertraining might simply be the number of data points (e.g., the model is more likely to be stuck in some local optimum). \n\n2. *Many existing CLIP recipes rely on high-resolution finetuning after the low-resolution pretraining (e.g.ALIGN, CoCa)...*\n\nGood observation! As far as we know, fine-tuning on higher resolution images is usually employed with large datasets (where under-training is not an issue). At small scale, we suspect that it would provide similar benefits, provided that the learning rate scheduler and optimizer state are reset. \n\n3. *For ablation, I think it'd be cleaner to compare with a baseline that is trained on the same number of epochs...* \n\nWe thank the reviewer for their recommendation. We have trained several baseline models, on different number of epochs, ranging from 30 to 200, on CC3M and CC12M. We have observed that all these models have almost the same downstream performance at the end of their training.  \n\n4. *The paper writing can use some improvement to expand on the introduction, method, analysis, and related work (still plenty of space left).*\n\nWe thank the author for their suggestion. We have indeed significantly revised the paper to include more details."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2738/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624461462,
                "cdate": 1700624461462,
                "tmdate": 1700625197951,
                "mdate": 1700625197951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]