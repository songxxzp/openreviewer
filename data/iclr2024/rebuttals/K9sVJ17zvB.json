[
    {
        "title": "VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation"
    },
    {
        "review": {
            "id": "ubnGI7qXwE",
            "forum": "K9sVJ17zvB",
            "replyto": "K9sVJ17zvB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_Cov9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_Cov9"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a diffusion-based method for video generation. The paper proposes the VersVideo model that leverages multi-excitation spatial-temporal convolution, a Mixture of Experts attention blocks, and a temporal compensated decoder to address the challenges associated with spatial-temporal dynamics. Additionally, it integrates the ControlNet model for versatile video generation, allowing for a wider range of visual conditions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-motivated and the proposal method is technically sound.\n2. The proposed method combines several existing powerful techniques to learn better spatiotemporal features.\n3. The proposed method achieves comparable performance on the MSR-VTT dataset."
                },
                "weaknesses": {
                    "value": "1. The proposed method directly combines existing techniques, such as excitation network, MoE to enhance spatiotemporal feature representation, which makes the novelty a bit weak. The proposed Multi-Excitation Convolution applies excitation convolution multiple times. Original excitation convolution should be cited.\n\n2. From Table 1, the proposed fails to achieve better performance than Make-A-Video. Even compared with the methods trained on WebVid-10M, the performance improvement is marginal.\n\n3. From Table 2, the performance of the proposed method is a lot worse than the previous STOA method, e.g., VideoFusion. The author explains this by \"the VersVideo-L model has a significantly smaller parameter of 500M\". Why not use a larger model to verify the performance? Moreover, 1) Make-A-Video also reports performance on the UCF dataset, the results should be included; 2) the author should also provide performance under the same resolution as previous methods for better comparison.\n\n4. Results in Table 2 and Table 3 are from different training datasets. Better to keep it consistent for better comparison."
                },
                "questions": {
                    "value": "see weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7134/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7134/Reviewer_Cov9",
                        "ICLR.cc/2024/Conference/Submission7134/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698572663774,
            "cdate": 1698572663774,
            "tmdate": 1700588804082,
            "mdate": 1700588804082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GrqL1E6WyD",
                "forum": "K9sVJ17zvB",
                "replyto": "ubnGI7qXwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cov9 [1/4]"
                    },
                    "comment": {
                        "value": "The reviewer **Cov9** primarily focuses on the significance of our contributions and the performance of our model in experiments. In response, we provide a clear explanation of our contributions, acknowledge the related works that inspired our methodology, and thoroughly analyze the performance of our model in comparison to other competing methods used in the experiment."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492217336,
                "cdate": 1700492217336,
                "tmdate": 1700492349496,
                "mdate": 1700492349496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SHpzrl96Q9",
                "forum": "K9sVJ17zvB",
                "replyto": "ubnGI7qXwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cov9 [2/4]"
                    },
                    "comment": {
                        "value": ">Q1: The proposed method directly combines existing techniques, such as excitation network, MoE to enhance spatiotemporal feature representation, which makes the novelty a bit weak. The proposed Multi-Excitation Convolution applies excitation convolution multiple times. Original excitation convolution should be cited.\n\nThank you for your expertise and the time you dedicated to reviewing our paper.\n\n\nIn this paper, we tackle three major challenges related to diffusion models for video generation. \n- Firstly, we focus on the denoising UNet, which is the core component responsible for modeling spatial-temporal dynamics. \n- Secondly, we address the VAE's limitations, which lead to flicker artifacts and inter-frame inconsistency caused by information loss during video encoding into latents and reconstruction into pixel space. \n- Finally, we aim to achieve controllable generation, allowing for video generation under a range of conditions.\n\n\nOur contributions strive to address these challenges:\n- We enhance the spatial-temporal capability of the denoising UNet with Multi-Excitation Convolution (MEC) and Mixture of Expert (MoE) spatial-temporal transformers, as shown in Table 1 of the paper.\n- We build the TCM-VAE for video decoding to reduce information loss and increase inter-frame consistency, as demonstrated in Table 3 and visually showcased in [video generation demo](https://docs.google.com/document/d/e/2PACX-1vQyICnc6WH8CgCCRuX0avsOI7MoO4gyPRQEro-v1UGu0nI60CTCiDksLUXOV5y_A597X41rL5mQdyER/pub) and [video reconstruction demo](https://anonymous-pages.github.io/video_demos/). \n- We propose a unified controllable video generation strategy for text, visual, image, and style conditions, as demonstrated in Fig. 6 and visually showcased in the [demo](https://anonymous-pages.github.io/video_demos/). \n\n\n**Multi-Excitatin Convolution (MEC) motivation.** In contrast to existing video diffusion models, which often restrict spatial-temporal performance due to the over-simplification of standard 3D operations, we propose multi-excitation paths for spatial-temporal convolutions with dimension pooling across different axes. \n\nThe proposed MEC is constructed by assembling four paths in parallel, allowing for the **activation of multi-type information in videos**. Our design is fundamentally inspired by the squeeze-and-excitation (SE) block [ref-1; ref-2], as it explicitly models channel/temporal interdependencies. Drawing inspiration from these two previous works, we design the excitation paths:\n1. Factorized 2D spatial + 1D temporal convolution. This path is a **simplified adaptation of 3D convolution** and is the only temporal module widely used in most existing video diffusion models [ref-3; ref-4; ref-5]. \n2. Spatial-temporal excitation. This path pools the 5-D tensor $(B, C, T, H, W)$ along channel dimension, resulting in the **spatial-temporal activation** mask $(B, 1, T, H, W)$ which is later processed with 3D convolution. \n3. Channel excitation. This path get global spatial information $(B, C, T, 1, 1)$ of the input feature $(B, C, T, H, W)$ by spatial average pooling.\n4. Motion excitation. The **motion information** is modeled by adjacent frames using the same squeeze and unsqueeze strategy. Related works have explored this, aiming to model motion information based on the feature level instead of the pixel level [ref-2; ref-6].\n\n\n\n*Reference*:\n\n[ref-1] Hu, J., Shen, L. and Sun, G., 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).\n\n[ref-2] Li, Y., Ji, B., Shi, X., Zhang, J., Kang, B. and Wang, L., 2020. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 909-918).\n\n[ref-3] Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D. and Zhou, J., 2023. VideoComposer: Compositional Video Synthesis with Motion Controllability. arXiv preprint arXiv:2306.02018.\n\n[ref-4] Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S. and Kreis, K., 2023. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22563-22575).\n\n[ref-5] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D. and Dai, B., 2023. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725.\n\n[ref-6] Jiang, B., Wang, M., Gan, W., Wu, W. and Yan, J., 2019. Stm: Spatiotemporal and motion encoding for action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 2000-2009)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492246384,
                "cdate": 1700492246384,
                "tmdate": 1700492341385,
                "mdate": 1700492341385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Y77N00ZY9",
                "forum": "K9sVJ17zvB",
                "replyto": "ubnGI7qXwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cov9 [3/4]"
                    },
                    "comment": {
                        "value": ">Q2: From Table 1, the proposed fails to achieve better performance than Make-A-Video. Even compared with the methods trained on WebVid-10M, the performance improvement is marginal.\n\nWe appreciate your thorough review and the effort you put into providing detailed feedback.\n\nIn Table 1, we employ the Frechet Video Distance (FVD) and CLIP similarity (CLIPSIM) as evaluation metrics for text-to-video generation. These two metrics are commonly utilized in most of video generation studies. The FVD metric assesses the similarity between real and generated videos, taking into account both video quality and motion dynamics. On the other hand, CLIPSIM measures the video quality and text alignment by averaging the CLIP similarity score of all generated frames.\n\n\nOur proposed *VersVideo-H* diffusion model, trained with WebVid-10M, consists of 2B parameters. In contrast, **Make-A-Video serves as a strong competing method with significantly larger parameters and training data.** It encompasses 9.72B parameters in total, including a 3.1B text-to-video diffusion model, a 1.3B prior model, a 3.1B frame interpolation model, and more. Make-A-Video is trained using 2.3B images, WebVid-10M videos, and a 10M video subset from HD-VILA-100M. As a result, Make-A-Video achieves the highest CLIPSIM score of $0.3049$, while our proposed VersVideo-H model achieves a slightly lower score of $0.3014$. \n\nMoreover, it's worth noting that Make-A-Video did not provide results for the FVD metric for motion dynamic evaluation.\n\n\nWhen comparing our proposed VersVideo model with the most recent models trained using WebVid-10M, it outperforms them all. For instance, Videocompoer (1.85B) and Videofusion (1.83B) have comparable model sizes, but their performance falls short with FVD scores of $580$ and $581$, and CLIPSIM scores of $0.2932$ and $0.2795$, respectively. In contrast, our proposed model achieves superior results with an FVD score of $421$ and a CLIPSIM score of $0.3014$. The second-best model that comes close to ours is SimDA that is an up-to-date concurrent work within two months of our submission. SimDA achieves an FVD score of $456$ and a CLIPSIM score of $0.2945$. These results highlight the performance of our VersVideo model over other recent video generation models.\n\n\n\n>Q3: From Table 2, the performance of the proposed method is a lot worse than the previous STOA method, e.g., VideoFusion. The author explains this by \"the VersVideo-L model has a significantly smaller parameter of 500M\". Why not use a larger model to verify the performance? \n\nThank you for your thorough review.\n\n\nIn this paper, we introduce two variants of *VersVideo*: *VersVideo-L* (500M) and *VersVideo-H* (2B). The primary purpose of *VersVideo-L* is for fast prototyping and conducting ablation studies.  *VersVideo-H* is trained for text-to-video generation and controllable generation tasks. If we were to use *VersVideo-H* for prototyping and ablation studies, it would require training several models with billions of parameters from scratch. This would result in an exceptionally high cost for validating model design and optimizing the network. Therefore, we choose to conduct the ablation study using *VersVideo-L* to validate our model design, and subsequently scale up the model to 2B parameters."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492293211,
                "cdate": 1700492293211,
                "tmdate": 1700492332319,
                "mdate": 1700492332319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bpd7g2Nkth",
            "forum": "K9sVJ17zvB",
            "replyto": "K9sVJ17zvB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_CWze"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_CWze"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of creating stable and controllable videos by proposing a versatile video generation model. In contrast to existing video diffusion models that often limit spatial-temporal performance due to oversimplification of standard 3D operations, VersVideo introduces multi-excitation paths for spatial-temporal convolutions with dimension pooling across different axes and multi-expert spatial-temporal attention blocks. This approach significantly improves the model's spatial-temporal performance without increasing training and inference costs. To address information loss during the transformation from pixel space to latent features and back, temporal modules are incorporated into the decoder to maintain inter-frame consistency. The paper also presents a unified ControlNet model suitable for various conditions, such as image, Canny, HED, depth, and style."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The introduction of multi-excitation paths and multi-expert spatial-temporal attention blocks enhances the model's spatial-temporal capabilities without significantly increasing computational cost. This is particularly impressive given the complexity of 3D operations in video generation. The solution proposed for minimizing the issue of information loss, mainly through the incorporation of the temporal module, helps achieve better inter-frame consistency, which is crucial for maintaining the temporal coherence of generated videos. The development of a unified ControlNet provides a more versatile approach to handle various conditions, leading to broad applicability.\n2. The quantitative improvements of the proposed method are notable. The related ablations offer reliable evidence about the effectiveness of the whole system."
                },
                "weaknesses": {
                    "value": "1. Leveraging pre-trained models could limit the model's versatility and the ability to generalize across different datasets. Is it possible to validate the proposed designs to other SD models?\n2. The notable quantitative and qualitative improvements are appreciated. However, some common issues are not discussed in the paper, e.g., hand generation, multiple object generation, instruction following, and how to respond to relative location descriptions, etc. If these cases cannot be well-addressed, then they should be included in a limitation discussion."
                },
                "questions": {
                    "value": "1. It might be beneficial to include a section discussing the limitations of your current approach and possible future work to address these limitations.\n2. If possible, consider releasing the code or at least providing more detailed implementation notes. This could help others replicate your results and build upon your work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786418222,
            "cdate": 1698786418222,
            "tmdate": 1699636844475,
            "mdate": 1699636844475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UEOPZJk4mO",
                "forum": "K9sVJ17zvB",
                "replyto": "Bpd7g2Nkth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CWze [1/3]"
                    },
                    "comment": {
                        "value": "The reviewer **CWze** primarily focused on the limitations and future scope of our work. The reviewer also encouraged us to add more details about the implementation. In response, we clarified our contributions and discussed the potential application of our work to other related studies. Additionally, we added more implementation details about the proposed MoE spatial-temporal transformer and TCM-VAE to ensure reproducibility.\n\n>Q1: Leveraging pre-trained models could limit the model's versatility and the ability to generalize across different datasets. Is it possible to validate the proposed designs to other SD models?\n\n \nThank you for your review and valuable suggestions. \n\nIncorporating temporal structures for video generation is a widely adopted approach to enhance existing text-to-image models, such as Stable Diffusion (SD). Several methods have been employed in this regard, including training the entire network [ref-1; ref-2; ref-3] or freezing the pretrained image network to ensure compatibility with other SD models [ref-4; ref-5; ref-6].\n\n\nOur designs can be utilized for other video diffusion models that are based on SD in three different ways:\n1. The over-simplified temporal resblock/transformer can be replaced with the proposed MEC and MoE transformer of the denoising UNet.\n2.  The image VAE can be replaced by our proposed TCM-VAE. For example, we can enhance the image VAE decoder of *VideoFusion* and *VideoComposer* by using our TCM-VAE decoder, without additional training. This module significantly reduces flicker artifacts, as shown in our [demo](https://docs.google.com/document/d/e/2PACX-1vQyICnc6WH8CgCCRuX0avsOI7MoO4gyPRQEro-v1UGu0nI60CTCiDksLUXOV5y_A597X41rL5mQdyER/pub).\n3.  Designing an unified control strategy outlined in our paper, rather than developing individual motion structure conditions with multiple individual ControlNets for video generation.\n\n\nReference:\n\n[ref-1] Esser, P., Chiu, J., Atighehchian, P., Granskog, J. and Germanidis, A., 2023. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 7346-7356).\n\n[ref-2] Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X. and Shou, M.Z., 2023. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 7623-7633).\n\n[ref-3] Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y. and Feng, J., 2022. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018.\n\n[ref-4] Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S. and Shi, H., 2023. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439.\n\n[ref-5] Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S. and Kreis, K., 2023. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22563-22575).\n\n[ref-6] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D. and Dai, B., 2023. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492016031,
                "cdate": 1700492016031,
                "tmdate": 1700492122559,
                "mdate": 1700492122559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4jDnPCFeIo",
                "forum": "K9sVJ17zvB",
                "replyto": "Bpd7g2Nkth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CWze [2/3]"
                    },
                    "comment": {
                        "value": ">Q2: The notable quantitative and qualitative improvements are appreciated. However, some common issues are not discussed in the paper, e.g., hand generation, multiple object generation, instruction following, and how to respond to relative location descriptions, etc. If these cases cannot be well-addressed, then they should be included in a limitation discussion.\n\nThank you for your insightful and detailed review of this point.\n\nThe common issues you mentioned are widely acknowledged in image diffusion generation and are also inherent in most video diffusion models. For instance, it is well-known that diffusion models struggle to create realistic hands, mainly due to the inherent complexity of the human hand and the variability and diversity of the hands in the training dataset. \n\nAnother challenge is text alignment, which involves multi-object generation and relative location assignment. Recently, significant progress has been made in this direction by Dalle-3 [ref-1] and other other works [ref-2] on image generation. They have improved text-to-image models by training them on enhanced captions auto-generated by a robust image-to-text model. This approach largely alleviates the text alignment problem. Similarly, addressing these specific problems in video generation necessitates specially tailored solutions. For instance, utilizing more precise textual descriptions for training videos or incorporating a wider range of hand data.\n\nSince *VersVideo* utilizes the WebVid-10M dataset, which contains video-text data from the internet, additional efforts in data curation/collection and related techniques for image generation are necessary to address these challenges. We acknowledge this limitation in our paper.\n\n\nReference:\n\n[ref-1] Betker, James, Gabriel Goh, Li Jing, \u2020 TimBrooks, Jianfeng Wang, Linjie Li, \u2020 LongOuyang, \u2020 JuntangZhuang, \u2020 JoyceLee, \u2020 YufeiGuo, \u2020 WesamManassra, \u2020 PrafullaDhariwal, \u2020 CaseyChu, \u2020 YunxinJiao and Aditya Ramesh, 2023. Improving Image Generation with Better Captions.  \n\n[ref-2] Segalis, E., Valevski, D., Lumen, D., Matias, Y. and Leviathan, Y., 2023. A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. arXiv preprint arXiv:2310.16656.\n\n\n>Q3: It might be beneficial to include a section discussing the limitations of your current approach and possible future work to address these limitations.\n\nThank you for your insightful comments and suggestions.\n\nDespite its strong generative capabilities and versatile guidance inputs, the proposed *VersVideo* has some limitations in its current state:\n1. The generated videos sometimes contain watermarks due to the training dataset WebVid-10M. To ensure high-quality generation, it is necessary to have watermark-free videos with textual descriptions.\n2. Our primary focus is on generating videos of a fixed length (e.g., $L = 24$). However, an interesting future direction would be to extend our method for long video synthesis, considering clip-by-clip generation.\n3. *VersVideo* inherits certain common challenges from image diffusion, including hand generation and text alignment. Specific solutions employed in image generation could serve as inspiration for video generation.\n4. The stability of image-to-video generation is not perfect. The input image cannot be consistently maintained throughout the generated video. It requires additional control mechanisms to preserve the spatial appearance of the input image more accurately.\n\n[A snapshot of our revision.](https://drive.google.com/file/d/1f1UdXPXBKIKyhMWgVA8SiF3w7FOx4Nt1/view?usp=sharing)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492066025,
                "cdate": 1700492066025,
                "tmdate": 1700492114297,
                "mdate": 1700492114297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u5Xro4mm8R",
                "forum": "K9sVJ17zvB",
                "replyto": "Bpd7g2Nkth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CWze [3/3]"
                    },
                    "comment": {
                        "value": ">Q4: If possible, consider releasing the code or at least providing more detailed implementation notes. This could help others replicate your results and build upon your work.\n\nThank you for taking the time to review our paper.\n\nWe provide details (e.g., hyperparameter, model, and optimizer) and experiment setups (e.g., datasets, metrics) in Section 3.2 and Appendix A, B, and C. We incorporated additional details regarding the key modules to facilitate implementation and enhance understanding.\n1. MoE spatial-temporal transformer.  In our paper, we introduce a new MoE scheme that trains experts without a router network, reducing the number of parameters introduced by the router and simplifying training. Our MoE assigns tokens to experts by randomly partitioning them into groups. At the end of each training iteration, we perform weight averaging on each MoE's experts. After training, we can simply average the experts of each MoE into a single FFN, without increasing inference cost.  Merging weights of models for better performance is widely acknowledged in large language models and image classification. This simple yet effective idea has been proven to improve accuracy and robustness. Similarly, our proposed MoE merges experts' weights to efficiently enhance the performance of the spatial-temporal transformer. Unlike conventional model ensembles, we average the model weights without incurring any additional inference or memory costs. [A snapshot of our revision](https://drive.google.com/file/d/1WXwEmOOBMi4Mr7dxF2U65-viJ1lf2A5J/view?usp=sharing).\n2. TCM-VAE. We introduce Temporal Compensation Modules (TCMs) to enhance temporal consistency, and a refinement UNet for quality enhancement, building upon an image pretrained VAE. To optimize training efficiency, we freeze the encoder and concentrate on training the video decoder. We incorporate TCMs into the decoder blocks to infuse temporal information into multi-level intermediate features. The implementation of TCMs leverages the multi-excitation module in Section 2.2.  More details about the TCM-VAE can be found in **Section2.3, page 5** and **Section C of the Appendix**. [A snapshot of our revision.](https://drive.google.com/file/d/1r6jnbaAk_OLFXTVXUyc1ogPlxD7ZxrrB/view?usp=sharing)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492101093,
                "cdate": 1700492101093,
                "tmdate": 1700493442347,
                "mdate": 1700493442347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sCUiYXxhOv",
                "forum": "K9sVJ17zvB",
                "replyto": "Bpd7g2Nkth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-Up Discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer CWze,\n\nWe sincerely value the time and effort you have dedicated to offering your perceptive observations on our paper. After thoroughly examining your feedback, we have formulated responses that we believe will effectively address your concerns.\n\nWe have expanded our discussion to further address the limitations of our model, as well as to outline the potential future directions this study could take.\n\nWe hope you can reach out to us at your earliest convenience.\n\nBest regards,"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705052482,
                "cdate": 1700705052482,
                "tmdate": 1700705136896,
                "mdate": 1700705136896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3jFlAL0Ut3",
                "forum": "K9sVJ17zvB",
                "replyto": "sCUiYXxhOv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_CWze"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_CWze"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your diligent efforts in addressing the concerns that were raised. However, after a thorough reconsideration of your paper in light of these responses, my assessment aligns with my initial rating. I encourage you to continue refining and developing your work. Your research is valuable and contributes significantly to the field."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730996219,
                "cdate": 1700730996219,
                "tmdate": 1700730996219,
                "mdate": 1700730996219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GhkYjE0iHo",
            "forum": "K9sVJ17zvB",
            "replyto": "K9sVJ17zvB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_eX2J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_eX2J"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes VersVideo for video generation. To capture the temporal information, multi-excitation paths for spatial-temporal convolutions and multi-expert spatial-temporal attention blocks. Also, the decoder of VAE is also finetuned to maintain inter-frame consistency. The authors demonstrate the effectiveness of VersVideo in multiple video generation tasks, e.g., text-to-video generation and conditional video generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper studies video generation and proposes several blocks to enhance the temporal consistency of generated videos.\n\n2) The generated videos have plausible results.\n\n3) The authors demonstrate the capability of the proposed network in the controllable generation setting."
                },
                "weaknesses": {
                    "value": "1) The details of the MoE attention are not explained very well. In section 2.2, the authors mentioned that they use MoE design to enhance attention. If my understanding is correct, the attention is performed on all channels, but the feedforward network after attention is applied to different groups. If the MoE attention is applied like this, how can the feature or attention be enhanced?\n\n2) Also, several details are missing for the temporal compensate decoder. In the paper, the authors do not mention what S1 stands for.\n\n3) For the Multi-excitation conv, the motivation for using four branches is not addressed clearly. The effectiveness of each branch is also not thoroughly ablated."
                },
                "questions": {
                    "value": "Please see my concerns in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819682337,
            "cdate": 1698819682337,
            "tmdate": 1699636844352,
            "mdate": 1699636844352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZOwbQ1DqLN",
                "forum": "K9sVJ17zvB",
                "replyto": "GhkYjE0iHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eX2J [1/3]"
                    },
                    "comment": {
                        "value": "The reviewer **eX2J** has raised concerns primarily about the methodology details in the paper. To address these concerns, we have added more information about the motivation and implementation of the proposed MoE spatial-temporal transformer and the temporal compensated VAE, with the help of illustrations and visual demonstrations. We have also conducted an ablation study to verify the effectiveness of each path in the multi-excitation convolution.\n\n\n\n> Q1: The details of the MoE attention are not explained very well. In section 2.2, the authors mentioned that they use MoE design to enhance attention. If my understanding is correct, the attention is performed on all channels, but the feedforward network after attention is applied to different groups. If the MoE attention is applied like this, how can the feature or attention be enhanced?\n\nThank you for highlighting this issue; we apologize for any confusion caused. \n\nOur design might be more appropriate to refer to it as the 'MoE Spatial-Temporal Transformer' rather than 'MoE Spatial-Temporal Attention.' \n\nThe role of the FFN in transformers is to process the information aggregated by the attention mechanism. It can learn to recognize and generate more intricate patterns based on the information it receives from the attention block. \n\nThe most recent prevalent training approach for transformers involves replacing the FFN layer with a sparse MoE [ref-1; ref-2; ref-3]. The sparse MoE includes multiple expert FFNs, each with unique weights, and a trainable routing network. During both the training and inference phases, this routing network selects a sparse set of experts for each input, enabling efficient scalability of transformer models through sparse computation.\n\nIn our paper, we introduce a new MoE scheme that trains experts without a router network, reducing the number of parameters introduced by the router and simplifying training. Our MoE assigns tokens to experts by randomly partitioning them into groups. At the end of each training iteration, we perform weight averaging on each MoE's experts. After training, we can simply average the experts of each MoE into a single FFN, without increasing inference cost. \n\n**Why does this MoE work?** Merging weights of models for better performance is widely acknowledged in large language models [ref-6] and image classification [ref-4; ref-5]. This simple yet effective idea has been proven to improve accuracy and robustness. Similarly, our proposed MoE merges experts' weights to efficiently enhance the performance of the spatial-temporal transformer. Unlike conventional model ensembles, we average the model weights without incurring any additional inference or memory costs.\n\n\n\nPlease kindly refer to **Section B in the Appendix** where we have added more details and descriptions of the MoE design. [A snapshot of our revision](https://drive.google.com/file/d/1WXwEmOOBMi4Mr7dxF2U65-viJ1lf2A5J/view?usp=sharing).\n\n_Reference_:\n\n[ref-1] Du, N., Huang, Y., Dai, A.M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A.W., Firat, O. and Zoph, B., 2022, June. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning (pp. 5547-5569). PMLR.\n\n[ref-2] Fedus, W., Zoph, B. and Shazeer, N., 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1), pp.5232-5270.\n\n[ref-3] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.\n\n[ref-4] Wortsman, M., Ilharco, G., Gadre, S.Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A.S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S. and Schmidt, L., 2022, June. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (pp. 23965-23998). PMLR.\n\n[ref-5] Ainsworth, S.K., Hayase, J. and Srinivasa, S., Git re-basin: Merging models modulo permutation symmetries, 2022. URL https://arxiv. org/abs/2209.04836.\n\n[ref-6] Jin, X., Ren, X., Preotiuc-Pietro, D. and Cheng, P., 2022. Dataless knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491653822,
                "cdate": 1700491653822,
                "tmdate": 1700491769857,
                "mdate": 1700491769857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HCpS3VKxmQ",
                "forum": "K9sVJ17zvB",
                "replyto": "GhkYjE0iHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eX2J [2/3]"
                    },
                    "comment": {
                        "value": ">Q2: Also, several details are missing for the temporal compensate decoder. In the paper, the authors do not mention what S1 stands for.\n\nThank you for pointing out this issue. We sincerely apologize for such confusions.\n\nIn Fig. 3, $\\{A_1, A_2,...,A_M\\}$ are encoder blocks; $\\{S_1, S_2, ..., S_M\\}$ are decoder blocks. See [Fig. 3](https://drive.google.com/file/d/1-OtQhJ5zHNQGo4zTq-BQgAZxIwI9Az1x/view?usp=sharing) in the revised paper. \n\nMost existing video latent diffusion models utilize image VAEs to reconstruct videos from latents. However, an image VAE without temporal modules fails to capture the dependencies between frames. This leads to a loss of temporal information in videos, which in turn results in flickering artifacts and temporal inconsistency.\n\nWe introduce Temporal Compensation Modules (TCMs) to enhance temporal consistency, and a refinement UNet for quality enhancement, building upon an image pretrained VAE. To optimize training efficiency, we freeze the encoder and concentrate on training the video decoder. We incorporate TCMs into the decoder blocks to infuse temporal information into multi-level intermediate features. The implementation of TCMs leverages the multi-excitation module in Section 2.2. \n\nMore details about the TCM-VAE can be found in **Section2.3, page 5** and **Section C of the Appendix**. [A snapshot of our revision.](https://drive.google.com/file/d/1r6jnbaAk_OLFXTVXUyc1ogPlxD7ZxrrB/view?usp=sharing)\n\nThe effectiveness of TCM-VAE in improving video quality is supported by quantitative metrics presented in **Table 3** of the paper, as well as visual examples available in the demo for video generation and reconstruction: [demo for video generation](https://docs.google.com/document/d/e/2PACX-1vQyICnc6WH8CgCCRuX0avsOI7MoO4gyPRQEro-v1UGu0nI60CTCiDksLUXOV5y_A597X41rL5mQdyER/pub) and [demo for video reconstruction](https://anonymous-pages.github.io/video_demos/)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491716265,
                "cdate": 1700491716265,
                "tmdate": 1700493091743,
                "mdate": 1700493091743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jrtV7ZN1DU",
                "forum": "K9sVJ17zvB",
                "replyto": "GhkYjE0iHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eX2J [3/3]"
                    },
                    "comment": {
                        "value": ">Q3: For the Multi-excitation conv, the motivation for using four branches is not addressed clearly. The effectiveness of each branch is also not thoroughly ablated.\n\n\nWe appreciate your thorough review and the effort you put into providing detailed feedback.\n\n\n**Multi-Excitatin Convolution (MEC) motivation.** In contrast to existing video diffusion models, which often restrict spatial-temporal performance due to the over-simplification of standard 3D operations, we propose multi-excitation paths for spatial-temporal convolutions with dimension pooling across different axes. \n\nThe proposed MEC is constructed by assembling four paths in parallel, allowing for the **activation of multi-type information in videos**. Our design is fundamentally inspired by the squeeze-and-excitation (SE) block [ref-1; ref-2], as it explicitly models channel/temporal interdependencies. Drawing inspiration from these two previous works, we design the excitation paths:\n1. Factorized 2D spatial + 1D temporal convolution. This path is a **simplified adaptation of 3D convolution** and is the only temporal module widely used in most existing video diffusion models [ref-3; ref-4; ref-5]. \n2. Spatial-temporal excitation. This path pools the 5-D tensor $(B, C, T, H, W)$ along channel dimension, resulting in the **spatial-temporal activation** mask $(B, 1, T, H, W)$ which is later processed with 3D convolution. \n3. Channel excitation. This path get global spatial information $(B, C, T, 1, 1)$ of the input feature $(B, C, T, H, W)$ by spatial average pooling.\n4. Motion excitation. The **motion information** is modeled by adjacent frames using the same squeeze and unsqueeze strategy. Related works have explored this, aiming to model motion information based on the feature level instead of the pixel level [ref-2; ref-6].\n\n\n\n**Ablation study of MEC.** An ablation study is conducted to validate each excitation path of MEC, which includes the MoE-STA component. MEC includes factorized 2D spatial + 1D temporal convolution (2D+1D), spatial-temporal excitation (STE), channel excitation (CE), and motion excitation (ME). Various versions of the *VersVideo-L* model are trained with and without these paths, followed by an evaluation of their class-conditional generation using the UCF-101 dataset. The results suggest that the excitation modules improve the performance of MEC. When integrating all four excitation paths, the MEC demonstrates superior performance in both IS and FVD.\n\n| 2D+1D | STE | CE | ME | IS $\\uparrow$ | FVD $\\downarrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | 69.5 | 249 |\n| $\\checkmark$ |  | $\\checkmark$ | $\\checkmark$ | 68.0 | 264 |\n| $\\checkmark$ | $\\checkmark$ |  | $\\checkmark$ | 67.7 | 266 |\n| $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |  | 68.3 | 260 |\n| $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | 72.9 | 207 |\n\nPlease refer to [**Section D in the Appendix**](https://drive.google.com/file/d/12SAET1FOi6bYzyCwwc3UDzT3qP0OBZHS/view?usp=sharing) for our revision.\n\n*Reference*:\n\n[ref-1] Hu, J., Shen, L. and Sun, G., 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).\n\n[ref-2] Li, Y., Ji, B., Shi, X., Zhang, J., Kang, B. and Wang, L., 2020. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 909-918).\n\n[ref-3] Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D. and Zhou, J., 2023. VideoComposer: Compositional Video Synthesis with Motion Controllability. arXiv preprint arXiv:2306.02018.\n\n[ref-4] Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S. and Kreis, K., 2023. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22563-22575).\n\n[ref-5] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D. and Dai, B., 2023. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725.\n\n[ref-6] Jiang, B., Wang, M., Gan, W., Wu, W. and Yan, J., 2019. Stm: Spatiotemporal and motion encoding for action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 2000-2009)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491747684,
                "cdate": 1700491747684,
                "tmdate": 1700491809088,
                "mdate": 1700491809088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DDjAbALP0H",
                "forum": "K9sVJ17zvB",
                "replyto": "KFcF9V1sTZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_eX2J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_eX2J"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussion"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for preparing the rebuttal. I agree with Reviewer 1xNe that visual results for the ablation study are required so that the effectiveness of the proposed modules can be better visualized.\n\nTherefore, I would keep my original rating.\n\nThanks,"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711528776,
                "cdate": 1700711528776,
                "tmdate": 1700711528776,
                "mdate": 1700711528776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B80LXGDOhW",
            "forum": "K9sVJ17zvB",
            "replyto": "K9sVJ17zvB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a versatile video generation model named VersVideo, which enhances spatial-temporal capability using textual, visual, and stylistic conditions. The model incorporates multi-excitation paths for spatial-temporal convolutions and multi-expert spatial-temporal attention blocks to improve temporal consistency and generation performance without escalating costs. It also uses temporal modules in the decoder to maintain inter-frame consistency and mitigate information loss. Besides, they design a unified ControlNet model suitable for various visual conditions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well organized. The authors have well categorized and summarized previous methods. The authors explain the shortcomings of previous methods and the motivation for proposing the method very clearly.\n- The authors provide web demos for visualization. It is helpful for the readers to examine the effectiveness of the proposed method.\n- The authors provide a specially designed module for perceiving temporal information for video generation tasks. The design of the module is very innovative, and it shows improvement in quantitative indicators.\n- The decoder design and consistency loss are helpful for maintaining temporal consistency."
                },
                "weaknesses": {
                    "value": "- The authors did not provide a visual comparison of the results. Providing a visual comparison would better demonstrate the effectiveness of the method. Besides, a user study is also needed to verify the effectiveness of the method.\n- Adding the impact of each module on visual effects in the ablation experiments would be more helpful in verifying what role each module plays.\n- The quality of the video-to-video editing results currently shown in the paper is not much better compared to other methods such as Render-A-Video and TokenFlow. The results generated in the current demo have severe color changes. Therefore, I doubt the effectiveness of this method in video fidelity.\n- Since many modules in the framework designed in the paper are related to temporal consistency, please provide indicators to measure temporal consistency. Currently, the paper only lists indicators of generation quality."
                },
                "questions": {
                    "value": "Can the current framework handle the situation with multiple condition inputs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7134/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7134/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859347995,
            "cdate": 1698859347995,
            "tmdate": 1699636844223,
            "mdate": 1699636844223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jGrdjMRWr1",
                "forum": "K9sVJ17zvB",
                "replyto": "B80LXGDOhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1xNe [1/2]"
                    },
                    "comment": {
                        "value": "Reviewer **1xNe** expressed a primary concern regarding the visual effects of our proposed model. In response, we made the folllowing revisions: \n1. Added more text2video visual results in comparison to other prior models.\n2. Conducted a user study to verify the effectiveness of *VersVideo*.\n3. Demonstrated the potential of multiple control conditions for video generation.\n\n\n> Q1: The authors did not provide a visual comparison of the results. Providing a visual comparison would better demonstrate the effectiveness of the method. Besides, a user study is also needed to verify the effectiveness of the method.\n\nThank you for your valuable feedback. \n\nWe incorporate more visual examples of competing methods for comparison in our demo, which can be found at [link](https://docs.google.com/document/u/1/d/e/2PACX-1vTpTofxtnoetQwgFcVaHWx01Offc2sc4T5zRwY6I-tueiECmpZKbnrYqRQ9pXTdXsbvJelg2Y6OaW4n/pub).  Additionally, we provide a comparison of conditioned generation at the end of [demo](https://anonymous-pages.github.io/video_demos/).\n\nTo conduct the user study, we selected four widely acknowledged and open-sourced models, namely Text2video-zero (Khachatryanet al., 2023), VideoFusion (Luo et al., 2023), Zeroscope (Hugging Face, 2023), and VideoComposer (Wang et al., 2023d), for visual comparison. The evaluation set consisted of 30 video prompts, and 16 evaluators compared the inter-frame consistency and overall video quality (including inter-frame consistency, alignment of text-video, and perceptual factors) between two videos - one from competing methods and one from our method, shown in a random sequence. Figure 11 shows that our method, *VersVideo*, was preferred over VideoFusion $83.3\\\\%$ of the time in terms of video quality, and over VideoComposer $88.5\\\\%$ of the time. Our method also performed significantly better than the baseline methods in terms of inter-frame consistency in the user study.\n\nFor detailed results, please refer to [**Section E in the Appendix**](https://drive.google.com/file/d/1N0yYBc__UTsc6sABkA3FPkN0VOINHWpP/view?usp=sharing).\n\n\n> Q2: Adding the impact of each module on visual effects in the ablation experiments would be more helpful in verifying what role each module plays.\n\n\nThank you for your insightful comments and suggestions.\n\nThere are two fundamental modules designed for denoising UNet, namely, MEC and MOE-STA, as illustrated in Table 2. These modules work collaboratively to enhance the spatial-temporal quality of the generated videos. However, it's challenging to dissect their individual contributions to the final results, despite our attempts to visualize the videos from these ablation studies. The diffusion model is a complex system that intertwines all modules, making it difficult to distinguish the impact of each module through qualitative evaluation. Therefore, we rely on quantitative metrics to reveal their subtle effects, as shown in Table 2.\n\nRegarding TCM-VAE, we have conducted an ablation study presented in Table 3, which covers both video generation and video reconstruction using quantitative metrics. In addition, we provide visual demonstrations for [video reconstruction](https://anonymous-pages.github.io/video_demos/) in the second part of the demo and [video generation](https://docs.google.com/document/d/e/2PACX-1vQyICnc6WH8CgCCRuX0avsOI7MoO4gyPRQEro-v1UGu0nI60CTCiDksLUXOV5y_A597X41rL5mQdyER/pub) for different video diffusion models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491161708,
                "cdate": 1700491161708,
                "tmdate": 1700492702113,
                "mdate": 1700492702113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rwavnSuied",
                "forum": "K9sVJ17zvB",
                "replyto": "B80LXGDOhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1xNe [2/2]"
                    },
                    "comment": {
                        "value": ">Q3: The quality of the video-to-video editing results currently shown in the paper is not much better compared to other methods such as Render-A-Video and TokenFlow. The results generated in the current demo have severe color changes. Therefore, I doubt the effectiveness of this method in video fidelity.\n\nThank you for your valuable feedbacks.\n\n\nIn our demo, the first example of video-to-video editing showcased severe color changes due to the text prompt ***\"cyberpunk city\"* which is associated with the metaverse, characterized by rapid color changes and flashing lights.**\n\nThere are two concurrent studies, Render-A-Video and TokenFlow, for video editing. The Render-A-Video is a video-to-video model that employs multiple pipelines. It leverages the off-the-shelf image model from [CivitAI](https://civitai.com/), a public platform that enables artists to share their personalized models. In addition, TokenFlow utilizes pre-trained image models and the PnP image diffusion model [ref-1] for video editing.\n\nIt would be reasonable to expect that their rendering results are more promising in terms of fidelity. This is because the video model is based on pretrained image models that were trained using high-quality images. However, there is a significant difference between their approach and the proposed *VersVideo* method. While Render-A-Video and TokenFlow are specialized for video-to-video applications, our model is designed to be versatile, allowing for a wider range of applications such as text-to-video, image-to-video conversion, and stylization.\n\n\n*Reference*:\n\n[ref-1] Tumanyan, N., Geyer, M., Bagon, S. and Dekel, T., 2023. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1921-1930).\n\n\n\n\n>Q4: Since many modules in the framework designed in the paper are related to temporal consistency, please provide indicators to measure temporal consistency. Currently, the paper only lists indicators of generation quality.\n\n\nThank you for your thoughtful comments.\n\nWe use the widely accepted Frechet Video Distance (FVD) [ref-1] and CLIP Similarity (CLIPSIM) [ref-2] as evaluation metrics for text-to-video generation. CLIPSIM assesses video quality and text alignment by averaging the CLIP similarity score of all generated frames. The FVD metric evaluates the similarity between real and generated videos, considering **both video quality and motion dynamics**. Therefore, FVD can serve as an indicator of temporal consistency. As most prior studies report FVD and CLIPSIM as the gold standard, we follow this practice to ensure a fair comparison.\n\n\n*Reference*:\n\n[ref-1] Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M. and Gelly, S., 2018. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717.\n\n[ref-2] Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G. and Duan, N., 2021. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806.\n\n\n>Q5: Can the current framework handle the situation with multiple condition inputs?\n\nThank you for your insightful comments.  Our model can handle multiple condition inputs.\n\nThe global condition, such as the text prompt and style, is inherently compatible with other conditions. For instance, the final two rows in Fig. 6 utilize depth maps for motion structure control, while simultaneously employing images for global style control. \n\nWhen it comes to the composition of multiple motion structures, such as the concurrent use of depth, canny, and HED, we offer several examples to demonstrate its practicality with the [visual examples](https://docs.google.com/document/d/e/2PACX-1vQTN8K7LDRgAEAEniaQYD2ePbogQ__Hht7ewRqlzh0RNCfI13bIEPPzZwl67AK-gluB-UsYBagLd-R0/pub)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491191834,
                "cdate": 1700491191834,
                "tmdate": 1700492916087,
                "mdate": 1700492916087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FEupWRKRpo",
                "forum": "K9sVJ17zvB",
                "replyto": "rwavnSuied",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors & Decision"
                    },
                    "comment": {
                        "value": "Thanks for the authors' reply. After reading the authors' responses, I decide to keep my previous score unchanged. At the same time, I would not be surprised if this paper is rejected. The reasons are listed below:\n\n*Disadvantages*:\n\n1. The author did not provide any change and analysis of the visual effects caused by each module in the ablation study, which is similar to the viewpoint of **reviewer eX2J**. The author only listed the impact of the final version of the VAE encoder on visual effects in the response. This makes me question the effectiveness of each module (especially on the visual effect).\n\n2. The authors claim that the proposed module can enhance spatio-temporal consistency, but there is no corresponding metrics to prove it. FVD and average CLIP score cannot measure from the above dimensions. I suggest using metrics such as Frame consistency and motion control metric in *VideoComposer* to measure frame consistency. In addition, the temporal error in this [link](https://github.com/phoenix104104/fast_blind_video_consistency#evaluation) can also be used.\n\n3. The authors did not explain how their model manages to handle multiple conditions simultaneously.\n\n*Advantages*:\n\nThis article is indeed well-organized. The proposed module is technically novel. The authors have partially demonstrated its effectiveness."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635644933,
                "cdate": 1700635644933,
                "tmdate": 1700635644933,
                "mdate": 1700635644933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nOEusqhx0h",
                "forum": "K9sVJ17zvB",
                "replyto": "B80LXGDOhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedbacks."
                    },
                    "comment": {
                        "value": "We appreciate your insightful feedback. We are sorry that our responses do not fully address your concerns.\n\n1. We strongly agree that decomposing each module's visual effect for the final results would greatly help to gain an intuitive understanding of their contributions. In the paper, we rely on the quantitative metric to reveal these improvements. Admittedly, this could be our limitation of the response. \n2. We are grateful for your suggestion regarding the frame consistency metric. We will add this metric shortly in the paper. \n3. In the visual example, we generate videos with depth, HED, and canny collaboratively.  Our model is capable of handling multiple motion structure conditions because we train separate adapters for each condition,  and the features of these conditions can be added together for generation. This way of handling multiple conditions aligns with the application of multiple ControlNets (Zhang et al., 2023) or multiple T2I Adapters (Mou et al., 2023)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654168085,
                "cdate": 1700654168085,
                "tmdate": 1700654274066,
                "mdate": 1700654274066,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]