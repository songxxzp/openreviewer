[
    {
        "title": "Lookahead Sharpness-Aware Minimization"
    },
    {
        "review": {
            "id": "se6AwtobSp",
            "forum": "4WKDwIaF7y",
            "replyto": "4WKDwIaF7y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_1P81"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_1P81"
            ],
            "content": {
                "summary": {
                    "value": "-\tThe paper proposes optimization methods using an extra gradient to improve the performance of SAM. In section 3, the authors propose a maximization with respect to $\\varepsilon$, while SAM only focuses on maximization with respect to $w$. Then, the authors propose simple optimization schemes, such as EG-SAM and OG-SAM, to mitigate the computational burden of extra gradient methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper has a solid mathematical background to derive the optimization methods. Furthermore, the proposed method can alleviate the negative effects of SAM, which can get stuck at saddle points.\n-\tThe paper combines the proposed method with other SAM-related papers, which can reduce the computational burden of EG-SAM.\n-\tThe proposed method has the same convergence rate as SAM, which does not require additional training epochs.\n-\tThe paper demonstrates the performance improvement of using an additional gradient in a wide range of datasets, outperforming existing methods. The authors include various architectures and datasets, including the noise settings."
                },
                "weaknesses": {
                    "value": "-\tPlease refer to questions."
                },
                "questions": {
                    "value": "I will happily raise the score if the authors can address the following questions:\n\n1.\tI am curious about the motivation behind the proposed method. I agree that using the extra gradient (where $w$ and $\\varepsilon$ correspond to $x$ and $y$ in Section 3) might be beneficial for training. Can you clarify why  EG (and correspondingly OG) are straightforward solutions in terms of SAM for escaping saddle points? Or the purpose of EG is to increase the loss of $L(w+\\varepsilon)$ by perturbing $\\varepsilon$? The loss maximization does not seem to align with performance gain [1].\n\n2.\tTo push further the question 1, I am not convinced that SAM+EG and EG-SAM (also OG) are not highly correlated because $\\varepsilon$ in EG-SAM is not calculated to enlarge the loss with respect to $\\varepsilon$. Rather, even though I did not calculate the ODE accurately, the behavior of EG-SAM seems somewhat related to implicit methods in ODEs (thus stable) rather than using explicit methods by using $\\hat{w_t}$ obtained from the next step in SGD terms.\n\n3.\tI agree that EG-SAM can optimize better in quadratic loss, as shown in Section 4 with ODE. However, the optimization itself is discrete, and some behaviors (such as SGD or catapult effects) cannot be explained in ODE settings. Thus, could the authors demonstrate the effects of EG-SAM in real situations with **SGD** to escape saddle points, such as in Fig. 7 in [2]?\n\n4.\tThe authors provide information about the effects on computational burden compared to %SAM. However, as SAM and EG-SAM both require 100%, they actually differ in terms of computational time. Therefore, wall clock computational time is needed to assess the computational overhead during real training.\n\n[1] https://arxiv.org/abs/2206.06232 \n\n[2] https://arxiv.org/abs/2301.06308"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698144027986,
            "cdate": 1698144027986,
            "tmdate": 1699636613785,
            "mdate": 1699636613785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eXQcJq6NuC",
                "forum": "4WKDwIaF7y",
                "replyto": "se6AwtobSp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 1P81 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive reviews. We summarize the reviewer's questions and present our responses below.\n\n\n---\n***Q1: I am curious about the motivation behind the proposed method. ... i) Can you clarify why EG (and correspondingly OG) are straightforward solutions in terms of SAM for escaping saddle points? \nii) Or the purpose of EG is to increase the loss of $L(w+\\varepsilon)$ by perturbing $\\varepsilon$ ? The loss maximization does not seem to align with performance gain [1].***\n\n\ni) The lookahead mechanism is designed to help to avoid saddle point by gathering more information about the landscape, as mentioned in Sec. 1. Additionally, another more intuitive explanation is that, as also pointed out by Reviewer hY3b, the EG-SAM update scheme can be written as\n$w_t =w_{t-1}-\\eta_t \\nabla L( w_{t-1}+\\epsilon^\\prime_t)$ where $\\epsilon^\\prime_t = (\\rho /||\\nabla L(w_{t-1})||-\\eta_t) \\nabla L(w_{t-1})$.\nNote that SAM's update scheme is \n$w_t =w_{t-1}-\\eta_t \\nabla L( w_{t-1}+\\epsilon_t)$, where\n$\\epsilon_t = (\\rho/||\\nabla L(w_{t-1})||)\\nabla L(w_{t-1})$.\nEG-SAM can be interpreted as suppressing perturbation in the early stages and allowing more perturbation in the final stages (as $\\epsilon_t$ decreases from large to small values). Since a larger perturbation can make the model more prone to being trapped in saddle points, as indicated by Kim et al. (2023), our method is more effective in avoiding saddle points than SAM.\n\nii)  Our EG does not aim to increase the loss of $L(w+\\varepsilon)$. As discussed in 1), our work is designed to avoid saddle points, which is  different from aiming to increase the loss value of $L(w+\\varepsilon)$. \n\n---\n\n***Q2: To push further the question 1, I am not convinced that SAM+EG and EG-SAM (also OG) are not highly correlated because $\\epsilon$\n in EG-SAM is not calculated to enlarge the loss with respect to $\\epsilon$.***\n\nWe agree that SAM+EG can enlarge the losses, as $\\epsilon$ is no longer a one-step gradient ascent. However, as discussed in A1, different from SAM+EG, EG-SAM is not designed for enlarging the loss. Thus, SAM+EG and EG-SAM are different.\n\nSpecifically, recall SAM can be formulated as a minimax problem (Eq. (1)) $\\min_{\\boldsymbol{w}} \\max_{\\boldsymbol{\\epsilon}:\\|\\boldsymbol{\\epsilon}\\| \\leq \\rho} L(\\boldsymbol{w}+\\boldsymbol{\\epsilon})$.\nAs EG is used for addressing minimax problem, \nSAM+EG can be directly implemented as the solver. However, for EG-SAM, it firstly uses a max oracle \n$  \\max_{\\boldsymbol{\\epsilon}:\\|\\boldsymbol{\\epsilon}\\| \\leq \\rho} L(\\boldsymbol{w}+\\boldsymbol{\\epsilon}) \\approx L(\\boldsymbol{w}+\\frac{\\rho \\nabla_{\\boldsymbol{w}} L(\\boldsymbol{w})}{\\|\\nabla_{\\boldsymbol{w}} L(\\boldsymbol{w})\\|}) $ \nfollowing the original SAM (Foret et al., 2020), then (Eq. 1) is converted to a minimization problem $\\min_{\\boldsymbol{w}} L(\\boldsymbol{w}+\\frac{\\rho \\nabla_{\\boldsymbol{w}} L(\\boldsymbol{w})}{\\|\\nabla_{\\boldsymbol{w}} L(\\boldsymbol{w})\\|}) $, and finally EG is designed to solve this problem. \n\n---\n***Q3: Rather, even though I did not calculate the ODE accurately, the behavior of EG-SAM seems somewhat related to implicit methods in ODEs (thus stable) rather than using explicit methods by using $\\hat{w}_t$\n obtained from the next step in SGD terms.***\n\nRegarding the ODE, we thank the reviewer for sharing the ODE perspective to help explain our method. Indeed, we agree with the reviewer that using future gradients to update the current model is somewhat similar to the implicit methods in ODE. We will further explore this connection in our future works.\n\n---\n***Q4: I agree that EG-SAM can optimize better in quadratic loss, as shown in Section 4 with ODE. However, the optimization itself is discrete, and some behaviors (such as SGD or catapult effects) cannot be explained in ODE settings. Thus, could the authors demonstrate the effects of EG-SAM in real situations with SGD to escape saddle points, such as in Fig. 7 in [2]?***\n\nThe reviewer might have some misunderstandings. We have already conducted a\ntoy\nexperiment at the beginning of  Sec. 3, which uses quadratic objective to show the discrete time behaviors of SAM and our methods around saddle point following (Compagnoni et al., 2023). Results reveal that discrete time EG-SAM can escape saddle point faster than SAM. \nFig. 7 in [2], as referred to by the reviewer,  similarly illustrates the discrete time behavior of SAM around saddle point. \n\nBesides, ODE is also widely used to analyse SAM (Kim et al., 2023; Compagnoni et al., 2023), and we follow the same analysis scheme."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458946028,
                "cdate": 1700458946028,
                "tmdate": 1700458946028,
                "mdate": 1700458946028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jfJGGvEraA",
                "forum": "4WKDwIaF7y",
                "replyto": "H2OCef2Hwu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_1P81"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_1P81"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal follow-up"
                    },
                    "comment": {
                        "value": "I have carefully reviewed the authors' rebuttal and the feedback from other reviewers. I thank for the authors for their comprehensive response and the additional effort they have put into enhancing the manuscript. Specifically, I correct my misunderstanding about Q1 and Q2. Still, I argue that some points should be further investigated as reviewer hY3b pointed out. Therefore, I will maintain my current score for the manuscript and await the input of other reviewers."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614044836,
                "cdate": 1700614044836,
                "tmdate": 1700614044836,
                "mdate": 1700614044836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "onk28kIpmn",
            "forum": "4WKDwIaF7y",
            "replyto": "4WKDwIaF7y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new version of multi-step SAM. They discuss convergence properties of the algorithm and present numerical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method seems to outperform SAM in the settings studied."
                },
                "weaknesses": {
                    "value": "- The idea of multi-step SAM is not new and has been explored before. The EG and OG versions proposed here are new, but I think the contribution of the work is marginal.\n\n- The theory is nice to have, but is not really insightful and the proofs are fairly standard.\n\n- I'm not fully convinced with the numerical experiments. They consider CIFAR + ResNet, and only one example of ImageNet + ResNet. No experiments on transformers or language models are done, which makes the experiments rather limited. Also, important version of SAM such as gSAM (https://arxiv.org/pdf/2203.08065.pdf) and mSAM (https://arxiv.org/pdf/2302.09693.pdf) are not included in the experiments. Also, I didn't find runtimes of methods, which I might've missed. \n \n- The presentation can improve. There are so many versions and abbreviations (EG,OG, AO, AMO) that it's difficult to see the points the authors are trying to make."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE",
                        "ICLR.cc/2024/Conference/Submission5815/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698199940924,
            "cdate": 1698199940924,
            "tmdate": 1700689445541,
            "mdate": 1700689445541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VFUiavztfp",
                "forum": "4WKDwIaF7y",
                "replyto": "onk28kIpmn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer gBpE"
                    },
                    "comment": {
                        "value": "Thank you for your all valuable suggestions. Below we respond to your key concerns point by point:\n\n---\n***Q1: The idea of multi-step SAM is not new and has been explored before. The EG and\nOG versions proposed here are new, but I think the contribution of the work is\nmarginal.***\n\ni) The reviewer might have some misunderstandings. Our EG-SAM significantly diverges from that of multi-step SAM methods like those in [1]: \n\n[1] aims to use multi-step SAM to yield a more precise solution of \nmax oracle $\\boldsymbol{\\epsilon}^* (\\boldsymbol{w}):= \\arg \\max_{\\boldsymbol{\\epsilon}:\\|\\boldsymbol{\\epsilon}\\| \\leq \\rho} L(\\boldsymbol{w}+\\boldsymbol{\\epsilon})$.  This method focuses on refining the maximization step within the SAM framework.\n\nIn contrast, our EG-SAM is primarily focused on avoiding saddle points and finding a better trajectory for convergence. This is achieved by incorporating a look-ahead mechanism. Unlike multi-step SAM (e.g., [1]), it is not designed specifically to refine the maximization step but rather to enhance overall convergence by avoiding saddle points.\n\nii) Regarding the contributions, \n1) we are the first to improve SAM by using look-ahead mechanism to avoid saddle points. \n\n2) We design different variants of look-ahead mechanism, e.g. EG-SAM, OG-SAM and AO-SAM, to help further reduce computational cost.  \n\n3) We provide theoretical guarantees. \n\n4) Our method outperforms other SOTA baselines consistently with different structures and in different datasets.\n\n---\n***Q2: The theory is nice to have, but is not really insightful and the proofs are fairly standard.***\n\ni) Regarding our proof is not really insightful,  we provide two distinct properties of EG-SAM: 1) EG-SAM can escape saddle points\nfaster than original SAM (Proposition 4.1), and 2) EG-SAM has more chance to converge at \nflatter minima (Proposition 4.2). These findings are novel and were not present in any previous work related to SAM. The ability to escape saddle points quickly and the increased probability of converging to flatter minima provide valuable insights into the advantages of our proposed methods.\n\nii) Regarding our proof is fairly standard, note that original SAM proof cannot be directly used to prove our proposed methods due to the following reasons:\n\n1) Recall in  OG-SAM, we have $\\hat{w_t}=w_{t-1}-\\eta_t g_{t-1}$, which includes past gradient $g_{t-1}$. This makes the standard SAM analysis (Andriushchenko \\& Flammarion, 2022) difficult to apply directly, as their analytical approach does not include a technique to address past gradients.\n\n2) In the proof of AO-SAM, we use an adaptive policy with OG (Jiang et al., 2023). Although (Jiang et al., 2023) provides a proof, it is only established for unnormalized SAM. Since ours is normalized SAM with OG, it inherits the difficulties of both the adaptive policy in normalized SAM and past gradient analysis.\n\n---\n***Q3: I'm not fully convinced with the numerical experiments. They consider CIFAR + ResNet, and only one example of ImageNet + ResNet. No experiments on transformers or language models are done, which makes the experiments rather limited. Also, important version of SAM such as gSAM  and mSAM are not included in the experiments. Also, I didn't find runtimes of methods, which I might've missed.***\n\n1) We added an experiment using vision transformer ViT-S16 on CIFAR-10 and CIFAR-100 in our response to Reviewer rtXV Q3. As can been seen, our proposed method also outperforms the baselines.\n\n2) We also added the performance comparison between GSAM [3], MSAM [4], and our AO-SAM\nin CIFAR-10 and CIFAR-100 with WideResNet-28-10 backbone, as suggested by the\nreviewer. From the below table, we find that \nour method also outperforms GSAM and MSAM.\n\n|     | CIFAR-10      | CIFAR-100   |\n|---------|-----------------|-----------------|\n| GSAM [3]| 97.42 \u00b1 0.03  | 84.48 \u00b1 0.06    |\n| MSAM [4]| 96.95 \u00b1 0.04  | 84.07 \u00b1 0.06   |\n| OG-SAM  | 97.49 \u00b1 0.02  | 84.80 \u00b1 0.11   |\n\n3)  We have also added the running time comparisons in our response to Reviewer 1P81's Q5. As can be seen, our proposed OG-SAM\nhas relatively same training time as SAM, while AO-SAM is faster than SAM.\n\n---\n***Q4: The presentation can improve. There are so many versions and abbreviations (EG, OG, AO, AMO) that it's difficult to see the points the authors are trying to make.***\n\nThank you for pointing out. we have included the full names alongside their abbreviations wherever necessary to enhance readability. \n\n---\nReference:\n\n[1] Kim, Hoki, et al. \"Exploring the effect of multi-step ascent in sharpness-aware minimization.\" arXiv preprint arXiv:2302.10181 (2023).\n\n[2] Dosovitskiy, Alexey, et al. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" ICLR. 2020.\n\n[3] Zhuang, Juntang, et al. \"Surrogate Gap Minimization Improves Sharpness-Aware Training.\" ICLR. 2021.\n\n[4] Behdin, Kayhan, et al. \"mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization.\" arXiv preprint arXiv:2302.09693 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458884918,
                "cdate": 1700458884918,
                "tmdate": 1700458884918,
                "mdate": 1700458884918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mVMJvMadpv",
                "forum": "4WKDwIaF7y",
                "replyto": "VFUiavztfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\n- The mentioned propositions are based on two strong assumptions: infinitesimal step sizes (ODE approximation) and quadratic loss, neither of which holds true in practice. Therefore, I don't think they theory can support the claims made.\n\n- I think the new experimental results look interesting, with the addition of these results to the paper, I increase my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689424983,
                "cdate": 1700689424983,
                "tmdate": 1700689424983,
                "mdate": 1700689424983,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gc1JP4j9Bv",
            "forum": "4WKDwIaF7y",
            "replyto": "4WKDwIaF7y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_hY3b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_hY3b"
            ],
            "content": {
                "summary": {
                    "value": "- The paper argues that sharpness-aware minimization can be stuck in saddle-points (of the loss function). \n- This motivates the EG-SAM scheme which modifies SAM by perturbing an *extrapolated* point (computed with the same gradient information as the perturbation). \n- Their alternative (optimistic) variant, OG-SAM, replaces the gradient in the extrapolation update with the gradient at the previous perturbed point. \n- They combine both methods with an existing adaptive policy (AE-SAM) and show favorable numerics on CIFAR10, CIFAR100 and ImageNet. \n- Theoretically, they provide a convergence result under gradient-Lipschitz and bounded variance for the average gradient norm with decreasing stepsize and perturbation radius $\\rho$.\n- They also show that an ODE related to EG-SAM has a smaller region of attraction then the SAM ODE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is easy to follow\n- It provides a good literature overview"
                },
                "weaknesses": {
                    "value": "I have a range of concerns starting with the formulation itself:\n\n- The EG-SAM update can be rewritten as:\n\n    $\\tilde w_t = w_{t-1} + (\\rho/\\|\\nabla L(w_{t-1})\\| - \\eta_t) \\nabla L(w_{t-1})$\n\n    $\\tilde w_t = w_{t-1} - \\eta_t \\nabla L(\\tilde w_{t})$\n\n    So when the stepsize is taken decreasing (as in the experiments) it is simply a scheme that perturbs less adversarially in the early training phase and more later. It would be instructive to plot the gradient norm at $w_{t-1}$ throughout training. How does EG-SAM compare against SAM with an (increasing) stepsize schedule for $\\rho$?\n- OG-SAM appears unnecessary since it does not save on gradient computations: EG-SAM simultaneous computes the extrapolation and perturbation computation so it already only requires two gradient computations. What is the motivation for OG-SAM in not saving gradient computations?\n\nTheoretically:\n\n- The convergence result in Section 4.2 requires strong assumptions on both the problem class and the a decreasing perturbation radius $\\rho$. Requiring both gradient-Lipschitz and function Lipschitz simultaneously is quite restrictive. More importantly, by decreasing $\\rho$ you essentially avoid the difficulty of the perturbation. One indication that your assumptions are too strong is that your argument would hold for any kind of (feasible) perturbation.\n    \n- I appreciate the attempt to contextualize the convergence result in Section 4.1 within the literature for extragradient (EG), but I find the comparisons quite misleading. Stating that their assumptions are restrictive whereas your assumption are more general is problematic. They are different problems entirely for multiple reasons:\n    - Different solution concepts: The EG literature treats convergence to a equilibrium, whereas the EG-SAM results only treats the gradient of the minimizing player.\n    - Different application: You apply EG to the minimization problem (not to treat the minimax)\n    The same issue appears in other sections when comparing rates:\n    - Section 3.1: even if assuming the loss in Eq. 1 is convex in $w$ then it is also *convex* in $\\varepsilon$, so the rate of $\\mathcal O(T^{-1/4})$ does not apply (we could still hope to be locally linear maybe for the max problem). The rates for SAM that are being compared against is for the gradient norm (so it is not considering convergence of the max-player).\n    - Section 3.2 mentions that the SAM+OG rate is much slower than SAM, but you are comparing both different problem settings (nonconvex-strongly concave vs nonconvex-nonconvex) and performance metrics.\n\n- The analysis in section 4.1 seems to follow directly from Compagnoni et al. 2023 Lemma 4.3: Eq. 19 is simply the SAM ODE (Eq. 18) with a different stepsize. More importantly, the extrapolation stepsize $\\eta$ in Eq. 19 should arguably not be fixed. Otherwise it corresponds to a discrete scheme which takes the extrapolation stepsize much larger than the actual stepsize.\n\nEmpirically: \n\n- I appreciate the effort of providing standard deviations for the experiments, but it is arguably even more important to sweep over $\\rho$ when comparing different SAM variants. The optimal $\\rho$ can differ significantly across models/datasets and can influence the ordering of the methods significantly.\n\n\nMinor:\n\n- $\\nabla F$ notation can be misleading since it is not a gradient. I suggest simply letting $F(z)=(\\nabla_x f(x,y), -\\nabla_y f(x,y))$\n- Maybe explicitly mention that the number of gradient oracles are 4 in Eq. 12 (instead of saying \"two approximated gradient steps\")\n- line 4 in Algorithm 3: specify what the AE-SAM update is\n\nTypos:\n\n- Eq. 9 and 10 has a missing stepsize for $\\varepsilon$\n- Second equation of section 3.2 has a missing hat and stepsize\n- Theorem 4.6 misses an index $t$ for $\\rho$"
                },
                "questions": {
                    "value": "- What is the value of $\\rho$ used for the different methods in the experiments?\n- Why is EG-SAM 150% SAM in Table 1?\n- Can you plot $\\|\\nabla L(w_{t-1})\\|$ throughout training? (or a mini-batch estimate)\n- Section 3.3 what is meant by \"has an approx solution [...] so one can directly apply extra-gradient\". Are you arguing based on Danskin's theorem? (the cited reference doesn't seem to mention the same problem)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698328837329,
            "cdate": 1698328837329,
            "tmdate": 1699636613535,
            "mdate": 1699636613535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QI3Wpg9VAM",
                "forum": "4WKDwIaF7y",
                "replyto": "Gc1JP4j9Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer hY3b (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer\u2019s helpful feedback on our submission. We summarize the reviewer\u2019s questions and the following is our responses:\n\n---\n\n***Q1: \nThe EG-SAM update can be rewritten as:\n... When the stepsize is taken decreasing (as in the experiments) it is simply a scheme that perturbs less adversarially in the early training phase and more later.***\n\n\nOur scheme is designed based on some non-trivial insights:\n\ni) This scheme is based on incorporating the lookahead mechanism into SAM, which has not been proposed previously.\n\nii) The scheme is also explainable. By suppressing perturbation during the early stages, our method is more effective at avoiding saddle points than the original SAM. This is because larger perturbations make the model more susceptible to being trapped in saddle points, as noted by Kim et al. (2023). Additionally, by allowing more perturbation in the final stages, our method encourages the model to find flat minima.\n\n---\n\n***Q2: It would be instructive to plot the gradient norm at $w_t$ \n throughout training.*** \n\nWe added gradient norm at $\\nabla L(w_t)$ in Figure 5, Sec. B, as suggested by the reviewer. As can be seen by the figure, the gradient of EG-SAM is larger than that of SAM in the early stages, but it converges to the same level as SAM in the final stage.\n\n---\n\n***Q3: How does EG-SAM compare against SAM with an (increasing) stepsize schedule for $\\rho$?***\n\nWe added an extra experiment regarding using increasing stepsize schedule for $\\rho$, i.e., $\\rho_t = \\rho_0+  \\frac{t(\\rho_{T}- \\rho_0)}{T}$, where $\\rho_{T}$ is the final value\n$\\rho_{T}$ (1.0 in our experiment), while $\\rho_0$ is the initial value of $\\rho$ (0.1 in our experiment).\nAs can been from the table, our method with increasing $\\rho$ still outperforms the SAM, and is comparable to original EG-SAM, while SAM with increasing $\\rho$ perform worse than original SAM method.\n\n\n\n|                              | CIFAR-10 with ResNet-18     |\n|------------------------------|-----------------------------|\n| SAM (Foret et al., 2021)     | 96.52 \u00b1 0.12                |\n| SAM increasing $\\rho$     | 94.67 \u00b1 0.03                |\n| EG-SAM                       | 96.86 \u00b1 0.01                |\n| EG-SAM-increasing $\\rho$     | 96.79 \u00b1 0.06                |\n\n\n---\n\n***Q4: OG-SAM appears unnecessary since it does not save on gradient computations: EG-SAM simultaneous computes the extrapolation and perturbation computation so it already only requires two gradient computations. What is the motivation for OG-SAM in not saving gradient computations?***\n  \nIn EG-SAM, we need to compute three variables at each step: $\\hat{w}$, $\\hat{\\epsilon}$, and $w$. In contrast, standard SAM and OG-SAM require only $\\hat{\\epsilon}$ and $w$ per step (OG-SAM reuses the gradient of  $\\hat{w}$), and thus we are saving gradients.\nBesides this computational efficiency, another advantage of OG-SAM is that it contains the past gradient \n$\\boldsymbol{g}_{t-1}$ at current step $t$. As discussed in Sec. 3.4, by leveraging extra information from the past gradient, it improves the performance (Rakhlin \\& Sridharan, 2013).\n\n---\n***Q5: The convergence result in Section 4.2 requires strong assumptions on both the i) problem class and ii) the a decreasing perturbation radius \n$\\rho$. ...\nMore importantly, by decreasing \n$\\rho$ you essentially avoid the difficulty of the perturbation. One indication that your assumptions are too strong is that your argument would hold for any kind of (feasible) perturbation.***\n\n\ni) Regarding the problem class assumptions, note that we do not introduce the function Lipschitz assumption. Our three assumptions are bounded variance, smoothness (gradient Lipschitz), and uniformly bounded gradient (Assumptions 4.3-4.5). These assumptions are widely used in the analysis of SAM and its variants (Mi et al., 2022; Dai et al.,\n2023; Zhang et al., 2023; Yue et al., 2023). Beyond SAM, these assumptions are utilized in analyzing different optimizers [3].\n\n\n\nii) Regarding decreasing $\\rho$, a recent paper [1] theoretically reveals that \nSAM with constant $\\rho$ CANNOT converge to stationary points. Therefore, assuming a decreasing \n$\\rho$ is necessary in theoretical analysis. Moreover, related works with decreasing $\\rho$ assumptions under stochastic gradient settings  are also found in related works (Mi et al., 2022; Dai et al.,\n2023; Zhang et al., 2023; Yue et al., 2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458728494,
                "cdate": 1700458728494,
                "tmdate": 1700458728494,
                "mdate": 1700458728494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ax73J1B9ma",
                "forum": "4WKDwIaF7y",
                "replyto": "VtdlFlZ2o5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_hY3b"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_hY3b"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for the thorough response. However, I don't find that it addresses my primary concerns, so I choose to maintain my score. In order to highlight what some of those are:\n\n\n- **Q4, Q12** I believe there is a big misunderstand concerning what is the computational bottleneck in SAM. The authors seems to count how many variables they maintain, but what matter computationally is the number of backpropagations. EG-SAM and OG-SAM seems to have the same number of backpropagations per iterations, in which case the motivation of OG-SAM remains unclear to me.\n    \n- _**Q3**_ For the (new) increasing $\\rho$ baseline, it seem like the choice of $\\rho$  is different from the other methods? The intention was to see whether increasing $\\rho$ (which can become *identical* to EG-SAM appropriate stepsize schedule) can perform similarly, so you need to at least pick $\\rho$ similarly.\n\n- _**Q5**_ You *do* implicitly assume function Lipschitz by assuming bounded variance. \n\n- _**Q6**_ I seems that the authors are misunderstanding my concern. The rates that the authors mentions concerning EG do not apply to their problem (which is *convex* in the max-player and not (strongly)-concave as otherwise assumed) and vice versa. I would be more careful with drawing parallels to classical minimax optimization as this can be misleading. \n\n- _**Q9**_ What is the choice of $\\rho$ of *your* method? You seems to search over a much more refined grid (0.01, 0.05, 0.08, 0.1, 0.5, 0.8, 1, 1.5, 1.8, 2), possibly explaining the difference in performance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559297459,
                "cdate": 1700559297459,
                "tmdate": 1700559297459,
                "mdate": 1700559297459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CClcC0zHro",
            "forum": "4WKDwIaF7y",
            "replyto": "4WKDwIaF7y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_rtXV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5815/Reviewer_rtXV"
            ],
            "content": {
                "summary": {
                    "value": "The paper tries to incorporate look-ahead mechanism into SAM to improve the performance of SAM. The authors proposed 3 methods: SAM + EG, SAM + OG and AO + SAM, in which the first one used extra-gradient information, the second method proposed to use optimistic-gradient and the last one used an adaptive gradient. The authors provided convergence analysis of those methods by proving that their convergence rates are similar to that of SAM. They also did some empirical works to verify the convergence rate, accuracies of their methods in CIFAR-10, CIFAR-100 and ImageNet using several network structures like ResNet-32, Resnet-18, WideResNet28-10."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well presented, clear structure. \nThe authors provides theoretical analysis for their methods. I did not check all the technical details, but I have a random check, it seems to be fine. \nThe experimental results have shown the improvement of the method over SAM. Even though the improvement is only around $1\\%$, but it is shown consistently over several network structures."
                },
                "weaknesses": {
                    "value": "Those proposed methods are only some variations of SAM with small changes.\nHence, the authors did not encounter further difficulties to obtain those convergence rate results.\nThe experiment results show  incremental enhancement  to SAM.  \nIn comparison with SAM paper, the authors had done less experiments for comparisons, i.e.  ResNet-101, ResNet-152 etc\nThere is also a little intuitive explanation/proof for advantages of the proposed methods. \nIt is decent work, not something fancy to read."
                },
                "questions": {
                    "value": "No question"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804622037,
            "cdate": 1698804622037,
            "tmdate": 1699636613439,
            "mdate": 1699636613439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pZEodAR7z9",
                "forum": "4WKDwIaF7y",
                "replyto": "CClcC0zHro",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer rtXV"
                    },
                    "comment": {
                        "value": "We much appreciate your positive and insightful comments on our paper. Here is our response to your questions point by point:\n\n---\n\n***Q1: Those proposed methods are only some variations of SAM with small changes. Hence, the authors did not encounter further difficulties to obtain those convergence rate results.***\n\ni) Our proposed methods include the following contributions:\n1) we improve SAM by using look-ahead mechanism to avoid saddle points. \n2) We design different variants of the look-ahead mechanism, e.g., EG-SAM, OG-SAM, and AO-SAM, to further reduce computational cost.\n3) We provide theoretical analysis.\nExperiments in Sec. 5 reveal that these modifications all help improve the performance of SAM.\n \n\n\nii) Adding a look-ahead mechanism introduces new difficulties to the proof:\n\n\n1) Recall in  OG-SAM, we have $\\hat{w_t}=w_{t-1}-\\eta_t g_{t-1}$, which includes past gradient $g_{t-1}$. This makes the standard SAM analysis (Andriushchenko \\& Flammarion, 2022) difficult to apply directly, as their analytical approach does not include a technique to address past gradients.\n\n2) In the proof of AO-SAM, we use an adaptive policy (Jiang et al., 2023) with OG. Although (Jiang et al., 2023) provides a proof, it is only established for unnormalized SAM. Since ours is normalized SAM with OG, it inherits the difficulties of both the adaptive policy and past gradient analysis.\n\n---\n***Q2: The experiment results show incremental enhancement to SAM.***\n\ni) Regarding the experimental results, while our method does not exhibit a\ndramatic improvement over the SOTA baselines, it consistently outperforms them\nacross various environments and with different backbone models. Additionally, the\nbaselines represent the current best-performing methods in the field. Therefore, it is common for the observed improvements to be relatively modest (Jiang et al.,\n2023; Du et al., 2022a; Mi et al., 2022). \n\nii) To further demonstrate that our improvement is NOT merely incremental, we have added a test of significance (T-test) on the noise-labeled CIFAR-100 dataset (Table 6). Our method shows statistical significance with a p-value less than 0.05. This supports our claim that our improvement is statistically significant, particularly for the noise-labeled dataset, which requires a higher degree of generalization ability.\n\n---\n\n***Q3: In comparison with SAM paper, the authors had done less experiments for comparisons, i.e. ResNet-101, ResNet-152 etc.***\n\nDue to resource limitations, we are sorry that our experiments did not include ResNet-101 and ResNet-152 on ImageNet. \n\nAs an alternative, we incorporated an extra experiment with a large neural network, Vision Transformer ViT-S16 [1], on the CIFAR-10 and CIFAR-100 dataset. The results from this experiment demonstrate that our method  outperform other baselines continually.\n\n\n\n|       | CIFAR-10 with ViT-S16 | CIFAR-100 with ViT-S16 |\n|-------|-----------------------|------------------------|\n| SAM (Foret et al., 2021)  | 87.37 \u00b1 0.09          | 63.23 \u00b1 0.25           |\n| ESAM (Du et al., 2022a) | 84.27 \u00b1 0.11          | 62.11 \u00b1 0.15           |\n| OG-SAM| 88.27 \u00b1 0.12          | 64.45 \u00b1 0.23           |\n\n\n---\n\n***Q4: There is also a little intuitive explanation/proof for advantages of the proposed methods. It is decent work, not something fancy to read.***\n\ni) As discussed in Section 1, the introduction of the extra gradient in our model allows for gathering more information about the landscape by looking further ahead. This facilitates finding a better trajectory for convergence, which is the motivation for incorporating the extra-gradient into SAM. \n\n\nii) Additionally, as also pointed out by Reviewer hY3b, the EG-SAM update scheme can be written as\n$w_t =w_{t-1}-\\eta_t \\nabla L\\left( w_{t-1}+\\epsilon^\\prime_t\\right)$ where\n$\\epsilon^\\prime_t = (\\rho /||\\nabla L(w_{t-1})||-\\eta_t)\\nabla L(w_{t-1})$.\nIn comparison, SAM's update scheme is \n$w_t =w_{t-1}-\\eta_t \\nabla L\\left( w_{t-1}+\\epsilon_t\\right)$, where\n$\\epsilon_t = (\\rho /||\\nabla L(w_{t-1})||)\\nabla L(w_{t-1})$. EG-SAM can thus be interpreted as it suppresses perturbation at the early stage (as $\\epsilon_t$ decreases from large to small values), and allows more perturbation in the final stage. Since larger perturbations make the model more prone to being trapped in saddle points (Kim et al., 2023), our method more effectively avoids these points compared to SAM by reducing perturbation. Furthermore, the increased perturbation in the final stage enables the model to find flat minima.\n\n---\n\nReferences:\n\n[1] Dosovitskiy, Alexey, et al. \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" ICLR. 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458630721,
                "cdate": 1700458630721,
                "tmdate": 1700458630721,
                "mdate": 1700458630721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JByG0622Jh",
                "forum": "4WKDwIaF7y",
                "replyto": "pZEodAR7z9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_rtXV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_rtXV"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their responses. I keep my initial score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727999884,
                "cdate": 1700727999884,
                "tmdate": 1700727999884,
                "mdate": 1700727999884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]