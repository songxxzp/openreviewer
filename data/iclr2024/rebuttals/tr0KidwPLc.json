[
    {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following"
    },
    {
        "review": {
            "id": "HfRg1N5wVh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
            ],
            "forum": "tr0KidwPLc",
            "replyto": "tr0KidwPLc",
            "content": {
                "summary": {
                    "value": "This paper proposes the LLMBar, aiming to evaluate if large language models (LLMs) can serve as an evaluator of the LLMs' instruction-following ability. LLMBar consists of two instruction sets, one collected from other benchmarks that are easy for LLM to identify and another generated with different strategies, incorporating sentence similarity, LLMs, etc, which is hard for LLM to identify. To further improve the ability to evaluate the instruction-following ability, the authors also propose a new prompting strategy by introducing a metric set generated by LLM itself to assist the evaluation. The benchmark experiments are performed on human-level and common open-sourced and proprietary LLMs with various prompting strategies, revealing a distinct ability between different LLMs and showing a significant gap between LLMs and human evaluators on the difficult dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The soundness of this paper is good. Instruction-following ability is an important ability of LLMs that has not been well-studied. This paper fills the gap in this area by introducing a manually constructed instruction dataset supervised by human annotators.\n2. The experiment results are from various common LLMs and prompting strategies, which are enough for understanding if a LLM is a good evaluator.\n3. This paper is well-written, easy to follow, and will have a wild interest in the LLM community."
                },
                "weaknesses": {
                    "value": "1. The instructions used for dataset construction contain ambiguous words. For example, the word \"imaginative\" will have different explanations from different people or LLMs in different aspects; such examples should also be deleted from the dataset.\n2. The authors use different prompting strategies for evaluation, but the dataset generation only uses plain prompts. It is interesting yet important to discover how different input instructions enhanced by CoT or other prompting strategies (i.e., $(I_{\\text{enhanced}}, O_1, O_2, p)$) affect the judgment of different LLMs.\n3. The proposed prompting strategy is not as good as the authors claim (there is no significant improvement on GPT4 and LLaMA2, from my point of view, and the output consistency is also not good enough when changing the order of inputs). I think the authors should rephrase the description of the proposed prompting strategies from different perspectives."
                },
                "questions": {
                    "value": "I found a large performance variance on Base-9 and Base-10. I think this is because the generated metrics largely affect the LLMs' performance. Could authors provide the generated metrics by different LLMs? \n\nAlso, the performance of the proposed method on GPT-3.5 and GPT-4 does not seem robust on the GPTOut, I'm curious about whether there exists some bias (or preference) of the generated metrics that affects the performance on different evaluation sets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697455493237,
            "cdate": 1697455493237,
            "tmdate": 1699636826802,
            "mdate": 1699636826802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KY4M22VJop",
                "forum": "tr0KidwPLc",
                "replyto": "HfRg1N5wVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 65gL [1/3]"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback! We agree that a strong capability in evaluating instruction-following is crucial for any LLM to serve as a reliable evaluator, and LLMBar bridges this research gap by investigating it. We believe that LLMBar, by offering high-quality and efficient meta-evaluation, will significantly contribute to the development of LLM evaluators. We will address your comments and suggestions in more detail below.\n\n**(1) The instructions used for dataset construction contain ambiguous words.**\n\nThanks for raising this point! We acknowledge that some usage of words in the Natural set may seem ambiguous. It is intended as we sample from existing datasets to construct the Natural set and try to keep a real-world distribution. However, we made every effort to make sure that in the provided output pairs, one is objectively better than the other without ambiguity. Taking the \u201cchicken in the library\u201d instance in Appendix A.1 as an example, Output 1 is objectively better than Output 2 as Output 2 is not \u201cimaginative\u201d at all. Such objectivity in preferences is also reflected by the 90% human agreement rate of our Natural set.\n\n**(2) The authors use different prompting strategies for evaluation, but the dataset generation only uses plain prompts.**\n\nIf you are suggesting using more sophisticated prompting for dataset generation, especially for the GPTInst subset (as it can change the distribution of instances\u2019 instructions), we appreciate your suggestion and will explore this direction in future work to increase the diversity of our dataset.\n\n**(3) It is interesting yet important to discover how different input instructions enhanced by CoT or other prompting strategies affect the judgment of different LLMs**\n\nReflecting on your proposed direction, we think this setup (of how instructions are like) is both practical and intriguing: the user's instruction includes \"metrics\" giving what a good or expected output should entail. As you mentioned, understanding how this kind of instructions influences LLM evaluators is important and interesting.\n\nGiven the labor-intensive nature of manually creating such instructions, we propose utilizing LLMs to do that. This practice actually translates into the following prompting strategy: we first generate a list of metrics and then incorporate these metrics into the instruction to create an enhanced instruction $I_\\text{enhanced}$. This enhanced instruction, along with the pair of outputs, is then fed into an LLM evaluator (we could use the basic **Vanilla*** prompting strategy here as the simplest version). We refer to this strategy as **Enhanced*** and evaluated it on LLMBar using GPT-4 and ChatGPT respectively. The results of the average accuracies are as follows:\n\nGPT-4:\n|      Strategy      | Natural | Neighbor | GPTInst | GPTOut | Manual |\n| :----------------: | :-----: | :------: | :-----: | :----: | :----: |\n|      Vanilla*      |  95.5   |   78.7   |  86.4   |  77.7  |  80.4  |\n|      Metrics*      |  93.0   |   83.2   |  89.7   |  73.4  |  81.5  |\n|   **Enhanced***    |  92.0   |   81.7   |  88.6   |  71.3  |  77.2  |\n| Metrics+Reference* |  96.0   |   85.4   |  89.7   |  72.3  |  83.7  |\n\nChatGPT:\n|      Strategy      | Natural | Neighbor | GPTInst | GPTOut | Manual |\n| :----------------: | :-----: | :------: | :-----: | :----: | :----: |\n|      Vanilla*      |  81.5   |   19.4   |  26.6   |  41.5  |  34.8  |\n|      Metrics*      |  81.5   |   28.4   |  35.9   |  41.5  |  43.5  |\n|   **Enhanced***    |  81.5   |   33.2   |  29.3   |  42.6  |  43.5  |\n| Metrics+Reference* |  82.5   |   38.1   |  35.9   |  38.3  |  43.5  |\n\nThe results indicate that **Enhanced*** significantly outperforms **Vanilla*** when using ChatGPT as the base LLM. This observation leads to two implications: (1) this strategy appears to effectively assist LLM evaluators, and (2) considering two meta-evaluation instances, even when their instructions convey similar intentions, variations in wording can significantly impact the performance of LLM evaluators. We believe these are both interesting directions for future research.\n\nNote: **Enhanced*** is quite similar to **Metrics***. The difference is that the LLM evaluator now regards the generated metrics as a part of the instance's instruction, instead of as a part of the evaluator's prompt (the prompt instructing LLM evaluators to do evaluation). This can be seen as a way to influence the distribution of instances' instructions. Therefore, it serves as both a strategy and an analysis experiment."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187510155,
                "cdate": 1700187510155,
                "tmdate": 1700187535030,
                "mdate": 1700187535030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HwTSIXlVZK",
                "forum": "tr0KidwPLc",
                "replyto": "R11Lsqj2ms",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the author's hard work in providing further details of extra experiments, evaluation process, and prompting case. My questions are well addressed and I will keep my score unchanged on this good paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547173438,
                "cdate": 1700547173438,
                "tmdate": 1700547173438,
                "mdate": 1700547173438,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rq2t6ZZbjQ",
            "forum": "tr0KidwPLc",
            "replyto": "tr0KidwPLc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7039/Reviewer_WsKe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7039/Reviewer_WsKe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a challenge meta-evaluator benchmark, LLMBar, used to assess the quality of the LLM-evaluator (LLM + prompt strategies) for instruction following. In addition, the paper provides empirical experiment result on various combination of the LLM model (open-sourced and non-open) and prompt strategies and shows improvement for a novel suite of prompt strategies"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength: \n* The paper is overall well-written, easy to follow \n* The paper addresses an important current problem of scalable evaluation of the LLM-evaluator\u2019s quality"
                },
                "weaknesses": {
                    "value": "Weakness\n* There is some confusion in how the evaluation set was generated, for example, in Figure 2. \u201cWe often use weaker models to generate O1 and stronger model to generate O2 \u2026\u201d  it is unclear what is weak and strong models are referring. I tried to search that definition in the text on page 4 \u201cNeighbor Instruction\u201d, but still couldn\u2019t find any. \n* Similarly, point 2 on page 4, not sure what \u201cmanual filtering and modification\u201d entail \n* The human evaluators are co-authors of the papers, which can raise issues of objectivity compared with other crowd-sourced human evaluators. What measures are in place to prevent the bias?\n* I am wondering whether it is a fair comparison to other benchmarks if the instructions are qualitatively different. For example, it is relatively easy to achieve high inter-rater reliability if the response is factual information rather than subjective\n* The experiment result part spent the majority of the session on the performance of the LLM evaluator on the newly proposed benchmark, while only a small section on comparing with other benchmarks. Since the benchmarks are qualitatively different (objective vs subjective), I am not sure whether the result is meaningful. It would be ideal to have another benchmark with comparable features (e.g., objectiveness and similar inter-rater agreement)"
                },
                "questions": {
                    "value": "Minor issues to be fixed, not factored in evaluation \n* Figure 5, end of the sentence, typo evaluators\u2019 capabilities \n* Please state the metrics used for calculating inter-rater reliability, e.g. Cohen Kappa etc. making sure it is consistent with other papers to be comparable"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Reviewer_WsKe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760582809,
            "cdate": 1698760582809,
            "tmdate": 1699636826677,
            "mdate": 1699636826677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0SOUikb26h",
                "forum": "tr0KidwPLc",
                "replyto": "Rq2t6ZZbjQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WsKe [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback! We are encouraged that you recognize LLMBar as resolving a major missing piece in the high-quality, scalable meta-evaluation during the current surge of LLM evaluator usage. We appreciate your detailed comments and suggestions and will address them below in detail.\n\n**(1) Some confusion in how the evaluation set was generated.**\n\nRegarding \u201c$O_1$ is generated by weaker models and $O_2$ by stronger models\u201d, this is employed mainly for the collection of the Neighbor set. We had provided the details in footnote 5 (in the old version, now deleted) : 'We generate $O_1$ using an instruction-tuned LLaMA-7B, and $O_2$ is taken as the reference output from original datasets, which are generated by text-davinci-003 in Alpaca, by humans in OpenAssistant, and by ChatGPT in ShareGPT'. We appreciate your raised point and acknowledge that including this detail in a footnote may have caused confusion for readers. In the updated version, we have moved this detail into the main text of Section 2.2 for better clarity.\n\n**(2) Not sure what \u201cmanual filtering and modification\u201d entail.**\n\nThis term refers to the final stage in constructing both the Natural and Adversarial sets. In this process, each instance is carefully examined by the authors. If there is no objective preference between two outputs, or if the label is incorrect, we will then modify the instance accordingly or discard it if making such modifications is difficult. We have provided a more detailed explanation in the updated version of Section 2.1. For specific examples of this process, please check out Appendix A.1, A.2, and A.4.\n\n**(3) The human evaluators are co-authors of the papers, which can raise issues of objectivity compared with other crowd-sourced human evaluators.**\n\nWe would like to clarify the procedure for conducting the human evaluation: For each instance, one author (annotator A) annotates the label, and another author (annotator B) annotates the instance again, without seeing the previous label. The annotator agreement rate is calculated as the percentage at which annotator A/B agree, which is also the human accuracy. To remain objective, the selected annotator B(s) have not seen the constructed data or annotator A\u2019s labels before.\n\nWe believe that this is a strength rather than a weakness. Our annotation task is objective by design and has very clear task definitions. The authors of the paper best understand the task instructions, and therefore, are best positioned to provide high-quality annotations. On the other hand, crowdworkers recruited through platforms like MTurk are more likely to introduce noise, such as misunderstanding task instructions. Due to this exact reason, several works in the past refer to author-annotated data as the \u201cexpert\u201d test set, contrasting it with crowdsourced data that is assumed to be of lower quality but easier to collect at scale (Goyal et al., 2022).\n\nIf the goal was to collect a diverse set of instructions or queries that reflect real-world usage distributions, then recruiting a diverse set of crowdworkers would be critical. However, in our setting, letting authors do the annotation and human evaluation is the best approach."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186893696,
                "cdate": 1700186893696,
                "tmdate": 1700272021338,
                "mdate": 1700272021338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ei5PGcHvbr",
                "forum": "tr0KidwPLc",
                "replyto": "Rq2t6ZZbjQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WsKe [2/2]"
                    },
                    "comment": {
                        "value": "**(4) I am wondering whether it is a fair comparison to other benchmarks if the instructions are qualitatively different. For example, it is relatively easy to achieve high inter-rater reliability if the response is factual information rather than subjective.**\n\nWe agree that LLMBar is qualitatively different from other benchmarks, and is designed to focus on one particular aspect of evaluation, i.e., instruction-following. We believe that it fills a critical gap in prior work. While studying previous meta-evaluation benchmarks (FairEval, LLMEval^2, MT-Bench), we observed that they cannot distinguish between different evaluators (see Figure 5) and fail to provide recommendations into which LLM evaluators are strongest. We hypothesize that one reason was that prior benchmarks cannot disentangle the effects of subjectivity and noise in their annotations, i.e., is the disagreement between annotators due to annotation errors/noise, or is it because the choice is inherently subjective? Therefore, they end up scoring most evaluators similarly. \n\nOur LLMBar benchmark addresses this gap in prior work by providing an **objective** (and also challenging) benchmark for comparing different evaluators. We intended to show such differences via our experiments and analysis. We show that\n+ LLMBar is more objective and the annotation quality is higher, hence it achieves higher human agreement as shown in Section 4.2. The objectivity and high-quality annotation make our benchmark more reliable and indicative.\n+ LLMBar is more challenging and distinguishes different evaluators better. This is demonstrated by Figure 5, where different evaluators perform similarly on other benchmarks but distinctly on LLMBar.\n\nWe argue that our experiments deliver a fair comparison, as the goal is to show how challenging, objective, and high-quality our benchmark is compared to other benchmarks via the experiments. In the future, we hope that LLMBar can serve as a \u201cstress test\u201d to compare different evaluators, possibly in addition to other aspects (other than instruction following) that practitioners want to evaluate.\n\n**(5) It would be ideal to have another benchmark with comparable features (e.g., objectiveness and similar inter-rater agreement)**\n\nWe believe that LLMBar is the **first** meta-evaluation benchmark that ensures objectivity and achieves an inter-annotator agreement of more than 90%. We focused on instruction following due to its importance in using LLMs. We hope that future work will build on the ideas in this work and introduce other objective benchmarks, possibly relating to other aspects apart from instruction following.\n\n**(6) Please state the metrics used for calculating inter-rater reliability, e.g. Cohen Kappa etc. making sure it is consistent with other papers to be comparable.**\n\nOur human agreement/accuracy is calculated as the percentage at which the two annotators agree with each other. In Section 4.2, we refer to the average human accuracy in FairEval and the agreement in MT-Bench is the average agreement among each pair of two annotators, so our calculation is mathematically equivalent to those of other papers we compared.\n\n**(7) Typo in the caption of Figure 5**\n\nThanks for pointing it out! We have fixed it in the updated version.\n\n## Reference\n\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. SNaC: Coherence Error Detection for Narrative Summarization. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 444\u2013463, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186946124,
                "cdate": 1700186946124,
                "tmdate": 1700188501532,
                "mdate": 1700188501532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NnBZ6u3hKj",
                "forum": "tr0KidwPLc",
                "replyto": "Rq2t6ZZbjQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder of the Discussion Period Deadline"
                    },
                    "comment": {
                        "value": "Dear Reviewer WsKe,\n\nWe hope this message finds you well. As we're approaching the last day of the reviewer-author discussion period, we wanted to gently follow up regarding our previous response to your review. We value your input highly and would greatly appreciate it if you could let us know whether our rebuttal has addressed your initial concerns, which were primarily clarification questions. Your feedback is crucial for us, and we remain open to further discussion if needed.\n\nThank you once again for your insightful review and the time you have invested in it.\n\nBest regards,\n\nAuthors 7039"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547444849,
                "cdate": 1700547444849,
                "tmdate": 1700547943419,
                "mdate": 1700547943419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qXXjPXSltK",
            "forum": "tr0KidwPLc",
            "replyto": "tr0KidwPLc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7039/Reviewer_kxW7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7039/Reviewer_kxW7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a challenging meta-evaluation benchmark, consisting of 100 natural and 319 adversarial <instruction, preferred output, non-preferred output> triplets, for evaluating instruction-following capabilities of LLMs. 100 natural instructions are carefully chosen from the existing benchmarks, AlpacaFarm and LLMEval. Adversarial set consists of 319 samples collected through 4 different strategies. 1) Uses neighbor search within the dataset to select a similar instruction, and then uses a strong LLM on the retrieved instruction to generate the non-preferred output. 2) Uses GPT-4 to generate a variant of instruction which is relevant but not similar and then uses the new instruction to generate the non-preferred output. 3) Uses GPT-4 to produce a superficially good but unhelpful response (non-preferred output). 4) Manual construction. The new dataset has very high inter-annotator agreement.\n\nOn the adversarial set they found that ChatGPT- and LLAMA-2-70B-based evaluators perform worse than the random chance. So, authors also propose new prompting strategies: Rules (a list of general rules to follow), Metrics (prompt LLM to generate a set of instruction-specific metrics and use them to evaluate the outputs) and Swap (generate scores for both the outputs orders o1,o2 and o2,o1; use LLM to generate final score by using combinations of both responses if they are contradictory). Their new prompting strategies improve the LLM-based evaluator's performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A possibly useful benchmark for evaluating instruction-following capabilities of LLM.\n\n- Useful insights on the capabilities of LLM-based evaluators (e.g., Llama-based and Chat-GPT-based evaluators incompetency in evaluating instruction-following capabilities of LLMs.)\n\n- Proposed prompting strategies improve evaluator's performance."
                },
                "weaknesses": {
                    "value": "Are LLMs likely to sample non-preferred output for instructions in the adversarial pool? Could you provide the distribution of generating preferred and non-preferred outputs for a few LLMs? It is unclear, whether the findings of the paper (or to say, improving performance of evaluator-LLMs on adversarial samples) would actually lead to improved reliability of LLM-based evaluations."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7039/Reviewer_kxW7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812155865,
            "cdate": 1698812155865,
            "tmdate": 1699636826552,
            "mdate": 1699636826552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WPuHPUjpD6",
                "forum": "tr0KidwPLc",
                "replyto": "qXXjPXSltK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kxW7"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback! As LLM evaluators become more commonly adopted, we are encouraged by your appreciation of the contributions that LLMBar and our prompting strategies provide to the research community. We believe LLMBar will contribute to standardized meta-evaluation for future LLM evaluators.\n\nTo address your concern **Are LLMs likely to sample non-preferred output for instructions in the adversarial pool? Could you provide the distribution of generating preferred and non-preferred outputs for a few LLMs?**:\n+ We randomly sampled 50 instructions from Adversarial and found that ChatGPT (`gpt-3.5-turbo-0613`), LLaMA-2-70B-Chat, and PaLM2 (`text-bison-001`) can faithfully follow 90%, 86%, and 84% of them respectively by human evaluation.\n+ Even though the LLM can generate instruction-following outputs, it does not mean the LLM can discern instruction-following outputs. This is shown in our results and echoed by West et al., 2023, which shows that being able to generate does not entail being able to understand. In this work, we focus on studying whether the LLM can distinguish instruction-following outputs from dispreferred outputs. The adversarial instances are constructed such that it is challenging for LLM evaluators to identify the instruction-following outputs, rather than being deliberately hard to generate based on the instruction. We have added this clarification in Section 2.2 of our updated version. Due to the page limit, we plan to discuss this point to provide more insights in our next version.\n+ Even GPT-4 is likely to generate outputs not following simple instructions (Wu et al., 2023; Li et al., 2023). This is a key motivation for our focus on evaluating whether LLM evaluators can detect instruction-following outputs, an effort that can facilitate future development of instruction-following models. For example, in the Neighbor set, there is an instance asking for **methods of learning** search engine optimization (SEO). The dispreferred output deviates by providing **specific ways of doing SEO**. We observed that GPT-4, when prompted with this instruction, tends to produce outputs similar to the dispreferred one.\n\nIn response to **whether the findings will improve LLM evaluators' ability**: Though LLMBar\u2019s Adversarial set has a distinct distribution, we believe that an improvement on it indicates a more reliable LLM evaluator in the real-world distribution \u2013 being able to distinguish instruction following from other superficial appeal should be a basic requirement for LLM evaluators, and failing to do so makes the evaluator misleading. However, we acknowledge the distribution shift and that the Adversarial set is more like a \u201cstress test\u201d, hence we also provide the Natural set as a reference. We see that the performance trends on the Natural set generally agree with that on the Adversarial set but with a smaller margin.\n\n## Reference\n\nWu, Zhaofeng, et al. \"Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.\" *arXiv preprint arXiv:2307.02477* (2023).\n\nLi, Shiyang, et al. \"Instruction-following evaluation through verbalizer manipulation.\" *arXiv preprint arXiv:2307.10558* (2023).\n\nWest, Peter, et al. \"The Generative AI Paradox:\" What It Can Create, It May Not Understand\".\" *arXiv preprint arXiv:2311.00059* (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186082155,
                "cdate": 1700186082155,
                "tmdate": 1700187869519,
                "mdate": 1700187869519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]