[
    {
        "title": "AdaFlood: Adaptive Flood Regularization"
    },
    {
        "review": {
            "id": "XwfN1ZNR7l",
            "forum": "70A6oo3Il2",
            "replyto": "70A6oo3Il2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7123/Reviewer_vFax"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7123/Reviewer_vFax"
            ],
            "content": {
                "summary": {
                    "value": "The author studies the flood regularization technique for relieving the overfitting of the deep neural network training and proposes an adaptive flood regularization technique, i.e., provides instance(sample)-wise adaptive weighting (flooding level) for calculating the flooding loss. This instance-wise weighting is determined by the auxiliary networks which are trained by subsets of the whole training dataset with five-fold cross-validation, and these auxiliary networks are fixed only to provide an estimation of the sample's difficulty, i.e., the loss between the auxiliary models' prediction and the input's ground-truth label. To mitigate the unavoidable imperfectness of the auxiliary model, the author also proposed a correction function to modify the prediction of the auxiliary model, i.e., linearly interpolates the predictions and the ground-truth labels. The author also provides ways to improve the training efficiency of the auxiliary networks by fine-tuning the last few layers of the auxiliary model such that they can use only one auxiliary model instead of multiple ones. The author also provides a theoretical intuition about the benefit of using adaptive flooding. Extensive experiments are conducted over text, images, asynchronous event sequences, and tabular data to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written, the idea of the present paper is reasonable, and the proposed technique is easy to follow. \n2. The proposed method is simple to implement.\n3. The benchmarking of the present paper is extensive."
                },
                "weaknesses": {
                    "value": "1. The limited novelty of the proposed paper. \n\nThe idea of using adaptive weighting for model training is widely used in existing machine learning areas, the more relevant ones can be curriculum learning and uncertainty learning. Moreover, as mentioned by the author in Related Work, using a held-out dataset to condition the training of the primary model is a well-known strategy in machine learning, e.g., in meta-learning, batch or data selection, neural architecture search, etc. In the present paper, the author integrates the adaptive flooding level for flooding regularization, which is just a naive combination of the existing methods. Thus the novelty of the present paper is limited.\n\nAlthough the author proposes a correction function to account for the imperfections of the prediction of the auxiliary model, the proposed correction function is very heuristic, and there is no theoretical analysis to reason about the soundness of this proposed function. The efficient training for the auxiliary networks also has a similar issue.\n\nOverall, although the author provided a theoretical intuition for Equation (3), which is the training objective for adaptive flooding, the technical details to optimize this loss function proposed by the present paper are too heuristic and hard to be a principle and systematic approach.\n\n2. The significance of the proposed method\n\nAlthough the reviewer acknowledges the effort the author has made to validate their proposed method by the different modality of data, the data, and model chosen by the author is still not convincing. For example, though the SVHN, CIFAR10, and CIFAR100 are commonly used classification datasets, their complexity, scale, and reality are much less than ImageNet and more recent datasets like LAION. Although the review acknowledges that the author has stated in the last two lines of Page 7 that the result of the proposed method is not significant, this still gives us a feeling that the proposed method will not significantly improve the DNN training to avoid overfitting given so marginal improvement on Table 2. It is better for the author to consider a more realistic and decent scale of the dataset to conduct the experiment to show that the proposed simple method can achieve much better performance than state-of-the-art (SOTA), otherwise, it is hard to demonstrate the significance of the proposed method.\n\n3. The efficient training strategy is mainly randomly re-initializing the last few layers of the model and then re-training with the relevant fold held out. How to define the term \"few\"? What's the principle here to choose those layers? The reviewer noticed that the author said ResNet-18 is a large model and used it in the experiment. Can the proposed method still work effectively when we use ViT for image classification and Transformer for text modality? Will this efficient training strategy still work?\n\n4. The author states in the third paragraph of page 5 that: \n\n\"This procedure is justified by recent understanding in the field that in typical settings, a single data point only loosely influence\nthe early layers of a network.\"\n\nThe author should provide explicit references to support this claim, e.g. what recent understanding does the author refer to? How do they relate to the proposed method?\n\nMinor:\n\n1. The `Unreg.` in Figure 3 can not be seen. The author may consider another format of the line for better visualization."
                },
                "questions": {
                    "value": "Please refer to the weakness section for the questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7123/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7123/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7123/Reviewer_vFax"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7123/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741535113,
            "cdate": 1698741535113,
            "tmdate": 1699636842540,
            "mdate": 1699636842540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FrHDSxKKqy",
                "forum": "70A6oo3Il2",
                "replyto": "XwfN1ZNR7l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vFax (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your comments. Please see the common response above as well as our responses below.\n\n\n> The limited novelty of the proposed paper. The idea of using adaptive weighting for model training is widely used in existing machine learning areas, the more relevant ones can be curriculum learning and uncertainty learning. using a held-out dataset to condition the training of the primary model is a well-known strategy in machine learning, e.g., in meta-learning, batch or data selection, neural architecture search, etc.\n\nIndeed, adaptive weighting and held-out datasets are each commonly used techniques in machine learning, as we mentioned in the paper. We are not aware, however, of any time they have been used together for something resembling the difficulty-aware regularization setting of our work. The vast majority of research is based on applying new combinations of existing tools in new settings; our paper is no different in this regard. We think this is a strength, and not a weakness: our method is built on well-tested tools that have individually proven effective in diverse fields, suggesting that our proposal is built on a solid foundation, and with the ability to share techniques and understanding of those tools across application areas.\n\nIf we missed any closely related work in those areas, however, we are eager to compare them to our approach and cite them.\n\n\n\n> Although the author proposes a correction function to account for the imperfections of the prediction of the auxiliary model, the proposed correction function is very heuristic, and there is no theoretical analysis to reason about the soundness of this proposed function. The efficient training for the auxiliary networks also has a similar issue.\n\nWe agree that the correction function is heuristic, but it is a very natural and intuitive way of correcting the predictions from auxiliary networks. It is very easy to implement, and it performs well (as we demonstrated in the multiple experiments).\n\nAs the reviewer pointed out, it would be indeed better if we had solid theoretical background for every component, but in practice this is extremely rare, especially for deep learning. The influence of even the most basic regularizer, weight decay, is still a subject of very active research among deep learning theorists (e.g. [1] from earlier this year). We think that our support for AdaFlood in Proposition 1 establishes a strong theoretical baseline, and the discussion in the following paragraph gives sufficient initial intuition about the behavior of the correction function.\n\nFor the efficient training of auxiliary networks, we expanded on the motivation in highly-overparameterized regimes in Appendix E of the revised paper. In summary, in the regime where networks are well-explained by their neural tangent kernels (NTKs), held-out predictions by an auxiliary network trained from scratch perfectly agree with predictions from a network trained according to our efficient algorithm. This is because the empirical NTK of the penultimate layer is identical for the two networks. Please refer to Appendix E for further details.\n\n[1] Joseph Shenouda, Rahul Parhi, Kangwook Lee, and Robert D. Nowak. Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. arXiv:2305.16534.\n\n \n\n> The significance of the proposed method. Though the SVHN, CIFAR10, and CIFAR100 are commonly used classification datasets, their complexity, scale, and reality are much less than ImageNet and more recent datasets like LAION.\n\nWe shared some results on larger datasets in the common response above, showing that AdaFlood still generally outperforms the other flooding methods (with even bigger gains in noisy-label settings). We are currently running experiments on the full ImageNet, and will add them to the revised paper once they are complete.\n\nWe\u2019d like to emphasize that, while we haven\u2019t shown results on the largest image classification datasets, we have demonstrated the versatility of our method across a variety of tasks including density estimation, regression, and classification, on settings including asynchronous time sequences, images, tabular, and text data."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514599817,
                "cdate": 1700514599817,
                "tmdate": 1700580634220,
                "mdate": 1700580634220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AUdLgXB30H",
                "forum": "70A6oo3Il2",
                "replyto": "XwfN1ZNR7l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vFax (2/3)"
                    },
                    "comment": {
                        "value": "> Although the author has stated [\u2026] the result of the proposed method is not significant, this still gives us a feeling that the proposed method will not significantly improve the DNN training to avoid overfitting given so marginal improvement on Table 2.\n\nWe first want to highlight that in the last two lines of page 7, we were not saying that our method\u2019s overall results were not significant \u2013 if that were the case, we wouldn\u2019t have submitted the paper. Instead, we were saying that on _SVHN in particular_, AdaFlood performs about the same as previous flooding methods. (Performance is probably basically saturated for this class of model on SVHN.) AdaFlood does give nontrivial improvements in accuracy and calibration on CIFAR10 and CIFAR100. In the results in the common response above, we also see even larger improvements on ImageNet100 (probably because its labels are inherently noisier). Notice in particular that the improvement of AdaFlood over iFlood is larger than the improvement of iFlood over Flood, or of Flood over baseline models.\n\nMoreover, it is far more robust to noise on these datasets than competitor methods. Outside of image classification, it also sees improvements in both original and noisy versions of many datasets across asynchronous time sequence density estimation, text classification, tabular regression.\n\n\n> The efficient training strategy is [\u2026] How to define the term \"few\"? What's the principle here to choose those layers? Can the proposed method still work effectively when we use ViT for image classification and Transformer for text modality? Will this efficient training strategy still work?\n\nRecall that the purpose of having auxiliary networks is to measure difficulty of each sample. Here, we avoid a model seeing the sample that we measure difficulty on, in training time. With this motivation, random initialization of last few layers of efficient training of auxiliary networks is to forget information about the samples we measure difficulties on.\n\nAccording to the conditions provided by [1] for forgetting, randomly initializing only the last layer satisfies these conditions; initializing more layers, of course, does as well. Also, in [2] as the title implies, re-training only the last layer is sufficient to remove spurious correlations that are more specific to individual data points. These results suggest that re-initializing at least one layer should be sufficient.\n\nThen, the question is how different predictions will be depending on the number of layers we randomly initialize. [3] provides a user guide for fine-tuning, depending on the gap between pre-training and adaptation steps. According to [3], our fine-tuning falls in the category where the required feature update is tiny (since we fine-tune on a subset of the training set), which means whether we initialize only the last layer or last few layers, it won\u2019t change the features too much at convergence.\n\nIn addition to this guidance from prior work, we explore the question of how many layers to reinitialize in Section 4.5. We can see in those results that the performance of AdaFlood stays essentially the same (< 0.11%) if we reinitialize only the last layer, layer 4 plus the last, or layers 3 and 4 plus the last. Thus, reinitializing only the last layer gives the fastest method at essentially the same performance.\n \nBased on this, we agree that it will be clearer if we are more specific when presenting the method, and so changed the description in the revised paper from \u201cthe last few layers\u201d to \u201cthe last layer.\u201d\n \nAs suggested, we also verified that the fine-tuning variant works for ViT. We took the ViT implementation from a publicly available repo (https://github.com/kentaroy47/vision-transformers-cifar10). To train multiple auxiliary networks, we split CIFAR10 into 20 folds, and trained 20 ViT models for $500$ epochs each as described in Sec 3.2. For the fine-tuning variant, we train a ViT model on the full CIFAR10, and then fine-tuned on 20 folds as described in Section 3.3. After collecting losses for $50,000$ training samples from both multiple and fine-tuned versions, we obtained a Spearman (rank) correlation between these predictions of 0.61, essentially the same as the correlation observed for ResNets. Thus, the efficient variant provides substantial signal to the \u201ccorrect\u201d adaptive flood levels with ViT as well.\n\nLastly, we want to emphasize that the efficient training of auxiliary networks saves computational cost in computing the adaptive flood levels, but it is not a \u201cmust-have\u201d component for the proposed AdaFlood method.\n\n[1] Hattie Zhou, Ankit Vani, Hugo Larochelle, Aaron Courville. \u201cFortuitous forgetting in connectionist networks\u201d. ICLR 2022.\n\n[2] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. \u201cLast layer re-training is sufficientfor robustness to spurious correlations\u201d. ICLR, 2023.\n\n[3] Yi Ren, Shangmin Guo, Wonho Bae, Danica J Sutherland. \u201cHow to Prepare Your Task Head for Finetuning\u201d. ICLR 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514670914,
                "cdate": 1700514670914,
                "tmdate": 1700580626693,
                "mdate": 1700580626693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OstBtbWHRt",
                "forum": "70A6oo3Il2",
                "replyto": "XwfN1ZNR7l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vFax (3/3)"
                    },
                    "comment": {
                        "value": "> The author states in the third paragraph of page 5 that: \"This procedure is justified by recent understanding in the field that in typical settings, a single data point only loosely influence the early layers of a network.\" The author should provide explicit references to support this claim, e.g. what recent understanding does the author refer to? How do they relate to the proposed method?\n\n \nFollowed by the sentence the reviewer mentioned, we explained that \u201cIn highly over-parameterized settings (the \u201ckernel regime\u201d) where neural tangent kernel (NTK) theory is a good approximation to the training of $f^{aux}$ [1], re-initializing the last layer would completely remove the effect of $x_i$ on the model.\u201d As mentioned for a previous question, we have added a more thorough discussion of this point to Appendix E.\n\nFor further intuition about loose influence on the early layers, here is our reasoning:\n\nAccording to [1], in the NTK regime where the width of a neural network is infinite, and loss function is the least-square loss, the prediction from the neural network and ridgeless kernel regression using NTK are equal in expectation. In other words, the prediction of a neural network $f$ on a test sample $\\tilde{x}$, say $f(\\tilde{x})$, is governed by a fixed kernel called NTK. For ridgeless kernel regression, adding data points does not change the kernel function but it does change the prediction by adding a row and column to the kernel matrix. Conversely, if we remove $x_i$ from the training set, $x_i$ will not affect on inference at all. \n\nEven if a neural network does not have infinite-width, it is shown that overparameterized neural networks can converge linearly to zero training loss with their parameters hardly change [2] i.e., the change in the Jacobian matrix is very small. Hence, in this regime, a single data point only loosely influence training.\n\nIn addition, [3] states that \u201c\u2026the gradient tends to get smaller as we move backward through the hidden layers. This means that neurons in the earlier layers learn much more slowly than neurons in later layers.\u201d Combined with the previous arguments, we hope that our statement for \u201ca single data point only loosely influence the early layers of a network\u201d makes sense now.\n\nThe paragraph where the we stated \u201ca single data point only loosely influence the early layers of a network\u201d was about justifying that re-initializing the last (few) layers of a pre-trained neural network is enough to forget about a data point, and so, it is reasonable to compute the difficulty on the point with a fine-tuned model. In the paragraph, we divided the parameter regimes as,\n\n- NTK regime \u2014 Re-initializing the last layer would completely remove the effect of $x_i$.\n- Non-NTK (more realistic) regime \u2014 Last layer re-training retains \u201ccore\u201d features and removes \u201cspurious\u201d ones that are more specific to individual data points [4]\n\nIf the reviewer thinks it is still unclear, please let us know. We are eager to discuss further and make the paper clearer.\n \n \n [1] Arthur Jacot, Franck Gabriel, and Clement Hongler. \u201cNeural tangent kernel: Convergence and generalization in neural networks\u201d, NeurIPS 2018.\n \n [2]Chizat, Lenaic and Oyallon, Edouard and Bach, Francis. \u201cOn lazy training in differentiable programming\u201d, NeurIPS 2019\n\n[3] Michael A. Nielsen, \"Neural Networks and Deep Learning\", Determination Press, 2015\n\n[4] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. \u201cLast layer re-training is sufficientfor robustness to spurious correlations\u201d, ICLR, 2023.\n\n\n\n> The `Unreg.`  in Figure 3 can not be seen. The author may consider another format of the line for better visualization.\n\nAs described in the Results paragraph in Section 4.3, we visualized $\\Delta Acc$, defined as the improvement in accuracy of a flood method over the accuracy of `Unreg.`  We also provided the baseline `Unreg.` accuracy in numbers, below the $\\Delta Acc$ lines. Visualizing in this way is helpful since the change in inherent difficulty between noise levels is much larger than the change in performance of different methods, and a plain accuracy plot (now shown as Figure 10 in the appendix) is overwhelmed by the difference in noise levels. We will consider other visualization formats, and would be happy to hear any suggestions that would be clearer while still making the difference between methods visible."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514706172,
                "cdate": 1700514706172,
                "tmdate": 1700580618743,
                "mdate": 1700580618743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "46SVxSDjoV",
            "forum": "70A6oo3Il2",
            "replyto": "70A6oo3Il2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7123/Reviewer_o6wC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7123/Reviewer_o6wC"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel regularization technique dubbed AdaFlood, casting a spotlight on individual training sample difficulty by adaptively modulating the flood level. It parades its versatility and prowess across a spectrum of datasets and tasks like density estimation, classification, and regression, where it triumphs in generalization, robustness to noise, and calibration facets. A set of experimental validations heightens the paper, echoing the efficacy of AdaFlood in comparison to existing flooding methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) AdaFlood emerges as a trailblazing regularization strategy that sensitively tunes the flood level according to the intrinsic difficulty of each training sample.\n\n(2) The experiments through the paper describe the AdaFlood\u2019s versatility and superiority across a diverse array of tasks and datasets.\n\n(3) Clarity and structural conformity characterize the paper\u2019s presentation, weaving a coherent narrative that fluently elucidates the mechanics and nuances of AdaFlood."
                },
                "weaknesses": {
                    "value": "(1) The method roots in a multi-view assumption, relying on auxiliary networks as different views to tailor their fitness. However, this assumption\u2019s fragility surfaces in scenarios where auxiliary networks falter, casting doubts over the accurate estimation of flood levels.\n\n(2) AdaFlood\u2019s embrace of instance-wise reweighting and difficulty estimation unfolds as a labor-intensive odyssey, and its relevance dims when meeting vast training sets. The rigidly uniform and offline nature of difficulty estimation may misalign with the dynamic ebb and flow of instance contributions across various training stages.\n\n(3) A confined exploration limited to toy datasets raises eyebrows, leaving unanswered questions regarding AdaFlood\u2019s effectiveness when unleashed on larger, real-world, and more challenging benchmarks.\n\n(4) The paper's theatrical stage seems restricted to closed-set supervised classification, leaving realms like self-supervised learning, like contrastive learning in vision and auto-regression in NLP, unnoticed.\n\n(5) The improvement seems to be marginal compared with the baselines in both visual and NLP tasks, according to Tab. 1 and Tab. 2."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7123/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779667650,
            "cdate": 1698779667650,
            "tmdate": 1699636842408,
            "mdate": 1699636842408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BWXtzBeBPo",
                "forum": "70A6oo3Il2",
                "replyto": "46SVxSDjoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer o6wC (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your comments. Please see the common response above as well as our responses below.\n\n\n> The method roots in a multi-view assumption, relying on auxiliary networks as different views to tailor their fitness. However, this assumption\u2019s fragility surfaces in scenarios where auxiliary networks falter, casting doubts over the accurate estimation of flood levels.\n\nWe first want to emphasize that we do not use a \u201cmulti-view assumption\u201d in the sense typically used in the field. We use multiple auxiliary networks, but each one is used for a subset of the data, and \u201csees\u201d the exact same features of the data \u2013 unlike multi-view learning, which uses multiple feature sets for the same underlying data points.\n\nThe motivation of using multiple auxiliary models is simply that model\u2019s training error on a particular data point is unlikely to be a good estimate of that data point\u2019s inherent difficulty; instead, we use a cross-validation scheme to evaluate the held-out error for each data point.\n\nAs to when \u201cauxiliary networks falter\u201d, we agree that utilizing auxiliary networks may harm training if auxiliary networks do not provide reasonably good predictions. Our correction function, introduced in Section 3.2, helps to mitigate this effect. As a reminder, the auxiliary network is not required to provide especially accurate predictions, but just to provide a useful-enough estimate of the difference in difficulty between samples to serve as an effective regularizer. \n\n\n\n\n>  AdaFlood\u2019s embrace of instance-wise reweighting and difficulty estimation unfolds as a labor-intensive odyssey, and its relevance dims when meeting vast training sets. \n\n\nIt is not easy to compute difficulty of samples, which is why there have been multiple works along that line such as C-score [1]. The challenge of computing difficulty lies on balance between learning good features and not seeing the samples we measure the difficult on. Due to this nature, the computation proportionally increases as the number of samples increases. It is, however, still meaningful to compute difficulties because it is a basis of many advances in efficient neural network training and inference [2, 3, 4]. Similarly, as we specified in Section 3.2, the cost of pre-computation can be further amortized over many training runs with different configurations or settings. We will release our measure of difficulties on image classification benchmark datasets to the public in the final version.\n\nMore importantly, we proposed an efficient variant of computing difficulty. Using this efficient fine-tuning variant, it is not necessary to train multiple neural networks. In Section 3.3, we demonstrated that its difficulty measure is highly correlated to that from multiple auxiliary networks i.e., their rank correlation is 0.63. Furthermore, in Section 4.5, we demonstrated that this efficient variant is orders of magnitude faster than the original version, without significantly harming downstream performance. \n\n[1] Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. \u201cCharacterizing structural regularities of labeled data in overparameterized models\u201d. ICML 2021.\n\n[2] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. \u201cSelection via proxy: Efficient data selection for deep learning\u201d. ICLR, 2020.\n\n[3] Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. \u201cLearning to teach\u201d. ICLR, 2018.\n\n[4] Changlin Li, GuangrunWang, BingWang, Xiaodan Liang, Zhihui Li, and Xiaojun Chang. \u201cDynamic slimmable network\u201d. In CVPR, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514320553,
                "cdate": 1700514320553,
                "tmdate": 1700580094557,
                "mdate": 1700580094557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m3FVKV47h6",
                "forum": "70A6oo3Il2",
                "replyto": "46SVxSDjoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer o6wC (2/2)"
                    },
                    "comment": {
                        "value": "> The rigidly uniform and offline nature of difficulty estimation may misalign with the dynamic ebb and flow of instance contributions across various training stages.\n\nOur assumption is that there is a single \u201ctrue\u201d difficulty associated with each sample, i.e. its inherent noise, or the loss the Bayes-optimal predictor would find on that point. This is a standard assumption made in more realistic theoretical models from machine learning theory and in statistics, corresponding to heteroskedastic noise levels. A few recent works trying to measure this inherent difficulty are [1, 2].\n\nTo our knowledge, no work in this area has considered dynamic notions of difficulty, which presumably would account for points which initially seem like noisy samples but eventually (once good-enough features are learned) are clear. Although this surely occurs over the training process, this behavior will be inherently tied to a model\u2019s particular learning path, and so any such estimate will likely be far more tightly coupled to a particular model\u2019s training process. We have a hard time picturing how to usefully estimate this in a fairly general setting, and think the gains may be limited compared to the effects from modeling heteroskedastic noise.\n\n[1] Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, Michael C Mozer. \u201cCharacterizing structural regularities of labeled data in overparameterized models\u201d. ICML 2021.\n\n[2] Pratyush Maini, Saurabh Garg, Zachary Lipton, J Zico Kolter. \u201cCharacterizing datapoints via\nsecond-split forgetting\u201d. NeurIPS 2022.\n\n\n\n> A confined exploration limited to toy datasets raises eyebrows, leaving unanswered questions regarding AdaFlood\u2019s effectiveness when unleashed on larger, real-world, and more challenging benchmarks.\n\nWe disagree with characterizing the datasets used in this submission as \u201ctoy datasets,\u201d and many of them are derived from real-world problems. For example, the Stack Overflow dataset contains 6,633 sequences with 480,414 events where an event is the acquisition of badges received by users in Stack Overflow; the Reddit dataset contains 10,000 sequences with 532,026 events where an event is posting in Reddit. Perhaps these days SVHN could be considered nearly a toy dataset, but there are still many papers in machine learning and computer vision with results focusing on CIFAR-10 and -100.\n\nAs requested, however, we ran additional experiments on larger-scale datasets, with results described in the common response. Overall, we see that AdaFlood still helps over unregularized and other flooding-regularized models, with even larger improvements than before in the noisy-label setting.\n\n\n\n> The paper's theatrical stage seems restricted to closed-set supervised classification, leaving realms like self-supervised learning, like contrastive learning in vision and auto-regression in NLP, unnoticed.\n\nOur experiments have been conducted on many domains with several tasks: density estimation for asynchronous time sequences, regression for tabular datasets, classification for image and text datasets. Density estimation and regression are very different from classification tasks. Therefore, it is hard to agree that our method is restricted to closed-set supervised classification.\n\nWe agree that we have not considered contrastive learning; it\u2019s hard to see how to apply flooding-type regularizers in that setting. Autoregressive NLP tasks, however, are also a form of density estimation, and have characteristics of both the asynchronous time sequence datasets and supervised classification settings that we consider.\n\n\n\n> The improvement seems to be marginal compared with the baselines in both visual and NLP tasks, according to Tab. 1 and Tab. 2.\n\nWe would like to correct that Table 1 is not for NLP tasks at all; it covers density estimation on asynchronous time sequences. More importantly, in Table 1, there are multiple measures where AdaFlood significantly outperforms the baselines: on the Uber dataset, the improvement over iFlood $7.38$ and $13.76$ in RMSE (standard error is around $1.5$), NLL: $0.22$ and $0.15$ (standard error is around $0.01$) with Intensity-free and THP+ models, respectively.\n\nFor Table 2, as we specified, AdaFlood is not significantly better than SVHN but there is noticeable improvement on CIFAR10 and 100 (differences outside of standard errors). In addition, AdaFlood performs significantly better in calibration and in noisy settings.\n\nWe provided further experiments on image classification datasets and tabular data in the previous response above. For those datasets, ours show more significant improvement over the baselines because the new datasets have more implicit noise compared to SVHN or CIFARs.\n\nWe would also like to highlight, with regards to the claim that our improvements are marginal, that the improvement of AdaFlood over iFlood is generally speaking larger than the improvement of iFlood over Flood, and of Flood over baselines."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514387994,
                "cdate": 1700514387994,
                "tmdate": 1700580115989,
                "mdate": 1700580115989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "byKO9HHKKG",
                "forum": "70A6oo3Il2",
                "replyto": "m3FVKV47h6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Reviewer_o6wC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Reviewer_o6wC"
                ],
                "content": {
                    "title": {
                        "value": "After Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the hard work of authors during response. But I cannot change the current rate due to the unconvincing arguments. Such instance-wise hardness reweighting by auxiliary networks looks like a multi-view-based semi/noisy-supervised learning. It is not an essentially innovated technique and hard to be applied for large-scale ML system with dynamic data flows."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710139045,
                "cdate": 1700710139045,
                "tmdate": 1700710139045,
                "mdate": 1700710139045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D7xxyxx3Tx",
            "forum": "70A6oo3Il2",
            "replyto": "70A6oo3Il2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7123/Reviewer_R2kv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7123/Reviewer_R2kv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new strategy to define the optimization target. It assumes different samples have different difficulty. The experiments are conducted on various datasets, including CIFAR for image and Reddit, Uber, Stack Overflow for asynchronous event sequence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-The proposed method is reasonable. Different samples has different difficulty. It is reasonable to use different loss.\n-The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "-This paper lacks experiments on large-scale dataset, such as ImageNet. CIFAR and SVHN are small datasets, which are very different from the real sense.\n\n-Does the adaptive strategy bring extra training cost? More detailed discussion is required."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7123/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840973777,
            "cdate": 1698840973777,
            "tmdate": 1699636842289,
            "mdate": 1699636842289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iuCXlpqM8J",
                "forum": "70A6oo3Il2",
                "replyto": "D7xxyxx3Tx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7123/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R2kv"
                    },
                    "comment": {
                        "value": "Thank you for your comments. Please see the common response above as well as our responses below.\n\n\n> This paper lacks experiments on large-scale dataset, such as ImageNet. CIFAR and SVHN are small datasets, which are very different from the real sense.\n\nThank you for the suggestion. We shared some results larger datasets in the common response above, showing that AdaFlood still generally outperforms the other flooding methods (with even bigger gains in noisy-label settings). We are currently running experiments on the full ImageNet, and will add them to the revised paper once they are complete.\n\nWe\u2019d like to emphasize that, while we haven\u2019t shown results on the largest image classification datasets, we have demonstrated the versatility of our method across a variety of tasks including density estimation, regression, and classification, on settings including asynchronous time sequences, images, tabular, and text data.\n\n\n\n> Does the adaptive strategy bring extra training cost? More detailed discussion is required.\n\nIn Section 4.5, we provided detailed discussion about the wall-clock time required to find the adaptive flood levels. Once we have these levels, there is essentially no computational overhead; adding one absolute value to the loss function is negligible.\n\nIf we use $M$ auxiliary networks, the time overhead is roughly $\\mathcal{O}(M T)$, where $T$ is the training time of each auxiliary model. However, the efficient fine-tuning variant can reduce this to $\\mathcal{O}(T + M t)$, where $t \\ll T$ is the amount of time to fine-tune a model, without losing performance. Furthermore, since these adaptive flood levels are not closely tied to the \u201cdownstream\u201d model, we can reuse them for a variety of downstream tasks, as was done e.g. in the C-score paper [1]. We will release these adaptive flood levels as a public difficulty measure, along with the code.\n\nAnother important point is that AdaFlood models converge faster than unregularized or Flood-based models. For instance, on CIFAR10, AdaFlood\u2019s validation loss converges after around 80 epochs, compared to 150 for unregularized models, 150 for Flood, and 100 for iFlood. Similarly, on CIFAR100, while AdaFlood and iFlood\u2019s validation loss converges after around 60 epochs, unregularized and Flood take around 150 epochs to converge. Thus, once we have adaptive flood levels, a significant amount of training time can be saved. This effect will be amplified as it is amortized for multiple training runs, e.g. when others use our publicly-released flood values.\n\n[1] Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. \u201cCharacterizing structural regularities of labeled data in overparameterized models\u201d. ICML 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7123/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514251427,
                "cdate": 1700514251427,
                "tmdate": 1700536098224,
                "mdate": 1700536098224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]