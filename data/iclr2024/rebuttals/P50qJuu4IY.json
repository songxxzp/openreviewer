[
    {
        "title": "Self-Supervised Learning with the Matching Gap"
    },
    {
        "review": {
            "id": "8dfM4ey5I4",
            "forum": "P50qJuu4IY",
            "replyto": "P50qJuu4IY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new self-supervised method that reduces the gap between the ground-truth matching and optimal matching. The proposed loss, matching gap, could potentially alleviate the problematic signal that force different views of the same image to collapse to the same point, even when they capture dramatically different contents, i.e. foreground vs background. To further ease the optimization, the paper proposes to learn the optimal matching via the Sinkhorn algorithm. Experimental results show that the proposed method performs on par with SOTA approaches with strong data augmentations and outperforms several latest self-supervised works with simpler data augmentations.\n\n***\nPost-rebuttal comment:\n\nFollowing the discussion with the authors regarding their revision, the reviewer has maintained the original rating (marginally below the acceptance threshold), as the revision has not fully addressed the reviewer\u2019s two primary concerns: i) the need for fair comparisons on the reported metrics (mainly about performance) ii) lack of sufficient justifications for the claimed benefits other than performance. The first concern was partially addressed in the rebuttal, which seems to indicate that MG is less effective than the baselines in terms of performance. The second concern was not addressed in the rebuttal due to time constraints,  making it difficult for the reviewer to understand the extra advantages of the work beyond the performance aspects. The reviewer suggests the author further explore and demonstrate MG\u2019s benefits beyond mere performance. The authors have recognized these shortcomings, and are committed to further improving the work in accordance with the feedback from both Reviewer 1FzB and the present reviewer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(S1) [Motivation] self-supervised learning learns feature representation via pretext tasks. As there are typically no human supervision involved. The \u201cground-truth\u201d signals of such tasks are usually pretty \u201cnoisy\u201d. The paper aims to alleviate the noisy supervision by reducing the gap between the ground-truth matching and optimal matching that could be computed on-the-fly. The reviewer believes this is an interesting topic.\n\n(S2) [Method] the paper proposes to approximate the optimal matching via the Sinkhorn algorithm, which could further ease the online optimization.\n\n(S3) [Ablation] Ablations on different components of the proposed method are included"
                },
                "weaknesses": {
                    "value": "(W1) [Evaluation] The evaluation section could have been more comprehensive. For example, when strong data augmentations are involved (Table 1), only several SSL baselines are included, e.g. MoCo-v3, DINO. The settings shown in Table 1 are also not consistent, e.g. different number of epochs, which makes it difficult to interpret the results, e.g. could the proposed method match DINO\u2019s performance when trained for the same number of epochs? Also, it would be beneficial to include architectures beyond ViT-B(L)/16, e.g. ViT-S, CNN, etc.\n\n(W2) [Performance] With strong augmentations, the proposed method shows no benefit compared to SOTA methods. The method outperforms several SSL approaches with weaker augmentations. However, at least from the application perspective, it is unclear to the reviewer what are the advantages of using only weak augmentations, especially when the training epochs are the same.\n\n(W3) [Claim] Some of the claims are not well-justified. For example, i) compared to CL that may \u201ccollapse representations for very different images\u201d, the proposed method learns diverse representations for different views of the same image; ii) stronger data augmentations could help mitigate the representation collapsing problem. In order to justify the claims, the authors may measure/compare the mean distance of different views of the same image, or different samples of the same classes, across different models (i.e. the proposed method and baselines) and settings (i.e. strong/weak augmentations)\n\nOverall, the paper studies an interesting topic, the proposed method is also technically sound. However, he reviewer has concerns about the evaluation, performance and some of the claims in the paper. At this moment, the reviewer rates the paper as marginally below the acceptance threshold."
                },
                "questions": {
                    "value": "N.A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791140980,
            "cdate": 1698791140980,
            "tmdate": 1700956884795,
            "mdate": 1700956884795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ECqvATlX6f",
                "forum": "P50qJuu4IY",
                "replyto": "8dfM4ey5I4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer PMfK"
                    },
                    "comment": {
                        "value": "> _[Evaluation] The evaluation section could have been more comprehensive. For example, when strong data augmentations are involved (Table 1), only several SSL baselines are included, e.g. MoCo-v3, DINO. The settings shown in Table 1 are also not consistent, e.g. different number of epochs, which makes it difficult to interpret the results, e.g. could the proposed method match DINO\u2019s performance when trained for the same number of epochs? Also, it would be beneficial to include architectures beyond ViT-B(L)/16, e.g. ViT-S, CNN, etc._\n\n\u25a0 In presenting (and evaluating) MG, our focus was not to reach SOTA performance, but to suggest a simple alternative to contrastive SSL losses that can naturally avoid collapse. We do show that a \u201clean\u201d implementation of the MG loss leads to comparable results to SOTA.\nInstead of chasing SOTA, we used our compute budget to explore MG\u2019s performance over diverse settings (decreasing amount of augmentations, removing momentum encoder and predictor heads, or other experiments in section A.3 \u201cAdditional ablation studies\u201d, results over smaller datasets (CIFAR-10, CIFAR-100, and SVHN) using ResNet-50 as backbone). \nWe found that MG is consistently near or at the top, despite being much simpler conceptually than most approaches. Following your (and other reviewers\u2019 comments), we are now running additional experiments, to study impact of #epochs.\n\n> _[Performance] With strong augmentations, the proposed method shows no benefit compared to SOTA methods. The method outperforms several SSL approaches with weaker augmentations. However, at least from the application perspective, it is unclear to the reviewer what are the advantages of using only weak augmentations, especially when the training epochs are the same._ \n\n\u25a0 As mentioned in our introduction, increasing the variety/strength of augmentations can be seen as a way to mitigate the tendency of contrastive losses to *force* instances of the same image to collapse in representation space. However, as nicely presented by Assran et al. 2023 the reliance on hand-crafted data augmentations is limiting and may introduce unwanted biases. With that, we find that attaining good performance with a smaller number of epochs, and a smaller set of augmentations, is a worthy direction. Besides, this can be also useful in settings (e.g. biomedical data) where the number of \u201cnatural\u201d augmentations is more limited than in vision tasks (and where even testing the hypotheses that many augmentations are needed, or not, is harder).\n\n> _[Claim] Some of the claims are not well-justified. For example, i) compared to CL that may \u201ccollapse representations for very different images\u201d, the proposed method learns diverse representations for different views of the same image; ii) stronger data augmentations could help mitigate the representation collapsing problem. In order to justify the claims, the authors may measure/compare the mean distance of different views of the same image, or different samples of the same classes, across different models (i.e. the proposed method and baselines) and settings (i.e. strong/weak augmentations)._\n\n\u25a0 We thank the reviewer for this suggestion. We are now evaluating statistical properties of the representation space of MG and alternative approaches, under different augmentations of the same image. We will update the response once we have results, and hope to include it in a revised manuscript version within the rebuttal period.\n\n> _[Overall] the paper studies an interesting topic, the proposed method is also technically sound. However, he reviewer has concerns about the evaluation, performance and some of the claims in the paper. At this moment, the reviewer rates the paper as marginally below the acceptance threshold._\n\n\u25a0 We hope that our response to the points presented above helped lift some of your concerns, and will be happy to answer any additional questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057020202,
                "cdate": 1700057020202,
                "tmdate": 1700057020202,
                "mdate": 1700057020202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qrzvqY5Mqw",
                "forum": "P50qJuu4IY",
                "replyto": "ECqvATlX6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
                ],
                "content": {
                    "title": {
                        "value": "response to the authors"
                    },
                    "comment": {
                        "value": "The reviewer would like to thank the authors for the rebuttal, and the promised new experiments. The reviewer would consider re-rating the submission when more evaluations are made available in the revised paper, e.g. apples-to-apples evaluations, justifying the claims made in the paper, etc."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610790111,
                "cdate": 1700610790111,
                "tmdate": 1700610790111,
                "mdate": 1700610790111,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LRaOgdNDNA",
                "forum": "P50qJuu4IY",
                "replyto": "8dfM4ey5I4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
                ],
                "content": {
                    "title": {
                        "value": "response to the authors"
                    },
                    "comment": {
                        "value": "The reviewer expects apples-to-apples experiments and justifications about claims, as in the reviewer\u2019s humble opinion, these are also expected by the ICLR audience, that the readers can interpret the results without speculation and get convinced by the claims. Eventually, the aim is to make it easier for the readers to learn the potential advantages of this work. \n\nGiven the pre-rebuttal results presented in Table 1, it is unclear to the reviewer if MG could match DINO when trained in the same number of epochs (i.e. 400). The rebuttal clarifies that MG's performance peaks at 300 epochs, making it less effective than DINO. Therefore, the readers may learn little about the advantages of MG in Table 1. Similarly, the reader may also learn little in Table 2 as the performance gaps, either positive or negative, are marginal. In Table 3, the reader may speculate that, with weak augmentations, how does MG perform compared to the baselines presented in Tables 1 and 2, as they are not included in Table 3. The paper also lacks convincing evidence for other claimed benefits of MG, like sample diversity (strongly indicated in the abstract) and simplicity, \n\nThe reviewer agrees with the authors that SOTA is not mandatory for ICLR and appreciates the authors\u2019 exploration of an alternative, potentially \u201csimpler\u201d loss that may yield more \u201cdiverse representations\u201d. However, the reviewer cannot find solid justifications for such advantages in the current revision.\n\nThe authors are encouraged to further showcase other potential benefits of MG that can be easier to evaluate, e.g. memory usage, etc."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686010151,
                "cdate": 1700686010151,
                "tmdate": 1700686822960,
                "mdate": 1700686822960,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W9ybmgKZdT",
            "forum": "P50qJuu4IY",
            "replyto": "P50qJuu4IY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime."
                },
                "weaknesses": {
                    "value": "1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3."
                },
                "questions": {
                    "value": "See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN",
                        "ICLR.cc/2024/Conference/Submission8379/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865306138,
            "cdate": 1698865306138,
            "tmdate": 1700656959880,
            "mdate": 1700656959880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oIXw4fMNVN",
                "forum": "P50qJuu4IY",
                "replyto": "W9ybmgKZdT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tJTN"
                    },
                    "comment": {
                        "value": "> _IOT vs MG. I Wonder if it improves the training speed/convergence or reduces the memory consumption?_\n\n\u25a0 Our MG approach will be at least twice as fast when it comes to computing the loss and its gradient, since only requires a forward pass. It will also be more memory efficient if one chooses to unroll the Sinkhorn solutions, or even faster if one chooses to differentiate implicitly the OT solution (see e.g. Luise et al. 2018), which requires solving a $n\\times n$ linear system.\n\nThat being said, in the large-scale setting that we have used \u2014 notice that IOT (Shi et al. 2023) was never evaluated on ImageNet \u2014  the overhead of running a ViT-B architecture far dominates the costs of running Sinkhorn when using a small batch size of 128, a fairly large entropic regularization ($\\epsilon$). The loss computation, whether MG or IOT, is negligible. The speedup of MG vs. IOT did not, therefore, play an important factor in the overall compute cost. As we mention in the conclusion, this tradeoff will change when using simpler feature architectures, larger batch sizes, or, as mentioned in the conclusion, considering other OT approaches that cannot be differentiated (e.g. low-rank solvers).\n\n> _Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss._\n\n\u25a0 In presenting MG our goal was not to suggest a combination of tricks that could reach SOTA performance, but rather to provide a conceptually simple alternative to contrastive SSL losses, with the hope that, when applied in  \u201cnaively\u201d, its performance will be consistently comparable or better than far more complex approaches that use several tricks. We believe we have delivered in that sense.\n\nFollowing this objective, we have used our computation budget to explore a variety of settings that showcase the robustness of MG, the simplifications it allows for, as well as its shortcomings. \n\nWe indeed relied on reported performances for baseline methods in the most comparable setting (e.g. matching backbone architecture). Here, it is important to note that while indeed the number of crops provides an advantage for MG in a majority of cases MG is evaluated over a **significantly lower number of epochs**.\n\nAt last, specifically concerning the comparison to I-JEPA, Assran et al. (2023) present the new setting, Image-based Joint-Embedding Predictive Architecture, in which the construction is by definition different: for each image, given a context block, predict the representations of various target blocks, where $M$ (typically setting $M=4$) non-overlapping targets are chosen. Hence, it is not straightforward to define an exact _fair_ comparison between these settings.\n\n> _Some notations are used without first introduced, e.g., $c$ in Introduction and $t$ in Sec. 3._\n\n\u25a0 $c(\\cdot,\\cdot)$ is simply a cost, this was defined at the beginning of Section 2.2, we now mention it earlier in the intro when first introduced. The function $t(\\cdot,\\cdot)$ was defined in Equation 3 (using the $:=$ symbol). We have now split it to make it more visible and clear.\n\n> _Overall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement._\n\n\u25a0 We thank you for highlighting these positive aspects of our paper. We thank you for your several comments and suggestions. We hope that you can appreciate that we have decided to focus our compute budget on various ablations, rather than aiming for a SOTA performance in a specific setting. Ultimately, we believe the picture we provide, where the contribution of MG is better outlined, isolated, and ablated, paints a more convincing picture."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985303317,
                "cdate": 1699985303317,
                "tmdate": 1699985303317,
                "mdate": 1699985303317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K2eYfdfFEK",
                "forum": "P50qJuu4IY",
                "replyto": "oIXw4fMNVN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their efforts in addressing the raised concerns.\n\nWhile I appreciate the simplicity of MG and its robustness to various tricks, I still find the performance not convincing, particularly in light of the saturated performance w.r.t. pre-training epochs. Moreover, the lack of direct comparison, such as using only two global views for all methods, obscures the true effectiveness of MG and hinders a comprehensive understanding of its capabilities.\n\nTherefore, I still lean toward a rejection but will increase the score to 5."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656923028,
                "cdate": 1700656923028,
                "tmdate": 1700656923028,
                "mdate": 1700656923028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J3IYuTbNEl",
            "forum": "P50qJuu4IY",
            "replyto": "P50qJuu4IY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new contrastive loss based on the matching gap. The proposed method is an extension of the paper \"Understanding and generalizing contrastive learning from the inverse optimal transport perspective\u201d. Also, the main idea of this paper is related to \"whether the cost of the identity ground-truth pairing is significantly higher than the optimal matching cost that can be achieved, and use their difference as a loss\"."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper provides an explicit relationship between the gradients of the proposed matching gap loss with that of InfoNCE."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is rather limited. This article only uses a previously proposed technique to improve the computational complexity of inverse OT-based contrast loss in the optimization process. I do not find any new insights related to the field of contrastive learning.\n2. This paper is really hard to follow. There are many mathematical symbols and proper nouns that lack explanation. For example, what is t in eq. 6 and 7, what is bistochastic matrices, and what are the difference between matching cost,  measuring agreement,  matching gap, and optimality gap?\n3. The organization of Section Introduction is superfluous. I cannot find the relationship between the first two paragraphs and the last paragraph.\n4. The experimental results cannot verify the effectiveness of the proposed method. First, the performance gain is pretty small. Second, there are many cases where the proposed method obtains a bad result.\n5. The \"A Link between InfoNCE and the Matching Gap\" part and the \"Our Contribution: Single Level Optimization with the Matching Gap\" part are so vague that I have read them many times without understanding the logical relationship."
                },
                "questions": {
                    "value": "1, How do you get eq. 8 from eq. 7?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698929713948,
            "cdate": 1698929713948,
            "tmdate": 1700645169887,
            "mdate": 1700645169887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ppZAZ6Qrzg",
                "forum": "P50qJuu4IY",
                "replyto": "J3IYuTbNEl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zo3d (I)"
                    },
                    "comment": {
                        "value": "> _The proposed method is an extension of the paper \"Understanding and generalizing contrastive learning from the inverse optimal transport perspective\u201d._ \n\n\u25a0 We respectfully disagree. While it is true that both our (MG) and (Shi et al. 2023) methods use optimal transport to define a SSL loss, our method cannot be characterized as an *extension* of (Shi et al. 2023). A single level optimization problem cannot be described as an extension of a bilevel problem.\n\nWe make this distinction clear several times in the paper, going as far as citing and differentiating us from (Shi et al. 2023) 14 times. In the introduction, we chose carefully our words, writing \u201cdraw inspiration from (Shi et al. 2023)\u201d (i.e. not \u201cextend\u201d). We credit (Shi et al. 2023) with an entire paragraph to introduce their approach (\u201cInspiration : \u2026\u201d in p.5), insisting on the _bilevel optimization_ approach taken by (Shi et al. 2023). We then clearly lay out the differences in section \u201cOur Contribution\u201d which is _single Level_. \n\nInstead, MG can be better described as an extension of infoNCE, blending the negative sampling idea of (Robinson et al. 2020) with the gap perspective outlined in Monge Gap (Uscidda and Cuturi 2023). We will \n\n> _The novelty of this paper is rather limited. This article only uses a previously proposed technique to improve the computational complexity of inverse OT-based contrast loss in the optimization process._\n\n\n\u25a0 We respectfully disagree, our method was not proposed previously, and our goal is not to improve the computational complexity of inverse OT. Improving inverse OT would require carrying out a more efficient differentiation of $\\rm{argmins}$ solutions (using e.g. unrolling or the implicit function theorem). Instead, our method **bypasses** those challenges **entirely** thanks to a different formulation, resulting in a much simpler optimization.\n\n\n> _Hard to follow. There are many mathematical symbols and proper nouns that lack explanation. For example, what is t in eq. 6 and 7, what is bistochastic matrices, and what are the difference between matching cost, measuring agreement, matching gap, and optimality gap?_\n\n\u25a0 We regret that the reviewer found the paper hard to follow. \nThe function $t$ is defined in Equation 3 ($:= $), in the revised manuscript we have made this definition more apparent; \nThe definition of bistochastic matrices is provided in Section 2.2, $\\mathcal{B}_n$, and is a standard concept (see. E.g. https://en.wikipedia.org/wiki/Bistochastic_matrix);\nThe \u201cmatching cost\u201d is defined in Equation 3;\n\u201cMeasuring agreement\u201d is used in a sentence to describe the IOT loss (\u201cInspiration: Measuring Agreement using Inverse OT.\u201d). This refers directly to a sentence above, _\u201cIn such cases, OT can be used as a way to quantify whether locations (xi, yi) are in agreement with the diagonal pairing\u201d_;\nThe _matching gap_ is the name of the method we introduce; \nThe optimality gap, or \u201cgap to optimality\u201d is a common concept in optimization, quantifying to what extent a solution is not optimal. We use this reference to highlight that the matching gap is simply the objective value of $J_n$ vs. the best possible objective value, as in Equation 7.\n\n\n> _The organization of Section Introduction is superfluous. I cannot find the relationship between the first two paragraphs and the last paragraph._\n\n\u25a0 We wrote the 3 paragraphs of the introduction to highlight the following blocks:\n*context*: the first paragraph provides context, introducing the importance of SSL with a contrastive view. \n*problem*: the second paragraph highlights that SSL has weaknesses due to feature collapse, which have been identified by previous works as arising from the promotion of invariance by contrastive losses such as InfoNCE\n*solution*: the third paragraph details our contribution to solve the problem above, the matching gap.\n\nIn the revised manuscript, we give these paragraphs an explicit title to clarify further the structure above."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985153390,
                "cdate": 1699985153390,
                "tmdate": 1699985153390,
                "mdate": 1699985153390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VkGIa4QiHk",
                "forum": "P50qJuu4IY",
                "replyto": "ciw4sqj6vT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "1. About novelty: 1) the authors have not explained why the bi-level problem is necessary and effective; 2) improving the computational complexity of inverse OT have been addressed many time in the OT field. Apply this in contrastive learning is not innovative at all. \n2. The authors claims that the aim of this paper is to explore the robustness, the shortcomings, and the ability of the MG. However, regarding the performance gain reported in this paper, I think this paper is overclaimed. \n3. The quality of the first draft was particularly low, containing many errors and ambiguities, and falling far short of acceptable standards. And in the rebuttal phase, the authors are not well interpreted and modified.\n\nTherefore, I low my original rating."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645117676,
                "cdate": 1700645117676,
                "tmdate": 1700645117676,
                "mdate": 1700645117676,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iXvSwlyNS4",
                "forum": "P50qJuu4IY",
                "replyto": "J3IYuTbNEl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, \n\nWe would like to address your response above. Despite our rebuttal and attempt at clarifying your concerns we are afraid there are some misunderstandings we turn to clarify.\n\n> _**Comment: 1. About novelty: 1) the authors have not explained why the bi-level problem is necessary and effective; 2) improving the computational complexity of inverse OT have been addressed many time in the OT field. Apply this in contrastive learning is not innovative at all.**_\n\n\u25a0  1) As we present it in the text \u201cOur Contribution: Single Level Optimization with the Matching Gap\u201d and attempted to explain in our previous rebuttal response, MG avoids a bi-level problem, hence it is unclear why we should clarify that a bi-level approach should be necessary or effective. Our message is the opposite, bi-level is not needed. If there was a mistake in your wording above, and your point was the opposite of what you wrote (you expect us to explain why bi-level is not necessary nor effective), then we refer you to our answer to reviewer **tJTN** for a discussion of this aspect.\n\n\u25a0 2) We are not improving the complexity of Inverse OT in our paper, nor extending inverse OT. We are only using OT (forward computation, no backward) for SSL. MG is a direct approach that avoids bi-level and inverse notions completely. This is presented in the paper, with a dedicated section \u201cOur Contribution: Single Level Optimization with the Matching Gap\u201d as well as following discussion. In addition, in the rebuttal we have described this in our responses above to **your** questions,\n\n-  _\u201dThe proposed method is an extension of the paper \"Understanding and generalizing contrastive learning from the inverse optimal transport perspective.\u201d_ \n\n-  \u201c_The novelty of this paper is rather limited. This article only uses a previously proposed technique to improve the computational complexity of inverse OT-based contrast loss in the optimization process.\u201d_\n\n\n> **_2. The authors claims that the aim of this paper is to explore the robustness, the shortcomings, and the ability of the MG. However, regarding the performance gain reported in this paper, I think this paper is overclaimed._**\n\n\u25a0  We would appreciate it if the reviewer could point out examples that we have overclaimed in that respect to allow us to tone down and/or relate to concrete claims.\n\n> **_3. The quality of the first draft was particularly low, containing many errors and ambiguities, and falling far short of acceptable standards. And in the rebuttal phase, the authors are not well interpreted and modified._**\n\n\u25a0  We were keen to see that other reviewers acknowledged the quality of writing and presentation. We do realize the work contained typos, however we find the above remark harsh and unjustified. Unjustified, because a score of 1 should be only used for trivially wrong or plagiarized papers."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653107458,
                "cdate": 1700653107458,
                "tmdate": 1700733195211,
                "mdate": 1700733195211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YbaUIYrEWr",
            "forum": "P50qJuu4IY",
            "replyto": "P50qJuu4IY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_1FzB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_1FzB"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggest a novel approach to unsupervised representation learning. Following previous work (e.g. IOT), the main idea is to use an optimal transport plan between different view to guide the metric learning process. The main contribution is in the way this is done, which relies on trying to match the *cost* of the plan, rather than the plan itself to the ground truth pairing of augmented views. \nThis seemingly simple change brings several advantages in training, with very competitive results, and is shown to have an interesting interpretation when compared to the commonly used InfoNCE loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1] The matching gap loss is an important finding that I expect to have impact on the field. It is well motivated as a way to allow the needed flexibility in the contrastive learning setup, where positive views should not necessarily be forced to the same point. \n2] Its ease of use and computational advantages are clearly shown - the ability to use the OT guidance without the need to differentiate through the Sinkhorn iterations, with computations involving only the pairwise n x n pairwise matrices.\n3] The analysis that compares the new loss to the known InfoNCE is very enlightening and gives a very good understanding about what is happening in the optimization.\n4] The paper is well written in all aspects, from the motivation, throughout the solution and experimental results."
                },
                "weaknesses": {
                    "value": "Here are some, but rather minor:\n1] Experimentation - I think that this new form of loss would be better justified if there would be empirical evidence that supports the intuitions (in addition to the standard benchmarking and ablations). It would perhaps be interesting to see how the embeddings of an augmented batch behave, in comparison to standard NCE, or some statistics of that kind.\n2] The formulation is restricted to the 2-view setting. While this is simple, it would be interesting to know whether there are effective generalizations to multi-view settings.\n3] There is no specification or discussion regarding batch size, which has an important role in contrastive learning. Supposedly, the compact computation and 2-view setup could allow for larger batch sizes. It would be interesting to see how performance scales with batch-size.\n4] Several minor inaccuracies (which don't affect the analysis or correctness): (i) Bistochastic should be non-negative (ii) Should be <P,logP> in Equation 3 (without the -1) (iii) Last row of the loss equation is wrong, resulting in a matrix rather than a value: should probably be \\eps<P,logP> instead of \\eps\\logP."
                },
                "questions": {
                    "value": "* Please related to the above 'weaknesses'\n* I understand (and am in favor of) the limited budget experimentation. Did you ablate on number of epochs, within the budget, to see if the dimnishing returns behavior is comparable to other methods?\n* Due to the approach that does not require positives to converge to the same point - Perhaps there is actually room for more aggressive augmentation, that can exploit a richer extension of the training data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699081605117,
            "cdate": 1699081605117,
            "tmdate": 1699637042764,
            "mdate": 1699637042764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WvVHEXcxVd",
                "forum": "P50qJuu4IY",
                "replyto": "YbaUIYrEWr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "We are very grateful for your encouraging comments, constructive review, and really great suggestions!\n\nWe wanted to start our rebuttal to your review with a few numbers that we were able to compile in the last week or so.\n\n> **_I think that this new form of loss would be better justified if there would be empirical evidence that supports the intuitions (in addition to the standard benchmarking and ablations). It would perhaps be interesting to see how the embeddings of an augmented batch behave, in comparison to standard NCE, or some statistics of that kind._** \n\nWe agree, and we have been working on such a presentation.\n\n> _**2] The formulation is restricted to the 2-view setting. While this is simple, it would be interesting to know whether there are effective generalizations to multi-view settings.**_\n\nThis is indeed a very interesting avenue for further research.\n\n> _**3] There is no specification or discussion regarding batch size, which has an important role in contrastive learning. Supposedly, the compact computation and 2-view setup could allow for larger batch sizes. It would be interesting to see how performance scales with batch-size.**_\n\nThanks for this great suggestion. As can be seen in our general response above, we have investigated this aspect. We reproduce these results below for convenience. Using **MG** in a setting strictly identical to that reported in Table 1 (300 epochs over ImageNet), we obtain the following accuracies\n\n| $n$ |  Linear Accuracy|\n|-----| ---------------|\n| 32 | 75.43 %  |\n| 48 | 75.83 % |\n| 64 | 76.29 % |\n| 96 | 76.71 % |\n| 128 | 76.70 % |\n| 192 | 73.24 % |\n\nAs can be seen in these results, performance is fairly stable across $n$ sizes, and maybe suggest that for larger $n$, one might need to readjust $\\varepsilon$ values. We are still running results for even smaller sizes (e.g. 16, 24) and will report them.\n\n> **_4] Several minor inaccuracies (which don't affect the analysis or correctness): (i) Bistochastic should be non-negative (ii) Should be <P,logP> in Equation 3 (without the -1) (iii) Last row of the loss equation is wrong, resulting in a matrix rather than a value: should probably be \\eps<P,logP> instead of \\eps\\logP._**\n\nMany thanks for spotting these typos:\n- (i) yes, we have added the \"+\"\n- (ii) this \"-1\" is a convention that is adopted, for instance, in the computational OT book (e.g. https://arxiv.org/pdf/1803.00567.pdf Eq. 4.1, p.57). This helps rewrite the dual more elegantly, and also plays a role for unbalanced formulations, where the generalized KL is used.\n- (iii) apologies for this ugly typo, now corrected.\n\n> **_Did you ablate on number of epochs, within the budget, to see if the dimnishing returns behavior is comparable to other methods?_**\n\nYes, we have ran this experiment, as reported above. As mentioned in the general remark, the performance did not improve markedly when going from 300 to 600, when keeping all other hyperparameters unchanged ($76.5 %$)\n\n> **_Due to the approach that does not require positives to converge to the same point - Perhaps there is actually room for more aggressive augmentation, that can exploit a richer extension of the training data?_**\n\nThis is a good remark. However, as we debate in the paper, we took the opposite direction, and did instead try to get rid of aggressive augmentations, to simplify the overall pipeline (see e.g. Table 3). Our intuition is that agressive augmentations are a form of regularization that is needed with InfoNCE."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504350399,
                "cdate": 1700504350399,
                "tmdate": 1700642609232,
                "mdate": 1700642609232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hOSD1BUbnk",
            "forum": "P50qJuu4IY",
            "replyto": "P50qJuu4IY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_iZps"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8379/Reviewer_iZps"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel contrastive loss based on optimal matching cost. The proposed matching gap loss may avoid feature collapse according to its property. The author conducts experiments mainly on ImageNet classification, and the experiment results show its superiority."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "i) The idea of introducing matching costs is reasonable. The experiment results show it superior to some baseline results\n\nii) The writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "i) As I know, DINOv2 and SwAV also use the optimal transport algorithm to solve contrastive learning. But I can not find the discussion about such methods in related work. I would like to find the discussion about the difference between Matching Gap and SwAV/DINO\n\nii) As for experiments. In Table 1, the performance of the Matching Gap is not as good as DINO, which is a strong baseline proposed 1 year before.\n\niii) The author only conducts downstream experiments on transfer classification. Many self-supervised learning methods evaluate the downstream detection(COOC, VOC) and segmentation(COCO, aed20k) performance. I think the simple downstream classification task is not enough."
                },
                "questions": {
                    "value": "Refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699582094743,
            "cdate": 1699582094743,
            "tmdate": 1699637042608,
            "mdate": 1699637042608,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vA8IQXdJEs",
                "forum": "P50qJuu4IY",
                "replyto": "hOSD1BUbnk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iZps"
                    },
                    "comment": {
                        "value": "> _As I know, DINOv2 and SwAV also use the optimal transport algorithm to solve contrastive learning. But I can not find the discussion about such methods in related work. I would like to find the discussion about the difference between Matching Gap and SwAV/DINO_\n\n\u25a0  Thanks for pointing this out. We agree that it is worth adding a discussion on this, which we did in the updated pdf, paragraph \u201cRelated works\u201d in Section 3. To develop: optimal transport does indeed plays a role in SwAV (Caron et al. 2020) and DINOv2 (Oquab et al. 2023), where optimal transport, and more specifically the *Sinkhorn algorithm*, is used as a differentiable *proxy* to obtain a *balanced* $k$-means clustering of representations (i.e. each cluster must capture a similar amount of mass).  Note that in DINO (Caron et al. 2021), Sinkhorn was replaced with Softmax (similar to a single Sinkhorn iteration), but was later reintroduced in DINOv2, following Ruan et al. (2022). Importantly, in both SwAV and DINOv2, OT is used as a low-level clustering routing, only *indirectly* involved in the final loss, which is a more standard KL-based quantity, on a discretization using those clusters. With that, because OT is used at an intermediate step, this requires, as in Inverse OT (Shi et al. 2023), differentiating the Sinkhorn iterations.\n\n\n> _As for experiments. In Table 1, the performance of the Matching Gap is not as good as DINO, which is a strong baseline proposed 1 year before._\n\n\u25a0  We agree that DINO (Caron et al. 2021) is indeed a strong baseline that builds upon SwAV (Caron et al. 2020), incorporating various tricks and a significant compute budget for hyperparameter optimization. In contrast to DINO, MG only proposes a _novel_ loss function, and we show that only that loss can lead to several simplifications in architecture/training procedure, as shown in the various experiments we conducted. For instance, in contrast to Table 7 \u201cImportant component for self-supervised ViT pretraining.\u201d in (Caron et al. 2021), which presents the importance of all components for training in DINO (with a complete failure in the absence of momentum encoder, row 2), our ablations show that MG is overall very robust, and able to work in various settings.\n\n\n> _The author only conducts downstream experiments on transfer classification. Many self-supervised learning methods evaluate the downstream detection(COOC, VOC) and segmentation(COCO, aed20k) performance. I think the simple downstream classification task is not enough._\n\n\u25a0 We thank the reviewer for making these suggestions. We are now conducting these evaluations and hope to include them in an updated version within the rebuttal period."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056869367,
                "cdate": 1700056869367,
                "tmdate": 1700056869367,
                "mdate": 1700056869367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P7SujRinD8",
                "forum": "P50qJuu4IY",
                "replyto": "vA8IQXdJEs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_iZps"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_iZps"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response. After reading the response and other reviewers' comments. I will keep my original score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486235739,
                "cdate": 1700486235739,
                "tmdate": 1700486235739,
                "mdate": 1700486235739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]