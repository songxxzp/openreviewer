[
    {
        "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"
    },
    {
        "review": {
            "id": "IMKz4ffJeD",
            "forum": "FIplmUWdm3",
            "replyto": "FIplmUWdm3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission54/Reviewer_rXfg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission54/Reviewer_rXfg"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed QLLM for low-bit weight-and-activation quantization of LLMs. QLLM consists of three techniques: channel disassembly to reduce outlier magnitude and channel assembly to maintain channel numbers, an adaptive strategy to find the optimal expansion ratios, and a low-rank fine-tuning method to reduce quantization error. Experiments show that the proposed method outperforms existing quantization methods, especially under low-bit setting (W4A4)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written and easy to follow. \n2. The proposed method outperforms existing ones under low-bit weight and activation quantization (W4A4). \n3. The ablation study is comprehensive, showing the effectiveness of each components (e.g., channel assembly, adaptive expansion rate, etc.)\n4. The efficient fine-tuning method seems like an efficient way to restore performance."
                },
                "weaknesses": {
                    "value": "1. The proposed method has a significant advantage *only* under very low-bit settings like W4A4. However, under W4A4, all methods (including QLLM) cannot achieve a reasonable accuracy. Take Table 1 as an example, the W4A4 LLaMA-65B accuracy of QLLM is only 59.83% (average), which is even lower than the FP16 accuracy of LLaMA-7B, which is 62.23%. The huge drop in accuracy makes the setting impractical. On the other hand, the proposed method does not outperform existing work like OmniQuant under higher precisions (e.g., W6A6). Therefore, it is questionable whether the proposed method has a practical advantage. \n2. The proposed disassembly and assembly method will lead to inevitable inference overhead when there is LayerNorm or activation functions, since extra layers are inserted in the the forward graph (also confirmed in Table 4, 27% overhead compared to INT8 under context length 256, which is negligible). Furthermore, it is unclear what kind of configurations (e.g., disassembly ratios) are used for the INT8 setting in Table 4, since there is no accuracy number reported under this setting. If we use a larger disassembly ratio, will the overhead be larger?\n3. It seems from Table 6 does directly updating the quantized weights (i.e., QAT) is always better than the LoRA tuning. Does it mean if we have enough GPUs for parallelized fine-tuning (e.g., FSDP), we should use QAT for the best accuracy?"
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Reviewer_rXfg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission54/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698280379333,
            "cdate": 1698280379333,
            "tmdate": 1700369725229,
            "mdate": 1700369725229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0d2i8Hzg6W",
                "forum": "FIplmUWdm3",
                "replyto": "IMKz4ffJeD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rXfg"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1.** The proposed method has a significant advantage only under very low-bit settings like W4A4. However, under W4A4, all methods (including QLLM) cannot achieve a reasonable accuracy. On the other hand, the proposed method does not outperform existing work like OmniQuant under higher precisions (e.g., W6A6). Therefore, it is questionable whether the proposed method has a practical advantage.\n\n**A1**. We would like to emphasize that the advantages of our QLLM extend beyond low-bit settings. It is crucial to comprehensively evaluate LLM performance using a variety of metrics, including perplexity (PPL) and zero-shot accuracy, across different benchmarks. Notably, in W6A6 quantization settings, QLLM consistently outperforms OmniQuant in terms of lower PPL on both WikiText2 and C4 across all models, as shown in Tables 1 and A. \n\nAdditionally, our method falls within the post-training quantization settings, where we need to quantize a pre-trained LLM using a small amount of calibration data with little computational overhead. This setup is in line with mainstream LLM quantization methods (Xiao et al., 2023, Wei et al., 2023, Shao et al., 2023) and inherently faces some performance constraints due to the smaller scale of data and training. Notably, in W4A4 settings, QLLM significantly outperforms the concurrent state-of-the-art quantization method, OmniQuant, by a large margin, as shown in Tables 1 and A. Moreover, our QLLM is complementary to large-scale fine-tuning to further improve performance. For example, by using 40 Attention-FFN Blocks for reconstruction and more calibration samples, our 4-bit quantized LLaMA-1-65B achieves **61.04% average accuracy on five zero-shot tasks**, which is **1.21%** higher than the result in the previous submission. Given the time constraints of this rebuttal period, we believe there is potential for more performance gains.\n\n**Q2**. The proposed disassembly and assembly method will lead to inevitable inference overhead when there are LayerNorm or activation functions, since extra layers are inserted in the forward graph (also confirmed in Table 4, 27% overhead compared to INT8 under context length 256, which is negligible). \n\n**A2**. We would like to correct a misconception about the inference cost associated with our QLLM. The actual additional inference cost, when compared to INT8 at a context length of 256, is only 15% rather than 27%, as shown in Table 4 in the initial submission. To further reduce this overhead, we implement our channel disassembly and assembly operation using Triton [A], optimizing these processes for enhanced efficiency. Additionally, the channel expansion ratios determined by our adaptive strategy are kept within a small range (as detailed in Q3), ensuring that the increase in inference overhead remains small. For detailed inference overhead, please refer to Q1 of the general response.\n\n**Reference**\n\n[A] Triton: an intermediate language and compiler for tiled neural network computations. MAPL 2019.\n\n**Q3**. It is unclear what kind of configurations (e.g., disassembly ratios) are used for the INT8 setting in Table 4. If we use a larger disassembly ratio, will the overhead be larger?\n\n**A3**. In Table 4, we set the channel expansion ratio $\\gamma$ to 0.01, which was chosen because it reflects the average channel expansion ratio across various layers. While a larger $\\gamma$ value does lead to increased overhead, our efficient implementation ensures that this increase remains small, as demonstrated in Table I of the general response. Importantly, as illustrated in Figure A, the channel expansion ratios determined by our adaptive strategy do not exceed 0.1. This cap acts as an effective upper bound, ensuring that the additional inference overhead is both predictable and constrained within manageable limits.\n\n**Q4**. It seems from Table 6 does tuning quantized weights directly (TQW) is always better than the efficient error correction (EEC). Does it mean if we have enough GPUs for parallelized fine-tuning (e.g., FSDP), we should use TQW for the best accuracy?\n\n**A4**. While TQW might be preferable in situations where ample GPU resources are available, our EEC shines in resource-limited settings, delivering performance that is on par with TQW. A notable example of EEC's efficiency is its ability to quantize the LLaMA-1-65B model using only a single 24GB consumer-grade GPU, such as the NVIDIA RTX 4090. \nThis efficiency makes EEC especially beneficial for researchers facing resource constraints, markedly improving the accessibility of quantization technology for LLMs. For a comparison that underscores the benefits of EEC over TQW, please refer to Q3 of the general response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361891929,
                "cdate": 1700361891929,
                "tmdate": 1700371611828,
                "mdate": 1700371611828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gDl1X8ibCD",
                "forum": "FIplmUWdm3",
                "replyto": "0d2i8Hzg6W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_rXfg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_rXfg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the informative rebuttal, which addresses part of my concerns. \n\nDespite the performance advantage gap being kind of small under W6A6, I would like to raise my ratings to 6 given other merits."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369863161,
                "cdate": 1700369863161,
                "tmdate": 1700369863161,
                "mdate": 1700369863161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tVUFCMjEfF",
            "forum": "FIplmUWdm3",
            "replyto": "FIplmUWdm3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission54/Reviewer_wUVT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission54/Reviewer_wUVT"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose QLLM, a precise and efficient post-training quantization method specifically designed for Large Language Models (LLMs). They address the challenge of activation outliers by introducing a gradient-free channel reassembly technique that redistributes the magnitudes of outlier channels across all channels. This ensures a more balanced activation range, facilitating accurate quantization and improving the performance of quantized LLMs. The authors also introduce channel assembly to maintain the original channel count by merging similar channels. Additionally, they propose an adaptive strategy to determine the optimal number of disassembled channels for each layer, based on minimizing the reassembly error. The proposed QLLM method achieves accurate quantized models efficiently, as demonstrated by extensive experiments on LLaMA-1 and LLaMA-2 datasets. For example, QLLM outperforms the previous state-of-the-art method by 7.89% on average accuracy across five zero-shot tasks for the 4-bit LLaMA-2-70B model, trained within 10 hours on a single A100-80G GPU."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The concepts of CHANNEL DISASSEMBLY and CHANNEL ASSEMBLY proposed in this paper appear to be novel. The findings presented in Table 2 provide evidence for the effectiveness of CD and CA.\n- The methodology described in this paper is straightforward and comprehensible.\n- The authors have conducted thorough experiments across various settings, which is commendable."
                },
                "weaknesses": {
                    "value": "## Major Concern:\n- While I acknowledge the efficiency of CHANNEL DISASSEMBLY and CHANNEL ASSEMBLY in the form of $y=xW^{l-1}W^{l}$, as explained in \"MORE DETAILS ABOUT THE EFFICIENT IMPLEMENTATION FOR CHANNEL REASSEMBLY\", I have reservations regarding its applicability to scenarios involving multiple inputs $X=\\\\{x_1,x_2,...,x_K\\\\}\\in\\mathbb{R}^{K\\times M}$ and a non-linear transformation $y=\\phi(xW^{l-1})W^{l}$, where $\\phi(\\cdot)$ represents a normalization layer followed by an activation function like GELU. Despite the authors' claim that \"the channel indexes for decomposition and aggregation are calculated offline using calibration data, which only introduces a minor extra overhead\", I remain unconvinced that the overhead is negligible. \n\n## Minor Comments:\n- It would be beneficial to provide a proof demonstrating that the approximation error introduced by Equation (4), in conjunction with the subsequent quantization error, is indeed smaller than the quantization error resulting from direct quantization.\n- I am also curious about the potential occurrence of outliers when employing $quant(W+AB)$ in the equation $y=quant(x)quant(W+AB)$. Additionally, I would appreciate further insights into the quantization error between $quant(X)quant(W+AB)$ and $quant(X)(quant(W)+AB)$."
                },
                "questions": {
                    "value": "- I am interested in whether the proposed CHANNEL DISASSEMBLY and CHANNEL ASSEMBLY methods can be extended to handle multiple inputs, specifically in the case of $Y=XW$ where $X\\in\\mathbb{R}^{batchsize\\times M}$. I have some concerns regarding the effectiveness of Equations (3) and (4) on $X\\in\\mathbb{R}^{batchsize\\times M}$ if the outliers differ among $X_{i,:}$, as the derivation relies on the approximation of the summation of scalars. Additionally, the authors mention that \"with the channel indexes for decomposition and aggregation calculated offline using calibration data\". Did the authors imply that the outliers often occur at the same index across different inputs? Otherwise, the effectiveness of the proposed methods may vary.\n\n- The notation of $\\beta$ in Equation (3) appears to correspond to $\\lfloor\\frac{\\min(X)}{\\alpha}\\rceil$.\n\n- It would be beneficial to include the baseline results of $\\gamma=0$ in Table 2.\n\n- According to Table 6 and the abstract, the training time of efficient error correction is reported to be around 1 hour, while the total time cost is 10 hours, suggesting that the adaptive CD/CA takes approximately 9 hours. Furthermore, the baseline result of tuning quantized weights directly (TQW) with 4 Attn-FFN Blocks seems to be quite strong, as it only takes one and a half hours with much smaller GPU memory overhead compared to the 16 Attn-FFN Blocks EEC results, and it even achieves better performance. Could the authors provide further clarification on this matter?\n\n- Did the FP results reported in Table 4 refer to FP32 or FP16 results?\n\n- I am curious about the baseline results of tuning quantized weights directly (TQW) with EFFICIENT GRADIENT-BASED ERROR CORRECTION. Did this setting outperform the proposed CD/CA + EEC?\n\n- Could the authors provide pseudo-codes to provide more detailed explanations of the \"additional disassembly and assembly layers that are designed to decompose and aggregate channels during runtime\"?\n\n- Could the authors further elaborate on why OmniQuant consistently performs well on W6A6 but performs extremely poorly on LLaMA-2-70B with W4A4?\n\n- It is worth noting that \"DIFFERENT NUMBERS OF CALIBRATION SAMPLES\" play a crucial role in performance. Did the authors follow the same setting as other baseline methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Reviewer_wUVT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission54/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698502002631,
            "cdate": 1698502002631,
            "tmdate": 1700623529930,
            "mdate": 1700623529930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IutDbA9iuc",
                "forum": "FIplmUWdm3",
                "replyto": "tVUFCMjEfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wUVT (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1**. I have reservations regarding its applicability to scenarios involving multiple inputs $\\mathbf{X}= \\\\{ \\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_K \\\\} \\in\\mathbb{R}^{K\\times M}$ and a non-linear transformation $\\mathbf{y}=\\phi(\\mathbf{x}\\mathbf{W}^{l-1})\\mathbf{W}^{l}$, where $\\phi(\\cdot)$ represents a normalization layer followed by an activation function like GELU. Despite the authors' claim that \"the channel indexes for decomposition and aggregation are calculated offline using calibration data, which only introduces a minor extra overhead\", I remain unconvinced that the overhead is negligible.\n\n**A1**. Please refer to Q1 of the general response for the inference overhead of channel reassembly. With our efficient Triton kernel implementation, our channel reassembly only incurs **4% additional runtime overhead** compared with W4A4.\n\n**Q2**. It would be beneficial to provide a proof demonstrating that the approximation error introduced by Equation (4), in conjunction with the subsequent quantization error, is indeed smaller than the quantization error resulting from direct quantization.\n\n**A2**. To further show the effectiveness of our channel reassembly (CR), we compare the average block-wise reconstruction error across the entire network before and after applying CR and show the results on a calibration set with 128 randomly selected 2048-token segments from WikiText2 in Table VIII. The results clearly demonstrate that using CR significantly lowers the reconstruction error, and thus improves the performance of the quantized models. We have included these results and the corresponding discussions in Section G of the supplementary material.\n\nTable VIII. Block-wise reconstruction error before and after channel reassembly (CR).\n| Model       | Method | Reconstruction Error |\n| ----------- | :----: | :------------------: |\n| LLaMA-1-7B  | w/o CR |         4.71         |\n| LLaMA-1-7B  | w/ CR  |       **2.74**       |\n| LLaMA-1-13B | w/o CR |         7.67         |\n| LLaMA-1-13B | w/ CR  |       **1.71**       |\n\n**Q3**. I am also curious about the potential occurrence of outliers when employing $\\mathrm{quant}(\\mathbf{W}+\\mathbf{A}\\mathbf{B})$ in the equation $\\mathbf{Y}=\\mathrm{quant}(\\mathbf{X})\\mathrm{quant}(\\mathbf{W}+\\mathbf{A}\\mathbf{B})$. \n\n**A3**. We empirically observe that merging the low-rank weights into the frozen weights using $\\mathrm{quant}(\\mathbf{W}+\\mathbf{A}\\mathbf{B})$ does not lead to an increase in outliers. This finding is supported by the high cosine similarity (**nearly 0.999**) of the channel-wise maximum/minimum values before and after the merging process. \n\n**Q4**. I would appreciate further insights into the quantization error between $\\mathrm{quant}(\\mathbf{X})\\mathrm{quant}(\\mathbf{W}+\\mathbf{A}\\mathbf{B})$ and $\\mathrm{quant}(\\mathbf{X})(\\mathrm{quant}(\\mathbf{W})+\\mathbf{A}\\mathbf{B})$.\n\n**A4**. As explained in Section 4.2, for $\\mathrm{quant}(\\mathbf{X})(\\mathrm{quant}(\\mathbf{W})+\\mathbf{A}\\mathbf{B})$, the low-rank weights $\\mathbf{A}$ and $\\mathbf{B}$ bring not only additional inference overhead due to the matrix multiplication between the full-precision $\\mathbf{A}\\mathbf{B}$ and $\\mathrm{quant}(\\mathbf{X})$ but also extra storage burden. To address this, we perform weight merging by $\\mathrm{quant}(\\mathbf{W}+\\mathbf{A}\\mathbf{B})$ after the reconstruction, which effectively avoids overhead but introduces additional quantization error. For 4-bit quantization, our empirical findings show that the average cosine similarity between $\\mathrm{quant}(\\mathbf{W})+\\mathbf{A}\\mathbf{B}$ and $\\mathrm{quant}(\\mathbf{W}+\\mathbf{A}\\mathbf{B})$ across the network is **over 0.999**, indicating a small quantization error. Additionally, we further employ sequential reconstruction to mitigate errors from previous layers, resulting in only a negligible performance drop. To demonstrate this, we compare the performance of QLLM with and without the weight merging. From Table IX, the weight merging only leads to a slight increase in perplexity. We have included the above discussions and corresponding results in Section L of the supplementary material.\n\nTable IX. Effect of the weight merging (WM) in the efficient error correction. We report the perplexity on WikiText2, PTB and C4.\n| Model       | Method | WikiText2 |  PTB  |  C4   | Avg.  |\n| ----------- | :----: | :-------: | :---: | :---: | :---: |\n| LLaMA-1-7B  | w/o WM |   9.35    | 15.93 | 11.93 | 12.40 |\n| LLaMA-1-7B  | w/ WM  |   9.65    | 16.56 | 12.29 | 12.83 |\n| LLaMA-1-13B | w/o WM |   8.29    | 13.69 | 10.40 | 10.79 |\n| LLaMA-1-13B | w/ WM  |   8.41    | 14.38 | 10.58 | 11.12 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361392569,
                "cdate": 1700361392569,
                "tmdate": 1700622374619,
                "mdate": 1700622374619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KXwZ2aALri",
                "forum": "FIplmUWdm3",
                "replyto": "tVUFCMjEfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wUVT (Part 2)"
                    },
                    "comment": {
                        "value": "**Q5**. I am interested in whether the proposed channel disassembly and channel assembly methods can be extended to handle multiple inputs. I have some concerns regarding the effectiveness of Equations (3) and (4) on $\\mathbf{X}\\in\\mathbb{R}^{batchsize\\times M}$ if the outliers differ among $\\mathbf{X}_{i,:}$, as the derivation relies on the approximation of the summation of scalars. Did the authors imply that the outliers often occur at the same index across different inputs?\n\n**A5**. Our channel disassembly and assembly are indeed designed to work with different inputs, effectively identifying outliers and similar channels. As we have highlighted in Section 1, drawing on recent findings from studies (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2023), activation outliers are often found to cluster within specific channels across a variety of inputs. This property is reflected in similar channels for channel assembly as well, which tend to exhibit consistent patterns irrespective of the input. Such uniformity allows for the identification of these channels with a small set of calibration data. We have included the discussions in Section 4.3 of the revised manuscript.\n\n**Q6**. The notation of $\\beta$ in Eq. (2) appears to correspond to $\\lfloor\\frac{\\min(X)}{\\alpha}\\rceil$.\n\n**A6**. We confirm that the notation of the zero-point $\\beta$ in Eq. (2) is correct, which serves the purpose of offsetting the range of values to enable storage of quantized values in an unsigned integer format.\n\n**Q7**. It would be beneficial to include the baseline results of $\\gamma=0$ in Table 2.\n\n**A7**. We would like to clarify that the baseline results for $\\gamma=0$ have been included in the first row of Table 2. To enhance clarity and prevent any potential misunderstandings, we have updated the presentation of these results in our revised manuscript as follows.\n\n|  CD   |  CA   |  CP   | Adaptive | $\\gamma$ | WikiText2 |  PTB   |   C4   |  Avg.  |\n| :---: | :---: | :---: | :------: | :------: | :-------: | :----: | :----: | :----: |\n|   \u2713   |       |       |          |    0     |  189.35   | 539.59 | 303.45 | 344.13 |\n\n**Q8**. According to Table 6 and the abstract, the training time of efficient error correction is reported to be around 1 hour, while the total time cost is 10 hours, suggesting that the adaptive channel disassembly/channel assembly takes approximately 9 hours.\n\n**A8**. We would like to clarify a misunderstanding regarding the training time. The time reported in Table 6 is for the 4-bit LLaMA-1-7B model, while the abstract shows the training time for the 4-bit LLaMA-2-70B model. Additionally, as discussed in Section 4.3, our channel disassembly and assembly process is highly training efficient, largely due to its gradient-free design. To illustrate, the channel disassembly and assembly for the 4-bit LLaMA-1-7B require only **9.6 minutes**. \n\n**Q9**. The baseline result of tuning quantized weights directly (TQW) with 4 Attn-FFN Blocks seems to be quite strong, as it only takes one and a half hours with much smaller GPU memory overhead compared to the 16 Attn-FFN Blocks efficient error correction (EEC) results, and it even achieves better performance. Could the authors provide further clarification on this matter?\n\n**A9**. We would like to clarify that both TQW and EEC are training strategies to further improve the performance of the quantized LLMs **after adaptive channel reassembly**. While TQW shows strength in certain scenarios, EEC offers significant advantages, particularly in resource efficiency. EEC specifically targets learning a limited set of low-rank weights, thereby reducing training costs and GPU memory requirements. This approach enables EEC to quantize larger models like the LLaMA-1-65B on a single 24GB GPU, a task not feasible with TQW. For a detailed comparison of EEC's advantages over TQW, please see Q3 of the general response.\n\n**Q10**. Did the FP results reported in Table 4 refer to FP32 or FP16 results?\n\n**A10**. The FP results reported in Table 4 refer to FP16 results. We have revised our manuscript.\n\n**Q11**. I am curious about the baseline results of tuning quantized weights with efficient error correction (EEC). Did this setting outperform the proposed CD/CA + EEC?\n\n**A11**. Please refer to Q2 of the general response for the detailed results. Using EEC only without channel reassembly results in poor performance as it suffers from activation outlier issues."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361720272,
                "cdate": 1700361720272,
                "tmdate": 1700371837351,
                "mdate": 1700371837351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8B71M26U1o",
                "forum": "FIplmUWdm3",
                "replyto": "tVUFCMjEfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wUVT (Part 3)"
                    },
                    "comment": {
                        "value": "**Q12**. Could the authors provide pseudo-codes to provide more detailed explanations of the \"additional disassembly and assembly layers that are designed to decompose and aggregate channels during runtime\"?\n\n**A12**. To provide a clearer understanding, we include the pseudo-codes of our disassembly and assembly during runtime in PyTorch style as follows: \n\n```python\ndef channel_disassembly(x, num_split):\n\"\"\" \n    x: input with shape of [batch, tokens, channels]\n    num_split: the number of sub-channels for each channel with shape of [channels]\n\"\"\"\n     \n    B, N, C = x.shape\n    x = x.view(B * N, C)\n    scaling = 1.0 / num_split   # compute the scaling factor of each channel\n    x = x / scaling                    # scale each channel\n    x = torch.repeat_interleave(x, num_split, dim=1) # perform channel decomposition\n    C = x.shape[1]\n    x = x.view(B, N, C)\n    return x\n```\n\n``` python\ndef channel_assembly(x, src_idx, dst_idx):\n\"\"\" \n    x: input with shape of [batch, tokens, channels] \n    src_idx: the channel index that will be merged in set A with shape of [#num_merged_channels]\n    dst_idx: the channel index that will be merged in set B with shape of [#num_merged_channels]\n\"\"\"\n    B, N, C = x.shape\n    ori_src_idx = torch.arange(0, C, 2, device=x.device)  # get index for set A\n    ori_dst_idx = torch.arange(1, C, 2, device=x.device)  # get index for set B\n    src, dst = x[..., ori_src_idx], x[..., ori_dst_idx]    # divide the channels into two sets A and B\n    src_C = src.shape[-1]    # get the channel number in set A\n    dst_C = dst.shape[-1]    # get the channel number in set B\n    \n    # A mask that indicates whether a channel is merged\n    channel_mask = torch.ones(C, device=x.device, dtype=x.dtype)\n    m_idx = ori_src_idx[src_idx]\n    channel_mask[m_idx] = 0.0\n\n    n, t1, c = src.shape\n    sub_src = src.gather(dim=-1, index=src_idx.expand(n, t1, r))   # get channels that will be merged in set A \n    dst = dst.scatter_reduce(-1, dst_idx.expand(n, t1, r), sub_src, reduce=mode)  # merge channels\n    src = src.view(B, N, src_C, 1)\n    dst = dst.view(B, N, dst_C, 1)\n\n    # concat set A and set B\n    if src_C == dst_C:\n        merged_x = torch.cat([src, dst], dim=-1).view(B, N, C)\n    else:\n        merged_x = torch.cat([src[..., :-1, :], dst], dim=-1).view(\n            B, N, src_C + dst_C - 1\n        )\n        merged_x = torch.cat([merged_x, src[..., -1, :].reshape(B, N, 1)], dim=-1).view(\n            B, N, src_C + dst_C\n        )\n    # remove the merged channels\n    merged_x = merged_x.index_select(-1, (channel_mask != 0).nonzero().squeeze())\n    return merged_x\n```\nWe have included the above pseudo-codes in Section D of the supplementary material.\n\n**Q13**. Could the authors further elaborate on why OmniQuant consistently performs well on W6A6 but performs extremely poorly on LLaMA-2-70B with W4A4?\n\n**A13**. The varied performance of OmniQuant can be attributed to the architecture of LLaMA-2-70B, which employs grouped-query attention (Ainslie et al., 2023) where each group of queries shares a single key and value head. Such architecture makes the learnable equivalent transformation in OmniQuant incompatible with grouped-query attention. In W6A6 settings, the impact of activation outliers is relatively minor, enabling partial learnable equivalent transformations to suffice in maintaining performance. However, in the W4A4 settings, the effect of activation outliers becomes more prominent. Under these conditions, the partial learnable equivalent transformation is insufficient to address the outlier issue, leading to notably poorer performance. We have included the analysis in Section E of the appendix.\n\n**Q14**. Did the authors follow the same number of calibration samples as other baseline methods?\n\n**A14**. Yes, we use the same number of calibration samples as OmniQuant, as already mentioned in the implementation details of Section 5."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361816540,
                "cdate": 1700361816540,
                "tmdate": 1700376394404,
                "mdate": 1700376394404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3uQwZ5F2Ho",
                "forum": "FIplmUWdm3",
                "replyto": "spdQi2taXE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_wUVT"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission54/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission54/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission54/Reviewers",
                    "ICLR.cc/2024/Conference/Submission54/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_wUVT"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to All Reviewers (Part 2)"
                    },
                    "comment": {
                        "value": "I would like to express my gratitude to the authors for providing detailed feedback. However, I still have some concerns regarding the effectiveness of the proposed method. \n- Firstly, the reported results of QLLM in Table II of **A2** appear to be unconvincing. It is noteworthy that the perplexity of W4A4 LLaMA-1-7B on WikiText2, PTB, and C4 with QLLM is exactly the same as the results of TQW with 4 attn-FFN block tuning, as reported in Table 6 of the original submission. Is this merely a coincidence?\n\n- Secondly, I agree with the authors' statement that \"the efficiency advantages of EEC are particularly notable in the context of larger models.\" However, the authors have not mentioned that the performance drop between TQW and the proposed EEC also becomes notable in the context of large models. Additionally, it seems that the TQW 4-bit model exhibits faster runtime speed compared to QLLM. Furthermore, the training setting of TQW is much simpler than the proposed two-step tuning. Considering that the quantization process is commonly deployed on cloud servers, I personally believe that the 12.13 GPU hours required for TQW are entirely acceptable.\n\n- Lastly, I would like to inquire whether the \"LLaMA-1-7B W4A16\" model still suffers from the outlier issue. In my opinion, the A4/A16 mix-precision setting could potentially serve as a strong baseline method."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533676935,
                "cdate": 1700533676935,
                "tmdate": 1700623558733,
                "mdate": 1700623558733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XHJ5cQRnhR",
            "forum": "FIplmUWdm3",
            "replyto": "FIplmUWdm3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission54/Reviewer_KacJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission54/Reviewer_KacJ"
            ],
            "content": {
                "summary": {
                    "value": "This work, QLLM, mainly addresses activation outliers for quantizing LLMs. QLLM proposes to first break the outlier channels into several channels, and then merge similar channels to keep the original channel number (by bipartite soft matching). To decide how many channels an outlier channel needs to be broken into, QLLM conducts a grid search of the outlier threshold hyper-parameter $\\theta$ using layer-wise reconstruction error as the objective. Then the channel-reassembled network is more suitable for activation quantization. Finally, QLLM adopts LoRA training with block-wise reconstruction error as the objective to restore performance. Experiments are conducted with LLaMA v1 and v2 models. Evaluations are conducted on WikiText, PTB, and C4 for PPL, and 5 zero-shot benchmarks for zero-shot accuracy. The algorithm performance experiments use the W6A6 and W4A4 settings, and the inference efficiency experiments use the W8A8 setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper is well-written, well-organized, and easy to follow.\n* Motivation: The activation outlier issue is important for quantizing activations in LLMs\n* Reasonable method: Channel splitting is a reasonable method to address the activation outlier issue."
                },
                "weaknesses": {
                    "value": "My major concern in this paper is whether its current evaluation can fully demonstrate its practicability.\n\n**About the inference efficiency**\n* While the algorithm perf. experiments are conducted with W6A6 and W4A4, inference efficiency experiments are only conducted with W8A8. I think implementing W4A4 kernels on some NVIDIA GPUs and demonstrating better efficiency-algo. perf. trade-off compared with W8A8 and existing W4A4 will make this paper stronger.\n* Table 4 shows QLLM has inference overhead compared to methods without channel reassembly. Does this overhead come from the additional disassembly and assembly layers for non-linear layers? A thorough breakdown of this overhead for differently-sized models (larger models, more compressed models such as 4bit) will help illustrate whether the method is really practical.\n* As the channel-assembly process introduces additional approximations, only applying the channel-disassembly process results in the best perplexity (which is shown in Table 2). So, to further demonstrate the necessity of applying channel assembly to keep the channel number fixed, the authors can show the inference efficiency when using the expanded channel number (only apply CD).\n\n**About the algorithm performance**\n* I'm curious how much influence the reassembly process still has as the training data amount goes up. I.e., do the authors compare with using only reconstruction-based LoRA tuning without CD, CA, and CP, under exactly the same training settings but maybe with more calibration data (e.g., 256 instead of 128).\n* A suggestion instead of a weakness: To raise more interest in using this method, I recommend doing some evaluations on chat models."
                },
                "questions": {
                    "value": "* Important points are listed in the weakness section.\n* Sec 4.1.1 uses $\\max(x_M)$ to determine the number of disassembly channels $T$ according to $\\theta$, why don't we use $\\max(|x_M|)$?\n* I wonder whether the CD results in Table 2 use the LoRA tuning? I would like to see the results of using CD without LoRA tuning as a reference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Reviewer_KacJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission54/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662411810,
            "cdate": 1698662411810,
            "tmdate": 1700534779036,
            "mdate": 1700534779036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fUbOCTFmUE",
                "forum": "FIplmUWdm3",
                "replyto": "XHJ5cQRnhR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KacJ (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1**. I think implementing W4A4 kernels on some NVIDIA GPUs and demonstrating better efficiency-algo. perf. trade-off compared with W8A8 and existing W4A4 will make this paper stronger.\n\n**A1**. Thanks for your valuable suggestion. Please refer to Q1 of the general response for the inference time results.\n\n**Q2**. Does the overhead come from the additional disassembly and assembly layers for non-linear layers? A thorough breakdown of this overhead for differently-sized models (larger models, more compressed models such as 4bit) will help illustrate whether the method is really practical.\n\n**A2**. Yes, the additional inference overhead comes from the additional disassembly and assembly layers after the non-linear layers. Please refer to Q1 of the general response for a thorough breakdown of the inference overhead for differently-sized models. Additionally, we further show the inference overhead of QLLM for 4-bit LLaMA-2-70B. From Table V, our QLLM introduces only a **2% additional overhead** compared with 4-bit fixed-point baseline.\n\nTable V. Inference throughput (tokens/s) comparisons of different models. The throughput is measured with a 2048-token segment on 8 NVIDIA RTX 4090 GPUs.\n| Model | Method | Inference Throughput (tokens/s) |\n| - | - | :-: |\n| LLaMA-2-70B | FP16 | 824 |\n| LLaMA-2-70B | W8A8 | 1406 |\n| LLaMA-2-70B | W4A4 | 1677 |\n| LLaMA-2-70B | W4A4 | 1638 |\n\n**Q3**. To further demonstrate the necessity of applying channel assembly to keep the channel number fixed, the authors can show the inference efficiency when using the expanded channel number (only apply CD).\n\n**A3**. Please refer to Q1 of the general response for the inference efficiency with and without channel assembly. Our results show that applying both channel disassembly and assembly leads to higher throughput compared to using disassembly alone as it is more hardware-friendly.\n\n**Q4**. I'm curious how much influence the reassembly process still has as the training data amount goes up. I.e., do the authors compare with using only efficient error correction (EEC) without CD, CA, and CP, under exactly the same training settings but maybe with more calibration data (e.g., 256 instead of 128).\n\n**A4**. Please refer to Q2 of the general response for the detailed results. Using EEC only without channel reassembly results in poor performance as it suffers from activation outlier issues.\n\n**Q5**. A suggestion instead of a weakness: To raise more interest in using this method, I recommend doing some evaluations on chat models.\n\n**A5**. Following your suggestion, we apply QLLM to quantize LLaMA-2-7B-Chat and LLaMA-2-13B-Chat to 4-bit. We compared our models with the concurrent state-of-the-art quantization method, OmniQuant. We use GPT-4 to assess the performance of the quantized models on a set of 80 sample questions in the Vicuna benchmark [A]. To eliminate the potential position bias [B], we conducted the comparisons in both orders (a vs. b and b vs. a) for each pair, amounting to a total of 160 trials. From Table VI, our QLLM consistently achieves much better performance than OmniQuant, demonstrating the effectiveness of our method on chat models. We have included the results and corresponding discussions in Section F of the revised manuscript.\n\nTable VI. Performance comparisons between QLLM and OmniQuant for chat models.\n| Model            |        Case        | Former Win |  Tie  | Former Lost |\n| ---------------- | :----------------: | :--------: | :---: | :---------: |\n| LLaMA-2-7B-Chat  | QLLM vs. OmniQuant |  **137**   |  19   |      4      |\n| LLaMA-2-13B-Chat | QLLM vs. OmniQuant |  **116**   |  24   |     20      |\n\n\n**Reference**\n\n[A] Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. https: //lmsys.org/blog/2023-03-30-vicuna/.\n\n[B] Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv 2023.\n\n**Q6**. Sec 4.1.1 uses $\\mathrm{max}(x_{M})$ to determine the number of disassembly channels $T$ according to $\\theta$, why don't we use $\\mathrm{max}(| x_{M} |)$?\n\n**A6**. We appreciate your attention. To clarify, we indeed utilize $\\mathrm{max}(| x_{M} |)$ to determine the number of disassembly channels $T$ based on $\\theta$. We have updated our manuscript accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361185321,
                "cdate": 1700361185321,
                "tmdate": 1700529808286,
                "mdate": 1700529808286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EAhhqiEdCr",
                "forum": "FIplmUWdm3",
                "replyto": "XHJ5cQRnhR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KacJ (Part 2)"
                    },
                    "comment": {
                        "value": "**Q7**. I wonder whether the CD results in Table 2 use efficient error correction (EEC)? I would like to see the results of using channel disassembly without EEC as a reference.\n\n**A7**. The channel disassembly (CD) results presented in Table 2 indeed incorporate our EEC. To further demonstrate the effectiveness of CD, we apply CD without EEC to obtain 4-bit LLaMA-1-13B and show the results in Table VII. We observe that the absence of both CD and EEC leads to a significant decline in the performance of the quantized model. Notably, using CD alone substantially reduces the performance degradation associated with quantization. Moreover, increasing the channel expansion ratio $\\gamma$ further improves the model's performance, which strongly shows the benefits of using CD to decompose the outlier channels. By incorporating both CD and EEC, the performance improvement is even more pronounced, underscoring the efficacy of EEC in conjunction with CD. We have included the results and discussions in Section H of the supplementary material.\n\nTable VII. Perplexity results of channel disassembly (CD) with and without efficient error correction (EEC). \u201c$\\gamma$\u201d is the channel expansion ratio. We report the perplexity of W4A4 LLaMA-1-13B on WikiText2, PTB and C4.\n| CD  |  EEC  | $\\gamma$ | WikiText2 |    PTB    |    C4     |   Avg.    |\n| --- | :---: | :------: | :-------: | :-------: | :-------: | :-------: |\n|     |       |    -     |  1702.34  |  1853.58  |  1159.41  |  1571.78  |\n| \u2713   |       |   0.01   |   19.34   |   45.36   |   23.25   |   29.32   |\n| \u2713   |   \u2713   |   0.01   | **8.31**  | **14.44** | **10.74** | **11.16** |\n| \u2713   |       |   0.03   |   12.11   |   24.73   |   14.38   |   17.07   |\n| \u2713   |   \u2713   |   0.03   | **8.01**  | **13.52** | **10.27** | **10.60** |\n| \u2713   |       |   0.05   |   11.4    |   23.53   |   13.62   |   16.18   |\n| \u2713   |   \u2713   |   0.05   | **7.85**  | **13.38** | **10.13** | **10.45** |\n| \u2713   |       |   0.07   |   11.13   |   23.47   |   13.45   |   16.02   |\n| \u2713   |   \u2713   |   0.07   | **7.81**  | **13.35** | **10.11** | **10.42** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361235781,
                "cdate": 1700361235781,
                "tmdate": 1700529891090,
                "mdate": 1700529891090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UtiEt5Y3K2",
                "forum": "FIplmUWdm3",
                "replyto": "XHJ5cQRnhR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer KacJ\n\nWe sincerely appreciate your considerable efforts in reviewing our paper. We have provided responses to your concerns, particularly regarding the inference efficiency of QLLM and morel results on the efficient error correction only. If there are any further concerns or questions, please do not hesitate to let us know.\n\nBest regards,\n\nAuthors of #54"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530556743,
                "cdate": 1700530556743,
                "tmdate": 1700530575859,
                "mdate": 1700530575859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PExity92vq",
                "forum": "FIplmUWdm3",
                "replyto": "UtiEt5Y3K2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_KacJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_KacJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response and the added experiment. It addresses most of my concerns. I'm raising the score to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534757125,
                "cdate": 1700534757125,
                "tmdate": 1700534757125,
                "mdate": 1700534757125,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rTIoug7iAi",
            "forum": "FIplmUWdm3",
            "replyto": "FIplmUWdm3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission54/Reviewer_dSZP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission54/Reviewer_dSZP"
            ],
            "content": {
                "summary": {
                    "value": "The paper is trying to address the challenge of LLM quantization associated with the well-known \"channel-specific outlier problem\", i.e. some specific channels in activations tend to have much larger range compared to the others, which will cause difficulty in choosing quantization scaling factor and degradation in accuracy. The main solution proposed is to disassemble an outlier channel into T channels, therefore, the magnitude of each channel becomes 1/T of the original channel. In order not to increase the computational cost by too much, the second part of the proposed method is to search for similar channels and merge them by averaging the activations and summation of the corresponding weights. As a result, total number of channels will be the same as the original tensors after these channel disassembly and assembly steps. Furthermore, a LORA/QLORA-like tuning is applied to greatly alleviate the cost of PTQ while enabling simultaneous tuning of multiple blocks, which is critical to improve model performance (perplexity). Finally, the inference benchmark results show that the proposed method will add ~20% overhead compared to INT8."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed channel decomposition/disassembly method is simple and effective.\n2. The method to determine disassembly threshold (theta) is reasonable and computationally affordable.\n3. Outperforms previous methods at W4A4 on Llama-1 and Llama-2.\n4. acceptable overhead compared to INT8."
                },
                "weaknesses": {
                    "value": "1. W6A6 results are included but only referenced a few times, mostly used in a sentence like \"QLLM is comparable with other methods at W6A6.\" Given that authors in general would like to impress readers by \"being better than others\" rather than \"being comparable to others,\" W6A6 really doesn't make a strong point in the main article. Plus W6A6 has almost no real HW support... The author might want to consider adding a few more comments/discussion on W6A6 results or moving them to appendix and make the main article more concise.  \n2. A little clarification about the inference time benchmark would be helpful. For example, readers might be interested in comparing the QLLM results with other quantization implementations, like weight-only quantization W4A16. Take auto-gptq (https://github.com/PanQiWei/AutoGPTQ/tree/main#inference-speed) as a reference, the \"speed\" usually is in the unit of token/sec or msec/token. Table 4 only says \"inference time\" and the unit is ms, which is a little unclear."
                },
                "questions": {
                    "value": "1. Table 2 shows \"Channel Disassembly only\" approach with just 1% of channel expansion ratio can already achieve comparable results with the final method. In fact, the reason to use channel assembly is mainly to reduce the computational cost. Author may want to add some comments or examples regarding the overhead incurred by this 1% extra channels, in order to justify the need for Channel Assembly method.  \n2. The paragraph of \"Inference efficiency\" as well as Table 4 didn't specify whether this \"FP\" model is FP32 or FP16. If it's FP32, author may also want to include FP16 results since FP16 is used as the baseline on accuracy tables. Also it is understandable that the implementation for INT computation may not be optimized. Therefore, instead of comparing the absolute run time, it would be helpful to include the additional Ops of QLLM compared to INT8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission54/Reviewer_dSZP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission54/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731763140,
            "cdate": 1698731763140,
            "tmdate": 1699635929448,
            "mdate": 1699635929448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "opxZa8ZTil",
                "forum": "FIplmUWdm3",
                "replyto": "rTIoug7iAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dSZP"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments.\n\n**Q1**. W6A6 really doesn't make a strong point in the main article. Plus W6A6 has almost no real HW support. The author might want to consider adding a few more comments/discussions on W6A6 results or moving them to the appendix and making the main article more concise.\n\n**A1**. Thanks for your advice. Following your suggestion, we expanded our discussions on W6A6 results. We acknowledge that W6A6 has limited hardware support in real-world applications. However, our QLLM still demonstrates performance benefits in these settings. For example, QLLM consistently surpasses the state-of-the-art quantization method in terms of lower perplexity across all models on both WikiText2 and C4 and achieves comparable accuracy on 5 zero-shot tasks, as shown in Tables 1 and A. While the absolute performance gains with 6-bit quantization might seem modest, this is partly due to the less pronounced effect of activation outliers at this bitwidth. Our approach's benefits are more pronounced in lower bitwidth scenarios, especially under W4A4 scenarios, as demonstrated in Tables 1, 5, and A. We have included the discussions in Section 5.1 of the revision.\n\n**Q2**. A little clarification about the inference time benchmark would be helpful. For example, readers might be interested in comparing the QLLM results with other quantization implementations, like weight-only quantization W4A16.\n\n**A2**. Following your suggestion, we compare our QLLM with weight-only quantization W4A16. Please refer to Q1 of the general response for the inference time results. Notably, even when compared to the already efficient W4A16, our QLLM still achieves a **1.1$\\times$ speedup**, underscoring its superior efficiency.\n\n**Q3.** Author may want to add some comments or examples regarding the overhead incurred by 1% extra channels from channel disassembly, in order to justify the need for channel assembly method.\n\n**A3**. Please refer to Q1 in the general response.\n\n**Q4**. The paragraph of \"Inference efficiency\" as well as Table 4 didn't specify whether this \"FP\" model is FP32 or FP16.\n\n**A4**. The FP in Table 4 denotes the FP16 model. We have revised our manuscript.\n\n**Q5**. It would be helpful to include the additional Ops of QLLM compared to INT8.\n\n**A5**. Following your suggestion, we measure the computational complexity of the quantized models using the Bit-Operation (BOP) count [A]. From Table IV, our 8-bit QLLM introduces only a slight increase in BOPs compared to the INT8 model but substantially lower than those of the FP16 counterpart. We have included the results and discussions in Section M of the supplementary material.\n\nTable IV. BOPs comparisons of different models. We report the results of LLaMA-1-7B with a mini-batch size of 1.\n| $L$  |      256      |      512      |     1024      |      2048      |\n| ---- | :-----------: | :-----------: | :-----------: | :------------: |\n| FP16 |    875.52T    |   1,766.40T   |   3,604.48T   |   7,493.12T    |\n| INT8 |    231.58T    |    467.64T    |    952.56T    |    1976.16T    |\n| QLLM | 231.58T+1.07M | 467.64T+2.14M | 952.56T+4.28M | 1976.16T+8.56M |\n\n**Reference**\n\n[A] Differentiable Joint Pruning and Quantization for Hardware Efficiency. ECCV 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361077490,
                "cdate": 1700361077490,
                "tmdate": 1700622339337,
                "mdate": 1700622339337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AXvLnTmC5f",
                "forum": "FIplmUWdm3",
                "replyto": "opxZa8ZTil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_dSZP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission54/Reviewer_dSZP"
                ],
                "content": {
                    "comment": {
                        "value": "My rating remains the same for this paper. Thanks for all the explanations, responses, and new results!"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission54/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596635996,
                "cdate": 1700596635996,
                "tmdate": 1700596635996,
                "mdate": 1700596635996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]