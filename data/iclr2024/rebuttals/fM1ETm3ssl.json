[
    {
        "title": "Towards Meta-Models for Automated Interpretability"
    },
    {
        "review": {
            "id": "0G8AKfi1X4",
            "forum": "fM1ETm3ssl",
            "replyto": "fM1ETm3ssl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7884/Reviewer_aCZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7884/Reviewer_aCZg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use meta-models to understand the internal properties of neural networks. The application includes backdoor detection and data distribution identification."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "See weaknesses."
                },
                "weaknesses": {
                    "value": "Though this paper focuses on an interesting topic, it is less than 9 pages and the amount of work is not enough."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675125170,
            "cdate": 1698675125170,
            "tmdate": 1699636967193,
            "mdate": 1699636967193,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ptXLLGIOox",
                "forum": "fM1ETm3ssl",
                "replyto": "0G8AKfi1X4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We believe research should be judged by the value of its contribution, not by quantity of work or number of pages. Could you please let us know if there are specific weaknesses in our experiments, or follow-up work that would be appropriate for us to do?\n\nWhile it is true that our results do not require much page-space to present (especially as we deferred many details to the appendices for conciseness), they provide evidence that meta-models can potentially be used on a variety of problems of previous interest.\n\nIt is also true that, as other reviewers have noted, we included to little detail at times - e.g. we failed to include example RASP programs that can be reconstructed by our meta-model. We aim to address these weaknesses in our revisions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261968804,
                "cdate": 1700261968804,
                "tmdate": 1700334235988,
                "mdate": 1700334235988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sOPCrC9bUP",
            "forum": "fM1ETm3ssl",
            "replyto": "fM1ETm3ssl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7884/Reviewer_HFmY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7884/Reviewer_HFmY"
            ],
            "content": {
                "summary": {
                    "value": "This research explores how meta-models can be used to interpret neural network models. The goal of the work is to improve the interpretability of complicated neural architectures by utilizing meta-models. By recovering RASP programs from model weights and using meta-models for backdoor identification in these basic models, they experiment to try and reverse-engineer neural networks. Amazingly, their solution outperforms numerous other approaches already in use, achieving over 99% accuracy in backdoor identification. The results signify a noteworthy advancement in the transparency and interpretability of neural networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tContribution to Open Source: The paper mentioned making datasets available for future work. Contributing to open-source ensures that the broader scientific community can benefit from, replicate, and build upon their work.\n2.\tImproved Interpretability: The research showcases a novel approach to recovering a program from transformer weights.\n3.\tHigh Accuracy: In their testing, the meta-model achieved impressive accuracy. Such high accuracy showcases the effectiveness and reliability of their approach."
                },
                "weaknesses": {
                    "value": "1.\tDependence on Large Amounts of Training Data: Their method relies heavily on having a significant amount of training data, which might not always be feasible for every application.\n2.\tModel Size: As mentioned in this paper, the models they worked with are relatively small, with an average of only 3,000 parameters. This contrasts with much larger models often used in deep learning, which can have millions or even billions of parameters. It is much better to see the performance of larger models.\n3.\tScope of Tasks: The tasks they tested are simpler compared to the full problem of reverse-engineering a large neural network."
                },
                "questions": {
                    "value": "1.\tThe baseline in this paper is not enough to show the performance of their methods, adding more baselines will be better. \n2.\tI hope this method can be tested on a larger model and prove to be effective."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814009822,
            "cdate": 1698814009822,
            "tmdate": 1699636967042,
            "mdate": 1699636967042,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F55PZY8xSg",
                "forum": "fM1ETm3ssl",
                "replyto": "sOPCrC9bUP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Summary of our respones\n\nThank you for your review. We\u2019re also excited to open-source our models and datasets, as we expect that a lot of progress can be made on meta-models as a research community develops for the domain, and we hope that the interpretability gains will be substantial.\n\n**Dependence on large amounts of training data.**\n\nReliance on training data is indeed a limitation we discuss in Section 5. However, we do not think that this is a weakness of our work, rather than a limitation of the meta-model framework. We note that we achieve strong results in the backdoor detection setting training on only 4-6 thousand datapoints. In addition, we propose several ways in which meta-models could potentially be scaled up in relatively more data-efficient ways.\n\n**Size of base models is too small.**\n\nIn Section 3.1 we show meta-models successfully classify base models up to 2 million parameters in size. We expect that meta-models can be scaled further, but this is out of scope for this paper.\n\n**Scope of tasks.**\n\nWe agree that this is a limitation, and we discussed it in the limitations section. We think we have demonstrated an appropriate amount of potential for what our paper aims to be: a solid proof-of-concept.\n\n**Not enough baselines.**\n\nWe are currently running more experiments. Note: we already compare against several baselines and exceed existing state-of-the-art performance.\n\n## Weakness #1\n\n> Dependence on Large Amounts of Training Data: Their method relies heavily on having a significant amount of training data, which might not always be feasible for every application.\n\nIt is true that reliance on training data is a limitation of the meta-model approach. While a meta-model may not need a large number of datapoints in an absolute sense, the training data may be expensive to obtain if each datapoint involves training a large model. Scaling to massive modern language or vision models is an important question, but out of scope for this work. Ways in which we might overcome the scaling challenge in future work include:\n1. Pretraining, e.g. masked weight prediction or contrastive learning. This might allow finetuning on new tasks with fewer examples.\n2. Experiments on generalization capabilities across scale. If a meta-model can (partially) generalize from small to large networks, then we may be able to reduce the number of large networks in the training data.\n3. Applying meta-models to subsections of a target network. Depending on the target task, a single network might yield many datapoints, for example if individual circuits within a network can be mapped to specific functions.\n\nWe have edited the paper to discuss these points (see Section 5).\n\n## Weakness #2 \n\n> Model Size: As mentioned in this paper, the models they worked with are relatively small, with an average of only 3,000 parameters. This contrasts with much larger models often used in deep learning, which can have millions or even billions of parameters. It is much better to see the performance of larger models.\n\nThis is an inaccurate representation of our experiments. Only the Tracr models had an average of 3,000 parameters. In the backdoors detection setting, models had around 70,000 parameters. In the hyperparameter-prediction setting, models had an average of 560,000 parameters, with the largest models up to 2,000,000 parameters.\n\n\n## Weakness #3\n\n> Scope of Tasks: The tasks they tested are simpler compared to the full problem of reverse-engineering a large neural network.\n\nYou are right that we still have a long way to go. However, the problem of fully reverse engineering neural networks is the subject of a large body of work and remains unsolved. As such, while this is a correct characterization that we acknowledge in Limitations, we do not think it is a weakness.\n\nWhile the tasks we work on are relatively straightforward, we hope that they demonstrate potential in this field. What further experiments would you be interested to see?\n\n\n## Questions\n\n> The baseline in this paper is not enough to show the performance of their methods, adding more baselines will be better.\n\nIt would be useful if you could clarify which baselines are missing. We compare against leading methods ([1] for hyperparameter prediction, [2,3] for backdoor detection), and achieve are state-of-the-art results. Do you have recommendations for how to proceed?\n\nNote: we are running experiments to compare our hyperparameter prediction performance against [4] and plan to add them to the revision.\n\n[1] https://arxiv.org/abs/2002.05688\n\n[2] https://arxiv.org/abs/1910.03137\n\n[3] https://arxiv.org/abs/1906.10842\n\n[4] https://arxiv.org/abs/2110.152880"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261896665,
                "cdate": 1700261896665,
                "tmdate": 1700263005731,
                "mdate": 1700263005731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VceNnin7jN",
            "forum": "fM1ETm3ssl",
            "replyto": "fM1ETm3ssl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7884/Reviewer_2Mjn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7884/Reviewer_2Mjn"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents what it terms a meta-model approach to mechanistic interpretability. Specifically, the approach is to create a model that takes in the weights (or other properties) of a model (a transformer), and then get the transformer to either 1) generate a human interpretable program (rasp) that corresponds or 2) detect backdoors. Stated differently, the goal of this work is to use a model to explain certain aspects of another model. The authors demonstrate the approach on a backdoor prediction task, and show that a meta-model can invert transformer weights that have been compiled by the tracr program."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Interesting solution to an important problem**: several of the demonstrations of the mechanistic interpretability paradigm have mostly been bespoke and tailored to single architecture-settings. Here the use of a model to predict the properties of a function that we seek to explain/interpret is interesting. The use of the tracr library as well is also very nice since that library was essentially design for the kind of tasks that it is used for in this work. \n\n- **Variety in tasks**: The authors show a variety in the kind of tasks that the approach can be used for. For example, one use it to reverse-engineer tracr programs. Another use is back-door detection. I particularly liked the next-token prediction framing, which is then used generate tracr programs of a model's weights. This kind of approach, if generalization, can be applied more extensively."
                },
                "weaknesses": {
                    "value": "I should state upfront, that I am fundamentally skeptical of the approach that this work pursues, but I am willing to rethink/update my review given feedback from the authors. \n\n- **Black-box model to explain a black-box model**: I think this approach is fundamentally limited because you are now using one model you don't understand to try to explain another model that you don't understand. What happens if the training data of the meta-model has backdoors in it for example? That is, you make it so that the back-door model gives interpretations that are benign for a model that is actually problematic. This might seem far-fetched, but I think one of the key limitations of this approach is that we are left to just 'trust' the output of the meta-model. But as we know, transformer-based models can easily learn spurious signals, and other problematic behavior, so it is unclear how this approach fundamentally solves the interpretability-problem at hand. Perhaps if we could always convert trained models to their tracr equivalent then we would have more confidence in this approach. However, to me, it is a non-starter that one cannot ascertain the reliability of the results of the meta-model.\n\n- **What does a mechanistic interpretation mean in this setting**: For the tracr program reconstruction, I think I get it. Here it seems like the tracr program itself constitute the explanation of the model weights. In the backdoor setting it is less clear. How does the backdoor classification task demonstrate interpretability? Specifically, how do I know how the meta-model is able to detect which model has a backdoor? Here I think you would actually want to make it input specific. Specifically, I think only a certain subsets of inputs trigger the wrong outputs for models with backdoors. It is the model's behavior on these inputs that we are most interested in. I don't quite see how the current setup helps us to do this."
                },
                "questions": {
                    "value": "I mixed in questions with the discussion on weaknesses above.\n\nIn addition to the points above, I have some questions about the tracr-program inversions section. \n- Can you show examples in the appendix of settings where you compile a program with tracr, and then reverse-engineer it with the meta-model. It would be helpful to compare the output of the meta-model and the original tracr program. \n- Are you not worried that that transformers that you get from tracr programs have much more sparse weights compared to SGD/ADAM trained models? Essentially, I am worried whether the demonstration here is too easy for the meta-model. \n- How do you envision that this meta-modeling approach will actually be used in practice, on real models, in the future? I struggle to see how one could ever train a meta model for a realistic setting, e.g., a vision transformer trained on ImageNet. Would I need to train thousands of imagenet models first to fit the metamodel?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699316507512,
            "cdate": 1699316507512,
            "tmdate": 1699636966945,
            "mdate": 1699636966945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vMYwAXbp4L",
                "forum": "fM1ETm3ssl",
                "replyto": "VceNnin7jN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for taking the time to leave an in-depth review. We\u2019re grateful for the detailed feedback, and we\u2019re glad that you found this to be an interesting new method. Hopefully we can address some of your concerns.\n\nWe summarize our responses here, and go into more detail in other comments.\n\n**Meta-models use a black-box to explain a black-box, so the approach is fundamentally limited.**\n  * We agree that this is a limitation of meta-models. However, meta-models can still be useful: if we view mechanistic interpretability as consisting of the two problems of 1) proposing an explanation and 2) verifying an explanation, then meta-models can help automate problem (1) despite being a black-box method. \n\n**How are meta-models doing \u201cmechanistic interpretability\u201d?**\n* Tracr: output programs constitute the explanation (as you say), so the meta-model is doing mechanistic interpretability.\n* Backdoor detection: the meta-model is doing interpretability (detecting backdoors), but not mechanistic interpretability.\n* Hyperparameter prediction: here the meta-model is not doing interpretability at all.\n\n**Can you show examples of programs you compile with tracr, then reverse-engineer?**\n* Thank you for this proposal! We plan to include such examples in the revision.\n\n**Won\u2019t real transformers be much harder to invert than Tracr transformers?**\n* Yes, possibly \u2015 we plan to find out in future work. We emphasize that our paper is exploratory work, and the first work to our knowledge that extracts a program from a transformer at all.\n\n**How are meta-models scalable?**\n* We\u2019ve added a paragraph on this topic to the revision in Section 5. Briefly, we anticipate large-scale pretraining to improve data-efficiency on new tasks, and we outline several tasks where the meta-model can be effectively used on a subgraph of the network, thus limiting the potential input size.\n\n**Changes to the paper**\n\nHere is a brief list of improvements we have made in response to your review so far:\n* We have expanded the Limitations section to discuss the problem of using a black box to interpret a black box (Section 4).\n* We have clarified what we mean by mechanistic interpretability and that the backdoor detection experiment does not count as *mechanistic* interpretability (beginning of Section 3.2).\n* We added a discussion of scalability concerns and suggestions to tackle them in Future Work (Section 5).\n* We have not yet added examples of decompiled RASP programs, but plan to add them in the final revision."
                    },
                    "title": {
                        "value": "Summary of our response"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259573665,
                "cdate": 1700259573665,
                "tmdate": 1700482897218,
                "mdate": 1700482897218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3OK9Yn30FG",
                "forum": "fM1ETm3ssl",
                "replyto": "VceNnin7jN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness #1"
                    },
                    "comment": {
                        "value": "You write:\n> Black-box model to explain a black-box model: I think this approach is fundamentally limited because you are now using one model you don't understand to try to explain another model that you don't understand. \n\nWe greatly appreciated this comment. We\u2019ve added a section in the Section 4 more precisely discussing this limitation of \u201ca black-box interpreting a black-box\u201d.\n\nMechanistic interpretability can be viewed as consisting of two problems:\n1. Proposing a (mechanistic) explanation for some behavior.\n2. Validating the correctness of an explanation.\n\nCurrently, automated methods exist for validating (e.g. [1]) but not proposing explanations. \n\nOur paper only addresses problem 1. We agree this is a limitation - without means of verifying an explanation, meta-models can only provide limited assurance. Applying meta-models to verification is beyond the scope of this work.\n\nQuoting more:\n> What happens if the training data of the meta-model has backdoors in it for example? That is, you make it so that the back-door model gives interpretations that are benign for a model that is actually problematic. This might seem far-fetched, but I think one of the key limitations of this approach is that we are left to just 'trust' the output of the meta-model. But as we know, transformer-based models can easily learn spurious signals, and other problematic behavior, so it is unclear how this approach fundamentally solves the interpretability-problem at hand. Perhaps if we could always convert trained models to their tracr equivalent then we would have more confidence in this approach. However, to me, it is a non-starter that one cannot ascertain the reliability of the results of the meta-model.\n\nWhile meta-models can certainly be backdoored, we emphasize our point above \u2013 that meta-models (at present) only propose an explanation, rather than verify it. If proposed explanations can be validated, then the performance of the meta-model need only be good on average rather than good in the worst case. While robustly validating mechanistic explanations is out of scope for our paper, some candidates include testing the base models\u2019 output vs the reconstructed RASP program, or (in a different setting) using causal scrubbing [1].\n\nWhen the meta-model\u2019s output cannot be verified, we caution against completely trusting predictions. We hope future work will make meta-model predictions more reliable and robust."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259651780,
                "cdate": 1700259651780,
                "tmdate": 1700259668436,
                "mdate": 1700259668436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JE14GniSsK",
                "forum": "fM1ETm3ssl",
                "replyto": "VceNnin7jN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to questions"
                    },
                    "comment": {
                        "value": "> Can you show examples in the appendix of settings where you compile a program with tracr, and then reverse-engineer it with the meta-model. It would be helpful to compare the output of the meta-model and the original tracr program.\n\nThank you for the suggestion. We will add a section to the Appendix with examples summarizing these results once we finish updating the Tracr experiments.\n\n> Are you not worried that that transformers that you get from tracr programs have much more sparse weights compared to SGD/ADAM trained models? Essentially, I am worried whether the demonstration here is too easy for the meta-model.\n\nThat\u2019s a good point. Yes, it is likely that Tracr-compiled transformers are easier to reverse-engineer than transformers trained to approximate the same program, and we mention this in Limitations. However, that this reverse engineering is possible at all is promising. We have added some future steps for this line of work: (1) reverse engineering trained transformers that were regularized to be \u201csimple\u201d in the same way as Tracr-compiled transformers, then (2) reverse engineering general transformers trained on the input/output of RASP programs.\n\n> How do you envision that this meta-modeling approach will actually be used in practice, on real models, in the future? I struggle to see how one could ever train a meta model for a realistic setting, e.g., a vision transformer trained on ImageNet. Would I need to train thousands of imagenet models first to fit the metamodel?\n\nThis is an important question. How to scale meta-models is an open research question, but it seems doable to us. Ways in which we might overcome the scaling challenge in future work include:\n1. Pretraining, e.g. masked weight prediction or contrastive learning. This might allow finetuning on new tasks with fewer examples.\n2. Experiments on generalization capabilities across scale. If a meta-model can (partially) generalize from small to large networks, then we may be able to reduce the number of large networks in the training data.\n3. Applying meta-models to subsections of a target network. Depending on the target task, a single network might yield many datapoints, for example if individual circuits within a network can be mapped to specific functions. This also means the meta-model could be smaller than the base model it helps to analyze.\n\nWe\u2019ve added a section called \u201cFuture Work\u201d, covering some ways in which we expect that meta-models might be useful. Some speculative examples include: highlighting backdoored neurons, producing simplified interpretations of neural network circuits, and identifying attention heads that perform a particular function. \n\n[1] https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259823683,
                "cdate": 1700259823683,
                "tmdate": 1700260146495,
                "mdate": 1700260146495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PowuPUaAMh",
                "forum": "fM1ETm3ssl",
                "replyto": "JE14GniSsK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Reviewer_2Mjn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Reviewer_2Mjn"
                ],
                "content": {
                    "title": {
                        "value": "Helpful clarifications"
                    },
                    "comment": {
                        "value": "Thanks to the authors for clarifying my understanding of this work. While I am still skeptical of the meta-model approach, the demonstration here is quite interesting. The part of the work that I most appreciate is generating tracr programs from model weights. That line of work is interesting, and could lead to more insights. It would be interesting to create a test-bed of transformer models where the behavior of the model is 'known a priori' and to empirically check that the tracr programs that you get from reverse engineering these models match( I assume the original tracr paper did this?)\n\n **Backdoor setting**: is there a way to generate tracr programs that are input (or group input) specific? Here is what I am trying to get that: the critical issue for that setting is that the model has undesirable behavior on inputs that contain the backdoor. However, just identifying the backdoor is great, but somewhat unsatisfactory. It'll be great to get a tracr program for the model's behavior on normal vs backdoored inputs. This way, hopefully we can look at the output of the tracr program to see what the issue is. Actually, here is one crude way to do this: train models on normal inputs vs backdoored inputs, then compare the tracr outputs of the model trained on normal inputs to those trained on backdoored inputs. How different are they?\n\nOverall, I think this work has some nice insights. I am not expecting any new experiments or updates to the paper on the basis of this response. I just wanted to give the authors additional feedback the future revision of the manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498207609,
                "cdate": 1700498207609,
                "tmdate": 1700498207609,
                "mdate": 1700498207609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EDppGqZjiT",
                "forum": "fM1ETm3ssl",
                "replyto": "VceNnin7jN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback!"
                    },
                    "comment": {
                        "value": "Thanks for the response! Are there changes we can make that would convince you to increase your score? We think we've responded to the weaknesses that you raised in your review --- we'd be curious to know what you see as the main points that are still not adequately addressed."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606945062,
                "cdate": 1700606945062,
                "tmdate": 1700607021204,
                "mdate": 1700607021204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pza965B11i",
                "forum": "fM1ETm3ssl",
                "replyto": "EDppGqZjiT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7884/Reviewer_2Mjn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7884/Reviewer_2Mjn"
                ],
                "content": {
                    "title": {
                        "value": "Satisfied with the response"
                    },
                    "comment": {
                        "value": "As it stands, there is no need for additional changes. I'd be happy to raise my score by 1, but it is unclear to me how that strengthens the case for the paper, at the current venue, given the other reviews. As I said before, my prior is to not be in favor of using one black-box to explain another black-box.  I'll need to confer with the other reviewers and digest the new updates. Regardless of the outcome, I think this work has some interesting findings."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7884/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672507499,
                "cdate": 1700672507499,
                "tmdate": 1700672507499,
                "mdate": 1700672507499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]