[
    {
        "title": "Training Adversarially Robust SNNs with Gradient Sparsity Regularization"
    },
    {
        "review": {
            "id": "u4NKHYq97d",
            "forum": "ZN8BaYVFkx",
            "replyto": "ZN8BaYVFkx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission24/Reviewer_FfwP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission24/Reviewer_FfwP"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the robustness of SNNs against adversarial perturbations. An initial robustness analysis reveals that SNNs are robust against random perturbations, but vulnerable against adversarial attacks. The proposed method incorporates the gradient sparsity regularization in the loss function to reduce the gap between the SNN robustness against random noise and adversarial perturbations. The experiments of the proposed method conducted on CIFAR-10 and CIFAR-100 dataset reveal higher SNN robustness compared to the traditional approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The tackled problem is relevant to the community.\n\n2. The proposed method is original.\n\n3. The experimental results show higher robustness of the proposed method compared to prior works."
                },
                "weaknesses": {
                    "value": "There are some aspects to clarify and improve. Please see the questions below."
                },
                "questions": {
                    "value": "1. Please discuss more in detail what are the key findings made in the existing literature in terms of SNN robustness, what are the limitations of the existing methods, and how the challenges are solved in this paper.\n\n2. Referring to Figure 1, what are the experiment settings used to generate the results? What is the SNN architecture? What is the adversarial attack algorithm?\n\n3. In Section 4.3, please discuss more clearly the differences between the approximation of the gradient regularization term employed in this paper and the related works.\n\n4. In Section 5, please discuss in detail all the parameters and hyperparameters used to conduct the experiments, as well as the tool flow. If possible, please provide the code in an online open-source repository.\n\n5. From Table 1 we can infer that, while the proposed method can improve the adversarial robustness, there is a significant accuracy loss for clean inputs compared to related works. Please discuss the limitations and potential solutions to overcome this issue.\n\n6. The experiments have been conducted only on CIFAR-10 and CIFAR-100 dataset. It is recommended to make experiments also on event-based datasets, which are typical benchmarks for SNNs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission24/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698148896615,
            "cdate": 1698148896615,
            "tmdate": 1699635925853,
            "mdate": 1699635925853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "891lfWAJTY",
                "forum": "ZN8BaYVFkx",
                "replyto": "u4NKHYq97d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FfwP (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the valuable comments. We are encouraged that you find the \ntackled problem is relevant to the community, our work original and our experimental results good. We would like to address your concerns and your questions in the following.\n\n**Q1: Discuss more in detail the key findings and limitations in the existing literature, and how the challenges are solved in this paper.**\n\nThanks for your valuable feedback! The key findings and limitations in the existing literature are as follows. Methods for improving the robustness of SNNs can be broadly categorized into two classes. The first class draws inspiration from ANNs. A typical representative is adversarial training and its variants. This approach has been shown to effectively defend against attacks that are used in the training phase. Another method is certified training, whose application to SNNs remains challenging. Current efforts in this area have primarily focused on the MNIST dataset. The second category consists of SNN-specific techniques designed to enhance robustness, such as using the Poisson encoder. However, the Poisson encoder generally yields worse accuracy on clean images than the direct encoding, and the robustness improvement caused by the Poisson encoder varies with the number of time-steps used.\n\nIn this paper, we propose the SR strategy which can be combined with adversarial training to boost the robustness of SNNs. The SR strategy is supported by a solid theoretical foundation, and experimental results on CIFAR-10 and CIFAR-100 demostrate its effectiveness in enhancing the robustness of SNNs.\n\nWe have revised Sec. 2.2 to provide a more detailed discussion of the key findings and limitations in the existing literature in terms of SNN robustness, as well as how the challenges are solved in this paper.\n\n**Q2. The experiment settings used to generate Figure 1.**\n\nThe experimental settings in Figure 1 are outlined as follows. The SNN architecture is a VGG-11 model. The adversarial attack employed is FGSM, with $\\epsilon$ ranging from $0.00$ to $0.25$. For the random attack on SNNs, a perturbation $\\delta$ is randomly generated to satisfy a uniform distribution within the hypercube $\\Vert \\delta \\Vert_\\infty \\leqslant \\epsilon$. Subsequently, $\\delta$ is added to the input image $x$ and the pixel value of the perturbed image are clipped to the range of [0, 1]. The classification accuracy is evaluated for all $x+\\delta$, where $x$ is drawn from the test set of CIFAR-10 (Figure 1(a)) and CIFAR-100 (Figure 1(b)). We have added these detailed settings in Sec. 4.1 of the revised version.\n\n**Q3: Discuss more clearly the differences between the approximation of the gradient regularization term employed in this paper and the related works.**\n\nRegarding the approximation method for computing the gradient norm, both our paper and the related work ([Finlay & Oberman 2021](https://www.sciencedirect.com/science/article/pii/S2666827020300177)) use the finite differences technique. The only difference is that, we take the sign of the gradient as the difference direction while Finlay & Oberman took the sign direction divided by $\\sqrt{N}$, where $N$ is the dimension of $x$. For other differences between the objectives of regularization terms in this paper and the related work, please refer to \"To All Reviewers\".\n\n**Q4: Discuss in detail all hyperparameters used to conduct the experiments, and provide the code.**\n\nWe would like to clarify that the setting of hyperparameters are discussed in Appendix C and D. Thanks for your suggestion! We have already uploaded the code in the revised supplementary material.\n\n**Q5: Please Discuss the limitations and potential solutions to overcome accuracy loss for clean inputs casued by SR.**\n\nThanks for the valuable suggestion! The enhancement of adversarial robustness through SR is accompanied by a notable loss in accurac on clean images. In future work, we aim to achieve a better trade-off between classification accuracy and adversarial robustness. One possible avenue we plan to explore is the utilization of the simulated annealing algorithm in SR* to dynamically adjust the weight assigned to the regularization term, potentially leading to a more balanced outcome in terms of both accuracy and robustness. \n\nWe have added these limitations and potential solutions in Section 6 of the revised paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157520568,
                "cdate": 1700157520568,
                "tmdate": 1700546923459,
                "mdate": 1700546923459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TAVVmgcfBR",
                "forum": "ZN8BaYVFkx",
                "replyto": "u4NKHYq97d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer FfwP, thanks again for your careful and valuable comments! Since the rebuttal discussion is due soon, we\u2019ll be appreciated to know whether our replies have addressed your questions. If there are any further clarifications required or any other concerns, please feel free to contact us. Many thanks!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546323560,
                "cdate": 1700546323560,
                "tmdate": 1700546323560,
                "mdate": 1700546323560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w3dX7N9xRD",
                "forum": "ZN8BaYVFkx",
                "replyto": "kTvticFiud",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_FfwP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_FfwP"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your responses. Considering together the other reviews and responses, my score is confirmed."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721564648,
                "cdate": 1700721564648,
                "tmdate": 1700721564648,
                "mdate": 1700721564648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zNF7wSnp23",
            "forum": "ZN8BaYVFkx",
            "replyto": "ZN8BaYVFkx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission24/Reviewer_HCoc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission24/Reviewer_HCoc"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of improving adversarial robustness of SNNs. Authors propose an input gradient sparsity promoting regularization scheme for training robust SNNs. An l0-norm penalty term on the input gradient is approximated via a sparsity-promoting l1-norm penalty, which is then again approximated with a softmax output regularization term using the finite differences method. Proposed approach is then combined with a traditional adversarial training method for SNNs, and empirically showcased on CIFAR-10/100."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Theoretical justification on why this method could be applicable for robustness is presented nicely.\n- Writing is clear and the storyline is presented well."
                },
                "weaknesses": {
                    "value": "- Robustness evaluations of the SNNs are ambiguous in a weak way, and need much more rigor & depth.\n- Limited innovation and justification from the ML security methodology aspect, as well as the practical side."
                },
                "questions": {
                    "value": "1) Can the authors clearly state/elaborate in their paper, in which ways their training algorithm is different than the work by [Finlay & Oberman 2019]? https://arxiv.org/pdf/1905.11468.pdf\n\n2) Proposed approach has several simplifications/assumptions on the ultimate gradient l0-norm regularization idea. At the end, the used training algorithm seems to become also similar to the well-known (clean) logit pairing approach [Kannan, Kurakin & Goodfellow, 2018]. At the very least, the objective simply uses adversarial examples for an output probability distribution regularizer, which has been the most fundamental form of adversarial training to date (see e.g., [Zhang et al. ICML 2019]). Hence all in all, I can not clearly see any innovation from the ML security side in this paper. Can the authors experimentally compare why their choice would be particularly any better in the case of SNNs, than using one of the other powerful adversarial training/regularization methods (e.g., TRADES [Zhang et al. ICML 2019] or adversarial logit pairing [Kannan, Kurakin & Goodfellow, 2018])?\n\n3) Authors claim to use an \"ensemble attack\" for white-box evaluations, which requires further clarification. Their implementation of an ensemble \"conducts multiple attacks on each sample and reports the strongest attack\". Can the authors provide an exemplary outlined test set case more clearly on how these evaluations are reported?\n\n4) For this attack ensemble to be meaningful and reveal any impact of gradient obfuscation, there should actually be cases where the surrogate gradient function is changing its width, rather than only its shape for fixed parameters. The width parameter $\\gamma=1$ of the triangular surrogate should be accounted for. Authors should run an extensive evaluation with for instance $\\gamma\\in[0.1,3.0]$ in fine-grained steps of 0.1, and demonstrate that changing this parameter does not at all influence the capability of the adversary any better than using different surrogate gradient shapes. This part overall needs strong empirical justification.\n\n5) Models were trained using PGD-5 adversarial examples, but the Algorithm 1 denotes adv. examples obtained via one-step FGSM. Which one is correct? Does this mean that PGD-5 is the general AT approach adopted in SR* models, but in any case the regularizer term was always obtained via FGSM? Needs clarification overall.\n\n6) Following my question above, Table 2 is a bit complicated in how SR and AT can be disentangled. In the case of ablation models without AT but SR, how were the adv. examples to compute the regularization term, obtained? None of these details are clear in the paper.\n\n7) Proof of Thm 1 in Appendix A seems agnostic to any function f, regardless of being an SNN. However, it appears to implicitly make the assumption that the SNN uses direct input coding. Can the authors comment if these assumptions would also hold for SNNs that use Poisson input coding, or SNNs that do not necessarily use IF neurons (i.e.., with membrane potential leak)? Wouldn't then the error that should be accounted for in the finite differences approximation would be large? Did the authors experiment with other types of more realistic SNNs at all?\n\n8) Did the authors perform any simulations on dynamic vision sensor data where SNNs are designed to be more beneficial for?\n\n9) Since PGD-5 is used via BPTT during training for 200 epochs, than the authors should outline a table with the computational overhead (wall-clock time) of their approach, in comparison to simple AT or RAT with PGD-5 as well.\n\n10) Figure 3 should also include adversarially trained models in comparison (not only Vanilla), since such SNNs are already implicitly inducing a similar behavior.\n\nMinor comment: Eq (3) & (4) are already defined as solutions of an adversarial example in the l_infty norm (since the sign function is used). Therefore at the end of Sec 3.2, authors should correct the \"l_p norm\", since p is already infty in this setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission24/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698165407508,
            "cdate": 1698165407508,
            "tmdate": 1699635925778,
            "mdate": 1699635925778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZZwbQgoLzi",
                "forum": "ZN8BaYVFkx",
                "replyto": "zNF7wSnp23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HCoc (1/4)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable insights. We sincerely appreciate your advice regarding our work. We apologize for any ambiguities in our description of the SR method that may have caused misunderstandings. In this rebuttal, we would like to first clarify the distinction between SR and adversarial training. Subsequently, we will address your concerns and provide responses to your questions. If you have any further suggestions or questions, please feel free to let us know.\n\n**Q1: Clarification of the SR strategy**\n\nIn Theorem 1 of the main text, we have proved that the disparity between the adversarial vulnerability and random vulnerability is upper bounded by the sparsity of $\\nabla_x f_y$. The main idea of our SR strategy is to regularize the value of $\\Vert \\nabla_x f_y(x,w) \\Vert_0$. **It is important to note that in our approach, $f(\\cdot, w)$ represents the model function, which differs from previous methods that utilize the multi-class calibrated loss function as a regularization term.** Here, the multi-class calibrated loss function $l(f(x,w),y)$, such as cross-entropy, is the loss function used to train a classifier where $y$ is the true label of $x$.  In other words, our proposed method introduces a regularization term that remains independent of the choice of loss function.\n\nTo achieve the regularization of $\\Vert \\nabla_x f_y(x,w) \\Vert_0$, we use $\\Vert \\nabla_x f_y(x,w) \\Vert_1$ to approximate the $\\ell_0$ norm of the gradient and then use finite difference method to make the regularization of $\\Vert \\nabla_x f_y(x,w) \\Vert_1$ computationally feasible. Theoretically, if direct backpropagation on $\\Vert \\nabla_x f_y(x,w) \\Vert_1$ were feasible, finite differences could be bypassed. However, as reported in [Finlay & Oberman's work](https://arxiv.org/pdf/1905.11468.pdf), directly calculating the backpropagation of $\\Vert \\nabla_x f_y(x,w) \\Vert_1$ on ANNs is very time-consuming. Additionally, we found that SNNs are not trainable with such double backpropagation approach.\n\n\nWe also find that the demonstration of Algorithm 1 in the main text is not clear enough, potentially leading readers to confuse our proposed SR algorithm with the adversarial training method and its variants. We would like to clarify that, as depicted in Algorithm 1 (Line 5 to Line 7), $\\hat{x}^i=x^i + hd^i$ is fundamentally different from an adversarial example in terms of both numerical value and meanings. Actually, it is used to calculate the difference quotient\n\\begin{equation}\n \\left| \\frac{\n        (f_{y^i}(\\hat{x}^i, w)) - (f_{y^i}(x^i, w))\n        }{\n        h\n        } \\right|\n\\tag{3-1}\n\\end{equation}\nwhich is the approximation of $\\Vert \\nabla_{x^i} (f_{y^i}(x^i, w)) \\Vert_1$. While theoretically, $d^i$ can be arbitrary, it is set to be the sign of the gradient in this paper. This choice aligns with the objective of approximating the $\\ell_1$ norm of the gradient, as explained in detail in Appendix B. \n\nAdditionally, $h$ represents the step size used for the approximation, rather than the attack strength. It is recommended to choose a small value for $h$ in Equation (3-1) to achieve a more accurate estimation of the $\\ell_1$ norm. As $h$ increases, the approximation results become less accurate, leading to a decrease in robustness. However, if $h$ were the attack strength in one-step FGSM, larger $h$ would contribute to better robustness (at the expense of decreased accuracy on clean images). The impact of $h$ in our training algorithm is different from the impact of $h$ as the attack strength in FGSM adversarial training. **In summary, the SR algorithm does not involve the calculation of adversarial samples, making it fundamentally different from adversarial training and its variations.**\n\nWe have revised the parts of the manuscript that may cause misunderstandings and added this clarification in Appendix G.\n\n**Q2: Differences between our training algorithm and the work by [Finlay & Oberman 2019](https://arxiv.org/pdf/1905.11468.pdf).**\n\nPlease refer to \"To All Reviewers\".\n\n**Q3: Differences between SR and (clean) logit pairing approach. Differences between SR and (adversarial) logit pairing approach, as well as TRADES.**\n\nPlease refer to \"To All Reviewers\"."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156592373,
                "cdate": 1700156592373,
                "tmdate": 1700546799346,
                "mdate": 1700546799346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kHOV3sLTft",
                "forum": "ZN8BaYVFkx",
                "replyto": "zNF7wSnp23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HCoc (2/4)"
                    },
                    "comment": {
                        "value": "**Q4: Further clarification of an \"ensemble attack\" for white-box evaluations.**\n\nWe would like to clarify that as dicussed in the Appendix D, to avoid gradient obfuscation, the ensemble attack involves utilizing a diverse set of attacks with different surrogate gradients and gradient backward methods. For each test image, we applied various attacks with the following setting. **We consider an ensemble attack to be successful for a test sample as long as the model is fooled by any of the attacks from the ensemble.**\n\n**Table R3. The experimental settings for the ensemble attack**\n| Attacks            | Attack 1 | Attack 2 | Attack 3   | Attack 4 |\n|--------------------|----------|----------|------------|----------|\n| Surrogate function | Triangle | Sigmoid  | Arctangent | Triangle |\n| hyperparameter     | 1        | 4        | 2          | /        |\n| Backward method    | STBP     | STBP     | STBP       | RGA      |\n\n**Q5: Extensive evaluation with for instance $\\gamma \\in [0.1,3.0]$.**\n\nThanks for your valuable suggestion, we have applied the attack with $\\gamma \\in [0.1, 3.0]$ and Table R4 reported the results of a PGD10 attack on VGG-11 models with different training algorithms on the CIFAR-10 dataset. \n\nWe compare three different attack combinations to evaluate the impact of different surrogate functions on the attack strength. We selecte RAT, PGD5-AT and SR*(PGD5+SR) model as the target models. For PGD10(w/o ensemble), we only use the Triangle-shaped surrogate function\uff0c which is identical to the one used in training. For PGD10 (w/ ensemble), we use the attack combination as described in the manuscript. For PGD10($\\gamma$ from [0.1, 3.0]), we incorporate 30 different Triangle-shaped surrogate function with its $\\gamma$ ranging from [0.1, 30], as suggested. \n\nWe find that both ensemble attack methods significantly improve attack performance and mitigate the impact of gradient obfuscation. This indicates that both shape and width of the surrogate function can influence the capability of the adversary. In addition, we acknowledge that the PGD10($\\gamma$ from [0.1, 3.0]) attack is slightly more effective than the ensemble attack used in this paper. However, it is important to note that the PGD10($\\gamma$ from [0.1, 3.0]) attack uses a 30-fold fine-grained grid search over attack hyperparameters for each image, which is considerably more computationally expensive compared to the ensemble attack used in our paper. \n\nIn conclusion, on one hand, we agree with reviewer HCoc's claim that both width and shape of the surrogate function can impact attack performance. We have included additional discussions about this in the limitation part. On the other hand, we would like to clarify that our contribution mainly lines in the defense algorithm, not the attack algorithm. In our paper, we use the same attack method for all comparisons to ensure a fair comparison of different methods. We believe that whether we use our current method or incorporate the attack combinations suggested by reviewer HCoc, it will not significantly influence the effectiveness of our defense strategy.\n\n**Table R4. The classification accuracy (%) under the ensemble attack with different &gamma;**\n| Attacks                   | RAT   | PGD-AT | SR*   |\n|---------------------------|-------|--------|-------|\n| PGD10 (w/o ensemble)      | 16.16 | 21.32  | 33.67 |\n| PGD10 (w/ ensemble)       | 11.53 | 18.18  | 30.54 |\n| PGD10 (\u03b3 from [0.1, 3.0]) | 11.87 | 16.16  | 27.06 |\n\n**Q6: Models were trained using PGD-5 adversarial examples, but the Algorithm 1 denotes adv. examples obtained via one-step FGSM. Which one is correct?**\n\nThanks for pointing it out! We would like to clarify that in the SR model, we do not use adversarial examples obtained via one-step FGSM to conduct adversarial training, as explained in \"Clarification of the SR strategy\".  For SR* models, we employ PGD-5 as the general adversarial training approach, and the training loss function is\n\\begin{equation}\n    \\mathcal{L}(x^i, y^i,w) = \\text{CE}(f(x_{adv}^i), y^i) + \n        \\lambda_{\\text{SR}}\\left\\Vert \\nabla_{x_{adv}^i} f(x_{adv}^i,w)\\right \\Vert_1\n    \\tag{3-2}\n\\end{equation}\nwhere $\\left\\Vert \\nabla_{x^i} f(x^i,w)\\right \\Vert_1$ is approximated by the finite differences, and $x_{adv}^i$ is obtained by PGD-5."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156895827,
                "cdate": 1700156895827,
                "tmdate": 1700546789988,
                "mdate": 1700546789988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1gDEf4bzjq",
                "forum": "ZN8BaYVFkx",
                "replyto": "zNF7wSnp23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HCoc (3/4)"
                    },
                    "comment": {
                        "value": "**Q7: Following my question above, Table 2 is a bit complicated in how SR and AT can be disentangled. In the case of ablation models without AT but SR, how were the adv. examples to compute the regularization term obtained?** \n\nAs clarified in the \"Clarification of the SR strategy\" section, the finite difference approximation employed in SR is not a type of adversarial training using one-step FGSM. Thus, SR is not entangled with AT, and the ablation models without AT but with SR do not involve adversarial examples. To provide further clarity on the distinction between SR and AT, we present the training loss function used in different training algorithms.\n\nFor PGD5-AT: \\begin{equation}\n    \\mathcal{L}(x^i, y^i,w) = \\text{CE}(f(x_{adv}^i), y^i)\n    \\tag{3-3}\n\\end{equation}\nFor single SR: \\begin{equation}\n    \\mathcal{L}(x^i, y^i,w) = \\text{CE}(f(x^i), y^i) + \n        \\lambda_{\\text{SR}}\\left\\Vert \\nabla_{x^i} f(x^i,w)\\right \\Vert_1\n        \\tag{3-4}\n\\end{equation}\nFor SR*(SR+PGD5): \\begin{equation}\n    \\mathcal{L}(x^i, y^i,w) = \\text{CE}(f(x_{adv}^i), y^i) + \n        \\lambda_{\\text{SR}}\\left\\Vert \\nabla_{x_{adv}^i} f(x_{adv}^i,w)\\right \\Vert_1\n        \\tag{3-5}\n\\end{equation}\n\n**Q8: If assumptions in this paper would also hold for SNNs that use Poisson input coding, or SNNs that use LIF?**\n\nThanks for pointing it out. Indeed, the SR strategy can be applied theoretically as long as SNN is differentiable through methods such as surrogate gradients. \n\nTo demonstrate this point, we apply SR to SNNs with VGG-11 structure consisted of **LIF neurons** on the CIFAR-10 dataset. The experimental results shown in Table R5 clearly demonstrate the effectiveness of the SR* strategy in enhancing the robustness of SNNs with LIF neurons. Models trained with SR* exhibit significantly improved robustness against all PGD attacks compared to models trained with AT, with a 4% decline in classification accuracy on clean images.\n\n**Table R5-1. Experiments on SNNs with LIF neurons**\n| Models    | Clean | PGD10 | PGD30 | PGD50 |\n|-----------|-------|-------|-------|-------|\n| VGG11-AT  | 89.77 | 17.07 | 14.55 | 14.27 |\n| VGG11-SR* | 85.60 | 30.80 | 28.52 | 28.38 |\n\nWe also trained two VGG-11 models on CIFAR-10 using **Poisson encoding**. During training, we use the same setting as reported in the main text with the only different being the input encoding. When evaluating robust performance, the Poisson input encoding can be treated as an input transformation and use the EOT method ([Athalye, Anish, Nicholas Carlini, & David Wagner](http://proceedings.mlr.press/v80/athalye18a.html)) to attack. According to the results in Table R5-2, the performance of SR model is higher than the Vanilla model and the robust performance of SNN trained with SR is even 15% higher than the vanilla SNN under PGD10 attack. That proves the effectiveness of the sparsity regularization.\n\n**Table R5-2. Experiments on SNNs using Poisson Encoding**\n| Methods    | Clean | FGSM  | PGD10 | \n|------------|-------|-------|-------|\n| Vanilla    | 82.71 | 26.58 | 17.69 |\n| SR         | 78.84 | 39.03 | 32.67 |\n\nThese experiments demostrate that our strategy also holds for SNNs with LIF neurons or with Poisson encoding.\n\n**Q9: Experiments on DVS datasets**\n\nWe use the VGG-11 architecture on DVS-CIFAR10 Dataset and compared Vanilla trained model and SR trained model. When evaluating, we perform the adversarial attack on preprocessed frames and add adversarial noise onto each frame. The robustness performance is shown in Table R6. The performance of SR trained model is significantly higher than the Vanilla model, which proves the effectiveness of SR method.\n\n**Table R6. Performance of models on DVS-CIFAR10**\n| Methods    | Clean | FGSM  | PGD10 | PGD30 | PGD50 |\n|------------|-------|-------|-------|-------|-------|\n| Vanilla    | 71.50 | 24.50 | 10.10 |  9.50 |  9.40 |\n| SR         | 71.70 | 42.00 | 34.10 | 34.30 | 33.60 |\n\n**Q10: The computational overhead of SR and SR\\*, in comparison to simple AT or RAT with PGD-5 as well.**\n\nThe computational costs for one epoch training of various algorithms, including vanilla, PGD5 AT, RAT, SR, and SR*(PGD5+SR), on CIFAR-10 using the VGG11 architecture are summarized in Table R6. From the table, we observe that single SR incurs a computational cost 1.5 times that of RAT but consumes less than half the time needed by PGD5 AT. The computational cost of SR* is the highest among all training algorithms since it combines both SR and AT. However, models trained with SR* acheive the best robustness compared to models trained with other methods. We have added this comparison in Appendix I.\n\n**Table R7. The computational cost in one epoch of different training algorithms**\n| Vanilla | PGD5 AT | RAT    |  SR    | SR*     |\n| ------- | ------- | -----  | ------ | ------- |\n| 65s     | 459s    | 134s   |  193s  | 583s    |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157306852,
                "cdate": 1700157306852,
                "tmdate": 1700546741859,
                "mdate": 1700546741859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "25b6Vs4j2Q",
                "forum": "ZN8BaYVFkx",
                "replyto": "zNF7wSnp23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the time and hope our responses helpful for your re-assessment of our work."
                    },
                    "comment": {
                        "value": "Dear reviewer HCoc, we sincerely hope our posted response can help to address your concerns on our paper and serve as a reference for your re-assessment of our work. If you have any further comments and questions, please let us know and we are glad to write a follow-up response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545936763,
                "cdate": 1700545936763,
                "tmdate": 1700545936763,
                "mdate": 1700545936763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u3sjlFuGhN",
                "forum": "ZN8BaYVFkx",
                "replyto": "zNF7wSnp23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HCoc (4/4)"
                    },
                    "comment": {
                        "value": "**Q11: Include adversarially trained models in Figure 3.**\n\nWe have added the gradient distribution of AT-trained models with respect to the input image in Figure 3. The distribution of gradient values for SNNs-SR* is more concentrated around zero compared to that of vanilla SNNs and SNNs-AT.\n\n**Q12: Minor comments**\n\nThanks for pointing it out. We correct the \"$\\ell_p$ norm\" at the end of Sec 3.2 in the revised version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546760256,
                "cdate": 1700546760256,
                "tmdate": 1700546773342,
                "mdate": 1700546773342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZTR4piJpaS",
                "forum": "ZN8BaYVFkx",
                "replyto": "u3sjlFuGhN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_HCoc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_HCoc"
                ],
                "content": {
                    "title": {
                        "value": "response to authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their clarifications and experiments. \n\nI think this work is not really convincing in its empirical contributions & implementations, as well as the particular novelty it introduces towards making SNNs more robust with reasonable trade-offs. Considering the other reviewers\u2019 comments, scores and confidences, I will keep my initial recommendation to reject. Here are some further feedback for the authors\u2019 from my side:\n\n- Given the results in Table R4, I believe there can be an adversary obfuscation present if one can simply manipulate the surrogate gradient parameter to attack better. Authors claim that the way attack combinations are implemented (as I did suggest) will not influence the effectiveness of their defense strategy. I disagree. Thorough evaluations are necessary in empirical defenses, and at any rate they might lead to more convincing results if defenses are reliable. More broadly, it influences the significance of the work for ML security. This work does not provide a certified robustness solution for SNNs. Since results are only empirical at CIFAR scale, the experiments and adversary evaluations should at least reveal worst-case performance (see [Carlini et al 2019], section \u201cskepticism of results\u201d).\n\n- Differences to adv. logit-based regularization methods: I now see the minor choice differences than existing defenses, which appears more clearly in formulas. But again, one should have than justified that this particular regularizer choice is specifically better for SNNs, than simply using TRADES with PGD-5 for instance. The novelty proposed is not really justified there.\n\n- Attack evals on DVS datasets raise some suspicion. In Table R6, PGD30 attacks appear to somehow perform worse than PGD10. This should not be the case, unless gradients are primarily obfuscated in a way. Did the authors question this, since this is an important flag to re-address attack evaluations (see [Carlini et al 2019], section \u201cbasic sanity tests\u201d)?\n\n- On Table R7: I thought the results in Table 1 of paper compared RAT, AT and SR* models that all exploit PGD-5 examples during training (this would be the fair comparison). However in Table R7 it turns out that the \u201cPGD5 AT\u201d models take ~3x longer than RAT, so I guess RAT currently does not harness the same strength of adversarial training budget? If this is the case, it would have been more fair to implement RAT with PGD5 as well, and see if SR* can outperform this SOTA defense at the same cost. Perhaps authors should also clearly add their used RAT objective to the list of Eqs (3-3), (3-4),(3-5) as well (i.e., is it PGD5 or single-step examples)?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641148262,
                "cdate": 1700641148262,
                "tmdate": 1700641148262,
                "mdate": 1700641148262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qHJ3uZGkln",
            "forum": "ZN8BaYVFkx",
            "replyto": "ZN8BaYVFkx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission24/Reviewer_JC8H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission24/Reviewer_JC8H"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to improve the robustness of an SNN model. They first identify the SNN robustness under random attack and adversarial attack. Then, they add a regularization term in loss to make the SNN model more robust under adversarial attack."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis work analyzed the SNN robustness under random attack and adversarial attack, which provides very meaningful observations.\n2.\tThe proposed idea is interesting that tries to shrink the gap between two attacks.\n3.\tDetailed experiments are presented to demonstrate the efficiency of the proposed method."
                },
                "weaknesses": {
                    "value": "The specialization of the regularization term is not highlighted, see common for details."
                },
                "questions": {
                    "value": "1.\tIn Formula 10, I think the left part tries to compute the gradient of input, however, the right parts compute the gradient of last layer.\n2.\tI think adding regularization in loss to improve the robustness is a widely used method. It is better to highlight the specialization, i.e. whether ANN can adopt this technique?\n3.\tWhether the proposed method will affect the robustness under random attack?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission24/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission24/Reviewer_JC8H",
                        "ICLR.cc/2024/Conference/Submission24/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission24/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655020658,
            "cdate": 1698655020658,
            "tmdate": 1700659359837,
            "mdate": 1700659359837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "At01a08ebc",
                "forum": "ZN8BaYVFkx",
                "replyto": "qHJ3uZGkln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JC8H"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments and suggestions. We are delight that you find our obervation meaningful, our idea interesting and experiments detailed. We would like to address your concerns and your questions in the following.\n\n**Q1: Details of Formula 10**\n\nThanks for pointing it out. We would like to clarify the notations used in Formula (10) as follows. Let $x$ denote the image, and $\\{x[1], x[2], \\dots, x[T]\\}$ represent the input image series. In our paper, we use $x[t]=x$ for all $t=1,\\dots,T.$ The network is denoted by $f$ with parameters $w$. The output of the network $f$ is a vector $f(x,w)\\in \\mathbb{R}^{N \\times 1}$, where $N$ represents the number of classes. The output vector $f(x,w)$ is obtained by applying the softmax function to the output of the last layer, i.e.\n\\begin{equation}\n    f(x, w) = softmax \\left( \n        \\sum_{t=1}^T s_1^L[t], \\dots, \\sum_{t=1}^T s_N^L[t]\n    \\right).\n    \\tag{2-1}\n\\end{equation}\nTherefore, the $y^{th}$ component of $f(x, w)$ where $y$ represents the true label is\n\\begin{equation}\n    f_y(x, w) = softmax_y \\left( \n        \\sum_{t=1}^T s_1^L[t], \\dots, \\sum_{t=1}^T s_N^L[t]\n    \\right).\n    \\tag{2-2}\n\\end{equation}\nAccording to the chain rule, the gradient of $f_y$ with respect to the input $x$ is\n\\begin{equation}\n    \\begin{split}\n        \\nabla_x f_y(x,w) &=  \\frac{\\partial softmax_y }{\\partial \\sum_t s_1^L[t]}\n                     \\cdot\n                    \\nabla_x \\left( \\sum_t s_1^L[t] \\right) + \\dots  + \n                    \\frac{\\partial softmax_y }{\\partial \\sum_t s_N^L[t]}\n                     \\cdot \\nabla_x \\left( \\sum_t s_N^L[t] \\right).\n    \\end{split}\n    \\tag{2-3}\n\\end{equation}\nIn Equation (2-3), the gradient of the $i^{th}$ component $\\nabla_x \\left( \\sum_t s_i^L[t] \\right)$ in the last layer with respect to $x$ can be further expresses as\n\\begin{equation}\n    \\nabla_x \\left( \\sum_t s_i^L[t] \\right) =  \\sum_t \\nabla_x s_i^L[t] \n    = \\sum_t \\sum_{\\tilde{t}=1}^t \\nabla_{x[\\tilde{t}]} s_i^L[t] .\n    \\tag{2-4}\n\\end{equation}\nFinally, the gradient $\\nabla_x f_y(x,w)$ is written as\n\\begin{equation}\n    \\nabla_x f_y(x,w) = \\sum_{i=1}^N \\left( \\frac{\\partial softmax_y\n                    }{\n                        \\partial \\sum_t s_i^L[t]\n                    }\n                    \\left(\n                        \\sum_{t=1}^T \\sum_{\\tilde{t}=1}^t \\nabla_{x[\\tilde{t}]} s_i^L[t]\n                    \\right) \\right).\n    \\tag{2-5}\n\\end{equation}\n\nWe revise Formula (10) in the latest version, and provide the detailed derivation of the formula in Appendix J.\n\n**Q2: Whether ANN can adopt SR?**\n\nThanks for pointing it out! Theorem 1 also holds true for ANNs which are differentiable. Therefore, this technique can be employed to enhance the robustness of ANNs. To demonstrate this point, we conduct experiments for ANNs with VGG11 architecture on the CIFAR-10 dataset. The classification accuracy (%) is reported in Table R2-1, which demonstrates that our can also boost the robustness of ANNs.\n\n**Table R2-1. Performance of ANNs on CIFAR-10 dataset**\n| ANN_Models   | Clean | FGSM  | PGD10 | PGD30 | PGD50 |\n|--------------|-------|-------|-------|-------|-------|\n| Vanilla      | 92.91 | 6.62  | 0.00  | 0.00  | 0.00  |\n| SR (\u03bb=0.004) | 88.64 | 48.96 | 31.39 | 27.24 | 26.77 |\n\n**Q3: Whether the proposed method will affect the robustness under random attack?**\n\nThanks for your suggestion. To evaluate that, we report the classification accuracy (%) of models trained with SR and SR*(PGD5+SR) under random attack (eps=0.1) with WideResNet-16 on CIFAR datasets. The results are shown in Table R2-2, we can find that the SR and SR* strategy can also improve the robustness of SNNs against random attacks.\n\n**Table R2-2. Compare the random robustness of different methods on CIFAR datasets**\n|           |               | Vanilla | SR     | SR*    |\n|-----------|---------------|---------|--------|--------|\n| CIFAR-10  | clean         | 93.89   | 91.94  | 85.63  |\n|  CIFAR-10 | random attack | 67.467  | 86.327 | 81.885 |\n| CIFAR-100 | clean         | 74.59   | 67.81  | 60.37  |\n| CIFAR-100 | random attack | 26.27   | 49.9   | 48.678 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156178468,
                "cdate": 1700156178468,
                "tmdate": 1700386687788,
                "mdate": 1700386687788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6gT4DnnPcs",
                "forum": "ZN8BaYVFkx",
                "replyto": "At01a08ebc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_JC8H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_JC8H"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the detailed explanation and additional experiments, and my concerns have been solved. This work is significant as it theoretically analyzes random and adversarial attacks. Additionally, addressing the security issues in SNN is crucial, making this research valuable for both SNN security and broader AI security considerations. Based on the authors' reply, I would like to raise my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659335467,
                "cdate": 1700659335467,
                "tmdate": 1700659335467,
                "mdate": 1700659335467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qw5yLT9iIe",
            "forum": "ZN8BaYVFkx",
            "replyto": "ZN8BaYVFkx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission24/Reviewer_bv7g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission24/Reviewer_bv7g"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the adversarial robustness of spiking neural networks. First, the authors observe that SNNs exhibit robustness against random perturbations, but display vulnerability to small-scale adversarial perturbations. After that, the authors derive some theoretical results on the bounds of the gap between the robustness of SNNs under these two kinds of perturbations, and show that it is upper bounded by the sparsity of gradients of the output probability with respect to the input image. Motivated by such observations and theoretical bounds, the authors propose an algorithm to add the gradient sparsity regularization term to the loss function during SNN training to narrow the gap between these two kinds of perturbations. Various experimental results on the CIFAR-10 and CIFAR-100 datasets show that the proposed algorithm enhances the robustness of SNNs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: The related works are adequately cited. The main results in this paper will certainly help us have a better understanding of the adversarial robustness of spiking neural networks. I have checked the technique parts and found that the proofs are solid. Some strengths of this paper are listed below:\n1. The authors provide several useful observations and theoretical bounds on the robustness against random perturbations and small-scale adversarial perturbations, and derive the upper bound of the gap between the robustness of SNNs under these two kinds of perturbations.\n\n2. Based on the observations and theoretical bounds, the authors proposed a novel loss function involving the gradient sparsity regularization term, which could improve the robustness of SNNs.  \n\n3. Various results verify the effectiveness of their proposed algorithms.\n\nQuality: This paper is technically sound.\n\nClarity: This paper is well written. I find it is easy to follow.\n\nSignificance: I think the results and the proposed algorithm in this paper are significant, as explained above."
                },
                "weaknesses": {
                    "value": "1. The paper conducts the experiments on the CIFAR-10 and CIFAR-100 datasets. Is it possible to conduct the experiments on more large-scale datasets, such as ImageNet or ImageNet-Tiny datasets? \n\n2. It requires some assumptions for Theorem 1 to be true, such as the function $f$ should be differentiable, and $\\epsilon$ should be small enough, could you add these assumptions to Theorem 1?\n   \nSome other minor questions:\n1. Line 4 in Proposition 1, Page 6,  The proof is provides -->  The proof is provided.\n2. The last line in (17), Page 15,  $\\epsilon$ -->  $\\epsilon^2$."
                },
                "questions": {
                    "value": "Please see the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission24/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission24/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission24/Reviewer_bv7g"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission24/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698920830488,
            "cdate": 1698920830488,
            "tmdate": 1700677389225,
            "mdate": 1700677389225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "flU3mPjFks",
                "forum": "ZN8BaYVFkx",
                "replyto": "qw5yLT9iIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bv7g"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments. We are encouraged that you find our paper technically sound, well-written and our results significant. We would like to address your concerns and your questions in the following.\n\n**Q1: Experiments on more large-scale datasets**\n\nWe trained three VGG-11 models on the TinyImageNet dataset: a vanilla model with no robustness enhancement (Vanilla), an FGSM [eps=2/255] adversarially trained model (AT), and a model using both FGSM [eps=2/255] adversarial training and sparsity regularization (SR*). Throughout training and evaluation, we cropped all images to a resolution of 64x64, which is more complex compared to the 32x32 images from CIFAR-10 datasets. \n\nUnder these conditions, the SR* model exhibited the best performance, achieving an accuracy of 10.82% under PGD50 attack and 14.89% accuracy under FGSM attack. In comparison to the pure AT model, the SR* model showed significantly better robustness against various attacks, highlighting the effectiveness of our method. \n\n**Table R1. Performance on TinyImageNet dataset**\n| ANN_Models           | Clean | FGSM  | PGD10 | PGD30 | PGD50 |\n|--------------        |-------|-------|-------|-------|-------|\n| Vanilla                       | 58.23 | 0.64  | 0.00  | 0.00  | 0.00  |\n| AT (FGSM, eps=2/255)          | 52.17 | 11.06 | 6.09  | 5.25  | 4.96  |\n| SR* (FGSM, eps=2/255 \u03bb=0.001) | 43.44 | 14.89 | 11.49 | 10.97 | 10.82 |\n\n**Q2: Add assumptions to Theorem 1**\n\nThanks for your suggestion! In our revised version, we have incorporated the assumptions that the function $f$ is differentiable and that $\\epsilon$ is sufficiently small into Theorem 1.\n\n\n**Q3. Other minor questions.**\n\nThanks for pointing it out! We have revised these typos and updated our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154837829,
                "cdate": 1700154837829,
                "tmdate": 1700386637512,
                "mdate": 1700386637512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mM7qvRh2PP",
                "forum": "ZN8BaYVFkx",
                "replyto": "flU3mPjFks",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_bv7g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission24/Reviewer_bv7g"
                ],
                "content": {
                    "title": {
                        "value": "Thanks the efforts of authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the response and it has sufficiently addressed my questions.After reading the other reviews, I still believe it is a good paper that will be of interest to the SNNs community. Therefore, I would like to keep my rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission24/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677566176,
                "cdate": 1700677566176,
                "tmdate": 1700677566176,
                "mdate": 1700677566176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]