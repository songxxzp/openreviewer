[
    {
        "title": "Meta-Referential Games to Learn Compositional Learning Behaviours"
    },
    {
        "review": {
            "id": "iJ0vju6qNt",
            "forum": "17BA0Tl2Id",
            "replyto": "17BA0Tl2Id",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_CPjL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_CPjL"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a benchmark for studying the ability of learning agents (in particular, multiagent RL learners) to learn compositional learning behaviors. The benchmark uses a meta-learning variant of referential games to instantiate this idea. The authors propose a \"symbolic continuous stimulus\" (SCS) representation to encode the semantic symbolic information in a domain-agnostic way, and then construct the datasets by drawing samples directly in this SCS space. The experimental evaluation shows that current approaches struggle to learn compositional learning behaviors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "######## Strengths ########\n- The overview of the problems of systematicity/compositionality, lingustic compositionality, and compositionality of Sec 2 is valuable and interesting \n- The problem of compositionality and compositional generalization is of interest to a large portion of the AI/ML community. Benchmarks in this direction are potentially highly impactful\n- The experimental evaluation appears to be complete and useful (though some discussion is missing)"
                },
                "weaknesses": {
                    "value": "######## Weaknesses ########\n- The description of the SCS is convoluted and hard to follow\n- The overall evaluation protocol of the meta-referential games is not sufficiently clear\n\n######## Recommendation ########\n\nI recommend accepting the paper. The technical quality of the submission is high, the problem is of interest, and the benchmarking results demonstrate that existing methods struggle to solve the benchmark. I do have several suggestions for improvement which I hope the authors take. \n\n######## Arguments ########\n\nThe main technical contribution of the paper is the problem formulation of meta-referential games and a synthetic benchmark that studies the setting. The idea is that, given a sufficient number of systematic generalization training problems, the listener/speaker agents should be able to learn a compositional learning behavior, such that they can generalize compositionally _in a new systematic generalization problem_. One additional technical contribution is the SCS, which is a domain-agnostic representation of a symbolic space. Unlike one-hot encodings, whose size depends on the number of values that each dimension can take, the SCS has a fixed size given a chosen dimensionality. For the benchmark, this implies that the different \"tasks\" can use varying semantic structures and the agents should still be able to meta-learn a compositional behavior.\n\nI also appreciate the discussion of systematicity and disentanglement, though I have some comments/questions about that. \n\nI have a few suggestions for improvement, which I think are necessary in order for the paper to be a complete technical contribution, which I summarize below:\n\n- Details of the SCS\n    - It's unclear what the tuple (d(i))_i... means. The authors then say that the \"shape of a stimulus ... is a vector over [-1,+1]^N_dim\". Is the shape a vector or is the representation a vector? If the vector is over [-1,+1] on every input, where does the d(i) the tuples factor in? The authors themselves state that the shape doesn't depend on the d(i)'s. \n    - The later description says l(i) \\in [1; d(i)] -- what does [1; d(i)] mean? is it the same as [1, dim(i)]? It seems that the authors might be using the two notations interchangeably\n    - My understanding is that for every dimension i, l(i) picks an \"index\" from 1 to d(i), which is precisely the value of the stimulus at dimension i. Then, a Gaussian is sampled around that index with a small enough variance such that all samples fall near l(i) and are not confused with l(i)-1 or l(i)+1. If this is the case (which I think Fig. 3 confirms), the authors should attempt to make their textual description a bit clearer. As it stands, it is a bit convoluted. \n    - The authors should carefully incorporate the answers to these questions and a cleaner explanation of the SCS in text.\n- Evaluation protocol of the meta-RGs\n    - My understanding of the first few lines is that generating \"differently semantically structured\" spaces is akin to generating many SCAN datasets. So each generated space is 1 SCAN dataset, and our goal will be to meta-learn a strategy that enables solving the ZSCT of a new SCAN dataset?\n    - \"a meta-referential game is composed of two phases\" -- I'm confused by this. Isn't each RG itself composed of two phases, and the meta-RG a wrapping process that presents the two agents with many such RGs?\n    - The authors put considerable efforts toward explaining the overall evaluation/training process, but it still doesn't appear to come through clearly. There are RGs and meta-RGs, shots and episodes. Each shot is a series of RGs. It is unclear exactly how all these pieces interact. I think the manuscript would leverage from one algorithm block that summarizes the overall process. For example:\n```\nAlgo: Meta-RG evaluation process\n\n    Meta-training phase:\n    for episode in NumberOfEpsiodes // loop over tasks=episodes\n        draw semantic structure\n        for shot in NumberOfShots   // loop over ...\n            draw component values\n            for RG in ...\n                draw stimulus\n                ...\n    Meta-testing phase:\n    freeze speaker\n    ...\n```\n    - The textual description is just too complex to come across clearly. Having an algorithmic description (and relying on it by referencing it in the textual description) might make things a lot clearer. \n    - But overall, my understanding is that the agent faces a set of meta-training settings, each of which fixes one symbolic space and consists of many training RGs and zero-shot RGs. Then the agent faces meta-testing RGs, which presumably have little data?"
                },
                "questions": {
                    "value": "######## Additional feedback ########\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nIntro\n- I'm not really sure I follow how the authors' view of online/offline relates to the RL view\n\nSec 2\n- Fig. 1 -- why does the receiver also observe the state? Is it just a \"noisy\" version of the state w distractor stimuli?\n- My understanding: the sender receives 1 input and communicates (potentially back-and-forth) with the listener, who additionally receives a set of inputs (potentially including the speaker's input or the same \"object\"). The task is for the receiver to determine, given messages from the sender, whether any of its observed stimuli match the speaker's. Some of this isn't explicitly stated, so it required looking at the figure. If there is such a 1-sentence explanation, I encourage the authors to include it at the beginning of their explanation before diving into the specific properties/variations. \n- This section is a perhaps too philosophical discussion of the relations between disentanglement and compositionality, but I don't think that's necessarily a bad thing\n\nSec 3\n- Authors state that in step N+2 the listener observes the input of the listener \"rather than an object-centric samples with the same semantic meaning\" --- but according to the definition, it's not _always_ the same semantic meaning, right? The game is to determine precisely whether the meaning is the same?\n- \"we propose a rule-based speaker\" -- At this point, it seems that the only learning agent is the listener. But then (in Sec 4) the authors apparently clarify that this is only an ablative test to see how well the listener can learn CLBs given a fixed (linguistically compositional) speaker. This should be either omitted from this section or stated more clearly\n\nSec 3.2\n- Vocabulary permutation: I wonder if it would be possible to construct a different stimulus representation that _doesn't_ require permutation to guarantee no cheating. Any insight from the authors on this? (In an ideal world, we would get a proof that no such representation exists, but an intuitive description of why that's difficult would also be valuable.)\n\nSec 4\n- The authors report only results of the test/zero-shot performance. While this is the metric of interest, I wonder if it's possible, because of the difficulty of RL/MARL training, that even training performance is low? That would conflate the standard RL issues witht he issues of CLB.\n\nSec 4.1\n- How is EoA measured? What about topsim/posdis/bosdis? What values should we expect for them? Is higher or lower better?\n- Generally, I would expect a discussion that goes beyond just the zero-shot accuracy\n\nTypos/style/grammar\n- Fig. 1 (and others): authors should use a vector version of the image, not PNG or JPEG -- the size is small and zooming in blurs all letters/symbols\n- Sec 2, \"Compositionality...\" -- \"...the work ofHupkes et al. (2019)\" --> missing space\n- Sec 2, \"Compositionality...\" -- \"... related contents\"(Fodor et al., 1988).\" --> missing space\n- Sec 2, \"Compositionality...\" -- topographic similarity (topsim) vs. posdis (positional disentanglement) -- maintain consistency of abbreviations and parentheses\n- Sec 2, \"Compositionality...\" -- I was initially confused by \"and interchangeably compositional behaviors and systematicity...\" because I thought you would use either of those two interchangeably with \"linguistic compositionality\". It would be clearer to write \"and compositional behaviors and systematicity interchangeably to ...\"\n- Once the authors define the RG acronym, they should avoid going back and forth between RG and referential game\n- Sec 3 -- \"Figure 4(left)\" --> missing space\n- Sec 3.1 -- \"relies on gaussian kernels\" --> capitalize Gaussian (throughout the manuscript)\n- Sec 3.1 -- \"Figure 4(right)\" --> missing space\n- Sec 3.2 -- \"an meta-referential game\" --> \"a meta...\"\n- \"we bring the readers attention on\" --> \"we bring the reader's attention to\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698415842529,
            "cdate": 1698415842529,
            "tmdate": 1699636603961,
            "mdate": 1699636603961,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CS6svEjxGr",
                "forum": "17BA0Tl2Id",
                "replyto": "iJ0vju6qNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1/4)"
                    },
                    "comment": {
                        "value": "Thank you for your very detailed and thorough review and feedback. We address them below.\nPlease let us know if our replies and proposed changes are satisfactory and whether they contribute ton increasing your rating of our paper.\n\n## Regarding 'some discussion [being] missing' :\n\nFollowing the feedback from Reviewer Hoy7, we clarified the experiments section and added a Discussion subsection to try to shore up our arguments.\nPlease refer to our rebuttal to Hoy7 for further details, and do let us know whether you see any other ways to improve the paper on that end.\n\n## Regarding Weaknesses in Description of the SCS :\n\nThank you for your feedback and suggestions for improvement.\n\nWe validate your understanding of the description of the SCS and include answers to your questions into the paper:\n\n1. The representation is indeed the one that is a vector, not the shape. We also clarify how the d(i)'s factor in our reformulation, as follows:\n\n    \"Stimuli in the SCS representation are vectors sampled from the continuous space $[-1,+1]^{N_{dim}}$. In comparison, stimuli in the OHE/MHE representation are vectors from the discrete space $\\{0,1\\}^{d_{OHE}}$ where $d_{OHE} = \\Sigma_{i=1}^{N_{dim}} d(i)$ depends on the $d(i)$'s.Note that SCS-represented stimuli have a shape that does not depend on the $d(i)$'s values, this is the \\textit{shape invariance property} of the SCS representation (see Figure~\\ref{fig:posdis-messages+SCS-vs-OHE_rep}(bottom) for an illustration). In the SCS representation, the $d(i)$'s do not shape the stimuli but only the semantic structure, i.e. representation and semantics are disentangled from each other. The $d(i)$'s shape the semantic by enforcing, for each factor dimension $i$, a partitionaing of the $[-1,+1]$ range into $d(i)$ value sections. Each partition corresponds to one of the $d(i)$ symbolic values available on the $i$-th factor dimension. Having explained how to build the SCS representation sampling space, we now describe how to sample stimuli from it. It starts with instantiating a specific latent meaning/symbol, embodied by latent values $l(i)$ on each factor dimension $i$, such that $l(i)\\in [1;d(i)]$. Then, the $i$-th entry of the stimulus is populated with a sample from a corresponding Gaussian distribution over the $l(i)$-th partition of the $[-1,+1$ range.\"\n\n\n2. We emphasised that \"$d(i)$ is the number of possible symbolic values for each latent/factor dimension $i$\" in order to make it easier to understand the range \"[1,d(i)] as the range from which latent/instantiated symbolic value l(i) is sampled, since there are d(i) such values.\n\n\nPlease let us know if these changes improves the paper and whether you can see any other way for us to make this part of the paper easier to understand.\n\n\n## Regarding Weaknesses in Description of the Evaluation Protocol :\n\nThank you for your detailed feedback, we have addressed some of it while answering to Reviewer qdfE, so we invite you to read our rebuttal and let us know whether our proposed changes are aligned with your expectations too.\nIn the meantime, we are still in the process of trying to address your feedback, but we thought it could be worth to start the conversation already in order to make sure that we are going in the right directions, as the end of the authors-reviewers discussion period is approaching fast..."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494149206,
                "cdate": 1700494149206,
                "tmdate": 1700679898214,
                "mdate": 1700679898214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4BCwe4rWyE",
                "forum": "17BA0Tl2Id",
                "replyto": "CS6svEjxGr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_CPjL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_CPjL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the partial response"
                    },
                    "comment": {
                        "value": "I thank the authors for uploading this partial response. Indeed, these clarifications go a long way toward making the paper stronger. I do not have any further questions, pending the authors' final response. I will wait to receive all information and then consider the response as a whole along with the other reviews to determine whether a change in score is warranted."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585087108,
                "cdate": 1700585087108,
                "tmdate": 1700585087108,
                "mdate": 1700585087108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flVl0JzLL1",
                "forum": "17BA0Tl2Id",
                "replyto": "iJ0vju6qNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (2/4)"
                    },
                    "comment": {
                        "value": "Thank you for your acknowledgement of our clarifications so far being instrumental in making the paper stronger.\nWe are providing further replies and proposed changes below :\n\n\n> So each generated space is 1 SCAN dataset, and our goal will be to meta-learn a strategy that enables solving the ZSCT of a new SCAN dataset?\n\nYour understanding is right. We would add a nuance in the fact each generated space is 1 domain-agnostic and proper-BP-instantiating SCAN dataset.\n\nIndeed, SCAN is on a specificly textual modality, and it does not instantiate a BP because each SCAN stimulus on its own clearly reveal the 'symbolic values' that they instantiate (which are words/tokens).\n\n> \"a meta-referential game is composed of two phases\" --I'm confused by this. Isn't each RG itself composed of two phases, and the meta-RG a wrapping process that presents the two agents with many such RGs?\n\nThe meta-RG is indeed a wrapping process that presents the two agents with many RGs, but the first RGs have a supporting role for the agents to learn about the symbolic space that they are dealing with on the current episode, and the last RGs are meant for us to evaluate the agents on their ability to solve ZSCTs in this new context, i.e. ability to perform CLBs.\n\nThus, we clearly mark a distinction between the supporting phase and the querying phase based on which type of stimulus is presented to the agents.\n\nRegarding your comment that common RGs themself are also 'composed of two phases', we are not sure what you are referring to?\n\n- If you mean to refer to the training and testing phase, then we mean to argue that those have more to do with the supervised learning loop in which common RGs are usually instantiated. Please refer to Algorithm 4 for clarification.\n\n- If you mean to refer to the first phase of a RG as being when the speaker operates, and the second phase as being when the listener operates, then we acknowledge that this is a possible framing of a RG, but we actually abide by a different framing which is based on communication rounds. Please refer to Denamganai et al., 2020a, or Algorithm 3 in Appendix A which details how a RG is defined in our framework.\n\n> The authors put considerable eorts toward explaining the overall evaluation/training process, but it still doesn't appear to come through clearly. There are RGs and meta-RGs, shots and episodes. Each shot is a series of RGs. It is unclear exactly how all these pieces interact. I think the manuscript would leverage from one algorithm block that summarizes the overall process.\n\nThank you for your advice of adding an algorithm block. We have added 5 of those in order to try to explain in as many details as necessary the nuances that are involved.\n\nThroughout Section 3, we have added references to the different lines of those algorithm in order to provide landmarks to the readers.\n\n> But overall, my understanding is that the agent faces a set of meta-training settings, e ach of which fixes one symbolic space and consists of many training RGs and zero-shot RGs. Then the agent faces meta-testing RGs, which presumably have little data?\n\nWe mean to highlight a misunderstanding here: the meta-training and meta-testing settings are maybe misleading naming :\n\nIn a supervised learning loop, what differentiates training from testing settings are the data being used in each setting, having defined training and testing splits of the dataset.\n\nBut, in the case of meta-learning with a distribution of task/dataset, each new episode that is generated by a new random seed (achieved by having the explicitly keeping a counter that is increasead by one at each reset of the environment, and this new counter value is used to seed the next environment) consist of never-before-trained-on data, therefore it is given that any new episode can be considered a testing-purpose episode.\n\nThus, our algorithm 5 is not split around a meta-training and a meta-testing phase, but rather solely around meta-RL episodes and we report the ZSCT accuracy on each episode as our testing performance metric because each new episode involves novel data that can be treated as test before the agents later use that data for training.\n\nIt does not invalidate our testing process since this data will never be seen by the agents again since the random seed will keep on increasing."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651084056,
                "cdate": 1700651084056,
                "tmdate": 1700679888033,
                "mdate": 1700679888033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "REaNDaBZi1",
                "forum": "17BA0Tl2Id",
                "replyto": "iJ0vju6qNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (3/4)"
                    },
                    "comment": {
                        "value": "> My understanding: the sender receives 1 input and communicates (potentially back-and-forth) with the listener, who additionally receives a set of inputs (potentially including the speaker's input or the same \"object\"). The task is for the receiver to determine, given messages from the sender, whether any of its observed stimuli match the speaker's. Some of this isn't explicitly stated, so it required looking at the gure. If there is such a 1-sentence explanation, I encourage the authors to include it at the beginning of their explanation before diving into the specic properties/variations.\n\nThank you for your feedback about the need for a 1-sentence-like explanation around RGs, we have included the following sentences based on yours :\n\n\"In short, the speaker receives a stimulus and communicates with the listener (up to $N$ back-and-forth using messages of at most $L$ tokens each), who additionally receives a set of $K+1$ stimuli (potentially including a semantically-similar stimulus as the speaker).\n\nThe task is for the listener to determine, given communication with the speaker, whether any of its observed stimuli match the speaker's.\"\n\n> Authors state that in step N+2 the listener observes the input of the listener \"rather than an object-centric samples with the same semantic meaning\" ---but according to the denition, it's not always the same semantic meaning, right? The game is to determine precisely whether the meaning is the same?\n\nYes, your understanding is correct, and we realise that our sentence makes it ambiguous. Thank you for your feedback, we reformulate this sentence as follows: \n\"Next, step $N+2$ is intended to provide feedback to the listener agent as its observation is replaced with the speaker's observation (cf. line $12$ and $18$ in Alg.5). Note that this is the exact stimulus that the speaker has been observing, rather than a **possible** object-centric sample.\"\n\n> \"we propose a rule-based speaker\" --At this point, it seems that the only learning agent is the listener. But then (in Sec 4) the authors apparently clarify that this is only an ablative test to see how well the listener can learn CLBs given a xed (linguistically compositional) speaker. This should be either omitted from this section or stated more clearly\n\nThank you for your advice, we are now omitting this sentence from this section and only unveiling it during the Experiment section.\n\n> Vocabulary permutation: I wonder if it would be possible to construct a dierent stimulus representation that doesn't require permutation to guarantee no cheating. Any insight from the authors on this? (In an ideal world, we would get a proof that no such representation exists, but an intuitive description of why that's dicult would also be valuable.)\n\nThe vocabulary permutation scheme is not only a trick to prevent the the agents from building a cheating language, it is also a legacy feature from the Emergent Communication field building towards AI's ability to communicate with 'strangers'/novel partners.\n\nWe ~~(will)~~ discuss this matter further in a subsequent response to Reviewer Hoy7's reply to our rebuttal.\n\nFollowing your prompt to build a symbolic space that would inherently guard against the emergence of a cheating language, we would propose **presenting** stimuli on a different range than the $[-1, +1]$ range that is currently used, for instance by applying an affine transformation (i.e. offset and rescaling) to the whole representation space before showing stimulus to the agents.\n\nIndeed, by applying a different randomly sampled affine transformation at each episode, we would prevent the agents from being able to expect the stimulus values to be bounded (with a max and min) or to have a minimal precision requirement.\n\nThus, the agents would not be able to build an analog-to-digital conversion-inspired language since the two properties of the data to make such a conversion would be lacking.\n\nWe have not experimented with it yet, but we are looking into this in order to relax the 'communication with stranger' aspect of the benchmark in a subsequent work, where the constructivity aspects can be studied with even less confounding variables.\n\nWe might be able to present some preliminary results of that context in the final version of the paper, should it be accepted for publication, but the current author-reviewer discussion period does not give us enough time to run all the necessary experiments, and we also think that the paper is already 'meaty'-enough as it is...\nPlease let us know your though on the matter."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651174307,
                "cdate": 1700651174307,
                "tmdate": 1700679877820,
                "mdate": 1700679877820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xA9x3xVKxa",
            "forum": "17BA0Tl2Id",
            "replyto": "17BA0Tl2Id",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_Hoy7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_Hoy7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a emergent communication benchmark/game called the\nmeta-referential game.  It is based on the familiar referential game from the\nEC literature but is posed in a meta-learning framework which requires the\nagents to establish communicative conventions within an episode of iterated\nreferential games.  Such a game requires agents to learn to dynamically acquire\nlanguage (i.e., over the course of an episode) rather than simply learn\na static mapping as happens in the standard referential game.  Empirical\nanalysis adds some context to how baseline approaches fare in different\nhyperparameter settings of the benchmark/game."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- (major) The benchmark introduces this concepts of receptivity and\n  constructivity (i.e., the ability to establish linguistic conventions within\n  an episode) into emergent language.  These are indeed present in human\n  language behavior but not often (if at all) discussed in the context of\n  emergent language.\n- (major) The meta-referential game is largely an appropriate extension to the\n  referential game which introduces the necessary concepts for intra-episode\n  learning without making too many changes (i.e., which could introduce too\n  many confounding factors)."
                },
                "weaknesses": {
                    "value": "- (major) The empirical results are difficult to interpret in a meaningful way\n  since the main ones are negative, and there are not many clear trend in the\n  rest.  While the primary contribution of this paper is the benchmark, it is\n  tough to see whether or not it will be of practical use based on the\n  empirical results presented.\n- (minor) On the level of clarity, the paper uses a lot of jargon that is a bit\n  distracting.  Even if most of these terms are defined, it makes for\n  a difficult read.  This could just be a background mismatch is I come from an\n  NLP/RL/emergent communication background.  Technical terms do make things\n  clearer and more precise in moderation, but when they proliferate, it\n  obscures instead.  Some terms I'm referring to:\n  - binging problem (\"binding\" itself is never actually defined, I think)\n  - compositional learning behavior\n  - reflexivity and constructivity\n  - object-centric versus stimulus-centric\n  - Chaa-RSC and Hill-RSC\n  - shape invariance property and semantically structured symbolic spaces\n  - Symbolic Continuous Stimulus\n- (minor) The \"Symbolic Continuous Stimulus\" seems to be a bit more complicated\n  than it needs to be; namely with the many layers of sampling (i.e., the\n  number of partition, the size of the partitions, the parameters of the\n  Gaussian, then the Gaussian itself) that just create the data distribution.\n  I do see how some of this is necessary to prevent confounding factors, but\n  I think preemptively ramping the complexity of the benchmark when it is not\n  even clear that current models can do much better than random chance might\n  not be the right move."
                },
                "questions": {
                    "value": "What do the empirical results show?  And how do these findings support the\nbenchmark?\n\n### Misc comments\n\n- It is a little confusing with all of the parameters \"shots\", \"steps\",\n  \"games\", \"meta games\" (although I understand why these are necessary).  To\n  alleviate this somewhat, it might be worthwhile to include a table that just\n  lists a sample set of interactions, observations, etc. in a table format\n  (which could definitely could be hand written/not real) to give a sense of\n  what the parameters correspond to.\n\n- Page 1\n  - \"In this work, we will primarily...\": don't use a \"respectively\" sentence structure here, it makes it very difficult to read this important sentence.\n- Page 2\n  - The definition of the binding problem is not clear at all since what \"binding\" actually is never defined -- it's somewhat circular\n  - \"(Lazaridou and Baroni, 2020)\" - use `\\citet`\n- Page 4\n  - \"semantical\" -> \"semantic\"\n  - \"S2B\" -> \"SB2\"? The postfix two usually represents a superscript.\n- Page 5\n  - \"segregated\" -> \"segregate\"\n  - First paragraph of Sec 3.1 was difficult to understand on the first\n    read-through.  It was clearer reading it a second time (after reading\n    through the whole paper), and think the reason is because SCS is not\n    discussed in detail until after this paragraph despite the fact that the\n    nature of SCS is important to understanding this paragraph.  This is\n    coupled with the fact that the \"binding problem\" is never full defined\n    (i.e., what \"binding\" is in the first place).\n  - Figure 2: what is the difference between the \"object-centric target\n    stimulus\" and the \"target stimulus\"?\n  - \"but not larger than the size of the partition section it should fit in\":\n    not possible since Gaussian distributions have infinite support for any\n    non-zero standard deviation.  Does SCS use rejection sampling to ensure\n    that out-of-bound samples do not get passed along?\n  - maybe just have uniform sampling from the partitions or just have Gaussian\n    sampling from a list of means\n  - how are the spaces partitioned?\n  - What is the structure of a semantic space, just the layout of partitions?\n- Page 6\n  - What is the \"shape invariance property\"?\n  - \"an meta-referential\" -> \"a meta-referential\"\n  - Figure 2: maybe referring to a \"referential game\" as a \"round\" would be\n    clearer\n  - \"attention on the fact\" -> \"attention to the fact\"\n  - Not clear what a \"random permutation of the vocabulary symbols\" means.\n- Page 7\n  - 4.Agent Architecture - It would be best to at least give a 2-sentence\n    summary of the arch.\n  - Adding this auxiliary loss definitely merits discussion in the overall\n    context of the benchmark, i.e., how it might affect what the benchmark\n    would and would not show.\n  - \"make emerge a new language\": rephrase; maybe \"invent a new language\"?\n  - \"resolution approach\": rephrase\n  - \"K = 0\": Seems out of place to parameterize a value when it is just going\n    to result in a binary task.\n  - \"goads us to think\" -> \"leads us to think\"\n  - Sec 4.2.1 - It is difficult to tell here if the results are showing\n    anything significant."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698529166768,
            "cdate": 1698529166768,
            "tmdate": 1699636603855,
            "mdate": 1699636603855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sRduzpugJa",
                "forum": "17BA0Tl2Id",
                "replyto": "xA9x3xVKxa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for your very detailed review and feedback, it is greatly appreciated.\nWe address them below:\n\n## Major Weakness Feedback :\n\nThank you for your feedback on the difficulty to interpret the paper's empirical results and the lack of trends in our experiment section.\nWe address it as follows:\n\nFirstly, we adapt the experiment's  titles to highlight the main takeaways and make it easier for the reader.\n\nSecondly, we re-framed the paragraphs introducing each experiments in order to make it clearer what is the trend/narrative in our experiments. \nWe used this opportunity to better emphasise the two main aspects of the problem, to wit the receptivity and constructivity aspects of CLBs, following your highlighting of those elements as major strengths of the paper.\n\nThen, we simplified the experiments of the main text, by pushing into the appendices our investigation of the impacts of memory and sampling budget on performance.\n\nThe space that we gained was used to include a Discussion subsection that summaries and re-frame the results in order to clarify the practical use of the benchmark.\nPlease refer to this as our answers to your two explicit questions on top of the  'Questions' section of your review.\n\nPlease let us know if these changes enhance your appreciation of the paper and/or whether you have any other suggestions.\n\n## Minor Weaknesses Feedback:\n\nThank you for your feedback, we have tried to reduce our usage of jargon and acronyms while abiding by the page limit.\nPlease let us know if you see any specific approach to further improve on that end.\n\n## Misc Comments :\n\nThank you for your miscellaneous comments, we address as much as possible in the following:\n\n- We reframed the critical sentence that previously used a 'respectively' framework.\n\n- We added [/re-use] as a sinonymous expression to 'bind' in our citation of Greff et al., 2020, and elaborated further what it means to solve the binding problem in the following terms: 'Solving the BP instantiated in such a context, i.e. re-using previously-acquired information in ways that serve the current situation, [...]'\n\n- We added to the main text the following sentence in order to clarify what is the relationship between the 'target stimulus' and the 'object-centric target stimulus' : `The adjective `object-centric' is used to qaulify a stimulus that is different from another but actually present the same meaning (e.g. same object, but seen under a different viewpoint).'\n\n- SCS does not use rejection sampling to ensure that out-of-bound sampled do not get passed along, but the standard deviation is actually sampled from a range that is tighter than the actual available space, so we assume this to be sufficient.\n\n- The structure of a semantic space is indeed the layout of partitions, if by 'layout' you refer to the parameters of the Gaussian kernels.\n\n- Regarding the 'shape invariance property', please refer to our answer to Reviewer 3drE. We address the feedback by adding a figure highlighting the shape invariance property (Figure 4 (bottom)).\n\n\nOnce again, we thank you for your review and feedback. Please let us know if our replies and made changes increase your ratings of our paper, and whether, in light of other reviews and rebuttals, you think of anything new to improve the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175115131,
                "cdate": 1700175115131,
                "tmdate": 1700466514439,
                "mdate": 1700466514439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BE63btVPQ2",
                "forum": "17BA0Tl2Id",
                "replyto": "sRduzpugJa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_Hoy7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_Hoy7"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the author's response, and I think the paper has been improved through that.\nIn light of this, I will raise my rating of \"Presentation\" from a 2 to 3.\nI will ultimately decide not to raise my overall rating of the paper above 6.\nI do think the paper is making _a_ contribution insofar as it is introducing a benchmark for compositional learning behaviors, which are themselves interesting.\nWhile they may be interesting, I am not convinced that they are important in a practical way.\nTo illustrate what I mean, I'll start with this quote from the paper:\n\n> [The] results validate the need for our benchmark and they highlight that\n> our efforts should be focused on constructivity aspects of CLBs.\n\nI think the importance of CLBs is apparent from a conceptual level, but what the experiments and results do not show is that CLBs (and specifically this benchmark) are particularly important to the development of emergent communication as a whole.\nThat is, I am not convinced that this benchmark would see widespread use in EC research as it progresses towards its own practical goals.\nI believe it has some use, hence my rating of a 6, but I am not sure it has a major use, which is what keeps me from championing this paper with an 8."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607111537,
                "cdate": 1700607111537,
                "tmdate": 1700607111537,
                "mdate": 1700607111537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OojxWvYgt1",
                "forum": "17BA0Tl2Id",
                "replyto": "xA9x3xVKxa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the Potential Widespred Use of our benchmark and CLBs (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your response and acknowledgement of the presentation of the paper having improved.\nThank you for your feedback regarding your doubts that CLBs and our benchmark at large can be interesting from a practical viewpoint and that they might not have widespread use.\nWe would like to discuss this point further and attempt to provide different arguments in order to argue that they might get widespread use, and especially not solely in the EC field but for the whole of the AI field :\n\n## Argument 1 : \nOur benchmark proposes a synthesis between Chaa-RSC and Hill-RSC towards identifying conditions/causes that lead to the emergence of systematicity in connectionnist architectures (neural network-based architectures - NNs).\nWe stress that Chaa-RSC stems from the work of Chaabouni et al. [1] which, with its 96 citations over the last 2 years, represents a very important work in the Emergent Communication subfield and beyond, as the number of citations highlights.\nOur benchmark provised an avenue for related research questions to be addressed by exposing relevant parameters and also providing an opening to the related hypotheses made by Hill et al [2], coming from the language grounding and embodied AI fields.\nHill et al.[2] has collected 93 citations so far, thus showing that it was an important contribution.\nSince our benchmark builds a bridge between these two (too-often-separated) subfields of NLP, by linking two papers that have garnered a more than fair interest, we argue that our benchmark is bound to be a fertile ground for subsequent and related research stemming from both the subfields of EC, language grounding, and Embodied AI.\n\n[1] : Chaabouni, Rahma, et al. \"Compositionality and Generalization In Emergent Languages.\"\u00a0_Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_. 2020.\n\n[2] : Hill, Felix, et al. \"Environmental drivers of systematicity and generalization in a situated agent.\"\u00a0_International Conference on Learning Representations_. 2019."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667222572,
                "cdate": 1700667222572,
                "tmdate": 1700667238271,
                "mdate": 1700667238271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5XKLmRCevv",
            "forum": "17BA0Tl2Id",
            "replyto": "17BA0Tl2Id",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_qdfE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_qdfE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Symbolic Behaviour Benchmark (S2B) to evaluate compositional learning behaviors (CLBs), especially the domain-agnostic binding problem (BP) instantiated by Symbolic Continuous Stimulus (SCS) representation.\nIt proposes a framework of Meta-Referential Games, a meta-learning extension of referential games (RGs).\nThe baseline results and error analysis show it is a compelling challenge.\nIt helps to make artificial agents collaborate with humans."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The benchmark evaluates compositional behavior and binding problems, which are important problems in artificial intelligence.\n\n- It proposes the Symbolic Continuous Stimulus instead of using the one-hot or the multi-hot encoded schemes.\n \n- It proposes the Meta-Referential Games framework, which extends common referential games."
                },
                "weaknesses": {
                    "value": "The main concern is that the benchmark may lack novelty.\nCompared with common referential games, the proposed benchmark has SCS stimuli representation and the meta-learning extension.\n\n(1) **Is the selection of representation essential for the benchmark of compositional generalization?**\n\nThe SCS representation has the advantage over one-hot or multi-hot representation.\nHowever, it might not be essentially very important for the game framework.\nFor compositional generalization, the core point is that the test data has new combinations of stimuli.\n\n(2) **The Meta-Referential Game framework and common referential games seem to have a similar protocol, so why only one of them is meta-learning?**\n\nIn the Meta-Referential Game framework, a game (episode) has a training phase and a test phase.\nDo common referential games also have these two phases?\nIf so, it seems strange to say the Meta-Referential Game framework is a \"meta-learning \" extension to common referential games.\n\nIn the proposed framework, the stimuli in test RGs are recombined in novel ways, different from common referential games. Still, this difference seems not related to whether it is a meta-learning framework or not."
                },
                "questions": {
                    "value": "(3) Does the SCS still have the advantage when used in general compositional generalization problems? How about in i.i.d. problems?\n\n(4) It might be more reader-friendly to increase the size of the figures or the font size in the figures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766890162,
            "cdate": 1698766890162,
            "tmdate": 1699636603739,
            "mdate": 1699636603739,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VtR6YvFvZS",
                "forum": "17BA0Tl2Id",
                "replyto": "5XKLmRCevv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review and feedback, we proceed to address you questions below, as well as a possible misunderstanding.\nPlease let us know whether those answers (below) and proposed changes (cf. revised paper) are helpful in increasing your ratings of our paper.\n\n## Regarding your main concern: lack of novelty :\n\nWe mean to bring your attention on a possible misunderstanding:\nYour review states that our benchmark lacks novelty (first sentence of the weaknesses paragraph) and that it 'evaluates compositional behaviours' (CBs) (cf. first strength bullet).\n\nThere are indeed many benchmark that **only** evaluates CBs, for instance:\n- SCAN [1] ;\n- gSCAN [2]; \n- COGS [3];\n\nBut, we mean to emphasise that our benchmark aims to evaluate compositional **learning** behaviours (CLBs). \nDespite the fact that 'CLB' is linguistically a very similar referring expression to 'CB', it is in fact a very different problem to tackle.\nThe problem of learning CLBs does involve compositionality like that of learning CBs, but it adds an extra difficulty to the task:\nIf we define CBs as \"the ability to generalise from combinations of known, **trained-on** atomic components to novel re-combinations of those very same atomic components\", then we can define CLBs as \"the ability to generalise from a few combinations of never-before-seen atomic components to novel re-combinations of those very same atomic components'.\nCLBs involve a few-shot learning aspect that is not present in CB.\n\nThus, CLBs have not been addressed in the AI literature so far, thus making it a novel problem to consider, as we attempted to detail in Section 5 Related Works.\n\nPlease let us know if this clarifies any possible misunderstanding.\n\nWe are re-writing the first and third paragraph of the introduction in order to shore up these aspects of our work, adding reference to [4] and more.\n\nWe would like to propose to maybe rename CBs into Compositional Inference-only Behaviours (CIBs) ?\nIf you think this is worth doing, please let us know?\n\n\n[1] : Lake, Brenden, and Marco Baroni. \"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks.\" International conference on machine learning. PMLR, 2018.\n\n[2] : Ruis, Laura, et al. \"A benchmark for systematic generalization in grounded language understanding.\" Advances in neural information processing systems 33 (2020): 19861-19872.\n\n[3] : Kim, Najoung, and Tal Linzen. \"COGS: A compositional generalization challenge based on semantic interpretation.\" arXiv preprint arXiv:2010.05465 (2020).\n\n[4] : Beck, Jacob, et al. \"A survey of meta-reinforcement learning.\" arXiv preprint arXiv:2301.08028 (2023).\n\n## Question 1: Is the selection of representation essential for the benchmark of compositional generalisation?\n\nIn light of our previous clarifying of the possible misunderstanding, we reframe the question to be: 'is the selection of the representation essential for the benchmark of CLBs?'\n\nWe have highlighted in the third paragraph of the introduction how CLBs involve the resolution of a binding problem (BP).\nIn appendix D.1 we provide evidence that the OHE representation does not instantiate a BP, on the contrary to our proposed SCS representation.\nThus, selection of the SCS representation is criticial for evaluation of CLBs (but not for CBs, as your initial question suggested). \n\nShould our paper be accepted, and therefore granted a 10th page of content, then we would propose to re-include appendix D.1 back into the main content.\nPlease let us know if this is satisfying to address your feedback or whether you would like to suggest something else?\n\n## Question 2: The Meta-Referential Game framework and common referential games seem to have a similar protocol, so why only one of them is meta-learning?\n\nWe have included in Appendix algorithms to detail the differences between common RG, sitting inside a supervised learning loop, and our proposed meta-RG which sits inside a meta-RL loop.\n\nWe agree with your assessment that the both of them have similar protocol, up to the extent of the following:\n- the type of data that is considered throughout the protocol ;\n- the position of the agents parameters' update step in the protocol ;\n- and, obviously, the type of skills that they involve: CLBs vs CBs (as emphasised above).\n\nPlease let us know whether you would appreciate us to add anything more than those algorithms (and previously discussed matters) to address your feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699911358106,
                "cdate": 1699911358106,
                "tmdate": 1699911358106,
                "mdate": 1699911358106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YImK3vTm2c",
                "forum": "17BA0Tl2Id",
                "replyto": "kFRQbmumkv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_qdfE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_qdfE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed explanations. I have the following comments.\n\n> Novelty\n\nThe paper\u2019s main topic is CLB, a compositional extension of the problem considered in common reference games (we may call it the learning behavior (LB) problem). By saying \u201dlack of novelty,\u201d I mean it\ndoes not seem difficult to discover the CLB problem, given the LB problem. For a machine learning task,\nthere is a straightforward way for compositional extension by reorganizing data according to atom (factor)\ncombinations, also used in this work (LB to CLB).\n\n> SCS representation\n\nSCS representation is useful for the binding problem (BP) in the CLB problem. However, the (domain-\nagnostic) BP also seems important in common reference games (LB problems). So, the SCS representation\nseems not directly related to compositionality, the paper\u2019s main topic. Instead, it is another dimension of\nextending common reference games.\n\n> Meta-learning\n\nIt seems to me that the position of the agent parameter update is not the essential difference. Common\nreference games can also update parameters at the end of an episode (batch update).\n\nIn Algorithm 5, training stimulus sampling does not depend on model parameter updates. So, all training\nstimuli can be sampled before starting training. More specifically, stimulus sampling (Line 6) and S-support\nupdate (Line 10) are not influenced by other steps (Line 7-9) in the loop. So, Line 6 and Line 10 can be\nin an independent loop inserted between Line 4 and Line 5. Given the training stimuli, the two algorithms\n(Algorithm 4 and 5) can share the rest of the process in an episode."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496223423,
                "cdate": 1700496223423,
                "tmdate": 1700496223423,
                "mdate": 1700496223423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lrLRoP8dzM",
                "forum": "17BA0Tl2Id",
                "replyto": "5XKLmRCevv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your follow-up comments. We appreciate you taking the time to continue this discussion. \n\n## Regarding Novelty:\n\nWe are afraid that we still fear there is a misunderstanding: while we agree with your comment that CLBs do build upon **the behaviours that are required to perform well in the context of referential games** (referring to those behaviours as 'LBs', as you propose the naming), but we disagree on the stance that CLBs involve a 'compositional extension'.\nIndeed, compositionality, or compositional behaviours (CBs), are already studied in the context of referential games[3] and variants, as seen in the field of emergent communication at large [1,2].\nRather, training learning agents to perform CLBs requires a **few-shot learning** adaptation (which is a type of meta-learning [4]), and therefore, we mean to argue that **CLBs involve a meta-learning extension to referential games**.\nWe apologize if we did not make this distinction clear earlier.\n\nWe hope that, in light of this clarification, the rest of the paper can be seen under a better light, and possibly help improve your ratings of our paper.\nPlease let us know if you have any other proposition for us to try to make the paper better.\n\n[1] : Lazaridou, Angeliki, and Marco Baroni. \"Emergent multi-agent communication in the deep learning era.\"\u00a0_arXiv preprint arXiv:2006.02419_\u00a0(2020).\n\n[2] : Choi, Edward, Angeliki Lazaridou, and Nando de Freitas. \"Compositional Obverter Communication Learning from Raw Visual Input.\"\u00a0_International Conference on Learning Representations_. 2018.\n\n[3] : Chaabouni, Rahma, et al. \"Compositionality and Generalization In Emergent Languages.\"\u00a0_Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_. 2020.\n\n[4] : Beck, Jacob, et al. \"A survey of meta-reinforcement learning.\" arXiv preprint arXiv:2301.08028 (2023)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575074860,
                "cdate": 1700575074860,
                "tmdate": 1700575228742,
                "mdate": 1700575228742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6AgJgVHw1g",
            "forum": "17BA0Tl2Id",
            "replyto": "17BA0Tl2Id",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_3drE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5753/Reviewer_3drE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a referential game benchmark to investigate the agent's ability to solve a domain-agnostic binding problem and exhibit compositional learning behaviors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Originality: \n\n    The proposed Symbolic Continuous Stimulus (SCS) and the meta-referential games benchmark built upon it are novel and interesting.\n\n+ Significance:\n\n    Probing and investigating the compositional learning behaviors are important for various machine learning communities."
                },
                "weaknesses": {
                    "value": "- Quality & Clarity:\n    \n    i) I am a bit confused about the claim that the proposed SCS is *shape invariant*. What does this specifically mean in the context of this paper? Would be great if the authors can give a clear definition of this property.\n\n\n   ii) Can the authors provide more insights and explanations about why SCS is a domain-agnostic representation?\n\n\n   ii) What is the architecture used for the Recall task experiment in appendix C.1? Is it possible that the performance gap is caused by the choice of implementation of the agents? My concern is whether the proposed SCS is universally more effective than OHE in terms of BP, regardless of the network architectures. Is there any theoretical evidence of this claim?\n\n  iv) How does the shape invariance property of the SCS representation translate into the meta-referential games?\n\n   v) The description of the meta-referential games is a bit abstract to me. It's also unclear to me how the compositionality is examined through the games. It would be great if the authors can provide an algorithm table to summarize the game procedure and show some game instances to facilitate understanding."
                },
                "questions": {
                    "value": "See the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5753/Reviewer_3drE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783551559,
            "cdate": 1698783551559,
            "tmdate": 1699636603609,
            "mdate": 1699636603609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G4LjGS2QHG",
                "forum": "17BA0Tl2Id",
                "replyto": "6AgJgVHw1g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your review, we address your feedback and questions below:\n\n## Question 1 and 4: What does the SCS being shape invariant specifically mean in the context of this paper and the meta-referential games?\n\nIn this paper, we present some symbolic stimuli to some learning AI agent.\nThe common approach when dealing with symbolic stimuli is to use the one-/multi-hot encoded representation. Since we consider compositionality, we ought to compare to the multi-hot encoded representation which supports compositionality: for instance, when dealing with a semantic space with $N_dim=2$ dimensions, where each dimension represent a different attribute for our semantic space, then each attribute/factor dimension allows instantiation of different values. For instance, on attribute/factor dimension $1$, we might have 3 possible values (meaning that $d(1)=3$), and on the second one, we might have 2 possible values (i.e. $d(2)=2$).\nThen, let us sample a stimulus out of this semantic space: for instance $s=(2,1)$, which instantiate value $2$ on attribute/factor dimension $1$, and value $1$ on attribute/factor dimension $2$.\nTypically, using the multi-hot encoded representation, we would represent this stimulus using $5$ binary digits (i.e. $d(1)+d(2)$), as follows : $(0 1 0 1 0)$.\nIn this representation, the first three binary digits are used for the encoding of the value of the attribute/factor dimension $1$ (since it has $3$ different values that could be instantiated), while the last two binary digits encode the value instantiated on attribute/factor dimension $2$.\n\nOn the otherhand, with the SCS representation, we would be using a vector with $2$ dimensions, one for each attribute/factor dimension of the sampled stimulus.\nWithout the need to enter into the details of this vector, we bring the reader attention onto the fact that the shape of the SCS representation does not depend on the number of possible values that can be instantiated on each attribute/factor dimensions (i.e. the $d(i)$'s), on the contrary to the OHE/MHE representation.\nThis means that if I sample a stimulus from a similar $N_{dim}=2$-semantic space, but with different number of possible values for each attribute/factor dimensions (i.e. different $(d(i))_{i\\in [1,2]}$), then the shape of the SCS representation will not change, it will still be a $2$-dimensional vector. This invariance with respect to the structure of the semantic space is what we refer to as the shape invariant property.\n\nIn this paper, at each meta-referential game, a different structure for the semantic space is sampled, but the same AI agents are used, so it is important that the stimuli representation remains compatible with our AI agent, therefore it is best if it does not change from one meta-referential game to another.\n\n### What about using an OHE/MHE representation based on the maximal number of values that any attribute/factor dimension may have throughout all the possible meta-referential game that we want our agents to play, i.e. $V_{max}$ ?\n\nWhile this would add shape invariance to the MHE representations of all stimuli coming from any semantic space's structure that we would want to consider, it would come with two limitations:\n\nFirstly, using the OHE/MHE representation as such would still imply that we would need to build into the AI agent a limitation on its input format, whereas the SCS representation theoretically allows us to input any stimulus coming from any symbolic space's structure. By using the term theoretically here, we mean to nuance that while we can feed such stimulus to the SCS-expecting AI agent, it remains to be seen whether the AI agent can operate on it, e.g. if we have only trained the AI agent with $V_{max}=5$ then if we suddenly feed it a stimulus that instantiates a 10-th value on a given attribute/factor dimension, then this is requiring the AI agent to perform out-of-distribution generalisation.\nThis is a problem that we hope to tackle in a future work, and thus the SCS representation is necessary to tackle it.\n\nSecondly, the OHE/MHE representation depends on some specific binary digits that directly represent the value instantiated on each attribute/factor dimension. In other words, even with the proposed $V_{max}$-related trick, the OHE/MHE representation still leaks some information to the AI agent about the structure of the semantic space it deals with. For instance, from the observation of a single stimulus whose OHE/MHE representation has its 3rd binary digit toggle to 1, it provides the information that there is at least $3$ possible values on attribute/factor dimension $1$.\nThis kind of information leakage will prevent instantiation of a proper Binding Problem and allow emergence of a cheating language as we described in Appendix C."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699879353735,
                "cdate": 1699879353735,
                "tmdate": 1699879704809,
                "mdate": 1699879704809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KRIV15OLYB",
                "forum": "17BA0Tl2Id",
                "replyto": "6AgJgVHw1g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (2/3)"
                    },
                    "comment": {
                        "value": "## Question 2: Why is SCS a domain-agnostic representation?\n\nBy the word 'domain', we refer to the different modalities that stimuli may come from, e.g. vision (for the visual domain), text, sound, etc...\nThus, by calling the SCS representation domain-agnostic, we imply that there is no longer any tie to any specific domain and therefore the data/stimulus may come from any domain.\n\nVAEs have been shown to being able to encode/embed/map data coming from any domain into their latent space. Therefore, we acknowledge their latent space to be domain-agnostic. And, since   the SCS representation mimics the latent space of an idealised VAE (with Gaussian kernels), we argue that the SCS representation is domain-agnostic.\n\n## Question 3a: What is the architecture of the agent in Experiment C.1 ?\n\nThe agents are the same baseline RL agents used in the main experiments presented in Section 4 (third line of the paragraph starting with 'Evaluation.').\nIn the case of OHE/MHE representation, we adapt the dimension of the input fully-connected layer accordingly though.\nWe can add details in the appendix if you see it necessary, please let us know?\nWe thought that our open-sourcing of the code was sufficient.\n\n## Question 3b : Is it possible that the performance gap is caused by the choice of implementation of the agents?\n\nWe cannot think of any other implementation choice for the MDP instantiated in the Recall task, but please let us know if you think that something specific ought to be tried.\n\n## Question 3c: SCS being universally more efficient than OHE in terms of BP?\n\nCould you clarify further your question, please, as we are confused about what you mean?\nIndeed, the experiment show worse asymptotic performance on the Recall task when using SCS, compared to using OHE.\nThus we are confused about what you refer to when calling the SCS being more efficient that the OHE.\n\n## Question 5a: Addition of an algorithm table to better detail the meta-referential game \n\nThank you for your suggestion, Reviewer CPjL also mentions it, we will add it to the appendix and attempt to draw a comparison with the normal referential game, sitting within a supervised learning loop, versus the meta-learning loop in which the meta-referential game sits.\n\n## Question 5b: Details about how the compositionality is examined:\n\nThank you for your feedback, but could you clarify which compositionality examination is ambiguous, please? Is it the linguistic one or the behavioural one, and which part exactly?\n\nThe behavioural compositionality, the one that is relevant in regards to performing CLBs, is examined via the ZSCT accuracy, which is the accuracy reported over the RG taking place during the querying/testing-phase of an RL episode.\n\nIt is detailed in Section 3.2. We reframe the section by departing from the terms 'training' and 'testing' and replacing them with 'supporting' and 'querying' in order to further emphasise the few-shot, meta-learning framework, and remove any confusion about when are agents' parameters update taking place.\nMoreover, we are adding in the Meta-RG algorithm (Algorithm 5) details about the ZSCT accuracy measure during the querying/testing phase.\nPlease let us know if this clarifies your doubts?\n\nAs far as the linguistic compositionality is concerned, we detailed how it is measured in the 'Evaluation' paragraph of Section 4.1. We are adding a reference to the newly-added common RG algorithm (Algorithm 4) which contains details regarding the ZSCT accuracy and systematic train-test split of the dataset.  Please let us know whether this addresses your confusion, and/or whether there remains any part of it that you would like to see further detailed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699879426804,
                "cdate": 1699879426804,
                "tmdate": 1699879695059,
                "mdate": 1699879695059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pZDbBOSwSE",
                "forum": "17BA0Tl2Id",
                "replyto": "KrTGzy88pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_3drE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5753/Reviewer_3drE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed and point-to-point responses. I appreciate the efforts that authors made to address my questions/concerns. Apologies I have very little background in the area of referential games and should have probably opted out. However I did read the text twice with interest. I am still not very clear about how the choice of SCS representation leads to the compositional learning behavior, and would therefore keep my score as it is. I would not feel uncomfortable if reviewers more familiar with this area consider this work a promising contribution and should be accepted though."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544244981,
                "cdate": 1700544244981,
                "tmdate": 1700544244981,
                "mdate": 1700544244981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]