[
    {
        "title": "An Embodied Generalist Agent in 3D World"
    },
    {
        "review": {
            "id": "o7tOXJPny3",
            "forum": "4QaKdsh15T",
            "replyto": "4QaKdsh15T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_bU9n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_bU9n"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces LEO, a method that distinguishes itself from other LLM-based generalist agents by being able to understand and interact with the 3D world. LEO is trained with a two-stage process called \"LEO-Align\" and \"LEO-Instruct\". In the first stage, LEO is trained to align 3D vision and language by captioning 3D inputs (objects, objects-in-the-scene, and scenes). In the second stage, LEO is fine-tuned to specific tasks. LEO is demonstrated across a large variety of tasks including 3D vision-language understanding and embodied tasks. The authors provide detailed quantitative results of their method across multiple tasks: Scan2Cap, ScanQA, SQA3D, CLIPort, and ObjNav. The authors additionally ablate LEO to study components of their method including pretraining alignment, scale, and LEO's multitask nature."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall this manuscript is very cleanly written, with well-presented figures. \n- The results relating to 3D vision-language understanding and the corresponding ablations are compelling and informative. \n- This work contains a novel and useful research direction, incorporating 3D understanding, LLMs, and embodied actions into a single model.\n- This paper does a good job of providing quantitative and qualitative results for LEO across a broad range of tasks."
                },
                "weaknesses": {
                    "value": "Overall the weaknesses in this work are found concerning the embodied actions tasks.\n- In the CLIPort experiment the authors only demonstrate results on 3 out of the 10 tasks from the original experiment. I would be curious to see the full results of the left-out tasks to have a complete comparison to the original baselines in this experiment. This full table should also be presented in the supplementary information.\n- The authors do not present any baselines to compare to for the embodied navigation task. The results that LEO obtains on the embodied navigation task are quite low when compared to other works in the embodied AI community. By just performing behavior cloning on 70k human demos, Habitat-Web [1] achieved a success rate of 27.8 on their MP3D test split. Furthermore, in PONI [2] the authors have a baseline called \"BC\" (Behavior Cloning) that is similar to LEO in modalities used and with the use of behavior cloning; however BC just utilizes a simple ResNet-50 architecture. This baseline performs comparably to LEO (3.8 Success vs 2.6/3.7 Success) on MP3D. Could the authors compare to previous work and comment on the \"BC\" baseline from PONI?\n\n[1] Ramrakhya, Ram, et al. \"Habitat-web: Learning embodied object-search strategies from human demonstrations at scale.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[2] Ramakrishnan, Santhosh Kumar, et al. \"Poni: Potential functions for objectgoal navigation with interaction-free learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "Major questions and comments are given in the weaknesses section. \nMinor questions and comments follow.\n- In Habitat-Web the authors found a large difference in imitation learning results when trained on shortest path trajectories (4.4 success) and human demonstrations (35.4 success). The authors in this work chose to use the shortest path trajectories as they were less noisy and were easier to learn. Can the authors clarify if training on the human demonstrations led to worse quantitative performance on the task?\n- In section 4.3 the authors claim they will present the soft-spl, but this is missing from Table 4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1150/Reviewer_bU9n"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698270795391,
            "cdate": 1698270795391,
            "tmdate": 1699636041175,
            "mdate": 1699636041175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OT0h4UiFjL",
                "forum": "4QaKdsh15T",
                "replyto": "o7tOXJPny3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Do our responses address your concerns? Thank you again for your valuable feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer bU9n,\n\nWe encourage you to view the demo video and animations we have provided at https://generalist-leo-anonymous.github.io. These resources offer a clearer understanding of the tasks we have benchmarked and the design of our model. We also added more visualizations of our collected data regarding at [this URL](https://docs.google.com/document/d/10KawUDUWWvSYwEVMDuwYMOYcwsdtxL97Teu87-5lZLs/edit?usp=sharing).\n\n**As we are approaching the end of the author-reviewer discussion phase, we want to check if our responses address your concerns well. Are there any further clarifications we could provide? We look forward to your feedback. Please feel free to let us know if we have missed any issues.**\n\nThank you again for your valuable feedback!\n\nBest,\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636239094,
                "cdate": 1700636239094,
                "tmdate": 1700670052722,
                "mdate": 1700670052722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jFkvvmktvi",
            "forum": "4QaKdsh15T",
            "replyto": "4QaKdsh15T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_Vn42"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_Vn42"
            ],
            "content": {
                "summary": {
                    "value": "The paper identifies a gap in large language models (LLMs) regarding their limited capability to understand and interact with the 3D world. To address this, the authors introduce LEO, an embodied multi-modal and multi-task agent adept at tasks in the 3D environment. LEO is trained in two stages: 3D vision-language alignment and 3D vision-language-action instruction tuning, supported by a vast dataset containing object and scene-level multi-modal tasks. Through comprehensive testing, LEO excels in tasks like 3D captioning, embodied reasoning, and robotic manipulation. The paper also shares findings that can guide the development of future embodied generalist agents."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe essence of the motivation is sound.\n2.\tThe proposed pipeline is somewhat novel.\n3.\tThe experiments cover many aspects. A lot of efforts have been made."
                },
                "weaknesses": {
                    "value": "1. On the motivation and coherence: I think there are some word accuracy issues in the motivation. \n\n(1)\tIn Intro - first paragraph: \u201cThis limitation \u2026 executing real-world tasks and the attainment of general intelligence.\u201d First, many works on generalist agents are able to execute in the real world. (e.g. GATO, [2205.06175] A Generalist Agent (arxiv.org); PALM-E, [2303.03378] PaLM-E: An Embodied Multimodal Language Model (arxiv.org). To note, I do not ask for a baseline comparison against these works, just to suggest a better description of this work\u2019s scope) The author should be more precise on what kinds of real-world tasks they cannot execute, and what kind of real-world tasks the paper plans to address. Second, about the attainment of general intelligence. How to define the scope of general intelligence? When the generalist agent is equipped with 3D understanding ability, can it attain general intelligence? I hope the author can narrow the scope of the statement by adding some transition sentences.\n\n(2)\tIn Intro \u2013 second paragraph: the three challenges, the creation of datasets, design of scalable models, and effective learning strategies. I don\u2019t see how it\u2019s relevant to the following solutions. (a) The dataset is fused with existing datasets with high-quality data prompted by the LLMs. How is that challenging? With all the assets and models existing and known before. (b) The scalability of the model is not adequately validated. I understand that training a model from 7B parameter to 13B is already quite demanding for most labs, but it is far from emerging scaling effects. (c) the training strategy in LEO is nothing special. Again, if the known techniques can do well, how is that challenging? \n\n2. On the novelty of the method: quite limited. To my understanding, it is a combination of all known techniques including tokenization, token embedding & LLM, and training & inference. Since they all \u201cfollow\u201d some previous works. Besides, as a multi-task learning model, or \u201cgeneralist agent\u201d as the author may prefer, how the output looks is also important, but it is briefly described in the main text, and scattered in many places all over the paper.\n\n3. On the novelty of dataset generation: it has more creative thoughts, but it is a pity to put too many details in the Appendix. I would suggest putting more of the method in the Appendix instead of the dataset generation details."
                },
                "questions": {
                    "value": "What this paper puzzles me most is that it does not reveal the challenges and necessity of incorporating 3D input. In the response, I would like to see:\n\n1.\tWhat\u2019s the particular technique to adopt to deal with the 3D input? How is it different from previous methods? Or when the previous method is combined, does it just work like that, or does something non-trivial happen?\n\n2.\tin this framework, according to the experiments, what ability requires 3D understanding to make it from 0 to 1 or improve a large margin? I think the author should focus on that, instead of trying to propose a \u201cgeneralist agent\u201d, which is exhaustive for a lab-level resource and really cannot produce any insightful outcomes. If the authors deem to the generalist agent story, then they should in-depth reveal the challenges, for example:\n\n(1) how much data does it need, the key to creating and preparing the dataset?\n\n(2) does the model parameter size matter at all, if 7B is already saturated (Fig 4-b) how about smaller models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694158329,
            "cdate": 1698694158329,
            "tmdate": 1699636041077,
            "mdate": 1699636041077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8k1rp8ZMNV",
                "forum": "4QaKdsh15T",
                "replyto": "jFkvvmktvi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "It has been a delightful discussion and we sincerely hope you can increase the evaluation score"
                    },
                    "comment": {
                        "value": "Dear Reviewer Vn42\n\nIt has been a delightful experience to engage in a discussion over the technical details and contributions of our submission. We greatly appreciate your rich experience and insightful feedback in this field. To summarize, we would like to highlight several key points:\n1. **Dataset Contribution Not Sufficiently Highlighted in the Main Paper**\nThank you for the acknowledgment. We will polish the paper and emphasize the challenges, efforts, and contributions of building the datasets in a proper way.\n\n2. **The Necessity of 3D for an Embodied Generalist Agent and Our Model's Evidence of This Requirement**\nWe show the experimental results that demonstrate that 3D input is essential for our 3D tasks. Fundamentally, most of our tasks are 3D based and do not rely on image inputs. We encourage you to view the demo video and animations we have provided at https://generalist-leo-anonymous.github.io. These resources offer a clearer understanding of the tasks we have benchmarked and the design of our model. The question of whether to use images as inputs for constructing 3D representations is indeed an open one. However, it is important to note that this topic falls outside the scope of our current paper. Our focus is on illustrating the essential role of 3D inputs in the functionality of our embodied generalist agent.\n\n3. **Concerns Over Limited Technical Contribution of the Method**\nWe firmly believe that striving for a more capable embodied generalist represents a long-term aspiration within our field. Our research, hopefully the first in a series, aims to establish a foundational milestone for the development of generalist agents that can effectively comprehend the real 3D world. *It is not uncommon for there to be differing opinions regarding the novelty of scientific work, particularly in endeavors that seek to break new ground*. However, we wish to emphasize our substantial contributions in this area. Our work involves a new model designed and trained based on a large language model, carefully curating benchmarks, and achieving robust performances across established benchmarks over a large spectrum of 3D and embodied tasks. These contributions are distinctive and play a pivotal role in advancing the community's efforts toward creating more capable generalist agents with an enhanced understanding of the 3D world.\n\n**In light of these considerations, we respectfully request the reviewers to reconsider and potentially increase the evaluation score of our work. We deeply value your feedback and hope that our contributions are recognized as significant steps forward in this exciting and evolving field.**"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669234494,
                "cdate": 1700669234494,
                "tmdate": 1700669583938,
                "mdate": 1700669583938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qHaa42QDQE",
            "forum": "4QaKdsh15T",
            "replyto": "4QaKdsh15T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_foVx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_foVx"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose an embodied multi-modal and multi-task generalist agent, aiming to improve the agent's ability to understand and interact with the 3D world. The experiments demonstrate the effectiveness of the proposed LEO under various embodied tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The motivation of this paper is interesting and reasonable since the LLM-based agent needs the ability to complete 3D tasks for applying in the real world. I agree with the viewpoint that \"advancements in 3D scene-level understanding have significantly lagged behind\".\n\n2. The LEO can perceive, ground, reason, plan, and act in the 3D world simultaneously and obtain promising results.\n\n3. The dataset and fine-tuning method for constructing generalist LEO provides good contributions and insights for the embodied AI community. \n\n4. Extensive details and demonstrations are provided in the appendix, which makes the work easy to follow, and experiments are sufficient."
                },
                "weaknesses": {
                    "value": "- The section of related works should be included in the main text to ensure the completeness of the paper.\n\n- The way to achieve vision-language-action simply follows RT-2, thus lacking some novel design and weakening the technical contribution of this work.\n\n- The authors need to further polish the writing."
                },
                "questions": {
                    "value": "- Why is the 3D point cloud data for the third-person global view needed by an embodied agent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767689393,
            "cdate": 1698767689393,
            "tmdate": 1699636040992,
            "mdate": 1699636040992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "A67lriQ5o8",
            "forum": "4QaKdsh15T",
            "replyto": "4QaKdsh15T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_ADo8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1150/Reviewer_ADo8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an embodied multi-modal agent called LEO that can understand and interact with 3D world. LEO builds on top of Vicuna-13B large language model (LLM), and learns to encode 2D images as well as 3D point-cloud of the environment. The model is capable of generating natural language text (like caption, or answers to questions about the environment), as well as actions (for navigation or manipulation). To train this model, the authors collect a large 3D vision-language alignment dataset at object-level and scene-level to learn grounding. Further, this  model is fine-tuned on several tasks including captioning, question-answering, dialogue, planning, navigation and manipulation."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors attempt a fairly ambitious idea of building generalist embodied agents which can not only understand 2D content, but also 3D scenes. The proposed approach aims to generate both text as well as actions. The effort to integrate all these signals into the model, and setting up training on so many tasks is commendable.\n- The authors rightly point out that progress in building generalist embodied agent is severely restricted by large-scale ground-truth data. Collecting such data manually is expensive and restrictive so I agree with the overall idea (regardless of its execution in the paer) of collecting this data semi-automatically using LLMs."
                },
                "weaknesses": {
                    "value": "1. I have strong concerns regarding automatic data-collection using LLM and its accuracy. For instance, in Table 21, the authors describe a single example from task-specific datasets. In that table:\n    1. task planning describes \u201cwiping away dust\u201d, \u201cchecking the stability of chair\u201d, \u201cadjusting the height of the chair\u201d, \u201cvacuuming dirt or crumbs\u201d, \u201cchecking the lighting\u201d. This low-level plan is clearly not grounded in the scene nor the embodiment of the robot. The chairs depicted in the scene cannot be adjusted, how will the robot change the temperature / ventilation / lighting of the scene. This kind of pre-training / fine-tuning is unnecessary and incorrect. \n    2. Similarly, the 3D captioning task example has \u201ca white cabinet in the corner of the room. in the direction from the door and from the inside. it will be on the left, there is a small brown table on the left side of the cabinet and a smaller table on the right side of the cabinet\u201d This example clearly has a lot of grammatical mistakes, and makes me question the GT data against which the system is evaluated. \n    3. For scene captioning, the generated caption has very little overlap with the scene. It describes the chair as being of the same color as the floor (which is incorrect). Their 3D positions are described incorrectly as well. \n2. The ablations performed in the paper are unclear and inadequate. The paper doesn\u2019t perform any component level ablations on the model (removing 2D images as input, removing 3D images as input, checking against a blind baseline). Without such ablations, there is no way to tell how important each components of the system are. Additionally, it\u2019s unclear what the take-aways message from ablations (Table 18, and Figure 4(a)) are.\n3. It seems that a bunch of decisions in the paper (box-based vs scene-graph prompting, etc) are taken by qualitatively comparing the output. This is an insufficient way to justify design decisions (specialy for GT data collection), and without good-faith human evaluation / quantitative evaluation, it\u2019s unclear if the GT can be trusted or not. This puts into question the entire GT dataset, and its unclear if any followup evaluation on this benchmark can be trusted or not. For instance, in Figure 13, it\u2019s unclear which one of box-based prompting or scene-graph based prompting is better.  Similarly, in Table 11, the author uses GPT-4\u2019s evaluation of two generated captions to decide that partial scene graph is sufficient. I think relying on GPT-4 to subjectively evaluate two captions and checking whether it\u2019s sufficiently grounded to a 3D scene is problematic. \n4. Results on object-navigation are surprisingly low. Can the authors comment on why is the success rate so low compared to state of the art object navigation methods. Additionally, the object navigation setup requires the agent to navigate \u201cunexplored\u201d environments. But the proposed approach assumes a static \u201cknown\u201d environment. Can the authors please clarify the experimental setup for object navigation?"
                },
                "questions": {
                    "value": "Apart from questions asked in the weakness section, here are some additional questions: \n\n1. How does the agent handle multi-step trajectory of the agent (observation action pair for multiple time steps)? Is the agent only trained as a markovian policy which doesn\u2019t require encoding the history? Or is the trajectory encoded by simply appending each observations sequentially into the LLM? \n2.  CLIPort numbers in Table 3 are lower than the highest reported numbers in the CLIPort paper. For completeness and accuracy, can the authors update the CLIPort numbers to the highest numbers reported in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699291514325,
            "cdate": 1699291514325,
            "tmdate": 1699636040933,
            "mdate": 1699636040933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7gG2D5BeEm",
                "forum": "4QaKdsh15T",
                "replyto": "A67lriQ5o8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ADo8 (1/N)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n> I have strong concerns regarding automatic data-collection using LLM and its accuracy...\n> 1. task planning describes...This kind of pre-training / fine-tuning is unnecessary and incorrect.\n> 2. Similarly, the 3D captioning task...This example clearly has a lot of grammatical mistakes, and makes me question the GT data against which the system is evaluated.\n> 3. For scene captioning...Their 3D positions are described incorrectly as well.\n\nThank you so much for pointing this out! Indeed, generating data using LLMs could be challenging, and that's why we meticulously design several refinement procedures to enhance the data quality (details shown in Appendix A.2, A.3 and A.4). Notably, considering 1) the inherently scarce 3D VL data, 2) the community\u2019s explorations in prompting LLM [1,2,3,4,5,6,7], and 3) our improved scene-graph-based prompting approach, we believe our approach can provide high-quality 3D VL data and greatly contribute to the 3D VL community. We add more visualizations of our data regarding your concerns at [this URL](https://docs.google.com/document/d/10KawUDUWWvSYwEVMDuwYMOYcwsdtxL97Teu87-5lZLs/edit?usp=sharing).\n\nMore specifically:\n\n1. The data example of task planning in Table 21. You mentioned the chair is not adjustable and the robot cannot change the temperature etc. However, we remind that the role of LEO in this task is an AI assistant that first of all, produce plans that are based on the current scene layouts (ex. furnitures in the scenes), then communicate with human and offer suggestions. It never means to actually perform those tasks. Therefore, as long as the produced plan is fully grounded (which it does, the plan in Table 21 clearly includes chairs and tables that are in the scene), and reasonable for humans (that includes adjusting the temperature), it will be viewed as a good plan.\n\n2. The 3D captioning data comes from Scan2Cap (ScanRefer) dataset, one of existing 3D VL datasets, which have some grammatical flaws not caused by us. Compared with these datasets, our own curated data features more natural textual phrases and are mostly grammatically correct thanks to ChatGPT. Check out more comparative examples [here](https://docs.google.com/document/d/10KawUDUWWvSYwEVMDuwYMOYcwsdtxL97Teu87-5lZLs/edit?usp=sharing)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458169113,
                "cdate": 1700458169113,
                "tmdate": 1700458182988,
                "mdate": 1700458182988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "160HxCEppC",
                "forum": "4QaKdsh15T",
                "replyto": "A67lriQ5o8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ADo8 (3/N)"
                    },
                    "comment": {
                        "value": "As you can see, without 3D or 2D information both lead to significant performance drop. Some intuition: since LEO does not have a recurrent policy like RNN (effectively, LEO is similar to [RT-2](https://arxiv.org/abs/2307.15818) in terms of the transformer-based feedforward policy), therefore in the navigation task, 2D provide necessary information for avoiding obstacles like walls, while 3D provides global map details making it possible for a feed-forward policy in LEO to learn to navigate to the target in the shorest path. We will include a more comprehensive ablation on 2D/3D in the final version.\n\nFor the existing ablations on data presented in Table 18 and Figure 4(a), our take-home messages have been made clear in sec. 4.4:\n\n- The two-stage training (align-then-instruct) is crucial, compared to instruction-tuning only.\n- Compositional generalization is challenging. The variant *ScanNetOnly*, after learning captioning on 3RScan (stage 1) and QA on ScanNet (stage 2), fails to generalize to perform QA on 3RScan.\n- More general data leads to weaker task-specific performances. Adding new-domain data slightly harms in-domain performances, e.g., Scan2Cap, (*ScanNetOnly* vs. *NoAct (VL)*). Scaling up instruction-tuning data brings significant improvements (*PartialData* vs. *Full (VLA)*). The incorporation of embodied acting data weakens VL performances (*Full (VLA)* vs. *NoAct (VL)*).\n\n\n> 3. ...This is an insufficient way to justify design decisions...I think relying on GPT-4 to subjectively evaluate two captions and checking whether it\u2019s sufficiently grounded to a 3D scene is problematic.\n\nThanks for raising this. The qualitative examples in our paper are used to illustrate the comparisons. The best way to rigorously assess the quality of such free-form data is human evaluation. Actually, we have manually examined numerous data examples to ensure the reliability of our data and meanwhile polish our refinement procedures. In Appendix A, we present quantitative analysis to justify our refinement procedures. To further quantitatively showcase our superiority over 3D-LLM [7], we analyze the answer accuracy for Object Counting questions based on the 3D-LLM released data and our data (it is hard to compare the data directly due to different source scenes). We present the results here:\n\n|  | 3D-LLM | ours w/o O-CoT | ours with O-CoT | ours after refinement |\n| --- | --- | --- | --- | --- |\n| accuracy (%) | 56.45 | 57.41 | 78.02 | 100 |\n\nSpecifically, we consider the questions starting with \u201cHow many\u201d and ending with \u201cin the room/bedroom/kitchen\u201d, and check the correctness according to the ground-truth labels/IDs. The accuracy of 3D-LLM released data is close to our raw accuracy (56.45 vs. 57.41), which indicates the similar performance of ChatGPT without advanced prompting technology such as O-CoT. This also shows the necessity of refinement procedures. Since there are no other salient response types such as Object Existence (e.g., \u201cIs there a table in the room?\u201d) in 3D-LLM\u2019s data, we only consider the accuracy of Object Counting as a quantitative indicator.\n\nTo sum up, we believe our data construction approach, which is indeed a combination of prompt ChatGPT through our proposed O-CoT and human examination, could provide accurate 3D-language data for training LEO. Moreover, our data quality is considerably better than our counterparts (3D-LLM). \n\n> ...it\u2019s unclear if the GT can be trusted or not. This puts into question the entire GT dataset.\n\nThanks for your comments. All the datasets for evaluating LEO (those appear in Table 2), are standard benchmarks introduced by the research community, not collected by us (and we believe they were not produced by ChatGPT). Therefore, the possible errors (which we believe are rare) in the constructed datasets, LEO-align and LEO-instruct should not affect the evaluation of LEO."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458359577,
                "cdate": 1700458359577,
                "tmdate": 1700571076569,
                "mdate": 1700571076569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7HGfif1SQC",
                "forum": "4QaKdsh15T",
                "replyto": "A67lriQ5o8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ADo8 (4/N)"
                    },
                    "comment": {
                        "value": "> 4. Results on object-navigation are surprisingly low...Can the authors please clarify the experimental setup for object navigation?\n\nThank you for pointing this out! We\u2019ve spotted some major issues in our objnav evaluation protocol since the submission of our manuscript. Therefore, we\u2019ve re-worked the whole objnav experiment and that part in both sec. 4.3 and the appendix has been completely renovated. Here are some key insights:\n\n1. We\u2019ve switched to using the standard habitat MP3D objnav validation set (MP3D-val) and the protocol available in habitat-lab (https://github.com/facebookresearch/habitat-lab). Therefore, comparing with baselines like Habitat-web becomes possible. Additionally, we also evaluate LEO on the newly introduced HM3D objnav validation set (HM3D-val) (note: this is \u201czero-shot\u201d as only MP3D scenes are used for training LEO) and compare it with a strong baseline from cortexbench [9]. For your convenience, we\u2019ve posted the results below. \n\n    |  | MP3D-val |  | HM3D-val |  |\n    | --- | --- | --- | --- | --- |\n    | Model | Success (\u2191) | SPL (\u2191) | Success (\u2191) | SPL (\u2191) |\n    | H.w. (shortest) | 4.4 | 2.2 | - | - |\n    | H.w. (70k demo) | 35.4 | 10.2 | - | - |\n    | VC-1 (ViT-B) | - | - | 57.1 | 31.4 |\n    | LEO | 23.1 | 15.2 | 23.1 | 19.1 |\n\n    We would like to summarize the findings below:\n\n    - First of all, after resolving this evaluation issue, LEO is in fact producing **reasonable** performances on both standard MP3D-val and even HM3D-val in a zero-shot fashion.\n    - Thanks to the 3D branch within LEO that processes 3D input and provides global 3D information(potentially offering coarse map details), LEO is able to attain better SPL than Habitat-web baseline, meaning that LEO is able to take shorter paths by leveraging the map information.\n    - Compared to VC-1, an agent trained on HM3D, LEO offers reasonable results given the fact that it has never been trained on HM3D scenes.\n\n2. Per your concern on LEO trained with human demonstrations vs. shortest path, we provide the following results in the appendix:\n\n    |  | MP3D-seen |  | MP3D-unseen |  |\n    | --- | --- | --- | --- | --- |\n    | Model | Success (\u2191) | SPL (\u2191) | Success (\u2191) | SPL (\u2191) |\n    | Habitat-web (shortest) | 4.4 | 2.2 | - | - |\n    | Habitat-web (70k demo) | 35.4 | 10.2 | - | - |\n    | LEO (shortest) | 23.1 | 15.2 | 11.1 | 9.6 |\n    | LEO (70k demo) | 7.1 | 5.3 | 8.9 | 8.6 |\n\n    As you can see, training with human demonstrations indeed leads to much worse performances of LEO and we believe the root cause is intuitive: \n\n    - LEO is able to process the extra global 3D scene input (a list of objects in the scene, including their point cloud and bbox information), therefore, learning with shortest path data makes sense \u2014 LEO could learn to convert the 3D scene input into map details, then learn to navigate to the target in shortest path by learning from the shortest path navigation data.\n    - Further, as in our updated sec. 4.3 and H.2, LEO indeed **does not have any recurrent structure** when it comes to action prediction, i.e. the only thing relayed from the past is an action history of 4 \u2014 LEO effectively can be viewed as a transformer-based feed-forward policy, similar to RT-2 [8]. Therefore learning from human demonstration could be extra difficult, as it includes exploration, which can be hard to learn without a recurrent structure like RNN.\n    - Note that the choice of not having a recurrent structure is a result of the computation efficiency and scalability vs. performance tradeoffs. We commit to exploring more on this in future work.\n\n> How does the agent handle multi-step trajectory of the agent (observation action pair for multiple time steps)? Is the agent only trained as a markovian policy which doesn\u2019t require encoding the history? Or is the trajectory encoded by simply appending each observations sequentially into the LLM?\n\nThanks for your comments. As we mentioned in the previous response, LEO is indeed a Markovian policy without any recurrent structure, and the only thing relayed from the past are 4 past actions. Therefore, 3D information (potentially offering map details) is indeed crucial, and learning from human demonstrations could fail as such trajectories include exploration and can be hard to learn without a recurrent structure.\n\n> CLIPort numbers in Table 3 are lower than the highest reported numbers in the CLIPort paper. For completeness and accuracy, can the authors update the CLIPort numbers to the highest numbers reported in the paper?\n\nThanks for raising this. We've checked the original CLIPort paper. We have added the performances of single-task CLIPort in Table 3 and discarded the \"multi-attr\" CLIPort variant, which utilizes \"unseen\" tasks (except for the task being evaluated). **This never happens in LEO** as only seen tasks are used for training. Therefore, comparing with these numbers would be **unfair** and we choose not to show them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458424909,
                "cdate": 1700458424909,
                "tmdate": 1700625505682,
                "mdate": 1700625505682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rmm9uRrS3Z",
                "forum": "4QaKdsh15T",
                "replyto": "A67lriQ5o8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ADo8 (5/N)"
                    },
                    "comment": {
                        "value": "We hope the above response can resolve your questions and concerns. Please let us know if there is any further question!\n\n[1] Yizhong Wang, et al. Self-instruct: Aligning language model with self generated instructions. ACL 2023.\n\n[2] Rohan Taori, et al. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\n[3] Baolin Peng, et al. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\n[4] Haotian Liu, et al. Visual instruction tuning. NeurIPS 2023.\n\n[5] Bo Li, et al. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425,\n2023.\n\n[6] Fuxiao Liu, et al. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.\n\n[7] Yining Hong, et al. 3d-llm: Injecting the 3d world into large language models. NeurIPS 2023.\n\n[8] Anthony Brohan, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\n\n[9] Arjun Majumdar, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458453075,
                "cdate": 1700458453075,
                "tmdate": 1700625464408,
                "mdate": 1700625464408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xxtgsJA0JY",
                "forum": "4QaKdsh15T",
                "replyto": "A67lriQ5o8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1150/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1150/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1150/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Do our responses address your concerns? Thank you again for your valuable feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer ADo8,\n\nWe encourage you to view the demo video and animations we have provided at https://generalist-leo-anonymous.github.io. These resources offer a clearer understanding of the tasks we have benchmarked and the design of our model. We also added more visualizations of our collected data regarding at [this URL](https://docs.google.com/document/d/10KawUDUWWvSYwEVMDuwYMOYcwsdtxL97Teu87-5lZLs/edit?usp=sharing).\n\n**As we are approaching the end of the author-reviewer discussion phase, we want to check if our responses address your concerns well. Are there any further clarifications we could provide? We look forward to your feedback. Please feel free to let us know if we have missed any issues.**\n\nThank you again for your valuable feedback!\n\nBest,\n\nAuthors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636322848,
                "cdate": 1700636322848,
                "tmdate": 1700670111083,
                "mdate": 1700670111083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]