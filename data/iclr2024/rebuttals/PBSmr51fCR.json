[
    {
        "title": "URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering"
    },
    {
        "review": {
            "id": "PXYrht8nRz",
            "forum": "PBSmr51fCR",
            "replyto": "PBSmr51fCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_xehP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_xehP"
            ],
            "content": {
                "summary": {
                    "value": "This paper learns a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. Firstly, to overcome the limitations of cross-view contrastive learning, URRL-IMVC incorporates an attention-based auto-encoder framework to fuse multi-view information and generate unified embeddings. Secondly, URRL-IMVC directly enhances the robustness of the unified embedding against view-missing conditions through KNN imputation and data augmentation techniques, eliminating the need for explicit missing view recovery. Finally, incremental improvements are introduced to further enhance the overall performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The originality, quality, and significance of this paper are supported by the proposed unified representation learning framework that efficiently fuses both multiview and neighborhood information, allowing for better capturing of consensus and complementary information.\n\n2. The clarity of this paper is clear based on the framework figure and the corresponding illustrations."
                },
                "weaknesses": {
                    "value": "1. The biggest problem of this paper is the limited novelty in formulation of URRL-IMVC\uff0cwhich learns a unified embedding that captures the comprehensive representation. The differences between URRL-IMVC and the closely related works can be analyzed from different aspects.\n\n2. The strategies including KNN imputation and data augmentation should be stated in details. Then the process of directly learning a robust representation capable of handling view-missing conditions without explicit missing view recovery is easily understood by the readers.\n\n3. In the experiments, the compared methods are not enough and the more datasets can be added, i.e., Table 2.\n\n4. The convergence analysis can be added in the experiment, which can be adopted to better the loss function."
                },
                "questions": {
                    "value": "Why the visualization for 4400 iteration is not significantly improved compared with 2400 iteration in Figure 4 for the experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5516/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5516/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5516/Reviewer_xehP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697808552532,
            "cdate": 1697808552532,
            "tmdate": 1699636565291,
            "mdate": 1699636565291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kjndkn5Owb",
                "forum": "PBSmr51fCR",
                "replyto": "PXYrht8nRz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xehP #1"
                    },
                    "comment": {
                        "value": "***W1: The biggest problem of this paper is the limited novelty in formulation of URRL-IMVC\uff0cwhich learns a unified embedding that captures the comprehensive representation. The differences between URRL-IMVC and the closely related works can be analyzed from different aspects.***\n\n**R1:** The key contribution of our paper lies in the novel approach of directly learning a unified and robust representation for the IMVC task.\n\n**Unified**: In the DIMVC field, the prevailing framework is cross-view contrastive learning [2] [3] [4] [5]. However, this design primarily emphasizes the consensus information in multi-view data while neglecting the valuable complementary information. Therefore, our approach goes beyond the cross-view contrastive framework and instead learns a unified representation using a meticulously designed attention-based network that effectively leverages complementary information.\n\n**Robust**: The primary approach for handling missing views in the DIMVC field is to first recover the missing views and then perform complete multi-view clustering [1] [2] [3] [4] [5]. However, the recovered views may be unreliable, and computational resources are wasted on repeatedly mapping between data and latent space [1]. To address this issue, we propose directly learning a latent embedding that is robust to view missing, without explicitly recovering the missing data. We have devised data augmentation and KNN imputation as preprocessing techniques, which, along with the unified representation learning framework, facilitate efficient and robust representation learning.\n\nOverall, our paper introduces a novel approach that directly learns a unified and robust representation for the IMVC task, surpassing the limitations posed by existing approaches in the field.\n\n***W2: The strategies including KNN imputation and data augmentation should be stated in details. Then the process of directly learning a robust representation capable of handling view-missing conditions without explicit missing view recovery is easily understood by the readers.***\n\n**R2:** A detailed description and formulation of the KNN imputation strategy is provided in Appendix B.5 and the data augmentation strategy in Appendix B.6. Given the length limitation of the conference paper (9 pages), we have to include most of the detailed formulations in the Appendix to ensure the paper's completeness. Additionally, in this revised version, we have added Algorithm 1 to describe the KNN imputation strategy, which we hope will facilitate easier interpretation. Regarding data augmentation, we believe that the formulation in equations 19 and 20 already provides sufficient detail."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514574948,
                "cdate": 1700514574948,
                "tmdate": 1700514574948,
                "mdate": 1700514574948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4zpd8BY3oi",
                "forum": "PBSmr51fCR",
                "replyto": "PXYrht8nRz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xehP #2"
                    },
                    "comment": {
                        "value": "***W3: In the experiments, the compared methods are not enough and the more datasets can be added, i.e., Table 2.***\n\n**R3:** In response to this concern, an additional comparison with IMVC methods that do not use deep neural networks has already been included in the previous version paper. It can be found in Table 6, Appendix C.3, and includes results from two extra datasets and five additional methods. Additionally, based on the suggestions from the reviewers, we have included two textual datasets (Table 4) for comparison in this revised version. The corresponding results are presented in Table 5 and below. On both datasets, our approach achieved state-of-the-art performance. \n\n| Datase\\Metrics   | BDGP Acc(\\%)                 | BDGP NMI(\\%)                 | BDGP ARI(\\%)                 | Reuters Acc(\\%)              | Reuters NMI(\\%)              | Reuters ARI(\\%)              |\n| ------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |\n| Completer        | 58.37 $\\pm$ 4.87              | 48.52 $\\pm$ 4.34              | 25.13 $\\pm$ 6.71              | 40.30 $\\pm$ 7.32              | 22.86 $\\pm$ 6.44              | 10.53 $\\pm$ 8.25              |\n| DSIMVC           | **95.71 $\\pm$ 0.21**         | **87.08 $\\pm$ 0.54**         | **89.62 $\\pm$ 0.51**         | 48.39 $\\pm$ 2.92              | **31.88 $\\pm$ 2.27**         | **26.04 $\\pm$ 2.27**         |\n| SURE             | 63.27 $\\pm$ 7.55              | 41.35 $\\pm$ 7.70              | 36.51 $\\pm$ 8.87              | $\\underline{48.63 \\pm 3.56}$ | 27.73 $\\pm$ 2.46              | 22.46 $\\pm$ 1.59              |\n| DCP              | 55.82 $\\pm$ 7.02              | 44.56 $\\pm$ 8.92              | 20.08 $\\pm$ 11.71            | 39.02 $\\pm$ 3.16              | 22.47 $\\pm$ 4.19              | 6.92 $\\pm$ 3.34               |\n| CPSPAN           | 81.40 $\\pm$ 8.64              | 66.19 $\\pm$ 6.61              | 64.71 $\\pm$ 9.89              | 39.35 $\\pm$ 2.13              | 14.47 $\\pm$ 2.01              | 12.37 $\\pm$ 1.79              |\n| RecFormer        | 49.76 $\\pm$ 3.49              | 38.62 $\\pm$ 3.30              | 19.01 $\\pm$ 2.68              | 39.70 $\\pm$ 5.14              | 17.27 $\\pm$ 2.83              | 14.82 $\\pm$ 3.19              |\n| URRL-IMVC (ours) | $\\underline{92.52 \\pm 5.71}$ | $\\underline{82.29 \\pm 5.66}$ | $\\underline{84.26 \\pm 7.57}$ | **49.91 $\\pm$ 2.44**         | $\\underline{29.49 \\pm 1.38}$ | $\\underline{25.71 \\pm 2.08}$ |\n\n***W4. The convergence analysis can be added in the experiment, which can be adopted to better the loss function.***\n\n**R4:** Thank you for the suggestion. In Figure 6, Appendix C.7, we have included a visualization of the loss and clustering performance throughout the training process. During the first training stage, we observe that the reconstruction and robustness loss converge. In the second stage, while the reconstruction loss continues to decrease, the robustness loss is compromised to prioritize the clustering loss. Furthermore, we have analyzed the clustering performance. During the first stage, the clustering performance exhibits some fluctuations due to the lack of clustering-oriented supervision. However, in the second stage, the clustering performance stabilizes with clustering loss. We believe that this additional analysis provides insights into the convergence behavior of our proposed method.\n\n**Reference**\n- [1]. Chengliang Liu, Jie Wen, Zhihao Wu, Xiaoling Luo, Chao Huang, and Yong Xu. Information recovery-driven deep incomplete multiview clustering network. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201311, 2023. doi: 10.1109/TNNLS.2023.3286918. \n- [2]. Jiaqi Jin, Siwei Wang, Zhibin Dong, Xinwang Liu, and En Zhu. Deep incomplete multi-view clustering with cross-view partial sample and prototype alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11600\u201311609, June 2023. \n- [3]. Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. Robust multi-view clustering with incomplete information. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):1055\u20131069, 2023b. doi: 10.1109/TPAMI.2022.3155499. \n- [4]. Huayi Tang and Yong Liu. Deep safe incomplete multi-view clustering: Theorem and algorithm. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 21090\u201321110. PMLR, 17\u201323 Jul 2022. \n- [5]. Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li, Jiancheng Lv, and Xi Peng. Completer: Incomplete multi-view clustering via contrastive prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11174\u201311183, June 2021."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514711488,
                "cdate": 1700514711488,
                "tmdate": 1700514869873,
                "mdate": 1700514869873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "10OqCV69pA",
                "forum": "PBSmr51fCR",
                "replyto": "PXYrht8nRz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xehP #3"
                    },
                    "comment": {
                        "value": "***Q1: Why the visualization for 4400 iteration is not significantly improved compared with 2400 iteration in Figure 4 for the experiment?***\n\n**A1:** Our method employs a two-stage training strategy (Algorithm 2, Appendix B.8.), where the first stage focuses on representation learning and the second stage emphasizes clustering learning. The second stage commences at 2200 iterations and continues until 4400 iterations. Similar to many other learning tasks such as classification, the training process exhibits quick convergence initially, which gradually slows down over time. This explains why the accuracy improvement in the first 200 iterations is 2.28%, whereas the last 2000 iterations only result in a 4% increase in accuracy. However, the visualization in Figure 4 clearly demonstrates a substantial enhancement in cluster compactness, validating the effectiveness of our clustering-oriented learning approach. For a more comprehensive understanding of our training process, we suggest referring to the newly added Figure 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514932920,
                "cdate": 1700514932920,
                "tmdate": 1700514932920,
                "mdate": 1700514932920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UDBnbQQG3y",
            "forum": "PBSmr51fCR",
            "replyto": "PBSmr51fCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_LBwN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_LBwN"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces URRL-IMVC, an incomplete multi-view clustering method that does not rely on cross-view contrastive learning or missing view recovery. Instead, it leverages the complementarity of information across views by fusing data from two carefully designed encoders. It also eliminates explicit missing view recovery by employing KNN imputation and data augmentation techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The organization of this paper is clear and the motivation is easy to understand.\n\n* Experimental results support the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "* This paper asserts that cross-view contrastive learning may overlook complementary information, and contrasting the unified embedding has the potential to capture a more comprehensive representation. However, there is a lack of both theoretical and experimental evidence to support these claims from either perspective.\n\n* More experiments need to be added to verify the sensitivity of model parameters, such as the setting of k in KNN and the initialization of cluster centers in clustering module.\n\n* The ablation studies on modules are rough. The effectiveness of incremental improvements on each module should be further investigated. \n\n* The related work Section appears to be somewhat concise. Some recent works should be discussed."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672735513,
            "cdate": 1698672735513,
            "tmdate": 1699636565192,
            "mdate": 1699636565192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "672XRE30Mo",
                "forum": "PBSmr51fCR",
                "replyto": "UDBnbQQG3y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LBwN #1"
                    },
                    "comment": {
                        "value": "***W1: This paper asserts that cross-view contrastive learning may overlook complementary information, and contrasting the unified embedding has the potential to capture a more comprehensive representation. However, there is a lack of both theoretical and experimental evidence to support these claims from either perspective.***\n\n**R1:** The limitations of cross-view contrastive learning are intuitive and have been previously studied in existing literature. It is intuitive to understand that if the contrastive learning objective is achieved, the resulting embeddings from each view would be almost identical, thereby disregarding any view-specific or complementary information. Furthermore, a theoretical analysis conducted by [1] emphasizes that cross-view contrastive alignment can actually reduce the number of distinct clusters in the representation space, and this effect becomes more pronounced with an increasing number of views. Additionally, [2] mentions the inherent conflict between the reconstruction target and the cross-view contrastive target, as the former aims to preserve view-specific information while the latter attempts to ignore it.\n\nIn contrast, our unified design overcomed this drawback by learning unified representations. Although currently lacking theoretical evidence due to the black box nature of the unified embedding. However, we have made two experimental observations that provide circumstantial evidence of the preservation and utilization of both consensus and complementary information. Firstly, we noticed in our experiments that our approach outperforms cross-view contrastive learning-based methods on datasets that possess more complementary information. For instance, on the 100Leaves dataset where the two views represent the shape and texture of a leaf, our approach exhibits greater advantages. Secondly, we observed that the clustering performance of cross-view contrastive learning-based approaches declines when additional views are added, as theoretically analyzed in [1]. In contrast, our approach consistently benefits from the inclusion of more views in the dataset, thereby overcoming this drawback. The results of the second experiment have been included in the revised version of the paper as Table 7 in Appendix C.3.\n\nThe results from Table 7 are as follows:\n| Views | CPSPAN Acc(\\%)               | CPSPAN NMI(\\%)               | CPSPAN ARI(\\%)               | URRL-IMVC (ours) Acc(\\%)     | URRL-IMVC (ours) NMI(\\%)     | URRL-IMVC (ours) ARI(\\%)     |\n| ------- | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |\n| 2     | 50.88 $\\pm$ 1.87              | 45.27 $\\pm$ 2.50              | 35.79 $\\pm$ 2.25              | 58.36 $\\pm$ 3.01              | 47.16 $\\pm$ 2.50              | 39.40 $\\pm$ 2.71              |\n| 3     | 73.17 $\\pm$ 4.27              | 61.40 $\\pm$ 4.29              | 55.37 $\\pm$ 5.46              | 77.60 $\\pm$ 0.88              | 67.61 $\\pm$ 0.98              | 63.97 $\\pm$ 1.33              |\n| 4     | **84.89 $\\pm$ 2.15**         | **75.37 $\\pm$ 2.45**         | **71.79 $\\pm$ 3.26**         | $\\underline{91.73 \\pm 0.47}$ | $\\underline{83.57 \\pm 0.68}$ | $\\underline{83.26 \\pm 0.76}$ |\n| 5     | $\\underline{77.62 \\pm 4.74}$ | $\\underline{69.70 \\pm 4.04}$ | $\\underline{63.23 \\pm 5.51}$ | **92.95 $\\pm$ 2.60**         | **86.29 $\\pm$ 1.76**         | **86.02 $\\pm$ 2.91**         |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514282194,
                "cdate": 1700514282194,
                "tmdate": 1700514282194,
                "mdate": 1700514282194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AjdsHcBie7",
                "forum": "PBSmr51fCR",
                "replyto": "UDBnbQQG3y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LBwN #2"
                    },
                    "comment": {
                        "value": "***W2: More experiments need to be added to verify the sensitivity of model parameters, such as the setting of k in KNN and the initialization of cluster centers in clustering module.***\n\n***W3: The ablation studies on modules are rough. The effectiveness of incremental improvements on each module should be further investigated.***\n\n**R2 & R3:** Thank you for your valuable feedback. \nTo address these concerns, we have already conducted extensive ablation studies to evaluate the impact of our design choices, which has been mentioned by Reviewer 3U96. However, due to the limited page length of our conference paper, we were unable to include these studies in the main paper. Instead, we have included them in Appendix C, where readers can find detailed information about our ablation experiments related to both design and hyperparameters. In Appendix C.5, we present the ablation studies focusing on design choices, which demonstrate the effectiveness of our proposed approach, such as the choice of initialization method. In Appendix C.6, we provide ablation studies specifically examining the sensitivity of hyperparameters, such as the setting of k in KNN. To ensure that these important results are not overlooked by readers, we have revised the paper and emphasized cross-references in the main text, directing readers to the relevant sections in the appendix. \n\nSpecifically, regarding the initialization of cluster centers in the clustering module, we provided information in Appendix C.1 \"Design Details.\" We used Agglomerative clustering with \"ward\" linkage, which ensures deterministic results across multiple runs with the same embeddings. To further investigate the impact of initialization methods, we have also conducted an ablation test using K-Means as the initialization method. We found Agglomerative clustering shows better and stabler performance. The results can be found in Table 10 and below.\n\n| Cluster Method | K-means         | Agglomerative        |\n| ---------------- | ----------------- | ---------------------- |\n| Initialize     | 84.84 $\\pm$ 6.08 | **90.38 $\\pm$ 1.46** |\n| Final          | 88.34 $\\pm$ 6.33 | **93.36 $\\pm$ 0.98** |\n\nRegarding the setting of k in KNN, the related Ablation study has been presented in Table 15. Near-optimal results can be achieved with k>=4. The result is unimodal with the best k = 4, and larger k values tend to provide more stable results (smaller standard deviation). The results from Table 15 are as follows:\n\n| k=1             | k=2             | k=4                  | k=8                          | k=16            |\n| ----------------- | ----------------- | ---------------------- | ------------------------------ | ----------------- |\n| 83.84 $\\pm$ 3.38 | 84.84 $\\pm$ 2.82 | **87.31 $\\pm$ 2.01** | $\\underline{87.00 \\pm 2.46}$ | 86.01 $\\pm$ 1.28 |\n\nWe hope that these experiments will effectively address your concerns about the sensitivity of model parameters and the investigation of incremental improvements on each module.\n\n***W4: The related work Section appears to be somewhat concise. Some recent works should be discussed.***\n\n**R4:** We acknowledge the importance of discussing recent works in our field. However, due to the length limitation of the paper, we had to move a portion of the related works section to Appendix A.1. This part provides an introduction to recent IMVC works that do not utilize deep neural networks. Additionally, in this revised version, we have included several more recent works in Appendix A.1 to ensure that we cover a broader range of relevant literature.\n\n**Reference**\n- [1]. Daniel J. Trosten, Sigurd L\u00f8kse, Robert Jenssen, and Michael C. Kampffmeyer. On the effects of self-supervision and contrastive alignment in deep multi-view clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 23976\u201323985, June 2023. \n- [2]. Jie Xu, Huayi Tang, Yazhou Ren, Liang Peng, Xiaofeng Zhu, and Lifang He. Multi-level feature learning for contrastive multi-view clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16051\u201316060, June 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514413239,
                "cdate": 1700514413239,
                "tmdate": 1700514413239,
                "mdate": 1700514413239,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nc1QMzRAk6",
            "forum": "PBSmr51fCR",
            "replyto": "PBSmr51fCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel Unified and Robust Representation Learning for Incomplete Multi-View Clustering, which tries to learn a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. The method proposed in this paper is without explicit missing view recovery procedure, which is orthogonal to existing missing view recovery-based methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper first provides the idea to simultaneously consider the multi-view information fusion and neighborhood information incorporation for clustering under the view-missing conditions.\n2. To my knowledge, using an attention-based auto-encoder framework to fuse multi-view information is somewhat novel in multi-view learning.\n3. The experimental results are significantly better than former methods."
                },
                "weaknesses": {
                    "value": "1. No evidence for the analysis on the computation cost and unreliability of explicit missing view recovery.\n2. The writing of the methodology is not compact and unclear: Some related contents are far away from each other, for example, Eq. (5) and its details.\n3. Some essential details are missing, for example, the calculation of KL divergence in Eq. (23) is too abstract to follow.\n4. Most of the formulations are postponed to the appendix, making the main text hard to be understood and undermining the clarity. \n5. No solid theoretical guarantee is provided for the proposed method."
                },
                "questions": {
                    "value": "1. What is the first output of NDE module in the output choice of the proposed NDE?\n2. How does the Siamese Encoder work in Figure 2? Do you mean the architectures of the upper and lower parts are identical with shared parameters?\n3. How to determine the level of noise added to the original incomplete multi-view data?\n4. Do you consider the private information in each view? In addition to the consensus information, the complementary information is also essential, which has been indicated in this paper. However, I cannot understand what is the mechanism used to leverage the view-specific information in the proposed method. Please clarify this in details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5516/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5516/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684387256,
            "cdate": 1698684387256,
            "tmdate": 1699636565094,
            "mdate": 1699636565094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iOV0oX3OEa",
                "forum": "PBSmr51fCR",
                "replyto": "Nc1QMzRAk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer umVo #1"
                    },
                    "comment": {
                        "value": "***W1: No evidence for the analysis on the computation cost and unreliability of explicit missing view recovery.***\n\n**R1**: Regarding the concern, we have added an ablation study in Table 12 to provide experimental evidence supporting our point of view. In this study, we adopted a similar framework to RecFormer [1] and constructed two identical networks. The first network was used to recover the missing views, while the second network was employed to cluster the recovered complete multi-view data. We achieved comparable results with this framework, which suggests that our robustness learning can effectively replace the role of a missing view recovery network while enjoying lower computational costs.\n\nFurthermore, our experiments have shown that without the robustness objective, the performance of this framework can deteriorate. This observation suggests that missing views recovered with simple strategies may indeed be unreliable. We believe that these findings strengthen our argument and provide empirical support for the analysis of the computation cost and unreliability of explicit missing view recovery.\n\nThe results of Table 12 are as follows, in which RL represents Robustness Learning:\n| Network 1 | Network 2 | Acc(\\%)                      | NMI(\\%)                      | ARI(\\%)                      |\n| ----------- | ----------- | ------------------------------ | ------------------------------ | ------------------------------ |\n| -         | with RL   | **92.95 $\\pm$ 2.60**         | **86.29 $\\pm$ 1.76**         | **86.02 $\\pm$ 2.91**         |\n| with RL   | with RL   | $\\underline{92.93 \\pm 2.64}$ | $\\underline{85.93 \\pm 1.74}$ | $\\underline{84.98 \\pm 2.84}$ |\n| with RL   | w/o RL    | 89.65 $\\pm$ 4.65              | 83.34 $\\pm$ 3.66              | 81.72 $\\pm$ 5.38              |\n| w/o RL    | w/o RL    | 87.23 $\\pm$ 12.06             | 82.28 $\\pm$ 6.62              | 79.96 $\\pm$ 11.12             |\n\n***W2: The writing of the methodology is not compact and unclear: Some related contents are far away from each other, for example, Eq. (5) and its details.***\n\n***W3: Some essential details are missing, for example, the calculation of KL divergence in Eq. (23) is too abstract to follow.***\n\n***W4: Most of the formulations are postponed to the appendix, making the main text hard to be understood and undermining the clarity.***\n\n**R2 & R3 & R4:** Given the paper length limitation of this conference, we had to move some of the detailed formulations and ablation studies to the Appendix. This may have caused difficulty in finding relevant equations or contents. In this revision, we have made efforts to address this concern by adding more cross-references and highlighting relevant content or formulations. We hope this will make it easier for readers to locate the information they need.\n\nRegarding the calculation of the KL divergence in Equation 23, it may appear abstract and difficult to follow due to a lack of reference to the Clustering Module section (Appendix B.4). We have included cross-references to help readers follow the calculation. \nWe appreciate your valuable advice for improving this paper.\n\n***W5. No solid theoretical guarantee is provided for the proposed method.***\n\n**R5:** We appreciate the reviewer's suggestion and acknowledge the importance of theoretical analysis about extracting or balancing consensus and complementary information in multi-view representation learning. Although we have some promising ideas, we have left them for future works due to the already extensive length and complexity of the paper. \n\n**Reference:**\n- [1]. Chengliang Liu, Jie Wen, Zhihao Wu, Xiaoling Luo, Chao Huang, and Yong Xu. Information recovery-driven deep incomplete multiview clustering network. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201311, 2023. doi: 10.1109/TNNLS.2023.3286918."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514050811,
                "cdate": 1700514050811,
                "tmdate": 1700514050811,
                "mdate": 1700514050811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gc2JKIUuSi",
                "forum": "PBSmr51fCR",
                "replyto": "Nc1QMzRAk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer umVo #2"
                    },
                    "comment": {
                        "value": "***Q1. What is the first output of NDE module in the output choice of the proposed NDE?***\n\n**A1**: In the Transformer Encoder, which is a sequence-to-sequence model, a sequence of vectors is provided as input and a corresponding sequence of vectors is generated as output. So the \"first output\" of the NDE module refers to the initial vector in the output vector sequence. It has a bias on the first vector in the input sequence, which is always the most reliable sample in KNN. To provide a more intuitive explanation of the output choice in the NDE and VDE, we have added Figure 5 in the revised manuscript. Additionally, we have updated equations 7 and 14 to incorporate the output choice procedure in the formulation.\n\n***Q2. How does the Siamese Encoder work in Figure 2? Do you mean the architectures of the upper and lower parts are identical with shared parameters?***\n\n**A2**: Yes, the Siamese Encoder in Figure 2 utilizes identical architectures with shared weights for both the upper and lower parts. This design allows for the extraction of unified representations for both augmented and unaugmented input data. By further incorporating a robustness loss, the Encoder is trained to extract robust representations in the presence of augmentation, and also in the presence of real missing conditions.\n\n***Q3. How to determine the level of noise added to the original incomplete multi-view data?***\n\n**A3**: In our approach, we employ three types of noise for data augmentation: (1) view dropout, (2) Gaussian noise, and (3) random dropout. For Gaussian noise and random dropout, we conducted a preliminary grid search to determine the optimal noise level. Since the input is normalized, we found that a fixed noise level of 0.05 yields good results for various datasets.\nRegarding view dropout, we conducted an ablation study (Table 14) to determine the best noise level based on different missing rates. We designed Equation 25, which maps the missing rate to the desired noise level. This allows us to dynamically adjust the noise level depending on the missing rate of data.\n\nThe results from Table 14 are as follows:\n| Parameter    | $\\phi_1 = 0$    | $\\phi_1 = 0.15$              | $\\phi_1 = 0.3$               | $\\phi_1 = 0.45$      | $\\phi_1 = 0.6$               | Equation 25   |\n| -------------- | ----------------- | ------------------------------ | ------------------------------ | ---------------------- | ------------------------------ | --------------- |\n| $m_r$ = 0.00 | 89.30 $\\pm$ 1.86 | $\\underline{89.50 \\pm 1.77}$ | 88.91 $\\pm$ 1.89              | **89.71 $\\pm$ 1.19** | 86.94 $\\pm$ 2.12              | $\\phi_1=0.15$ |\n| $m_r$ = 0.25 | 86.15 $\\pm$ 2.42 | $\\underline{88.12 \\pm 1.90}$ | **88.56 $\\pm$ 1.55**         | 86.51 $\\pm$ 3.33      | 83.57 $\\pm$ 4.74              | $\\phi_1=0.17$ |\n| $m_r$ = 0.50 | 83.35 $\\pm$ 1.94 | **86.12 $\\pm$ 1.57**         | $\\underline{85.61 \\pm 2.08}$ | 83.25 $\\pm$ 3.86      | 82.25 $\\pm$ 4.82              | $\\phi_1=0.23$ |\n| $m_r$ = 0.75 | 81.41 $\\pm$ 2.79 | $\\underline{81.78 \\pm 3.86}$ | **82.89 $\\pm$ 4.51**         | 80.82 $\\pm$ 3.58      | 77.17 $\\pm$ 5.63              | $\\phi_1=0.32$ |\n| $m_r$ = 1.00 | 76.31 $\\pm$ 3.00 | 77.53 $\\pm$ 3.63              | 77.02 $\\pm$ 3.91              | **80.21 $\\pm$ 2.94** | $\\underline{79.19 \\pm 3.67}$ | $\\phi_1=0.46$ |\n\n***Q4. Do you consider the private information in each view? In addition to the consensus information, the complementary information is also essential, which has been indicated in this paper. However, I cannot understand what is the mechanism used to leverage the view-specific information in the proposed method. Please clarify this in details.***\n\n**A4:** Our proposed method incorporates the preservation of complementary information by training an auto-encoder to reconstruct the input from the unified embedding. This design ensures that the unified embedding have to capture complete information from each view, in order to correctly reconstruction of the multi-view input. Consequently, both consensus and complementary information are retained within the unified embedding. Furthermore, since the unified embedding is directly used for clustering, both types of information are effectively leveraged in our approach.\n\nFor a more detailed discussion and evidence regarding the preservation of complementary information, we kindly refer you to our response R1 to Reviewer LBwN, where we provide additional insights and supporting evidence."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514144565,
                "cdate": 1700514144565,
                "tmdate": 1700514144565,
                "mdate": 1700514144565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "erXi8olAJz",
                "forum": "PBSmr51fCR",
                "replyto": "gc2JKIUuSi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. My final decision will be released soon, after the discussion with other reviewer and AC."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637595881,
                "cdate": 1700637595881,
                "tmdate": 1700637595881,
                "mdate": 1700637595881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IhVOf0OIcE",
            "forum": "PBSmr51fCR",
            "replyto": "PBSmr51fCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_3U96"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5516/Reviewer_3U96"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a deep representation learning network is proposed for incomplete multi-view clustering. The method exploits the KNN imputation approach to fill the missing views and integrates the augmentation strategy. Many experiments, especially many ablation experiments are conducted to validate the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors conducted many ablation experiments to validate the methods."
                },
                "weaknesses": {
                    "value": "1. The experiments are not sufficient. Firstly, there are no experiments on large-scale datasets. Secondly, the authors only evaluate the method on the image datasets, where all views are extracted from the image. \n2. Efficiency and computational complexity are also the very important metric to evaluate the method. However, these are ignored.\n3. The novelty of the method seems not strong but the method seems very complex. Imputation the missing views for incomplete multi-view clustering is not new and has many related works. For example, the work \u2018Deep safe incomplete multi-view clustering: Theorem and algorithm\u2019 also exploits the KNN imputation for missing views. The method used Augmentation and KNN imputation to fill the missing views. However, the authors do not visualize the imputed missing views. This is not reasonable. In many existing works, such as \u2018Dual contrastive prediction for incomplete multi-view representation learning\u2019, the imputed missing views can be visualized to make the approach look more credible. However, just using ablation experiments is not convincing enough."
                },
                "questions": {
                    "value": "1. How to validate the robustness as a robust method proposed in the paper? \n2. K-Nearest-Neighbor (KNN) imputation is introduced in the paper. Is the method sensitive to the nearest neighbor numbers?\n3. From Table 1, the feature dimensions of the datasets are not large, even very small. For example, one feature dimension of handwritten datasets is just 6. Is it necessary to use deep neural networks even Transformer to extract its features again?\n4. What is the impact of the design of deep neural network layers and the selection of dimensions for each layer on clustering results?\n5. For the experimental results, why are the experimental results you provided lower than the original papers? For example, DCP on the Scene 15 dataset is much lower than the published papers. In addition, how were the experimental results of Completer obtained? The original Completer is proposed for two view data which cannot be applied on the datasets you exploited in the paper directly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688749821,
            "cdate": 1698688749821,
            "tmdate": 1699636564957,
            "mdate": 1699636564957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fH5C9EtMDO",
                "forum": "PBSmr51fCR",
                "replyto": "IhVOf0OIcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3U96 #1"
                    },
                    "comment": {
                        "value": "***W1: The experiments are not sufficient. Firstly, there are no experiments on large-scale datasets. Secondly, the authors only evaluate the method on the image datasets, where all views are extracted from the image.***\n\n**R1:** We would like to address the concerns regarding the experiments. \n\nFirstly, we would like to note that the largest datasets we used in our experiments are Reuters (18,758 samples) and ALOI_Deep (10,800 samples), which are roughly at the same scale as prior works such as RecFormer [1] (10,800 samples), SURE [2] (30,000 samples), and DSIMVC [3] (6,773 samples). We acknowledge that experiments on larger-scale datasets are important and we plan to explore them in future work due to limited time.\n\nSecondly, we acknowledge the limitation of only evaluating our method on image datasets where all views are extracted from the image. To address this concern, we have made revisions to our manuscript. In this revision, we have included two textual datasets for comparison. The statistics of these datasets are summarized in Table 4, and the results of our approach with textual features are recorded in Table 5 in Appendix C.3. BDGP is a multi-modal dataset with 1,750-dimensional visual features and 79-dimensional textual features as two views, while Reuters is a multilingual dataset where English and French are used as two views. The results achieved by our approach with textual features demonstrate its competitive effectiveness in different modalities. The main results of Table 5 are as follows:\n\n\n| Dataset\\Metrics   | BDGP Acc(\\%)                 | BDGP NMI(\\%)                 | BDGP ARI(\\%)                 | Reuters Acc(\\%)              | Reuters NMI(\\%)              | Reuters ARI(\\%)              |\n| ------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |\n| Completer        | 58.37 $\\pm$ 4.87 | 48.52 $\\pm$ 4.34 | 25.13 $\\pm$ 6.71 | 40.30 $\\pm$ 7.32 | 22.86 $\\pm$ 6.44 | 10.53 $\\pm$ 8.25 |\n| DSIMVC           | **95.71 $\\pm$ 0.21**         | **87.08 $\\pm$ 0.54**         | **89.62 $\\pm$ 0.51**         | 48.39 $\\pm$ 2.92 | **31.88 $\\pm$ 2.27**         | **26.04 $\\pm$ 2.27**         |\n| SURE             | 63.27 $\\pm$ 7.55 | 41.35 $\\pm$ 7.70 | 36.51 $\\pm$ 8.87 | $\\underline{48.63 \\pm 3.56}$ | 27.73 $\\pm$ 2.46 | 22.46 $\\pm$ 1.59 |\n| DCP              | 55.82 $\\pm$ 7.02 | 44.56 $\\pm$ 8.92 | 20.08 $\\pm$ 11.71 | 39.02 $\\pm$ 3.16 | 22.47 $\\pm$ 4.19 | 6.92 $\\pm$ 3.34$              |\n| CPSPAN           | 81.40 $\\pm$ 8.64 | 66.19 $\\pm$ 6.61 | 64.71 $\\pm$ 9.89 | 39.35 $\\pm$ 2.13 | 14.47 $\\pm$ 2.01 | 12.37 $\\pm$ 1.79 |\n| RecFormer        | 49.76 $\\pm$ 3.49 | 38.62 $\\pm$ 3.30 | 19.01 $\\pm$ 2.68 | 39.70 $\\pm$ 5.14 | 17.27 $\\pm$ 2.83 | 14.82 $\\pm$ 3.19 |\n| URRL-IMVC (ours) | $\\underline{92.52 \\pm 5.71}$ | $\\underline{82.29 \\pm 5.66}$ | $\\underline{84.26 \\pm 7.57}$ | **49.91 $\\pm$ 2.44** | $\\underline{29.49 \\pm 1.38}$ | $\\underline{25.71 \\pm 2.08}$ |\n\n***W2: Efficiency and computational complexity are also the very important metric to evaluate the method. However, these are ignored.***\n\n**R2**: Thank you for bringing this to our attention. We have now included an analysis of model parameters and computational costs (Multiply-Adds, MACs) in Table 8 of Appendix C.3. The results demonstrate that our model surpasses other state-of-the-art DIMVC methods in terms of both model size and computational cost. It is worth mentioning that our network hyperparameters, such as the dimensions of the feed-forward networks, were roughly chosen similarly to prior DIMVC methods, and we maintained the same configuration (except for the input layers) across all datasets. However, we acknowledge that there is still room for improvement in carefully designing the network architecture tailored to different datasets. The main results of Table 8 are as follows:\n\n\n| Approach         | MACs (M)           | Parameters (M)     |\n| ------------------ | -------------------- | -------------------- |\n| Completer        | 8.98               | 6.82               |\n| DSIMVC           | 9.89               | 9.33               |\n| SURE             | 8.24               | 8.24               |\n| DCP              | 9.02               | 9.01               |\n| CPSPAN           | $\\underline{4.92}$ | $\\underline{4.93}$ |\n| RecFormer        | 12.08              | 6.09               |\n| URRL-IMVC (ours) | **1.54**           | **1.01**           |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513230407,
                "cdate": 1700513230407,
                "tmdate": 1700513230407,
                "mdate": 1700513230407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IbijOi0kas",
                "forum": "PBSmr51fCR",
                "replyto": "IhVOf0OIcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3U96 #2"
                    },
                    "comment": {
                        "value": "***W3: The novelty of the method seems not strong but the method seems very complex. Imputation the missing views for incomplete multi-view clustering is not new and has many related works. For example, the work \u2018Deep safe incomplete multi-view clustering: Theorem and algorithm\u2019 also exploits the KNN imputation for missing views. The method used Augmentation and KNN imputation to fill the missing views. However, the authors do not visualize the imputed missing views. This is not reasonable. In many existing works, such as \u2018Dual contrastive prediction for incomplete multi-view representation learning\u2019, the imputed missing views can be visualized to make the approach look more credible. However, just using ablation experiments is not convincing enough.***\n\n**R3:** We appreciate the reviewer's comments and we would like to clarify certain points. Although KNN Imputation is widely used in prior IMVC approaches, its utilization in our framework and the way to further process the imputation is different. \n\nIn contrast to previous approaches like DSIMVC [3] and CPSPAN [4], where KNN Imputation is used as a missing view recovery strategy in the latent space, our framework is recovery-free, as mentioned in the paper (page 3, contribution 2). KNN Imputation serves as a preprocessing strategy in the data space, providing hints for learning a robust representation. **While the KNN Imputation algorithm itself is not novel, its utilization in our framework is different**.\n\nAdditionally, unlike prior works that use naive strategies to fuse searched KNNs, we retain the noisy raw KNN data and fuse it with well-designed attention-based Encoders (Neighbor Dimensional Encoder, NDE). **This approach is distinctive and explains why we did not visualize the imputed missing views, as they are raw data from the dataset**.\n\n**Finally, KNN Imputation is just a component of our novel framework**, which directly learns a unified and robust representation for the IMVC task, surpassing the limitations posed by existing approaches in the field.\n\n**Unified**: In the DIMVC field, the prevailing framework is cross-view contrastive learning. However, this design primarily emphasizes the consensus information in multi-view data while neglecting the valuable complementary information. Therefore, our approach goes beyond the cross-view contrastive framework and instead learns a unified representation using a meticulously designed attention-based network that effectively leverages complementary information.\n\n**Robust**: The primary approach for handling missing views in the DIMVC field is to first recover the missing views and then perform complete multi-view clustering. However, the recovered views may be unreliable, and computational resources are wasted on repeatedly mapping between data and latent space [1]. To address this issue, we propose directly learning a latent embedding that is robust to view missing, without explicitly recovering the missing data. We have devised data augmentation and KNN imputation as preprocessing techniques, which, along with the unified representation learning framework, facilitate efficient and robust representation learning.\n\n\n**Reference:**\n- [1]. Chengliang Liu, Jie Wen, Zhihao Wu, Xiaoling Luo, Chao Huang, and Yong Xu. Information recovery-driven deep incomplete multiview clustering network. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201311, 2023. doi: 10.1109/TNNLS.2023.3286918.\n- [2]. Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. Robust multi-view clustering with incomplete information. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):1055\u20131069, 2023b. doi: 10.1109/TPAMI.2022.3155499.\n- [3]. Huayi Tang and Yong Liu. Deep safe incomplete multi-view clustering: Theorem and algorithm. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 21090\u201321110. PMLR, 17\u201323 Jul 2022.\n- [4]. Jiaqi Jin, Siwei Wang, Zhibin Dong, Xinwang Liu, and En Zhu. Deep incomplete multi-view clustering with cross-view partial sample and prototype alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11600\u201311609, June 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513421043,
                "cdate": 1700513421043,
                "tmdate": 1700513421043,
                "mdate": 1700513421043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LCcbPguYlW",
                "forum": "PBSmr51fCR",
                "replyto": "IhVOf0OIcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3U96 #3"
                    },
                    "comment": {
                        "value": "***Q1: How to validate the robustness as a robust method proposed in the paper?***\n\n**A1:** We believe that the robustness of our proposed method can be explained from three aspects. \n\nFirstly, its stability has been validated through multiple runs of the same experiment. Table 2 presents the results of 10 runs, showing that our approach achieved a relatively low standard deviation. This indicates the reliability and consistency of our method.\n\nSecondly, the robustness of our method has been examined under different view-missing conditions. Figure 3 demonstrates that our approach exhibits a smaller decline in performance compared to other state-of-the-art methods even when the missing rate reaches 0.75. This suggests that our method is more robust in handling missing data.\n\nFinally, the robustness of our approach is determined by experiments on various datasets, including image-based and text-based. Our approach consistently achieves state-of-the-art performance, proving its robustness to different data types. \n\nIt is also worth noting that when we use the term \"robust\" in the title, we specifically refer to the robustness of the embeddings generated by our method. This refers to the ability of the unified embedding to remain consistent even under different missing conditions. To ensure this robustness, we have incorporated a robustness loss and an augmentation strategy during the training process. These mechanisms work together to enhance the stability and consistency of the embeddings, ultimately resulting in a more robust method.\n\n***Q2: K-Nearest-Neighbor (KNN) imputation is introduced in the paper. Is the method sensitive to the nearest neighbor numbers?***\n**A2:** According to current observations, near-optimal results can be achieved with k>=4. The related Ablation study has been presented in Table 15. The result is unimodal with the best k = 4, and larger k values tend to provide more stable results (smaller standard deviation). The main results of Table 15 are as follows:\n\n| k=1 | k=2 | k=4 | k=8 | k=16 |\n| --- | --- | --- | --- | --- |\n| 83.84 $\\pm$ 3.38 | 84.84 $\\pm$ 2.82 | **87.31 $\\pm$ 2.01** | $\\underline{87.00 \\pm 2.46}$ | 86.01 $\\pm$ 1.28 |\n\n***Q3: From Table 1, the feature dimensions of the datasets are not large, even very small. For example, one feature dimension of handwritten datasets is just 6. Is it necessary to use deep neural networks even Transformer to extract its features again?***\n\n**A3:** It is important to consider the use of deep neural networks, or even Transformers, to extract features again for datasets with small feature dimensions, such as the handwritten datasets mentioned in Table 1. While it may seem unnecessary at first glance, there are several reasons why we believe it can be beneficial for the IMVC task.\n\nFirstly, raw data often lack sufficient clustering characteristics. For instance, in the case of the Handwritten dataset, simply concatenating the data and performing clustering does not lead to satisfactory results, as demonstrated in RecFormer [1]. This highlights the need for additional feature extraction methods to capture more meaningful representations.\n\nSecondly, deep neural networks and Transformers not only extract features but also possess the capability to store learned knowledge, such as data correlation and distribution. These learned representations are crucial for handling complex cross-view correlations and missing data conditions. \n\nFinally, in our specific case, Transformers are employed to fuse information from multiple views and incorporate knowledge from nearest neighbors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513603542,
                "cdate": 1700513603542,
                "tmdate": 1700513603542,
                "mdate": 1700513603542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zkOGovXOm1",
                "forum": "PBSmr51fCR",
                "replyto": "IhVOf0OIcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3U96 #4"
                    },
                    "comment": {
                        "value": "***Q4: What is the impact of the design of deep neural network layers and the selection of dimensions for each layer on clustering results?***\n\n**A4:** We performed an ablation study to investigate the impact of the design of deep neural network layers and the selection of dimensions. The results of this study are presented in Table 13 in Appendix C.6.\n\nOur findings suggest that larger models do not necessarily lead to better clustering accuracy. In fact, the best results were obtained with a balanced setting, where there were 2 Transformer blocks in both the NDE and VDE, and the FFN and embedding dimensions were set to 256.\n\nThis observation can be explained by the fact that small models may struggle to learn the complex fusion mapping between raw data and unified embedding, while larger models can easily overfit and weaken the relationship between the latent space and data space.\n\nIt is worth noting that the design of deep neural network layers and the selection of dimensions have not been a primary focus in prior works. For example, some previous studies have used simplistic designs such as 3-layer MLPs. Therefore, the network hyperparameters in our approach were not finely tuned, and the same setting was applied to all datasets. \n\nThe main results of Table 16 are as follows:\n| Transformer Blocks | FFN Dimension | Embedding Dimension | MACs | Parameters | Accuracy |\n| ------------------ | ------------- | --------------------- | ---- | ---------- | -------- |\n| 1 | 128 | 128 | 29.73 | 12.07 | 91.86 $\\pm$ 1.80 |\n| 2 | 128 | 128 | 33.65 | 13.03 | 91.29 $\\pm$ 3.24 |\n| 2 | 256 | 128 | 43.04 | 16.69 | 90.33 $\\pm$ 5.26 |\n| 2 | 256 | 256 | 43.98 | 17.13 | **92.95 $\\pm$ 2.60** |\n| 4 | 256 | 256 | 60.08 | 21.07 | $\\underline{91.92 \\pm 2.57}$ |\n| 4 | 512 | 256 | 97.05 | 34.14 | 89.31 $\\pm$ 5.11 |\n| 4 | 512 | 512 | 103.34 | 36.39 | 90.02 $\\pm$ 3.29 |\n\n***Q5: For the experimental results, why are the experimental results you provided lower than the original papers? For example, DCP on the Scene 15 dataset is much lower than the published papers. In addition, how were the experimental results of Completer obtained? The original Completer is proposed for two-view data which cannot be applied to the datasets you exploited in the paper directly.***\n\n**A5:** It is common to observe differences in experimental results compared to original papers due to various factors such as dataset settings, view missing conditions, randomness, and adapting strategies. For example, the Handwritten dataset used in RecFormer [1] has 5 views, while CPSPAN [2] used a variant with 6 views. There is also a discrepancy in the number of samples in the Caltech7 dataset reported by these two methods (1474 vs 1400), and their reported results have a 35% difference in accuracy (86% vs 51%) on this dataset.\n\nTo ensure a fair comparison, we aligned the results of prior works with our dataset settings as described in Section 4.1 and Appendix C.2. For all methods compared in Table 2, we reproduced their results on our chosen dataset and used the same missing conditions. In Table 4, we adapted our method to match the settings in MPC [3]. However, some prior works are challenging to adapt to different numbers of views, which can hinder real-world applications. For example, the Completer method can only handle 2-view datasets, and the DCP method requires extensive modifications to handle datasets with more than 3 views. In such cases, we randomly select views to evaluate on datasets with a higher number of views. We have provided detailed experimental information in Appendix C.2 for a clearer understanding of our experimental setup.\n\nThe reported lower result for DCP compared to the original paper is due to the difference in the number of views. Our Scene-15 dataset has only 2 views, whereas the DCP paper considered 3 views. Previously, an incorrect adaptation from the 3-view implementation to the 2-view dataset was used. We have now reproduced the result using the correct setup and updated it in Table 2. The result now is slightly lower than the original paper, which may be due to the randomness or the difference in the number of views.\n\nFor Completer, we randomly select 2 views for evaluation on datasets with more than 2 views since the method is challenging to adapt to a higher number of views.\n\nBesides, we have carefully checked the experimental settings of reproduced methods and found that the experimental results are consistent with the original papers. We hope this revised response addresses your concerns. Thank you for your valuable feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513787358,
                "cdate": 1700513787358,
                "tmdate": 1700513787358,
                "mdate": 1700513787358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Ei1B5zAWh",
                "forum": "PBSmr51fCR",
                "replyto": "IhVOf0OIcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3U96 #5"
                    },
                    "comment": {
                        "value": "**Reference:**\n- [1]. Chengliang Liu, Jie Wen, Zhihao Wu, Xiaoling Luo, Chao Huang, and Yong Xu. Information recovery-driven deep incomplete multiview clustering network. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201311, 2023. doi: 10.1109/TNNLS.2023.3286918. \n- [2]. Jiaqi Jin, Siwei Wang, Zhibin Dong, Xinwang Liu, and En Zhu. Deep incomplete multi-view clus- tering with cross-view partial sample and prototype alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11600\u201311609, June 2023. \n- [3]. Junjie Liu, Junlong Liu, Shaotian Yan, Rongxin Jiang, Xiang Tian, Boxuan Gu, Yaowu Chen, Chen Shen, and Jianqiang Huang. Mpc: Multi-view probabilistic clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9509\u20139518, June 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513826754,
                "cdate": 1700513826754,
                "tmdate": 1700513826754,
                "mdate": 1700513826754,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]