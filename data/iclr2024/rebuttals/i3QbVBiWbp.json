[
    {
        "title": "Learning Informative Latent Representation for Quantum State Tomography"
    },
    {
        "review": {
            "id": "lwthci0i9h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
            ],
            "forum": "i3QbVBiWbp",
            "replyto": "i3QbVBiWbp",
            "content": {
                "summary": {
                    "value": "The paper proposes a transformer-based autoencoder architecture for quantum state tomography (QST) with imperfect measurement data. A transformer-based encoder is pre-trained to extract informative latent representation (ILR) with the task of measurement frequency reconstruction, which is succeeded by a transformer-based decoder to estimate quantum states from the measurement operators and frequencies. Extensive simulations and experiments demonstrate the remarkable ability of the proposed model to deal with imperfect measurement data in QST."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a novel and interesting idea of building Transformer-based neural network for quantum state tomography.\n\n- The paper clearly introduces the QST preliminaries, well presents the ill-posed challenge for QST and insightfully discusses the related works.\n\n- The experiment result shows that transformer auto-encoder can reconstruct quantum states far better than the baseline models from imperfect measurement data."
                },
                "weaknesses": {
                    "value": "- The idea of using transformer self-attention layers for QST is not strongly motivated, and hence not theoretically sound to me. \n\n- The model does scales poorly with the number of qubits due to the exponential number of operators in a complete set of QST measurement, so the contribution is limited.\n\n- The latent representation contains a mixture of encoded features and raw input features. This seems not reasonable in principle for transformer-based models, especially when the raw features and encoded features are quite different across different samples.\n\n- The experiment is a bit slim and cannot well show the value of the proposed model. \n  - It appears that the baseline models are linear regression models without pre-training, so it is an unfair comparison because the proposed model is exposed to far more data than the baseline models due to the presence of the encoder.  Are there stronger NN baselines? Is it possible to train the non-pre-training baselines with both pre-training data and training-data for this work?\n  - The ablation study is missing, whereas it is necessary for this model to justify the design of different model components, such as i) having the missed operators and ii) training a frequency decoder instead of directing the state decoder in pre-training."
                },
                "questions": {
                    "value": "Please kindly see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697284540486,
            "cdate": 1697284540486,
            "tmdate": 1700587506391,
            "mdate": 1700587506391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ktmtiXxfFS",
                "forum": "i3QbVBiWbp",
                "replyto": "lwthci0i9h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bkkh  -- Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful comments for helping us improve our paper. Please see below our response to your specific questions.\n\n---\n\n`Q1: The idea of using transformer self-attention layers for QST is not strongly motivated, and hence not theoretically sound to me.`\n\nA1: Thank you very much for your comments. Compared with other networks (such as fully connected networks and convolutional neural networks), the transformer enables to characterize the correlation between different measurements. Like sentences in natural language, entangled quantum states feature long-range correlations among their constituent qubits. Results in revised Figure 4 could verify the superiority of the transformer structure than FCN and CNN. To make it more readable, we made the following modifications:\n\n>(page 3, revised version)\n\n>`Inspired by the potential of attention layers to capture long-range correlation among their constituent qubits (Cha et. al., 2021), we design a transformer-based autoencoder` with a pre-training technique to reconstruct quantum states using the intermediate latent extracted from imperfect data.\n\n\n---\n`Q2: The model scales poorly with the number of qubits due to the exponential number of operators in a complete set of QST measurements, so the contribution is limited.`\n\nA2: Our method aims to deal with the imperfect measurement data usually encountered in practical applications. We admit that the proposed method does not aim to solve the exponential growth in the number of measurement operators. However, our method has demonstrated its advantage over other methods under limited measurement copies or incomplete measurements. \n\nTo demonstrate the effectiveness of our proposed method, we have implemented experiments on the IBM quantum devices $\\{ibmq\\_{manila}}$ (Qiskit-IBM-Q only supports up to 5 qubits), with results summarized in Table 2 (see Page 8, Section 4.2 in the main text). Although our ILR model is trained using simulated data that do not consider the noise model in real quantum devices, the results show excellent robustness of our ILR model in practical applications compared with LRE and the IBM-built method.\n\nIn addition, we have presented the performance of predicting properties for higher qubits using two-qubit Pauli measurements on nearest-neighbor qubits, with $(n-1)*36$ measurement types, which does not introduce exponential scaling. The results in Table 1 and Table 2 (in the main text) demonstrate the scalable ability of the highly informative latent representation in predicting properties under resource-constrained conditions.  \n\n\n\n---\n\n`Q3: The latent representation contains a mixture of encoded features and raw input features.`\n\nA3: Figure 3 provides a diagrammatic sketch that may lead to confusion. In reality, the symbol $\\mathbf{O}$ undergoes initial embedding through a linear layer before being added to the encoder feature. This process is elaborated on page 5, where it states, \"In the decoding phase, we linearly embed the missed operators $(\\mathbf{O} \\setminus \\mathbf{\\widetilde{O}}) \\in \\mathbb{R}^{m\\times 2d^2}$, which are omitted in the encoder input, as remedy tokens added to $\\hat{\\mathbf{E}}$.\" Therefore, it's important to note that $\\mathbf{O}$ does not represent raw input features; instead, it is embedded, aligning with the standard approach in transformer-based models. \n\n---"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700390771755,
                "cdate": 1700390771755,
                "tmdate": 1700390771755,
                "mdate": 1700390771755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "11hrHEHDNf",
                "forum": "i3QbVBiWbp",
                "replyto": "lwthci0i9h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_bkkh"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their efforts on clarifying the questions and improving the paper.  Most of my concerns have been addressed.  However, I am still not convinced of the motivation and soundness of the proposed approach, so I will raise the score to 5.\n\nSome minor points: \n- Q3: Do I understand it correctly that the missed operators are each embedded to dimension $2d^2$ and linearly projected to L-dim vector? If so, it means the original operators and missed operators are not rendered equally, and this may still lead to minor theoretical issues.\n\n-Q5: I think the same model structure can still be used to estimate the density matrix without having the missing operators in the decoder (correct me if I am wrong). I ask the question expecting to see how the model performs in comparison to pre-training (naively predicting p from f for existing operators) and prediction without involving any missing operators, because that indicates the benefit of introducing missing operators in decoder.  Similarly, the pre-trained task is different from the training task, so I think it may be helpful to compare it against pre-training with the target of estimating $\\rho$ or $\\mu$ (or even both) to see why it makes sense."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587452243,
                "cdate": 1700587452243,
                "tmdate": 1700587520567,
                "mdate": 1700587520567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TRlfnBySmV",
            "forum": "i3QbVBiWbp",
            "replyto": "i3QbVBiWbp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces a transformer-based architecture designed to address the challenge of quantum state tomography with imperfect measurement data. The authors present the encoder-decoder framework of their model and illustrate a pre-training technique for the encoder, enabling it to reconstruct high-quality frequencies from imperfectly measured data. Furthermore, the authors show the model's effectiveness by employing it in the reconstruction of arbitrary 2-qubit and 4-qubit quantum states, as well as in the prediction of their properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This manuscript presents a versatile model capable of simultaneously performing quantum tomography and predicting quantum properties.\n- The paper introduces a pre-training strategy aimed at enhancing the robustness of the proposed model.\n- This paper applies the proposed model to quantum state tomography of arbitrary quantum states rather than focusing on specific states or predefined quantum state sets."
                },
                "weaknesses": {
                    "value": "- I have doubts about the scalability of the proposed model for large-scale quantum systems, especially considering the exponential growth in the number of cube operators required. This implies that the dimension of the input layer for this model would increase exponentially . If this holds true, the resulting model would become exceedingly large when applied to large-scale quantum systems.\n\n  For another, the experiments about QST in this paper are limited to 2-qubit and 4-qubit quantum states. Even for 4-qubit pure states, when $N_t = 100$ and no operators are masked, the reconstruction fidelity is approximately $1-e^{-2} \\approx 0.865$, which is not high. If this limitation is attributed to the relatively small value of $N_t$, the authors may consider conducting additional experiments on 4-qubit states (or even larger quantum system) to address this concern. I was unable to locate such experiments in the appendix, which predominantly shows a series of additional experiments conducted on 2-qubit states. \n\n- I believe the proposed model lacks novelty in some sense. While it incorporates a transformer architecture, the fundamental encoder-decoder framework closely resembles those found in existing references [1] and [2] for quantum state tomography and quantum state learning. Furthermore, I feel that the pre-training strategy introduced here is similar to the setting in [2], which involves predicting measurement results for unmeasured bases.  I would appreciate it if the authors could clarify the distinction between \"masked\" operators in this paper and the unmeasured bases described in [2].\n\n  [1] Ahmed, Shahnawaz, et al. \"Quantum state tomography with conditional generative adversarial networks.\" *Physical Review Letters* 127.14 (2021): 140502.\n\n  [2] Zhu, Yan, et al. \"Flexible learning of quantum states with generative query neural networks.\" *Nature Communications* 13.1 (2022): 6222."
                },
                "questions": {
                    "value": "Major concerns:\n\n- I have stated two major concerns in the \"Weakness\" section above, with one relating to scalability and the other relating to novelty.\n\nMinor questions:\n\n- In the part of predicting quantum properties, the authors utilize locally rotated GHZ states and W states. Could the authors provide information on the range of values associated with the properties to be predicted for these two types of states?\n\n- I have some doubts about the motivation of using the model for property prediction. Why not compute the properties directly from the predicted density matrix, especially considering that the state decoder is designed to generate the density matrix as the output?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698566687502,
            "cdate": 1698566687502,
            "tmdate": 1699636056200,
            "mdate": 1699636056200,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yTbQCAiw7O",
                "forum": "i3QbVBiWbp",
                "replyto": "TRlfnBySmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wK9G -- Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful comments for helping us improve our paper. Please see below our response to your specific questions.\n\n`Q1: The scalability of the proposed model for large-scale quantum systems.`\n\nA1: Our method aims to deal with the imperfect measurement data usually encountered in practical applications. We admit that the proposed method does not aim to solve the exponential growth in the number of measurement operators. However, our method has demonstrated its advantage over other methods under limited measurement copies or incomplete measurements. In addition, we have presented the performance of predicting properties for higher qubits using two-qubit Pauli measurements on nearest-neighbor qubits, with $(n-1)*36$ measurement types, which does not introduce exponential scaling. The results in Table 1 and Table 2 (in the main text) demonstrate the scalable ability of the highly informative latent representation in predicting properties under resource-constrained conditions.  \n\n---\n\n`Q2: The experiments about QST in this paper are limited to 2-qubit and 4-qubit quantum states. Even for 4-qubit pure states, and no operators are masked, the reconstruction fidelity is approximately $1-e^{-2} \\sim 0.89$, which is not high.`\n\nA2: Thank you very much for your comments. There seems to have confusion about the definition of \\textbf{log of infidelity}. In this work, all of the values regarding \\textbf{log of infidelity} are actually obtained using $\\log_{10}(\\cdot)$ rather than $\\log_e(\\cdot)$. In that case, the current value for -2, actually corresponds to the fidelity value of $1-10^{-2} \\sim 0.99$. By comparsion, the other methods achieve values about $1-10^{-1} \\sim 0.9$. In addition, when masking 40\\% of the operators, our method achieves the result better than other methods under no masking operators. Those \nresults in Fig. 4 (b) demonstrate the superiority of our method in dealing with few measurement copies. \n\nTo make it more readable, we have made the following modifications:\n\n>(page 4, revised version)\n\n>The log of infidelity is measured as $\\bar{F}=\\log_{10}(1-F(\\rho,\\hat{\\rho}))\\in \\mathbb{R}$,\n\n---\n\n`Q3: The proposed model lacks novelty in some sense. While it incorporates a transformer architecture, the fundamental encoder-decoder framework closely resembles those found in existing references [1] and [2] for quantum state tomography and quantum state learning.`\n\nA3: Thank you very much for your comments. There is a significant difference between our method and the other two methods. \n\nCompared with the method in [1], our method aims to learn a latent representation from imperfect measured data, which can be subsequently applied to different tasks, while their method is specifically designed for reconstructing density matrices. In addition, their method introduces a discriminator to distinguish between fake data and real data, while we introduce a pre-training strategy together with a shared transformer autoencoder and decoders. Compared with GAN which is difficult to train, our approach adopts a pre-training strategy to achieve a good performance in the fine-tuning stage. \n\nCompared with the method in [2], our method is a general approach that can be applied to different tasks. Their method aims to predict measured statistics of unseen measurements, which still require further steps to obtain the density matrix or properties of quantum states. Owing to the different goals, our method and the method in [2] exhibit distinctions regarding the latent representation, the architecture, and the training procedure. \n\n(1) Latent representations: Their method aims to learn a lower-dimensional representation of quantum states to predict measured statistics of new measurements. Our method aims to take advantage of the expressive ability of highly informative representation in the latent space to characterize quantum states under resource-constrained conditions. \n\n(2) Network architectures: Their method utilizes an FCN-based representation network and an LSTM-based generation network to learn the measured statistics of new measurements, while our method utilizes a transformer-encoder with global reception fields to extract intrinsic features from imperfect data and a transformer-decoder to characterize quantum states in different levels. \n\n(3) Training procedures: Their method trains the combination of the representation network and the generation network as a whole, while our method trains the model within two stages, firstly pre-training and then fine-tuning, with the shared parts trained twice. This is a common practice in the ML community to make the best use of data to bring in improved generalization for different tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700390237445,
                "cdate": 1700390237445,
                "tmdate": 1700391341550,
                "mdate": 1700391341550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eRTBUHNoJQ",
                "forum": "i3QbVBiWbp",
                "replyto": "yTbQCAiw7O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_wK9G"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts in clarifying my questions. However, I will maintain my rating.\n\nScalability: As the authors mentioned in their response, their proposed model cannot solve the exponential growth in the number of measurement operators needed for QST. In other words, the current proposed model is not compatible with large-scale systems, limiting its applicability to small quantum systems.\n\nNovelty: While I acknowledge the difference of the proposed model and its implementation compared to previous approaches, it seems to me that the fundamental framework remains quite similar."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453697237,
                "cdate": 1700453697237,
                "tmdate": 1700453697237,
                "mdate": 1700453697237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yMkIeQpFbM",
            "forum": "i3QbVBiWbp",
            "replyto": "i3QbVBiWbp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors a transformer-based autoencoder architecture tailored for quantum state tomography with imperfect measurement data. However, the introduction of quantum mechanics is not explicit. In addition, some important points should be emphasized."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "One significant advantage of this method is its capability to provide more comprehensive information when dealing with imperfect measurement data. By using a transformer-based encoder, it effectively extracts latent information from imperfect measurement data, improving the accuracy of quantum state estimation."
                },
                "weaknesses": {
                    "value": "Please review the comments below."
                },
                "questions": {
                    "value": "1. On Page 1, the authors have mentioned that, \"To uniquely identify a quantum state, the measurements must be informatively complete to provide\nall the information about \u03c1 (Je\u02c7 zek et al., 2003). The exponential scaling of parameters in $\\rho$ requires\nan exponentially increasing number of measurements, each of which requires a sufficient number\nof identical copies (Gebhart et al., 2023).\" However, this statement is not entirely precise. When the density matrix is low-rank [1] or takes the form of a matrix product operator [2], the POVM may not be informatively complete. Consequently, when a low-dimensional structure exists within the density matrix, many traditional methods can be applied with significantly fewer repeated measurements, which is an important direction to explore compared to neural network-based approaches. The reviewer suggests that this structural aspect should be included in the introduction.\n\n[1] J. Haah, A. Harrow, Z. Ji, X. Wu, and N. Yu, \u201cSample-optimal tomography of quantum states,\u201d IEEE Transactions on Information\nTheory, vol. 63, no. 9, pp. 5628\u20135641, 2017.\n\n[2] Zhen Qin, Casey Jameson, Zhexuan Gong, Michael B Wakin, and Zhihui Zhu.  \u201cStable tomography for structured quantum states,\u201d arXiv preprint arXiv:2306.09432, 2023.\n\n2. In Section 3.1, PRELIMINARIES ABOUT QST, it is advisable to use the notation $2^n$ instead of just $d$. This change is necessary to establish the proper context for the definition of a qubit as introduced in Section 3.2, THE ILL-POSED QST PROBLEM. Additionally, it would be beneficial to introduce the concepts of Hermitian, positive semidefinite (PSD) structure, and unit trace in the density matrix earlier in the section for improved clarity.\n\n3. In Figure 2, due to the missing definition of qubit, for readers without any quantum background, it is hard to compute the total number of density matrices. Consequently, the number of missed measurements will be meaningless.\n\n4. In part \"QST process using a transformer-based autoencoder\", should the architecture need to be designed anew for different qubits and POVMs, the authors should underscore this requirement.\n\n5. The reviewers suggests that the authors should add the convergence rate of infidelity for different algorithms.\n\n6. In the section 4.2 RECONSTRUCTING DENSITY MATRICES, the use of 2-qubit and 4-qubit examples may be considered limited. It would be beneficial to include discussions involving at least 8-qubit systems for a more comprehensive analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690173307,
            "cdate": 1698690173307,
            "tmdate": 1699636056117,
            "mdate": 1699636056117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LaZ96C8jjc",
                "forum": "i3QbVBiWbp",
                "replyto": "yMkIeQpFbM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aafB -- Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful comments for helping us improve our paper. Please see below our response to your specific questions.\n\n---\n\n`Q1:  The statement about informative measurement is not entirely precise. This structural aspect should be included in the introduction with two references.`\n\nA1: Thank you very much for your constructive comments. There exist some effective methods that are tailored for special quantum states. Our method is designed for QST with limited measurements that may happen in practical QST scenarios and can be applied to general quantum states without any restrictions. Following your valuable comments, we have made the following modifications:\n\n>(page 1, revised version)\n\n>`Generally, to uniquely identify a quantum state, the measurements must be informatively complete to provide all the information about $\\rho$~(Je\u02c7zek et al., 2003), except for the special case when density matrices of quantum states are low-rank (Haah et al., 2017) or take the form of matrix product operators (Qin et al., 2023).`\n\n---\n\n`Q2: it is advisable to use the notation \n $2^n$ instead of just $d$. It would be beneficial to introduce the concepts of Hermitian, positive semidefinite (PSD) structure, and unit trace in the density matrix earlier in the section for improved clarity.`\n\nA2: Thank you very much for your constructive comments. We have made the following modifications:\n\n>(page 3, revised version)\n\n>QST is a process of extracting useful information about a quantum state based on a set of measurements. `For a $n$-qubit quantum system with dimension $d=2^n$`, the full mathematical representation of a quantum state can be described as a density matrix $\\rho\\in \\mathbb{C}^{d\\times d}$.\n\n>(page 1, revised version)\n\n>In the physical world, the full description of a quantum state can be represented as a density matrix $\\rho$, `i.e., a positive-definite Hermitian matrix with unit trace $\\rho=\\rho^{\\dagger}$, $\\textup{Tr}(\\rho)=1$.`\n\n---\n\n`Q3: In Figure 2, due to the missing definition of the number of qubits, for readers without any quantum background, it is hard to compute the total number of density matrices.`\n\nA3: Thank you very much for your constructive comments. We have included the definition of the number of qubits in Section 3.2, with the following modifications:\n\n>(page 3, revised version)\n\n>QST is a process of extracting useful information about a quantum state based on a set of measurements. `For a $n$-qubit quantum system with dimension $d=2^n$ `, the full mathematical representation of a quantum state can be described as a density matrix $\\rho\\in \\mathbb{C}^{d\\times d}$.\n\nIn Figure 2, we perform QST on 2-qubit systems based on cube measurements with a total of 36 measurements.  To make it more readable, we have made the following modifications:\n\n\n>(page 4, revised version)\n\n>Specifically, we observe a sharp decrease in infidelity as the number of copies increases, `while masking 8 operators among the original complete cube measurements (with a total of 36 measurements) results in a large increase in infidelity.` The underlying reasons might be that the noise addition and information loss in the imperfect measurement data result in a diminished representation of quantum states.\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389324494,
                "cdate": 1700389324494,
                "tmdate": 1700389324494,
                "mdate": 1700389324494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6C3SqfrJav",
                "forum": "i3QbVBiWbp",
                "replyto": "sB80CGslv9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aafB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your rebuttal. I will maintain my current score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634554140,
                "cdate": 1700634554140,
                "tmdate": 1700634554140,
                "mdate": 1700634554140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4L0VKDjQc6",
            "forum": "i3QbVBiWbp",
            "replyto": "i3QbVBiWbp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
            ],
            "content": {
                "summary": {
                    "value": "The submission extends the concept of the masked autoencoder to enhance the sample complexity of quantum state tomography. The authors have conducted numerical simulations involving systems of up to 12 qubits to assess the performance of their proposal. Nonetheless, several statements throughout the paper and the configurations used in the numerical simulations introduce confusion, making it challenging to discern the precise contributions of the submission."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The utilization of deep learning techniques to improve quantum state tomography (QST) represents an emerging and promising field. Nevertheless, the current body of work focused on designing specialized learning models for quantum state tomography remains relatively limited. The submission effectively addresses this gap and presents intriguing results."
                },
                "weaknesses": {
                    "value": "The primary weakness of the submission stems from inaccuracies in statements and the presence of confusing settings. The presence of incorrect or imprecise statements obscures the novelty and technical contributions of the proposed method. Additionally, while the authors have conducted a series of numerical simulations, the absence of a comparative analysis with state-of-the-art methods hinders our ability to gauge the practical advancements offered by the proposed method."
                },
                "questions": {
                    "value": "1)  The motivation behind designing the auto-decoder structure is not entirely clear. It remains uncertain whether the authors aim to directly adapt the concept of Masked autoencoders to tackle QST tasks or if deeper insights are guiding this choice. Providing more context on this decision would enhance the submission's coherence.\n\n2) The use of a state decoder to predict state properties appears to introduce confusion. If a user's primary interest lies in estimating specific properties, more efficient methods may be available than the proposed approach. It is essential to consider that state reconstruction, even with the inclusion of masked operations, can be resource-intensive and time-consuming.  \n\n3) The numerical simulations are limited to older methods for QST. Consequently, it remains uncertain whether the purported contributions and advantages can be effectively realized in practical applications. To establish the practicality and competitiveness of the proposed approach, a systematic examination involving a wider spectrum of advanced deep learning methods is imperative. For instance, recent studies [Ahmed, Shahnawaz, et al. \"Quantum state tomography with conditional generative adversarial networks.\" Physical Review Letters 127.14 (2021): 140502.]  have explored the use of incomplete POVM information in conjunction with a generative adversarial learning scheme to address QST tasks, and a thorough comparative analysis with such contemporary approaches would greatly enhance the submission's value and relevance. \n\n4) In Table 3, the authors benchmark the proposed method for estimating coherence and entanglement of GHZ and W states with 8/12 qubits. Given that this task has also been investigated in a study by Zhu et al. in 2022, a comparative study becomes imperative. The relevant results would provide valuable insights into the relative strengths and weaknesses of these two methods for the specified task."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736138703,
            "cdate": 1698736138703,
            "tmdate": 1699636056043,
            "mdate": 1699636056043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "huRRUyWsYq",
                "forum": "i3QbVBiWbp",
                "replyto": "4L0VKDjQc6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aFck -- Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful comments for helping us improve our paper. Please see below our response to your specific questions.\n\n---\n`Q1: The motivation behind designing the auto-decoder structure is not entirely clear.`\n\nA1: Thank you very much for your comments. We drew inspiration from the implementation of mask autoencoder, but focusing on different fields and having different start points. The motivation behind this is that in practical applications, QST may be challenged by incomplete measurement settings or limited measurement copies to obtain inaccurate statistics. For example, in solid-state systems, the process associated with measuring one copy of a quantum state can be time-consuming, and implementing a sufficient number of measurement operators requires complex and costly experimental setups. Given a coherence time (beyond which quantum states may change), the total copies of identical states for measurements may be constrained. In this scenario, the measurement data may be a complete but inaccurate frequency vector (a few copies assigned to each measurement operator ) or an accurate but incomplete frequency vector (sufficient copies assigned to partial measurement operators that are experimentally easy to generate). The two factors are collectively referred to as imperfect scenarios (i.e., ill-posed problems) in this paper, which hinder the precise reconstruction of a quantum state (see Fig. 2, page 4 in the main text). Recognizing the representation reduction posed by the ill-posed problem, we turn to a powerful neural architecture to extract a latent representation that may compensate for imperfect data. Specifically, we propose a transformer-based autoencoder to address the challenge of imperfect measurement data in practical QST applications. \n\n---\n`Q2: The use of a state decoder to predict state properties appears to introduce confusion.`\n\nA2: In this work, we aim to solve two problems using a unified framework, with the pre-training process implemented (see Fig. 3., page 5 in the main text). The pre-training process can retrieve high-quality frequencies from imperfect data to enhance the expressiveness of the latent representation. Then, based on the informative latent representation, we introduce a state decoder to reconstruct density matrices and a property decoder to predict properties, respectively. For each task, its decoder is fine-tuned separately, with the pre-trained part i.e., the encoder part shared, which can reduce the model size. We also implement experiments in Appendix E to verify that directly using a property predictor can obtain better performance than calculating the properties from the reconstructed density matrices.\n\nAdditionally, we consider that in practical applications, implementing a sufficient number of measurement operators requires complex and costly experimental setups, and measuring large copies of a quantum state can be time-consuming. The masked operation serves the purpose of diminishing the total sets of measurement operators, while the pre-training strategy aims to minimize the number of measurement shots required for each state. In fact, reducing the number of measurement copies may demonstrate the effective performance of our method. \n\n---\n`Q3: It remains uncertain whether the purported contributions and advantages can be effectively realized in practical applications.?`\n\nA3: Our method is designed for practical applications that suffer from limited measurement resources, i.e., limited copies or incomplete measurements that are usually encountered in practical applications. To demonstrate the effectiveness of our proposed method, we have implemented experiments on the IBM quantum devices $ibmq\\_{manila}$, with results summarized in Table 2 (see Page 8, Section 4.2 in the main text). Although our ILR model is trained using simulated data that do not consider the noise model in real quantum devices, the results show excellent robustness of our ILR model in practical applications compared with LRE and the IBM-built method.\n\n---"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388205616,
                "cdate": 1700388205616,
                "tmdate": 1700388205616,
                "mdate": 1700388205616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5jFemH2oEr",
                "forum": "i3QbVBiWbp",
                "replyto": "4L0VKDjQc6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aFck -- Part 2"
                    },
                    "comment": {
                        "value": "---\n\n`Q4: A  systematic examination involving a wider spectrum of advanced deep learning methods is imperative. Especially GAN-based method [2]`\n\nA4: Thank you very much for your constructive comments. To demonstrate the effectiveness of our method, we have implemented two additional deep learning methods, the convolutional neural networks (CNN)-based QST [1] and GAN-based method [2] (PRL), with results provided in Figure 4 (page 8, revised version). \nMoreover, we include ILR without employing the pretraining strategy to ensure a fair comparison with other methods that do not utilize this specific strategy. In these ill-posed scenarios, the transformer-based autoencoder architecture outperforms FCN and CNN architectures. Moreover, the incorporation of a pre-training strategy significantly enhances the performance of the autoencoder architecture. Following your comments, we have made the following modifications:\n\n> (page 7, revised version)\n\n> Here, we implement the FCN (Ma et al., 2021), LRE (Qi et al., 2013), and MLE (Je\u02c7zek et al., 2003),\n`CNN-based method (Lohani et al., 2020) and GAN-based QST method (Ahmed et al., 2021` for comparison.\n\n---\n\n`Q5: Compared with Zhu et al. in 2022, about GHZ and W states with 8/12 qubits.`\n\nA5: We would like to compare these two methods within the same scenario. However, despite both methods considering GHZ and W states, their applications and settings differ, making it challenging to establish uniformity. Due to the challenges associated with implementing their method in our specific scenario, accomplishing a fair reconstruction of their methods will demand additional time and effort on our part. We are still working to address this aspect.\n\n\nBefore posting the comparison result, we currently analyze the differences between the two methods. \nBoth our work and [Zhu et al. 2022]'s method explore the idea of extracting hidden features from imperfect measurement data to learn quantum states. However, their method aims to learn a lower-dimensional representation of quantum states from measured statistics and operators, and our method aims to learn an informative latent representation from noisy and incomplete measurements that helps solve practical problems. \n\nSpecifically, the two methods exhibit the following distinctions in their implementation:\n\n1) Latent representations: Their method aims to learn a lower-dimensional representation of quantum states to predict measured statistics of new measurements. Our method aims to take advantage of the expressive ability of highly informative representation in the latent space to characterize quantum states from imperfect measurement data. \n\n2) Network architectures: Their method utilizes an FCN-based representation network and an LSTM-based generation network to learn the measured statistics of new measurements, while our method utilizes a transformer-encoder with global reception fields to extract intrinsic features from imperfect data and a transformer-decoder to characterize quantum states in different levels.\n\n3) Training procedures: Their method trains the combination of the representation network and the generation network as a whole, while our method trains the model within two stages, firstly pre-training and then fine-tuning, with the shared parts trained twice. This is a common practice in the ML community to make the best use of data to bring in improved generalization for different tasks.\n\n---\n\n[1] Sanjaya Lohani et al. Machine learning assisted quantum state estimation. Machine Learning: Science and Technology, 1(3): 035007, 2020.\n\n[2] Ahmed Shahnawaz et al. Quantum state tomography with conditional generative adversarial networks. Physical Review Letters 127.14 (2021): 140502\n\n[3] Yan Zhu et al. Flexible learning of quantum states with generative query neural networks. Nature Communications, 13\n(1):1\u201310, 2022.\n\n---\n\nWe kindly request that you inform us if there are any other concerns that might have been overlooked, or if you have new ones."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388515654,
                "cdate": 1700388515654,
                "tmdate": 1700388719136,
                "mdate": 1700388719136,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iULLxD5vig",
                "forum": "i3QbVBiWbp",
                "replyto": "4L0VKDjQc6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. While I recognize your attempt to employ a single model for various quantum tasks, I remain unconvinced about employing one model for two markedly distinct types of tasks. Specifically, in quantum state tomography, the learning model needs to capture comprehensive information about the quantum state. In contrast, predicting the properties of quantum states may only require capturing partial information. Due to this concern, I maintain my score at 3."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654777691,
                "cdate": 1700654777691,
                "tmdate": 1700654777691,
                "mdate": 1700654777691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VkoCbmyr4m",
                "forum": "i3QbVBiWbp",
                "replyto": "4L0VKDjQc6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1292/Reviewer_aFck"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. While measured statistics enable both full and partial estimation, theoretical investigations reveal an exponential gap in the required number of measurements for achieving full versus partial estimation within a tolerable error. It is implausible for deep learning models to bridge this gap unless: 1) the authors claim that deep learning can efficiently and accurately simulate the dynamics of quantum computers; or 2) the explored states are highly restrictive, resulting in a simple low-dimensional representation (as seen in the numerical simulation). This is why I suggested the authors adopt the setting in [Zhu et al. 2022] to assess the proposed method's performance in learning a class of ground states around 50 qubits. I am willing to reconsider my evaluation if the authors provide compelling evidence demonstrating that deep learning can achieve a polynomial number of training examples and measurements per training example with respect to the qubit number in that scale. Btw, I never discount the use of pretraining models. My primary concern centers on the specific focus on two downstream tasks."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666565521,
                "cdate": 1700666565521,
                "tmdate": 1700666565521,
                "mdate": 1700666565521,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]