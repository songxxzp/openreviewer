[
    {
        "title": "SynBench: Evaluating Pretrained Representations for Image Classification using Synthetic Data"
    },
    {
        "review": {
            "id": "W3Qd8upYAJ",
            "forum": "9RLC0J2N9n",
            "replyto": "9RLC0J2N9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_af4U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_af4U"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of evaluating the quality of a pretrained model (for classification) without any data from or knowledge of the downstream tasks for which it will be used. The idea is to generate binary classification data from a Gaussian mixture model and evaluate how well the representation separates the two classes relative to a theoretically optimal classifier. Models are evaluated in terms of standard accuracy and $\\epsilon$-robust accuracy (in a certain adversarial sense). Depending on the experiment, evaluation metrics are based on CIFAR10, SVHN, TinyImageNet, or the average performance over the 27 benchmark tasks from the CLIP paper.\n\nGenerally, this paper was interesting to read and clear, but there are some issues that need to be addressed before it is ready for publication."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The goal of the paper is ambitious. If the paper solved the problem they pose, it would be very impactful. \n* The paper considers its data and results from a number of interesting perspectives, speaking to a general concern for thorough evaluation. \n* The idea put forth in the paper is interesting and worthy of further exploration.\n* The paper is well-edited and clearly written."
                },
                "weaknesses": {
                    "value": "* The paper assumes that the raw input data for each class has a Gaussian distribution. In computer vision, this means assuming that, in pixel space, two visual categories have Gaussian distributions. This is akin to assuming we start with quite a good representation! What's the point of representation learning if that's where we start? Why would you transform your data? \n* One of the key claims of the paper is that the \"Pearson correlation between SynBench-Scores and the average real-life task accuracy is larger than 0.9\". Flipping to Table 7 in the appendix, we learn that these Pearson correlations are based on $n=5$ data points! It is statistically unacceptable to make this claim without reporting confidence intervals. Running through the [standard calculations](http://faculty.washington.edu/gloftus/P317-318/Useful_Information/r_to_z/PearsonrCIs.pdf), it seems to me that the confidence interval for the Pearson correlation of 0.92 would be $[-0.81, 1.0]$. That is, this correlation value is not very meaningful. This seems to be a highly misleading error, but I'm eager to be corrected if I'm off base on this point. In the absence of this claim, there is little evidence that \"real world\" task performance is related to the scores computed in this paper. \n* There are many hyperparameters in this paper and no mention of hyperparameter tuning, e.g \"$a_t$ ranging from 0.7 to 0.9\" or \"$\\epsilon$ from 0 to 0.8\" or \"attack strength 0.2\" etc. Can details be provided? If not, the reader should probably assume that the hyperparameters are charitable to the proposed method. \n* SimCLRv2 has the highest SynBench-Score in Table 1, but is generally understood to perform more poorly than the other techniques in the table in common evaluation protocols (see e.g the papers for DINO, BYOL, MAE, or other more recent methods). No mention is made of this in the discussion. This seems like a fairly large problem for the proposed method. \n* The paper considers CIFAR10 and TinyImageNet as \"real-life downstream data\" (see Sec. 4.3) - I think many in computer vision would disagree with this characterization. \n* The paper cites Zoran and Weiss 2012 to support the claim that natural image statistics can be well-represented by GMMs. However, that work focuses only on small image patches and (as far as I know) makes no claim that entire natural images can be well-represented by GMMs. Isn't this a misleading use of that reference? (Appendix A.1)."
                },
                "questions": {
                    "value": "See \"Weaknesses\" for supporting details on these questions.\n1. A few big picture conceptual questions: Why should we even think it's possible to evaluate the quality of a representation for arbitrary classification tasks without any knowledge or data related to downstream tasks for which it will be used? Moreover, why should we think it's possible to boil that down to a single scalar? What does \"quality\" even mean without reference to downstream tasks?\n2. Are the Pearson correlations appropriately reported (see weaknesses)? Absent that, is there strong evidence that the method in the paper produces scores that are predictive of performance on downstream tasks? \n3. Why is it not a problem for the proposed method that SimCLR has the highest score in Table 1, despite being generally understood to underperform DINO and supervised pretraining? \n4. The Gaussian assumption is basically an assumption that we start with a very good representation - this seems pretty unrealistic. Why would we bother transforming our data with a pretrained model at that point? \n5. Is the reference Zoran and Weiss used appropriately in Appendix A.1? \n6. How were the hyperparameters in this paper tuned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536282911,
            "cdate": 1698536282911,
            "tmdate": 1699636671666,
            "mdate": 1699636671666,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OlLvfgZQe3",
                "forum": "9RLC0J2N9n",
                "replyto": "W3Qd8upYAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the comments and we believe there are many points that we need to clarify. We wish our explanations can eliminate doubts. (1/3)"
                    },
                    "comment": {
                        "value": "## 1. Why Gaussuian distribution\nWe view SynBench as a \"necessary\" and \"minimum\" model test. We concur with the reviewer's argument that assuming a Gaussian distribution for raw input data in pixel space implies starting with a significantly structured and \"good\" representation. This assumption is intentional and serves the specific purpose of establishing a foundational test. If a pretrained representation network cannot preserve the quality of data drawn from such simple, well-defined distribution (indicated by a SynBench-score close to 1), it raises significant concerns about its potential performance in more complex, real-world scenarios. In our test, we do see that the seperability of the representations could degrade by significant margins compared with their reference behavior in the well-defined simple input space.\n\nTo draw an analogy for easier understanding, consider a hgh-performing language model. One might test its basic capabilities by checking whether it correctly writes \"2\" after \"1+1=\" or classifies \"bad\" as a negative word. Failure to perform these straightforward tasks with clear expected outcomes would suggest fundamental deficiencies in the model. \n\nMoreover, we also believe that our designed task with conditional Gaussian is feasible. For a feature extractor robust to noises/good at denoising, all class-1 synthetic samples shall be treated as point $\\mu$ and all class-2 samples as point $-\\mu$, which are then trivial to classify.\n\n## 2. Pearson and confidence interval\nLet $r$ be the Pearson correlation coefficient, $p$ be the number of models. We ran the calculation based on the reference shared by the reviewer, and see that the upper and lower confidence interval limits in z-space are $0.5\\ln(\\frac{1+r}{1-r}) \\pm 1.96\\sqrt{\\frac{1}{p-3}}=1.589\\pm 1.386$. Translating to r-space by $r=\\frac{e^{2z}-1}{e^{2z}+1}$ yields the upper limit of 0.995 and the lower limit of 0.203. That said, we believe the confidence interval for the Pearson correlation of 0.92 is instead $[0.203,0.995]$ if the desired confidence level is $95\\%$. We are not sure how the reviewer obtained $[-0.81,1.0]$ and wish the reviewer can let us know to make sure we are on the same page. \n\nIn the following Table 4.1, we added four efficient nets' SynBench-scores, together with the average of their reported performance on 27 downstream tasks in [Radford et al., 2021](https://arxiv.org/pdf/2103.00020.pdf), Table 10. We ran the same calculation for the Pearson correlation coefficient $r=0.87$ and $p=9$ to obtain the confidence interval of $[0.488,0.972]$ which suggest at least moderate correlation upto strong corelation.\n\n| | ViT-B/16 | ViT-L/16 | ViT-B/32 | Resnet50-SimCLRv2 | Resnet101-SimCLRv2 | ENet_b0   | ENet_b1   | ENet_b2   | ENet_b3   | Pearson correlation |\n|----------|----------|----------|----------|-------------------|--------------------|------|------|------|------|---------------------|\n| Accuracy (%) | 74.3     | 75.5     | 72.6     | 75.4              | 75.4               | 72.5 | 72.6 | 73.1 | 73.9 | 1.0                 |\n| SynBench (n=2048)  | 0.33     | 0.26     | 0.0      | 0.66              | 0.60               | 0.02 | 0.04 | 0    | 0    | 0.85                |\n| SynBench (n=4096)  | 0.52     | 0.49     | 0.01     | 0.69              | 0.84               | 0.13 | 0.13 | 0.09 | 0.03 | 0.87                |\n\nTable 4.1 The correlation between SynBench-score and the average accuracy on 27 real-life tasks.\n\n \n## 3. Role of hyperparameters \n\n*Threshold accuracy $a_t$*. $a_t$ is a user-specified threshold accuracy based on their utility requirement. To give a complete picture, we calculate all SynBench-score from 0.7 to 0.9 in table 6 in the paper. By referring to the table, users can see which model might be more suitable for him/her. Throughout our analysis, we used $a_t=0.7$ since in average the reallife tasks accuracy is above 70% (Table 7 in the paper). \n\n*Robust margin $\\epsilon$*. Recall that $\\epsilon$-robust Bayes optimal classifier give the optimal classifier if the expected perturbation norm is subject to $\\epsilon$ budget. When SynBench is used to evaluate clean accuracy without adversarial attacks as in Section 4.2, we let $\\epsilon=0$. When we want to perform $\\epsilon$-robust linear probing to increase the robustness against adversarial examples as in Section 4.3, we let $\\epsilon=\\text{argmax}_\\epsilon \\text{SynBench-Score}$.\n\n*Attack strength*. Throughout our analysis, we let the attack strength be 0.2. This is guided by our wish to distinguish each model's robustness level to adversarial attacks. If the attack strength is too strong, the robust accuracy of all models will decrease to near zero, while if the attack is too weak, no misclassification will occur. We note that this hyperparameter is only for attack evaluation, and it won't affect the computation of SynBench score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360473690,
                "cdate": 1700360473690,
                "tmdate": 1700360473690,
                "mdate": 1700360473690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xHtBUwicxm",
                "forum": "9RLC0J2N9n",
                "replyto": "W3Qd8upYAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the comments and we believe there are many points that we need to clarify. We wish our explanations can eliminate doubts. (2/3)"
                    },
                    "comment": {
                        "value": "## 4. SimCLRv2 has higher SynBench-Score than DINO\nWe need to note that SynBench is to evaluate specific models, not methodology. As there might have many subtle differences in training parameters/optimizers/datasets, we can only make conclusions for models used in our paper. In our paper, we use SimCLRv2 models from the [simclr official repository](https://github.com/google-research/simclr), specifically, we use SimCLRv2 models with resnet depth 50 and 100, both with width 2X and SK True. For example, if we try to compare with DINO models, we can refer to [DINO official repository](https://github.com/facebookresearch/dino) and see they are generally on par with the specific SimCLRv2 models we considered (see table 4.2). From this example, we don't see these two specific SimCLRv2 models to perform more poorly than DINO, and our paper makes no conclusions for BYOL and MAE. As such, we do not see there is a problem for our proposed method.\n\n| | ViT-S/16-DINO | ViT-S/8-DINO | ViT-B/16-DINO |ViT-B/8-DINO | ResNet50-SimCLRv2 | ResNet101-SimCLRv2 |\n|----------|----------|----------|----------|-------------------|--------------------|------|\nImageNet acc. | 77.0% | 79.7% | 78.2% | 80.1% | 77.7% | 79.0%|\n\nTable 4.2 The linear probing accuracy on ImageNet1k reported in the original repository.\n\n\n##  5. CIFAR10 and TinyImageNet as \"real-life downstream data\" in Section 4.3 \nThese are popular CV downstream datasets. We have updated our paper accordingly.\n\n## 6. GMMs' capability of modeling the statistics of natural images\nWe believe the claim the reviewer is referring to is \"Our use of Gaussian mixtures for analysis is supported by its capability of modeling the statistics of natural images (Zoran \\& Weiss, 2012) and prior arts ......\". Since our target is pretrained vision models, the capabilities of preceiving contrasts and edges etc are centric (this is also the motivation for convolutional kernels for ResNets, images patches for ViTs etc). From the [reference](https://papers.nips.cc/paper_files/paper/2012/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf), we quote \"Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images\" and it is not hard to see that these statistics include contrast, textures at different scales and orientations, and boundaries of objects in the reference. If the reviewer is curious about how images patches connects to whole images, there are some discussions in the literature, [From Learning Models of Natural Image Patches to Whole Image Restoration](https://people.csail.mit.edu/danielzoran/EPLLICCVCameraReady.pdf) and [From Patches to Natural Images via Hierarchical Dirichlet Processes](https://www.michaelchughes.com/papers/JiHughesSudderth_PracticalBNPWorkshop_2016.pdf). We note that another reason for these seminal works to study from image pathes instead is \"Due to the high dimensionality of the images captured by modern cameras\" (quote from [From Patches to Images: A Nonparametric Generative Model](https://proceedings.mlr.press/v70/ji17a/ji17a.pdf)). Afterall, at the bare minimum we all know GMMs are universal approximators of densities ([Universal Approximation Using Radial-Basis-Function\nNetworks](https://direct.mit.edu/neco/article/3/2/246/5580/Universal-Approximation-Using-Radial-Basis), [Networks for approximation and learning](https://ieeexplore.ieee.org/document/58326)), meaning any smooth density can be approximated with any speci\ufb01c nonzero amount of error by a Gaussian mixture model with enough components.\n\nTo our knowledge, we don't know which part is misleading but we are willing to remove or modify the sentence (\"Our use of Gaussian mixtures for analysis is supported by its capability of modeling the statistics of natural images (Zoran \\& Weiss, 2012) and prior arts ......\") following the reviewer's suggestion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360591928,
                "cdate": 1700360591928,
                "tmdate": 1700360591928,
                "mdate": 1700360591928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q4eP3rseQO",
                "forum": "9RLC0J2N9n",
                "replyto": "W3Qd8upYAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the comments and we believe there are many points that we need to clarify. We wish our explanations can eliminate doubts. (3/3)"
                    },
                    "comment": {
                        "value": "## 7. How can SynBench predict classification performance across a broad range of tasks\nThink of how representation learning research typically evaluate a model for transfer learning - by running tests on broad range of downstream tasks. And the reason behind this is to see how the model behaves in different scenerios. To theorize things, we believe the general behavior of a pretrained representation is measured by how it perform on tasks of different difficulty levels. That is why we think a fundamental part of our design is to simulate tasks of different difficulty levels. One difference between SynBench and a traditional probing test is that, for example, we are using the classification problem of two highly overlapped Gassuain, instead of classifying ImageNet21k. We hope this clarification builds enough intuition to understand the following:\n1. We vary $s$ in equation 2 from 0.1 to 5 in increments of 0.1, which correspond to optimal accuracy (groundtruth difficulty) ranging from 55% to 100% and 50 difficulty levels. If we refer to this [figure](https://imgur.com/a/rH819Jm), we see each of the red points correspond to one of our similated trials with difficulty levels (x-axis).\n2. Baseline methods are task/data dependant, which means they are somewhat bound to tasks of that similar difficulty levels. If we refer to the same [figure](https://imgur.com/a/rH819Jm), it could be the single purple point with fixed level of difficulty.  \n3. If we include certain knowledge of possible downstream data properties, say locality of pixel dependencies, then the prediction will indeed be more accurate (see our section 4.4)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360619045,
                "cdate": 1700360619045,
                "tmdate": 1700360619045,
                "mdate": 1700360619045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6FjkyVRfgA",
                "forum": "9RLC0J2N9n",
                "replyto": "W3Qd8upYAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on Our Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer af4U,\n\nWe would like to thank you again for your review! As the rebuttal/discussion phase is nearing its end, we want to check in to see if we have dispelled all your concerns, or if there are any additional points we can help clarify.\n\nWe understand that the discussion period is short, and your support and feedback are very important to us. We are truly grateful for your insights and the time you've dedicated to reviewing our work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588395717,
                "cdate": 1700588395717,
                "tmdate": 1700588395717,
                "mdate": 1700588395717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0e8fzyCMP9",
                "forum": "9RLC0J2N9n",
                "replyto": "W3Qd8upYAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_af4U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_af4U"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response! Apologies for getting back to you so late. \n\nMy fundamental issue remains. The paper is making a rather extraordinary claim: that assessing performance on separating two Gaussians could be predictive of performance on arbitrary downstream classification tasks. I don't see enough strong evidence to support that extraordinary claim. Some specific responses can be found below. \n\n**Regarding the Gaussian distributions**, I tend to agree with other reviewers that it seems like a fairly contrived task, and it is difficult to see why it would be predictive of performance on arbitrary downstream tasks. It seems like the authors more or less agree (with the \"contrived\" part, anyway) - based on the authors' response, SynBench is supposed to be a low bar that any reasonable representation should clear. But then shouldn't any model that performs decently well on real-world tasks excel on SynBench? If so, what are we to make of e.g. Table 4.1 in the rebuttal, where the EfficientNet backbones have awful SynBench scores but good real-world accuracies? \n\n**Regarding the confidence intervals**, you are correct that I messed up the calculation and I agree with your numbers. However, I think my point stands. A key claim of the paper - \"The Pearson correlation coefficient between SynBench-Scores and the average real-life task accuracy is larger than 0.9\" - is pretty misleading if the $95\\%$ confidence interval is $[0.203, 0.995]$. Moreover, with such a wide confidence interval, this does not seem to be strong evidence that the proposed approach works much better than other approaches. Based on the confidence intervals we're discussing, it seems like $n=5$ is likely too small to draw strong conclusions about Pearson's $r$. While the additional results in Table 4.1 are interesting, there is no comparison to other methods. The scores also vary wildly between backbones that have similar real-task performance, which raises more questions that it answers. \n\n**Regarding GMMs and natural images**, you are correct that I'm referring to the claim that the paper's claim that the \"use of Gaussian mixtures for analysis is supported by its capability of modeling the statistics of natural images\". Since this paper uses Gaussian mixtures very differently than Zoran & Weiss (i.e. two Gaussians for the whole image vs. tens of Gaussians for modeling patches) this strikes me as a misleading use of that reference. What evidence in Zoran & Weiss supports modeling entire natural images using a mixture of a small number of Gaussians? \n\n**Regarding model ranking**, I didn't realize the SimCLR models were 2x variants. Probably it's best to update the tables to rename e.g. \"Resnet50-SimCLRv2\" to something like \"Resnet50(2x)-SimCLRv2\", since \"Resnet50\" by itself typically just denotes the 1x version. But there are still ranking reversals - why should the ResNet101 variant have a higher SynBench score than the ResNet50 variant, when the 101 has better performance than the 50? It seems like an important point to understand. \n\nAt the moment I lean towards keeping my rating as-is - I think the paper has potential, but currently there are too many question marks that have direct consequences for the central claims of the paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627764050,
                "cdate": 1700627764050,
                "tmdate": 1700628966857,
                "mdate": 1700628966857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XHw6rSlDt1",
                "forum": "9RLC0J2N9n",
                "replyto": "W3Qd8upYAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for getting back to us. (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for getting back to us. We are also glad that we still have a little time to run the additional results the reviewer wants to see and share more of our thoughts! Your comments and discussions are intrumental to our work. \n\nBefore we provide our detailed responses, we would like to use this opportunity to reiterate the overarching goal and contributions of this paper. Our primary objective is to introduce a new task-agnostic framework for evaluating the robustness-accuracy of pretrained representations and to inspire other task-agnostic benchmarking designs. This paper already encompasses a substantial amount of material (what is the metric, how can we use the metric to inform performance, how can we use the metric to select robust linear probing parameters), and we believe it is prudent to reserve some aspects for future exploration and development, including improving its rank efficiency (though SynBench already performs the best among all compared baselines).\n\n> what are we to make of e.g. Table 4.1 in the rebuttal, especially EfficientNet backbones\n\nWe want to draw to the reviewer's attention that EfficientNet backbones perform almost the worst on average (except ViT-B/32), and also on some tasks (e.g. Food101, CIFAR10, CIFAR100, SUN397, Caltech101, Flowers, FER2013, Country211, UCF101, Kinetics700). If we check their SynBench scores, we see, when $n=8192$, SynBench scores of EfficientNets are around 0.1 while ViT-B/32 is 0.01. By that, we think SynBench indeed informs the general performance.\n\n> baselinse with additional models\n\nThank you for confirming with us and we are glad that we are on the same page regarding the calculation. In the following table, we have added the baseline results to make Table 4.1 more complete and have also included the equivalent figure of Figure 4 of the paper [here](https://imgur.com/a/I6M2qz9). \n\n|n| Name | ViT-B/16 | ViT-L/16 | ViT-B/32 | Resnet50(2x)-SimCLRv2 | Resnet101(2x)-SimCLRv2 | ENet_b0   | ENet_b1   | ENet_b2   | ENet_b3   | Pearson correlation |\n|----------|----------|----------|----------|----------|-------------------|--------------------|------|------|------|------|---------------------|\n|| Accuracy (%) | 74.3     | 75.5     | 72.6     | 75.4              | 75.4               | 72.5 | 72.6 | 73.1 | 73.9 | 1.0                 |\n|n=2048| SynBench | 0.33     | 0.26     | 0.0      | 0.66              | 0.60               | 0.02 | 0.04 | 0    | 0    | **0.85**                |\n|| Val loss | 3.10 | 4.12 | 4.10 | 1.31 | 0.98 | 4.66 | 3.56 | 6.82 | 3.88 | -0.63\n|| MDL | 6820.76 | 8094.06 | 8198.55 | 5881.34 | 2882.36 | 8950.38 | 7654.88 | 15816.05 | 8138.87 | -0.53\n||LogME|-0.726 | -0.724 | -0.729 | 2.791 | 1.503 | -0.721 | -0.726 | -0.725 | -0.729 | 0.67\n||SFDA| 0.584 | 0.635 | 0.567 | 0.947 | 0.593 | 0.534 | 0.515 | 0.751 | 0.823 | 0.44\n|n=8192| SynBench   | 0.52     | 0.49     | 0.01     | 0.69              | 0.84               | 0.13 | 0.13 | 0.09 | 0.03 | **0.87**                |\n|| Val loss | 0.73 | 1.50 | 2.92 | 0.62 | 0.52 | 4.27 | 2.03 | 4.33 | 2.56 | -0.78\n|| MDL |9939.13 | 17672.6 | 23332.98 | 9646.09 | 5443.43 | 32511.61 |  19479.78 | 43202.85 | 25964.38 | -0.69\n||LogME| -0.710 | -0.707 | -0.727 | -0.599 | -0.622 | -0.714 | -0.719 | -0.721 | -0.725 | 0.71\n||SFDA| 0.525 | 0.531 | 0.513 | 0.581 | 0.543 | 0.510 | 0.505 | 0.524 | 0.525 | 0.78\n|n=32768| SynBench   | 0.59 | 0.58 | 0.02 | 0.81 | 0.87 | 0.19 | 0.19 | 0.17 | 0.09 | **0.88**\n|| Val loss |  0.68 | 0.79 | 3.91 | 0.53 | 0.51 | 1.11 | 0.79 | 2.60 | 1.11 | -0.58\n|| MDL |30848.99 | 38718.04 | 107960.49 | 22022.08 | 17166.0 | 56621.37 | 39158.90 | 109706.34 | 56621.37 | -0.67\n||LogME| -0.686  | -0.687  | -0.725  | -0.580  | -0.608 | -0.713 | -0.719 | -0.715 | -0.718 | 0.79 \n||SFDA|0.517 | 0.518 | 0.505 | 0.545 | 0.534 | 0.505 | 0.504 | 0.508 | 0.508 | 0.84 \n\nTable 4.1R The correlation between SynBench-score and the average accuracy on 27 real-life tasks.\n\n> GMMs and natural images\n\nWe propose to revise the sentence as\n\n*In an earlier work that discusses the connections between GMMs and natural image modeling, it has been demonstrated that \"Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images\". Our adoption of Gaussian mixtures for analysis is inspired by this finding, as the image patches in our synthetic data are also Gaussians. Additionally, the use of Gaussian is further supported by their capability to act as universal approximators of densities as well as by existing literature on Gaussian design.*\n\nWe are open to modifying or removing this sentence should the reviewer find it necessary."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698073562,
                "cdate": 1700698073562,
                "tmdate": 1700698402701,
                "mdate": 1700698402701,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "88yducsX4u",
            "forum": "9RLC0J2N9n",
            "replyto": "9RLC0J2N9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_4o5T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_4o5T"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a task-agnostic evaluation framework based on synthetic data to estimate how well pretrained representations transfer to downstream tasks, and how robust they are. Concretely, the paper proposes to generate data from a mixture of Gaussians, and to measure how well separable according to mixture correspondence this data is when embedded with a pretrained representation network, compared to how separable the data is in the input space. The paper derives a corresponding theory, proposes a benchmark score and numerically investigates how well this score correlates with the robustness and transferability of different representations to a variety of tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Better quality and more efficient evaluation methods are an important area of active research. Model robustness, while having been improved with increased model and data size, is still unsolved. The paper aims to address both these aspects. Further, further since the proposed method relies on synthetic data, it can avoid issues related to privacy and mitigate undesired biases."
                },
                "weaknesses": {
                    "value": "I generally found the paper rather hard to follow. It is often unclear if the authors are targeting adversarial robustness, or how well a representation transfers or both.\n\nI might well have missed a central point of the paper, but I fundamentally doubt that it is possible\n1. solely based on Gaussian mixtures with two components,\n2. without any knowledge about the target downstream tasks, \n\nto accurately predict classification performance of feature extractors across a broad range of downstream tasks. The baselines such as (Whitney et al., 2020) all rely on measures that are derived from the feature extractor and the downstream task/data, whereas the current method performs the predictions solely based on the feature extractor and synthetic data, while claiming to outperform the baselines. \n\nAnother aspect that I found surprising is that the theory does not depend on the properties of the pretrained feature extractor, for example its Lipschitz constant, which is usually the case in similar contexts.\n\nOverall, I cannot recommend acceptance at this point without further clarifications from the authors."
                },
                "questions": {
                    "value": "I could not find any individual results of the proposed method on the 27 tasks from (Radford et al.) (the correlation is plotted in Figure 4). I would be interested to see the SynthBench score per task and model, and how well it correlates with downstream performance per data set."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787681861,
            "cdate": 1698787681861,
            "tmdate": 1699636671544,
            "mdate": 1699636671544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HdwuRpTvIy",
                "forum": "9RLC0J2N9n",
                "replyto": "88yducsX4u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the comments and we wish our explanations can dispel concerns. (1/3)"
                    },
                    "comment": {
                        "value": "## 1. SynBench's target\nSynBench's target covers both adversarial robustness and representation seperability (accuracy). To be more precise, we believe a good pretrained representation network should produce representations that entail good seperability, either *with* or *without*, adversarial perturbations. Please see our paper section 1, the second paragraph, for motivations of our scope. SynBench-score as a metric is designed to serve this purpose and perform evaluations. By definition, SynBench-score is the relative AUC of expected bound-threshold accuracy curve (see equation 3). When SynBench is used to evaluate clean accuracy as in Section 4.2, we let $\\epsilon=0$, whearas when it is used to suggest robust linear probing parameters $\\epsilon$ as in Section 4.3, we let $\\epsilon=\\text{argmax}_\\epsilon \\text{SynBench-Score}$.\n\n## 2. How can SynBench predict classification performance across a broad range of tasks\nThink of how representation learning research typically evaluate a model for transfer learning - by running tests on broad range of downstream tasks. And the reason behind this is to see how the model behaves in different scenerios. To theorize things, we believe the general behavior of a pretrained representation is measured by how it perform on tasks of different difficulty levels. That is why we think a fundamental part of our design is to simulate tasks of different difficulty levels. One difference between SynBench and a traditional probing test is that, for example, we are using the classification problem of two highly overlapped Gassuain, instead of classifying ImageNet21k. We hope this clarification builds enough intuition to understand the following:\n1. We vary $s$ in equation 2 from 0.1 to 5 in increments of 0.1, which correspond to optimal accuracy (groundtruth difficulty) ranging from 55% to 100% and 50 difficulty levels. If we refer to this [figure](https://imgur.com/a/rH819Jm), we see each of the red points correspond to one of our similated trials with difficulty levels (x-axis).\n2. Baseline methods are task/data dependant, which means they are somewhat bound to tasks of that similar difficulty levels. If we refer to the same [figure](https://imgur.com/a/rH819Jm), it could be the single purple point with fixed level of difficulty.  \n3. If we include certain knowledge of possible downstream data properties, say locality of pixel dependencies, then the prediction will indeed be more accurate (see our section 4.4).\n\n## 3. Discussions: dependency on the properties of the pretrained feature extractor\nWe thank the reviewer for bringing this up. If possible, can the reviewer please share the literatures mentioned that use properties of the pretrained representations in similar contexts with us? In our theory, the reference optimal behavior is completely independant of the pretrained feature extractor (the denominator in Equation 3), and the evaluation depends on the feature extractor via the numerator. During this process, we do not explicitly use properties such as Lipschitzness. To our knowledge, the connection between Lipschitzness of the representation network and the downstream task accuracy via linear probing is not obvious."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359428099,
                "cdate": 1700359428099,
                "tmdate": 1700359910561,
                "mdate": 1700359910561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zcbr4QORyZ",
                "forum": "9RLC0J2N9n",
                "replyto": "88yducsX4u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the comments and we wish our explanations can dispel concerns. (2/3)"
                    },
                    "comment": {
                        "value": "## 4. Correlation breakdowns\nAs SynthBench score is not dependant on task, we gave the SynthBench score of each model in Table 7 in the paper. We calculate how SynthBench score correlates with downstream performance per data set in the following Table 3.1. \n\n| Datasets| Food101 | CIFAR10 | CIFAR100 | birdsnap | SUN397 | StanfordCars | Aircraft  |  VOC2007 |  DTD |  Pets | Caltech101 | Flowers | MNIST | FER2013 \n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |-------- | -------- | -------- | -------- | -------- | -------- | --------  \n| SynBench correlation | **0.01** | -0.30 | -0.50 | -0.33 | -0.32 | **0.90** | 0.87 | **0.64** | 0.86 | **0.40** | **0.09** | -0.64 | 0.56 | **0.81** |\n|Val loss | -0.31 | 0.07 | 0.24 | 0.03 | 0.03 | -0.82 | -0.70 | -0.80 | -0.66 | -0.63 | 0.02 | 0.37 | -0.33 | -0.85 | \n|MDL| -0.18 | **0.19** | **0.37** | **0.17** | **0.16** | -0.84 | -0.77 | -0.76 | -0.75 | -0.54 | -0.01 | **0.49** | -0.41 | -0.82 | \n|LogME| -0.48 | -0.70 | -0.83 | -0.74 | -0.74 | 0.85 | **0.95** | 0.22 | **0.98** | -0.13 | -0.01 | -0.92 | **0.85** | 0.55 | \n|SFDA| -0.41 | -0.66 | -0.77 | -0.67 | -0.69 | 0.88 | **0.95** | 0.24 | 0.96 | -0.07 | -0.07 | -0.87 | 0.84 | 0.60 | \n| Datasets| STL10  | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST |  ImageNet | AVG acc.|\n| SynBench correlation | -0.40 | 0.77 | 0.91 | 0.59 | 0.40 | **0.96** | **0.90** | **0.81** | **0.64** | 0.72 | -0.59 | 0.35 | **0.30** | **0.92** |\n|Val loss | 0.11 | -0.54 | -0.76 | -0.34 | -0.14 | -0.96 | -0.99 | -0.93 | -0.82 | -0.48 | 0.34 | -0.22 | -0.56 | -0.92 |\n|MDL| **0.23** | -0.64 | -0.82 | -0.43 | -0.25 | -0.97 | -0.96 | -0.87 | -0.74 | -0.59 | **0.47** | -0.32 | -0.45 | -0.91|\n|LogME| -0.80 | **0.97** | **0.96** | **0.85** | **0.81** | 0.69 | 0.59 | 0.45 | 0.17 | **0.97** | -0.88 | **0.41** | -0.22 | 0.72 |\n|SFDA| -0.75 | 0.93 | **0.96** | 0.82 | 0.77 | 0.70 | 0.64 | 0.51 | 0.24 | 0.94 | -0.83 | 0.34 | -0.15 | 0.77 |\n\nTable 3.1 The correlation between SynBench-score and individual downstream task.\n\n### **Subset of OOD tasks**\nWe further analyze SynBench score's correlation to the subset of OOD tasks. In the following Table 3.2, we computed the Frechet Inception Distance (FID) scores from ImageNet21k to the downstream tasks, and used them as the indicator of how OOD are the tasks. We then computed SynBench-score correlation with tasks that have FID scores larger than a threshold {50,100,150,200}. We do want to note that not all models in our analysis are pretrained with ImageNet21k; however, since ImageNet21k has become a go-to pretraining dataset, we assume samples therein are in-distribution.\n\nFrom table 3.3, we see that if we don't apply filter on FID (or equivelantly let threshold be 0), the initial correlation was 0.92. As we gradually increase the threshold to 50, 100, 150, and even 200, the correlation stays above 0.8, indeed suggesting SynBench's robustness to OOD tasks.\n\n| Datasets| Food101 | CIFAR10 | CIFAR100 | birdsnap | SUN397 | StanfordCars | Aircraft  |  VOC2007 |  DTD |  Pets | Caltech101 | Flowers | MNIST | FER2013 | \n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |-------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| FID to ImageNet21k| 100.81 | 115.47 | 96.22 | 102.39 | 54.78 | 154.81 | 206.47 | 52.30 | 98.37 | 104.15 | 53.51 | 112.64 | 301.28 |175.75 |  \n| Datasets| STL10  | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST |  ImageNet | \n| FID to ImageNet21k| 71.19 | 142.62| 104.80 | 156.81 | 163.92 | 36.72 | 235.63 | 79.40 | time out | 194.64 | 86.64 | 368.13 | 17.78 | \n\nTable 3.2 The Frechet Inception Distance (FID) scores from ImageNet21k to 27 downstream tasks.\n\n| FID| > 0 (all tasks)| > 50 | >100 | >150 | > 200 |\n|-------- |-------- |-------- |-------- |-------- | -------- | \n| SynBench Correlation | 0.92 | 0.93 | 0.93 | 0.82 | 0.92|\n\nTable 3.3 The correlation between SynBench-score and the average accuracy of FID-thresholded downstream tasks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359968463,
                "cdate": 1700359968463,
                "tmdate": 1700360049114,
                "mdate": 1700360049114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JtqnStmRoD",
                "forum": "9RLC0J2N9n",
                "replyto": "88yducsX4u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the comments and we wish our explanations can dispel concerns. (3/3)"
                    },
                    "comment": {
                        "value": "### **Subset of more challenging tasks**\nWe further analyze SynBench score's correlation to the subset of more challenging tasks. When we check how SynBench can serve as a performance metric of pretrained models, we used the average accuracy of 27 downstream tasks as the proxy of the general performance. Among the 27 tasks, there are indeed datasets that are large and complex, inclduing ImageNet. In the following Table 3.4, we highlight 3 subsets of tasks that represent more challenging datsets in different dimensions (number of classes, data types, task types). \n1. For datasets that have more than 100 classes (Food101, Birdsnap, SUN397, StanfordCars, Aircraft, Caltech101, Flowers, Country211, UCF101, Kinetics700, ImageNet), SynBench-score correlates with their average performance with correlation of 0.56, compared with the best baseline (SFDA) of 0.19.\n2. For video datasets (UCF101 and Kinetics 700), SynBench-score correlates with their acerage performance with correlation of 0.72, compared with the best baseline (SFDA) of 0.36.\n3. For the visual reasoning and question-answering dataset, CLEVR,, SynBench-score correlates with its performance with correlation of 0.72, while LogME and SFDA demonstrate even stronger correlation ($>0.9$).\n\nOverall, SynBench shows robust performance across these break-down groups.\n\n|  Large/complex datasets |  datasets w/ #classes>100 |  video datasets (UCF101 and Kinetics 700) |  visual reasoning/QA dataset | dataset average|   \n| -------- | -------- | -------- | -------- | -------- | \n| SynBench| **0.56** | **0.72** | 0.72 | **0.80** |\n|Val loss  |-0.75 | -0.88| -0.48 | -0.91 | \n|MDL | -0.66 | -0.81 | -0.59 | -0.85 |\n|LogME | 0.11 | 0.30 |**0.97** | 0.45|\n|SFDA | 0.19 | 0.36|0.94 | 0.51 |\n\nTable 3.4 The correlation between SynBench-score and subsets of downstream tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360122436,
                "cdate": 1700360122436,
                "tmdate": 1700360147962,
                "mdate": 1700360147962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tkSvWtbjun",
                "forum": "9RLC0J2N9n",
                "replyto": "88yducsX4u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on Our Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4o5T,\n\nWe would like to thank you again for your review! As the rebuttal/discussion phase is nearing its end, we want to check in to see if we have dispelled all your concerns, or if there are any additional points we can help clarify.\n\nWe understand that the discussion period is short, and your support and feedback are very important to us. We are truly grateful for your insights and the time you've dedicated to reviewing our work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588440801,
                "cdate": 1700588440801,
                "tmdate": 1700588440801,
                "mdate": 1700588440801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "153fpnqTKf",
                "forum": "9RLC0J2N9n",
                "replyto": "88yducsX4u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_4o5T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_4o5T"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their effort in providing a detailed rebuttal. Overall the provided rebuttal material confirmed my original opinion. Here are responses to some of the author's points in the rebuttal.\n\n**2. How can SynBench predict classification performance across a broad range of tasks**\n\nI stand by my original assessment that some information about the downstream tasks is required to make reasonable predictions about the performance of predictors on these tasks. A mixture of two Gaussians in pixel space is an extremely inaccurate model of natural images. As an insightful example the authors can compute the SynBench for the identity function (i.e. directly in pixel space, without any feature extractor) and compare its performance with features extracted with strong image classification models.\n\n**3. Dependency on the properties of the pretrained feature extractor**\n\nThe very first paper on adversarial examples (Szegedy et al., 2013) analyzes the stability of the network as a function of the Lipschitz constants of the individual network layers. Many follow-up works rely on similar properties (searching for \"Lipschitz\" in the 14.7k citing articles of (Szegedy et al., 2013) leads to 1.65k results on Google Scholar).\n\n**4. Correlation breakdowns**\n\nI thank the authors for providing the detailed breakdown. Unfortunately, this confirms that SynBench is not very predictive for the downstream performance across a broad range of tasks. The correlations for the individual data sets are very inconsistent: some are negative, some are positive, some are close to zero."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593524099,
                "cdate": 1700593524099,
                "tmdate": 1700593594103,
                "mdate": 1700593594103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rcZKyBcsga",
            "forum": "9RLC0J2N9n",
            "replyto": "9RLC0J2N9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_97TE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_97TE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SynBench, a method to evaluate the representations of pretrained models using synthetic data. The synthetic dataset is a class-conditional Gaussian in a binary classification setting. The proposed metric measures the accuracy and robustness on this constructed synthetic dataset (proxy task). SynBench-score is then defined as the ratio of area-under-curve between the representations obtained from pretrained models and the reference. The results on various image classification tasks demonstrate that SynBench-Score vastly outperforms baseline methods across wide range of supervised and self-supervised pretrained models. The paper also delves into the potential applications of this metric, discussing scenarios where it could be beneficial."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper provides a very comprehensive set of results with various backbones where SynBench-Score outperforms the baseline methods. The correlation of SynBench-Score is quite high even with limited number of samples.\n2. Practically, this can potentially be a very useful metric given that it does not require any real data. The motivation of the paper is well explained and the authors give various scenarios where this metric would be useful.\n3. Overall, this is mostly a complete paper with the authors discussing runtime analysis, limitations and the algorithm of SynBench."
                },
                "weaknesses": {
                    "value": "The authors only consider the linear probing paradigm in evaluation of pretrained models. In practice, finetuning is also a common way to use these pretrained models. It is not clear how this metric would perform in the finetuning setup."
                },
                "questions": {
                    "value": "1. What is the number of test samples in the synthetic dataset? I am not sure if I saw this in the paper.\n2. In the Algorithm (A.6), what are $z_1$ and $z_2$ in Line 12?\n3. It would be interesting to analyse the correlation on a subset of tasks instead of average of 27 tasks. For instance, it may be the case that metric performs better on OOD tasks compared to transfer learning on some datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6178/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6178/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6178/Reviewer_97TE"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798468037,
            "cdate": 1698798468037,
            "tmdate": 1699636671431,
            "mdate": 1699636671431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bIGyNskCyO",
                "forum": "9RLC0J2N9n",
                "replyto": "rcZKyBcsga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the positive comments and questions for us. We are happy to clarify the details and give additional explanations."
                    },
                    "comment": {
                        "value": "## 1. How SynBench would perform in the finetuning setup\nAs discussed in specified in Section 3.3, SynBench is designed to evaluate the pretrained representations parameterized by $\\theta$. In the scenerio when finetuning is performed, the parameters of the representation network will be updated, hence resulting in another pretrained representations (a separate model) from the perspective of SynBench. In our paper, we have shown how to use SynBench to compare representation networks before and after finetuning (Section 4.2, Comparing model attributes). In our example, ``ViT-B/16`` and ``ViT-B/16-in21k`` were both pretrained on ImageNet21k with supervision, whereas ``ViT-B/16`` is further finetuned on Imagenet 1k (name convention is adopted from [PyTorch Image Models](https://github.com/huggingface/pytorch-image-models)). The result shows the utility of SynBench in comparing original and finetuned models.\n\n## 2. The number of test samples in the synthetic dataset\nThere are 2048 test samples in the synthetic dataset. We have included this detail to the updated paper.\n\n## 3. $z_1$ and $z_2$ in Line 12 of Algorithm A.6\nThe subscript denotes the class label of the representation. That is, $z_1^{train,i}$ is the $i$-th training sample from class 1, and $z_2^{train,j}$ is the $j$-th training sample from class 2. We have included this detail to the updated paper.\n\n## 4. How does SynBench inform model performance on OOD tasks vs. others\nWe thank the reviewer for raising this question. In the following Table 2.1, we computed the Frechet Inception Distance (FID) scores from ImageNet21k to the downstream tasks, and used them as the indicator of how OOD are the tasks. We then computed SynBench-score correlation with tasks that have FID scores larger than a threshold {50,100,150,200}. We do want to note that not all models in our analysis are pretrained with ImageNet21k; however, since ImageNet21k has become a go-to pretraining dataset, we assume samples therein are in-distribution.\n\nFrom table 2.2, we see that if we don't apply filter on FID (or equivelantly let threshold be 0), the initial correlation was 0.92. As we gradually increase the threshold to 50, 100, 150, and even 200, the correlation stays above 0.8, indeed suggesting SynBench's robustness to OOD tasks.\n\n| Datasets| Food101 | CIFAR10 | CIFAR100 | birdsnap | SUN397 | StanfordCars | Aircraft  |  VOC2007 |  DTD |  Pets | Caltech101 | Flowers | MNIST | FER2013 | \n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |-------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| FID to ImageNet21k| 100.81 | 115.47 | 96.22 | 102.39 | 54.78 | 154.81 | 206.47 | 52.30 | 98.37 | 104.15 | 53.51 | 112.64 | 301.28 |175.75 |  \n| Datasets| STL10  | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST |  ImageNet | \n| FID to ImageNet21k| 71.19 | 142.62| 104.80 | 156.81 | 163.92 | 36.72 | 235.63 | 79.40 | time out | 194.64 | 86.64 | 368.13 | 17.78 | \n\nTable 2.1 The Frechet Inception Distance (FID) scores from ImageNet21k to 27 downstream tasks.\n\n| FID| > 0 (all tasks)| > 50 | >100 | >150 | > 200 |\n|-------- |-------- |-------- |-------- |-------- | -------- | \n| SynBench Correlation | 0.92 | 0.93 | 0.93 | 0.82 | 0.92|\n\nTable 2.2 The correlation between SynBench-score and the average accuracy of FID-thresholded downstream tasks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359148299,
                "cdate": 1700359148299,
                "tmdate": 1700359148299,
                "mdate": 1700359148299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tsj2L42CdW",
                "forum": "9RLC0J2N9n",
                "replyto": "rcZKyBcsga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on Our Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer 97TE,\n\nWe would like to thank you again for your review! As the rebuttal/discussion phase is nearing its end, we want to check in to see if we have dispelled all your concerns, or if there are any additional points we can help clarify.\n\nWe understand that the discussion period is short, and your support and feedback are very important to us. We are truly grateful for your insights and the time you've dedicated to reviewing our work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588517598,
                "cdate": 1700588517598,
                "tmdate": 1700588517598,
                "mdate": 1700588517598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fvt0ERnOI4",
                "forum": "9RLC0J2N9n",
                "replyto": "tsj2L42CdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_97TE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_97TE"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "I thank the reviewer for the response. Table 2.1 and Table 2.2 discussed in the rebuttal is certainly interesting. Having looked at the valid concerns raised by other reviewers, I'll maintain my score for now."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684415252,
                "cdate": 1700684415252,
                "tmdate": 1700684415252,
                "mdate": 1700684415252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "swKkuFZ2MJ",
            "forum": "9RLC0J2N9n",
            "replyto": "9RLC0J2N9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_S4rE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6178/Reviewer_S4rE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SynBench, a task-agnostic framework designed for the evaluation of pre-trained representations using synthesized data derived from data prior. Notably, SynBench is independent of downstream image classification datasets or tasks. The experimental results demonstrate a strong correlation between SynBench scores and the model's performance as assessed through measures of adversarial robustness and standard accuracy. Additionally, SynBench proves to be helpful for guiding the design and selection of hyperparameters in robust linear probing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It is interesting to design a proxy task for the quality evaluation of pre-trained representations. This approach offers a fresh perspective on assessing the quality of representations without relying on downstream datasets. \n\n- Experiments show the effectiveness of SynBench-Scores in indicating real-life task accuracy.\n\n- The paper demonstrates a high level of quality in its methodology and experimental design."
                },
                "weaknesses": {
                    "value": "- Robustness to Deviating Distributions \n\n  It would be valuable to assess SynBench's robustness when facing uncommon real-world data distributions. For instance, applying SynBench to datasets like DomainNet, which contains diverse and domain-shifted data, can provide insights into its adaptability to varying data sources and distributions. Demonstrating SynBench's effectiveness under such conditions would strengthen its applicability and reliability.\n\n\n- Scalability to Tasks with More Classes\n\n   The paper primarily uses datasets with limited categories for experiments. It's important to explore how SynBench performs on tasks with a more extensive range of classes, such as ImageNet.  It would help understand the framework's scalability and whether it maintains its effectiveness when applied to larger and more complex datasets. This expansion of experiments can provide a clearer picture of SynBench's utility across diverse tasks."
                },
                "questions": {
                    "value": "see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847144301,
            "cdate": 1698847144301,
            "tmdate": 1699636671318,
            "mdate": 1699636671318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zC98S0C2Qx",
                "forum": "9RLC0J2N9n",
                "replyto": "swKkuFZ2MJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer for the positive comments and questions for us. We hope our additional analysis completes the picture."
                    },
                    "comment": {
                        "value": "## 1. How does SynBench inform model performance on OOD tasks vs. others\nWe agree that showing SynBench's robustness to OOD tasks will strengthen the applicability and reliability. In the following Table 1.1, we computed the Frechet Inception Distance (FID) scores from ImageNet21k to the downstream tasks, and used them as the indicator of how OOD are the tasks. We then computed SynBench-score correlation with tasks that have FID scores larger than a threshold {50,100,150,200}. We do want to note that not all models in our analysis are pretrained with ImageNet21k; however, since ImageNet21k has become a go-to pretraining dataset, we assume samples therein are in-distribution.\n\nFrom table 1.2, we see that if we don't apply filter on FID (or equivelantly let threshold be 0), the initial correlation was 0.92. As we gradually increase the threshold to 50, 100, 150, and even 200, the correlation stays above 0.8, indeed suggesting SynBench's robustness to OOD tasks.\n\n| Datasets| Food101 | CIFAR10 | CIFAR100 | birdsnap | SUN397 | StanfordCars | Aircraft  |  VOC2007 |  DTD |  Pets | Caltech101 | Flowers | MNIST | FER2013 | \n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |-------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| FID to ImageNet21k| 100.81 | 115.47 | 96.22 | 102.39 | 54.78 | 154.81 | 206.47 | 52.30 | 98.37 | 104.15 | 53.51 | 112.64 | 301.28 |175.75 |  \n| Datasets| STL10  | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST |  ImageNet | \n| FID to ImageNet21k| 71.19 | 142.62| 104.80 | 156.81 | 163.92 | 36.72 | 235.63 | 79.40 | time out | 194.64 | 86.64 | 368.13 | 17.78 | \n\nTable 1.1 The Frechet Inception Distance (FID) scores from ImageNet21k to 27 downstream tasks.\n\n| FID| > 0 (all tasks)| > 50 | >100 | >150 | > 200 |\n|-------- |-------- |-------- |-------- |-------- | -------- | \n| SynBench Correlation | 0.92 | 0.93 | 0.93 | 0.82 | 0.92|\n\nTable 1.2 The correlation between SynBench-score and the average accuracy of FID-thresholded downstream tasks.\n\n## 2. Whether SynBench maintains its effectiveness when applied to larger and more complex datasets\nWhen we check how SynBench can serve as a performance metric of pretrained models, we used the average accuracy of 27 downstream tasks as the proxy of the general performance. Among the 27 tasks, there are indeed datasets that are large and complex, inclduing ImageNet. In the following Table 1.3, we highlight 3 subsets of tasks that represent more challenging datsets in different dimensions (number of classes, data types, task types). \n1. For datasets that have more than 100 classes (Food101, Birdsnap, SUN397, StanfordCars, Aircraft, Caltech101, Flowers, Country211, UCF101, Kinetics700, ImageNet), SynBench-score correlates with their average performance with correlation of 0.56, compared with the best baseline (SFDA) of 0.19.\n2. For video datasets (UCF101 and Kinetics 700), SynBench-score correlates with their acerage performance with correlation of 0.72, compared with the best baseline (SFDA) of 0.36.\n3. For the visual reasoning and question-answering dataset, CLEVR,, SynBench-score correlates with its performance with correlation of 0.72, while LogME and SFDA demonstrate even stronger correlation ($>0.9$).\n\nOverall, SynBench shows robust performance across these break-down groups.\n\n|  Large/complex datasets |  datasets w/ #classes>100 |  video datasets (UCF101 and Kinetics 700) |  visual reasoning/QA dataset | dataset average|   \n| -------- | -------- | -------- | -------- | -------- | \n| SynBench| **0.56** | **0.72** | 0.72 | **0.80** |\n|Val loss  |-0.75 | -0.88| -0.48 | -0.91 | \n|MDL | -0.66 | -0.81 | -0.59 | -0.85 |\n|LogME | 0.11 | 0.30 |**0.97** | 0.45|\n|SFDA | 0.19 | 0.36|0.94 | 0.51 |\n\nTable 1.3 The correlation between SynBench-score and subsets of downstream tasks."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358593260,
                "cdate": 1700358593260,
                "tmdate": 1700358869185,
                "mdate": 1700358869185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "73XxF6McDG",
                "forum": "9RLC0J2N9n",
                "replyto": "swKkuFZ2MJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on Our Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer S4rE,\n\nWe would like to thank you again for your review! As the rebuttal/discussion phase is nearing its end, we want to check in to see if we have dispelled all your concerns, or if there are any additional points we can help clarify.\n\nWe understand that the discussion period is short, and your support and feedback are very important to us. We are truly grateful for your insights and the time you've dedicated to reviewing our work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588566559,
                "cdate": 1700588566559,
                "tmdate": 1700588566559,
                "mdate": 1700588566559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3blrRlArHy",
                "forum": "9RLC0J2N9n",
                "replyto": "zC98S0C2Qx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_S4rE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6178/Reviewer_S4rE"
                ],
                "content": {
                    "comment": {
                        "value": "After checking the rebuttals, most of my concerns are solved. I recommend including the additional experiments and discussions during the rebuttal in the revised paper."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717127476,
                "cdate": 1700717127476,
                "tmdate": 1700717127476,
                "mdate": 1700717127476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]