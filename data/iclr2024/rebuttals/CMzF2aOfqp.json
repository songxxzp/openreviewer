[
    {
        "title": "Early Stopping Against Label Noise Without Validation Data"
    },
    {
        "review": {
            "id": "bprylPlM8U",
            "forum": "CMzF2aOfqp",
            "replyto": "CMzF2aOfqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
            ],
            "content": {
                "summary": {
                    "value": "Early stopping is one of the most prevalent approaches to select model. However, it requires additional validation data. This paper proposes a new method to early stop without using the validation data empirically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It does not require additional validation data for model selection.\n- Experiments on various structures and settings."
                },
                "weaknesses": {
                    "value": "- What authors proposed is only supported by the empirical result.\n- Following the question 2, more discussions may be needed for the related previous researches.\n- Since the metic is moveing average over k epochs, sensitivity analysis over k is needed."
                },
                "questions": {
                    "value": "- It may not work well for extreme cases, e.g. class imbalanced dataset. Any solution for those settings? If not, some assumptions could be specified for the setting.\n- What is the difference between the previouse studies'[1,2] findings and what authors propose in section 3.2 (fitting\nmislabeled examples impairs the overall model\u2019s fitting performance)?\n- Will PC monotonically decrease before its local minima? In other words, are there no fluctuations? or should we need some threshold?\n- How will this pattern change when utilized with additional regularizations e.g. data augmentation? Will it be consistent or will it flutuate before it goes to local minima?\n- For Table 1 and Table 2, which algorithm is utilized (just Cross Entropy?)? If utilized with several algorithm managing noisy labels, how much different between best and label wave?\n- Want to see result on more noise condition for table 3 and 4.\n- How about on real noise, e.g. Clothing1M?\n- Will this criterion fit to another task, e.g. semantic segmentation?\n\n[1] Wei, J., Liu, H., Liu, T., Niu, G., Sugiyama, M., & Liu, Y. (2022, June). To Smooth or Not? When Label Smoothing Meets Noisy Labels. In International Conference on Machine Learning (pp. 23589-23614). PMLR.\n\n[2] Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., & Liu, Y. (2020, October). Learning with Instance-Dependent Label Noise: A Sample Sieve Approach. In International Conference on Learning Representations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG",
                        "ICLR.cc/2024/Conference/Submission32/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697548541379,
            "cdate": 1697548541379,
            "tmdate": 1700726513457,
            "mdate": 1700726513457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w9frs62wdr",
                "forum": "CMzF2aOfqp",
                "replyto": "bprylPlM8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D1rG (Part1)"
                    },
                    "comment": {
                        "value": "We answer the questions one by one as follows. We are really open to further discussion.\n\n\n**Question.1 - Class imbalanced dataset**\nWe have tested the performance of our method on class imbalanced datasets, by setting the Imbalance Factor to 0.1 and the Noise Ratio (Sym.) to 0.4, with other settings consistent with those described in Section 4.1. Our experiments were conducted using our method in Cross-Entropy (CE) and class imbalanced method LDAM [1].\n\n| Methods   | Label Wave | Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |      55.31\u00b10.86%      |       56.03\u00b10.55%         |\n| LDAM   |      61.07\u00b10.88%      |      61.80\u00b10.93%         |\n\nThe experimental results affirm that our method can effectively identify early stopping points when learning from class imbalanced noisy datasets. While we emphasize the effectiveness of the Label Wave method in such scenarios, we acknowledge, as your intuition, that it may not perform equally well in more extreme cases. For example, when applied to the CIFAR-100 training set with an Imbalance Factor of 0.01 and a Sym. Noise Ratio of 0.4, the model's test accuracy exhibited significant fluctuations, leading to the ineffectiveness of the Label Wave method. We wish to clarify that our method is intended to identify appropriate early stopping points; thus, scenarios where early stopping itself is not applicable fall outside the scope of our research.\n\n[1] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.\n\n**Question.2 - Difference between the previous studies**\nOur research, along with the findings of [1, 2], aims to deepen the understanding of how a model's performance is impacted by fitting incorrectly labeled examples. However, we believe that these studies contribute to this understanding in different ways.\n\nBoth [1] and [2] have introduced new methods to handle noisy labels based on their findings, and thus have made notable contributions to the field. Specifically, [1] discusses the effects of Label Smoothing (LS) and Negative Label Smoothing (NLS) in the context of label noise, mainly focusing on how to adjust label smoothing strategies. [2] tackles instance-dependent label noise, and concentrates on effectively sieving out wrongly labeled instances from the training data.\n\nWhile [1] and [2] have significantly contributed to the field by exploring how noisy labels affect a model's generalization performance; our research distinctly points out how fitting noisy labels can impact the model's fitting performance on the training set. In Section 3.2, we delve deeper into the specific effects of fitting mislabeled examples on the overall fitting performance on the training set. Our study, by designing metrics called ''stability'' and ''variability'', discusses how the model's fitting performance on the training set is influenced by fitting incorrectly labeled examples as the training progresses. Moreover, we highlight a transitional phase, which we term ''learning confusion patterns''. We believe that this perspective has not been fully explored in previous studies.\n\n[1] Wei, J., Liu, H., Liu, T., Niu, G., Sugiyama, M., & Liu, Y. (2022, June). To Smooth or Not? When Label Smoothing Meets Noisy Labels. In International Conference on Machine Learning.\n\n[2] Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., & Liu, Y. (2020, October). Learning with Instance-Dependent Label Noise: A Sample Sieve Approach. In International Conference on Learning Representations."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295058118,
                "cdate": 1700295058118,
                "tmdate": 1700295058118,
                "mdate": 1700295058118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4dOPuiD8sg",
                "forum": "CMzF2aOfqp",
                "replyto": "bprylPlM8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D1rG (Part2)"
                    },
                    "comment": {
                        "value": "**Question 3 - PC Fluctuations & Sensitivity Analysis over k**\nHere, we address both Weakness 3 and Question 3. Based on your constructive suggestions, we have conducted additional analysis.\n\n1. Monotonicity of PC Values:\nYou inquired about the fluctuations in PC values and suggested setting some thresholds. We fully endorse your suggestion that applying a specific threshold to label wave in practice would further enhance the robustness of our method. In our existing experiments, we have found that Moving Averages and \"Patience\" are sufficient to accurately help us identity the appropriate early stopping point through PC. Therefore, to ensure the simplicity and straightforward of our proposed method, we did not add an additional threshold in this paper.\n\n2. Sensitivity Analysis of k Values:\nIn response to your suggestion, we conducted a sensitivity analysis on the k value for Moving Averages. Specifically, we analyzed the correlation between the PC after applying different k values of moving averages and test accuracy:\n\n| k   | Pearson Correlation Coefficient |\n|-----------|-----------------------|\n| 1        |      -0.8565      |\n| 2        |      -0.9435      | \n| 3        |      -0.9637      |\n| 5        |      -0.9367      |\n| 10        |      -0.9406     |\n\n(Experiment take the same settings as Sec. 4.1 and Appendix B)\n\nWe observed that as the k value changes, the Pearson correlation coefficient between the moving average of PC values and test accuracy exhibits some fluctuations but overall maintains a very strong negative correlation. This suggests that while considering the setting of k values is necessary, different k values do not significantly alter the relationship between the moving average of PC values and test accuracy. We will provide detailed sensitivity analysis charts in the further revision.\n\n\n\n**Question.4 - Additional regularizations**\nBased on the reviewers' detailed comments, we tested the effectiveness of our Label Wave method with additional regularization methods. It is worth noting that our proposed Label Wave method itself includes BN and Data Augmentation, i.e., Label Wave + B, D as described below. Beyond the reviewer's comments, we have also tested the setting where these two regularization methods are not included. These regularization methods include:\n\n*A. Mixup\nB. BN\nC. Dropout\nD. Data Augmentation.*\n(All experiments take the same settings as Sec. 4.1)\n\n| Methods   | Label Wave | Global Maximum |\n|-----------|-----------------------|---------------------------|\n| Label Wave        |       66.79\u00b10.39%      |    67.15\u00b10.49%  |\n| Label Wave + B, D |  81.61\u00b10.44%    |    81.76\u00b10.30%    |\n| Label Wave + B, C, D     |   83.57\u00b10.24%    |   83.77\u00b10.32%    |\n| Label Wave + A, B, D      |   82.38\u00b10.56%      |     83.09\u00b10.22%      |\n| Label Wave + A, B, C, D  |83.67\u00b10.45%|  84.05\u00b10.35%     |\n\nThe experimental results show that adding or subtracting these regularization methods in the experimental setting does not affect the effectiveness of the label wave method for identifying early stopping points.\n\n\n**Question.5 - LNL algorithms**\nYes, Tables 1 and 2 evaluate the effectiveness of the label wave method when using Cross-Entropy Loss under various setting modifications. \nIn Section 4.2, Tables 3 and 4 examine a range of algorithms for managing noisy labels. The differences between the maximum test accuracy and the label wave outcomes are shown below:\n\n| Methods   | CIFAR10 - Label Wave | CIFAR10 - Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |        81.61\u00b10.44%               |81.76\u00b10.30% |\n| Taylor-CE |        85.06\u00b10.30%               |85.43\u00b10.37%     |\n| ELR       |        90.45\u00b10.52%               |90.76\u00b10.70% |\n| CDR       |        87.69\u00b10.10%               |87.80\u00b10.24% |\n| CORES     |        87.74\u00b10.13%               |87.95\u00b10.21% |\n| NLS       |        83.45\u00b10.19%               |83.62\u00b10.37%     |\n| SOP       |        88.42\u00b10.38%               |88.82\u00b10.46%     |\n\n\n| Methods   | CIFAR100 - Label Wave | CIFAR100 - Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |        50.96\u00b10.30%    |  51.05\u00b10.33%   |\n| Taylor-CE |        57.64\u00b10.28%    |  57.99\u00b10.30%   |\n| ELR       |        65.36\u00b10.39%    |  66.33\u00b10.93%   |\n| CDR       |        63.34\u00b10.15%    |  63.54\u00b10.28%   |\n| CORES     |        45.03\u00b10.38%    |  45.75\u00b10.27%   |\n| NLS       |        58.05\u00b10.15%    |  58.32\u00b10.35%   |\n| SOP       |        68.53\u00b10.30%    |  68.78\u00b10.27%   |\n\n\n**Question.6 - More noise condition for table 3 and 4**\nDue to computational resource constraints, we will provide results with more noise condition for Tables 3 and 4 in subsequent Comments and further revision."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295109279,
                "cdate": 1700295109279,
                "tmdate": 1700295109279,
                "mdate": 1700295109279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w9BOruhhon",
                "forum": "CMzF2aOfqp",
                "replyto": "bprylPlM8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D1rG (Part3)"
                    },
                    "comment": {
                        "value": "**Question.7 - Real noise**\nWe respectfully wish to highlight that our testing on the CIFAR-10N [1] dataset involved human-annotated real-world noise, showcasing the Label Wave method's effectiveness in real-world scenarios. Here, we present the outcomes of applying our label wave method on the Clothing1M dataset. \n\n| Methods   | Label Wave | Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |      70.12\u00b10.34%      |       70.56\u00b10.11%         |\n\n[1] Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., & Liu, Y. (2021, October). Learning with noisy labels revisited: A study using real-world human annotations. In International Conference on Learning Representations.\n\n\n**Question.8 - Semantic segmentation**\nConsidering that our aim is to demonstrate that the concept of \"learning confusion patterns\" helps detect the transitional point in learning with noisy labels, we did not consider tasks like semantic segmentation and focused solely on classification tasks.\n\n\nWe extend our sincere thanks for your patience in reviewing our work and deeply appreciate your informative comments towards improving our manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295125984,
                "cdate": 1700295125984,
                "tmdate": 1700295125984,
                "mdate": 1700295125984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tleG7K8qFk",
                "forum": "CMzF2aOfqp",
                "replyto": "bprylPlM8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank authors for their sincere efforts to relieve my worries. I think this research has some interesing ideas, but I still have concerns as follows:\n\n- Since this strategy cannot be theoretically supported, many experimental results would be required.\ne.g. Which type of CIFAR10N the authors experimented? All 5 types? If so, results of all types should be reported. Although I agree the noisy label of CIFAR10N is realistic, since the image itself is 32$\\times$32 CIFAR, it may be limited to represent more realistic situations.\n- I also like the result reporting type as Table 3 and 4. yet, results with more diverse settings of noise condition will be more convincing.\n- I think results with clean dataset will make this study more convincing.\n\nTherefore, currently I will keep my score as before."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483443388,
                "cdate": 1700483443388,
                "tmdate": 1700555981423,
                "mdate": 1700555981423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bMKpxLbkob",
                "forum": "CMzF2aOfqp",
                "replyto": "bprylPlM8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their sincere efforts to address my concerns. With the inclusion of several additional experimental results, I am inclined to increase my score by +1 (as a result, 6). However, I would like to clarify that my acceptance of this paper is contingent upon the inclusion of the total experiments. As the current manuscript, without any modifications, falls below the acceptance threshold based on my previous scoring, I look forward to seeing the complete set of experiments in the revised version."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726495044,
                "cdate": 1700726495044,
                "tmdate": 1700726556022,
                "mdate": 1700726556022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g5N1fHyRD5",
            "forum": "CMzF2aOfqp",
            "replyto": "CMzF2aOfqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an interesting method to perform early stopping in case of label noise without needing a validation set. The writing is good and the method provides some insights. However, there are concerns on the applicability of the method and the experiment set up is insufficient to evaluate the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method is interesting and has some insights.\n2. The paper is well written, and everything is presented nicely. \n3. Good results are shown on certain noise datasets."
                },
                "weaknesses": {
                    "value": "1. My biggest concern is the applicability of the method. Currently it\u2019s not clear the method works well on what scenarios. \n2. Experiment evaluation is far from sufficient and the setup can be improved."
                },
                "questions": {
                    "value": "1. Regarding applicability, my intuition is that the method only works well when there is \u201csignificant\u201d amount of \u201crandom\u201d label noise in the training set. \u201cRandom\u201d is because the method relies on that the model fits simple patterns first and then learn random patterns from the noise. What if the label noise also only include simple patterns? E.g. black donkeys are mostly labeled as horses? The method also requires significant amount of label noise so that the model prediction fluctuate to a degree to be detected by the method. This is also reflected by the fact that experiments only considers >20% noise.  Can the authors provide more insights into this through discussion or experiments?\n2. Only datasets with synthetic noise are considered. How does the method work on real datasets with real-world noise?\n3. The baselines seem to use a noisy validation set, and evaluation is done on a clean test set. It makes more sense split the clean test set to create a clean validation set or to create a clean validation set from the training set. This is because we want to ensure the validation set has a same distribution as the test set, i.e. to be clean.\n4. What happens if the amount of label noise is less than 20%? Including label noise from 0-20% can help better understand the method.\n5. In the motivation of not using a validation set is that using a validation set reduces training set size and thus decreases performce, by this motivation, the method is targeting domains with limited amount of training set. How small the dataset size should be when it\u2019s preferred to consider this method? Could you provide some analysis on this?\n6. In order to show the method\u2019s effectiveness, more datasets from diverse domains should be considered."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "see questions"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698335332055,
            "cdate": 1698335332055,
            "tmdate": 1700605128175,
            "mdate": 1700605128175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "28nBlO8S3W",
                "forum": "CMzF2aOfqp",
                "replyto": "g5N1fHyRD5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R2TX (Part1)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for your detailed and insightful review. We are glad that you find the paper is interesting and has insights. \nBelow we address the questions you\u2019ve raised about the Label Wave method, and we remain open to further discussion if there are any additional comments on these questions.\n\n**Response to Question 1 and Question 4 (Applicability of the Label Wave method)**\n\nWe appreciate the insightful concerns you have expressed about the applicability of the Label Wave method, particularly in scenarios with different types and amounts of label noise. We will detailedly address these concerns below in Part A - Types of Label Noise, and Part B - Amounts of Label Noise.\n\n*Part A: Types of Label Noise*\n\n1. Broad Applicability Across Noise Types: Our method is effective not only in the presence of random label noise but also in more complex scenarios. We have conducted extensive experiments to prove our method's applicability to both symmetric noise and instance-dependent label noise [1], as well as in real-world environments with label noise [2, 3] (refer to Section 4.1, Tables 1 and 2). These experiments demonstrate that our method is capable of effectively identifying early stopping points across various types of label noise studied in the community of learning with noisy labels.\n2. Limitations in Specific Noise Types: While we highlight the effectiveness of the Label Wave method, it is consistent with your intuition that our method may not work equally well across all types of label noise. Recent research [4] highlights a type of noise known as subclass-dominant label noise, where early stopping has proven to be ineffective. We wish to clarify that our method is designed to identify appropriate early stopping points; hence, scenarios where early stopping itself is inapplicable fall outside the scope of our research.\n\n*Part B: Amounts of Label Noise*\n\n1. Effectiveness in Low Noise Scenarios: We would like to emphasize that our method remains effective in training sets with noise levels below 20%. Our experiments show that the method works well even with a noise rate as low as 10%. The choice to experiment with noise rates fixed between 20%-80% was based on the common range of dataset noise discussed in the research of learning with noisy labels.\n2. Limitations at Very Low Noise Levels: However, we recognize that the concerns raised about the method's effectiveness at extremely low noise levels are valid. Our primary goal in this paper is to demonstrate how the concept of \"learning confusion patterns\" aids in detecting the transitional point in learning with noisy labels. To maintain simplicity, intuitiveness, and computational efficiency, we used the variability metric explored in Section 3.1, termed as prediction changes or PC (Section 3.3). Designing metrics that track the fluctuations of the model\u2019s fitting performance on a broader range can be an effective way to improve our work. One initial intuition is to discuss the degree of fluctuations in prediction changes, which could help identify the early stopping point even when there is no obvious first local minimum in prediction changes.\n3. 0% Label Noise: We have noted the suggestion to discuss scenarios where label noise is zero. However, as discussed in our paper, our method is not applicable to typical clean training sets. In cases of completely clean datasets, modern deep neural networks often exhibit benign overfitting [5, 6], making early stopping unnecessary. Scenarios where early stopping itself is inapplicable are beyond the scope of our method's intended use.\n\n[1] Part-dependent label noise: Towards instance-dependent label noise, NeurIPS 2020.\n[2] Learning with noisy labels revisited: A study using real-world human annotations, ICLR 2022.\n[3] Learning from Massive Noisy Labeled Data for Image Classification, CVPR 2015.\n[4] Subclass-Dominant Label Noise: A Counterexample for the Success of Early Stopping, NeurIPS 2023.\n[5] Benign overfitting in linear regression, PNAS 2020.\n[6] Benign overfitting in classification: Provably counter label noise with larger models, ICLR 2023.\n\n\n**Response to Question 2 and Question 6 (More datasets & Real-world noise)**\nWe respectfully wish to highlight that our testing on the CIFAR-10N dataset involved human-annotated real-world noise, showcasing the Label Wave method's effectiveness in real-world scenarios. Furthermore, our experiments conducted on the NEWS and Tiny-ImageNet datasets have demonstrated our method's broad applicability across various domains. Here, we present the outcomes of applying our Label Wave method on the Clothing1M dataset. We are committed to supplementing more results using real-world datasets in the further revision.\n\n| Methods   | Label Wave | Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |      70.12\u00b10.34%      |       70.56\u00b10.11%         |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294862772,
                "cdate": 1700294862772,
                "tmdate": 1700294862772,
                "mdate": 1700294862772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kdW3xXRfZJ",
                "forum": "CMzF2aOfqp",
                "replyto": "g5N1fHyRD5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R2TX (Part2)"
                    },
                    "comment": {
                        "value": "**Response to Question 3 (Validation set)**\nWe really appreciate your wonderful suggestion about using a clean validation set to align with a clean test set distribution. However, in the context of our research on learning with noisy labels, we have intentionally chosen to maintain consistency between the distributions of the validation and training sets. This decision is grounded in the following considerations:\n\n1. *Universality of Label Wave*: Our proposed method for identifying early stopping points using the Label Wave mthods aims to offer a universally applicable solution for learning with noisy labels. A prevalent assumption in this field, as reflected in numerous studies [7-10], is the presence of a training set with unreliable labels alongside a test set with reliable labels. In these cases, the training set is often divided into two parts: one for training and the other serving as a noisy validation set for early stopping. By adhering to this widely accepted setting, our method aligns with and can be directly compared with the method that utilizes a noisy validation set for identifying early stopping points.\n2. *Label Wave in Real-World Applications*: In many real-world applications, it is common to encounter datasets where all available data is unreliable or noisy. This means that we cannot obtain an additional clean validation set for identifying early stopping points. Therefore, we only compare methods that utilize a noisy validation set for identifying early stopping points, as this reflects a more realistic and practical situation.\n\nIn conclusion, we use a noisy validation set to ensure a fair and relevant evaluation of our method within the specific context of learning with noisy labels.\n\n[7] Early-learning regularization prevents memorization of noisy labels, NeurIPS 2020.\n[8] Robust Early-learning: Hindering the Memorization of Noisy Labels, ICLR 2021.\n[9] Mitigating Memorization of Noisy Labels by Clipping the Model Prediction, ICML 2023.\n[10] Late stopping: Avoiding confidently learning from mislabeled examples, ICCV2023. \n\n\n**Response to Question 5 (Training data size)**\nAlthough our method targets domains with limited training data, its fundamental principle of using all available data effectively applies universally when the training and validation sets have similar distributions. In such cases, incorporating the validation data into the training set can always be positive for the model performance, regardless of training data size or validation data split ratio.\n\n\nWe really appreciate your feedback as it significantly contributes to refining our research. We are committed to investigating these aspects to extend the applicability of the Label Wave method."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294908959,
                "cdate": 1700294908959,
                "tmdate": 1700294908959,
                "mdate": 1700294908959,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KiPzlcwbs3",
                "forum": "CMzF2aOfqp",
                "replyto": "kdW3xXRfZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "I thank the authors for clarifying the questions and also for the extra experiment on the Clothing 1M dataset. Most of my concerns are addressed but I have two extra questions:\n\n1. The authors claims that 20%-80% label noise is what people commonly do, but not citations are provided. Also, some recent work also considers 0%-20% label noise[1]. Even if this is commonly done, it doesn't mean this is the correct way to do. For example, the CIFAR-10N dataset, using CIFAR-10N-aggregate or  CIFAR-10N-random (both have <20% label noise) is more realistic than CIFAR-10N-worse. I appreciate that the proposed method works well for heavy noise, but I do believe showing how it behaves in case of less noise is also important. I believe even if the method doesn't work well for less noise it doesn't undermine the paper's contribution, but instead the fact of presenting the results provides a more comprehensive view of the method.\n\n2. The authors claim that for different training dataset size, incorporating the validation data into the training set can always be positive. When the training dataset size is very large, one can just use a fixed amount e.g. 10K data points for validation purpose. Since the training dataset size is large, including these extra 10k data makes little difference, but using the 10K data points can be very helpful for selecting the best epoch to prevent overfitting. I can imagine the proposed method is not needed if the dataset size is super large. The question here is that we want to figure out in what dataset size region, the method is useful and better than simply using a validation set. This is crucial for understanding the applicability of the method. \n\n[1] Wei, Jiaheng, et al. \"Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations.\" International Conference on Learning Representations. 2022."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413799261,
                "cdate": 1700413799261,
                "tmdate": 1700413799261,
                "mdate": 1700413799261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U0evzwFJVN",
                "forum": "CMzF2aOfqp",
                "replyto": "ldrja8AQ2Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my questions. The extra experiments and explanation address my concern. I will increase my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605097217,
                "cdate": 1700605097217,
                "tmdate": 1700605097217,
                "mdate": 1700605097217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "daVtRF6F72",
            "forum": "CMzF2aOfqp",
            "replyto": "CMzF2aOfqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an important topic of learning against noisy labels. Specifically, the authors mainly focus on how to automatical detect the transitioning point for early stopping, i.e., from fitting to clean to fitting to noise. There are two proposed key metrics so-called \"stability\" and \"variability\", and and the method uses \"prediction change\" as the mean of detecting early-stop point. The results show that the method can detect the point accurately by showing the test accuracy difference compared to that obtained from the global maximum point."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I felt there are multiple strengths of this work.\n* n-depth analysis: This work provides phase 1 to phase 3 analysis to understand how the DNN learns knowledge from noisy data.\n* Well-defined metric: Several useful metrics are proposed: prediction changes, stability, and variability.\n* The paper is well organized and easy to read."
                },
                "weaknesses": {
                    "value": "There are several weaknesses in this work, which may be useful to polish the paper.\n* **Missing important reference:** I know references that are highly related to this work but unfortunately not mentioned and compared. These two papers also tackled exactly the same point and mentioned similar intuitions on what the best early stopping point is. These papers are worth mentioning and comparing. \n[1] How does early stopping help generalization against label noise, arXiv 2019\n[2] Robust learning by self-transition for handling noisy labels, KDD 2021\n\n* **Unclear setup for practicality:** Detecting an early stop point is very important in the industry, especially when using strong regularization techniques together. For example, in the computer vision domain, using Mixup (or Cutmix, etc), Batch Norm, Dropout, and other architecture-specific regularization (e.g., stochastic depth for Vision Transformers) is a must-need. These kinds of strong regularization obviously change the learning behavior of DNNs like the training and testing curves. For the complete study toward practical methods, these recipes of training should be considered altogether, or theoretical support is needed.\n\nThese two major issues contribute the most when determining my review score."
                },
                "questions": {
                    "value": "Please address the two major weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern was detected."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739707556,
            "cdate": 1698739707556,
            "tmdate": 1700552130852,
            "mdate": 1700552130852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wYtOupWVNB",
                "forum": "CMzF2aOfqp",
                "replyto": "daVtRF6F72",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LTbz"
                    },
                    "comment": {
                        "value": "We extend our heartfelt gratitude to the reviewer for your detailed constructive feedback, addressing both the content of our paper and the applicability of our proposed method. Below we address the comments you\u2019ve raised. We are very open to further discussion. \n\n**Weakness.1 - Related Work**\n\nIn revised manuscript, we have added the references [1] and [2] in Related Work.\n\n[1] and [2] conceptualize model learning with noisy labels as two stages: the first stage learns from clean examples, and the second stage overfits to mislabeled examples. By identifying the largest safe set in the first stage and focusing on learning from that set in the latter stage, these methods effectively avoid the model's overfitting to mislabeled examples. Notably, [2] provides a method to identify the point where MP(t) equals MR(t) without additional supervision, thereby determining an early stopping point. [1] and [2] indeed provide good insights into the study of learning with noisy labels from two perspectives: the analysis of learning stages and the provision of practical learning with noisy label methods.\n\nHere, we will emphasize the advantages of our proposed method from two aspects: the learning stages analysis and the model selection method.\n\n*Learning Stages:*\n\nWe introduce a novel stage between the stage dominated by learning simple patterns and overfitting mislabeled examples, named \"Learning Confused Patterns\". In this stage, the model begins to learn from mislabeled examples, leading to simultaneous declines in generalization and fitting performance.\n\n*Model Selection:*\n\nThe Label Wave method not only requires no additional supervision but also has the advantages of being simple and efficient:\n1. Simple Concept: It is based on the principle that the first local minimum in prediction changes marks the best early stopping point. Hence, the Label Wave method does not need to estimate the noise rate and is effective across a wide range of noise rates.\n2. Direct Application: It tracks prediction changes during the training process, allowing the model to halt before learning confusing patterns. This approach avoids the complexity and computational costs of statistical models like the GMM and EM algorithms used in [2] and [3].\n\n[1] How does early stopping help generalization against label noise, arXiv 2019 \n[2] Robust learning by self-transition for handling noisy labels, KDD 2021\n[3] Selc: self-ensemble label correction improves learning with noisy labels, IJCAI 2022\n\n**Weakness.2 - Strong Regularization Techniques**\n\nBased on the reviewers' detailed comments, we tested the effectiveness of our Label Wave method with additional regularization methods. It is worth noting that our proposed Label Wave method itself includes BN and Data Augmentation, i.e., Label Wave + B, D as described below. Beyond the reviewer's comments, we have also tested the setting where these two regularization methods are not included. These regularization methods include:\n\n*A. Mixup\nB. BN\nC. Dropout\nD. Data Augmentation.*\n(All experiments take the same settings as Sec. 4.1)\n\n| Methods   | Label Wave | Global Maximum |\n|-----------|-----------------------|---------------------------|\n| Label Wave        |       66.79\u00b10.39%      |    67.15\u00b10.49%  |\n| Label Wave + B, D |  81.61\u00b10.44%    |    81.76\u00b10.30%    |\n| Label Wave + B, C, D     |   83.57\u00b10.24%    |   83.77\u00b10.32%    |\n| Label Wave + A, B, D      |   82.38\u00b10.56%      |     83.09\u00b10.22%    |\n| Label Wave + A, B, C, D  |83.67\u00b10.45%|  84.05\u00b10.35%     |\n\n\nThe experimental results show that adding or subtracting these regularization methods in the experimental setting does not affect the effectiveness of the label wave method for identifying early stopping points.\n\nIn the end, we thank the reviewer for the two constructive comments mentioned above, and addressing them really improves our paper to a great extent."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294800432,
                "cdate": 1700294800432,
                "tmdate": 1700294800432,
                "mdate": 1700294800432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jVt4I94cd9",
                "forum": "CMzF2aOfqp",
                "replyto": "daVtRF6F72",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the response from my questions. I carefully read the provided explanation and additional experiments. However, I still have some ambiguity. \n\n**Weakness.1 - Related Work**\n\n[1,2] will act as strong competitors to identify the early stopping point. I agree with that the proposed approach has distinct merits over them, but I am not sure if the proposed method outperforms the two existing works. \n\n**Weakness.2 - Strong Regularization Techniques**\n\nIt was good to show effectiveness of the label wave even after applying data augmentation. But I wonder if the improvement still remains compared with Existing Label Noise Training without Label Wave. If strong augmentation are applied, the movement of test error will significantly change, e.g., the test error keeps going down over all training period. If this is the case, the early stopping is not required. \n\nI am also open to further discussion."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465420724,
                "cdate": 1700465420724,
                "tmdate": 1700465463093,
                "mdate": 1700465463093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KTYieumI1Z",
                "forum": "CMzF2aOfqp",
                "replyto": "daVtRF6F72",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
                ],
                "content": {
                    "title": {
                        "value": "Response."
                    },
                    "comment": {
                        "value": "Appreciate your hard working. The provided answer cleared my first concern, but still have concern for the second question. I saw many situations where validation and test errors are consistently going down even when there are noisy labels in training data (especially for real-world datasets, or when noise ratio is not significant). \n\nAssuming the test error is like Figure 1 sounds a bit strict. These test error movement trends may vary depending on the training setup (e.g. augmentation, optimizer, learning rate, architecture, etc.). I think it would be nice if the proposed method had more insights to add (or expand on) its advantages in all scenarios rather than simply mentioning something that is out of scope.\n\nI increased my score +1."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552619457,
                "cdate": 1700552619457,
                "tmdate": 1700552658715,
                "mdate": 1700552658715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QTioTnlsUJ",
            "forum": "CMzF2aOfqp",
            "replyto": "CMzF2aOfqp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission32/Reviewer_8gHt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission32/Reviewer_8gHt"
            ],
            "content": {
                "summary": {
                    "value": "In the face of label noise, this publication presented an early halting technique using the Label Wave approach. Although the finding in this publication is intriguing, it only makes a little contribution to label noise bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This publication presented learning perplexing patterns, a transitional stage in learning with noisy labels."
                },
                "weaknesses": {
                    "value": "The cifar10/100 dataset is utilized in this studies; however, real-world datasets such as webvision and food101 should be employed to confirm the efficacy of the suggested approach. I'm interested in seeing these outcomes.\n\nIn order to determine whether the suggested method chooses the best classifier, I would like to examine the maximum test accuracy during the training phase. \n\nGiven that the focus of this research is label noise, studies should compare the state-of-the-art techniques currently used for label noise learning, like DivideMix[1], ELR[2], AugDesc[3] and so on.\n\n[1] Li, Junnan, Richard Socher, and Steven CH Hoi. \"Dividemix: Learning with noisy labels as semi-supervised learning.\" arXiv\npreprint arXiv:2002.07394 (2020).\n\n[2] Liu, Sheng, et al. \"Early-learning regularization prevents memorization of noisy labels.\" Advances in neural information\nprocessing systems 33 (2020): 20331-20342.\n\n[3] Nishi, Kento, et al. \"Augmentation strategies for learning with noisy labels.\" Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2021."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770541747,
            "cdate": 1698770541747,
            "tmdate": 1699635926338,
            "mdate": 1699635926338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0T8w9WOXNi",
                "forum": "CMzF2aOfqp",
                "replyto": "QTioTnlsUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission32/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8gHt"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your comments. We are glad you point out that the \u2018learning confusion patterns\u2019 presented in our paper is intriguing. Below we address the concerns you\u2019ve raised, and we\u2019d be very open to further discussion.\n\n1. **SOTA semi-supervised LNL methods on real-world datasets.**\n\nWe would like to humbly point out that the CIFAR-10N dataset we have tested is a dataset labeled with human-annotated real-world noise. Besides, the experiments on NEWS and Tiny-ImageNet have demonstrated the applicability of our method on datasets from diverse domains. Meanwhile, our method is very simple and straightforward that can be used to strengthen existing methods, and thus cannot be directly compared with the state-of-the-art semi-supervised learning with noisy labels methods.\n\nTo address the common concerns of the reviewers, and in order to improve the credibility of our method, we tested the efficacy of our method applied to semi-supervised methods on real-world datasets.\n\nHere, we present the outcomes of applying our Label Wave method in state-of-the-art semi-supervised methods, on the Clothing1M dataset. Notably, we have already evaluated ELR [2] in our paper (i.e., in Tables 3 and 4 on Page 8). Therefore, our current focus is to apply our Label Wave method to the methods including CE (baseline), DivideMix [1], and AugDesc [3]. We are committed to supplementing more results using real-world datasets and semi-supervised methods in the further revision.\n \n\n| Methods   | Label Wave | Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |      70.12\u00b10.34%      |       70.56\u00b10.11%         |\n| DivideMix |      71.71\u00b10.50%      |       71.90\u00b10.36%         |\n| AugDesc   |      73.91\u00b10.60%      |       74.34\u00b10.58%         |\n\n\n2. **Maximum test accuracy during the training phase.**\n\nWe would like to humbly point out that in Tables 1 and 2 (in Section 4.1), we presented a direct comparison between the test accuracy of models chosen through Label Wave and the maximum test accuracy model during the training phase. This comparison validates the effectiveness of our Label Wave method.\n\nFor results in Tables 3 and 4 (Section 4.2), the difference between maximum test accuracy and label wave selected model are shown below:\n\n| Methods   | CIFAR10 - Label Wave | CIFAR10 - Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |        81.61\u00b10.44%               |81.76\u00b10.30% |\n| Taylor-CE |        85.06\u00b10.30%               |85.43\u00b10.37%     |\n| ELR       |        90.45\u00b10.52%               |90.76\u00b10.70% |\n| CDR       |        87.69\u00b10.10%               |87.80\u00b10.24% |\n| CORES     |        87.74\u00b10.13%               |87.95\u00b10.21% |\n| NLS       |        83.45\u00b10.19%               |83.62\u00b10.37%     |\n| SOP       |        88.42\u00b10.38%               |88.82\u00b10.46%     |\n\n\n\n| Methods   | CIFAR100 - Label Wave | CIFAR100 - Global Maximum |\n|-----------|-----------------------|---------------------------|\n| CE        |        50.96\u00b10.30%    |  51.05\u00b10.33%   |\n| Taylor-CE |        57.64\u00b10.28%    |  57.99\u00b10.30%   |\n| ELR       |        65.36\u00b10.39%    |  66.33\u00b10.93%   |\n| CDR       |        63.34\u00b10.15%    |  63.54\u00b10.28%   |\n| CORES     |        45.03\u00b10.38%    |  45.75\u00b10.27%   |\n| NLS       |        58.05\u00b10.15%    |  58.32\u00b10.35%   |\n| SOP       |        68.53\u00b10.30%    |  68.78\u00b10.27%   |\n\n\n\nIn the end, we thank the reviewer for your detailed further revision comments that helped to improve the credibility of our method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission32/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294740629,
                "cdate": 1700294740629,
                "tmdate": 1700294740629,
                "mdate": 1700294740629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]