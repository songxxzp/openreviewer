[
    {
        "title": "Order-Preserving GFlowNets"
    },
    {
        "review": {
            "id": "L63cazreAh",
            "forum": "VXDPXuq4oG",
            "replyto": "VXDPXuq4oG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission479/Reviewer_CCax"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission479/Reviewer_CCax"
            ],
            "content": {
                "summary": {
                    "value": "A new method called Order-Preserving GFlowNets (OP-GFNs) has been proposed to address issues with Generative Flow Networks (GFlowNets) in sampling candidates with probabilities proportional to a given reward. GFlowNets can only be used with a predefined scalar reward, which can be computationally expensive or not directly accessible, and the conventional practice of raising the reward to a higher exponent may vary across environments. OP-GFNs eliminate the need for an explicit formulation of the reward function by sampling with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates. The training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective maximization tasks, concentrating on candidates of a higher hierarchy in the ordering to ensure exploration at the beginning and exploitation towards the end of the training. OP-GFNs demonstrate state-of-the-art performance in both single-objective maximization and multi-objective Pareto front approximation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies an interesting problem of GFlowNets -- training an order-preserving GFlowNets. The paper is also well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "> Note, that the optimal $\\beta$ heavily depends on the geometric landscape of $u(x)$.\n\nThere have been a few related works that attempt to determine the optimal $\\beta$ either from the perspective of dynamically annealing temperature or training a temeperature-conditioned GFlowNet that can generalize across a set of different temperatures. \n\nIn addition, it would be better to also compare OP-GFN with these baselines besides GFN-$\\beta$.\n\nKim, Minsu, Joohwan Ko, Dinghuai Zhang, Ling Pan, Taeyoung Yun, Woochang Kim, Jinkyoo Park, and Yoshua Bengio. \"Learning to Scale Logits for Temperature-Conditional GFlowNets.\" *arXiv preprint arXiv:2310.02823* (2023).\n\nZhang, David W., Corrado Rainone, Markus Peschl, and Roberto Bondesan. \"Robust Scheduling with GFlowNets.\" In *The Eleventh International Conference on Learning Representations*. 2022.\n\n> Therefore, we only focus on evaluating the GFlowNet\u2019s ability to discover the maximal objective.\n\nIf the goal is to discover the maximal objective, why do you use GFlowNets and not RL, wher the latter learns an optimal policy that maximizes the return.\n\n>  The ability of standard GFlowNets, i.e. $R(x) = u(x)$, is sensitive to $R_0$.\n\nThe performance of trajectory balance is indeed senstive to the value of $R_0$ (which is also mentioned in the paper), which leads to large variance. However, this is usually not the case for flow matching, detailed balance, and sub-trajectory balance. It would be better to make this claim in a more clear way. In addition, it would be better to include a more extensive study of OP-GFN with other more advanced learning objectives (e.g., FM, DB, SubTB with temporal difference based objective) with a more thorough discussion. \n\n> Large $R_0$ encourages exploration but hinders exploitation since a perfectly trained GFlowNet will also sample non-optimal candidates with high probability; whereas low $R_0$ encourages exploitation but hinders exploration since low reward valleys hinder the discovery of maximal reward areas far from the initial state. \n\n$R_0$ actually determines the sparsity of the reward function -- with a larger $R_0$, the reward function is less sparse, while a smaller $R_0$ corresponds to a high sparsity in the reward function. Therefore, it is inappropriate to say that \"Large $R_0$ encourages exploration\" and \"low $R_0$\" encourages exploitataion. Whether encouraging exploration (or exploitation) for the agent is determined by the learning algorithm itself, instead of $R_0$ (which is the underlying environment).\n\n> We observe that the OP-GFN outperforms the GFN-\u03b2 method by a significant margin.\n\nIt would be better to also include more advanced baselines and methods in the field of NAS."
                },
                "questions": {
                    "value": "Please refer to the weaknesses part in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Reviewer_CCax"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588198969,
            "cdate": 1698588198969,
            "tmdate": 1700708666651,
            "mdate": 1700708666651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OoLwYE6oLT",
                "forum": "VXDPXuq4oG",
                "replyto": "L63cazreAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CCax"
                    },
                    "comment": {
                        "value": "1. **Related work**. Please see the **Common Response: Related Works**. \n\n2. **RL vs OP-GFNs**. Please see the **Common Response: Extensive Experiments \\& RL vs OP-GFNs**. \n\n3. **More baselines**. We think we mainly use the HyperGrid for sanity checking instead of sweeping experiments to compare performances, so we only consider TB here. Later, in the NAS environment, we have implemented and compared OP-TB with various baselines, including FM, DB, subTB. Please see Section E.4.5. To conclude, OP has significant improvements on all the methods. We are currently running additional experiments for the molecular design environments, which we will add as new baselines to the updated manuscript. Please see the **Common Response: Extensive Experiments**. \n\n4. **$R_0$ and exploration-exploitation**. Sorry for the confusion. What we wanted to explain is that a high sparsity in the reward function is not good for GFNs to explore different modes. Therefore, small $R_0$ is a harder case for traditional GFN methods to discover the mode on the upper-right corner, see Figure E.4 for details. However, since the OP-GFN only focuses on the ordering, it is independent of $R_0$. We change the term \"encourages\" to \"facilitates\". \n\n5. **Advanced baselines in NAS**. Our OP-GFNs are multi-trial sampling methods. In the NAS environments, we have compared our results against three classes multi-trial sampling methods. 1) evolutionary strategy, e.g., REA; 2) reinforcement learning (RL)-based methods, e.g., REINFORCE, 3) HPO methods, e.g., BOHB. We think these three algorithms is representative of previous baselines. We also include other GFN methods, including (OP-)DB, (OP-)FM, and (OP-)subTB. However, for the widely-used optimization-based methods, such as DARTS, since they are not multi-trial sampling, belonging to a different class of methods from OP-GFNs, we do not include the comparison."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953761806,
                "cdate": 1699953761806,
                "tmdate": 1699953761806,
                "mdate": 1699953761806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0IuYnUD8bp",
                "forum": "VXDPXuq4oG",
                "replyto": "L63cazreAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any further comments from the Reviewer CCax?"
                    },
                    "comment": {
                        "value": "We appreciate your time and effort in reviewing our paper. Given the upcoming deadline for the end of the discussion period, we are wondering whether the added experiments and our responses have adequately addressed your concerns. We are willing to make more improvements based on your further comments."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658579859,
                "cdate": 1700658579859,
                "tmdate": 1700658579859,
                "mdate": 1700658579859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zSIg3LRJid",
                "forum": "VXDPXuq4oG",
                "replyto": "L63cazreAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Friendly reminder: The Discussion Period is ending in fewer than 12 hours."
                    },
                    "comment": {
                        "value": "Dear Reviewer CCax,\n\nWe appreciate your time and effort in reviewing our paper. Since we have a limited 12-hour window for the author-reviewer discussion, we kindly ask for your valuable feedback on our rebuttal. We are willing to make further improvements based on your feedback.\n\nBest,\nAuthors."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704818441,
                "cdate": 1700704818441,
                "tmdate": 1700704818441,
                "mdate": 1700704818441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CpbuOfADNX",
                "forum": "VXDPXuq4oG",
                "replyto": "L63cazreAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_CCax"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_CCax"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the additional experiments, which have clarified my concerns. \n\nRegarding Point 3 (Later, in the NAS environment, we have implemented and compared OP-TB with various baselines, including FM, DB, subTB.), I found the results and the conclusion that \"We obeserve that non-OP-TB methods can achieve similar or better performance with OP-TB, which validate the effectiveness and generality of order-preserving methods.\" very interesting, and better support that the OP method is general, which could further improve the paper. Please incorporate these results in the final version for a more in-depth comparison. \n\nI have therefore raised my score."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708650540,
                "cdate": 1700708650540,
                "tmdate": 1700708693637,
                "mdate": 1700708693637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yb38vgMPim",
            "forum": "VXDPXuq4oG",
            "replyto": "VXDPXuq4oG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission479/Reviewer_Ndf3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission479/Reviewer_Ndf3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes OP-GFN, order-preserving GFlowNets, that sample with probabilities in proportion to a learned reward function instead of explicitly defined reward, the learned reward is compatible with a provided partial order on the candidates. OP-GFN promotes exploration in the early stages and exploitation in the later stages of the training by gradually sparsifying the reward function during the training. The authors provide theoretical proof that the learned reward is piecewise log-linear with respect to the ranking by the objective function. OP-GFN are proposed for both the single-objective maximization and multi-objective Pareto approximation problems, where experimental results on synthesis environment HyperGrid and two real-world applications NATS-Bench and molecular designs demonstrate competitive performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theoretical contribution of the paper is sound and novel, where1)  the log reward being piecewise linear with respect to the ranking enables exploration on the early training stages and the exploitation on the later sparsified reward R is a useful and desirable feature in many training procedures; 2) matching the flow F with a sufficiently trained OP-GFN will assign high flow value on non-terminal states that on the trajectories ending in maximal reward candidates, enabling sampling optimal candidates exponentially more often than non-optimal candidates.\t\t\t\n\nThere is a range of comprehensive experimental results with details in the main document or in the supplementary material\n\nThe paper is well written and organized, making it easy to follow."
                },
                "weaknesses": {
                    "value": "There can be more comparison against other state-of-the-art methods in the experimental section. The experiments, such as the hypergrid, only compares against the GFN-TB method. If the exploration-exploitation is the main benefit of OR-GFN in single objective problems, having more baseline comparison that encourages exploration of TB (such as https://arxiv.org/pdf/2306.17693.pdf or other variations of the reward besides beta-exponentiating) can make the paper stronger. \n\nIn single-objective maximization, the form of the local label y of the Pareto set is explicitly given in section 3.2, however in the multi-objective case, it is not clear how the label y can be calculated a priori, when the Pareto fronts are unknown. \u201cWhen the true Pareto front is unknown, we use a discretization of the extreme faces of the objective space hypercube as P.\u201d It is not clear whether the proposed method works due to the good estimate of the P, i.e., whether GGN can also benefit from incorporating estimated Pareto fronts into reward definition somehow."
                },
                "questions": {
                    "value": "In section 4.2, the results shown are for the top-100 candidates, are the results similar for other choices of K? \n\nIn the experiment on neural architecture search, in Figure 4.2 the test accuracy comparison is against the clocktime, could you plot against the number of samples to understand the impact of learning the reward and sample efficiency of OR\u2013GFN?\n\nHave you considered other reward schemes besides the exponentially scaled reward? For example, some UCB-variant of the reward to balance between exploration vs exploitation? \n\nRelated work on encouraging exploration of GFlowNets: https://arxiv.org/pdf/2306.17693.pdf\n\n\nMinor Comments:\nFigure 4.1 caption, typo \u201cTopk\u201d\nFigure 4.1, should the legend be \u2018TB-OP\u2019 instead of \u2018TB-RP\u2019?\nIn section 4.1, typo \u201cTB\u2018s performance\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684943897,
            "cdate": 1698684943897,
            "tmdate": 1699635974461,
            "mdate": 1699635974461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4pxl5Wf1hk",
                "forum": "VXDPXuq4oG",
                "replyto": "yb38vgMPim",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ndf3"
                    },
                    "comment": {
                        "value": "Thank you for thoroughly checking our paper, pointing out related work, and suggesting improvements to the experimental parts. We try to address your concerns below:\n\n1. **Exploration in TB**. Thanks for your insightful reference. We will add the paper to the related work.  We think the idea of Thompson sampling could be useful in OP-GFNs as well. However, one of the main benefits of our work is that it naturally balances the exploration and exploitation as the training goes, without the need for manually designed exploration steps. Specifically, compared with Thompson Sampling, sampling from $K$ different forward policies increases the computational budget for $\\times K$. Please see the **Common Response Related Work**.\n\n2. **Pareto front** Sorry for the confusion, the sentence \"When the true Pareto front is unknown, we use a discretization of the extreme faces of the objective space hypercube as $P$\", is used only in the evaluation instead of the training. When we do not know the true Pareto front, we can only use evenly spaced points on the outermost faces of the $[0,1]^D$ cube to measure the closeness or diversity of the generated samples. \nIn the training, we describe how to label the samples in \"Section 3.1, 1) Order preserving\". In every step, we sample a batch of terminal states $X$ and label 1 to those which are non-dominated in $X$, and 0 otherwise. We will add this description to make it more clearly in the paper. Please note the local distribution in Section 3.2 is a special case of those in Section 3.1.   \n\n3. **Value of $k$ in top$k$** We think it might be unnecessary to compare many different $k$s. Since $k$ is only the moving window to reduce the possible variance in measuring the optimal candidates. \n\n4. **Plotting against the clock time** We follow Nats-Bench paper to plot against the clock time. We think when evaluating the accuracy is costly, it makes more sense to use the clock time than the number of samples to measure the sampling efficiency. When obtaining the reward is easy, such as in the molecular design, we then will use the number of samples. \n\n5. **Reward schemes other than GFN-$\\beta$**. Please see the **Common Response: Related Works**. \n\n6. **Minor: typos**. Thanks for pointing out the typos. We have corrected them in the updated manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953695588,
                "cdate": 1699953695588,
                "tmdate": 1699953695588,
                "mdate": 1699953695588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kmwOmogYkA",
                "forum": "VXDPXuq4oG",
                "replyto": "4pxl5Wf1hk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_Ndf3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_Ndf3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions and the additional experiments as well as discussion in the paper. I keep a positive score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696066166,
                "cdate": 1700696066166,
                "tmdate": 1700696066166,
                "mdate": 1700696066166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TnSsz3q0gd",
            "forum": "VXDPXuq4oG",
            "replyto": "VXDPXuq4oG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission479/Reviewer_ipVV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission479/Reviewer_ipVV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use the GFlowNet framework to learn samplers of optimal points based on given orderings. This is useful because it applies to single and multi-objective problems alike.\n\nThe method works by learning a reward function that minimizes the divergence between the optimality distribution (i.e. the indicator function around the argmax of $\\mathcal{X}$) and the distribution induced by the learned reward function, as learned by a GFlowNet. This function is learned simultaneously with the corresponding GFlowNet sampler that tries to match this non-stationary reward, which at convergence neatly results in finding the optimal points."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea proposed in the paper is solid and the execution is well done; the method is tested on a whole variety of tasks and relevant setups.  \nThe idea certainly relates to other rank-based methods, such as those in RL & search, but stands on its own in the GFlowNet framework."
                },
                "weaknesses": {
                    "value": "My biggest criticism of the paper is really its presentation.\n\nIt's really not clear what the algorithm actually is, readers have to go all the way into the appendix to find it, and even there some questions remain, are $\\mathcal{T}$ and $\\mathcal{D}$ distinct? What is $\\hat R$ trained on? Does it have distinct parameters? Shared? etc.   \nFrom scrolling through the appendix, it appears that there are, understandably, a number of tricks that can be used. Most of them have some form of ablation in one task or the other, but a cohesive summary is lacking. The main criticism here is that it's important to disentangle what the contribution is from the algorithm itself, and from the tricks.\n\nInvestigative figures like Fig. E.6 should really be in the main body of the paper, and it would be much nicer to have such a figure for more complex domains. Generally the paper doesn't do a great job of showing _why_ the method works empirically."
                },
                "questions": {
                    "value": "Notes:\n- For someone not familiar with GFNs, Eq (3) may not be obvious (R being parameterized \"through\" P_F P_B & Z)\n- I'm not sure that I fully appreciate the \"piecewise linear\" part of the contribution. Is there something I'm missing?\n    - First of all, the whole thing is discrete and operating on sets. It makes no sense to talk about piecewise linearity because we are not in $\\mathbb{R}$, the pieces \"in between\" don't even exist.\n    - Ignoring the above, the shape of the function doesn't really matter? What matters is $\\mathrm{order}(a,b) \\equiv \\mathrm{order}(\\hat R(a), \\hat R(b))$, which follows in a fairly straightforward way once we assume 0 loss.\n- Proposition 3/6 is similarly weird, why this specific choice of MDP? I'd recommend either introducing why this choice matters, or maybe at least state that the general case is much harder to provide theoretical statements for. I find the current form a bit awkward\n- 4.2; the authors seem to be using the molecular setup of Shen et al., not Bengio et al.. Shen et al.'s setup is quite different (and more simple, the state spaces are made to be much small so that computing $p(x; \\theta)$ becomes tractable). It would be appropriate to note this. It's not clear what \"choices\" the authors are using from Shen et al. since quite a few are introduced in that work (which appear to have a significant impact)\n- \"optionally use the backward KL regularization (-KL) and trajectories augmentation (-AUG) introduced in Appendix E.1\", these options don't seem to have that much impact, but it's a bit weird that they're just mentioned in passing in the main text.\n- \"we observe that OP-GFNs can learn a highly sparse reward function that concentrates on the true Pareto solutions, outperforming PC-GFNs.\" It's a bit weird to claim here that OP-GFNs are outperforming PC-GFNs, because they learn entirely different things. From Fig 5.1 it seems like PC-GFN is attributing some probability mass on the Pareto front points, which suggests to me that it has discovered them. \n    - This is maybe more of a comment on RL literature, but I'm also not a fan of comparing algorithms on a grid. Gridworlds are useful to sanity check algorithms, not to compare algorithms.\n- In Sections 4 & 5, it would be good to clarify what work introduced what task and what MDP setup which is being used.\n- Why build on GFlowNets? Don't get me wrong I'm a big fan of GFNs, but it seems to me like the use-case of that framework is to obtain \"smooth\" energy-based amortized samplers, whereas the proposed work is meant to converge to an extremely peaky distribution where essentially $p(x^*)=1$, and is much more akin to finding the optimal greedy policy in RL. There may be a very good reasons (I can think of some ;) but I think the paper could do a better job of explaining them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Reviewer_ipVV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685002634,
            "cdate": 1698685002634,
            "tmdate": 1699635974383,
            "mdate": 1699635974383,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ajvREkC28V",
                "forum": "VXDPXuq4oG",
                "replyto": "TnSsz3q0gd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ipVV"
                    },
                    "comment": {
                        "value": "Thank you for thoroughly checking our paper and for providing valuable feedback about notations, structure and content presentation. However, with the current paper layout and the given page limit, we see ourselves faced with a lot of restrictions, that make it very hard to restructure the whole paper. Addressing your proposals one-by-one: \n\n**Weaknesses**:\n\n1. **Minor: $\\mathcal{D}$ or $\\mathcal{T}$**. Sorry for the confusion, $\\mathcal{D}$ is a typo, it should indeed be $\\mathcal{T}$ in Algorithm 1. We have corrected it in the updated manuscript.\n\n2. **What $\\widehat{R}$ is trained on?** $\\widehat{R}$ is trained on the order-preserving loss $L_{OP} $, its parameters depend on GFN parametrization used, i.e., the parameters of: 1) $P_F$, $P_B$ and $Z$, 2) $P_F$, $P_B$ and $F$, or 3) $F$, as given in Table 2.1. parametrized by the GFN parametrization, such as $\\widehat{R}_{\\rm TB}(x;\\theta)$. \n\n3. **Disentangling algorithms with tricks**. Sorry for the confusion. We optionally use backward augmentation and backward KL regularization in the NAS environments, and clearly specify their usage by (-KL) and (-AUG) in the plots or tables. We have observed from Figure 4.2 that the major improvements are from the OP method, and these tricks only yield marginal improvements. More specifically, all 3 variations of OP-GFNs that include selections of KL regularization or training trajectory augmentation are very close to each other. In other environments, we did not include additional tricks when comparing with traditional GFNs but followed the same setup as the original papers. In other words, the only difference is the training criteria. \n\n4. **Investigative figures in the appendix**. We admit some figures are left in the appendix. However, there is a 9-page limit, so we chose to put the ablation studies and auxiliary results in the appendix. We felt, that discussing the properties of the trained OP-GFN gives a good insight about what kind of generating distribution the OP-GFN learns. Hence, we sacrificed some space to include these theoretic results in the main paper and put the validation of the theoretical results in the appendix. \n\n**Questions**:\n\n1. **Descriptions of GFNs**. We succinctly describe the GFNs in Table 2.1. We have made a clearer reference to this table in the updated manuscript.  \n\n2. **Log piecewise linear**. We admit \"log piecewise linear\"  is not a good naming, and might be a confusing term. Following your arguments, we replace it with \u201cpiecewise geometric progression\u201d. The result of \"piecewise geometric progression\" is not as trivial as it sounds, especially when there are candidates of the same objective value $u(\\cdot)$, please refer Propositions 2 and 5 for details. \n% \\lukas{However, even if it is trivial it characterizes the distribution that is learned by the OP GFN. Therefore, we feel it is a necessary part of the paper in order to be complete.}\n\n3. **Sequence prepend/append MDP**. Sequence prepend/append MDP are widely used in molecule generation, such as in Section 4.2, \u201cFor the SIX6, PHO4, QM9, sEH environment, we use the sequence prepend/append MDP in Definition 1\u201d. Therefore, theoretical analysis based on this MDP structure is meaningful. \n\n4. **Molecular setup**. We inherit the basic setup as Shen et al for trajectory balance, and for OP methods, we only change the training loss. Please see Section E.3.2, and **Common Response: Extensive Experiments**. \n\n5. **Definition of -KL and -AUG**. We write the names in the main text because it appear in Figure 4.2, we defer the detailed description to the appendix due to the page limit. \n\n6. **PC-GFNs vs OP-GFNs in HyperGrid**. In Figure 5.1, PC-GFNs indeed attribute some probability mass on the Pareto front points, but not on all such points, such as \"currin-shubert\", it misses a lot of Pareto front solutions. Please see Figure F.1, this is mainly because \"currin-shubert\"'s Pareto front is concave, and linear scalarization in PC-GFNs inherently cannot deal with a concave Pareto front. We point out this weakness in Section 2.2. On the contrary, OP-GFNs can recover all the solutions, regardless of the Pareto front shape, and the landscape terminal flow is much more sparse than PC-GFNs.  \n\n7. **Sanity checking in HyperGrid**. Thanks for your advice on sanity checking in the grid-worlds. We perform the sanity checking in HyperGrid in the multi-objective optimization, since only in the hypergrid, the Pareto front can be explicitly calculated. We also perform extensive experiments on real-world environments, including molecular designs and NAS. \n\n8. **Details in Section 4, 5**. We put the detailed setup in Sections 4 and 5 in the appendix due to the page limit. We hope the experiments' succinct description and reference to the appendix in the main text, as well as \"Appendix A: an overview\" could help you navigate the detailed settings. \n\n9. **RL vs OP-GFNs**. Please see the **Common Response: RL vs OP-GFNs**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953515889,
                "cdate": 1699953515889,
                "tmdate": 1699954677758,
                "mdate": 1699954677758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LxklfjpTMP",
                "forum": "VXDPXuq4oG",
                "replyto": "ajvREkC28V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_ipVV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_ipVV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and clarifications. I think the method is great, and am maintaining my score. I encourage the authors to keep working on the presentation and writing to make this work as impactful as possible.\n\nNote that, for molecules, I pointed out the use of Shen et al.'s setup because I thought it should be clearer what scale of problems are being tackled in this paper. Many GFlowNet papers have used the sEH reward with a fragment-graph-based MDP with ~100 fragments, which is _much_ harder (graphs rather than sequences are generated, and $|X|$ is at least $10^{16}$) than the append-prepend MDP with ~20 fragments ($|X|$ is about $10^7$)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697091036,
                "cdate": 1700697091036,
                "tmdate": 1700697091036,
                "mdate": 1700697091036,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LsWHk9A3Wg",
                "forum": "VXDPXuq4oG",
                "replyto": "TnSsz3q0gd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ipVV"
                    },
                    "comment": {
                        "value": "We are thankful to the reviewer for recognizing and acknowledging our work. We will continue working on improving our writing and presentation skills.\n\nWe are also grateful for the identification of various sEH setups in different papers. This will be emphasized in Section 4.2 and Appendix E.3 in the updated paper. Besides, we have utilized the fragment-graph-based MDP in \"Section 5.4: Fragment-Based Molecule Generation\" to analyze OP-GFN's multiobjective performance, where maximizing sEH is one of the objectives."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699764630,
                "cdate": 1700699764630,
                "tmdate": 1700700617129,
                "mdate": 1700700617129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LKEVDVrJsv",
            "forum": "VXDPXuq4oG",
            "replyto": "VXDPXuq4oG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission479/Reviewer_8SYj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission479/Reviewer_8SYj"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an innovative approach for training GFlowNets, eliminating the necessity for an explicit formulation of the reward function while ensuring compatibility with the provided order of candidates. This method involves the simultaneous training of a reward function, which maintains the order of samples in conjunction with GFlowNet. The authors conducted comprehensive experiments in both single and multi-objective settings, revealing that their proposed approach yields outstanding results in comparison to prior methods when dealing with only pairwise order relations and non-convex Pareto fronts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper conducts extensive experiments under different objective settings and domains. Especially, the paper conducts experiments on NAS benchmark, which is a first attempt to apply GFlowNets into NAS while it is natural as we can make neural architecture by adding operations in a sequence manner. It also achieves superior results compared to other baselines in NAS benchmark.\n\nThe paper also tackles multi-objective problems with a non-convex Pareto front, which is hard to solve with prior multi-objective GFN methods such as PC-GFN. Additionally, the paper eliminates the necessity for fine-tuning the temperature parameter, \u03b2, which plays a critical role in GFlowNets' performance."
                },
                "weaknesses": {
                    "value": "1. There is a possibility of encountering non-stationarity issues when jointly training GFlowNets and the reward function. It might be worth exploring alternative training strategies to mitigate this potential challenge.\n\n2. Experimental results are not that persuasive, having little improvement over baselines. For example, this work just compares with simple GFN baseline in molecular tasks, more competitive baselines (e.g., subTB, FL-GFN, RL methods) are needed. \n\n---\n\n\n**Discussion needed regarding temperature conditioning methods**\n\nThere are some researches on temperature-conditioned GFlowNets, which learn a single model for multiple reward functions conditioned on different temperatures. It may be more persuasive to compare the proposed method with the following literature [1], [2], especially in single-objective settings. Note that reviewer understands that temperature-conditioned GFlowNets are too recently proposed, so the authors could not reflect this in this submission but suggest making some discussion in the final version for future researchers!\n\n[1] Zhang, David W., et al. \"Robust scheduling with GFlowNets.\" arXiv preprint arXiv:2302.05446 (2023).\n\n[2] Kim, Minsu, et al. \"Learning to Scale Logits for Temperature-Conditional GFlowNets.\" arXiv preprint arXiv:2310.02823 (2023).\n\n---\n\n**Decision**\n\nThis work is novel, well-written, easy to understand, and provides insight for GFN communities. Although experimental results do not strictly outperform every competitive baseline, this work provides a lot of experiments over various tasks. To this end, the score would be 6."
                },
                "questions": {
                    "value": "**Section 4.2**\n\nAuthors say that they compare the proposed method with GFN-$\\beta$, where $\\beta$ is the selected value from the previous work. As far as I know, the temperature is not tuned in the previous work. What if we assign high values to $\\beta$? It seems that we can achieve higher topk reward by sacrificing diversity when we assign high values to $\\beta$ and authors only consider maximal objective in single-experiment setting\n\n**Section 5.2, 5.4**\n\nAuthors say that they use a replay buffer and do off-policy training. I am curious that as the reward function is trained across training, off-policy training may lead to degrading performance. \n\n**Minor Questions**\n\nSection 4.1) $u_0\\$ seems a typo. Maybe $R_0$?\n\nSection 4.3) Authors propose two implementation tricks, KL regularization and trajectories augmentation. Especially trajectories augmentation has shown superior results in the previous paper. It is hard to capture when those two tricks are applied. While authors say trajectory augmentation is applied in molecule design (E.3.2), main paper say that trajectories augmentation is optionally used in NAS environment. It may be helpful when those two tricks are applied across differnt experiment settings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Reviewer_8SYj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813716879,
            "cdate": 1698813716879,
            "tmdate": 1699635974311,
            "mdate": 1699635974311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DScvZsBFTZ",
                "forum": "VXDPXuq4oG",
                "replyto": "LKEVDVrJsv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8SYj"
                    },
                    "comment": {
                        "value": "Thank you for your well-structured feedback and for pointing out very recent, and strongly related literature. As mentioned in the general response, we added a discussion to the related work section. Concerning your questions:\n\n1. **Non-stationarity issues**. We find the non-stationary issues are not very serious in the single-objective case. On the contrary, if we do not learn but define a reward in prior, like with GFN-$\\beta$, we cannot obtain similar performance like with OP-GFNs. defining a reward in prior by GFN-$\\beta$ methods are not as good as the OP-GFNs. Please see Appendix E.4.4. ablation studies. However,  In the multi-objective cases, we observe the non-stationary issues arise, and we choose to use the replay buffer to stabilize the training. Please see the second paragraph of Section 5.2. We added a sentence to make this more clear. \n\n2. **More competitive baselines** From Figure 4.1, Figure 4.2, Figure 5.1, Figure F.1, our algorithm has proven to be more effective than previous methods. We have already include the subTB (Appendix E.4.5) and RL methods (REINFORCE, Figure 4.2) in NAS environments, which are, to our knowledge, strong baselines. Could you please elaborate in more detail, which baselines you would like to see in comparison? For the molecular design, please see the **Common Response: Extensive Experiments**. \n\n3. **Temperature conditioning methods** Please see the **Common Response: Related Work**. \n\n4. **Setting higher $\\beta$** Using GFN for stochastic optimization, (1) we do not only consider the maximal reward, but we also need to pay attention to the diversity (exploration). For example, in Section 4.2 (molecular design) we also measure the number of different optimal candidates. If the reward is very sparse, the sampler will only visit states with near-zero rewards, and cannot explore further to visit the various different maximal candidates. We validate this in Figure E.4, when setting $R_0=0$, it yields a very sparse reward landscape. As a result, and the sampler can only sample candidates near the initial points. (2) even if we only consider the maximal value regardless of the diversity, a large $\\beta$ will still not be a good choice. Since a large $\\beta$ might hinder the exploration to optimal candidates distant to the initialization. To point out this problem, we tested various For example, we test various $\\beta$ in NAS in Figure E.8. Setting $\\beta=128$ does not perform better than $\\beta=16$. \n\n5. **Replay buffer** In the sparse reward landscape, off-policy training has been shown to help stabilize the training. Please refer to Appendix C.1 of the paper \"Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular Design\" (https://arxiv.org/pdf/2306.04620.pdf). We add an additional sentence in Section 5.2 to state it more clearly. \n\n6. **Minor: $u_0$ or $R_0$** We always use $R_0$ to denote the constant in the objective of HyperGrid in the whole paper. Since either $u_0$ or $R_0$ merely represents a constant, we decide to use $R_0$ to align with the whole paper's notations, including the updated Figure E.5.\n\n7. **Minor: trajectory augmentation** We always use trajectory augmentation in molecule design as suggested in Appendix E.3.2. In the NAS environment, we optionally use trajectory augmentation, and use \"-AUG\" to if we use this augmentation in the algorithm, see Figure 4.2."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953272977,
                "cdate": 1699953272977,
                "tmdate": 1699953272977,
                "mdate": 1699953272977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5JmafYygT3",
                "forum": "VXDPXuq4oG",
                "replyto": "DScvZsBFTZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_8SYj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_8SYj"
                ],
                "content": {
                    "title": {
                        "value": "Nice work"
                    },
                    "comment": {
                        "value": "Thank you for addressing my concerns with your responses. They have provided clarity, and I maintain a positive score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446922712,
                "cdate": 1700446922712,
                "tmdate": 1700446922712,
                "mdate": 1700446922712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "buhrqJgiNK",
            "forum": "VXDPXuq4oG",
            "replyto": "VXDPXuq4oG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission479/Reviewer_BMH3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission479/Reviewer_BMH3"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to extend GFlowNets, called Order-Preserving GFlowNets (OP-GFN), to sample candidates in proportion to a reward function such that the sampling is consistent with the provided partial order of the candidates. By this extension, they show how exploration and exploitation can be controlled, such that candidates higher in the hierarchy can be given more preference. The method shows benefits for both single objective and multi-objective cases by conducing experiments on a variety of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces an important extension of the GFlowNets for multi-objective optimization when D > 1 objectives need to be optimized. \n2. The work also discusses how an efficient utilization of the GFlowNet policy can be achieved in difficult to explore settings.\n3. The theoretical results and analysis are useful to understand the proposed method and its advantages.\n4. The work also provides a good overview of the literature to benefit the reader."
                },
                "weaknesses": {
                    "value": "1. The experiments section can be expanded to include more difficult environments. For example, for hypergrid,higher values of H and N can be tested as larger grids will help analyzing the exploration problems better.\n2. Detailed balance objective can perform reasonably well in many settings. It will be beneficial to include it in all methods and numbers reported.\n3. It will be useful to add standard deviation and error bars across experiments. It will also be useful to better understand the variance across different GFlowNet objectives and baseline methods.\n\nBy addressing these concerns with experiments, the authors will help address the empirical limitations to strengthen their theoretical discussions and the overall contribution."
                },
                "questions": {
                    "value": "1. Is it possible to include some more difficult configurations of the environments? Hypergrid is one easy example for such an experiment.\n2. It would be helpful if the variations across different seeds and runs could be provided for the experiments.\n3. A hyperparam search for the best learning rate can also be sometimes a big contributing factor across different learning methods.\n\nIf these could be provided along with the comments in the weaknesses, section it will help in addressing most of my concerns towards the experimental contribution of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission479/Reviewer_BMH3"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877979810,
            "cdate": 1698877979810,
            "tmdate": 1700687783143,
            "mdate": 1700687783143,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TZqHdoEEOh",
                "forum": "VXDPXuq4oG",
                "replyto": "buhrqJgiNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BMH3"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and for providing a specific list how to improve our work. We agree with your assessment and took the effort to run additional experiments, in the hope of raising the score. We address your concerns below:\n\n**1. More complex environments**. In the submitted paper, we included many complex real-world environments apart from apart from HyperGrid, which includes molecular generation tasks and NAS. Such tasks are standard in evaluating GFNs in previous papers. We mostly use HyperGrid only for sanity-checking and therefore did not consider more complex configurations with larger $H$ and $D$ at first. To understand the exploration of the algorithm better, in the molecular design environments, we record the number of distinct optimal candidates. Please see the **Common Response: Extensive Experiments**. Furthermore, we additionally consider two high-dimensional settings in HyperGrid: $(D,H)=(3,32)$ and $(D,H)=(4,16)$, please see Section 4.1, and updated Figure E.5. We consider the following three ratios to measure exploration-exploitation: 1) \\#(distinctly visited states)/\\#(all the states); 2) \\#(distinctly visited maximal states)/ \\#(all the maximal states); 3) In the most recently 4000 visited states, \\#(distinctly maximal states)/4000. A good sampling algorithm should have a small ratio 1), and a large ratio 2), 3).  We confirm that TB\u2019s performance is sensitive to the choice of $R_0$, and observe that OP-TB can recover all maximal areas more efficiently, and sample maximal candidates with higher probability after visiting fewer distinct candidates. \n\n**2. Including the detailed balance objective.** The submitted paper contains many experiments with detailed balance in the NAS environment in Appendix E.4.5, where we do not observe a significant gain from the trajectory balance. Therefore, we did not include detailed balance in other environments. However, we ran additional experiments and added detailed balance to the molecular design. Please see the **Common Response: Extensive Experiments**.  \n\n**3. Adding error bars**. We agree with you and add the error bars to the experiments in the main paper, i.e., to the molecular design environments (Please see the **Common Response: Extensive Experiments**). We observe that the OP-TB's variance is small, and outperforms other methods beyond the error bars (especially in sehstr). \n\n**4. A hyperparameter search for the best learning rate**  We use the Adam optimizer for training criteria and parametrizations (DB, TB, subTB, OP). The Adam optimizer adaptively computes individual learning rates for\ndifferent parameters. Therefore, we did not finetune the learning rate of the Adam optimizer. For example, in the NAS environment, we use the initial learning rate $10^{-3}$ for network parameters and $10^{-1}$ for $Z$'s parameters.  \nWe think this comparison is fair because it shows that OP-GFN can outperform standard GFN by only replacing the training criterion. In other words, our results are not cherry-picked from the learning rate."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952940666,
                "cdate": 1699952940666,
                "tmdate": 1700581792145,
                "mdate": 1700581792145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EZcdxmSDGb",
                "forum": "VXDPXuq4oG",
                "replyto": "buhrqJgiNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any further comments from the Reviewer BMH3?"
                    },
                    "comment": {
                        "value": "We appreciate your time and effort in reviewing our paper. Given the upcoming deadline for the end of the discussion period, we are wondering whether the added experiments and our responses have adequately addressed your concerns. We are willing to make more improvements based on your further comments."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658464016,
                "cdate": 1700658464016,
                "tmdate": 1700658464016,
                "mdate": 1700658464016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dcLMy2Hmba",
                "forum": "VXDPXuq4oG",
                "replyto": "buhrqJgiNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_BMH3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Reviewer_BMH3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for clarifying the concerns and for updating the work. I have revised my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687882716,
                "cdate": 1700687882716,
                "tmdate": 1700687882716,
                "mdate": 1700687882716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Qe0OxMCNK",
                "forum": "VXDPXuq4oG",
                "replyto": "buhrqJgiNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BMH3"
                    },
                    "comment": {
                        "value": "We are thankful to the reviewer for recognizing and acknowledging our work."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688664734,
                "cdate": 1700688664734,
                "tmdate": 1700688692781,
                "mdate": 1700688692781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]