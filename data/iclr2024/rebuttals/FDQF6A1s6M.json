[
    {
        "title": "LOQA: Learning with Opponent Q-Learning Awareness"
    },
    {
        "review": {
            "id": "uEMyzmtlb4",
            "forum": "FDQF6A1s6M",
            "replyto": "FDQF6A1s6M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_iBDc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_iBDc"
            ],
            "content": {
                "summary": {
                    "value": "the paper presents LOQA, a decentralized reinforcement learning algorithm that optimizes individual utility while promoting cooperation among adversaries in partially competitive environments. LOQA is designed to achieve state-of-the-art performance in benchmark scenarios like the Iterated Prisoner's Dilemma and the Coin Game, making it a promising approach for practical multi-agent applications. The paper provides a detailed description of the LOQA algorithm, including its opponent Q-learning awareness assumption, and presents experimental results that demonstrate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Quality: The paper provides a detailed description of the LOQA algorithm, including its opponent Q-learning awareness assumption, and presents experimental results that demonstrate its effectiveness. The experiments are well-designed and the results are statistically significant."
                },
                "weaknesses": {
                    "value": "the LOQA algorithm assumes the opponent acts accordingly to an inner action-value function and is designed for environments with discrete action spaces. This means that LOQA is unable to shape other opponents that do not necessarily follow this assumption. The assumptions and dependencies of algorithms are strong, making it difficult to handle continuous action space problems and multi-agent issues."
                },
                "questions": {
                    "value": "Q1: The IPD and coingame mentioned in the paper mainly refer to two general-sum games. Can the algorithm be applied to other types of environments, such as zero-sum games, or more complex gaming environments, such as StarCraft II?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Reviewer_iBDc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697623022172,
            "cdate": 1697623022172,
            "tmdate": 1699636326504,
            "mdate": 1699636326504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "6NKd6pl6z1",
            "forum": "FDQF6A1s6M",
            "replyto": "FDQF6A1s6M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_SYs7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_SYs7"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for decentralized learning in general-sum games, building on the assumption that adversaries sample actions according to their action-value function. The experiments demonstrate significant improvements in wall-clock time and performance against POLA in the coin game.\n\nI am recommending borderline acceptance for this work, as the method is a useful contribution towards scaling LOLA-based methods. However, I would like to see the evaluation strengthened with more and increasingly complex environments, as well as more than one baseline (particularly M-FOS)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The related work is detailed, covering LOLA and meta-learning approaches to social dilemmas.\n2. The method is clearly motivated and described. The underlying assumption is well explained and plausible.\n3. The evaluation demonstrates a significant improvement in computational efficiency against POLA, with the analysis demonstrated in figures 3 and 4 showing this scaling behavior well.\n4. Figure 1 presents a validation that LOQA is capable of learning tit-for-tat-like strategies in IPD."
                },
                "weaknesses": {
                    "value": "1. The evaluation is limited in diversity, comparing LOQA to a single baseline algorithm (POLA) on a single environment (coin game). The selection of POLA vs alternative LOLA extensions is justified, but I am unsure how M-FOS is neglected when it has also been demonstrated to be effective at the coin game? Furthermore, the choice to only evaluate against POLA on the coin game is limiting, when full-history IPD and chicken would be equally interesting and strengthen the results.\n2. The scaling performance of LOQA would be better demonstrated by including further, more complex environments, rather than just scaling up the grid size of the coin game. Since previous LOLA extensions have been prohibitively expensive, this has not been possible, but it appears LOQA runs in a very reasonable amount of time on the largest of these tasks. This gives the opportunity to demonstrate LOQA on a \"more complex and realistic scenario\", rather than just extrapolating from its scaling performance here.\n3. Whilst the training dynamics of POLA are consistent across seeds, LOQA seems to have significant variance (Figure 4). The number of LOQA seeds should be increased beyond 3 to handle this variance."
                },
                "questions": {
                    "value": "1. Nitpick: The scaling of LOQA in figure 3 would be clearer with a log-scale y-axis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Reviewer_SYs7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709778083,
            "cdate": 1698709778083,
            "tmdate": 1699636326402,
            "mdate": 1699636326402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ehjF4czcUe",
                "forum": "FDQF6A1s6M",
                "replyto": "6NKd6pl6z1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their time and thoughtful feedback.\n\n**Weaknesses:**\n\n1. We agree with the reviewer in all of these points. In particular, we will incorporate M-FOS as a baseline for all of our experiments and add full history IPD and Chicken. We did not incorporate M-FOS initially because according to Figure 7 of the M-FOS paper the solution achieved is marginally better than a naive (PPO) agent.\n\n2. This is a valid observation, but we believe that it holds LOQA to a higher standard than POLA or M-FOS. It also makes potential results difficult to interpret since more complex environments lack strong baselines, therefore putting an extra burden for us in terms of adapting and extending existing methods. \n\n3. We will also run more LOQA seeds since we believe it is reasonable and doable before the rebuttal deadline.\n\n**Questions:**\n\n1. We will try a variation of Figure 3 using a log scale and keep it if it still clearly showcases the results.\n\nProvided that we make all of the changes promised above, would you be willing to increase your score?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699737282380,
                "cdate": 1699737282380,
                "tmdate": 1699737282380,
                "mdate": 1699737282380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdCJMSrufV",
                "forum": "FDQF6A1s6M",
                "replyto": "ehjF4czcUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_SYs7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_SYs7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your rebuttal. In summary, I am currently maintaining my score pending a large revision.\n\n> we believe that it holds LOQA to a higher standard than POLA or M-FOS.\n\nYou are correct about this point. I do not think this is a requirement for the paper to be published, however, it is a reasonable expectation for follow-up work to raise the standard of contribution. Particularly in this case, when computational scaling is a major contribution of the proposed method, it would be strongly encouraging to see it evaluated on a task with meaningfully scaled complexity. If existing methods entirely fail to solve the method - leading to no strong baselines - this should be easy to demonstrate and would only strengthen the evaluation.\n\n> We will incorporate M-FOS as a baseline for all of our experiments and add full history IPD and Chicken...\n> We will also run more LOQA seeds...\n\nI appreciate your receptiveness regarding these experiments, which I believe would strengthen the paper significantly. I would be surprised if these results were achievable in the rebuttal period alone. Assuming they are positive, the paper would also require restructuring and rewriting to include said results.\n\nIf all of this was achieved before the end of the rebuttal period, I would be inclined to increase my score. However, I suspect this will be a stretch and risks making the paper incoherent if rushed. Regardless, including these results in a future revision would lead to a strong submission, which I encourage you to pursue."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420665389,
                "cdate": 1700420665389,
                "tmdate": 1700420665389,
                "mdate": 1700420665389,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TgmKeVXaqA",
            "forum": "FDQF6A1s6M",
            "replyto": "FDQF6A1s6M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_rBSb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_rBSb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes LOQA -- a new member of the LOLA family (next to COLA and POLA). Conceptually, LOQA is different in that it assumes that the opponent uses Q-leaning (more specifically, that the probability of its action in a state is proportional to the corresponding Q-value relative to Q-values of other actions). The advantage of this algorithm is that it can achieve performance comparable to POLA (previous state-of-the-art of the family) but faster, which also improves scalability to bigger environments. This claim is complemented by experiments in the Coin Game, which is a two-agent coin collection game on a small grid with 2 agents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Solutions to conditional (equilibrium-based) cooperation in general and improvements of LOLA in particular are relevant to MARL.\n- The paper is straightforward and well-written.\n- The experiments are sound, I especially like fig. 2."
                },
                "weaknesses": {
                    "value": "- Related work lacks discussion of MARL approaches to learning prosocial equilibria other than reciprocity-based or opponent-shaping-based, such as reward redistribution https://ala2020.vub.ac.be/papers/ALA2020_paper_45.pdf https://arxiv.org/abs/2004.13332, mediation https://arxiv.org/pdf/2306.08419.pdf, contracts https://arxiv.org/pdf/2208.10469.pdf, and similarity-based equilibria https://arxiv.org/pdf/2211.14468.pdf.\n- Some limitations of previous LOLA-based approaches that LOQA does not fix are unmentioned in the Limitations section: e.g., it is only applicable to environments with 2 agents.\n- The contribution is limited to modifying an existing algorithm and improving its speed, but not performance. The new algorithm is also quite complex. I do not think the contribution is sufficient for ICLR. A workshop would be a better fit."
                },
                "questions": {
                    "value": "- I am not sure why in 5.1 the Q-value $\\hat{Q}^2$ is approximated as an empirical return. Since we need access to the real $Q^2$ or its approximation through opponent modeling regardless, can we not use a 1-step or n-step temporal difference instead (which would give a better bias-variance)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765657044,
            "cdate": 1698765657044,
            "tmdate": 1699636326306,
            "mdate": 1699636326306,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o9dzQX6wsF",
                "forum": "FDQF6A1s6M",
                "replyto": "TgmKeVXaqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy that the reviewer finds LOQA to be a scalable method for solving social dilemmas. \n\n**Summary:**\n\nLOQA **is not** an algorithm that can be considered *\u201ca new member of the LOLA family\u201d* as it is fundamentally different. We would like to remind the reviewer that algorithms in the LOLA family differentiate w.r.t. imagined opponent parameter updates that are computed explicitly. This causes significant computational burden, making these algorithms hard to scale. LOQA on the other hand differentiates w.r.t. the opponent\u2019s Q function (via reinforce), which is estimated by observing the returns of trajectories. Therefore LOQA does not compute gradients w.r.t. explicit optimization steps.\n\n**Weaknesses:**\n\n1. We thank the reviewer for the suggested additions to the relevant literature, in particular we believe that similarity-based equilibria should be discussed. However, we want to emphasize that we are interested in the setup of solving social dilemmas without third parties (reward redistribution,  mediation), contracts, or any other changes to the underlying game.\n \n2. This statement is not true, and should be clarified further. Both LOLA and LOQA can be extended to games with more than two players. LOQA would do it by adding extra log terms for each extra player to the loss, whereas LOLA would require computing imagined updates for each new player. This is not explicitly done as social dilemmas with two players are already difficult to solve for current algorithms.\n\n3. We respectfully disagree with the reviewer. LOQA does not belong to the LOLA family and as such is not a modification of an existing algorithm. We feel that LOQA is being held to a higher standard than existing methods in the literature. How is a novel method that improves the state of the art for neural network solutions to social dilemmas (both in terms of optimality and speed) only suited for a workshop, especially considering that previous publications (POLA, M-FOS) have been published to top machine learning venues? Also, we kindly request the reviewer to clarify the definition of 'complexity' in this context as we believe LOQA is relatively simple. In contrast to previous work, LOQA does not have inner outer loops, differentiation through optimization steps, and meta-games. It would be helpful if they could offer arguments to support the views that a) LOQA is more complex compared to existing methods, and b) why this kind of complexity might be considered undesirable.\n\n**Questions:**\n\n1. We experimented with bootstrapped versions of the $\\hat{Q}^2$ estimate for many different lengths initially with worse results than using the full trajectory. We believe that, since the only differentiable parts of the estimates are the rewards, making the full trajectory differentiable works better for longer games.  \n\nWe are surprised by the suggestion that our paper is better suited for a workshop. Given that we have addressed the reviewer\u2019s concerns and criticism, we ask the reviewer to reconsider their score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699737222818,
                "cdate": 1699737222818,
                "tmdate": 1699737222818,
                "mdate": 1699737222818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tZIRtgt4Q3",
                "forum": "FDQF6A1s6M",
                "replyto": "o9dzQX6wsF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_rBSb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_rBSb"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "The authors' rebuttal has not provided much new information and instead emphasizes points already made in the paper (which is somewhat useful) and channels the authors' frustration and subjective disagreement (which is not useful). Still, I will clarify my assessment so that authors can improve their manuscript.\n\n**Summary**\n\nThis is a semantic argument and thus is unconstructive to argue about.\n\n**Weaknesses**\n\nThe assessment that much of the literature I provided should not even discussed as a part of related work is obviously inadequate. Notice that I do not propose to compare with these methods, as I agree with the authors' points about those methods changing the game / introducing a third party, unlike LOQA. Still, they solve the same problem, and unlike LOQA, are applicable to n-player games, some of them even to MeltingPot (that is much more complex than the coin game). So whichever limitations they may have, they have advantages over LOQA. Both should be discussed in an adequate literature review.\n\nRegarding the n-player games, the authors claim in the rebuttal that their algorithm is applicable, but to my understanding, the paper does not discuss it and certainly does not demonstrate it. There's a limit to how much the argument that LOQA is held to a higher standard can justify. N-player does not equal MeltingPot. In the literature that the authors so readily dismissed, there already exist examples of sequential n-player games with high-dimensional but relatively simple state spaces, which could match the LOQA's higher standard. There is no reason to not demonstrate LOQA's applicability to these games, which in my opinion would significantly strengthen the manuscript (as the previous algorithms in the LOLA family have not been demonstrated in n-player games).\n\nMy assessment of incremental contribution stands. However more reasonable the method may seem to authors than predecessors, the general approach is the same, and the empirical results are not impressive enough.\n\n**Questions**\n\nThank you for the clarification, makes sense to me."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699879743819,
                "cdate": 1699879743819,
                "tmdate": 1699879743819,
                "mdate": 1699879743819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mw0bBhF2tr",
                "forum": "FDQF6A1s6M",
                "replyto": "2kGfEzCrYc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_rBSb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_rBSb"
                ],
                "content": {
                    "comment": {
                        "value": "Providing results in the Coin Dilemma with 3 agents would improve the contribution, but I'm unsure if it will be sufficient. I suggest the authors take the time to do a thorough experimental investigation and try even more agents, rather than rushing during the rebuttal period."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140525953,
                "cdate": 1700140525953,
                "tmdate": 1700140525953,
                "mdate": 1700140525953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8HCDRu7qhO",
            "forum": "FDQF6A1s6M",
            "replyto": "FDQF6A1s6M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_xNg3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_xNg3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces learning with opponent Q-learning awareness (LOQA) which optimizes cooperation in mixed-motive environments by assuming the opponent samples actions proportionally to Q values. This method is computationally lighter compared with prior works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I like the idea of deriving some cooperative solutions in mixed-motive games without computing the meta-game solutions. There were many efforts in this direction but only a few paid off, the main limitation lies in the scalability of the multi-agent problems.\n2. The paper is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "In general, the experimental part has room for improvement\n1. When this line of research on LOLA has a few prior works, a comparison with a decent amount of previous works is necessary so that we know the proposed method is better. The good performance of a particular method under a particular environment is not the reason to abandon other methods, especially when POLA [1] did not compare with M-FOS [2]\n2. Results on the IPD and coin environment may be a bit preliminary when we jointly consider the contribution of the algorithm (efficiency). More complex games like Meltingpot 2.0 [3] with some clearly diverse background agent policies (cooperating for different variations of time and defect afterward) can be a good benchmark for the completeness of the experiments\n\n[1] Zhao, S., Lu, C., Grosse, R. B., & Foerster, J. (2022). Proximal Learning With Opponent-Learning Awareness. Advances in Neural Information Processing Systems, 35, 26324-26336.\n\n[2] Lu, C., Willi, T., De Witt, C. A. S., & Foerster, J. (2022, June). Model-free opponent shaping. In International Conference on Machine Learning (pp. 14398-14411). PMLR.\n\n[3] Agapiou, J. P., Vezhnevets, A. S., Du\u00e9\u00f1ez-Guzm\u00e1n, E. A., Matyas, J., Mao, Y., Sunehag, P., ... & Leibo, J. Z. (2022). Melting Pot 2.0. arXiv preprint arXiv:2211.13746."
                },
                "questions": {
                    "value": "1. (Comments) The authors should use \\citep{} for (Author, Year) citations\n2. Any intuitions on why the other work[1], meta-game + model-free opponent-shaping does not work on coin game?\n\n[1] Lu, C., Willi, T., De Witt, C. A. S., & Foerster, J. (2022, June). Model-free opponent shaping. In International Conference on Machine Learning (pp. 14398-14411). PMLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814043532,
            "cdate": 1698814043532,
            "tmdate": 1699636326218,
            "mdate": 1699636326218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e08q4Lwe6T",
                "forum": "FDQF6A1s6M",
                "replyto": "8HCDRu7qhO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful feedback. \n\n**Weaknesses:**\n\n1. First, we want to restate that there are not many existing methods that work in the Coin Game. That being said, we are only aware of POLA and M-FOS; LOLA has been shown to not work by the POLA authors. We will add an M-FOS comparison as we believe it is a valid and reasonable concern. We did not do so initially because we had reason to believe that the M-FOS results were marginally better than those of naive agents. We refer the reviewer to Figure 7. in the M-FOS paper, where the odds of taking their own coin reach 0.54 which demonstrates weak cooperation compared to LOQA and POLA.\n\n2. We agree that running LOQA in more complex environments like Meltingpot 2.0 would strengthen the experimental part of the paper, but LOQA is being held to a much higher standard than the existing papers in the literature. In LOLA, M-FOS and POLA, the environments are limited to IPD, the Coin Game and some other matrix form games, but we are being asked to run experiments in environments for which baselines do not exist.  Consider that running these experiments with LOQA and other baseline methods requires significant engineering effort.\n\nWe will also add experiments for full history IPD and Chicken. Assuming that we perform these changes and add M-FOS as a baseline, would you be willing to increase your score?"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699737021476,
                "cdate": 1699737021476,
                "tmdate": 1699737021476,
                "mdate": 1699737021476,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YccfvfyahN",
                "forum": "FDQF6A1s6M",
                "replyto": "e08q4Lwe6T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_xNg3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_xNg3"
                ],
                "content": {
                    "comment": {
                        "value": "My first impression of this paper is it was written in a rush. However, if the work has been improved over the months and the authors would update their manuscripts during the rebuttal, this effort should be valued and I would be happy to read it again. \n\nPlease highlight the changes with different colors. Please note that \"increasing scores\" is not guaranteed and the scores will based on the updated version of the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507053468,
                "cdate": 1700507053468,
                "tmdate": 1700507053468,
                "mdate": 1700507053468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8eiUCb3j53",
            "forum": "FDQF6A1s6M",
            "replyto": "FDQF6A1s6M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_F4Df"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3698/Reviewer_F4Df"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a decentralized multi-agent learning algorithm that fosters cooperation among agents even in adversarial settings, which they term as partially competitive environments. They provide experimental validation with Iterated Prisoners' Dilemma and Coin Game. Their key claim is that their proposed algorithm achieves state-of-the-art performance in these games with low computational cost. The authors here assume that each agent has access to the Q values of all other agents or can estimate them using the observations and rewards of all other agents."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors try to address the computational challenges faced by other MARL algorithms for sequential social dilemmas, by proposing an algorithm where each agent maintains an estimate of the Q values of all its opponents in order to determine its own policy improvement."
                },
                "weaknesses": {
                    "value": "1. There are several papers in literature that provide decentralized algorithms to achieve individually and socially optimal solution in sequential social dilemmas. One of the criticism of these papers is the additional information needed by these algorithms, which is often not available in the real-world. This paper also has the same limitations. \n2. I think the novelty in the proposed method is limited based on the papers cited by it. The key idea is that each agent model the opponents policy using the rewards obtained by the opponents."
                },
                "questions": {
                    "value": "1. Can the authors concretely define \"partially competitive settings\"?\n2. What is the information structure assumed for each agent? If each agent can see the entire world state and also observe the actions of the other agent, then the policy for each agent should also depend on the history of the actions of the agents in addition to the current state for no loss of optimality.\n3. How can the opponent's true Q function be replaces with an estimate by the agent? Are we assuming that opponent observations and rewards are common information in the game? This is often not the case in real-worls multi-agent settings.\n4. It is not clear if LOQA will extend to all general-sum games. Also, the sub-optimality for each agent due to the modified objective function needs to be quantified and bounded. Are the authors proposing LOQA only for sequential social dilemmas?\n5. Can the authors establish or reason about the solution concept achieved using LOQA? Will it be a socially optimal solution. In non-symmetric games what will be the extent of sub-optimality for each agent?\n6. By modifying the objective function (returns) optimized by each agent, LOQA changes the underlying game. Can the authors show that a Nash equilibrium (or any refinement of it) of this modified game is a socially optimal solution of the original game?\n7. Also, will decentralized REINFORCE algorithm lead to attaining a Nash equilibrium in the above-defined modified game?\n8. How does this analysis extend to the case when all agents in a game are using LOQA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3698/Reviewer_F4Df"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3698/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828016161,
            "cdate": 1698828016161,
            "tmdate": 1699636326125,
            "mdate": 1699636326125,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FoSn6wozy3",
                "forum": "FDQF6A1s6M",
                "replyto": "8eiUCb3j53",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Strengths:**\n\nWe thank the reviewer for their time. The reviewer states that the strength of the paper lies in *\u201cproposing  an algorithm where each agent maintains an estimate of the Q values of all its opponents in order to determine its own policy improvement\u201d*. We would like to clarify that estimating the Q values of the opponents is not enough to determine a policy improvement and is not a contribution of our work. The key idea of LOQA is that this estimate of the Q value is controllable by the policy of the agent: a LOQA agent exercises this control over the Q values of the opponent to incentivize it to select actions that are beneficial for it. \n\n**Weaknesses:**\n\n1. The reviewer writes that *\u201cThere are several papers in literature that provide decentralized algorithms to achieve individually and socially optimal solution in sequential social dilemmas\u201d*. Besides those that we have included in the literature review we are not aware of the existence of such papers, we would like the reviewer to point them out. For those in the literature review we have clearly enunciated their differences and disadvantages in comparison to LOQA.\n\n2. We believe that the reviewer may have missed the key idea of the paper, namely that the Q value of the opponent can be differentiated w.r.t. the agent\u2019s policy parameters via reinforce. In such a way, it is a fundamentally different algorithm from those that we cite in the paper. This idea enables major computational savings. In contrast to previous works which compute explicit gradients w.r.t to optimization steps or model the problem as a meta-game (both of which are computationally expensive), LOQA uses reinforce estimators that are fast and easy to compute. \n\n**Questions**\n\n1. We agree with the reviewer that \"partially competitive settings\" is not clearly defined. We will replace this term with \u201csocial dilemmas\u201d.\n\n2. Our policies in the Coin Game are GRU policies that aggregate the entire observation history (without actions) up to the current time step through the hidden state propagation. Therefore, the agents condition on the entire observation history. \n\n3. Yes, we assume that the rewards of both agents can be observed by both agents. This might not be the case in many settings, but we believe it is a reasonable assumption as previous works also make it.\n\n4. The reviewer\u2019s concern that LOQA may not generalize to all general sum games is valid, but we intended to demonstrate its usefulness only in social dilemmas.\n\n5. LOQA achieves a Nash equilibrium (tit-for-tat) on one-step history IPD as demonstrated by the experimental results. Beyond that it is difficult to formally characterize the solutions found by LOQA and it is not the scope of the paper.\n\n6. LOQA does not modify the returns optimized by each agent, so this statement is not true.\n\n7. As stated in (6), there is no modification to the underlying game, thus we will not address this question.\n\n8. Then again refer to (6).\n\nWe ask the reviewer to increase their score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699736944959,
                "cdate": 1699736944959,
                "tmdate": 1699737308173,
                "mdate": 1699737308173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O56OiE216o",
                "forum": "FDQF6A1s6M",
                "replyto": "FoSn6wozy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_F4Df"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3698/Reviewer_F4Df"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses. On reading these responses and the comments by the other reviewers, I think I will retain my current score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3698/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670184951,
                "cdate": 1700670184951,
                "tmdate": 1700670184951,
                "mdate": 1700670184951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]