[
    {
        "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models"
    },
    {
        "review": {
            "id": "FsML81KwZb",
            "forum": "AqN23oqraW",
            "replyto": "AqN23oqraW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_vPns"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_vPns"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how to thoroughly evaluating LLMs on its knowledge capability.  Inspired by cognitive science, the authors establish an extensive benchmark that focuses on memorization, understanding, applying and creating respectively. For each capability, the authors have chosen/created new tasks for that capability and evaluate a significant amount of LLMs to draw insights conclusions on those experiments.\n\nThe author also introduces a new metric for knowledge creating, which in the experiments shows a notable correlation to faithfulness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles a timely and important issues which evaluates the LLM capability instead of just evaluating on some tasks. To answer this question seems still hard and the paper has selected various datasets and made sensible grouping to evaluate the four aspect that it mentions. \n\nThe paper has run the experiments for several seasons for now and has shown some interesting trends that correlate with model size. The paper also proposes a novel metric for knowledge creation that seems interesting and notably correlated with faithfulness."
                },
                "weaknesses": {
                    "value": "There are several questions that I think arise after reading the paper, I would consider these just missing some clarity:\nThe paper said that \"Comparing the second-season results with the first season, the rankings of most open-source models have declined\" but Table 2 and Table 3 seem to show results of four levels of the same season.\nWhy COPEN and 2wikiMultiQA are also considered as exclusive?\n\nOne of the potential strength of the paper is to analyse the results on the fresh data of each season that the paper claims; however, we don't find such results in the current version. Meanwhile, the paper draws some conclusions (the ones related to knowledge) that don't seem to be part of the contributions of this particular paper."
                },
                "questions": {
                    "value": "The paper said that \"Comparing the second-season results with the first season, the rankings of most open-source models have declined\" but Table 2 and Table 3 seem to show results of four levels of the same season.\n\nWhy COPEN and 2wikiMultiQA are also considered as exclusive?\n\nHow the knowledge is designed in Figure 2 to apply self contrast metric please?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698524962266,
            "cdate": 1698524962266,
            "tmdate": 1699637157767,
            "mdate": 1699637157767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pnnw1BxPui",
                "forum": "AqN23oqraW",
                "replyto": "FsML81KwZb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for investing your time and expertise in reviewing our work. We are grateful for your recognition of our efforts in the design of framework and the evolving tests, and we are delighted to clarify the concerns and answer the questions you raised.\n\n- *The paper said that \"Comparing the second-season results with the first season, the rankings of most open-source models have declined\" but Table 2 and Table 3 seem to show results of four levels of the same season.*\n\nDue to space constraints, we placed the complete results of the first season in Appendix E.2. In the content, we primarily present the results of the second season (the latest season at the time of submission) and its comparison to the first season, which is reflected in the Rank column for each level.\n\n- *Why COPEN and 2wikiMultiQA are also considered as exclusive?*\n\nTo ensure the reliability of our evaluation, we reached out to the authors of several works, including COPEN and 2WikiMutiQA, at the very beginning of our project. We obtained their **unpublished held-out test sets** for evaluation in KoLA. As explained in the caption of Table 1, \u201cExclusive tasks mean their test sets are newly developed or sponsored by the original authors and were not publicly disclosed.\u201d Based on this criterion, we labeled these tasks as \u201cExclusive.\u201d\n\n- *How the knowledge is designed in Figure 2 to apply self contrast metric please?*\n\nTo more clearly demonstrate the evaluation process of the knowledge creation task, here we use the example in Figure 2 for a detailed explanation. The process can be divided into three main steps: \n\nFirst, the model to be evaluated generates a completion $T$ using only the context $C$ (The Battle of Evesham marked the defeat of Simon de Montfort...), allowing it to freely create content. Second, the model uses both the context $C$ and event knowledge $K$ (i.e., the structured content in the middle part of the figure) as inputs, to generate the other completion $T_{k}$. Third, by contrasting the model's generated results $T$ and $T_{k}$ under these two conditions, we can calculate the rationality of the model's event knowledge creation, $\\partial \\left( T, T_{k} \\right)$ (its ability to freely create subsequent event knowledge, e.g., *Prince Edward\u2019s Escape*). Together with the other two contrast metrics, this comparison ultimately leads to the scoring of the knowledge creation level. We hope this explanation can further aid your understanding of the paper.\n\n- *One of the potential strength of the paper is to analyse the results on the fresh data of each season that the paper claims; however, we don't find such results in the current version.*\n\nWe analyzed the results on evolving tasks compared with known tasks in Section 3 (the second point of \u201cDesign Analysis\u201d), which shows that the known-evolving performance gap is closer at the higher levels, indicating that the high-level abilities are more generalizable. We totally agree that a thorough analysis on evolving data is essential. We plan to execute a retrospective diachronic analysis after completing the first phase (six seasons as planned) and hope to have intriguing findings.\n\n- *Meanwhile, the paper draws some conclusions (the ones related to knowledge) that don't seem to be part of the contributions of this particular paper.*\n\nWe believe that the experimental conclusions of this paper can provide contributions in two key aspects. First, the analysis upon a Bloom\u2019s cognitive taxonomy, a rather unique aspect of this paper, offers a new perspective in analyzing the knowledge capabilities of LLMs. Second, although some speculations about model performance, such as the alignment tax, have been widely discussed, our results provide new experimental support for these speculations. As other reviewers have mentioned, although some conclusions might not be novel, the evaluation results provided by this framework are valuable. Additionally, KoLA, as a long-term maintained benchmark, can be utilized to observe whether these issues show improvement in the future.\n\n**[Final Remark]** Thank you again for reviewing our paper and for the pleased comments. We hope that our response and clarification have addressed your questions and concerns. We sincerely invite you to engage with us if you have more questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141336929,
                "cdate": 1700141336929,
                "tmdate": 1700141336929,
                "mdate": 1700141336929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fuJoWy81KL",
                "forum": "AqN23oqraW",
                "replyto": "Pnnw1BxPui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9199/Reviewer_vPns"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9199/Reviewer_vPns"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed explanations"
                    },
                    "comment": {
                        "value": "I appreciate the authors' detailed feedback. The main contribution of this work is the extensive benchmark with different levels (knowledge, reasoning, etc.) of data of different nature (evolving or not evolving, etc.) with inspiration drawn from cognitive science. However, although I agree with the novelty of this idea and the significant efforts authors have shown in the project as presented in the paper, I am not convinced after reading the paper that the hierarchy with the chosen benchmarks bring any benefits for 1) better understanding LLM capacity 2) the practical development or usage for LLMs. Thus I am not particularly excited of this work but I certainly have no strong arguments to reject this paper either."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593361682,
                "cdate": 1700593361682,
                "tmdate": 1700593361682,
                "mdate": 1700593361682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UgYxmbT5fP",
            "forum": "AqN23oqraW",
            "replyto": "AqN23oqraW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_ryRT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_ryRT"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a world knowledge benchmark for LLM, focusing on three aspects: (1) ability modeling; (2) evolving benchmark built upon emerging corpora and (3) evaluation criteria. The author also presented metrics of SOTA LLM's performance on the benchmark and provided insights observed from the evaluation results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The paper presents a new LLM benchmark with some innovations, including constructing benchmark with emerging corpora, evaluating model's knowledge creation capability.\nS2. The paper evaluated major SOTA LLMs, providing good comparison in model's capability from different perspective.\nS3. The paper reads well, easy to follow."
                },
                "weaknesses": {
                    "value": "W1. Benchmark on emerging corpora is a great idea and it is quite encouraging to see the authors promised to refresh the benchmark regularly. However, it is not clear how to maintain such benchmark in the long term.  \nW2. It is not clear why we need another new LLM benchmark. Given all different benchmarks available publicly, I am not convinced KoLA is a must-have addition. \nW3. It is not clear why the standardized overall scoring can give better idea than simple ranking."
                },
                "questions": {
                    "value": "Q1. The paper identified four knowledge capabilities. It is clear on knowledge memorization and knowledge creation. However, it is vague to distinguish knowledge understanding and knowledge application. Take knowledge understanding as an example, would reading comprehension a task of knowledge understanding or knowledge attention? Why knowledge understanding only has extraction tasks?\nQ2. Benchmark on emerging corpora is a great idea and it is quite encouraging to see the authors promised to refresh the benchmark regularly. However, it is not clear how to maintain such benchmark in the long term. As the time goes by, some evolving benchmark is no longer most up-to-date, how to handle the benchmark data? How would this evolving benchmark interact with the known dataset?  \nQ3. How much does the new KoLA benchmark differ from existing LLM benchmark? There are so many available benchmark published, each focusing on one/multiple capabilities of the model. Why do we need KoLA? What if we combined existing ones? \nQ4. It is not clear the contribution/motivation with standardized overall scoring. As the score would depend on the evaluated models, it will change a lot as more evaluated models would be added in. Also why this is better than simple ranking?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9199/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9199/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9199/Reviewer_ryRT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821577955,
            "cdate": 1698821577955,
            "tmdate": 1699637157625,
            "mdate": 1699637157625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mP4JMl0ZPa",
                "forum": "AqN23oqraW",
                "replyto": "UgYxmbT5fP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to review our work, and for acknowledging our overall design, experimental analysis, and writing. We are glad to answer the questions you've raised.\n\n- Q1: *The paper identified four knowledge capabilities. It is clear on knowledge memorization and knowledge creation. However, it is vague to distinguish knowledge understanding and knowledge application. Take knowledge understanding as an example, would reading comprehension a task of knowledge understanding or knowledge attention? Why knowledge understanding only has extraction tasks?*\n\nThis is a valuable question, and we are pleased to discuss it with you. \n\nIn Bloom's Cognitive Taxonomy [1], *Understanding* can be described as \u201dDetermining the meaning of instructional messages\u201c, typically involving actions like \u201cExplain, Summarize.\u201d *Applying* is described as \u201cCarrying out or using a procedure in a given situation\u201d, generally including actions such as 'Implement, Execute\u2019. Indeed, since the revision of Bloom's theory [2], there has been considerable debate about the demarcation between these two cognitive levels. \n\nDespite ongoing discussions, one consensus principle to differentiate them is whether it involves information and abilities beyond the given content [3]. Consequently, we categorize tasks involving information extraction (which ensures that the output information is faithfully extracted from the input text) as Understanding. It is worth noting that the Knowledge Understanding (KU) level is not limited to information extraction; it also includes the ability of conceptual abstraction, as seen in our investigated task COPEN. Under this criterion, if a reading comprehension question requires only knowledge from the text, we would classify it as a KU task. Once it involves more complex background knowledge and reasoning, it might lean more towards a Knowledge Application (KA) task.\n\nRef:\n\n[1] Krathwohl D R. A revision of Bloom's taxonomy: An overview[J]. Theory into practice, 2002, 41(4): 212-218.\n\n[2] Forehand M. Bloom's taxonomy: Original and revised[J]. Emerging perspectives on learning, teaching, and technology, 2005, 8: 41-44.\n\n[3] Wilson L O. Anderson and Krathwohl Bloom\u2019s taxonomy revised understanding the new version of Bloom\u2019s taxonomy[J]. The Second Principle, 2016, 1(1): 1-8.\n\n- Q2: *Benchmark on emerging corpora is a great idea and it is quite encouraging to see the authors promised to refresh the benchmark regularly. However, it is not clear how to maintain such benchmark in the long term. As the time goes by, some evolving benchmark is no longer most up-to-date, how to handle the benchmark data? How would this evolving benchmark interact with the known dataset?*\n\nAs stated in Section 2 of our paper, we are committed to updating our evaluation for at least six seasons. After this, we will further extend tasks to introduce new versions of KoLA. The data for every new season will be up-to-date to ensure the test data is not leaked, maintaining the relative fairness of the evaluation. For the ended seasons, we do not plan to add them to the known set immediately. Instead, they will be publicly released after the completion of six seasons, and a retrospective analysis of all past seasons will be conducted. The primary workload for running a new season includes collecting and annotating new data, constructing a new list of models, and publishing evaluation results. To host these tasks, we have established a maintenance team and carefully controlled costs. This includes adding a large amount of automated pre-annotation (as described in Appendix B) and controlling the size of the evaluation set. As mentioned in our discussion with reviewer 1m8m, we expect the cost of every season will be under USD 2,000. Therefore, we are confident to fulfill the committed maintenance and we also welcome contributions from the community."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141237230,
                "cdate": 1700141237230,
                "tmdate": 1700141237230,
                "mdate": 1700141237230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2mTTJZa647",
            "forum": "AqN23oqraW",
            "replyto": "AqN23oqraW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_VBLJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_VBLJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposes a world knowledge assessment benchmark KoLA that consists of factual knowledge from Wikidata to evaluate the world knowledge of large language models (LLMs). KoLA consists of a four-level taxonomy: knowledge memorization/knowledge understanding/knowledge applying/knowledge creating. The first two tasks focus on directly extracting/generating the information associated with the corresponding world knowledge, and the last two focus on the application of knowledge in reasoning and text generation. \n\nThe data comes from both the data already exposed to LLMs (known data) and the data appeared afterwards (evolving data). Results show that pre-alignment/instruction-tuning models has higher correlation between model size and knowledge memory performance. However, instruction-tuning empowers the models with high-level abilities (e.g. knowledge applying tasks) and commercial models still have leading performance in general."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The knowledge benchmark fills the blank of thoroughly evaluating world knowledge of current large language models. The taxonomy is carefully designed and rich experiments on model choices are conducted.\n- The ever evolving setup has long-term benefit in considering the generalization problem in knowledge-intensive tasks.\n- The self-contrast metric has a good motivation in balancing the hallucination in knowledge-based generation.\n- The annotation team is of strong educational background."
                },
                "weaknesses": {
                    "value": "- The knowledge-wise strength of Rouge-L used in Eq. 3 doesn't look to be strong enough in capturing the knowledge association especially when T is a free-generation result, maybe replacing the measurement with another model (e.g. a entailment model) would be better? (w/ additional computational cost)"
                },
                "questions": {
                    "value": "Comments\n- The caption of the figures should include necessary details to understand the them if the space allows (e.g. Figure 4)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699024617302,
            "cdate": 1699024617302,
            "tmdate": 1699637157520,
            "mdate": 1699637157520,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j2pqrD5xcX",
                "forum": "AqN23oqraW",
                "replyto": "2mTTJZa647",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your careful review and constructive points. Your assessment stating that our 'knowledge benchmark fills the blank of thoroughly evaluating world knowledge of current large language models' is highly encouraging to us. According to your comments, we have edited the manuscript as your suggestions as follows:\n\n- *The knowledge-wise strength of Rouge-L used in Eq. 3 doesn't look to be strong enough in capturing the knowledge association especially when T is a free-generation result, maybe replacing the measurement with another model (e.g. a entailment model) would be better? (w/ additional computational cost)*\n\nEmploying models for automatic evaluation of knowledge aspects is indeed a valuable suggestion and represents a promising direction. In the design phase of KoLA, we considered such an approach. However, we ultimately chose model-free evaluation metrics to potentially prevent the evaluation model from favoring certain types of models. In the early experiments, we tried various metrics, including some model-based metrics like BertScore, BLEU, etc., and test whether their results align with human evaluations. Bases on these, we finally choose Rouge-L. We plan to continue exploring the integration of your suggested model-based evaluation methods in subsequent seasons. Thank you once again for your insightful recommendation!\n\n- *The caption of the figures should include necessary details to understand the them if the space allows (e.g. Figure 4).*\n\nWe appreciate the reviewer's suggestion to elaborate on this detail. Although we mentioned this part in the first paragraph of Appendix B.4, we overlooked emphasizing this information in the caption of Figure 4. Following your reminder, we have *enriched the captions of all the tables and figures in the paper*, further enhancing the readability of the article.\n\n**[Final Remark]** We thank you again for investing the time and effort to review our paper and for the helpful comments that helped us improve the submission. We hope that our responses will address your concerns on the evaluation metrics and caption details and sincerely invite you to engage with us if you have more questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140996931,
                "cdate": 1700140996931,
                "tmdate": 1700140996931,
                "mdate": 1700140996931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IMnHdJT18s",
                "forum": "AqN23oqraW",
                "replyto": "j2pqrD5xcX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9199/Reviewer_VBLJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9199/Reviewer_VBLJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for the reply and update the presentation of the manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659094085,
                "cdate": 1700659094085,
                "tmdate": 1700659094085,
                "mdate": 1700659094085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MLJop0Utwh",
            "forum": "AqN23oqraW",
            "replyto": "AqN23oqraW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_1m8m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9199/Reviewer_1m8m"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents KoLA, an interesting Knowledge-oriented LLM Assessment benchmark. It assesses LLMs on four-level knowledge-related abilities: knowledge memorization, knowledge understanding, knowledge applying and knowledge creating, with known and evolving data sources.\n\nThe authors plan to make available their data, leaderboard, participation information, and supporting tools upon acceptance. They plan to host  a new competition season every three months, updating their evolving data sources, inviting participations from both open and commercial LLMs. The paper reports analysis of two searsons run comparing 28 open-source and commercial LLMs. \n\nI found the framework to be very interesting and insightful. The community will benefit from such a large scale analysis over knowledge-related abilities with known and evolving sources.  The paper is very well written."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The tools and data from the paper will be released upon acceptance. \n\nThe community will benefit from such a large scale analysis over knowledge-related abilities with known and evolving sources. The presented analysis is already very insightful.\n\nThe breakdown of the task using knowledge-related abilities with known and evolving sources is compelling to assess LLMs evolving capabilities."
                },
                "weaknesses": {
                    "value": "I don\u2019t see any major weaknesses in the paper. \n\nOne could argue that it is just an analysis paper, some of the insights that were drawn here might not be novel. But I feel that the presented framework will be valuable to the community. The authors have done a very good job explaining the framework in detail."
                },
                "questions": {
                    "value": "I would have liked to see the human evaluation results in the main part of the paper. It would strengthen the analysis.\n\nDo authors discuss the costs involved with these studies for both seasons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699483565236,
            "cdate": 1699483565236,
            "tmdate": 1699637157413,
            "mdate": 1699637157413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UF8CI4dYTk",
                "forum": "AqN23oqraW",
                "replyto": "MLJop0Utwh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9199/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback. We are greatly delighted to note your recognition of the contributions our paper makes in terms of data, evaluation design, and results analysis. We are happy to address the questions you\u2019ve raised.\n\n- *I would have liked to see the human evaluation results in the main part of the paper. It would strengthen the analysis.*\n\nAdding the human evaluation to the main text is indeed a good suggestion. However, due to the constraints of length, we attempted but failed to include the results of human evaluation and its background in the content. In our revised paper, we have further highlighted this part (in Section 3), with the aim of directing readers' attention to the corresponding appendix.\n\n- *Do authors discuss the costs involved with these studies for both seasons?*\n\nThis is a valuable concern. To sustain the project over the long term, we have kept the budget for each season below USD 2,000. Generally, each season\u2019s expenditure comprises two main parts: 1) the cost of data annotation, and 2) the expenses for model deployment and API calls. \n\nThe data annotation includes labeling knowledge triples and event arguments. As introduced in Appendix B, to reduce the difficulty and cost of annotation, we have pre-labeled the collected text automatically, allowing annotators to focus on the core tasks. Overall, each season requires approximately USD 660-700 for triple annotation and USD 400-420 for event annotation.\n\nDue to the scale of the KoLA test sets, the costs for model deployment and API calls have been kept within an acceptable range. The deployment expenses, estimated from GPU usage time, are approximately USD 200-240, while the API incurs an additional cost of USD 150-180.\n\nOverall, for the two seasons completed thus far, the costs have not exceeded USD 1,600 per season. With the anticipated increase in the number of models in future seasons, we believe that a budget of USD 2,000 per season should be sufficient to maintain the normal operation of the project.\n\n**[Final Remark]** Thank you once again for recognizing our work and offering invaluable suggestions for improvement. Please feel invited to leave more comments in case you have additional questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140929467,
                "cdate": 1700140929467,
                "tmdate": 1700140929467,
                "mdate": 1700140929467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]