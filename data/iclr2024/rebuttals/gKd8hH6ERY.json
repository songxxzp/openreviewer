[
    {
        "title": "Large Scene Synthesis Controlled With Detailed Text Using View-wise Conditional Joint Diffusion With Hierarchical Spatial Controls"
    },
    {
        "review": {
            "id": "CYhK5QFTqX",
            "forum": "gKd8hH6ERY",
            "replyto": "gKd8hH6ERY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_F2WC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_F2WC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a three-stage system that deals with text-to-image generation with long, detailed text conditions. The first stage utilizes finetuned LLMs to generate bounding boxes and human pose keypoints based on global, group, and instance descriptions. The second stage consists of paralleled streams of diffusion process for regional image generation. The third stage refines the consistency of the whole image through multiple rounds of backward-forward diffusion process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The studied problem is well-motivated and challenging.\n- The high-level idea of the proposed method is straightforward and easy to understand."
                },
                "weaknesses": {
                    "value": "Despite the above strengths, the paper has some severe issues in technical details that I am not sure if they are due to the poor writing and presentation of the method. \n- Each stage of the system lacks important details for readers to fully understand what's behind the results. For example, I don't understand the aligning process in Sec. 3.1 because there are simply no details about what happens. What are the models used to extract all these annotations? It would be much better to specifically write the model names instead of using a generic name. There are more questions to ask: how did you form the instruction-answer pairs? How many pose keypoints are generated for each group layout? In Sec. 3.2, what specifically is \"attention modulation\"? I don't think it's a widely known technique/method name that requires a detailed explanation.\n\n- The method seems like a simple combination of multiple previous methods. VCJD seems like a combination of pre-trained ControlNet+\"attention modulation\" from previous work (Kim et al. 2023b). PPHE seems similar to the superresolution operation proposed in Imagen and other literature. \n\n- The experiments seem not standard and convincing enough. MultiDiffusion is a pretty weak region-guided image generation baseline. Other models like GLIGEN/ReCo/Layout-Guidance should be considered as well. I also feel that there are problems with the evaluation dataset.\n\nMinor:\n- Terms are not well defined. For example, what are global, group, and instance descriptions (especially group)? Why would a group description be necessary? It seems that the system assumes all three types of input from human users."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4963/Reviewer_F2WC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702323117,
            "cdate": 1698702323117,
            "tmdate": 1699636483137,
            "mdate": 1699636483137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "JUlFXEhAva",
            "forum": "gKd8hH6ERY",
            "replyto": "gKd8hH6ERY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_TMzU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_TMzU"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present a novel approach to generating large scenes, providing a detailed analysis of the specific challenges associated with this task. They introduce valuable techniques to address these challenges effectively. Given the paper\u2019s substantial contributions to the field, I believe it warrants acceptance, and I recommend it for publication."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors have developed a method to generate large scenes from textual descriptions, starting by intelligently deducing keypoints and bounding boxes directly from the language input. This approach seems both clever and logical, demonstrating a thoughtful integration of language understanding and scene generation.\n\n\nThe issue of controllability in large scene generation, especially when incorporating additional text descriptions and dealing with a multitude of objects and human figures, remains underexplored. Addressing this challenge is crucial for advancing the field.\n\n\n\nThe authors have taken a commendable initiative to address this issue by compiling a dataset tailored to this specific problem.\n\n\n\nTo infer the bounding box and pose from textual descriptions, the authors employ a language model, a method that is justifiably reasonable and aligns with popular practices in the autonomous driving domain, similar to applications of ChatGPT.\n\nUpon acquiring the bounding box and pose information in textual format, the authors adeptly convert these details into a visual representation. This approach is logical and effectively bridges the gap between linguistic descriptions and visual content.\n\nLeveraging a diffusion model with pose and bounding box conditions is a well-justified approach, especially given my extensive experience in this domain.\n\n\n\nAlgorithm 1 appears to be both feasible in terms of implementation and reasonable in its approach, indicating a well-thought-out strategy.\n\n\nThe technique of randomly swapping adjacent pixels is a novel and logical approach. I am curious to know if this methodology was initially developed by the authors.\n\nThe authors have achieved impressive results.\n\n\n\nBy analyzing Figures 6d and 6e, it becomes apparent that the technique of randomly swapping pixels plays a significant role. In Figure 6e, the skateboard is correctly positioned under the people\u2019s feet, adhering to logical placement. However, in Figure 6d, the skateboard is misplaced and not under the people\u2019s feet, resulting in a scenario that defies common understanding and expectations. This comparison underscores the effectiveness and importance of the pixel-swapping method."
                },
                "weaknesses": {
                    "value": "The title of this paper appears to be excessively lengthy and somewhat perplexing, which could potentially lead to misunderstandings. I suggest revising it for greater clarity and conciseness.\n\n\nThe technique of randomly swapping adjacent pixels is a novel and logical approach. I am curious to know if this methodology was initially developed by the authors.\n\n\n\n\nIn this paper, I deem the ablation study to be of significant importance, warranting a more comprehensive presentation of results and additional figures for a thorough understanding.\n\n\nThere appears to be a technical issue with the PDF (downloaded version) of this paper. Specifically, my computer encounters a crash each time I navigate to the pages containing Figures 2 and 3.  I kindly request the authors to verify and resolve this potential problem to ensure smooth accessibility and review of the material."
                },
                "questions": {
                    "value": "See #Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793373463,
            "cdate": 1698793373463,
            "tmdate": 1699636483059,
            "mdate": 1699636483059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "z9XA4mdp22",
            "forum": "gKd8hH6ERY",
            "replyto": "gKd8hH6ERY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_EaVt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_EaVt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method called DetText2Scene for rendering high resolution images with better consistency with respect to conditioning text prompts. To do so, they first leverage a large language model (LLM) to generate spatial keypoint-box layouts from textual descriptions. Secondly, they designed a view-wise conditioned joint diffusion process that synthesizes images conditioned on these layouts. Finally, they found it beneficial to add a pixel perturbation-based hierarchical component, which consists in progressively upsampling and refining the scene for better global consistency and image quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to follow.\n2. The idea of using a LLM to generate spatial layouts and provide local guidance for large scene image synthesis is original and interesting. \n3. The authors carefully evaluate the layout generation performance and show good performance.\n4. The paper demonstrates superior performance of its approach compared to its two main concurrent MultiDiffusion and SyncDiffusion, in terms of quantitative and qualitative metrics as well as user studies."
                },
                "weaknesses": {
                    "value": "1. Figure 3 is difficult to read.\n2. The evaluation setting of Table 3 in not clear and i didn\u2019t find where this table was mentioned in the experimental section .\n3. The experimental section could be more complete with an evaluation on whether the View-wise conditioned joint diffusion model synthesizes images that are spatially coherent with the bounding boxes (group, human and object masks) on which it is conditioned. Also, there is no quantitative comparison of image quality.\n4. Besides, the ablation study is quite limited as it is only done qualitatively by showing one example. \n5. Some implementation details are lacking."
                },
                "questions": {
                    "value": "1. Could you increase text font size of Figure 3 so that it\u2019s more readable ? Besides, the drawing describing the view-wise conditioned joint diffusion process could explain better how the U-Net is conditioned on the segments.\n2. Could you clarify Table 3 by mentioning it in the experimental section of the main paper? More specifically, how does the experiments in Table 3 differ from Table 1 ? Which region masks did you use to condition R-Multidiffusion?\n3. To evaluate the spatial coherence of synthesized images with their conditioning masks, it would be interesting to include mIoU metrics like in MultiDiffusion paper to evaluate the consistency with respect to the generated layouts by passing the synthesized images through a pretrained segmentor.\n4. Adding FID metric to measure image quality like it is done in the two main concurrent works MultiDiffusion and Syncdiffusion would strengthen the experimental section.\n5. To have a more complete ablation study, I encourage the authors to quantitatively evaluate the benefit of the different components (ie. keypoint, box layout and PPHE) on CLIP score as well as N_human matching performance.\n6. About implementation details, could you specify which patch size and sliding window were used to divide the large-scene image in small sub-regions? Is it the same as in MultiDiffusion ? Does it change depending on the enhancement stage of the pixel perturbation-based hierarchical method ? Besides, I didn't find a comparison of the inference time between DetText2Scene and the two other works MultiDiffusion and SyncDiffusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4963/Reviewer_EaVt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699440989276,
            "cdate": 1699440989276,
            "tmdate": 1699636482980,
            "mdate": 1699636482980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "rtGOqyjcTO",
            "forum": "gKd8hH6ERY",
            "replyto": "gKd8hH6ERY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_WVZo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4963/Reviewer_WVZo"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a multi-step generation method that first generates keypoint-box layout and then uses that to condition on the diffusion process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It is an interesting approach to use LLMs to generate layout first before generating image"
                },
                "weaknesses": {
                    "value": "- Paper is hard to follow. For example, it is unclear what \"prompt key-grounded masks\" and \"prompt dictionary\"are and how they are obtained. The subsection on \"PIXEL PERTURBATION-BASED HIERARCHICAL ENHANCEMENT\" is ambiguous.\n- There's a lot of typos in the entire paper. For example,\n> \"genreation\" in Algorithm 1\n> \"qulaity\" in Sec 4.4\n> \"SyncDiffuison\" in Sec 4.1\n- The paper's title is on Large Scene Synthesis but the proposed method seems to be targeted at generating better people/humans or humanoid figures on images. There's a serious mismatch between the title/task and proposed method."
                },
                "questions": {
                    "value": "- Why is the segmentation mask in Figure 3(a) different than the one in Figure 3(b)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "It is heavily focused on human generation. There may be bias coming from LLMs and the paper's model."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699592707452,
            "cdate": 1699592707452,
            "tmdate": 1699636482895,
            "mdate": 1699636482895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]