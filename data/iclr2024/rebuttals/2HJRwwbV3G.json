[
    {
        "title": "What does the Knowledge Neuron Thesis Have to do with Knowledge?"
    },
    {
        "review": {
            "id": "3kEEVUD9n2",
            "forum": "2HJRwwbV3G",
            "replyto": "2HJRwwbV3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2106/Reviewer_7T91"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2106/Reviewer_7T91"
            ],
            "content": {
                "summary": {
                    "value": "This work revisits the hypothesis that knowledge in pre-trained transformers may be limited to a few neurons. They do this by running two different model editing methods to identify neurons that are responsible for specific syntactic phenomena using the BLIMP minimal pairs dataset. While they do find a small number of mostly local knowledge neurons responsible for syntax, they find that intervening on these neurons is not enough to change model predictions in a robust way.\n\nThe paper also borrows from past work on factual editing, and along with their own results, conclude that the knowledge neuron thesis is flawed because intervening on these neurons is not sufficient to reliably change model behavior. Thus, they call for a more \"holistic\" approach that studies not individual neurons but entire layer structures and attention mechanisms as the motif for interpretability."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper is extremely well-written with clear arguments, and experimentation.\n- The results presented bring a lot of clarity to interpretability of transformers\n- A lot of previous model editing techniques fail to systematically study if effect of edits are just local or if they are systematic, while this work does this very comprehensively.\n- In my understanding, there is no prior work that applies editing techniques to identify \"syntax neurons\" and this aspect of the paper is quite novel"
                },
                "weaknesses": {
                    "value": "One weakness of the paper is that some of the presentation of experiments could be cleaned up substantially. Some specific suggestions for improvements:\n\n- Results in 3.2 (first paragraph) seem quite loaded. This paragraph presents attribution scores, shows that identified neurons have regularity, the affect of causal interventions, how identified neurons have more to do with frequency cues than syntax etc. I think these results could be broken up into their own paragraphs. \n\n- It would also be great to clearly show which results are taken from prior work. I suspect atleast Table-1 and Figure-5(c) is taken from prior work?"
                },
                "questions": {
                    "value": "- I would be interested to know what the authors think interpretability research should focus on i.e. it appears that knowledge is mostly distributed and not necessarily isolated to specific neurons. There is some discussion around using attention weights and the underlying model circuit towards the end but it would be good to have a slightly more extended discussion around this.\n\n- Some missing references on \"decision making circuits\":\n  - Clark et al. 2019 (What Does BERT Look at? An Analysis of BERT\u2019s Attention): They use attention patterns to identify syntactic knowledge in models.\n  - Murty et al. 2023 (Characterizing Intrinsic Compositionality in Transformers with Tree Projections): They identify tree-structured circuits as a way to study generalization in transformers.\n  - Wu et al. 2023 (Interpretability at Scale: Identifying Causal Mechanisms in Alpaca): Finds alignments b/w model hidden states and symbolic algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627849370,
            "cdate": 1698627849370,
            "tmdate": 1699636143153,
            "mdate": 1699636143153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xZf1QwuFfP",
                "forum": "2HJRwwbV3G",
                "replyto": "3kEEVUD9n2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2106/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for the insightful review!\n\nWe agree that the results presented in the first paragraph of section 3.2 are overly compact.  We have reflected that and we will break it into several paragraphs in the updated version of the paper.\n\nTable 1 presents our own results from our newly proposed criteria and datasets. Figure 6c was Yao et al.'s (2023) evaluation. We will include a citation in the figure caption as well to avoid the confusion.\n\n- **Q1: Future interpretability research directions.**\n    \n    In our opinion, we think the recent work that tries to identify \u201ccircuits\u201d in PLMs is a promising direction. Our paper shows that the recall of facts and the expression of linguistic phenomena may follow similar mechanisms but they are certainly much more complex than a simple key-value dictionary.\n    \n    However, the community is still at a very early development stage in creating this circuit mode of interpretation. The circuit identification methods are ad hoc and can only be applied to a small set of tasks. In future work, we will try to formalize the circuit interpretation framework and apply it to more tasks and phenomena.\n    \n    We will extend our discussion to this in the updated draft. And we would like to thank the reviewer for reminding us of the circuit interpretation references.\n    \n- **Q2: Missing references.**\n    \n    Thank you for also reminding us of the Clark reference. We will include it, as well as the circuit references in the updated version of the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079776111,
                "cdate": 1700079776111,
                "tmdate": 1700079776111,
                "mdate": 1700079776111,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lshau2x4wY",
            "forum": "2HJRwwbV3G",
            "replyto": "2HJRwwbV3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2106/Reviewer_c8FP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2106/Reviewer_c8FP"
            ],
            "content": {
                "summary": {
                    "value": "This paper reassess the Knowledge Neuron Thesis in two ways, with syntatic minimal pairs and with generalizing to bijective relationships and synonyms. Theranalysis shows the limitation of current knowledge identification and editing, suggesting the need for more sophisticated understanding of inner mechanism of a language model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper introduces many new practices for the rigorous study of knowledge neuron thesis, including using minimal pairs and t-test.\n2. Broadening the definition of knowledge neural and connecting it to prior works in linguist phenomena.\n3. Through and diverse analysis."
                },
                "weaknesses": {
                    "value": "1. If I have to nitpick, section 4 felt a bit disjoint from the rest of the paper and is not fully fledged."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855403865,
            "cdate": 1698855403865,
            "tmdate": 1699636143088,
            "mdate": 1699636143088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mfPZwwhZY4",
                "forum": "2HJRwwbV3G",
                "replyto": "Lshau2x4wY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2106/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and the recognition!\n\nThank you for the feedback on the flow of the paper. We will reword the end of section 3 and section 4 to improve continuity."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079643411,
                "cdate": 1700079643411,
                "tmdate": 1700079643411,
                "mdate": 1700079643411,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o7UNI7il9c",
            "forum": "2HJRwwbV3G",
            "replyto": "2HJRwwbV3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2106/Reviewer_ZPK4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2106/Reviewer_ZPK4"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the \u201cknowledge neuron\u201d hypothesis in a number of pretrained LLMs, namely, that factual knowledge can be localized to a small number of neurons, and that ablation of those neurons alters the probability of, and/or the final chosen output token. It further extends knowledge to include syntactic or formal knowledge, and similarly finds small number of neurons that can be ablated to suppress their respective represented knowledge, particularly distributed throughout the later layers. However, through previous and additionally proposed metrics, in particular emphasizing bi-directionality and synonym-agnosticism, the authors argue that the discovered knowledge neurons cannot be considered to contain anything like \u201cknowledge\u201d, but simply conserve token correlations found in the training text."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, I found the paper to be clearly written, except for some very technical and linguistics-specific concepts that warrant more explanation (and earlier). Its usage of intuitive examples and graphical illustrations throughout the text was very helpful for me to understand its arguments. Lastly, the experiments seem comprehensive, and convincingly demonstrates the authors\u2019 two main claims: the existence of syntax-knowledge neurons in LLMs analogous to fact-knowledge neurons, and that neither sets of knowledge neurons can be considered to robustly represent \u201cknowledge\u201d."
                },
                "weaknesses": {
                    "value": "Despite its technical soundness, I\u2019m personally struggling to understand the significance of these findings on a larger scale, though I must admit that this is not my field. In particular, I feel that such detailed investigations of LLMs on a more \u201ccognitive\u201d level, i.e., assigning individual neurons to be representing concepts / knowledge wholesale, is orthogonal to dissecting the computational mechanisms of attention-based LLMs, and is more suitable for a conference like ACL. This is not really the fault of this particular paper, but the literature they attempt to address (which are predominantly published in ACL), though ironically, this paper raises exactly the point that such \u201cknowledge neuron\u201d search in LLMs may be ill-advised, given that these models are simply token sequence autocomplete machines. \n\nNevertheless, given its current scope and that we are explicitly asked to assess significance of contribution to the field (of machine learning), I recommend borderline rejection for ICLR (but would otherwise strongly recommend acceptance for, e.g., ACL!). But I would be willing to convinced that it\u2019s within scope if the AC and other reviewers disagree."
                },
                "questions": {
                    "value": "- one of my major concerns is the neuron selection procedure, which a priori limits the number of knowledge neurons to be 2-5. As I understand it, this procedure was not proposed by the authors, but in my opinion this process alone excludes the possibility of distributed representation, a much more reasonable null-hypothesis, resulting in a seemingly important discovery of knowledge neurons but in fact represents very little of the actual computations in the model. This is very reminiscent of the \u201cgrandmother\u201d or \u201cJennifer Aniston\u201d neuron type of work in neuroscience, and narrows the scope of the investigation arbitrarily and prematurely. Some investigation of distributed representation would, in my opinion, increase the impact and reach of this work\n\n- the paper seems to be \u201con the fence\u201d and sometimes explicitly contradicting itself. For example, page 7 states \u201cLMs process and express the two types of knowledge using the same mechanism.\u201d while the high-level conclusion of the paper, iiuc, is that there is no \u201cknowledge neurons\u201d. I think it would improve readability if the authors can find a more consistent messaging.\n\n- the paper references a small number of previous works heavily, and without prior knowledge in the field, it\u2019s hard for me to assess how much of the contributions are novel. The assessment of syntactic knowledge and newly proposed metrics are clearly new contributions, but a small \u201ccontributions\u201d section explicitly and concisely summarizing this would be helpful.\n\n- most of the illustrative examples (Figs 2-6) are on the case of determiner-noun, and some successful examples of the other two cases (subject-verb, gender and number agreement) would be even more convincing. Apologies if I had missed this in the supplemental.\n\n- Formal and functional competence are referenced in the paragraph after figure 1, but without definitions, which can be confusing for a naive reader. The definition in the later paragraph was very helpful, and may be better if moved to be earlier."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2106/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2106/Reviewer_ZPK4",
                        "ICLR.cc/2024/Conference/Submission2106/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2106/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698866532122,
            "cdate": 1698866532122,
            "tmdate": 1700498251409,
            "mdate": 1700498251409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hghDjnLxA1",
                "forum": "2HJRwwbV3G",
                "replyto": "o7UNI7il9c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2106/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2106/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the great review and the insightful feedback!\n\nHowever, we want to respectfully disagree with the reviewer's assertion that our work is studying LLMs on a \"cognitive\" level and not suitable for ICLR. Our paper is about interpreting and understanding the function of each transformer component to bring direct model editing of LMs to fruition. If the knowledge neuron thesis were to hold, direct model editing could be implemented by simply editing the MLP neuron activations or weights. This will significantly enhance the safety and privacy aspects of LLMs. But as we proved in this paper, such a simple fix does not exist. The limitations of current KN-based model editing methods originated from the limitation of the framework that only edits MLP neurons. Future improvement with only simple neuron manipulation may be a futile endeavour. We must resort to a more intricate means of interpretation and manipulation beyond the individual neuron level. Therefore, our work is about interpreting the representations within the transformer architecture learned by LLMs, which has more of a focus on the architecture of transformers than linguistics. This is also why we chose to submit to ICLR rather than an *ACL conference, despite similar deadlines.\n\nThe misconception may have arisen from our citation of formal vs. functional competence work from cognitive science. In fact, in this respect, we are \"anti-cognitive,\" because we dispute such appeals to pseudo-cognition in mechanistic interpretability work. In this paper, we disputed two such points in prior work, i.e., Dai et al.'s (2022) attribution of layer depth to the distinction between syntax and semantics, and both Dai et al.'s (2022) and Meng et al's (2022, 2023) strident use of the term \"knowledge.\" In the paper we called for grounding the interpretation of the computational mechanisms of LLMs to robustly defined, human-interpretable tasks such as syntactic phenomena rather than broad analogies to human cognition.\n\nLastly, we wish to point out that the ICLR references we have cited in our paper are very much in the same vein as our work, and so there shouldn't be a question as to the suitability of our paper to ICLR. Our work re-examines current mechanistic interpretability work (Meng et al., 2023) and, as we have discussed with reviewer @7T91, we concur with the future trend of interpreting LMs with \"circuits\" (Wang et al., 2022). So both sets of papers \u2014 the past and future of our research programme \u2014 were published at ICLR.\n\n- **Q1: Neuron selection procedure.**\n\n  The KN thesis hypothesizes that \"knowledge\" can be localised to a small number of neurons. There are two parts to this thesis: 1) the representation is localised to a small number of neurons, and 2) the representations that LMs capture encode \"knowledge.\" Our paper confirms and extends the first part of the thesis to syntactic tasks, but refutes the second part \u2014 the representation localises complex patterns, but cannot, by any commonly accepted definition, constitute knowledge. We did not change the neuron number limit because the MLP weights can indeed still be interpreted as patterns in a localised fashion.\n\n  We also strongly agree with the reviewer's intuition that knowledge can reside in the network in a much more distributed fashion, and this is the message we are trying to communicate. We further emphasize this in the conclusion of the paper. It is obstinate and unreasonable to claim that LMs do not capture any knowledge at all, yet it is oversimplistic to adopt the view of the KN thesis to claim that knowledge is directly stored in the MLP weights just as in key-value dictionaries. Future breakthrough in mechanistic interpretability must adopt a more distributed approach that looks beyond the MLP weights. We are dedicated to moving towards that goal in our future work.\n\n- **Q2: Consistent messaging.**\n\n  As the reviewer has summarized, the main point of this paper is that \"the existence of syntax-knowledge neurons in LLMs analogous to fact-knowledge neurons, and that neither sets of knowledge neurons can be considered to robustly represent 'knowledge'.\" This is a wording issue. We re-examined the paper and have updated the wordings to make it more consistent. For example, the sentence on page 7 is reworded as \"LMs solve the two types of tasks (syntactic and factual) using the same mechanism.\"\n\n- **Q3: A \"contributions\" section.**\n\n  Such a good point! We will add such a section.\n\n- **Q4: Illustration of other linguistic phenomena.**\n\n  Because of space limitations, we must place the other linguistic phenomena (subject-verb, gender and number anaphor agreement) in the appendix.\n\n- **Q5: Definition of formal and functional competence.**\n\n  Thank you for the suggestion. This will certainly provide the readers with more context. We will move the brief definition of formal and functional competence to the introduction where it first appears."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2106/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079622369,
                "cdate": 1700079622369,
                "tmdate": 1700081531349,
                "mdate": 1700081531349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]