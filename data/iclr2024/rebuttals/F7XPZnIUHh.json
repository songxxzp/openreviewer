[
    {
        "title": "Adversarial Learning of Decomposed Representations for Treatment Effect Estimation"
    },
    {
        "review": {
            "id": "7G2nSzwVqz",
            "forum": "F7XPZnIUHh",
            "replyto": "F7XPZnIUHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_912z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_912z"
            ],
            "content": {
                "summary": {
                    "value": "The main contributions of this paper are the ADR algorithm for decomposed representations in ITE estimation, a precise definition of variables decomposition, and theoretical analysis showing the benefits of this decomposition approach, including the variance lower bound of the CATE estimand. The ADR algorithm demonstrates its effectiveness through empirical validation and can be applied to a variety of treatment data types."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1)The paper introduces the concept of $\\mathbf{I, C, A}$ based on causal graphs and proves that this decomposition can be identified from observational data. \n\n(2) A novel ADR algorithm is proposed, leveraging adversarial modules to ensure independence and conditional independence relations. \n\n(3) This ADR algorithm is applicable to both categorical and numerical treatments and is supported by both theory and empirical results."
                },
                "weaknesses": {
                    "value": "**Presentation**: There are many unclear statements. For example, `The ITE refers to $Y_i(t) \u2212 Y_i(0)$.' why do not write it as $Y_i(1) \u2212 Y_i(0)$? Eq.(1) is only presented for a binary treatment. How to define ITE or CATE for other types of treatments?\n\n\n**Novelty**: The use of decomposed representation for identifying adjustment sets in causal inference has been previously explored in the literature. This paper likely builds upon existing methods and concepts while potentially introducing novel insights or improvements. In essence, several conclusions in the article may have already been substantiated. Additionally, the manuscript does not reference the literature that employs sufficient dimension reduction for learning the adjustment set.\n\n**Contribution**: The conclusion in Theorem 3.2 has been proved by previous work[1,2], and both works also allow latent variables. So, the developed ADR can be regarded as a restricted version of the implementation of these two works. Therefore, the contributions of the work is not high enough for ICLR. \n\n[1] Entner D, Hoyer P, Spirtes P. Data-driven covariate selection for nonparametric estimation of causal effects[C]//Artificial intelligence and statistics. PMLR, 2013: 256-264. \n\n[2] Cheng D, Li J, Liu L, et al. Local search for efficient causal effect estimation[J]. IEEE Transactions on Knowledge & Data Engineering, 2022 (01): 1-14."
                },
                "questions": {
                    "value": "Q1, `To deal with the issue, the common practice is to introduce pre-treatment covariates such that {Y (t)|x} =d {Y |t, x} (ignorability assumption).' Is it correct? If there are only pre-treatment covariates, it implies that there are no descendants of both $T$ and $Y$ in the set of covariates. How can we ensure $ignorability$ hold?\n\nQ2, Eq.(2): `E[Y(t)|x]=E[Y(t)|x,T =t]=E[Y|x,T =t]', Can we really transform the potential outcome prediction problem into a supervised learning problem? \n\n\nQ3. For the causal DAG in Fig. 1 (b),  does ADR also apply when $X_1$ is an unobserved variable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Reviewer_912z"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697589569992,
            "cdate": 1697589569992,
            "tmdate": 1699636182931,
            "mdate": 1699636182931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "exASLmnuoC",
                "forum": "F7XPZnIUHh",
                "replyto": "7G2nSzwVqz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- Q1 [ignorability assumption]\n\ni) To our understanding, we believe that *\"introducing all the pre-treatment covariates\"* is a safe choice, but may not be an efficient one.\nActually, the quoted sentence was referenced from [1]. We have also mentioned this in the paper (the 1st line of page 2).\nThe full discussion can be accessed from https://ftp.cs.ucla.edu/pub/stat_ser/r345-sim.pdf.\n\nii) *If there are only pre-treatment covariates, it implies that there are no descendants of both $T$ and $Y$ in the set of covariates. How can we ensure ignorability hold?*\\\nThe **ignorabity** assumption means that $\\boldsymbol{X}$ blocked all the \\textit{\\bf{back-door paths}} from $T$ to $Y$ in the form $T\\leftarrow \\cdots Y$.\nTherefore, the descendants of $T$ and $Y$ are not necessary to block such paths.\n \n\nReferences:\\\n[1] DB Rubin. Author\u2019s reply (to judea pearl\u2019s and arvid sj\u00f6lander\u2019s letters to the editor). Statistics in\nMedicine, 28:1420\u20131423, 2009.\n\n\n\n- Q2 [identification of E[Y(t)|x]]\n\nSure, under the ignorability assumption, $\\mathbb{E}[Y(t)|x]=\\mathbb{E}[Y|x, t]$, which is the common practice in literature of treatment effect estimation.\nHowever, we care about the potential outcome for all the units, instead of the ones that receive treatment $T_i=t$.\nThat is, we need to estimate the counterfactual outcome $Y_i(t)$ for the units with $T_i\\neq t$ \nThis is why the inference of potential outcomes (and also the CATE) is more challenging than the common supervised learning problem.\n\n\n- Q3 [for the case with unobserved variables]\n\nThanks for the question.\nIn this case, since $X_1$ is not a confounder, ADR can be applied when $X_1$ is unobserved."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469573627,
                "cdate": 1700469573627,
                "tmdate": 1700469573627,
                "mdate": 1700469573627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MyQepsVI2Q",
                "forum": "F7XPZnIUHh",
                "replyto": "exASLmnuoC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_912z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_912z"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your rebuttal. \n\n\n(1) Regarding the assumption of ignorability, it is advisable to adhere to the standard assumption. The assumption of pretreatment alone is insufficient for establishing ignorability, as is well understood in the field.\n\n(2) Regarding  `E[Y(t)|x]=E[Y|x,T =t]',  the notation is confusing. E[Y|x, T =t] is a standard supervising learning model in your current notation. So it would be better to make this clear.\n\n(3) In fact, there is no confounding bias between $T$ and $Y$ in Fig. 1 (b) since the back-door path $T<-X_1->X_2<-X_3->Y$ is naturally blocked by the empty set. How can ADR remove the information of $X_2$ from the adjustment set $\\mathbf{C}$ if $X_1$ is unobserved or $\\{X_1, X_3\\}$ are unobserved (M-bias)?  \n\nNote that in Fig. 1(b), $X_2$ is not an IV based on the standard definition of IV. Hence, it would be better to change another name for this."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545368232,
                "cdate": 1700545368232,
                "tmdate": 1700545368232,
                "mdate": 1700545368232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3JsCXmjOM1",
            "forum": "F7XPZnIUHh",
            "replyto": "F7XPZnIUHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_sLak"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_sLak"
            ],
            "content": {
                "summary": {
                    "value": "**Post-rebuttal update**: I maintain my score but add my final reply to the author(s).\n\n*On Thm 3.1* It seems the quotation agrees with my understanding, and I guess the \"their\" does not refer to $\\hat\\theta_n$. All in all, do you agree that this result is not essential for the method? If so, I still suggest moving it to the Appendix. Otherwise, you should explain its importance together with clearer writing.\n\n*On Def 3.1* I believe we agree that there are admissible sets diff from yours and might provide better finite sample performance. So you need to admit and explain this, particularly because your defs of I, C, A are diff from usual.\n\nI still do not agree this method is theoretically guaranteed in a strict sense. You may want to explain in what sense you mean it.\n\n**End update**\n\nAs for the problem of treatment effect estimation under unconfoundedness, the paper proposes to decompose the observed covariates into three disjoint sets, roughly corresponding to the usual concepts of instrumental, confounding, and adjustment variables. The variables are defined in graphical terms, and then reduced to independence relationships, by Theorem 3.2. An adversarial learning approach is proposed to induce the independence. Experiments show the benefits of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The separation of covariates into three disjoint sets using Def 3.1 is an interesting idea.\n\nThe main theoretical results seem correct (proofs not checked, but I have my own (rough) proofs and cannot come up with counterexamples).\n\nExperiments show the representations are practically decomposed, and the ablation study shows the usefulness of the theoretical ideas."
                },
                "weaknesses": {
                    "value": "**Some problematic theoretical developments and discussions**\n\n*Th 3.1* (variance lower bound). The statement seems incorrect or has typo(s). For CATE, the bound should depend on the value of x, but your eq of V takes expectation on X. Moreover, for consistent estimators, V should depend on n, and V \u2192 0 as n \u2192 inf, but your V is a constant wrt n. Anyway, I don\u2019t see this result has a strong relationship to the method (or else I will give a lower score), you could remove this result if you cannot fix it.\n\n*Def 3.1* deviates from standard notions in the literature and also has practical limitations. For example, \n\n- in your Fig 1a, if there is an X4 that is a parent of both X1 and X2, this is usually understood as a confounder and can improve the precision of estimation, but is excluded from your approach.\n- your Fig 1b is actually the \u201cM-bias\u201d case (see model 7 [here](https://ftp.cs.ucla.edu/pub/stat_ser/r493.pdf)). X1 and X3 both satisfy the backdoor criteria and could be understood as confounders. Here, besides X3, both X1 and X2, which are excluded from your approach, could possibly improve the precision (though X2 alone is a bad control).\n\nI suggest being clear that the definition is nonstandard, discussing and comparing it to usual notions (possibly in the Appendix). In particular, you should mention there are possible variables in your IVs that are good to add as controls. \n\nSee Questions for more comments.\n\n**The method is theoretically motivated but *not* theoretically guaranteed.** \n\nProp 3.2 seems correct but the learning approach is not sufficient. Taking (i), I agree that independence means larger L_A than dependence, but, there can be many different functions A that give the independence. Worse, some A could take a confounder but \u201ccleverly\u201d through away the dependence on T. Similar comments apply to (ii). Could your theory rule out these concerns?\n\nThe ADR algorithm does not precisely enforce the required independence or even the approach in Prop 3.2, because L_A, L_C, L_I contain both prediction and adversarial terms, so the ADR is a trade-off but not a direct implementation of the theory. Moreover, training the losses together with hyper-parameters adds yet another layer of trade-off.\n\nI suggest weakening the claims on this contribution."
                },
                "questions": {
                    "value": "I will read the rebuttal and revised paper and raise my score to 6 if the issues/questions in Weaknesses are addressed. Some further points are as below.\n\nProp 3.1 (i) I think we can say \u201ceither\u2026or\u2026\u201d which is stronger than simply \u201cor.\u201d Also, it is safer to say \u201cX \\indep T and X \\indep Y\u201d which is weaker than the joint independence and seems enough. \n\nIt is confusing to only stress C in the last statement of Th 3.2. In fact, A may also be sufficient, as in your Fig 1b.\n\nThe comments below Th 3.2 are confusing. It is an identification because the 3 sets of variables are determined by the observable joint distribution, through the conditional independence requirements. In fact, the definition of I/C/A implicitly assumes graphical structures, and you reduce the graphical structure to independence by *causal Markov and faithfulness assumptions*. Indeed, these *are* the \u201cfurther assumptions\u201d you also use.\n\nAdd experiments that directly evaluate identification and decomposition. Actually, Fig 3 and 5 show the method does not fully identify and decompose the covariates. Thus, it is meaningful to examine this more closely. For example, we could build several datasets with only one I, C, A respectively, and plot the learned I, C, A against the truth.\n\nAs to identifiable representation, the recent advance in using deep identifiable model (e.g., [1]) to estimate treatment effect (e.g., [2, 3]) is worth discussing in the related work. \n\n[1] Khemakhem, Ilyes, et al. \"Variational autoencoders and nonlinear ICA: A unifying framework.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\n[2] Wu, Pengzhou Abel, and Kenji Fukumizu. \"beta-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap.\" International Conference on Learning Representations (2022).\n\n[3] Ma, Wenao, et al. \"Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data.\" International Conference on Medical Image Computing and Computer-Assisted Intervention., 2023.\n\nMinor (did not affect the score):\n\nIt is bad to use the abbreviation ITE for the Individual-level Treatment Effect. Maybe you could use \u201cILTE\u201d instead. Actually, \u201cITE\u201d in your paper refers to both ILTE/CATE and eq1, which is the correct definition of ITE.\n\n\"Adjustment variables\" usually mean the set of variables conditional on which the confounding is removed. Only in some ML papers do adjustment variables refer to those variables that affect Y but not T. This is another often-seen misnomer in the ML community.\n\nThe \\mathcal(L) in Prop 3.2 should be a typo."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Reviewer_sLak",
                        "ICLR.cc/2024/Conference/Submission2464/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698073723491,
            "cdate": 1698073723491,
            "tmdate": 1700836112198,
            "mdate": 1700836112198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ev8xcY0OMU",
                "forum": "F7XPZnIUHh",
                "replyto": "3JsCXmjOM1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your efforts and valuable comments on our paper. We address your concerns below:\n\n- Q1 [questions on the Thm 3.1]\n\nThanks for the interest in Thm 3.1. It seems like there might be some understanding due to this result.\n\t\ni) *Why the variance bound is constant w.r.t. $n$.*\n\nPlease note that Thm 3.1  is about the \\textit{asymptotic variance bound}, instead of the \\textit{variance} of the estimation. \n\tThe asymptotic variance refers to the variance of the limit distribution, instead of the limit of the variance.\n\tPlease allow us explain this with a simple example. \n\tSuppose that $\\{X_i\\stackrel{i.i.d.}{\\sim}N(\\mu, \\sigma^2)\\}$ and $\\hat{\\mu}=\\frac{\\sum_i X_i}{n}$ is the estimation of $\\mu$, then $\\sqrt{n}(\\hat{\\mu}-{\\mu})\\stackrel{d}{\\rightarrow} N(0, \\sigma^2)$.\n\tIn this case, the asymptotic variance of $\\hat{\\mu}$ is $\\sigma^2$, and the variance of $\\hat{\\mu}$ is $\\sigma^2/n$. Please kindly refer to [1] for the formal definition.\n\t\nii) *Why the variance bound is not relevant to $x$*. \n\nSuppose that $\\tau_0$ is the underling value, and $\\widehat{\\tau}(x)$ is the estimation. Then the variance of $\\tau(x)$ refers to $\\mathbb{E}[\\tau_0-\\widehat{\\tau}(X)]^2$, a real number. The conditional variance  $\\mathbb{E}[\\tau_x-\\widehat{\\tau}(X)]^2$ is a function of $X$.\n\nReferences:\\\n[1] Van der Vaart A W. Asymptotic statistics[M]. Cambridge university press, 2000.\n\n\n- Q2 [questions on Def 3.1]\n\nThe terms \"Instrument variables\" and \"Adjustment Variables\" were borrowed from the notations from DeR-CFR [2].\nWe admit that such names may run at a risk of deviating from the standard notions. \n\ni)  *``in your Fig 1a, if there is an $X_4$ that is a parent of both $X_1$ and $X_2$, this is usually understood as a confounder and can improve the precision of estimation, but is excluded from your approach.\"*\n\nIn this case, $P(Y|X_2, X_3, do(t))=P(Y|X_2, X_3, X_4, do(t))$, which means that adding $X_4$ is spurious after including $\\{X_2, X_3\\}$.\nMore generally, we may prove that $p(Y|do(t), \\boldsymbol{X})=p(y|do(t), \\boldsymbol{C}, \\boldsymbol{A})$. We have added this result in Prop 3.2.\n\nii) *``your Fig 1b is actually the \u201cM-bias\u201d case (see model 7 here). X1 and X3 both satisfy the backdoor criteria and could be understood as confounders. Here, besides X3, both X1 and X2, which are excluded from your approach, could possibly improve the precision (though X2 alone is a bad control).\"*\n\nIn the example of Fig 1b, there is no unblocked back-door path(s) from $T$ to $Y$. Therefore, we could not agree that $X_1$ and $X_3$ could be called as confounders. However, both $X_1$ and $X_3$ are admissible set in [3] that satisfies $p(y|x_1, do(t))=p(y|x_1, t)$ and  $p(y|x_1, do(t))=p(y|x_1, t)$.\n\n\nReferences:\\\n[2] Anpeng Wu, Junkun Yuan, Kun Kuang, Bo Li, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei\nWu. Learning decomposed representations for treatment effect estimation. IEEE Transactions on Knowledge and Data Engineering, 35(5):4989\u20135001, 2022.\\\n[3] Pearl J, Paz A. Confounding equivalence in causal inference[J]. Journal of Causal Inference, 2014, 2(1): 75-93.\n\n\n- Q3 [questions on Prop 3.2]\n\nThe learning approach is a direct implementation of Prop 3.2.\nThe $\\hat{h}\\_{A\\rightarrow T}(\\cdot)$ in Prop 3.2 corresponds to the ancillary predictor $h_{A\\rightarrow T}(\\cdot)$.\nProp 3.2 claims when $\\mathcal{L}_A$ is maximized when $A\\perp T$, so we maximize $\\mathcal{L}_A$ in the learning process.\nCould you please be more specific on the question *``Worse, some A could take a confounder but \u201ccleverly\u201d through away the dependence on T. Similar comments apply to (ii). \"*."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469389993,
                "cdate": 1700469389993,
                "tmdate": 1700469389993,
                "mdate": 1700469389993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AhD8ik16gd",
                "forum": "F7XPZnIUHh",
                "replyto": "3JsCXmjOM1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_sLak"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_sLak"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. However, most of my concerns remain.\n\n**On Thm 3.1**\n\ni) In your example, I would call $\\sigma^2/n$ as the \u201casymptotic variance\u201d of $\\hat\\mu$, while $\\sigma^2$ is the asymptotic variance of $\\sqrt{n}\\hat\\mu$. Could you give the relevant page number(s) to the book you referred?\n\nii) Here, the estimator is $\\hat\\tau$, a function of a *fixed* (non-random) x, while an estimator is by definition a function of a *random* sample. Thus when we talk about the variance of the estimate $\\hat\\tau$, we see $\\hat\\tau$ as an RV depends on the sample, and the expectation should be taken on the sample (but not the RV X). Also, I do not understand your distinction between $\\tau_0$ and $\\tau_x$, actually, when you say \u201cthe variance of\u00a0$\\tau(x)$\u201d, the conditional on x is implicitly introduced. You might refer to another thing, but this confusion should be addressed.\n\n**On Def 3.1**\n\ni) That X4 can be removed means that X2 and X3 are sufficient for controlling the confounding, but *not* that X4 is spurious. Including X4 can improve the precision of estimation, similar to those settings where variables are \u201cpossibly good for precision\u201d in [1].\n\nii) Please refer to [1], which says the path T-X1-X2-X3-Y is a back-door path (conditional on X2). Specifically, the sets {}, {X1}, {X3}, {X1, X2},  {X2, X3}, {X1, X3}, {X1, X2, X3} all satisfy the back-door criterion, that is, all the subsets of {X1, X2, X3} *except* {X2}.\n\n[1] Cinelli, Carlos, Andrew Forney, and Judea Pearl. \"A crash course in good and bad controls.\"\u00a0*Sociological Methods & Research*\u00a0(2022): 00491241221099552. **Do read this one** (linked in my original review) and you will come back and thank me. (Pearl himself is an author and referred to it several times on Twitter.)\n\n**On Prop 3.2**\n\nI do not agree \u201cThe learning approach is a direct implementation of Prop 3.2\u201d. There was a typo in my review and I should have written \u201c*throw away the dependence on T*\u201d, by which I meant, that for a confounder C, there can be an A such that A(C) \\indep T."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495082679,
                "cdate": 1700495082679,
                "tmdate": 1700499782726,
                "mdate": 1700499782726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6KB8uhckjw",
            "forum": "F7XPZnIUHh",
            "replyto": "F7XPZnIUHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_8Xzt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_8Xzt"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses efficient estimation of Conditional Average Treatment Effects (CATE), working primarily in the case where the covariate set is high-dimensional and contains different kinds of pre-treatment covariates (e.g., confounders, IVs, variables only affecting the outcome)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "See below for a contextual discussion of strengths and perceived weaknesses."
                },
                "weaknesses": {
                    "value": "In my view, this paper strikes me as overall well-written and motivated (albeit somewhat heavy on notation which could limit its broader impact). The assumptions used in the paper are standard for observational inference (which I view as a strength of the paper). The point that variance bounds are affected by pre-treatment covariate number and that distinguishing between kinds of pre-treatment estimation variance bounds in an effort to improve the bound is intriguing, as is the notion that we can distinguish between pre-treatment covariates of different types in an identified manner. \n\nMy main comments concern the ability of readers to evaluate the contribution of the paper in view of the literature. For example, what is the relationship between the work on semiparametric efficiency bounds in effect estimation with some of discussion here. The literature on, e.g., semi-parametric efficiency is often focused on ATE (as opposed to CATE estimation as here), but even a discussion of the efficiency of the approach here for the ATE vs. in that setting would be most informative for this reviewer. \n\nOn a related note, the paper could further improve its contribution by evaluating observational ATE recovery against some of the most commonly used methods for that (e.g., doubly robust methods and something simple like inverse propensity score weighting). If readers can see that the proposals here by improving observational CATEs also improve observational ATEs (which have extremely broad applicability in existing applied work, much more than observational CATEs), the paper's contribution would be enhanced. \n\nOn another note, the decomposition of I(X), C(X), and A(X) would be extremely useful in practice. However, one limitation is that in any given experiment, we cannot know/validate for sure (and if there is good a priori reason to suspect a covariate is an I, C, or A adjustment could proceed directly with that knowledge). Nevertheless, if the authors could obtain a case (perhaps from, e.g., the biological context where biophysical relations are approximately known) where the decomposition provides useful information to the investigator, I would think the contribution would also be improved. By the way, it would be very convincing if the approach here was somehow better than using a priori knowledge of the decomposition directly. \n\nA few small comments: \n\n(1) Not to sound pedantic, but the writing at the sentence/paragraph level is somewhat stronger than across sections. For example, there is much discussion of the variance bound in the theory section, but this emphasis disappears later on. The paper can sometimes feel disjointed (as if separate contributions are fused). \n\n(2) I would edit the \"Algorithm 1\" text to remove the reference to (I believe) the specific optimizer Adam. Optimizers will come and go with time and presumably, the contribution here is more general, and other optimizers would work as well in principle."
                },
                "questions": {
                    "value": "One question concerns whether the authors intend investigators to actually examine the inferred decomposition of X, or whether the motivation is mainly or exclusively efficient CATE estimation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803611072,
            "cdate": 1698803611072,
            "tmdate": 1699636182767,
            "mdate": 1699636182767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1fmxCfaIWC",
                "forum": "F7XPZnIUHh",
                "replyto": "6KB8uhckjw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your efforts and valuable comments on our paper. Our answers are as below:\n\n- Q1 [the purpose of decomposition]\n\nThanks for the question. \nWe admit that the original purpose is to facilitate a more efficient CATE estimation.\nHowever, we believe that such decomposition is beneficial for further analyses on the covariates, especially in the case where interpretation is important.\n\n- Other Comments\n\nAs suggested, we haved revised Algorithm 1 remove the reference to  the specific optimizer Adamin the updated version. Thanks for the kind suggestion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468675921,
                "cdate": 1700468675921,
                "tmdate": 1700468675921,
                "mdate": 1700468675921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YzMsri0PFn",
            "forum": "F7XPZnIUHh",
            "replyto": "F7XPZnIUHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_Yi3t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_Yi3t"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the disentanglement of instrumental variables (I), adjustment variables (A), and confounders (C) (distinguished according to their dependence on the treatment and outcome variables) from covariates for causal effect inference.\nThey provide an identifiable definition of these variables and a method based on adversarial training in which two discriminators predict the treatment and outcome variables from A and I, respectively, and the representation extractors for A and I counter them."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Since the representation balancing for CATE estimation is pointed out as not capturing the whole CATE estimation errors in literature, representation decomposition is a promising direction as a response.\n* In this context, the first identifiable formulation of representation decomposition through an adversarial formulation would be a very mainline approach.\n* A simulation-based experiment clearly illustrates its superiority in disentanglement performance compared to some existing methods."
                },
                "weaknesses": {
                    "value": "1. The aim is not clear. The disentanglement itself seems to be the aim, and it is not clear how it contributes to the accuracy of the CATE (see Question 1).\n1. The design of the loss function is somewhat heuristic and a logical explanation or guarantee is insufficient (see Question 2).\n1. The adversarial joint objective is not in a convex-concave formulation, which means there is no guarantee of convergence. Intuitively, it seems very unstable.\n    * Are there any existing studies of such a formulation that *maximizes* the loss function such as the MSE?\n    * While maximizing the MSE by the adversary is easily accomplished by making the predictions infinity, it seems to be difficult to predict it accurately.\n    * It may be helpful to show realistic convergence using a learning curve."
                },
                "questions": {
                    "value": "1. What is the purpose of the decomposition? The original purpose was to combine weighting only w.r.t. the confounders in DR-CFR, in my understanding. Confounder variables should be limited to necessary ones to alleviate the estimation variance due to extreme weights. The proposed method does not use weighting and thus I am confused about its aim.\n    1. A possible reason for the above question is to limit the input, i.e., excluding instrumental variables from the input of the predictor, as suggested in Thm 3.1. Although, Thm 3.1 is only about the variance lower bound and I am not sure if that is dominant or critical in the estimation error. Does excluding I(x) from the input of the predictor really have a decisive impact? Any theory about the whole risk bound of the proposed method, or an ablation experiment on the \"with-I(x) model\" $f_{C\\cup A\\cup I\\cup T \\to Y}$ instead of $f_{C\\cup A\\cup T \\to Y}$ might provide empirical evidence.\n1. Why $L_A$ does not include the accuracy of $f_{C\\cup A\\cup I \\to Y}$? A(x) is input to $f_{C\\cup A\\cup I \\to Y}$, but the gradient for the connection is stopped. Does not this have any negative impact on the whole design of the optimization procedure?\n\nMinors:\n\n* P4 Theorem 3.2 stats -> states\n* P4 Hassanpour & Greiner (2020). -> [Hassanpour & Greiner (2020)]."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Reviewer_Yi3t"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815155146,
            "cdate": 1698815155146,
            "tmdate": 1699636182672,
            "mdate": 1699636182672,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EYxrjSQBNS",
                "forum": "F7XPZnIUHh",
                "replyto": "YzMsri0PFn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your efforts and valuable comments on our paper. We address your concerns below:\n\n- [Q1. purpose of the decomposition]\n\nWe agree that it is important to answer the question for implementing such decomposition for the ITE estimation.\nIt is absolutely correct that the purpose is to `` limit the confounder variables to necessary ones to alleviate the estimation variance due to extreme weights\". \nHowever, it should be noted that, even we do not adopt the inverse probability weighting method, the estimation variance also increase with the aggravation of data imbalance as Thm 3.1 suggests.\nWe may have a further understanding by reviewing the development of the literature.\n\n\nFirstly, the balanced representation leaning algorithms [1, 2] were proposed to reduce the data imbalance caused by selection bias. \nExcept from the empirical evidence, [2] explained the benefit from the view of generalization bound.\n\nSecondly, [3] pointed out that leaning the balanced representation alone may overlook the necessary confounding information and hence should learn the decomposed representation. However, the loss terms of DR algorthm in [3] was not complete and hence was not able to learn the decomposed representations. [4] proposed the DeR algorithm to realize the decomposition learning under the situation of binary treatment and response.\nTo deal with the case for continuous treatment and adapt to both categorical and continuous response, we analyzed the theoretical property of the decomposed representations and propose the ADR algorithm.\n\n\n\n\nReferences:\\\n[1] Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual\ninference. In International Conference on Machine Learning, pp. 3020\u20133029. PMLR, 2016.\\\n[2] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In International Conference on Machine Learning, pp. 3076\u20133085.\nPMLR, 2017.\\\n[3] Negar Hassanpour and Russell Greiner. Learning disentangled representations for counterfactual\nregression. In International Conference on Learning Representations, 2020\\\n[4] Anpeng Wu, Junkun Yuan, Kun Kuang, Bo Li, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei\nWu. Learning decomposed representations for treatment effect estimation. IEEE Transactions on\nKnowledge and Data Engineering, 35(5):4989\u20135001, 2022.\n\n- [Q2. the design of $\\mathcal{L}_A$]\n\nWe have added the loss of $f\\_{A\\cup T\\rightarrow Y}(\\cdot)$ in $\\mathcal{L}\\_A$ to learn the representation that are predictive to $Y$.\nThe loss were based on the result in Thm 3.2.\nWe minimize the loss of $f\\_{A\\cup T\\rightarrow Y}(\\cdot)$ to realize $\\boldsymbol{A}\\not\\perp Y$ and\nmaximize the loss of $h\\_{A\\rightarrow T}(\\cdot)$ to realize $\\boldsymbol{A}\\perp T$.\n\n\n- [Q3. the convergence guarantee]\nThanks for the insightful question.\n\ni) ``*Are there any existing studies of such a formulation that maximizes the loss function such as the MSE?*\"  \n\nA similar formulation is that in GAN [5], the generator is updated by maximizing the cross-entropy loss (the discriminator us undated by minimizing the cross-entropy loss). In this case, the optimal predictor has an explicit solution and such that the convergence can be readily proved (see Sec 4.2 in [5])\n\t\nii) ``*While maximizing the MSE by the adversary is easily accomplished by making the predictions infinity, it seems to be difficult to predict it accurately.*\"\n\nPlease kindly note that in maximizing the loss $\\\\mathcal{L}\\_A^h:=\\sum_i l(h\\_{A\\\\rightarrow T}(A(\\\\boldsymbol{x}_i)), t_i)$, \nthe $h\\_{A\\rightarrow T}(\\cdot)$ is fixed and supposed to be (close to) the optimal predictor given $A(\\cdot)$. \n\tAs is suggested by the proof of Prop 3.2, \n\t$\\mathcal{L}\\_A^h\\approx Var[\\mathbb{E}[T|A(X)]]=Var(T)-\\mathbb{E}[Var[T|A(X)]]$ that is less equal than $Var(T)$ and is maximized when $A(X)\\perp T$.\n\tPractically, we restrict the values of representation networks by the $\\mathcal{L}_O$ in eq (7) such that $(\\sum \\overline{W}[k]-1)^2$ is close to zero.\n\t\niii) ``*It may be helpful to show realistic convergence using a learning curve.*\" \n\nThanks for the suggestion. We have checked the tensorboard results saved in the training process. For example, in the case of Synthetic Dataset (continuous treatment). It can be seen that $\\mathcal{L}_A^h$ has an oscillation at the very beginning and then starts to increase and end at a steady level after 2.5k steps.\n\tWe have added the relevant results in the Supplementary.\n\tUnfortunately, we have not yet come up with a way to theoretically prove the convergence for continuous treatment.\n\tWe suppose that the result would be similar to Proposition 2 in GAN [5].\n\t\n\t\nReferences:\\\n[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Y. Bengio. Generative adversarial nets. In Neural Information Processing\nSystems, 2014."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468431201,
                "cdate": 1700468431201,
                "tmdate": 1700468526540,
                "mdate": 1700468526540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mSWur75btp",
            "forum": "F7XPZnIUHh",
            "replyto": "F7XPZnIUHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a new decomposed representation learning method for conditional average treatment effect (CATE) estimation. It is based on a theoretic property that all the covariates in the valid adjustment set can be either instrumental variables, adjustment variables, confounders, or background noise variables, and that this this decomposition is identifiable from the observational distribution. The paper then develops an adversarial learning technique to decompose the covariates into three categories of instrumental variables, adjustment variables, and confounders. The authors compare their method, namely, adversarial learning of decomposed representations (ADR), with the existing representation learning baselines for CATE estimation on several synthetic and semi-synthetic benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is clearly written and well-structured. I found the theoretic results of the paper regarding the decomposition of the insightful and important for representation learning for CATE. For example, I appreciate that the authors provided formal identification guarantees for the decomposed representation, i.e., Prop. 3.1 and Theorem 3.2. Also, the experimental results on decomposing, i.e., Figures 3-5, are very informative."
                },
                "weaknesses": {
                    "value": "There are several issues in this paper:\n  1. Error in derivations. I spotted two issues. First, Theorem 3.2 claims that $\\mathbf{C}$ is a valid set $\\mathbf{X}\u2019$ in the definition of the instrumental variables. On the other hand, by looking at the example in Fig. 1 (b), $\\mathbf{C} = \\varnothing$, but $X_2 \\notindependent Y \\mid T$. Second, there seems to be an erroneous statement in the proof of Prop. 3.2, that the equality in the expectation $\\mathbb{E} (T \\mid A(X) ) = \\mathbb{E}(T)$ implies the independence, $T \\independent A(X)$, which is not true, if $T$ is continuous. Specifically, there could be inequalities wrt. to higher moments. Those two issues are further very important for the correct implementation of the ADR.\n2. Novelty. The implementation of the decomposed representation learning with adversarial representations, namely, ADR, was already proposed in [1], and this work is not even mentioned in the related work or included as a baseline. Therefore, the paper has only a marginal contribution.\n3. Implementation and tuning. Some details are missing on the implementation of the baselines, e.g., the dimensionalities of the representations.  Also, the authors did not provide any details on how to choose the dimensionalities of the decomposed representations in their method, which is a very important issue in practice, e.g., for the IHDP benchmark. Therefore, it is impossible to say, whether the empirical evaluation was fair.\n\nI am open to raising my score if the authors address all my concerns. \n\nReferences:\n[1] Chauhan, V. K., Molaei, S., Tania, M. H., Thakur, A., Zhu, T., & Clifton, D. A. (2023, April). Adversarial de-confounding in individualised treatment effects estimation. In\u00a0International Conference on Artificial Intelligence and Statistics\u00a0(pp. 837-849). PMLR."
                },
                "questions": {
                    "value": "See the section on weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699381368570,
            "cdate": 1699381368570,
            "tmdate": 1699636182589,
            "mdate": 1699636182589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zHxPf87vew",
                "forum": "F7XPZnIUHh",
                "replyto": "mSWur75btp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your efforts and valuable comments on our paper. We address your concerns below:\n\n- [Q1] Thanks a lot for pointing out the questionable part in our work.\n\ni) As for the claim that $\\boldsymbol{I}\\perp Y|T\\cup \\boldsymbol{C}$ in Thm 3.2, we admit the definition for $\\boldsymbol{I}$ (instrumental variables) was inappropriate.\nWe should rule out the \"collider\" variables in the definition of $\\boldsymbol{I}$.\nUnder this updated definition, $\\boldsymbol{I}=\\{X_1\\}$ for Figure 1(b). We have revised the paper accordingly, and also corrected the corresponding proof for Thm 3.2 in the Supplementary.\n\nii) We apologize that the careless statement in the proof of Prop 3.2 caused your confusion.\nWe have changed \\textit{\"when $\\mathbb{E}(T|A(X))=\\mathbb{E}(T)$, i.e., $T\\perp A(X)$\"} as \\textit{\"when $T\\perp A(X)$, we have $\\mathbb{E}(T|A(X))=\\mathbb{E}(T)$\"}.\nIt should be noted that Prop 3.2 claims that $\\mathcal{L}_A$ is maximized when $A\\perp T$, which is in align with the corrected statement.\n\n- [Q2] Thanks for recommending the interesting literature. \n\nWe have to point out that, [1] is very different from our proposed ADR algorithm, although the title seems similar. \n\nFirstly, the SNnet+ proposed in [1] \naims to maximize the loss of treatment classifier on top of the confounder representation $\\boldsymbol{C}$.\nBy contrast, we maximize the loss of treatment classifier for the adjustment representation $\\boldsymbol{A}$.\n\nSecondly, the implementation is different.\nThe SNnet+ added a gradient reversal layer to reverse the direction of the gradient $\\frac{\\partial L_t}{\\partial W_c}$ in the back-forward pass.\nOur proposed ADR algorithm adopts an iterative training process that firstly trains the treatment classifier of $\\boldsymbol{A}$ given the representation, and then fix the classifier and maximize the loss in updating the representation.\n\n\nReference: \\\n[1] Chauhan, V. K., Molaei, S., Tania, M. H., Thakur, A., Zhu, T., \\& Clifton, D. A. (2023, April). Adversarial de-confounding in individualised treatment effects estimation. In International Conference on Artificial Intelligence and Statistics (pp. 837-849). PMLR.\n\n- [Q3] Implementation Details\n\nWe adopted the same representation dimension (256) and the same structured MLPs for the predictors on top of the representations in the experiments.\nFor a fair comparison, the values of such parameters were also in align with the compared literature [2]. The implementation details were added in the Supplementary (see \\verb|Supplementary/code|).\nThanks for pointing out this problem, we have  \nadded the relevant information in Section 5.\n\nReferences: \\\n[2] Anpeng Wu, Junkun Yuan, Kun Kuang, Bo Li, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei\nWu. Learning decomposed representations for treatment effect estimation. IEEE Transactions on\nKnowledge and Data Engineering, 35(5):4989\u20135001, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467892825,
                "cdate": 1700467892825,
                "tmdate": 1700468110288,
                "mdate": 1700468110288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MkZvkrnkOK",
                "forum": "F7XPZnIUHh",
                "replyto": "zHxPf87vew",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for the explanations and the clarifications. Nevertheless, the following very important issues persist in the paper:\n\n- [Q1] (ii) So you agree, that in general, the equality of the conditional expectations does not imply independence? If yes, it seems to be wrong that ADR enforces the independence with the equality of the conditional expectations.\n\n- [Q2] I would expect SNnet+ to be included as a baseline in this case.\n\nThus, I still tend to keep my score the same."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699004187,
                "cdate": 1700699004187,
                "tmdate": 1700699004187,
                "mdate": 1700699004187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ExmcsSsftF",
                "forum": "F7XPZnIUHh",
                "replyto": "VJiFEu7hgz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2464/Reviewer_zHRm"
                ],
                "content": {
                    "comment": {
                        "value": "I disagree. The equality of conditional expectations does not imply independence, especially when $A(X)$ is a continuous representation. Instead, we need to enforce the equality between conditional distributions, as this is the definition of conditional independence."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734249329,
                "cdate": 1700734249329,
                "tmdate": 1700734249329,
                "mdate": 1700734249329,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]