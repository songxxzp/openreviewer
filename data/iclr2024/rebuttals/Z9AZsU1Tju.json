[
    {
        "title": "Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning"
    },
    {
        "review": {
            "id": "yXpNkavRWV",
            "forum": "Z9AZsU1Tju",
            "replyto": "Z9AZsU1Tju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_JgMF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_JgMF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an Information-Theoretic Hierarchical Perception (ITHP) model, based on the concept of information bottleneck, for effective multimodal fusion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The proposed Information-Theoretic Hierarchical Perception model looks novel to me; This stands in contrast to mainstream multimodal fusion where different modalities are treated equally, and the final prediction is usually acquired by concatenating the features from each modality. The concept of designating a prime modality and using the other modalities to guide the flow of information looks interesting, and makes sense to me --- In many multimodal tasks, modalities do not contribute equally to the final prediction.\n\n+ The experimental results are strong. The analysis about varying Lagrange multiplier in Sec. 3.1 provides good insight.\n\n+ Many design choices (e.g. latent state size, order of modalities) are thoroughly evaluated in the appendix."
                },
                "weaknesses": {
                    "value": "+ As the authors already point out in Section 5, the proposed approach requires a predefined order of modalities, which could be a problem when there are > 3 modalities and we do not have prior information about the multimodal task.\n\n+ Would the proposed hierarchical architecture introduce some inference latency compared with the standard non-hierarchical approaches (Take Figure 3 as an example, say compared to concatenation of $X_0$, $X_1$ and $X_2$ and then passed to an MLP head)?\n\n+ It seems in the current framework, feature extraction from raw video, audio & text & the proposed ITHP are separate (first feature extraction then use the encoded features in the hierarchical information flow). The temporal nature of these data is not utilized. I wonder if the authors could briefly comment on the potential of extending the ITHP model to handle sequence data (that naturally comes with a temporal order)."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617476311,
            "cdate": 1698617476311,
            "tmdate": 1699636800419,
            "mdate": 1699636800419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hN6vbd6CiD",
                "forum": "Z9AZsU1Tju",
                "replyto": "yXpNkavRWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JgMF"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging the novelty and sensibility of our Information-Theoretic Hierarchical Perception (ITHP) model, as well as for recognizing the strength of our experimental results. We also greatly value your insightful comments, which have guided us in further enhancing our paper. Here  we provide the point-wise response to your concerns:\n\n**Algorithms for defining the order of modalities:** Thank you for pointing out your concern on defining the order of the modalities when there are > 3 modalities and we do not have prior information about the multimodal task. The sequence of modalities indeed influences the performance of our model. When the number of modalities increases and there is a lack of relevant prior knowledge, determining the richness of information in each modality and the sequence among them becomes crucial. To address this problem, we have developed practical algorithms to define the order of modalities: 1. We employ Sample Entropy (SampEn) to assess the information richness within different modalities, which serves as the basis for ranking these modalities. 2. For specific downstream tasks, we implement a greedy algorithm in conjunction with a submodular function to more comprehensively determine the order of the modalities. Detailed explanations and algorithms for these methods are provided in the appendix J (page 26-27), as also mentioned in the 'Response to all reviewers' comments above.\n\n**Inference latency:** Thank you for your valuable suggestion to test the inference latency, which has enabled a more thorough assessment of our model's performance. In response to your concern about the inference latency of our proposed hierarchical architecture as compared to standard non-hierarchical approaches, we conducted a comparative analysis using a simple MLP head. This involved concatenating modalities M0, M1, and M2 and processing them through an MLP head, representative of the predictor component in our ITHP model. The inference latency for this simple configuration was approximately 0.00221 ms per sample. In contrast, our full ITHP model, incorporating a hierarchical structure, recorded an inference latency of 0.0120 ms per sample using the same experiment setting. It's noteworthy that during inference, the detectors in our model are not active, contributing to its overall efficiency. Despite the anticipated increase in latency compared to an MLP head due to its hierarchical nature, our model remains competitive in terms of efficiency. We appreciate your suggestion and have included the above-mentioned discussion in Appendix G (page 24)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563366228,
                "cdate": 1700563366228,
                "tmdate": 1700563366228,
                "mdate": 1700563366228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "knTYo4aLf9",
            "forum": "Z9AZsU1Tju",
            "replyto": "Z9AZsU1Tju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a neuro-inspired sequential and hierarchical processing model for multi-modality data. The key is to balance the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. The paper presents the theory formulation of the processing steps and evaluate on two multimodality tasks, sarcasm detection and sentiment analysis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper provided detailed formulation of the perception process and deduction of the loss function.\n2.\tThe paper conducted detailed experiments on the ablation of hyperparameters."
                },
                "weaknesses": {
                    "value": "1.\tThe inspiration from neuroscience research seems simple, and unnecessary to the main processing pipeline of the main work, making the main work distracting.\n2.\tThe order of the modality in processing the data seems quite important, which highly impacts the performance. For such an importance factor, it is better to provide further theoretical analysis or practical guidance on it.\n3.\tThe paper should include some recent SOTA works, at lease works in 2022, on comparing the sentiment analysis."
                },
                "questions": {
                    "value": "1.\tWhy sequential processing is good? Except for inspiration from neural inspirations. This operation introduces the extra problem of the order of modality. \n2.\tDoes the best beta and gamma share across different tasks? Or it is needed to balance for different tasks, making this method quite cumbersome."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6883/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6883/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675728029,
            "cdate": 1698675728029,
            "tmdate": 1700729312252,
            "mdate": 1700729312252,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GNOEwVWR0k",
                "forum": "Z9AZsU1Tju",
                "replyto": "knTYo4aLf9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4bfy"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review of our paper, we greatly value your insightful comments. Your suggestions have helped us enhance the content of our paper. Here we provide our responses to the concerns you raised.\n\n**Regarding to the inspiration from neuroscience research:** Our model is inspired by neuroscience research, highlighting how the brain hierarchically processes multimodal information. We have integrated an information bottleneck framework into our design to emulate the hierarchical process, which stands as an innovation in our work and sets our model apart from traditional approaches. Rather than initiating with a fusion structure as is common in conventional models, our model adopts a comprehensive, information theoretic approach.\n\n**Sequential processing and order of modalities:** We are grateful for your recommendations regarding offering theoretical analysis or practical guidance for determining the order of modalities. Based on your advice and comments from other reviewers, we believe it is necessary to provide practical guidance for defining the order of modalities. We have developed practical algorithms to define the primary modality and the order of modalities: (i) We employ Sample Entropy (SampEn) to assess the information richness within different modalities, which serves as the basis for ranking these modalities. (ii) For specific downstream tasks, we implement a greedy algorithm in conjunction with a submodular function to more comprehensively determine the order of the modalities. Detailed explanations and algorithms for these methods are provided in the appendix J (page 26-27), as also mentioned in the 'Response to all reviewers' comments above.\n\nBased on the algorithms mentioned above, we validated the modal selection order used in our sarcasm detection experiments. The computed sample entropies for the V/T/A modalities are 2.388/2.063/0.071, respectively. This indicates that the ranking of information richness for these modalities should be V\u2192T\u2192A. The outcome derived from applying the submodular function method also indicates a V\u2192T\u2192A sequence. The results of corresponding experiments for varying primary modalities are presented in  Section 5 (page 9) and the results are shown in Appendix F.5 (Page 23). We also present the results below. This table illustrates the effects of changing the primary modality (modal order) on weighted precision, recall, and F-score. \n\n| Metric              | V\u2192T\u2192A | V\u2192A\u2192T | T\u2192V\u2192A | T\u2192A\u2192V | A\u2192V\u2192T | A\u2192T\u2192V |\n|:---------------------|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Weighted Precision  | **75.3**  | 73.7  | 73.0  | 72.0  | 71.4  | 70.8  |\n| Weighted Recall     | **75.2** | 73.5  | 72.8  | 71.8  | 71.2  | 70.6  |\n| Weighted F-Score    | **75.2**  | 73.4  | 72.9  | 71.7  | 71.5  | 70.6  |\n\nTable 1. Varying Orders of Modalities.\n\nTable 1 showcases the impact of changing the order of modalities on the weighted Precision, Recall, and F-Score across both sarcastic and non-sarcastic classes, averaged across five folds. Underlined values highlight the best results for each metric. The modalities are denoted as: $T$ for text, $A$ for audio, and $V$ for video.\nThe results show that the order of modalities, particularly the designation of the primary one, considerably influences the performance of the model. The results also indicate that the order of modalities should be V\u2192T\u2192A, which is consistent with the conclusions drawn from our algorithm."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563234701,
                "cdate": 1700563234701,
                "tmdate": 1700563234701,
                "mdate": 1700563234701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U1VVaG7OTc",
                "forum": "Z9AZsU1Tju",
                "replyto": "qL7KcYzhNn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the author for detailed response.  My concerns on the order of modalities have been addressed.  My concern is also on the sequential pipeline, as the final goal is fusing all the information together, is it a better way compared to fusing all the modalities once together? When introducing the information bottleneck to percept the first two modalities, the third modality is not considered, will some complementary information to the third modality be neglect? As the sequential pipeline is one of the most important points of the paper, I would suggest a theoretical proof and experiments to demonstrate the advantage of the sequential pipeline.\n\nCurrently, I keep my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703384869,
                "cdate": 1700703384869,
                "tmdate": 1700703384869,
                "mdate": 1700703384869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2qLEc5g04U",
                "forum": "Z9AZsU1Tju",
                "replyto": "knTYo4aLf9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your prompt response and for acknowledging the resolution of the order of modalities concern. Here we provide the point-wise response to your remaining concerns:\n\n1.\tRegarding your concern about the sequential pipeline versus fusion of all modalities once together, we wish to highlight that, while other state-of-the-art baseline models adopt strategies to fuse all modalities at once, the experiments have demonstrated the superiority of our hierarchical approach (refer to Table 1,2 and 3 in the main body of the article, Table 4 in Appendix E, Table 5 in Appendix F.1 and Table 6 in Appendix F.3).\n\n2.\tYour point about the potential loss of complementary information in the third modality is well-taken. Our method, however, ensures that no modality is neglected. Specifically, the information bottleneck is applied not only between the state $B_0$ and the second modality $X_1$ but also between the state $B_1$ and the third modality $X_2$. The entire process is data-driven, and the final information flow considers all modalities, representing the optimal solution. This guarantees that all modalities are considered, utilizing the information of the third modality (please refer to Appendix C (Pages 14-17) for the detailed design of our model).\n\n3.\tOur sequential pipeline design is not arbitrary but is grounded in neuroscience principles. We believe that the hierarchical design allows for a more natural and robust integration of modalities, which is evidenced in our experiment results compared with the baseline fusion models. We also provide the detailed explanation and theoretical basis for designing our hierarchical model based on the information bottleneck in Appendix C (Pages 14-17).\n\nWe hope this response has addressed your concerns, and we sincerely appreciate your feedback, which is instrumental in enhancing the clarity of our work. We genuinely value your contributions and assure you that any further concerns will be carefully considered."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709350535,
                "cdate": 1700709350535,
                "tmdate": 1700710104580,
                "mdate": 1700710104580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oulOKXDORX",
                "forum": "Z9AZsU1Tju",
                "replyto": "2qLEc5g04U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Reviewer_4bfy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further response.  I understood that the total loss considers the third modality, and the hyper-parameters beta and gamma are introduced, that is why I asked the sensitivity to those introduced hyper-parameters. Since the proposed hierarchical design can introduce more human prior to control the information fusion process. I would suggest the authors can provide more discussion on this from theoretical perspective, rather than providing the indirect evidence of experimental results or bio-inspiration. It will make this paper stronger.  Even though my concerns are not fully addressed, I would like to increase my score to 6, since the paper has made good contribution."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729292741,
                "cdate": 1700729292741,
                "tmdate": 1700729292741,
                "mdate": 1700729292741,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cGDdoifyTz",
            "forum": "Z9AZsU1Tju",
            "replyto": "Z9AZsU1Tju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_uxTC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_uxTC"
            ],
            "content": {
                "summary": {
                    "value": "The authors develop a new Information-Theoretic Hierarchical Perception (ITHP) model that designates a prime modality and regards the remaining modalities as detectors in the information pathway, serving to distill the flow of information. The primary modality yields the highest degree of information extraction, with subsequent modalities contributing information in a sequentially ordered manner.  To address the challenge of high dimensionality of multimodal data, they construct \"Information bottlenecks.\" These bottlenecks function as compressed latent representations of the data, with each bottleneck responsible for compressing a single modality while retaining  the relevant information of other modalities. \n\nThe method shows impressive performance on the CMU-MOSI dataset surpassing human-level performance for sentiment analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The fundamental premise of the research paper is interesting. It employs the concept of hierarchical information flow in a multi-modal context which was shown to be useful for downstream tasks like sentiment analysis and sarcasm detection.  \n\nThe paper is well written and easy to understand."
                },
                "weaknesses": {
                    "value": "Comparison with other methods in Table 1 for sarcasm detection is insufficient. The authors need to provide comparison results on either additional datasets [3] or other existing methods in literature [4].  \n \n[3] Cai Y, Cai H, Wan X. Multi-modal sarcasm detection in twitter with hierarchical fusion model. InProceedings of the 57th annual meeting of the association for computational linguistics 2019 Jul (pp. 2506-2515). \n \n[4] Wen C, Jia G, Yang J. DIP: Dual Incongruity Perceiving Network for Sarcasm Detection. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 2540-2550). \n\nThe paper is specific to the task sentiment analysis whereas the title and presentation of the paper refers to downstream tasks in multimodal learning as a whole. To support the claim the authors should report the method\u2019s performance on some general tasks like visual question answering.  \n\nThe literature survey needs to be updated. They are several recent methods of multimodal fusion and representation learning: \n \n[1] Xue Z, Marculescu R. Dynamic multimodal fusion. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 2574-2583). \n\n \n\n \n\nMissing table 5 and 10 on page 9 which were referred to in text."
                },
                "questions": {
                    "value": "The authors have established a direct quantitative association between the embedding size of modality X and its corresponding priority order for the task. However, the embedding size can be contingent upon the model's architectural characteristics. Does changing the model which provides the embedding has any effect on the priority order?  \n\nHow is embedding size related to the amount of context information present in a modality?  \n\nFor experiments in table 1, is there any effect of changing the audio and text priority, when all three of the modalities are present?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6883/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6883/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6883/Reviewer_uxTC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698931897398,
            "cdate": 1698931897398,
            "tmdate": 1699636800176,
            "mdate": 1699636800176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "70ccOll738",
                "forum": "Z9AZsU1Tju",
                "replyto": "cGDdoifyTz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uxTC"
                    },
                    "comment": {
                        "value": "Thank you for a thorough review of our paper and for recognizing the fundamental premise of our research. We also appreciate your recommendations of related works [1,2,3]. In our revised manuscript, we have expanded the discussion on related literature to appeal to a wider audience. Below, we provide the detailed point-wise response to address your concerns.\n\n**Comparative analysis of additional methods for sarcasm detection:** Thank you for sharing the related works [2,3] on sarcasm detection. Specifically, HFM [2] proposed a task-oriented hierarchical fusion model, which incorporates early fusion, representation fusion, and modality fusion techniques to enhance the comprehension and representation of each modality. We tested HFM using the MUStARD dataset, applying its modality fusion module under the same experimental conditions as our original study. The results, now included in Table 6 of Appendix F.3 (page 21) of the revised manuscript, are as follows:\n\n| Model | Precision | Recall | F-Score |\n|-------|-----------|--------|---------|\n| MSDM  | 71.9      | 71.4   | 71.5    |\n| HFM [2] | 69.4    | 68.8   | 68.9    |\n| ITHP  | **75.3**  | **75.2** | **75.2** |\n\nTable 1: Results of Sarcasm Detection on the MUStARD dataset. \n\nThis table displays the weighted Precision, Recall, and F-Score for the V-T-A (video, text, audio) modality, averaged across five folds. Bold figures represent the highest scores achieved. As the results indicate, our ITHP model demonstrates a notable increase of 8.5% in precision, 9.3% in recall, and 9.1% in F-score compared to the HFM [2] model. These improvements underscore the superiority of our model in sarcasm detection.\n\nThe DIP [3] model processes inputs consisting of two modalities: images and text. It computes a relation matrix of their feature vectors, which is then fed into two modules, SSC and SID, for further processing and eventual concat for prediction. This model architecture, specifically its requirement for modality correlation matrix computation and the use of two distinct branches for sentiment analysis, does not align well with our sarcasm datasets. Consequently, it is not feasible to conduct equivalent comparative experiments. Regardless, we appreciate your sharing of the related works, and we have also discussed them in our related work in Appendix B (page 14) to reach a broader audience as follows:\n\n\u201cThe latest work in the field of sarcasm detection has expanded our research horizons. The HFM [2] utilizes early fusion and representation fusion techniques to refine feature representations for each modality. It integrates information from various modalities to better utilize the available data and enhance sarcasm detection performance. The DIP [3] creatively adopts dual incongruity perception at factual and affective levels, enabling effective detection of sarcasm in multi-modal data.\u201d\n\n**Literature survey update:** Thank you for introducing us to the exceptional work [1], it will greatly enhance our literature review. Diverging from the prevalent static fusion approaches, DynMM presents an innovative perspective by adaptively fusing multimodal data and creating data-dependent forward paths during inference. The main highlight of the DynMM model lies in its ability to significantly reduce computational resource consumption while maintaining relatively high model performance. \n\nWe also compared the performance of our ITHP model with DynMM on the CMU-MOSEI dataset. DynMM achieved an accuracy of 79.8% and a Mean Absolute Error (MAE) of 0.60. In contrast, our model achieved an accuracy of 87.3% and an MAE of 0.56, surpassing DynMM by over 9.4% in accuracy and exhibiting a 6.7% lower MAE. These results further underscore the superiority of our model in terms of accuracy. We have also discuss DynMM in in Appendix B (page 14): \n\n\u201cThe DynMM [1] creates the possibility of reducing computing resource consumption. By introducing dynamic multi-modal fusion, which fuses input data from multiple modalities adaptively, leading to reduced computation, improved representation power, and enhanced robustness.\u201d\n\n[1] Xue Z, Marculescu R. Dynamic multimodal fusion. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 2574-2583).\n\n[2] Cai Y, Cai H, Wan X. Multi-modal sarcasm detection in twitter with hierarchical fusion model. InProceedings of the 57th annual meeting of the association for computational linguistics 2019 Jul (pp. 2506-2515).\n\n[3] Wen C, Jia G, Yang J. DIP: Dual Incongruity Perceiving Network for Sarcasm Detection. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 2540-2550)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562877080,
                "cdate": 1700562877080,
                "tmdate": 1700562877080,
                "mdate": 1700562877080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z77NWvpv5t",
                "forum": "Z9AZsU1Tju",
                "replyto": "owk3JGEVuk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Reviewer_uxTC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Reviewer_uxTC"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have addressed the concerns regarding the order of modality to be considered during training. They have proposed a greedy based approach which is more general than depending on the embedding sizes. The literature and comparisons with recent baselines has been updated as well. They have reported the validation of SampEn and greedy algorithm for sarcasm detection but I think since the entire hypothesis of the paper is dependent on the order of modality the results should also have been validated for the chosen T-A-V for sentiment analysis task. It is difficult to make a concrete comment with results on a single dataset (Mustard) which has kind of an obvious order (V-T-A).\n\nI keep my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698298279,
                "cdate": 1700698298279,
                "tmdate": 1700698298279,
                "mdate": 1700698298279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XjWuFPM9RZ",
            "forum": "Z9AZsU1Tju",
            "replyto": "Z9AZsU1Tju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_Qqaa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_Qqaa"
            ],
            "content": {
                "summary": {
                    "value": "- This paper introduces the Information-theoretic Hierarchical Perception (ITHP) model, which aims to integrate and process information from multiple modalities effectively. The authors show inspiring motivation corresponding neuroscience research to a model that designates a prime modality and utilizes the concept of the information bottleneck to construct compact and informative latent states, which enable to balance between preserving relevant information and reducing noise in the latent states.\n\n- This paper tries to justify the validity of design philosophy and the effectiveness of the proposed models experimentally with the performance comparison of two tasks such as (1) sarcasm detection from videos and (2) multimodal sentiment analysis. They seem to show better scores in both tasks. (note that seems not to pursue state-of-the-art performances)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper presents interesting connections between neuroscience and proposed models. It provides good motivations for designing the proposed models.\ufeff This integration of neuroscience principles into the design of the models adds a novel perspective to the field of multimodal learning.\n\n- The authors demonstrate the effectiveness with competitive performances of the proposed models in the two tasks."
                },
                "weaknesses": {
                    "value": "- Even though this paper shows interesting modeling design and experimental results, there is a gap between state-of-the-art methods such as UniMSE and SPECTRA, for example, with respect to performances. What kinds of criteria to choose the model configuration and comparative methods in the paper?\n\n- This paper seems not to present enough information on experimental setting and implementation to achieve reproducibility.\n\n- It lacks justification for the component choices. Which points should be clarified through experiments? Why BERT and DeBERTa are utilized as language models? \n\n\n* References\n\n  - UniMSE : G. Hu, et al., UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition, EMNLP 2022\n\n  - SPECTRA : T. Yu, et al., Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment, ACL 2023"
                },
                "questions": {
                    "value": "See the Weaknesses above and answer, please."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699382476822,
            "cdate": 1699382476822,
            "tmdate": 1699636800070,
            "mdate": 1699636800070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TGlYW0RixR",
                "forum": "Z9AZsU1Tju",
                "replyto": "XjWuFPM9RZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Qqaa"
                    },
                    "comment": {
                        "value": "We sincerely thank you for highlighting the good motivations behind our models and recognizing how their integration of neuroscience principles adds a novel perspective to multimodal learning. We also thank you for sharing these two interesting related works [1,2], which we have discussed in the revised version to reach a broader audience. Additionally, we provide detailed answers to address your questions below.\n\n\n**Regarding the comparison between our method, ITHP, against SOTA methods such as UniMSE and SPECTRA:**  Thank you for sharing these related works [1,2], we have first added them into the references and discussed them in the related work section in Appendix B (page 14) as follows:\n\u201cThe current state-of-the-art model presents novel insights, UniMSE [1]  innovatively integrates multimodal sentiment analysis and emotion recognition in conversations into a unified framework, enhancing prediction accuracy by leveraging the synergies and complementary aspects of sentiments and emotions. SPECTRA [2] is pioneering in spoken dialog understanding by pre-training on speech and text, featuring a unique temporal position prediction task for precise speech-text alignment .\u201d\n\nWe have also expanded our baselines to include more recent works such as UniMSE [1], MIB [3] and BBFN [4] for a more comprehensive comparison, as detailed in the updated Table 2 (page 8) in the paper. The results show that our model continues to surpass the baselines.\n\nRegarding the comparison with the models you mentioned, particularly UniMSE [1] and SPECTRA [2]: First, regarding performance, we outperform UniMSE on CMU-MOSEI and CMU-MOSI, two common evaluation benchmarks shared by both of the works, by around 1% and 3% points in terms of F1 score. Meanwhile, SPECTRA is surpassed by 1% in MOSI datasets, demonstrating our model\u2019s superiority. Here we would like to point out that our data preprocessing is based on the previous paper MAG [5], which only provides off-the-shelf representations of the images and audio. However, both UniMSE and SPECTRA, adopted a different pipeline (e.g., in UniMSE, they use more powerful tools such as effecientNet to extract features from the video, coupled with additional A/V-sLSTM to further distill the multimodal signals). In this sense, the comparison with Self-MM, MMIM, and MAG are considered as more fair baselines given the same pre-processing pipelines, where ITHP has dominated in performance against all of the above.\nSecond, our ITHP is an information-theoretic with guaranteed proof of aggregating the most useful information across different modalities. Previous works, such as UniMSE and SPECTRA, either *empirically* instead of *theoretically* adopts contrastive loss to force the latent space to be as distinctive as possible, or *empirically* aligns sequential audio-text features which are not generalizable to images.\n\n[1] UniMSE : G. Hu, et al., UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition, EMNLP 2022\n\n[2] SPECTRA : T. Yu, et al., Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment, ACL 2023\n\n[3] MIB: Mai S, Zeng Y, Hu H. Multimodal information bottleneck: Learning minimal sufficient unimodal and multimodal representations. IEEE Transactions on Multimedia. 2022.\n\n[4] Han, Wei, et al. \"Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis.\" Proceedings of the 2021 International Conference on Multimodal Interaction. 2021.\n\n[5] Rahman, Wasifur, et al. \"Integrating multimodal information in large pretrained transformers.\" Proceedings of the conference. Association for Computational Linguistics. Meeting. Vol. 2020. NIH Public Access, 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562707820,
                "cdate": 1700562707820,
                "tmdate": 1700562707820,
                "mdate": 1700562707820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uzOrvfKXMd",
            "forum": "Z9AZsU1Tju",
            "replyto": "Z9AZsU1Tju",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_mWBz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6883/Reviewer_mWBz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ITHP, a brain-inspired hierarchical information perception model that fuses information from different modalities. ITHP employs the information bottleneck framework to extract relevant information from different modalities in a hierarchical/sequential manner. Extensive experiments on MUStARD, CMU-MOSI, and CMU-MOSEI demonstrate the effectiveness of ITHP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper is clear and well-organized.\n- the proposed ITHP is intuitive and well-grounded in information theory and provides a novel view of information fusing in multi-modal learning.\n- extensive experiments demonstrate the effectiveness of ITHP."
                },
                "weaknesses": {
                    "value": "- ITHP extracts commonly encoded information from different modalities; however, would this procedure be called \"fusion\" appropriately?\n    - suppose the primary modality is $M_p$ and a secondary modality be $M_1$. ITHP cannot capture information encoded in $M_1$ but not $M_p$. In this sense, ITHP would be an information distillation rather than a fusion method.\n    - when talking about \"fusion,\" one expects to integrate information of **different** types.\n    - in addition, when both modalities contain similar information, if the one in $M_1$ is noisy, would ITHP be affected? Can ITHP correctly identify the noise in $M_1$ and mainly use the information from $M_p$ instead?\n    - as the main idea of ITHP is to distill relevant information from different modalities, the starting point would be important. However, the experiments do not show results with different primary modalities."
                },
                "questions": {
                    "value": "- how do you choose the primary modality? And how would different choices of primary modality affect the performance of ITHP?\n- how would ITHP perform in the presence of noise in some of the modalities (the primary or the secondary ones)?\n- how can ITHP accommodate tasks that require combining complementary information from different modalities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699428627617,
            "cdate": 1699428627617,
            "tmdate": 1699636799940,
            "mdate": 1699636799940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UTgAmFqaAw",
                "forum": "Z9AZsU1Tju",
                "replyto": "uzOrvfKXMd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mWBz"
                    },
                    "comment": {
                        "value": "Thank you for your meticulous review and valuable insights. We sincerely appreciate your recognition of the ITHP model\u2019s intuitiveness and its strong grounding in information theory. In the following, we offer our detailed responses and clarifications to address the concerns and queries you have highlighted.\n\n**The usage of \"fusion\" in our method:** Thank you for highlighting the usage of the term \u201cfusion\u201d in our study. We are grateful for your detailed attention to this aspect, and we have thoroughly reviewed our manuscript to ensure that this term is used accurately and appropriately. Fusion models primarily aim to integrate information from various modalities into a unified representation [1,2]. In terms of this objective, our work can certainly be viewed as a fusion model constructing a compact representation by utilizing information from various modalities. In terms of technique, we indeed adopt a distinct hierarchical approach compared to traditional fusion models, which stands as a significant innovation in our work. Although the secondary modalities serve as detectors in our model, it is worthy noting that, within the deep learning architecture, our model has certain capability to indirectly learn the complementary information from secondary modalities. As explained in Appendix H (page 24):\u201d by harnessing the capabilities of our deep learning framework, our model has a certain ability to learn the complementary information from secondary modalities. To achieve a harmonious representation, we\u2019ve incorporated specific terms in the loss function. Specifically, taking the 3-modalities learning problem as an example, term $L_{1} =\u2212\\beta\\cdot I\\left (B_{0};X_{1} \\right) $ is tasked with supervising the knowledge transfer from $X_{1}$ to $B_{0}$, while term $L_{2} =\u2212\\gamma\\cdot I\\left (B_{1};X_{2} \\right) $ is responsible for supervising the knowledge dissemination from $X_{2}$ to $B_{1}$. Through this framework, the ITHP model is primed to discern and incorporate complementary information from non-input modalities.\u201d In summary, we prefer to use the term 'fusion' in the context of our mode, and we will make it clear in our revised version. \n\n[1] Gao, Jing, et al. \"A survey on deep learning for multimodal data fusion.\" Neural Computation 32.5 (2020): 829-864.\n\n[2] Atrey, Pradeep K., et al. \"Multimodal fusion for multimedia analysis: a survey.\" Multimedia systems 16 (2010): 345-379."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562360536,
                "cdate": 1700562360536,
                "tmdate": 1700562360536,
                "mdate": 1700562360536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RcVNzb5GcW",
                "forum": "Z9AZsU1Tju",
                "replyto": "uzOrvfKXMd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continue 1"
                    },
                    "comment": {
                        "value": "**Model performance in the presence of noise:** Thanks for the insightful suggestions on adding noise to the modality $M_{p}$ and modality $M_{1}$. We believe these experiments will bring valuable insights to further elucidate our model. Theoretically, our model filters redundant information within its information-bottleneck hierarchical structure, and we anticipate it will have advantages over traditional fusion models in terms of reducing the impact of noise. According to your suggestions, we conducted the noise addition experiments as follows:\n\nIn the sarcasm detection dataset, we introduced noise to the normalized data of both the primary and secondary modalities separately to test the model's resistance to noise interference.  We apply a series of Gaussian noises with a mean of 0 and a varying standard deviation ranging from 0.05 to 0.3, increasing in increments of 0.05. For each noise level, we repeated the experiment 50 times and calculated the average result. We have included these experiments in the revised version (Please refer to Appendix F.6, page 23-24), and the results are presented below:\n\n\n| $\\sigma$ | -     | 0.05  | 0.1   | 0.15  | 0.2   | 0.25  | 0.3   |\n|----------------------|-------|-------|-------|-------|-------|-------|-------|\n| **MSDM Precision** | 0.719 | 0.711 (-1.11%) | 0.709 (-1.39%) | 0.707 (-1.67%) | 0.704 (-2.09%) | 0.700 (-2.64%) | 0.688 (-4.31%) |\n| **MSDM Recall** | 0.714 | 0.705 (-1.26%) | 0.702 (-1.68%) | 0.700 (-1.96%) | 0.695 (-2.66%) | 0.691 (-3.22%) | 0.679 (-4.90%) |\n| **ITHP Precision** | 0.753 | 0.751 (-0.27%) | 0.748 (-0.66%) | 0.747 (-0.80%) | 0.746 (-0.93%) | 0.743 (-1.33%) | 0.740 (-1.73%) |\n| **ITHP Recall** | 0.752 | 0.750 (-0.27%) | 0.745 (-0.93%) | 0.743 (-1.20%) | 0.742 (-1.33%) | 0.740 (-1.60%) | 0.736 (-2.13%) |\n\nTable 1. Evaluating Gaussian Noise Impact on Sarcasm Detection in Text Modality. \n\nTable 1 presents the weighted precision and recall for sarcasm detection on the MUStARD dataset, with an analysis of Gaussian noise addition to the text ($M_{1}$) modality. Results are averaged over five cross-validation folds and 50 trials for each level of noise intensity. Each column indicates the standard deviation (represented by $\\sigma$) of the introduced Gaussian noise with a mean of 0. Changes in performance metrics relative to the noise-free baseline are expressed as percentages\n\n\n\n| $\\sigma$ | -      | 0.05     | 0.1      | 0.15     | 0.2      | 0.25      | 0.3       |\n|----------------|--------------|------------|-----------|------------|------------|------------|-------------|\n| **MSDM Precision**| 0.719         | 0.709 (-1.39%)    | 0.705 (-1.95%)    | 0.702 (-2.36%)    | 0.698 (-2.92%)    | 0.692 (-3.76%)    | 0.683 (-5.01%)    |\n| **MSDM Recall**   | 0.714         | 0.703 (-1.54%)    | 0.700 (-1.96%)    | 0.694 (-2.80%)    | 0.689 (-3.50%)    | 0.678 (-5.04%)    | 0.668 (-6.44%)    |\n| **ITHP Precision**    | 0.753         | 0.748 (-0.66%)    | 0.745 (-1.06%)    | 0.743 (-1.33%)    | 0.742 (-1.46%)    | 0.739 (-1.86%)    | 0.735 (-2.39%)    |\n| **ITHP Recall**       | 0.752         | 0.745 (-0.93%)    | 0.741 (-1.46%)    | 0.740 (-1.60%)    | 0.738 (-1.86%)    | 0.733 (-2.53%)    | 0.730 (-2.93%)    |\n\nTable 2. Evaluating Gaussian Noise Impact on Sarcasm Detection in Video Modality. \n\nThis table presents the weighted precision and recall for sarcasm detection on the MUStARD dataset, with an analysis of Gaussian noise addition to the video ($M_{p}$) modality. Results are averaged over five cross-validation folds and 50 trials for each level of noise intensity. Each column indicates the standard deviation (represented by $\\sigma$) of the introduced Gaussian noise with a mean of 0. Changes in performance metrics relative to the noise-free baseline are expressed as percentages\n\nThe results shown in Table 1 and Table 2 demonstrate that, compared to the baseline model, our model exhibits greater robustness to noise interference. The impact of adding noise to the primary modality is more significant than the effect of noise on the secondary modality."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562451870,
                "cdate": 1700562451870,
                "tmdate": 1700562593637,
                "mdate": 1700562593637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZaiKaE16bs",
                "forum": "Z9AZsU1Tju",
                "replyto": "uzOrvfKXMd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continue 2"
                    },
                    "comment": {
                        "value": "**Experiments with different primary modalities:** \nWe appreciate your question regarding the selection of the primary modality and its impact on the ITHP model's performance. In fact, we have already conducted the experiments for varying primary modalities as highlighted in  Section 5 (page 9) and the results are shown in Appendix F.5 (Page 23). We also present the results below. This table illustrates the effects of changing the primary modality (modal order) on weighted precision, recall, and F-score. \n\n| Metric              | V\u2192T\u2192A | V\u2192A\u2192T | T\u2192V\u2192A | T\u2192A\u2192V | A\u2192V\u2192T | A\u2192T\u2192V |\n|:---------------------|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Weighted Precision  | **75.3**  | 73.7  | 73.0  | 72.0  | 71.4  | 70.8  |\n| Weighted Recall     | **75.2**  | 73.5  | 72.8  | 71.8  | 71.2  | 70.6  |\n| Weighted F-Score    | **75.2**  | 73.4  | 72.9  | 71.7  | 71.5  | 70.6  |\n\nTable 3. Varying Orders of Modalities.\n\nThe table showcases the impact of changing the order of modalities on the weighted Precision, Recall, and F-Score across both sarcastic and non-sarcastic classes, averaged across five folds. Underlined values highlight the best results for each metric. The modalities are denoted as: $T$ for text, $A$ for audio, and $V$ for video.\n\nThe results show that the order of modalities, particularly the designation of the primary one, considerably influences the performance of the model. In many cases, we can rely on prior knowledge to guide the selection of the primary modality and its sequence. However, when such knowledge is lacking, effective methods are essential for determining these aspects. In response to the feedback from reviewers, we have developed practical algorithms to define the primary modality and the order of modalities: (i) We employ Adapting Sample Entropy (SampEn) to assess the information richness within different modalities, which serves as the basis for ranking these modalities. (ii) For specific downstream tasks, we implement a greedy algorithm in conjunction with a submodular function to more comprehensively determine the order of the modalities. Detailed explanations and algorithms for these methods are provided in the appendix J (page 26-27), as also mentioned in the 'Response to all reviewers' comments above.\n\nBased on the methods for modalities ranking we proposed, we validated the modal selection order used in our experiments. For the first method, the computed SampEn for the V/T/A modalities are 2.388/2.063/0.071, respectively. This indicates that the ranking of information richness for these modalities should be V\u2192T\u2192A. Additionally, by applying the algorithm (ii), we obtained the same modality order of V\u2192T\u2192A. Hence, we chose V as our primary modality. This aligns with the results obtained in our experiments, confirming the viability of our proposed detection criteria.\n\n**Learning complementary information:** As we mentioned in the above response (**The usage of \"fusion\" in our method**), our ITHP model, built within the deep learning architecture has a certain ability to learn the complementary information from secondary modalities. Please refer to Appendix H (page 24) for details."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562558763,
                "cdate": 1700562558763,
                "tmdate": 1700562600619,
                "mdate": 1700562600619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]