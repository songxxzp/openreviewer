[
    {
        "title": "MindAgent: Emergent Gaming Interaction"
    },
    {
        "review": {
            "id": "I2EMoNlmZa",
            "forum": "p9pBJv1DTz",
            "replyto": "p9pBJv1DTz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_YZDa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_YZDa"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new text-based gaming multi-agent benchmark CuisineWorld, inspired by the video game Overcooked, proposes a method MindAgent for central control of multiple agents using LLM by designing prompts, and conducts experiments with the benchmark and the method including human experiments."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a new text-based multi-agent cooperation benchmark, with a vivid visual appearance.\n\n- The paper conducts experiments involving more than 2 agents and human.\n\n- The paper shows some efforts in transferring the method in real-world gaming scenarios Mindcraft."
                },
                "weaknesses": {
                    "value": "1. The Benchmark's Contribution Falls Short\n\n- The CuisineWorld, based on the video game Overcooked which is popular for measuring multi-agent cooperation and has many existing environments already[1][2]. The benchmark introduces more cuisines but falls short in diversifying kitchen maps when compared to previous Overcooked environments.\n\n- Table 12 made a good summary of related benchmarks though some related embodied multi-agent cooperation benchmarks with most of the features in the table are missing, such as [3][4].\n \n- While CuisineWorld boasts an appealing visual design, it does not directly align with the paper's experiments. The paper primarily relies on text-based states and interactions (Figure 26, even during human experiments, where humans are restricted to a textual interface with lots of text description of the game state)\n\n- In section D.2, the statement \"In this game, the actions of human players can be obtained from an inverse dynamic model by checking pre-conditions and post-effects\" is confusing to me. How was Figure 22 obtained? It seems to be in real-time with human players using the keyboard to move, how's the time step defined here and the \"goto\" action for the LLM agent implemented?\n\n- The benchmark predominantly employs a \"new auto-metric collaboration score CoS for assessing the collaboration efficiency\". However, this metric is defined as the average task success rate with different time intervals for each dish that appears highly tailored to the specific environment and lacks a clear connection to \"cooperation efficiency\".\n\n- The absence of a train/test split is concerning, as prompt engineering can substantially impact performance. A thorough understanding of how the prompt is tuned for different tasks is crucial.\n\n- From C.3.4, level 3 has only two similar recipes which differ only with the words \"salmon\" and \"tuna\", only one demonstration may decrease the task difficulty significantly for the LMs, raising doubts about the benchmark formation.\n\n2. Claims Need Stronger Support from Results\n\n- In section 5.1, The paper claims \"more agents will lead to higher collaboration efficiencies. Thus, indicating that the LLM dispatcher can coordinate additional agents to execute tasks more efficiently\". However, from Table 1, 4 agents perform worse than 3 agents in most scenarios, and even 2 agents achieve the highest score in levels 2 and 4, which contradicts the claim. It may provide more insights if the paper can provide more analysis on these contradictory results instead of only ablating on level_3, which seems to be the only level that \"looks normal\" (a.k.a 2 agents < 3 agents < 4 agents with somewhat clear gaps)\n\n- Table 4, \"For a fair comparison, all tests employed identical prompt inputs\" using \"identical\" prompt for different LM families may not be \"fair\", especially if the prompt is \"tuned\" specifically for one model. More details on the prompt engineering process may help clarify these concerns.\n\n- Table 3 presents a perplexing scenario where four agents using a two-agent demo outperform four agents using a four-agent demo, albeit marginally. This result could benefit from a clearer explanation.\n\n- Novel game adaptation of Minecraft seems very promising, but the details are extremely limited. It would be more convincing if there were more details and formal experiments on it. For example, how is the \"adaptation\" conducted? What's the additional human effort required (such as prompt engineering and rounds of re-playing for prompt tuning)?\n\n3. Method's Limited Contribution\n\n- The method employed in the paper primarily relies on intensive prompt engineering, without introducing novel designs for multi-agent cooperation.\n\n- It's well known that LLMs can perform \"in-context learning\", providing demonstrations and reasoning steps can help improve performance, there's nothing new to take away from the method and experiments.\n\n- As mentioned above, the \"emergent ability\" of MindAgent is not well supported by the results.\n\n- Providing only screenshots of part of the prompts as in Figures 6 and 7 is not enough. More details on the full prompt and game episode may help clarify these concerns.\n\n[1] On the utility of learning about humans for human-ai coordination\n[2] Too many cooks: Bayesian inference for coordinating multi-agent collaboration\n[3] Building cooperative embodied agents modularly with large language models\n[4] A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks"
                },
                "questions": {
                    "value": "Please address the concerns raised in the Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698541162559,
            "cdate": 1698541162559,
            "tmdate": 1699636256134,
            "mdate": 1699636256134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7IDHsA9jDy",
                "forum": "p9pBJv1DTz",
                "replyto": "I2EMoNlmZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YZDa 1/N"
                    },
                    "comment": {
                        "value": "> The CuisineWorld, based on the video game Overcooked which is popular for measuring multi-agent cooperation and has many existing environments already[1][2]. The benchmark introduces more cuisines but falls short in diversifying kitchen maps when compared to previous Overcooked environments.\n\nWe are grateful for your insightful comment regarding the design of our CuisineWorld benchmark in relation to existing environments based on the popular video game Overcooked. Our approach, indeed, differs from the environments presented in references [1] and [2]. While these existing environments offer a variety of kitchen maps, our benchmark is specifically tailored to explore high-level multi-agent planning and coordination using Large Language Model-based Multi-agent Systems (MAS).\n\n\nTo achieve this, instead of focusing on diversifying kitchen maps, which often necessitates low-level control of agent motion, we have introduced several unique elements to enhance the complexity and diversity of the tasks:\n\n- **Enhanced Kitchen Layouts**: We have diversified the layout of kitchens by incorporating a wider range of **tools** (both in types and quantities) and **ingredients**. This approach differs from [1] and [2], where the emphasis is not on the variety of kitchen tools and ingredients. For example, in [1], there are only two types of ingredients and two types of tools. In [2], there are two types of ingredients, and two types of tools. In comparision, there are 27 unique ingredients, and 10 tools in CuisineWorld.  \n\n\n- **Complex Task Design**: Our benchmark includes a broader spectrum of recipes, varying significantly in difficulty levels. This variation is not just in terms of the number of tools and ingredients required but also in the intermediate steps involved in each recipe. We invite you to refer to Figure 23 for a detailed illustration. This aspect of task complexity, particularly in the context of high-level planning, is not extensively explored in [1] and [2]. In [1,2] there are very limited number of dishes, 1 and 3 respectively. However, in CuisineWorld, there are 33 unique dishes. \n\n\n- **Multi-Dish Episodes and Collaborative Strategy Assessment**: We require agents to complete multiple dishes within a single episode, with the types of dishes varying to challenge and assess the collaborative strategies of the agents. Our level design ensures that there are shared intermediate steps among the types of dishes in a single episode. The system is tasked with multiple different goals at the same time. This approach allows us to use metrics like 'Collaborative Score' (CoS) to evaluate how agents collaborate to achieve higher dish throughput. This dynamic aspect of collaboration, especially in the context of dish expiration and shared tasks, offers a new dimension to the study of multi-agent cooperation, which is distinct from the environments in [1] and [2]. In [1] the goal is to finish as many dishes as possible in a limited amount of time. In [2], the goal is to finish one dish in the least amount of time. Both of them do not consider the density of the tasks (interval between dish orders coming to the kitchen) and its effect on coordination. As we mentioned earlier, this concpet (changing density of the tasks and measuring collaboration proficiency upon it) is at heart of CuisineWorld and our CoS metric, and it has demonstrated its effectiveness of benchmarking **collaboration between LLMs and human-NPCs** (as indicated in the abstract).\n\n> Table 12 made a good summary of related benchmarks though some related embodied multi-agent cooperation benchmarks with most of the features in the table are missing, such as [3][4].\n\nThanks for pointing this out. We updated the paper and added the missing references."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548125062,
                "cdate": 1700548125062,
                "tmdate": 1700548125062,
                "mdate": 1700548125062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XAwmSt1v4U",
                "forum": "p9pBJv1DTz",
                "replyto": "I2EMoNlmZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YZDa 4/N"
                    },
                    "comment": {
                        "value": "> From C.3.4, level 3 has only two similar recipes which differ only with the words \"salmon\" and \"tuna\", only one demonstration may decrease the task difficulty significantly for the LMs, raising doubts about the benchmark formation.\n    \nWe appreciate your concern about the perceived task simplicity in Level 3 of CuisineWorld, where the recipes differ only in terms of \"salmon\" and \"tuna.\" However, we respectfully disagree with the notion that a single demonstration significantly decreases task difficulty for LLMs. Our benchmark's design is not centered around creating complex recipes for LLMs to master, but rather on introducing and evaluating the complexity of collaboration through carefully designed recipe combinations.\n\nIn Level 3, the challenge for LLMs is to effectively coordinate agents around shared tasks and bottleneck resources. A strategic collaboration example could involve assigning one agent to prepare cooked rice\u2014a common component in all dishes\u2014while others handle different kitchen stations, such as mixers or chopping boards. This requires the LLMs to identify and manage shared resources and task bottlenecks, and to establish an efficient \u201cpipeline\u201d for task execution, which significantly enhances throughput. Such strategic planning and coordination cannot be trivially inferred from a single demonstration.\n    \nFurthermore, it is important to note that the demonstrations provided to the LLMs are based on Level 0 tasks, which are fundamentally different from those in Level 3. Therefore, the LLMs are essentially working on novel tasks that require adaptive and strategic thinking beyond replicating demonstrated actions. \n    \nThis aspects of our design underscores the benchmark's focus on assessing high-level planning and collaboration, rather than recipe complexity.\n\n> In section 5.1, The paper claims \"more agents will lead to higher collaboration efficiencies. Thus, indicating that the LLM dispatcher can coordinate additional agents to execute tasks more efficiently\". However, from Table 1, 4 agents perform worse than 3 agents in most scenarios, and even 2 agents achieve the highest score in levels 2 and 4, which contradicts the claim. It may provide more insights if the paper can provide more analysis on these contradictory results instead of only ablating on level_3, which seems to be the only level that \"looks normal\" (a.k.a 2 agents < 3 agents < 4 agents with somewhat clear gaps)\n    \nThank you for highlighting the discrepancies in our results concerning the relationship between the number of agents and collaboration efficiency. We have revised our claim to more accurately reflect that while generally increasing the number of agents can lead to higher efficiency, there are scenarios where adding more agents may actually hinder performance.\n    \nOur revised claim now acknowledges that the relationship between the number of agents and task efficiency is not always linear and can be influenced by various factors, including task complexity and the nature of the agents' collaboration. We have added detailed discussions around this revised claim to explore these nuances and provide a more comprehensive understanding of when and why more agents may or may not lead to better performance.\n    \nIn our initial study, we chose Level 3 for our ablation study because it showed a clear pattern where increasing the number of agents improved performance. We've updated our paper to explain that this choice was based on the observation that the LLM performed well on this level, offering a stable platform to assess the impact of varying agent numbers. When LLMs do not perform well on certain levels, additional unknown factors might confound the results, making it challenging to isolate the effects of ablated factors on performance.\n\nIn response to your suggestion, we have conducted a preliminary qualitative analysis and found the following failure mode: \n    \n1. Inability to prioritize the task order. Sometimes LLM will skip the task that is on the top of the task queue and leads to task expiration. Therefore, the success rate will drop. \n\n2. Unable to understand occupy() state instruction. In our CuisineWorld, agents need to wait for a varying timesteps before the cooking is done. The wait time depends on the specific tool. When agents attempt to take out ingredients immediately after activating the tool, actions will fail. Instead of keeping waiting, the agents might work on other task components which will slow down the overall progress. \n    \n3. Unable to allocate agents to the correct subgoals when there are multiple concurrent dishes. This is especially the case when there are 4 agents."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548326889,
                "cdate": 1700548326889,
                "tmdate": 1700548826823,
                "mdate": 1700548826823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EuP6AzYJvU",
                "forum": "p9pBJv1DTz",
                "replyto": "w077KAkOY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Reviewer_YZDa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Reviewer_YZDa"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed responses! I highly appreciate it and some of my concerns are addressed. However, my main concern remains: the claims made in this paper are not well supported by the experiments and results provided, and there are not many new efforts in the revision to improve this. Therefore I'm keeping my score for this version of the paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720506661,
                "cdate": 1700720506661,
                "tmdate": 1700720506661,
                "mdate": 1700720506661,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nMaGAKrJ1U",
                "forum": "p9pBJv1DTz",
                "replyto": "I2EMoNlmZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">  \"Thanks for the detailed responses! I highly appreciate it and some of my concerns are addressed. However, my main concern remains: the claims made in this paper are not well supported by the experiments and results provided, there are not many new efforts in the revision to improve this.\"\n\nWe thank you for the constructive comments and helpful suggestions again. We address your main concerns below and describe every change that was made to the manuscript in our revision. \n \n> The CuisineWorld, based on the video game Overcooked which is popular for measuring multi-agent cooperation and has many existing environments already[1][2]. The benchmark introduces more cuisines but falls short in diversifying kitchen maps when compared to previous Overcooked environments.\n\nWe addressed this comment in red-ink of Appendix Sec. E.1 in the new revision paper.\n       \n> In section D.2, the statement \"In this game, the actions of human players can be obtained from an inverse dynamic model by checking pre-conditions and post-effects\" is confusing to me. How was Figure 22 obtained? It seems to be in real-time with human players using the keyboard to move, how's the time step defined here and the \"goto\" action for the LLM agent implemented?\n\nWe addressed your comments in red-ink of Appendix Sec. E.2. (we add one more section in the appendix of the revision, so the Sec. E.2 maps to the previous Sec. D.2 as per your comments.)\n    \n> The benchmark predominantly employs a \"new auto-metric collaboration score CoS for assessing the collaboration efficiency\". However, this metric is defined as the average task success rate with different time intervals for each dish that appears highly tailored to the specific environment and lacks a clear connection to \"cooperation efficiency\".\n\nWe addressed these comments in red-ink of Appendix Sec. E.6 in our revision.\n    \n> In section 5.1, The paper claims \"more agents will lead to higher collaboration efficiencies. Thus, indicating that the LLM dispatcher can coordinate additional agents to execute tasks more efficiently\". However, from Table 1, 4 agents perform worse than 3 agents in most scenarios, and even 2 agents achieve the highest score in levels 2 and 4, which contradicts the claim. It may provide more insights if the paper can provide more analysis on these contradictory results instead of only ablating on level_3, which seems to be the only level that \"looks normal\" (a.k.a 2 agents < 3 agents < 4 agents with somewhat clear gaps)\n    \nWe addressed your comments in red-ink of Section 5.1 findings.\n    \n> Table 4, \"For a fair comparison, all tests employed identical prompt inputs\" using \"identical\" prompt for different LM families may not be \"fair\", especially if the prompt is \"tuned\" specifically for one model. More details on the prompt engineering process may help clarify these concerns.\n    \nWe describe the detailed prompt engineering process in red-ink Section B of our revision.\n\n> Table 3 presents a perplexing scenario where four agents using a two-agent demo outperform four agents using a four-agent demo, albeit marginally. This result could benefit from a clearer explanation.\n\nWe addressed your comments in red-ink of Section 6.2 of our revision.\n\n> Novel game adaptation of Minecraft seems very promising, but the details are extremely limited. It would be more convincing if there were more details and formal experiments on it. For example, how is the \"adaptation\" conducted? What's the additional human effort required (such as prompt engineering and rounds of re-playing for prompt tuning)?\n\nWe addressed your comments in red-ink of Appendix Sec. F.1 for our revision.\n    \n> The method employed in the paper primarily relies on intensive prompt engineering, without introducing novel designs for multi-agent cooperation. .... ....... Providing only screenshots of part of the prompts as in Figures 6 and 7 is not enough. More details on the full prompt and game episode may help clarify these concerns.\n    \nWe addressed this issue in Appendix Section H and Section B. We add parts of the log as \"The full interaction history for finishing one dish in level 9\", which is shown in Sec. H. and we provide the full game log  [here](https://drive.google.com/file/d/1pLViwnC_ngL4cArDnjhLqVchZKZ-6sp6/view?usp=sharing) for your reference. Due to the page limitaion, we can't attach the full game log in our revised paper. Hope for your understanding. \n    \n> Table 12 made a good summary of related benchmarks though some related embodied multi-agent cooperation benchmarks with most of the features in the table are missing, such as [3][4]. \n    \nWe compare our work vs [3][4] in the Table 13 of Appendix Section B1 with red-ink, and we cited the papers and discussed the differences with our work in line 3, line4, line 5, line 11, and line 12 in the section 2 as well as our revised manuscript ."
                    },
                    "title": {
                        "value": "Response to Reviewer YZDa, 8/N"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736105565,
                "cdate": 1700736105565,
                "tmdate": 1700738367256,
                "mdate": 1700738367256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NztA0vAnLr",
                "forum": "p9pBJv1DTz",
                "replyto": "I2EMoNlmZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Thanks for the detailed responses! I highly appreciate it and some of my concerns are addressed. However, my main concern remains: the claims made in this paper are not well supported by the experiments and results provided, and there are not many new efforts in the revision to improve this. Therefore I'm keeping my score for this version of the paper.\n\nThanks for the thoughtful comments. Furthermore, we also conducted additional reinforcement learning experiments. Results and comparison with our MindAgent results are be added in the Appendix Table 12 for supporting our claims. \n    \nWe really appreciate your very helpful suggestion and all your constructive feedback. We addressed each of your comments in our revised manuscript for your reference."
                    },
                    "title": {
                        "value": "Response to Reviewer YZDa, 9/N"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736131405,
                "cdate": 1700736131405,
                "tmdate": 1700739692832,
                "mdate": 1700739692832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0C9yoDABpQ",
            "forum": "p9pBJv1DTz",
            "replyto": "p9pBJv1DTz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_RwfK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_RwfK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an infrastructure, MindAgent, for evaluating planning and coordination capabilities in the context of gaming interaction. To facilitate multi-agent planning capabilities of LLMs, the paper designs an effective set of prompt templates, memory modules, and state and action processing modules. Additionally, the paper reformulates the optimization objectives and constraints of multi-agent planning into natural language descriptions and uses them as prompts to guide LLM planning. The paper also introduces a virtual kitchen game called CuisineWorld as a benchmark for LLM-based multi-agent planning. Furthermore, the paper evaluates the planning capabilities of various LLMs in multi-agent collaboration tasks and human-agent collaboration tasks in CuisineWorld using MindAgent."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "***Originality & Significance***\n\nAlthough this work is mainly application-oriented for LLMs, its novelty lies in proposing an infrastructure to explore the potential of LLMs in multi-agent planning and conducting experiments on multiple LLMs. I believe this work will inspire the LLM community and provide a valuable test bed for evaluating LLM capabilities.\n\n***Quality & Clarity***\n\nThis work provides detailed descriptions and examples of the components of MindAgent. The paper also offers a good description of the environment setup and level settings in CuisineWorld. Furthermore, the paper validates the abilities of multiple LLMs, such as GPT-4, through multi-agent collaboration and human-agent collaboration tasks, and provides detailed experimental settings. I think the paper is clear and of high quality."
                },
                "weaknesses": {
                    "value": "Please refer to the questions section."
                },
                "questions": {
                    "value": "1. Have the authors considered incorporating a human-agent communication module in MindAgent? As suggested by Gao et al. [1], introducing interpretable human-agent communication into collaborative games can effectively improve human-agent collaboration performance and human subjective preferences. Natural language is the best medium for human-agent communication, which is also a natural advantage of LLM-based agents in human-agent collaboration.\n\n2. Can the design of MindAgent support collaborative games that require more domain-specific knowledge? For example, Multiplayer Online Battle Arena (MOBA) games [1], First-person Shooter (FPS) games [2], and Diplomacy [3] have very complex gameplay and their outcomes heavily depend on the planning and collaboration capabilities of the agents. The authors could discuss how the infrastructure needs to be modified when extending it to other games, which would enhance the generalisability of the infrastructure.\n\n---\n\n[1] Gao, Yiming, et al. Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective. ICLR. 2023.\n\n[2] Jaderberg, Max, et al. Human-level performance in 3D multiplayer games with population-based reinforcement learning. Science. 2019.\n\n[3] FAIR, et al. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768453851,
            "cdate": 1698768453851,
            "tmdate": 1699636256058,
            "mdate": 1699636256058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CLTPTssR4R",
                "forum": "p9pBJv1DTz",
                "replyto": "0C9yoDABpQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RwfK"
                    },
                    "comment": {
                        "value": "> Have the authors considered incorporating a human-agent communication module in MindAgent? As suggested by Gao et al. [1], introducing interpretable human-agent communication into collaborative games can effectively improve human-agent collaboration performance and human subjective preferences. Natural language is the best medium for human-agent communication, which is also a natural advantage of LLM-based agents in human-agent collaboration.\n\nThank you for highlighting the potential of incorporating human-agent communication in MindAgent, as suggested by Gao et al. [1]. We agree that introducing interpretable human-agent communication in collaborative games is a promising direction for enhancing human-agent collaboration. Accordingly, we have updated our paper to include citations and discussions related to [1], acknowledging its relevance to our work.\n\n\nWe adopted a straightforward method for facilitating communication between the human user and the LLM-based agent. Participants in the experiment were instructed to verbally express their intents or comments. These verbal inputs were then transcribed and integrated into the LLM\u2019s context as \u201ccurrent human instruction: <instruction>\u201d.\n\n\nImplementation and Results: This simple yet effective approach allowed for real-time adjustments in the behavior of the LLM-based agents based on human inputs. For instance, in our demo videos can be found in the supplementary videos, when a user suggested a specific strategy or highlighted a priority task, the agent was able to promptly adapt its actions accordingly. This led to a more dynamic and responsive collaboration between humans and agents, showcasing the potential of natural language communication in enhancing the collaborative experience.\n\n\nIn summary, while our initial trials in human-agent communication are relatively basic, they clearly demonstrate the effectiveness of integrating natural language communication in LLM-based collaborative environments. We plan to further develop and refine this aspect of our system in future work, drawing inspiration from the findings of Gao et al. [1] and other seminal works in the field.\n    \n\n> Can the design of MindAgent support collaborative games that require more domain-specific knowledge? For example, Multiplayer Online Battle Arena (MOBA) games [1], First-person Shooter (FPS) games [2], and Diplomacy [3] have very complex gameplay and their outcomes heavily depend on the planning and collaboration capabilities of the agents. The authors could discuss how the infrastructure needs to be modified when extending it to other games, which would enhance the generalisability of the infrastructure.\n\n    \nWe thank the reviewer for suggesting the exploration of domain-specific knowledge in collaborative games like MOBA, FPS, and Diplomacy, and its implications for the design of MindAgent. Our current work in CuisineWorld has indeed laid the groundwork for such adaptations, primarily through the use of In-Context Learning (ICL).\n\nIn CuisineWorld and Minecraft, we inject domain-specific knowledge (such as recipes) directly into the context, which the Large Language Model System utilizes to inform its decision-making and collaboration strategies. This demonstrates the feasibility of adapting MindAgent to other domains where specific knowledge plays a crucial role.  In addition, techniques like Retrieval-Augmented Generation (RAG) and Fine Tuning could be pivotal in further developing MindAgent's capabilities to handle such domain-specific complexities.\n\nThanks for the suggestions, and we have updated the paper accordingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547984249,
                "cdate": 1700547984249,
                "tmdate": 1700547984249,
                "mdate": 1700547984249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nNInwZg9Lu",
            "forum": "p9pBJv1DTz",
            "replyto": "p9pBJv1DTz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_DUbr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_DUbr"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an infrastructure for LLM-based agents to perform task distribution across a number of agents. It focus on planning and coordination ability of LLM with in-context learning from a few examples. The work tests on a multi-agent gaming environment, CuisineWorld, as well as a few other multi-agent collaboration environments, and obtain promising results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Contributions:\n\n1. Evaluate LLM agent in a multi-agent gaming environment; test LLM's ability to serve as a task allocator.\n\n2. Proposes a collaboration score to evaluate and benchmark coordination agents."
                },
                "weaknesses": {
                    "value": "1. Technical novelty: The technical novelty of this work is lacking because the multi-agent setting is really just adding an LLM call to allocate what different agents should do. Concretely, it is a simple prompting and what is supposed to be an optimization problem is all packed into one LLM call. This doesn't seem to be a very principled way of studying the agent allocation. \n\n2. Problem setting: the multi-agent collaboration problem is reduced to a top-down allocation: a distributor distributes tasks for agents to do. But for a collaboration problem to work, there should also be communication between the agents, which this work does not study.\n- the title is also quite misleading: it is titled \"emergent interaction\" however, there is no real interaction between the agents, i.e., communication of agent's individual's ability, limitation, progress etc. It is more of a allocation / coordination problem of a central agent.\n\n3. Experimental results: The work evaluate on Minecraft environment, but there has been quite a few LLM-based Minecraft agents, and the work does not offer comparison between this method and existing works as baselines."
                },
                "questions": {
                    "value": "1. What are the common failure cases of the LLM agent allocation? Under what environment / optimization constraints would the agent fail to allocate works correctly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3098/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3098/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3098/Reviewer_DUbr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826823611,
            "cdate": 1698826823611,
            "tmdate": 1699636255984,
            "mdate": 1699636255984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jY7oOltw8U",
                "forum": "p9pBJv1DTz",
                "replyto": "nNInwZg9Lu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DUbr 1/N"
                    },
                    "comment": {
                        "value": "> Technical novelty: The technical novelty of this work is lacking because the multi-agent setting is really just adding an LLM call to allocate what different agents should do. Concretely, it is a simple prompting and what is supposed to be an optimization problem is all packed into one LLM call. This doesn't seem to be a very principled way of studying the agent allocation.\n\nThank you for the comments. First of all, we should point out that **the motivation of this paper is not about proposing a set of agent allocation tasks to compare planning proficiency between LLMs and other canonical planning methods (ex. what [1] did)**, but the following (**in our abstract**):\n\n```\nHowever, despite the introduction of numerous gaming frameworks, the community lacks adequate benchmarks that support the implementation of a general multi-agent infrastructure encompassing **collaboration between LLMs and human-NPCs**. We propose a novel infrastructure-MindAgent\u2014for evaluating planning and coordination capabilities in the context of gaming interaction. \n```\n\nThat is, we simply would like to explore whether LLMs, which enjoys a powerful and native natural language interface that can interact with humans, can support sohphsticated multi-agent collaboration and human-NPC collaboration tasks, and this results in our benchmark. We've made this very clear -- the subject of study is **collaboration between LLMs and human-NPCs**. \n\nThat being said, we do agree that showing some results of the canonical planning approach, including RL, could offer a better understanding of the dynamics of the collaboration tasks. Below, we provide some results of using a canonical RL algorithm on the proposed benchmarks for LLMs:\n\nWe implemented a masked version of Proximal Policy Optimization (PPO) approach. In additon, we provide heavily engineered dense reward. Despite leveraging privileged information (masked action, what action is valid during the execution), the RL strategies show limited success, even in two-agent settings. For level 1, PPO achieve 45.14% success rate and for level 4, PPO achieves 59.8% for \\tau=2.5 after training for 30M steps or 500, 000 episodes. In comparison, GPT-4 agent achieves 95%  success rate on both settings. \n\nFurther the trained RL policy cannot adapt to varying numbers of agents and to new tasks, or even a change in the number of ingredients. This limitation underscores the complexity and dynamic nature of our environment. \n\n[1] https://arxiv.org/abs/2302.06706\n\n> Problem setting: the multi-agent collaboration problem is reduced to a top-down allocation: a distributor distributes tasks for agents to do. But for a collaboration problem to work, there should also be communication between the agents, which this work does not study.\n\nWe thank the reviewer for pointing out that our method is a top-down allocation. The multi-agent task planning problem [1], is a very difficult problem when they are multiple dependencies.  We want to address that is different from previous settings as in [1], where the task is known priori. In our settings, the tasks are not known prior to the agents simulating the experience of a real kitchen, dish orders keep flooding in. This modification will only push the original problem harder. \n\nWe acknowledge that communication between agents is indeed a critical aspect of collaboration, and our current model does not explicitly study inter-agent communication. This is a limitation of our study, and we believe it represents an exciting avenue for future research. The integration of direct agent-to-agent communication would undoubtedly add a rich layer of complexity and realism to the collaboration model, offering insights into emergent behaviors, negotiation strategies, and decentralized decision-making processes. \n\nHowever, this addition will also add communication cost which might not be ideal for gaming applications where a faster response is desired. Therefore, in this specific work (MindAgent) that focuses on **collaboration between LLMs and human-NPCs** in **gaming scenarios**, we choose to anchor our study on a centralized planning scheme.\n\n[1] Korsah et al. A comprehensive taxonomy for multi-robot task allocation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547795823,
                "cdate": 1700547795823,
                "tmdate": 1700547795823,
                "mdate": 1700547795823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m5B0RalEqJ",
            "forum": "p9pBJv1DTz",
            "replyto": "p9pBJv1DTz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_tSpT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_tSpT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new benchmark CuisineWorld for evaluating the planning and coordination capabilities of LLMs in multi-agent settings. Different from previous research, this benchmark is characterized by (1) its incorporation of multi-task objectives, (2) the involvement of more than two agents, and (3) the utilization of a centralized system for coordination. In this context, an LLM functions as the centralized system. At every timestep, the LLM processes each agent's state along with a prompt including recipes, demonstrations, the fundamental rules of the game, and memory history, and outputs the optimal actions for the agents. The paper demonstrates that GPT-4 has a strong planning capabilities on the proposed benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The creation and development of a new benchmark, entailing a substantial amount of effort, is a significant strength of the paper.\n- The introduction of a novel metric, CoS (Coordination Score), to assess coordination capabilities is a noteworthy contribution."
                },
                "weaknesses": {
                    "value": "- The proposed LLM coordination system, MineAgent, lacks novelty as its approach of leveraging scratchpad or memory has been extensively explored in prior research.\n- The proposed environment features limited state and action spaces and furnishes all the requisite recipes to solve tasks, possibly oversimplifying the challenge. In this situation, heuristic or RL planners could be readily employed. However, the paper does not provide comparisons with these approaches.\n- The proposed environment bypasses low-level control, further oversimplifying the problem. From my understanding, an agent can move to any location with a single action, without the need to consider spatial information. This eliminates the need for spatial reasoning in the LLM planner.\n- The paper claims that the LLM can seamlessly adapt to new planning problems across different domains, but this assertion is questionable considering the dependence on context length. While the simplicity of the current setting allows all environment information to be described within 1K tokens, this approach may prove inadequate for more challenging environments. Additionally, engaging in manual prompt engineering to enhance performance may entail significant monetary costs.\n- The description of the environment setting requires further clarification. Does $\\tau_\\mathrm{int, (1)}$ mean that a new task will be added at every timestep? What is the maximum horizon of an episode?\n- The paper only highlights the emergent behavior of GPT-4 while neglecting to discuss its potential drawbacks. A more balanced perspective could be achieved by including examples of GPT-4's failure cases."
                },
                "questions": {
                    "value": "See the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3098/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3098/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3098/Reviewer_tSpT"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698988527826,
            "cdate": 1698988527826,
            "tmdate": 1699636255871,
            "mdate": 1699636255871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EfcIT0mlzN",
                "forum": "p9pBJv1DTz",
                "replyto": "m5B0RalEqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tSpT 1/N"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n> The proposed LLM coordination system, MineAgent, lacks novelty as its approach of leveraging scratchpad or memory has been extensively explored in prior research.\n\nFirst, we want to emphasize that compare to the generative agents, we are using a centralized coordination planning. In our work, we have a  complex combinatorial  action space,  each agent has a action space of 221 due to the presence of multiple ingredients, multiple tools available in the environment., and with four agents, the action space becomes 2385443281 due to exponential effect of adding more agents. In addition, our work explored sophisticated spatial-temporal reasoning. To efficiently collaborate, agents need to work on multiple different tasks at the same time, manage to avoid space conflict (two or more agents cannot work on the same tools), etc. \n\nIn  addition, compared with concurrent work focus on planning (ex. [1]), we have more agents, more environments, and our environments are extensible and adaptable.\n\nThirdly, we created a new benchmark with extensive experiment. We even perform cross-domain experiments to adapt our settings to Minecraft. \n\n[1] JARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n\n\n> The proposed environment features limited state and action spaces and furnishes all the requisite recipes to solve tasks, possibly oversimplifying the challenge. In this situation, heuristic or RL planners could be readily employed. However, the paper does not provide comparisons with these approaches.\n\nThank you for the comments. First of all, we should point out that **the motivation of this paper is not about proposing a set of planning tasks to compare planning proficiency between LLMs and other canonical planning methods (ex. what [1] did)**, but the following (**in our abstract**):\n\n```\nHowever, despite the introduction of numerous gaming frameworks, the community lacks adequate benchmarks that support the implementation of a general multi-agent infrastructure encompassing collaboration between LLMs and human-NPCs. We propose a novel infrastructure-MindAgent\u2014for evaluating planning and coordination capabilities in the context of gaming interaction. \n```\n\nThat is, we simply would like to explore whether LLMs, which enjoys a powerful and native natural language interface that can interact with humans, can support sohphsticated multi-agent collaboration and human-NPC collaboration tasks, and this results in our benchmark. We've made this very clear -- the subject of study is **collaboration between LLMs and human-NPCs**. \n\nThat being said, we do agree that showing some results of the canonical planning approach, including RL, could offer a better understanding of the dynamics of the collaboration tasks. Below, we try to address your concern on action space, alternative methods, etc:\n\nWe respectfully disagree the reviewer's observation regarding the \"limited state and action spaces\" of our proposed environment. However, we argue that the complexity of our environment is not trivial due to the presence of multiple and duplicated tools and ingredients. In a single-agent scenario, the action space expands to 221, and with four distinct agents, it grows exponentially to approximately 2.39 billion. This immense action space presents a unique challenge specific to multi-agent research.\n\nAddressing the enormous action space, we implemented a masked version of Proximal Policy Optimization (PPO) approach. In additon, we provide heavily engineered dense reward. Despite leveraging privileged information (masked action, what action is valid during the execution), the RL strategies show limited success, even in two-agent settings. For level 1, PPO achieve 45.14% success rate and for level 4, PPO achieves 59.8% for \\tau=2.5 after training for 30M steps or 500, 000 episodes. In comparison, GPT-4 agent achieves 95%  success rate on both settings. \n\nFurther the trained RL policy cannot adapt to varying numbers of agents and to new tasks, or even a change in the number of ingredients. This limitation underscores the complexity and dynamic nature of our environment. \n\n[1] https://arxiv.org/abs/2302.06706"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547527159,
                "cdate": 1700547527159,
                "tmdate": 1700547527159,
                "mdate": 1700547527159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ve2Q2bKuDN",
                "forum": "p9pBJv1DTz",
                "replyto": "m5B0RalEqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tSpT 3/N"
                    },
                    "comment": {
                        "value": ">  The paper claims that the LLM can seamlessly adapt to new planning problems across different domains, but this assertion is questionable considering the dependence on context length. While the simplicity of the current setting allows all environment information to be described within 1K tokens, this approach may prove inadequate for more challenging environments. Additionally, engaging in manual prompt engineering to enhance performance may entail significant monetary costs.\n\nWe thank the reviewer for pointing out that context length might be a issue for general planning. Indeed, there could be more sophisticated tasks with more complicated and long interaction. But:\n\n1. First of all, MindAgent aims at **benchmarking the multi-agent gaming interaction planning of LLMs and between LLMs and humans**. Per our results suggested, even for this relatively simplified scenarios with shorter interaction history and limited context-length requirement, the collaboration score can still low, meaning that the context-length is not the major concern given the current collaboration proficiency of LLMs.\n\n2. Moreover, even from the perspective of how can LLMs handle the context length, the context length is becoming less of a issue as the current version of GPT-4 can handle 128k context tokens in a single prompt,the approximately equivalent of 300 pages of text as suggested by OpenAI[6], which could allow sufficient interaction history. Moreover, we don\u2019t need to look at the full interaction history as is the case in our work. History trimming is a common practice  as in [4,5] and can further mitigate the context length limit. In addition, for tasks with interaction that could be extremely complicated, we can optional use memory-based approach as in [1,2,3].\n\n[1] Retrieval augmented generation for knowledge-intensive nlp tasks\n\n[2] Generation-augmented retrieval for open-domain question answering\n\n[3] JARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n\n[4] Compressing Context to Enhance Inference Efficiency of Large Language Models\n\n[5] LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\n\n[6] https://openai.com/blog/new-models-and-developer-products-announced-at-devday\n\n> The description of the environment setting requires further clarification. Does  \\tau_int,(1)  mean that a new task will be added at every timestep? What is the maximum horizon of an episode?\n\n\nWe apologize for the confusion.  \\tau_int,(1) means a new task will be added after a fixed time step. The specific time value of \\tau_int(1) depends on the environment complexity. We compute this value based on the minimum number of steps to complete the task in a single agent setting.\n\nHere are more details on how the task complexity and subsequently, \\tau_int is determined, these can also be found in Appendix Section E.4\n\nIn our approach, we initially construct a task graph delineating subgoals, which serves as the foundation for our computations. We then apply breadth-first search in a single-agent context to determine the optimal task sequence. This sequence is a key component for calculating task intervals in multi-agent collaboration scenarios. For each tool requiring activation, we incorporate its activation wait time into the respective task intervals. Additionally, for each new connection in the task graph, we increase the total task interval time by tripling the edge time. We assume each subgoal requires at least, goto, get and put 3 actions. The cumulative task interval is subsequently adjusted by a scaling factor of 0.3 and the variable $\\tau$. We pick the value $tau$ ranging from 1.0, 1.5, 2.0, 2.5 and 3.0 to represent different task difficulties.  This process enables us to effectively compute task intervals tailored for multi-agent collaborative environments of varied difficulties.\n\nWe have updated the manuscript to make this more clear.\n\n> The paper only highlights the emergent behavior of GPT-4 while neglecting to discuss its potential drawbacks. A more balanced perspective could be achieved by including examples of GPT-4's failure cases.\n\nWe thank the reviewer for pointing this out. We have updated the paper accordingly. Here we list the common failure mode: \n\n1.  GPT-4 heavily rely on feedback, without feedback its performance drops significantly as indicated in our Table 2.  \n\n2. GPT-4 is sensitive to prompt inputs as indicated in Table3, without inference knowledge, the performance will drop. We have updated the paper with these new discussions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547719411,
                "cdate": 1700547719411,
                "tmdate": 1700552363770,
                "mdate": 1700552363770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oCo1Qgtiet",
            "forum": "p9pBJv1DTz",
            "replyto": "p9pBJv1DTz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_t1AF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3098/Reviewer_t1AF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new gaming scenario and related benchmark based on a multi-agent virtual kitchen environment, CuisineWorld.  It introduces MindAgent, which demonstrates the in-context learning multiagent planning capacity of LLMs and brings several prompting techniques that help facilitate their planning ability. Extensive evaluations are conducted with multiple LLMs and prompting settings on the benchmark, including deploying the system into real-world gaming scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The work is solid and important to the community which provides a benchmark that supports the implementation of a general multi-agent infrastructure that encompasses collaboration between large language models (LLMs) and human-NPCs.\n\nThe paper is well-written and organized."
                },
                "weaknesses": {
                    "value": "1.\tThe font size of Figure 4 is too small.\n2.\tThe paper seems not provide a clear definition of the terms $q_{pim}$ and $c_{pim}$ in Equation 2."
                },
                "questions": {
                    "value": "As weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699093939890,
            "cdate": 1699093939890,
            "tmdate": 1699636255787,
            "mdate": 1699636255787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qp8KFjTOxj",
                "forum": "p9pBJv1DTz",
                "replyto": "oCo1Qgtiet",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t1AF"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n> The font size of Figure 4 is too small.\n\nThanks for the suggestion, we will update the paper accordingly.\n\n> The paper seems not provide a clear definition of the terms .\n\nThanks for pointing this out.  We apologize for the confusions. Here q refers to the utility/rewards the system will generate if the agent perform the specified action. C refers to the cost of the action. We have updated the paper to make this point more clear."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547412640,
                "cdate": 1700547412640,
                "tmdate": 1700547412640,
                "mdate": 1700547412640,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]