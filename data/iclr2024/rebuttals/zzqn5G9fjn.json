[
    {
        "title": "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages"
    },
    {
        "review": {
            "id": "UQfBBoocAY",
            "forum": "zzqn5G9fjn",
            "replyto": "zzqn5G9fjn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_xDut"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_xDut"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations."
                },
                "weaknesses": {
                    "value": "Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title."
                },
                "questions": {
                    "value": "The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437142685,
            "cdate": 1698437142685,
            "tmdate": 1699636121514,
            "mdate": 1699636121514,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rRhrzwZnkI",
                "forum": "zzqn5G9fjn",
                "replyto": "UQfBBoocAY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xDut"
                    },
                    "comment": {
                        "value": "### \n\nWe thank the reviewer for the insightful and positive feedback!\n\n**W1**\n\n> the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages:\u00a0https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThank you for recommending these excellent datasets for our evaluation.\n\nWe agree that diversifying our dataset to include African and Indic languages will significantly strengthen our paper's scope and alignment with its title. To address this, we have initiated experiments with MasakhaNEWS and plan to conduct further research with MasakhaNER and IndicNLP Catalog datasets shortly.\n\n**MasakhaNEWS** is a benchmark dataset for news topic classification covering 16 languages widely spoken in Africa, where African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. The task involves categorizing news articles into different categories like sports, business, entertainment, and politics. We choose English, Hausa, Kiswahili, French and Yor\u00f9b\u00e1 in our preliminary experiments. We sample 1433 instances for training and 411 for evaluation sets for each language.\n\nTable 2 in our revised paper presents the preliminary results of experiments focused on MasakhaNEWS. When employing Federated Prompt Tuning in comparison to the monolingual and centralized counterparts, in general, a significant gain in accuracy is observed when adopting our Federated approach compared with the monolingual baseline.\n\n| Method                        | eng   | fra   | hau   | swa   | yor   | Avg  |\n|-------------------------------|-------|-------|-------|-------|-------|------|\n| PE_Monolingual                | 79.08 | 84.91 | 75.18 | 76.64 | 52.8  | 73.7 |\n| PE_Centralized                | 79.81 | 87.10 | 80.78 | 84.18 | 64.48 | 79.3 |\n| PE_FL (Ours)  | 82.99 | 89.81 | 65.96 | 86.16 | 57.20 | 76.4 |\n\nQ1\n\n> The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the\u00a0`t+1`\u00a0on the last term does not make sense to me.\n\nWe appreciate your detailed feedback on the notation! We have revised the formula on page 5 and added more details to avoid any confusion in our updated version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503088099,
                "cdate": 1700503088099,
                "tmdate": 1700503088099,
                "mdate": 1700503088099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "56teHpaKWS",
                "forum": "zzqn5G9fjn",
                "replyto": "rRhrzwZnkI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_xDut"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_xDut"
                ],
                "content": {
                    "title": {
                        "value": "Feedback"
                    },
                    "comment": {
                        "value": "Thanks for the new results and integration. Can you please also report the experiments on African languages ?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503415148,
                "cdate": 1700503415148,
                "tmdate": 1700503415148,
                "mdate": 1700503415148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YhvDQa0GKX",
            "forum": "zzqn5G9fjn",
            "replyto": "zzqn5G9fjn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_E7Lk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_E7Lk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines"
                },
                "weaknesses": {
                    "value": "- The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review."
                },
                "questions": {
                    "value": "- Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Reviewer_E7Lk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698484432194,
            "cdate": 1698484432194,
            "tmdate": 1700794322411,
            "mdate": 1700794322411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xflh6hdtx5",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7Lk [1/6]"
                    },
                    "comment": {
                        "value": "## Q1\n> Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n\nThanks for pointing it out. We have corrected the citations for all references in our revised paper.\n\n## Q2\n> Many grammatical errors, such as \"Throughout the fine-tuning...\"\"\n\nWe appreciate your feedback on the grammatical errors. We have revised the grammar to avoid any confusion in our updated version.\n\n## W4\n> Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\nWe appreciate the feedback. Our current paradigm is general-purpose and can be easily adapted to other generative and knowledge-based tasks. In response, we have expanded our evaluations to encompass a broader range of scenarios, addressing the concern of limited task selection. This rebuttal is part of a series, and we will provide additional results during the discussion period.\n\n\n* Low-resource Dataset \n\nOur setting and results: **MasakhaNEWS**, covering 16 languages widely spoken in Africa, where African languages are severely under-represented in NLP research due to a lack of datasets covering several NLP tasks. The task involves categorizing news articles into different categories like sports, business, entertainment, and politics. We chose English, Hausa, Kiswahili, French and Yor\u00f9b\u00e1 in our preliminary experiments. We sample 1433 instances for training and 411 for evaluation sets for each language.\n\nTable 2 in our revised paper presents the preliminary results of experiments focused on MasakhaNEWS. When employing Federated Prompt Tuning in comparison to the monolingual and centralized counterparts, in general, a significant gain in accuracy is observed when adopting our Federated approach compared with the monolingual baseline.\n\n| Method | eng | fra | hau | swa | yor | Avg |\n| --- | --- | --- | --- | --- | --- | --- |\n| PE_Monolingual | 79.08 | 84.91 | 75.18 | 76.64 | 52.8 | 73.7 |\n| PE_Centralized | 79.81 | 87.10 | 80.78 | 84.18 | 64.48 | 79.3 |\n| PE_FL (Ours) | 82.99 | 89.81 | 65.96 | 86.16 | 57.20 | 76.4 |\n\n* Question Answering\n\nDataset and our setting: **MultiLingual Question Answering (MLQA)** is a benchmark dataset for cross-lingual question-answering performance, covering English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. We sample 1433 instances for training and 411 for evaluation sets for each language.\n\n* Machine Translation\n\nDataset and our setting: **UN Corpus** is a Machine Translation dataset of official records from the UN proceedings over the years 1990 to 2014, covering six languages: English, French, Spanish, Russian, Chinese, and Arabic. we sample 10k in each direction for training and 5k each for evaluation sets. We cover three machine translation directions: En \u2192 Fr, Ar \u2192 Es, Ru \u2192 Zh, and sample 10k in each direction for training and 5k each for evaluation sets.\n\nOnce we get the result, we\u2019ll update our response and our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620900102,
                "cdate": 1700620900102,
                "tmdate": 1700634961433,
                "mdate": 1700634961433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EE6qo0wrDh",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7Lk [2/6]"
                    },
                    "comment": {
                        "value": "## W1\n> The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is **no novelty, such as modification or adjustment to the method that may have give a better results**. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n\nWe appreciate the opportunity to address the concerns raised by the reviewer and would like to defend our proposal, emphasizing its novelty and significance. In summary, we would like to clarify that our paper introduces federated prompt tuning as a solution to help address the **linguistic and geographic boundaries** hindering the application of LLMs to **various regions and lower-resource languages**.\n\nWe would like to further clarify from the following two aspects:\n\n1. We emphasize the unique background and pressing need of our work, as we noticed the review may **overlook the part of multilingual and low-resource language in the initial review**.\n\nFirst, our work primarily focuses on the under-representation of multilingual and low-resource languages in large language models. As natural language processing technologies advance, not all languages have been treated equally by developers and researchers. There are around 7,000 languages spoken in the world, and approximately 400 languages have more than 1 million speakers. However, there is scarce coverage of multilingual datasets. This is especially true for low-resource languages, where data scarcity is a major bottleneck. Furthermore, the under-indexing of certain languages is also driven by access to compute resources. Mobile data, compute, and other computational resources may often be expensive or unavailable in regions that are home to **under-represented languages**. Unless we address this disproportionate representation head-on, we risk perpetuating this divide and further widening the gap in language access to new technologies. One pressing example is **biomedical data**. Due to its global scale, this digital content is accessible in a variety of languages, yet most existing NLP tools remain English-centric. This situation highlights the need for effective strategies: how can we exploit abundant labeled data from resource-rich languages to make predictions in resource-lean languages?\n\nAlso, we wanted to highlight the urgency and timeliness of the problem. The problem is very timely compared to other application scenarios. It was not even considered a year ago. Previously, due to the smaller size of language models, the demand for data was not as high, and different kinds and sources of data were treated equally. Currently, the progress of LLMs, their usability, the amount of attention they receive, and the increased regulation on data, compound and lead to the urgency of this problem, where we are among **the first batch to attempt to break both lingual and physical barriers**."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621027830,
                "cdate": 1700621027830,
                "tmdate": 1700634951395,
                "mdate": 1700634951395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2AQaFDHTgl",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7Lk [3/6]"
                    },
                    "comment": {
                        "value": "2. We emphasize the simplicity and effectiveness of our proposed paradigm, in which case **modification or adjustment are not necessary but might make it complex**.\n\nIn previous cases, data transmission was always one-directional. Existing approaches focus on solving this locally, for example, through cross-lingual transfer, as well as data augmentation and preference training to address these bottlenecks.\n\nIn our paper, we approach it from a collaborative (two-directional) perspective. By training LLMs collaboratively across multiple participants without sharing raw data, the accuracy, robustness, and generalizability of LLMs can be enhanced by **leveraging collective knowledge and exposing models to a wider range of linguistic patterns**.\n\nThere exists very little research from such a collaborative perspective for low-resource languages. **Our findings open up new avenues for exploration and have the potential to inspire future research in this area**.\n\n**Following the principle of Occam's razor**, we adopt the concept of \"federated\" as a simple and established concept to describe our solution to the problem, which not only contributes a timely and practical solution to a rapidly evolving field but also vividly depicts the key innovation of our paradigm: knowledge sharing and aggregation without data transmission.\n\nAdditionally, from the federated learning perspective, as far as we know, we are the **first paper to investigate the data efficiency and transferability** brought by federated learning, and we believe this sheds some light on how federated learning can benefit LLM training in terms of generalizability and stability, beyond simply mitigating compliance risks.\n\n[1] Joshi, Pratik, et al. The state and fate of linguistic diversity and inclusion in the NLP world. ACL 2020.\n\n[2] B\u00e9rard et al., A Multilingual Neural Machine Translation Model for Biomedical Data NLP-COVID19 2020.\n\n[3] Lauscher et al., From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers. EMNLP 2020.\n\n[4] Xia et al., Generalized Data Augmentation for Low-Resource Translation. ACL 2019."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621046946,
                "cdate": 1700621046946,
                "tmdate": 1700740462151,
                "mdate": 1700740462151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VROOU63S7h",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7Lk [4/6]"
                    },
                    "comment": {
                        "value": "## W2\n> Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application. The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation. There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n\nWe thank the reviewer for the insightful comments and concerns regarding privacy! We appreciate the opportunity to clarify this aspect of our work. It's important to note that multilingual finetuning here is not an approach for preserving privacy but rather a problem we aim to solve.\n\n1. Our approach inherently supports data privacy, specifically by complying with international data privacy regulations. This compliance minimizes the need for cross-border data transmission, ensuring legal compliance and facilitating collaboration among entities with limited local computing resources, as detailed in Section 3.2.\n\n2. We directly address privacy concerns by reducing the volume of transmitted data, thereby limiting potential privacy breaches. As demonstrated in Section 5.4, transmitting fewer parameters significantly reduces the risk of privacy leakage, aligning our methodology with the privacy focus highlighted in the abstract.\n\n3. We would like to provide evidence for alleviating memorization from the following aspects:\n\n(1) By freezing the core language model parameters, we prevent the model from altering its foundational understanding of language. Consequently, the prompt encoder reduces the risk of memorizing specific lexical cues and spurious correlations, as discussed in Section 5.1 [1].\n\n(2) Components of Federated Learning play an essential role in reducing unintended memorization [2]. Specifically, clustering data according to users\u2014a key design element in FL\u2014significantly reduces such memorization. Additionally, using the Federated Averaging method for training further decreases the risk.\n\n4. Regarding privacy protection, we acknowledge that we did not add extra privacy protection techniques to defend against potential privacy attacks, such as gradient inversion. Therefore, we appreciate the reviewer's valid points and have revised our paper to clarify our contribution to privacy and removed related claims about the capability of privacy protection to avoid confusion. However, various methods like secure aggregation (SA) and differential privacy (DP) can be applied in conjunction with our pipeline to further enhance privacy protection.\n\n[1] Lester, Brian, Rami Al-Rfou, and Noah Constant. \"The Power of Scale for Parameter-Efficient Prompt Tuning.\" EMNLP 2021.\n\n[2] Thakkar et al. \"Understanding Unintended Memorization in Language Models Under Federated Learning.\" PrivateNLP 2021."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623297302,
                "cdate": 1700623297302,
                "tmdate": 1700634934025,
                "mdate": 1700634934025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wEArYj9sVv",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7Lk [5/6]"
                    },
                    "comment": {
                        "value": "## W3\n> There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n> The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n\nThank you for your valuable suggestions! Following the reviewer's constructive feedback, we have implemented experiments with LoRA (r=8, lora_alpha=16, lora_dropout=0.1) and summarized the results in the table below.\n\nTable 4 in our revised paper presents the preliminary results of experiments on the NC task. Bold scores indicate the best performance between Prompt Tuning and LoRA in each column.\n\n| Method                          | en   | es   | fr   | de   | ru   | Avg  |\n|---------------------------------|------|------|------|------|------|------|\n| Monolingual                     | 92.4 | 84.7 | 79.5 | 88.3 | 89.0 | 86.8 |\n| Centralized                     | 93.9 | 86.7 | 82.9 | 89.5 | 88.6 | 88.3 |\n| FL (IID)                        | 94.1 | 86.9 | 82.7 | 89.4 | 88.8 | 88.4 |\n| FL (Non-IID)                    | 92.4 | 86.3 | 81.2 | 88.9 | 84.7 | 86.7 |\n| PE_Monolingual                  | 82.9 | 59.7 | 47.3 | 71.4 | 60.0 | 64.3 |\n| PE_Centralized                  | 89.1 | 76.2 | 67.4 | 78.8 | 75.9 | 77.5 |\n| PE_FL (IID) (Ours)              | 91.2 | 82.2 | 76.5 | 86.4 | 81.6 | 83.6 |\n| PE_FL (Prompt Tuning) (Non-IID) (Ours) | 87.8 | **79.2** | 73.7 | **83.1** | 79.5 | **80.7** |\n| PE_FL (LoRA) (Non-IID) (Ours)   | **89.3** | 76.0 | **75.4** | 75.8 | **83.2** | 79.9 |\n\nWe also conducted a comparison of parameter efficiency and communication overhead in the NC task:\n\n| Method                   | # Trainable Params | Communication Cost |\n|--------------------------|--------------------|--------------------|\n| Federated Full Finetuning| 278,655,764        | 108GB              |\n| Federated Prompt Tuning (Ours) | 1,202,708     | 478.93MB           |\n| Federated LoRA (Ours)    | 1,491,476          | 593.92MB           |\n\nAdditionally, we have included Figure 7 in Section 5.4 of our revised paper to clearly illustrate the comparison between Prompt Tuning and LoRA.\n\nThe results demonstrate that Federated LoRA and Federated Prompt Tuning achieve comparable performance, with Federated Prompt Tuning showing a slight advantage. In terms of data transmission and communication cost, Prompt Tuning requires only about 80% of the resources compared to LoRA, and leads to less privacy leakage.\n\nFurthermore, as the pretrained model scales up, the performance of Prompt Tuning rapidly improves, approaching or even surpassing full finetuning. This indicates its significant potential. Prompt Tuning also enhances the overall model's generalization capabilities. The prompt encoder acts as a mechanism to extract linguistic-specific patterns on the client and general linguistic patterns on the server, showcasing advantages that adapters cannot match.\n\nThis justifies our use of Federated Prompt Tuning in our research, considering its efficiency in terms of parameters and communication, as well as its capability for generalization and adaptation to low-resource languages, which are crucial and undoubtedly worth it.\n\nRegarding QLoRA, we did not consider quantization in our study since our focus is solely on updating the parameters of prompt encoders on clients and the server, keeping the pre-trained model frozen. QLoRA involves quantizing the pre-trained models, which falls outside the scope of our discussion and does not contribute to reducing communication costs, a key bottleneck in the federated setting."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633721164,
                "cdate": 1700633721164,
                "tmdate": 1700634932480,
                "mdate": 1700634932480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZOfMKYcpxM",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7Lk [6/6]"
                    },
                    "comment": {
                        "value": "please see our response to W5 in our first comment"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634656186,
                "cdate": 1700634656186,
                "tmdate": 1700634886655,
                "mdate": 1700634886655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PqtS4eN4sI",
                "forum": "zzqn5G9fjn",
                "replyto": "YhvDQa0GKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Machine Tanslation results"
                    },
                    "comment": {
                        "value": "We are pleased to present our preliminary results for Machine Translation. The process took more time than the classification tasks.\n\nOur pre-trained model is [M2M-100 with 418M parameters](https://huggingface.co/facebook/m2m100_418M), a many-to-many MT model capable of translating between any pairing of 100 languages.\n\nThe table below displays our preliminary results on fine-tuning a machine translation model using the UN Corpus dataset. The scores are measured with sacreBLEU; higher scores indicate better performance.\n\nWe can use the pre-trained model as a baseline without fine-tuning. In all cases, fine-tuning demonstrates improvement compared to using the pre-trained model directly. Additionally, our federated prompt tuning paradigm outperforms its centralized counterpart (33.4 avg. BLEU for Ours vs. 32.5 for Centralized), demonstrating the superiority of our proposed paradigm.\n\n| Method          | En-Fr | Ar-Es | Ru-Zh | Avg  |\n|-----------------|-------|-------|-------|------|\n| Pretrained Model     | 31.4  | 27.4  | 27.9  | 28.9 |\n| PE_Centralized  | 33.9  | 31.9  | 30.7  | 32.5 |\n| PE_PT (Non-IID) (Ours)| 34.3  | 33.8  | 32.1  | 33.4 |"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722818198,
                "cdate": 1700722818198,
                "tmdate": 1700722818198,
                "mdate": 1700722818198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yJ6uMWYzMY",
            "forum": "zzqn5G9fjn",
            "replyto": "zzqn5G9fjn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable."
                },
                "weaknesses": {
                    "value": "- poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation."
                },
                "questions": {
                    "value": "In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r",
                        "ICLR.cc/2024/Conference/Submission1909/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731849876,
            "cdate": 1698731849876,
            "tmdate": 1700723834276,
            "mdate": 1700723834276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "brp8kZt2wb",
                "forum": "zzqn5G9fjn",
                "replyto": "56teHpaKWS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
                ],
                "content": {
                    "title": {
                        "value": "English and French are not low resource languages"
                    },
                    "comment": {
                        "value": "Also, how to make sense of the fact that in PE_Centralized, low resource languages like hausa and swahili outperformed english?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538685078,
                "cdate": 1700538685078,
                "tmdate": 1700538685078,
                "mdate": 1700538685078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ywpwzEIXOE",
                "forum": "zzqn5G9fjn",
                "replyto": "yJ6uMWYzMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AG4r [1/3]"
                    },
                    "comment": {
                        "value": "### Q\n\n> In section 5.4.1\n> In both the NC and XNLI tasks, despite the total number of parameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting for less than 0.5% of the total.\n> Could the authors clarify which part of the model is being fine-tuned?\n\nYes, we clarify that we only update the prompt encoders. This includes the parameters of the local prompt encoders $h_k$ on Client k, and the parameters of the global encoder $h_g$ on the server in the revised paper (referred to as $h_{global}$ in the original paper). During this process, we keep the pre-trained language models frozen at all times.\n\nTherefore, the trainable parameters are solely those within the prompt encoders, in contrast to the total number of parameters involved in full fine-tuning.\n\nTo further clarify and avoid any confusion, we have revised some details in Section 3. Additionally, we have attached a figure (Figure 2) in the revised version of the paper, illustrating the architecture of our prompt encoder and the tuning process.\n\n### W1\n> poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n\nWe apologize for any confusion caused by the current citation format. We have corrected the citations for all references in our revised paper. \n\nWe realize the oversight in not defining certain abbreviations, such as NLI (Natural Language Inference), PFL (Prompt Federated Learning), and PLM (Pre-trained Language Models), at their first occurrence in the text. We appreciate you highlighting this point. In the revised paper, we have ensured that all abbreviations are defined upon their first use. \n\n### W2\n> claims unverifiable: no code release.\n\nWe provide an anonymized version of the code repository, accessible through this link: https://anonymous.4open.science/r/Breaking_Physical_and_Linguistic_Borders-F1C5. \n\n### W3\n> conflating existing metrics with innovation: language distance is not a new concept. \n\nThank you for your insightful comments on our paper. \n\nWe acknowledge and agree with your review that the concept of language distance is not novel, having been explored in various contexts previously. However, we emphasize that our work introduces this concept within a unique and specific scenario: multilingual federated tuning. Our novel application provides a fresh perspective on language distance as a metric that illustrates the **transferability** relationships in multilingual NLP. This enables us to analyze the performance of federated prompt tuning and local monolingual transfer learning for low-resource languages.\n\n\nTo avoid overemphasizing the novelty of the language distance concept itself, we've amended the language in the abstract from \"introduce language distance as a new concept\" to \"present a new notion of language distance\" in our revised paper. This modification more accurately reflects our contribution.\n\nWe refer to [1] and [2] for our language distance measurement. Furthermore, we are open to learning about any additional references or works related to language distance that the reviewers may be aware of. If there are specific references that we have overlooked or that could further strengthen our work, we welcome their inclusion to enhance the comprehensiveness and depth of our research. \n\n[1] Malaviya et al., Learning Language Representations for Typology Prediction. *EMNLP 2017*.\n\n[2] Littell et al., URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. *EACL 2017*."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554043299,
                "cdate": 1700554043299,
                "tmdate": 1700564302063,
                "mdate": 1700564302063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sAfU4fX5PA",
                "forum": "zzqn5G9fjn",
                "replyto": "yJ6uMWYzMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AG4r [2/3]"
                    },
                    "comment": {
                        "value": "## W4\n\n> conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters.\n> \n\n### Regarding concerns about baselines\n\nWe appreciate the opportunity to clarify this aspect of our work.\n\nIn previous cases, data transmission was always one-directional. Existing approaches focus on solving this locally, for example, through local transfer with monolingual data.\n\nIn our paper, we approach it from a collaborative perspective, which we call federated prompt tuning in our paper. By training LLMs collaboratively across multiple participants without sharing raw data, the accuracy, robustness, and generalizability of LLMs can be enhanced by leveraging collective knowledge and exposing models to a wider range of linguistic patterns.\n\nAs you mentioned, there exists very little research from such a collaborative perspective for low-resource languages. **Our findings open up new avenues for exploration and have the potential to inspire future research in this area.**\n\nWhat we would like to demonstrate is not simply the performance boost. It\u2019s the data efficiency (Section 5.2) and transferability for different language similarities (Section 5.3) of our paradigm\u2019s superiority on low-resource languages. \n\n-----\n\n### Regarding concerns about prompts\n\nWe would like to clarify that the prompts in our paper are NOT the same as classifier model input, and they are suited for all decoders-style LLMs. To further clarify the prompt tuning procedure and the prompt construction, we've added more details in Section 3 and Appendix B, and C in the revised version.\n\nInstead of selecting discrete text prompts in a manual or automated fashion, in our paradigm, we utilize **virtual prompt** embeddings that can be optimized via gradient descent. Specifically, each prompt encoder, whether global or local, takes a series of virtual tokens, which are updated during tuning to better aid the model.\n\nFigure 2 in our revised paper shows how our prompt tuning works on both clients and the server. Specifically, a textual prompt tailored for a specific task and input text is passed to the model. The task-specific virtual tokens are retrieved based on the textual prompt. With the input text tokenized, the discrete word token embeddings are retrieved. Then virtual token embeddings are inserted among discrete token embeddings and passed together into the pretrained models. Therefore, **the prompt is adaptable across various pre-trained model architectures, including decoder-style, encoder-style, or encoder-decoder style.**\n\n----\n\n### Regarding concerns about privacy\n\nWe appreciate the opportunity to clarify this aspect of our work.\n\n- Our approach inherently supports data privacy. Specifically, it complies with international data privacy regulations by minimizing the need for cross-border data transmission. This not only ensures legal compliance but also facilitates collaboration among entities with limited local computing resources, as detailed in section 3.2.\n\n- We directly address privacy concerns by reducing the volume of transmitted data, thereby limiting potential privacy breaches. As demonstrated in section 5.4, transmitting fewer parameters significantly reduces the risk of privacy leakage, aligning our methodology with the privacy focus highlighted in the abstract."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564121623,
                "cdate": 1700564121623,
                "tmdate": 1700564195084,
                "mdate": 1700564195084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AZQXEQokPt",
                "forum": "zzqn5G9fjn",
                "replyto": "Op4gfmZ8Yv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to learning about the results"
                    },
                    "comment": {
                        "value": "Look forward to learning about the results on Question Answering and Machine Translation. \nUpdated soundness"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614804185,
                "cdate": 1700614804185,
                "tmdate": 1700614804185,
                "mdate": 1700614804185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bqNtuDRRpV",
                "forum": "zzqn5G9fjn",
                "replyto": "yJ6uMWYzMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Machine Tanslation results"
                    },
                    "comment": {
                        "value": "We are pleased to present our preliminary results for Machine Translation. The finetuning took more time than the classification tasks.\n\nOur pre-trained model is [M2M-100 with 418M parameters](https://huggingface.co/facebook/m2m100_418M), a many-to-many MT model capable of translating between any pairing of 100 languages.\n\nThe table below shows our preliminary results on fine-tuning a machine translation model using the UN Corpus dataset. The scores are measured with sacreBLEU; higher scores indicate better performance.\n\nWe can use the pre-trained model as a baseline without fine-tuning. In all cases, fine-tuning demonstrates improvement compared to using the pre-trained model directly. Additionally, our federated prompt tuning paradigm outperforms its centralized counterpart (33.4 avg. BLEU for Ours vs. 32.5 for Centralized), demonstrating the superiority of our proposed paradigm.\n\n| Method          | En-Fr | Ar-Es | Ru-Zh | Avg  |\n|-----------------|-------|-------|-------|------|\n| Pretrained Model     | 31.4  | 27.4  | 27.9  | 28.9 |\n| PE_Centralized  | 33.9  | 31.9  | 30.7  | 32.5 |\n| PE_PT (Non-IID) (Ours)| 34.3  | 33.8  | 32.1  | 33.4 |"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722688161,
                "cdate": 1700722688161,
                "tmdate": 1700723050826,
                "mdate": 1700723050826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XVhgQAtsxS",
                "forum": "zzqn5G9fjn",
                "replyto": "bqNtuDRRpV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Reviewer_AG4r"
                ],
                "content": {
                    "title": {
                        "value": "Comprehensive evaluation"
                    },
                    "comment": {
                        "value": "Given the demonstrated performance of federated prompt tuning, I've updated the soundness score. Thank you for the followup."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723818585,
                "cdate": 1700723818585,
                "tmdate": 1700723818585,
                "mdate": 1700723818585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DwcYUFIxnh",
            "forum": "zzqn5G9fjn",
            "replyto": "zzqn5G9fjn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_LsRx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1909/Reviewer_LsRx"
            ],
            "content": {
                "summary": {
                    "value": "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "- In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix."
                },
                "questions": {
                    "value": "Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1909/Reviewer_LsRx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767055794,
            "cdate": 1698767055794,
            "tmdate": 1700887244625,
            "mdate": 1700887244625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JXKCnds4TR",
                "forum": "zzqn5G9fjn",
                "replyto": "DwcYUFIxnh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LsRx [1/2]"
                    },
                    "comment": {
                        "value": "Thanks so much for your valuable comments and feedback.\n\n## Q1\n\n> Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n\nYes. This phenomenon has also been observed in previous works on Federated Learning [1]. Here are some possible reasons (Section 5.1, Page 7):\n\nFirstly, **Federated Learning** has a **weight averaging** effect via the aggregation of clients\u2019 models, which could increase the generalization of the global model, further leading to higher performance [2]. Additionally, by freezing the core language model parameters and **only learning the prompt representations**, **prompt tuning** reduces the model\u2019s ability to overfit a dataset by memorizing specific lexical cues and spurious correlations [3].\n\nThese reasons demonstrate the superiority of our federated prompt tuning over the traditional centralized finetuning paradigm, offering a more robust and generalizable approach for low-resource languages.\n\n[1] Rehman, Yasar Abbas Ur, et al. \"Federated self-supervised learning for video understanding.\" *ECCV* 2022.\n\n[2] Izmailov, Pavel, et al. \"Averaging weights leads to wider optima and better generalization.\"  *UAI 2018* .\n\n[3] Lester, Brian, Rami Al-Rfou, and Noah Constant. \"The power of scale for parameter-efficient prompt tuning.\"  *EMNLP 2021* .\n\n------\n\n## Q2 & W2:\n\n> Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix.\n\n> How did you tune the training and parameter averaging?\n\nTo further clarify the prompt tuning procedure and the hyperparameters, we've added more details in Seciton 2 and Appendix B, C in the revised version.\n\n### Virtual Prompt\n\nWe have also included a detailed figure 2 in our revised paper to more clearly show how the prompts are constructed and tuned.\n\nIn general, instead of selecting discrete text prompts in a manual or automated fashion, in our Multilingual Federated Prompt Tuning paradigm, we utilize **virtual prompt embeddings that can be optimized via gradient descent**. The primary objective of each **prompt encoder **is to generate an effective prompt embedding for each client based on **task specific virtual tokens**, to guide the PLM in producing the desired outputs.\n\nFigure 2 in our revised paper shows the how our prompt tuning works on both clients and server. Specifically, a textual prompt tailored for a specific task and input text are passed to the model. Then task specific virtual tokens are retrieved based on the textual prompt. With the input text tokenized, the discrete word token embeddings are retrieved. Then virtual token embeddings are inserted among discrete token embeddings and passed together into the PLM. \n\n-----\n\n### Federated Prompt Averaging\n\nIn every communication round $t$, Federated Prompt Averaging includes the following steps.\n\n**Initialization**:\nThe server initializes the global prompt encoder $h_g^{t}$. Each client initializes its local prompt encoder $h_0^{t}, h_1^{t}, \\ldots h_k^{t}$.\n\n**Client Selection:**\nWe select a fraction $C$ of the total $K$ clients for training. This subset size is $m = \\max(C \\times K, 1)$. The subset we choose is denoted as $S$.\n\n**Local Encoder Tuning:**\nEach client $k$ in $S$ fetches the current global prompt encoder $h_g^{t}$, and assembles it with the PLM. During the local training on the local data $\\mathcal{D}_k$, The PLM's parameters stay fixed while local prompt encoder parameters $h_k^{t}$ are tuned.     \n\n\n**Aggregation**:\nThe server aggregates updates from all clients using weighted average. The global prompt encoder $h_g^{t+1}$ is updated based on the received parameters $h_k^{t}$ from clients for the next round of federated prompt tuning:\n\n$$h\\_g^{t+1}=\\sum\\_{k=1}^K \\frac{\\left|\\mathcal{D}\\_k\\right|}{\\sum\\_{k=1}^K\\left|\\mathcal{D\\}_k\\right|} h\\_k^{t}\n$$\n\n------\n\n### Hyper-parameters\n\n\nFor all of the experiments, we report results using the 1e-3 learning rate, and we use early stopping (5 epochs of no improvement).\n\nFor FL experiments, we adjust the parameter $\\alpha$ that controls the mixture of languages in the dataset. An $\\alpha$ value of 1.0 signifies a uniform mixture of all languages, while values closer to 0 indicate a dominant representation of individual languages or a more separated mixture.\n\nWhen we use Prompt Tuning to optimize the parameter efficiency, the prompt tuning init text is *Predict the category given the following news article* for all the News Classification tasks. By providing the string of words, we initialize virtual token embeddings from existing embedding weights. This string is tokenized and tiled or truncated to match the number of virtual tokens, which is 1 in our experiments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507451980,
                "cdate": 1700507451980,
                "tmdate": 1700634986952,
                "mdate": 1700634986952,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kQFyazCUf1",
                "forum": "zzqn5G9fjn",
                "replyto": "DwcYUFIxnh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LsRx [2/2]"
                    },
                    "comment": {
                        "value": "## W1:\n\n> In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n\nWe would like to kindly defend our proposal.\n\nTo further clarify the significance and originality of our work, we've added our motivation with Multilingual NLP Background in Appendix A and the Contribution paragraph in Section 1 in our updated paper.\n\nIn the paper, we introduced federated prompt tuning as a solution to help address the linguistic and geographic boundaries hindering the application of LLMs to various regions and lower-resource languages. Here we would like to provide some clarification about the motivation and significance of our research in the following two aspects.\n\n### Multilingual NLP and Low-resource Languages\n\nAs natural language processing technologies advance, not all languages have been treated equally by developers and researchers. There are around 7,000 languages spoken in the world, and approximately 400 languages have more than 1 million speakers. However, there is scarce coverage of multilingual datasets. This is especially true for low-resource languages, where data scarcity is a major bottleneck. Furthermore, the under-indexing of certain languages is also driven by access to compute resources. Mobile data, compute, and other computational resources may often be expensive or unavailable in regions that are home to under-represented languages. Unless we address this disproportionate representation head-on, we risk perpetuating this divide and further widening the gap in language access to new technologies [1].\n\nOne pressing example is biomedical data. Due to its global scale, this digital content is accessible in a variety of languages, yet most existing NLP tools remain English-centric [2].\n\nThis situation highlights the need for effective strategies: how can we exploit abundant labeled data from resource-rich languages to make predictions in resource-lean languages?\n\n### Timeliness of breaking physical and linguistic barriers in the LLM era\n\nWe wanted to highlight the urgency of the problem. The problem is very timely compared to other \"application scenarios.\" It was not even considered a year ago. Previously, due to the smaller size of language models, the demand for data was not as high, and different kinds and sources of data were treated equally. Currently, the progress of LLMs, their usability, the amount of attention they receive, and the increased regulation on data, compound and lead to the urgency of this problem, where we are among the first batch to attempt to break both lingual and physical barriers.\n\nIn previous cases, data transmission was always one-directional. Existing approaches focus on solving this locally, for example, through cross-lingual transfer, as well as data augmentation and preference training to address these bottlenecks [3, 4].\n\nIn our paper, we approach it from a collaborative perspective. By training LLMs collaboratively across multiple participants without sharing raw data, the accuracy, robustness, and generalizability of LLMs can be enhanced by leveraging collective knowledge and exposing models to a wider range of linguistic patterns.\n\nThere exists very little research from such a collaborative perspective for low-resource languages. With data and computing power being very important yet limited for LLMs, we've never needed such a lightweight collaborative paradigm more urgently than we do right now.\n\nSo we introduce the concept of \"federated\" as a simple and established progression to our problem, which not only contributes a timely and practical solution to a rapidly evolving field, but also vividly depicts the key innovation of our paradigm: knowledge sharing and aggregation (double direction) without data transmission.\n\nAdditionally, from the federated learning perspective, as far as we know, we are the first paper to investigate the data efficiency and transferability brought by federated learning, and we believe this sheds some light on how federated learning can benefit LLM on training generalizability and stability, beyond simply mitigating compliance risks.\n\n[1] Joshi, Pratik, et al. The state and fate of linguistic diversity and inclusion in the NLP world.  *ACL 2020*.\n\n[2] B\u00e9rard et al., A Multilingual Neural Machine Translation Model for Biomedical Data *NLP-COVID19 2020*.\n\n[3] Lauscher et al., From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers, *EMNLP 2020*.\n\n[4] Xia et al., Generalized Data Augmentation for Low-Resource Translation *ACL 2019*.\n\n---\n\n## Suggestions:\n\n> Figure number is missing on Page 2\n> \"As depicted in Figure , \"\n> Missing Figure/Table\n> \"This translates to over 99% reduction in the communication overhead shown in 3\"\n> Typo\n> \"Finetuning accuracy across different lanugages on the NC task.\"\n> \n\nWe appreciate your detailed suggestions on the typos and missing figure/table numbers! We have fixed them all in our updated version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516945861,
                "cdate": 1700516945861,
                "tmdate": 1700634997743,
                "mdate": 1700634997743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D7an9Prcnr",
                "forum": "zzqn5G9fjn",
                "replyto": "DwcYUFIxnh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1909/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer LsRx,\n\nWe would like to sincerely thank you again for your time in reviewing our work!\n\nWe understand you might be quite busy. However, as the discussion deadline is approaching, would you mind checking our response and confirming whether you have any further concerns or questions? Any further comments and discussions are welcomed!\n\nBest Regards,\n\nThe authors of Paper1909"
                    },
                    "title": {
                        "value": "Looking forward to your responses or further suggestions/comments!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731864835,
                "cdate": 1700731864835,
                "tmdate": 1700731911418,
                "mdate": 1700731911418,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]