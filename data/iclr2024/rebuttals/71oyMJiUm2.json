[
    {
        "title": "TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation"
    },
    {
        "review": {
            "id": "Nk57oWjcVo",
            "forum": "71oyMJiUm2",
            "replyto": "71oyMJiUm2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission924/Reviewer_d3Xc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission924/Reviewer_d3Xc"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new audio-visual speech synthesizer for talking head translation (audio in e.g. English -> audio & video in e.g. Spanish). Instead of relying on a cascade of models (e.g. STT -> translation -> TTS -> Speech to Video) it presents a single model that, without any intermediate mapping to audio or text, produces audio and video in parallel from the original audio stream via a speech-to-unit translation model (S2UT) and unit-to-audio-visual-speech synthesizer (Unit2Lip) which is the first to generate both of these modalities in parallel. The authors also introduce a Bounded Duration Predictor to predict the duration of each unit and ensure that the duration of the generated video is the same as the original video. The authors train AV-resynthesis on LRS2 and A2AV translation on LRS3-T. They find that this model achieves SOTA results and outperforms Cascaded models on MOS, and is competitive on BLEU (but not better - this is understandable since these cascaded models are trained on a lot more text data, as the authors mention), while being around 4x faster due to the parallel AV generation. The authors illustrate some examples of translation towards the end of the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I think this paper has valuable strengths. The proposed method makes sense as an alternative to cascaded models which are obviously suboptimal due to accumulated errors and non-parallelizable generation of audio and video. The architecture seems quite standard but reasonable. The training procedure seems adequate and the introduction of a bounded duration predictor is clever, and helps solve a non-trivial issue in this field. The draft is well-written and clear in most parts and the figures are clear and helpful in understanding what this method is trying to achieve. The results are clear and well-presented, and the authors are upfront about the method's limitations (weaker BLEU compared to cascaded models, for example). The discussion is insightful and welcome, and the conclusions are valid."
                },
                "weaknesses": {
                    "value": "Although the results are sufficient to show that this model is effective, I would appreciate more ablations. These could be on the loss coefficients, for example, or some ablations on the architectural choices (since the architecture as a whole is effectively taken for granted) would be welcome to have a bit more insight into why these components were chosen. The use, for example, of other discrete units as an intermediate representation (the units from EnCodec, or its many variants, or quantized versions of other recent SSL methods) could be explored and would help the readers understand why HuBERT was chosen here specifically. Table 4 should be move to the appendix in my opinion - it's insightful but does not add enough to justify including it in the main draft.\n\nAlthough I understood it after a few reads, I don't think the Bound Duration Predictor section (4.2) is particularly well-written. There is, for example, a sentence that I believe must have a typo since it does not seem to make sense \"(...) we will first the frame represented\nby 0.2 in 2.2 will be discarded due to its low weight and the sequence of input discrete units can be represented as (...)\". Appendix section A is helpful but could also be written in a more eloquent way, in my opinion, and the pseudocode could be a bit more readable. \n\nTypos: \n - Table 1, for FID, lower is better, so the arrow next to it should be pointing down, not up.\n- Page 7 - \"Implement details\" should be \"Implementation details\""
                },
                "questions": {
                    "value": "-  Do the authors plan to release code for 1. training 2. inference and 3. pre-trained models? This would be very valuable for the community as a whole."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission924/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission924/Reviewer_d3Xc",
                        "ICLR.cc/2024/Conference/Submission924/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698037714023,
            "cdate": 1698037714023,
            "tmdate": 1700471620736,
            "mdate": 1700471620736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "pEqHCXGLrO",
            "forum": "71oyMJiUm2",
            "replyto": "71oyMJiUm2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission924/Reviewer_Qjzu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission924/Reviewer_Qjzu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents work towards translating audiovisual speech from one language to audiovisual speech in different languages.  Rather than the traditional pipeline that applies a sequence of models (ASR > MT > TTS), where errors compound, a direct approach is adopted here via units learned using self-supervision.  This results is better audiovisual speech quality in terms of objective and subjective metrics, and results in >4x improvement in generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is tackling a challenging problem.  Speech to speech translation is an important problem, and adding the visual modality significantly increases the complexity of the problem.\n+ The authors have considered both objective and subjective assessment, and the results demonstrate the utility of their approach.\n+ The paper builds nicely on prior work, and the use of open data helps with reproducibility."
                },
                "weaknesses": {
                    "value": "- The requirement to impose the duration constraint between the source and the target videos seems like a limitation for translation.  I can easily imagine cases where there is a significant mismatch in the length of the source and the target videos.  This seems to be a restriction because of the jarring effects of the background (which still are present in some of the examples) rather than the translation itself.  For example, the short phrase \u201cbucket list\u201d in English translates to \u201clista de cosas por hacer antes de morir\" in Spanish (although there are shortened forms)."
                },
                "questions": {
                    "value": "- I am slightly confused about the significance of the difference between extracting units from only acoustic speech vs. from the acoustic component of audiovisual speech.  In both cases there is only acoustic speech being clustered and so would the units not be equivalent?\n- In the example for the bounded duration predictor (Section 4.2) \u2014 the predicted sequence has a duration of 9 units (2.2 + 1.8 + 2.3 + 2.7) but should span T=10 units.  The paper refers to the first frame at 0.2 being discarded because of the \u201clow weight\u201d.  Why is the first frame at 0.2, and what weight?  From the example, it looks like you just round the predicted durations to the nearest integer and assume that number of repetitions for the duration, but this is not clear from the wording.\n- Should Figure 2(b) be referenced in Section 4.3?\n- You should state that a five-point Likert scale is used for the MOS ratings.  Also, although you state what is being measured, it would be useful to have the exact wording of the instructions for the viewers in these tests.  How the viewers are instructed to rate the videos can impact what they are actually looking for in the videos."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698255489765,
            "cdate": 1698255489765,
            "tmdate": 1699636019258,
            "mdate": 1699636019258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "QFfrGwr1LF",
            "forum": "71oyMJiUm2",
            "replyto": "71oyMJiUm2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission924/Reviewer_hZGE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission924/Reviewer_hZGE"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes TransFace to tackle the task of talking head translation: translates a source video into a target video with underlying audio language properly translated and lip-sync correctly. Traditionally, the translation process is done in a cascade manner where multiple components such as ASR, TTS, Wav2Lip are connected, which requires the pipeline to generate audio first, and then generate video based on audio. This work proposes a simpler pipeline that combines S2U and Unit2Lip where the first component translates the source language into target discrete units, and the second component synthesizes audio and video simultaneously. This approach achieves better synchronization and inference speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work proposes using target discrete units to synthesize target audio and target video at the same time, which helps the synchronization and inference speed. \nThe experiment contains evaluation from many different aspects and the results look convincing."
                },
                "weaknesses": {
                    "value": "Other than adapting the discrete units and duration normalization, many components and approaches seem to be identical to the previous works (i.e. Wav2Lip).\n\nSection 4.3, 4.4 describe the main model and should include more details. for example, it was not clear to me how a and v are computed in Synchronicity Loss. There are also some ambiguous descriptions in a few places, for example, In section 4.2. the author claims that the S2UT model decodes the phoneme sequence. however, If I understand correctly, the proposed approach is predicting discrete units, not phoneme units. why it is decoding phonemes?"
                },
                "questions": {
                    "value": "what's the importance of imposing isometric conditions? it seems natural to me that different languages might get different durations depending on the contents. Imposing the same duration condition might lead to unnatural video generation.\n\nduring the generation, does the top frame part also get modified to some extent or it is kept the same as the input?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734658358,
            "cdate": 1698734658358,
            "tmdate": 1699636019180,
            "mdate": 1699636019180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tbHhSYEkCc",
            "forum": "71oyMJiUm2",
            "replyto": "71oyMJiUm2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission924/Reviewer_hwnr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission924/Reviewer_hwnr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model for talking head translation, TransFace, which can directly translate audio-visual speech into audio-visual speech in other languages. It\nconsists of a speech-to-unit translation model to convert audio speech into discrete\nunits and a unit-based audio-visual speech synthesizer, Unit2Lip, to re-synthesize\nsynchronized audio-visual speech from discrete units in parallel. The model improves synchronization and boosts inference speed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Talking head translation has many practical applications but has not been systematically studied by prior work. The proposed method, with a duration predictor adapted to isometric setting, achieves better synchronization between audio and video. The proposed model has better inference speed compared to baseline systems."
                },
                "weaknesses": {
                    "value": "1. The proposed method lacks novelty in general. The unit-based approach has been widely used in speech synthesis, both in translation (e.g., [1]) and audio-visual setting (e.g., [2]). Duration prediction is standard in speech synthesis and has also been widely used in unit-based generation (e.g., [1]). \n2. The proposed method is evaluated on a constrained benchmark based on synthetic speech with limited video length. It is unclear how it compares to simple cascaded baselines under more challenging scenario. There is inconsistency between the BLEU score and MOS score in the translation results (row 5 vs. 8, Table 2) when compared to the baselines.\n\n[1]. Lee et al., 2021 Textless speech-to-speech translation on real data. \n\n[2]. Hsu et al., 2023 ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement"
                },
                "questions": {
                    "value": "Which reference set is used for calculating the FID statistics (the mean & standard deviation), and what model is employed for FID computation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission924/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission924/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission924/Reviewer_hwnr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809385281,
            "cdate": 1698809385281,
            "tmdate": 1700708194827,
            "mdate": 1700708194827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9E3dcv9Qpq",
                "forum": "71oyMJiUm2",
                "replyto": "tbHhSYEkCc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission924/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Voice cloning was not performed in most of the previous S2ST work."
                    },
                    "comment": {
                        "value": "Hello reviewer hwnr, **we've noticed that there may be some inherent impressions regarding the S2ST task, particularly with a focus on voice cloning**. To ensure a fair assessment and prevent any potential bias, we've curated demo pages from past speech-to-speech translation works for your consideration:\n\n1. ****Textless Speech-to-Speech Translation on Real Data [1]:**** https://facebookresearch.github.io/speech_translation/textless_s2st_real_data/index.html\n2. ****Direct Speech-to-Speech Translation With Discrete Units [2]:**** https://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html\n3. **TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation [3]:** https://transpeech.github.io/\n4. **SeamlessM4T-Massively Multilingual Multimodal Machine Translation [4]:** https://seamless.metademolab.com/\n\nThese demos highlight the emphasis on **improving the consistency of the translated speech content, prioritizing this aspect over the realization of voice cloning**. We trust that these demonstrations will provide valuable insights for your evaluation. If you have any further questions or require clarification, please feel free to engage in a discussion with us!\n\n[1] Lee A, Gong H, Duquenne P A, et al. Textless speech-to-speech translation on real data[J]. arXiv preprint arXiv:2112.08352, 2021.\n\n[2] Lee A, Chen P J, Wang C, et al. Direct speech-to-speech translation with discrete units[J]. arXiv preprint arXiv:2107.05604, 2021.\n\n[3] Huang R, Liu J, Liu H, et al. Transpeech: Speech-to-speech translation with bilateral perturbation[J]. arXiv preprint arXiv:2205.12523, 2022.\n\n[4] Barrault L, Chung Y A, Meglioli M C, et al. SeamlessM4T-Massively Multilingual & Multimodal Machine Translation[J]. arXiv preprint arXiv:2308.11596, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478691415,
                "cdate": 1700478691415,
                "tmdate": 1700478691415,
                "mdate": 1700478691415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]