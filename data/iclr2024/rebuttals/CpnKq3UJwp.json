[
    {
        "title": "Efficient Multi-agent Reinforcement Learning by Planning"
    },
    {
        "review": {
            "id": "vq3jbnESFy",
            "forum": "CpnKq3UJwp",
            "replyto": "CpnKq3UJwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a model-based multi-agent RL approach similar to MuZero, performing MCTS over the joint action space of all agents as imagined by learned models of the dynamics and reward. The authors propose to use optimism (Optimistic Search Lambda in conjunction with Advantage-Weighted Policy Optimization) to improve the performance over the behavioral cloning loss used in Sampled MuZero. Experiments on the SMAC benchmark compare the performance of this method with some previously proposed model-based and model-free MARL methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper demonstrates better or similar performance compared with several baselines. The paper is easy to follow (at least for someone familiar with MuZero and related works). Regarding novelty, I have not found any previous application of MuZero to multi-agent setting."
                },
                "weaknesses": {
                    "value": "There are several major weaknesses of the paper.\n\n1. The empirical validation of the method is very weak.\n    1. Despite the authors proposing the method to tackle large action spaces and mentioning the 27m_vs_30m settings as a motivating example in Section 3.2, there is no experiment in these large action space settings.\n    2. There\u2019s a lack of experimental validation beyond the SMAC benchmark, and the same benchmarks settings are used across all results and ablations. For example, Google Research Football and Multi-agent MuJoCo would be candidates for other tasks.\n    3. The SMAC benchmark is outdated and should be replaced by the SMACv2 benchmark, as many tasks have been shown to be trivially solvable due to the lack of stochasticity in the SMAC benchmark.\n    4. The baselines used are relatively few compared to other published MARL works. There should be comparison with other recent methods like MBVD, RODE, CDS, etc.\n    5. It\u2019s not clear how many seeds the work used for the main Table 3. The work mentions 3 seeds for followup ablations, but this is too few to ensure that the performance is not due to luck. The results should be reported across at least 10 seeds.\n2. To my knowledge, the search-based component of methods like MuZero are very reduced and not significantly useful for many non-board game tasks. For example, while MuZero utilizes a large search depth for Go, it uses a very small search depth for Atari, which raises the questions of how much performance gain the search component contributes. This work mentions that it uses 5 unroll steps, which is still very small. For MCTS-based works like this one, there should be empirical investigations with respect to the number of unroll steps at both training and test time to see whether search is offering any benefits.\n4. As this work mostly treats multi-agent decision problem as a single-agent RL with a very large action space, the authors should demonstrate $OS(\\lambda)$ and AWPO more extensively on (single-agent) decision problems with large action space; indeed, the author could even solely consider these single-agent settings rather than bringing in the multi-agent SMAC benchmark at all. The single-agent bandit experiment in Figure 1 is too simplistic and insufficient to demonstrate the authors\u2019 point. Also, in very large action spaces, MCTS with a limited budget (e.g. $N = 100$ MCTS simulations as considered in this work) seems very sparse and unlikely to simulate the same action multiple times from a given state. This would significantly weaken the motivation for $OS(\\lambda)$, which relies on simulating each action many times from a given state. In such large action spaces, a very relevant heuristics baseline is to sample $N$ simulations (each starting with a different action) with the learned policy and simply follow the action given by the best simulation. Overall, there's insufficient experiments demonstrating that $OS(\\lambda)$ and AWPO solve the stated large action space problem without suffering from unintended side-effects of the additional optimism, and there should be more trivial heuristics baselines which may also work well for searching large action spaces with a fixed budget."
                },
                "questions": {
                    "value": "Is MCTS being performed at evaluation time, or is only the learned policy used? If search is being performed at evaluation time, there should be a quantification of the search overhead compared with baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3293/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig",
                        "ICLR.cc/2024/Conference/Submission3293/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625498570,
            "cdate": 1698625498570,
            "tmdate": 1700723025911,
            "mdate": 1700723025911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cOuROGp7Ky",
                "forum": "CpnKq3UJwp",
                "replyto": "vq3jbnESFy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your valuable time, constructive comments, and dedication to helping improve the quality of our paper. The insightful feedback have been instrumental in identifying areas for improvement and refining our presentation. In response to your questions and concerns, we have made the following revisions and clarifications:\n\n**Q1: The search-based component of methods like MuZero are reduced and not significantly useful for many non-board game tasks.**\n\nThank you for raising concerns about the usefulness of search-based methods like MuZero in non-board game tasks. While it is true that model-free RL has achieved remarkable success in various tasks, it is important to recognize the growing importance of search-based methods, such as MCTS, in recent years.\n\nAlthough search-based methods may seem less significant compared to model-free RL, they  have actually demonstrated valuable contributions across different domains. EfficientZero [1], for example, has achieved impressive results on standard RL benchmarks. According to the paper, it achieves 194.3% mean human performance on the Atari 100k benchmark with only two hours of real-time game experience. Additionally, there are extensions of MuZero in continuous control [2,3] that outperform model-free baselines in terms of data efficiency. These results strongly indicate the effectiveness of search-based methods in these benchmarks.\n\nMoreover, search-based methods have made significant breakthroughs in application domains. For instance, AlphaTensor [4] discovered faster matrix multiplication algorithms, marking the first improvement in this area in 50 years. AlphaDev [5], its sister work, has also found faster sorting algorithms, some of which have been integrated into the standard sort function in the LLVM standard C++ library. Furthermore, these methods have been extended to NP-hard math problems, further demonstrating their potential and applicability.\n\nIn the field of AGI research, search-based methods show promise in providing reasonability for LLM-agents. Many recent works [6-8] are exploring the use of search-based methods to enhance the capabilities of large language models, indicating their potential for advancing natural language understanding and reasoning.\n\nOverall, it is crucial to acknowledge that search-based methods have the potential to be utilized in numerous important domains. With this in mind, the objective of this paper is to contribute to the application of search-based methods in the multi-agent domain, serving as a foundation for further exploration and progress in this field.\n\n[1] Ye, Weirui, et al. \"Mastering atari games with limited data.\"\u00a0*Advances in Neural Information Processing Systems*\u00a034 (2021): 25476-25488.\n\n[2] Hansen, Nicklas, Xiaolong Wang and Hao Su. \u201cTemporal Difference Learning for Model Predictive Control.\u201d\u00a0*International Conference on Machine Learning*\u00a0(2022).\n\n[3] Hubert, Thomas, et al. \"Learning and planning in complex action spaces.\"\u00a0*International Conference on Machine Learning*. PMLR, 2021.\n\n[4] Fawzi, Alhussein, et al. \"Discovering faster matrix multiplication algorithms with reinforcement learning.\" *Nature* 610 (2022): 47-53.\n\n[5] Mankowitz, Daniel J., et al. \"Faster sorting algorithms discovered using deep reinforcement learning.\"\u00a0*Nature*\u00a0618.7964 (2023): 257-263.\n\n[6] Yao, Shunyu, et al. \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models.\" arXiv preprint arXiv:2305.10601 (2023).\n\n[7] Feng, Xidong, et al. \"Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training.\" arXiv preprint arXiv:2309.17179 (2023).\n\n[8] Zhao, Zirui, Wee Sun Lee, and David Hsu. \"Large Language Models as Commonsense Knowledge for Large-Scale Task Planning.\" arXiv preprint arXiv:2305.14078 (2023).\n\n**Q2: For example, while MuZero utilizes a large search depth for Go, it uses a very small search depth for Atari, which raises the questions of how much performance gain the search component contributes. This work mentions that it uses 5 unroll steps, which is still very small.**\n\nIt appears there may be a misconception about its role in our model. The \u201d*unroll step*\u201d refers to the expansion steps during the model-learning phase and is distinct from the search depth used in the search phase. In our work, we set the *unroll step* to 5, mirroring the configuration used by MuZero in board games like Go. This setting, while seemingly modest, allows us to maintain a balance between learning a robust model and computational efficiency. It's important to note that despite an *unroll step* of 5, our model can achieve a search depth exceeding 20, with a total number of simulations ($N$) being 50. This demonstrates that a smaller *unroll step* is effective and sufficient for learning an accurate model, as also evidenced by MuZero's performance in complex games. We chose this approach primarily for efficiency while ensuring minimal model error."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656652611,
                "cdate": 1700656652611,
                "tmdate": 1700656652611,
                "mdate": 1700656652611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KwNUxxW8Oq",
                "forum": "CpnKq3UJwp",
                "replyto": "vq3jbnESFy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: There is no experiment in large action space settings like *27_vs_30m*.**\n\nWe are sorry for mentioning *27m_vs_30m* in our motivation section while remaining it unevaluated in our experiment section due to the relatively large time consumption for our algorithm in this map (MAPPO uses 10M data for this map). Because of the stress of computing power, we cannot provide another experiment for it. However, we would like to point out that there are still similar tasks like *10m_vs_11m* whose action space is $17^{10}\\approx 2\\times 10^{12}$, which is also considerably larger than the most complex environment evaluated by Sampled Muzero (Go, $361$ actions). Therefore, while we couldn't include *27m_vs_30m*, our chosen experiments still showcase the algorithm's robustness in complex environments.\n\nWe hope this clarification assures you of our algorithm's applicability to a range of scenarios, including those with large action spaces, even though not all could be directly tested due to resource constraints.\n\n**Q4: There\u2019s a lack of experimental validation beyond the SMAC benchmark, and the same benchmarks settings are used across all results and ablations. For example, Google Research Football and Multi-agent MuJoCo would be candidates for other tasks.**\n\nWe agree on the importance of validating the algorithm's performance in other tasks beyond the SMAC benchmark. It would be beneficial to validate the performance of MAZero in other tasks beyond the SMAC benchmark. We further benchmark MAZero on Google Research Football *academy_pass_and_shoot_with_keeper*, the result is shown below(see Appendix E, Table 11), where our method outperforms baselines in terms of sample efficiency.\n\n- Comparisons on *academy_pass_and_shoot_with_keeper,* Google Research Football.\n    \n    \n    | Env steps | MAZero | CDS | RODE | QMIX |\n    | --- | --- | --- | --- | --- |\n    | 500k | **0.123 \u00b1 0.089** | 0.069 \u00b1 0.041 | 0 | 0 |\n    | 1M | **0.214 \u00b1 0.072** | 0.148 \u00b1 0.117 | 0 | 0 |\n    | 2M | **0.619 \u00b1 0.114** | 0.426 \u00b1 0.083 | 0.290 \u00b1 0.104 | 0 |\n\n**Q5: The SMAC benchmark is outdated and should be replaced by the SMACv2 benchmark, as many tasks have been shown to be trivially solvable due to the lack of stochasticity in the SMAC benchmark.**\n\nWe appreciate the reviewer for the suggestion. Nevertheless, we did not evaluate MAZero in SMACv2 in our paper for three reasons:\n\n- **Focus on Deterministic Environments**: The primary focus of our research is deterministic environments, as opposed to stochastic ones. MAZero, building upon the legacy of MuZero, is designed for deterministic models. Our Optimistic Search Lambda technique, in particular, is tailored to leverage deterministic models. While testing on SMACv2 is possible, we believe it might detract from the clarity of our research objectives, which are centered on deterministic settings.\n- **Baseline Comparability**: The SMAC benchmark has been widely used for evaluating numerous baseline algorithms. Some baselines like MAMBA are benchmarked on SMAC, not SMACv2. To facilitate straightforward reproduction of results and ensure fair and relevant comparisons, we opted for the SMAC benchmark. This choice ensures that our findings are directly comparable to a broad range of existing research.\n- **Relevance of SMAC Despite Low Stochasticity**: Even though SMAC is recognized for its lower stochasticity compared to SMACv2, it still presents a meaningful and non-trivial challenge for multi-agent model learning and searching. Evaluating MAZero on SMAC effectively demonstrates our model's capabilities and the efficacy of our search techniques within this context.\n\nWe acknowledge the evolving landscape of benchmarks in this field and the potential value of including SMACv2 in future work. However, for the purposes of this paper, we believe that SMAC remains a relevant and suitable choice for demonstrating the strengths and applications of MAZero.\n\n**Q6: The baselines used are relatively few compared to other published MARL works. There should be comparison with other recent methods like MBVD, RODE, CDS, etc.**\n\nWe appreciate the reviewer\u2019s suggestion. We have added RODE and CDS as our baseline (see Figure 3) and results show that MAZero significantly outperform them, especially in terms of sample efficiency. We do not add MBVD as the baseline since we fail to find any open-source implementation of it.\n\n**Q7: It\u2019s not clear how many seeds the work used for the main Figure 3. The work mentions 3 seeds for followup ablations, but this is too few to ensure that the performance is not due to luck. The results should be reported across at least 10 seeds.**\n\nWe have considered the importance of using more seeds for the main Figure 3 to ensure robust evaluation results. We have conducted extra experiments and update Figure 3 with 10 random seeds."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656783531,
                "cdate": 1700656783531,
                "tmdate": 1700656783531,
                "mdate": 1700656783531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "86yaerxol4",
                "forum": "CpnKq3UJwp",
                "replyto": "vq3jbnESFy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q8: The authors should demonstrate OS($\\lambda$) and AWPO more extensively on (single-agent) decision problems with large action space**\n\nWe have considered demonstrating the effectiveness of OS($\\lambda$) and AWPO in single-agent decision problems with large action spaces. Specifically, we choose *LunarLander* environment but discretize action space into $400$ actions. Additionally, we select the *Walker2D* scenario in MoJoCo environment and discretize each dimension of continuous action space into 7 actions, i.e., $6^7 \\approx 280,000$ legal actions. Tables 9 and 10 in Appendix D present the results, demonstrating the enhancement of both techniques in terms of learning efficiency and final performance.\n\n- Ablation study on *LunarLander*.\n    \n    \n    | Env steps | MAZero | w/o OS($\\lambda$) and AWPO |\n    | --- | --- | --- |\n    | 250k | **184.6 \u00b1 22.8** | 104.0 \u00b1 87.5 |\n    | 500k | **259.8 \u00b1 12.9** | 227.7 \u00b1 56.3 |\n    | 1M | **276.9 \u00b1 2.9** | 274.1 \u00b1 3.5 |\n- Ablation study on *Walker2D*, MuJoCo.\n    \n    \n    | Env steps | MAZero | w/o OS($\\lambda$) and AWPO | TD3 | SAC |\n    | --- | --- | --- | --- | --- |\n    | 300k | **3424 \u00b1 246** | 2302 \u00b1 472 | 1101 \u00b1 386 | 1989 \u00b1 500 |\n    | 500k | **4507 \u00b1 411** | 3859 \u00b1 424 | 2878 \u00b1 343 | 3381 \u00b1 329 |\n    | 1M | **5189 \u00b1 382** | 4266 \u00b1 509 | 3946 \u00b1 292 | 4314 \u00b1 256 |\n\n**Q9: In very large action spaces, MCTS with a limited budget (e.g. $N=100$ MCTS simulations as considered in this work) seems very sparse and unlikely to simulate the same action multiple times from a given state. This would significantly weaken the motivation for OS($\\lambda$), which relies on simulating each action many times from a given state.** \n\nThank you for highlighting concerns regarding the application of OS($\\lambda$) in large action spaces with limited MCTS simulation budgets. We recognize the importance of clarity in explaining our methods and appreciate the opportunity to elucidate further.\n\nOS($\\lambda$), as employed in our work, is built upon the framework of Sampled MCTS, wherein each node expansion is conducted exactly once. This is a crucial aspect that seems to have been misunderstood by the reviewer. When a node $u$ is expanded, its set of child nodes, typically limited to a size of $K$ (about $5$ or $10$ in our settings), is determined through an on-policy distribution $\\beta$. This 'sampled' aspect ensures that each expanded node receives a unique value estimate through a forward pass of the value network, followed by backpropagation to the root. Later, when an expanded node is revisited, we recursively choose a son of it and do the same thing. Therefore, each expanded node will get its value network forwarded exactly once.\n\nContrary to the classical sampled MCTS, the final value of a node is the average of all estimations in its subtree (each node $x$ in its subtree constitutes an estimation in the form of $r_1+r_2+\u2026+v(x)$), which is too pessimistic when the model is deterministic and action space is large. This motivates us to design a more optimistic value of expanded nodes.\n\nWhen action space is large and the simulation budget $N$ is limited, the sampling procedure in classical sampled MCTS restricts the degree of the tree to at most $K$ ($K \\ll N$). This restriction enables deeper search depth, mitigating the sparsity issues. While OS($\\lambda$) performs optimistic value estimation, enhancing the search efficiency by effectively navigating the limited tree structure.\n\nWe hope this clarification underscores the suitability and efficacy of OS($\\lambda$) in handling the challenges posed by large action spaces and limited simulation budgets.\n\n**Q10: Is MCTS being performed at evaluation time, or is only the learned policy used? If search is being performed at evaluation time, there should be a quantification of the search overhead compared with baselines.** \n\nAs requested by the reviewer, we have conducted additional ablation study about whether or not perform MCTS in Appendix C, Table 8. Results on SMAC environments show that agents maintain comparable performance without MCTS planning during evaluation.\n\n| Map | Env steps | w MCTS | w/o MCTS | performance ratio |\n| --- | --- | --- | --- | --- |\n| 3m | 50k | 0.985 \u00b1 0.015 | 0.936 \u00b1 0.107 | 95.0 \u00b1 10.8% |\n| 2m_vs_1z | 50k | 1.0 \u00b1 0.0 | 1.0 \u00b1 0.0 | 100 \u00b1 0.0% |\n| so_many_baneling | 50k | 0.959 \u00b1 0.023 | 0.938 \u00b1 0.045 | 97.8 \u00b1 4.7% |\n| 2s_vs_1sc | 100k | 0.948 \u00b1 0.072 | 0.623 \u00b1 0.185 | 65.7 \u00b1 19.5% |\n| 2c_vs_64zg | 400k | 0.893 \u00b1 0.114 | 0.768 \u00b1 0.182 | 86.0 \u00b1 20.4% |\n| 5m_vs_6m | 1M | 0.875 \u00b1 0.031 | 0.821 \u00b1 0.165 | 93.8 \u00b1 18.9% |\n| 8m_vs_9m | 1M | 0.906 \u00b1 0.092 | 0.855 \u00b1 0.127 | 94.4 \u00b1 14.0% |\n| 10m_vs_11m | 1M | 0.922 \u00b1 0.064 | 0.863 \u00b1 0.023 | 93.6 \u00b1 2.5% |\n| average performance |  | 100% | 90.1 \u00b1 11.3 % |  |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656811039,
                "cdate": 1700656811039,
                "tmdate": 1700656811039,
                "mdate": 1700656811039,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zR2KDvErHa",
                "forum": "CpnKq3UJwp",
                "replyto": "86yaerxol4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing several of my concerns. Here I list my updated thoughts about the paper:\n1. After revisiting the literature on MuZero and EfficientZero, I do agree with the authors that MCTS-based approaches (in the representation space) may be useful for model-free settings like Atari (and SMAC, as shown in this paper). I am well familiar with the effectiveness of AlphaZero/MCTS for naturally model-based settings like Go, matrix multiplication (AlphaTensor), and assembly code generation. This is also demonstrated by the author's Appendix C Table 8: searching in the representation space does help.\n2. I am more encouraged by the author's additional comparisons with Sampled MuZero (i.e. MAZero w/o OS($\\lambda$) and AWPO) in LunarLander and Walker2D. However, selecting one Google Research Football task is insufficient in eliminating doubts of cherrypicking, and I would like to see more tasks.\n3. As helped by the author's clarifications, I can see why the authors have chosen relatively small SMAC environments now, due to the inherent scalability limitations of Sampled MuZero, despite the addition of proposed OS($\\lambda$) and AWPO. I think these choices of environments is fairly reasonable.\n\nI would be happy to raise my score to a 5 for now. Reflecting the above thoughts, my main outstanding doubts are:\n1. Given the algorithm's strong dependence on the performance of Sampled MuZero (as noted in my point #3 above), why is Sampled MuZero *not* a part of Figure 3 but instead relegated to ablation in Figure 5? I'm not thoroughly convinced that the method consistently outperforms Sampled MuZero, given only two SMAC comparisons (8m and 2c_vs_64zg) as well as LunarLander and Walker2D, with the majority of SMAC environments left out. **Why was Sampled MuZero not tested for 5m_vs_6m, 8m_vs_9m, 10m_vs_11m, 3m, 2m_vs_1z, so_many_banelings, and 2s_vs_1sc, while the 8m environment was not even introduced in Table 3?**\n2. A single Google Research Football task does not adequately demonstrate the performance of the method in that setting. For example, CDS uses three tasks: academy_3_vs_1_with_keeper, academy_counterattack_hard, and a designed full-field scenario 3_vs_1_with_keeper (full field). I would ideally want to see both MAZero and Sampled MuZero on multiple GRF tasks.\n\nUnfortunately, I know the authors have limited time to respond to these concerns. Though I'm keeping my score at a 5 for now, I'd be happy to keep an open mind if discussions with other reviewers are needed to make a decision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673602404,
                "cdate": 1700673602404,
                "tmdate": 1700673602404,
                "mdate": 1700673602404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zSGgE67c2U",
                "forum": "CpnKq3UJwp",
                "replyto": "vq3jbnESFy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback"
                    },
                    "comment": {
                        "value": "Thank you for raising the score and acknowledging our rebuttal results. We greatly appreciate your positive response to our additional experiments. Regarding your question about conducting ablation on only two SMAC maps, we specifically chose those maps because they exhibited the most apparent differences. This selection allowed us to effectively highlight the effects of the MCTS improvements. By focusing on these maps, we were able to demonstrate the impact of the ablation more clearly. Furthermore, we would like to clarify that the results presented in Table 3 were evaluated directly using the final model trained with MAZero, rather than starting from scratch. This approach enabled us to compare the performance whether or not conduct MCTS during evaluation in a more practical setting. Furthermore, we have extended the comparison between MAZero and Sampled MuZero across all the maps tested. Please note that in the supplemental experiment, Sampled MuZero uses the shared dynamic network we proposed; otherwise, the performance is very poor in SMAC.\n\n|  | steps | MAZero | Sampled MuZero(MAZero network) | Sampled MuZero(flattened network) |\n| --- | --- | --- | --- | --- |\n| 3m | 50k | 0.985 \u00b1 0.015 | 0.951 \u00b1 0.034 | 0.150 \u00b1 0.178 |\n| 2m_vs_1z | 50k | 1.0 \u00b1 0.0 | 1.0 \u00b1 0.0 | 0.803 \u00b1 0.091 |\n| so_many_baneling | 50k | 0.959 \u00b1 0.023 | 0.747 \u00b1 0.112 | 0.0 \u00b1 0.0 |\n| 2s_vs_1sc | 100k | 0.948 \u00b1 0.072 | 0.721 \u00b1 0.124 | 0.0 \u00b1 0.0 |\n| 2c_vs_64zg | 400k | 0.893 \u00b1 0.114 | 0.374 \u00b1 0.124 | 0.0 \u00b1 0.0 |\n| 8m | 1M | 0.993 \u00b1 0.009 | 0.751 \u00b1 0.078 | 0.0 \u00b1 0.0 |\n| 5m_vs_6m | 1M | 0.875 \u00b1 0.031 | 0.696 \u00b1 0.047 | 0.0 \u00b1 0.0 |\n| 8m_vs_9m | 1M | 0.906 \u00b1 0.092 | 0.812\u00a0\u00b1 0.089 | 0.0 \u00b1 0.0 |\n| 10m_vs_11m | 1M | 0.922 \u00b1 0.064 | 0.738 \u00b1\u00a00.205 | 0.0 \u00b1 0.0 |\n\nWe understand your concerns regarding the limited number of Google Research Football tasks we evaluated. Due to time constraints, we were unable to test our method on multiple tasks. However, we believe that the results we have presented lay a solid foundation for further investigation. We greatly appreciate your open-mindedness and willingness to consider discussions with other reviewers if necessary. We hope that our future work will address these concerns and provide more comprehensive results across a wider range of tasks."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721915704,
                "cdate": 1700721915704,
                "tmdate": 1700722028288,
                "mdate": 1700722028288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i2KMx7Mse2",
                "forum": "CpnKq3UJwp",
                "replyto": "zSGgE67c2U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_efig"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing these additional results. These have addressed my previous concern that the proposed OS($\\lambda$) and AWPO might not provide much benefit over Sampled MuZero. I trust that the authors will incorporate all discussed results during the rebuttal into the paper, preferably incorporating Sampled MuZero (or MAZero w/o OS($\\lambda$) and AWPO) into Figure 3 as full curves rather than just the win rate at the end (and perhaps removing Figure 5).\n\nI will raise my score to a 6. My concerns about the predominantly using the SMAC environments remain."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722990182,
                "cdate": 1700722990182,
                "tmdate": 1700722990182,
                "mdate": 1700722990182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0eQMhcYIYq",
            "forum": "CpnKq3UJwp",
            "replyto": "CpnKq3UJwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a novel algorithm, MAZero, based on the model-based ideas in MuZero, for the multi agent setting in order to improve problems with sample efficiency. Because the model based approach requires roll-outs to select policies, an efficient method of tree search is required, and so Optimistic Search Lambda (a method which weights optimistic values more highly) and Advantage-Weighted Policy Optimization (which uses a novel policy loss function based on the optimistic search lambda values).\n\nThe authors then compare this model based MARL algorithm to a number of others on the Starcraft Multi-Agent Challenge followed by a number of ablations of the different methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is in general well-written, and clear, with thorough appendices for the details. \n\nThe results given show that this model-based approach is not only more sample efficient than model-free approaches in the MARL setting but also, importantly, tractable when the Optimistic Search Lambda Algorithm and Weighted Policy Optimization are used within the tree-search. While these results are not surprising, such an approach has not been taken before and so there is definitely originality and significance to these results within the domain explored."
                },
                "weaknesses": {
                    "value": "The major issue comes down to the single, very specific domain that this has been tested on. While the results are, as stated above, impressive, they are only impressive in this single domain, and it would not seem difficult to show that they are just as significant in other domains with different types of action and state spaces (continuous, discrete, visual, tabular). \n\nIn addition, I believe that it should become standard within the community to utilise the evaluation protocol of Gorsanne et al (https://arxiv.org/abs/2209.10485) in the Multi-agent setting. These standard practices have not been followed and I believe weaken what could be a strong case for the effectiveness of these algorithms.\n\nNo discussion is given to hyper parameter tuning, or the choice of hyperparameters for the baselines that MAZero is being compared against.\n\nOn a stylistic note, the paper has a lot of grammatical typos and needs to be gone over thoroughly. An LLM should be able to pick up all of these mistakes.\n\nI would not use the word \"ingenious\" in the abstract to describe your own algorithm, or, as later used \"audacious\".\nCTDE is not spelled out the first time it is mentioned.\nThe different loss terms in equation 2 are not spelled out.\nIs Figure 1 a single seed? It looks remarkably smooth."
                },
                "questions": {
                    "value": "The questions all relate to the weaknesses described in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3293/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3293/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778848358,
            "cdate": 1698778848358,
            "tmdate": 1700726374921,
            "mdate": 1700726374921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lhI2lehts7",
                "forum": "CpnKq3UJwp",
                "replyto": "0eQMhcYIYq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your thoughtful comments, which have allowed us to improve the quality of our manuscript and to post a revised version. In what follows, we address the raised questions and weaknesses point-by-point.\n\n**Q1: Show that they are just as significant in other domains with different types of action and state spaces (continuous, discrete, visual, tabular).**\n\nThank you for highlighting the broader applicability of our work to various domains with different types of action and state spaces. You've rightly pointed out that our current focus with MAZero has been primarily on discrete environments. In an effort to extend our evaluation to continuous tasks, we employed a method of discretizing the continuous action space into a larger discrete action space. The results of this adaptation can be seen in Tables 9 and 10 of our paper, where MAZero demonstrates notable performance advantages in comparison to baseline algorithms. Due to the time limitation, we do not evaluate MAZero on visual and tabular tasks.\n\n**Q2: Use the evaluation protocol of Gorsanne et al (https://arxiv.org/abs/2209.10485)**\n\nWe agree on the importance of utilizing standard evaluation protocol within the community. Because of the stress of computing resources and limitation of time, we cannot restart the whole experiments under such a standard protocol during the response period. We will reorganize the results of all experiments in the next few weeks.\n\n**Q3: No discussion is given to hyper parameter tuning, or the choice of hyper-parameters for the baselines that MAZero is being compared against.**\n\nWe have discussed some crucial hyper-parameters (e.g. $\\rho, \\lambda$ used in OS($\\lambda$)) in ablation study in Appendix C. Detailed hyper-parameters are provided in Table 1, which are common settings in Sampled MuZero and EfficientZero.\n\nWe have added the choice of hyper-parameters for the baselines in Appendix B.3.\n\n**Q4: Is Figure 1 a single seed? It looks remarkably smooth.**\n\nWe appreciate the reviewer\u2019s careful observation! Yes, only a single seed is used, and it is done intentionally. We will briefly explain the motivation here, the full experimental setting, and details were listed in Appendix B.4. The aim of this experiment is to compare the behavior of these two different loss functions, i.e., BC and AWPO. To ensure that the comparison was as controlled and unbiased as possible, we eliminated potential variables that may influence the outcome. In this way, we \n\n- eradicate the randomness in policy evaluation by calculating the expected reward to evaluate the policies.\n- choose the same parameterization (softmax policy), and the same random seed (which only affects the initialization of the policy weights), this is why these two curves have the same starting point.\n- eliminate the randomness in SGD optimization, where the stochasticity comes from the randomness of sampling the subset (cf. Sampled MCTS) by calculating the expectation of the loss (the formula can be found in Appendix B.4), that is why these curves look smooth.\n\nWe hope this explanation clarifies the rationale behind our experimental setup and the resulting smoothness observed in Figure 1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656599038,
                "cdate": 1700656599038,
                "tmdate": 1700656599038,
                "mdate": 1700656599038,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l7ttDGSoHK",
                "forum": "CpnKq3UJwp",
                "replyto": "lhI2lehts7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
                ],
                "content": {
                    "comment": {
                        "value": "I thank that authors for their careful responses. I still believe however that the application only to SMAC environments means that the results are hard to extrapolate and thus hard to judge. Using this algorithm on a more diverse set of environments and using robust evaluation metrics would make this a substantially stronger paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684951489,
                "cdate": 1700684951489,
                "tmdate": 1700684951489,
                "mdate": 1700684951489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LTbX8obOSB",
                "forum": "CpnKq3UJwp",
                "replyto": "0eQMhcYIYq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your quick response. In response to your concerns about the experiments, we have conducted additional experiments beyond the SMAC environments. These experiments include LunarLander, MuJoCo, and Google Research Football. The results presented in Tables 9, 10, and 11 demonstrate the advantages of our algorithm across a diverse range of environments. Furthermore, we have also conducted experiments with an increased seed data. In future versions, we will prioritize considering the evaluation protocol you recommended.\n- Experiments on *LunarLander*.\n    \n    \n    | Env steps | MAZero | w/o OS($\\lambda$) and AWPO |\n    | --- | --- | --- |\n    | 250k | **184.6 \u00b1 22.8** | 104.0 \u00b1 87.5 |\n    | 500k | **259.8 \u00b1 12.9** | 227.7 \u00b1 56.3 |\n    | 1M | **276.9 \u00b1 2.9** | 274.1 \u00b1 3.5 |\n- Experiments on *Walker2D*, MuJoCo.\n    \n    \n    | Env steps | MAZero | w/o OS($\\lambda$) and AWPO | TD3 | SAC |\n    | --- | --- | --- | --- | --- |\n    | 300k | **3424 \u00b1 246** | 2302 \u00b1 472 | 1101 \u00b1 386 | 1989 \u00b1 500 |\n    | 500k | **4507 \u00b1 411** | 3859 \u00b1 424 | 2878 \u00b1 343 | 3381 \u00b1 329 |\n    | 1M | **5189 \u00b1 382** | 4266 \u00b1 509 | 3946 \u00b1 292 | 4314 \u00b1 256 |\n- Experiments on *academy_pass_and_shoot_with_keeper,* Google Research Football.\n    \n    \n    | Env steps | MAZero | CDS | RODE | QMIX |\n    | --- | --- | --- | --- | --- |\n    | 500k | **0.123 \u00b1 0.089** | 0.069 \u00b1 0.041 | 0 | 0 |\n    | 1M | **0.214 \u00b1 0.072** | 0.148 \u00b1 0.117 | 0 | 0 |\n    | 2M | **0.619 \u00b1 0.114** | 0.426 \u00b1 0.083 | 0.290 \u00b1 0.104 | 0 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704804441,
                "cdate": 1700704804441,
                "tmdate": 1700715860446,
                "mdate": 1700715860446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fXrfkarRFI",
                "forum": "CpnKq3UJwp",
                "replyto": "LTbX8obOSB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Reviewer_MHMy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for noting this. I have updated my scoring accordingly."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726406679,
                "cdate": 1700726406679,
                "tmdate": 1700726406679,
                "mdate": 1700726406679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7lf5rN7hW6",
            "forum": "CpnKq3UJwp",
            "replyto": "CpnKq3UJwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_6eaK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_6eaK"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents MAZero, a model-based multi-agent reinforcement learning (MARL) algorithm that combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search. The authors propose an ingenious network structure to facilitate distributed execution and parameter sharing. They introduce two novel techniques, Optimistic Search Lambda (OS(\u03bb)) and Advantage-Weighted Policy Optimization (AWPO), to enhance search efficiency in deterministic environments with sizable action spaces. Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* MAZero is the first empirically effective approach that extends the MuZero paradigm into multi-agent cooperative environments.\n* The proposed OS(\u03bb) and AWPO techniques improve search efficiency in large action spaces.\n* Extensive experiments on the SMAC benchmark demonstrate the effectiveness of MAZero in terms of sample efficiency and performance."
                },
                "weaknesses": {
                    "value": "* The paper focuses on deterministic environments, and it is unclear how well MAZero would perform in stochastic environments.\n* The proposed techniques may not be applicable to all types of multi-agent environments, and further research is needed to generalize the approach."
                },
                "questions": {
                    "value": "* How does MAZero perform in stochastic environments compared to deterministic ones?\n* Can the proposed OS(\u03bb) and AWPO techniques be applied to other model-based MARL algorithms?\n* Are there any potential drawbacks or limitations of the proposed network structure that the authors have not discussed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808103173,
            "cdate": 1698808103173,
            "tmdate": 1699636278103,
            "mdate": 1699636278103,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IlK87eMZ4Q",
                "forum": "CpnKq3UJwp",
                "replyto": "7lf5rN7hW6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your strong support recognizing the novelty of our work. We additionally are grateful for your comments and questions that we aim to address point-by-point in what follows.\n\n**Q1: How does MAZero perform in stochastic environments compared to deterministic ones?**\n\nOur algorithm primarily focuses on deterministic environments, as stated in this paper. We have developed a deterministic model based on the previous work of Muzero. Our OS($\\lambda$) technique is specifically designed to optimize this deterministic model. However, we have also conducted additional experiments beyond the SMAC environments, including LunarLander, MuJoCo, and Google Research Football, which exhibit more randomness compared to SMAC. The results in Tables 9, 10, and 11 demonstrate that MAZero performs well to some extent in stochastic environments.\n\n- Ablation study on *LunarLander*.\n    \n    \n    | Env steps | MAZero | w/o OS($\\lambda$) and AWPO |\n    | --- | --- | --- |\n    | 250k | **184.6 \u00b1 22.8** | 104.0 \u00b1 87.5 |\n    | 500k | **259.8 \u00b1 12.9** | 227.7 \u00b1 56.3 |\n    | 1M | **276.9 \u00b1 2.9** | 274.1 \u00b1 3.5 |\n- Ablation study on *Walker2D*, MuJoCo.\n    \n    \n    | Env steps | MAZero | w/o OS($\\lambda$) and AWPO | TD3 | SAC |\n    | --- | --- | --- | --- | --- |\n    | 300k | **3424 \u00b1 246** | 2302 \u00b1 472 | 1101 \u00b1 386 | 1989 \u00b1 500 |\n    | 500k | **4507 \u00b1 411** | 3859 \u00b1 424 | 2878 \u00b1 343 | 3381 \u00b1 329 |\n    | 1M | **5189 \u00b1 382** | 4266 \u00b1 509 | 3946 \u00b1 292 | 4314 \u00b1 256 |\n- Comparisons on *academy_pass_and_shoot_with_keeper,* Google Research Football.\n    \n    \n    | Env steps | MAZero | CDS | RODE | QMIX |\n    | --- | --- | --- | --- | --- |\n    | 500k | **0.123 \u00b1 0.089** | 0.069 \u00b1 0.041 | 0 | 0 |\n    | 1M | **0.214 \u00b1 0.072** | 0.148 \u00b1 0.117 | 0 | 0 |\n    | 2M | **0.619 \u00b1 0.114** | 0.426 \u00b1 0.083 | 0.290 \u00b1 0.104 | 0 |\n\n**Q2: Can the proposed OS($\\lambda$) and AWPO techniques be applied to other model-based MARL algorithms?**\n\nOS($\\lambda$) and AWPO are specially designed for MCTS algorithms. We don\u2019t think it can be applied to other model-based MARL algorithms like the dreamer-based MBMARL algorithms (e.g. our baseline method MAMBA).\n\n**Q3: Are there any potential drawbacks or limitations of the proposed network structure that the authors have not discussed?**\n\nWe recognize the limitations of the proposed network structure and have outlined potential drawbacks. Some potential drawbacks or limitations of the proposed network structure include:\n\n- The reliance on centralized training, which may not scale well to very large numbers of agents or highly heterogeneous environments.\n- Although MAZero is more sample-efficiency than model-free MARL method, it is relatively computationally expensive."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656567745,
                "cdate": 1700656567745,
                "tmdate": 1700656567745,
                "mdate": 1700656567745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "th69FZTPq1",
            "forum": "CpnKq3UJwp",
            "replyto": "CpnKq3UJwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_iouX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3293/Reviewer_iouX"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers MARL in a game environment, using a model-based approach. The heart of the paper is to extend MuZero, a model-based single agent RL algorithm that incorporates planning, to the multi-agent case in a Dec-POMDP setup.  The new algorithm is called mullti-agent Zero (MAZero). The extension of MuZero is achieved by adding and modifying the 3 functions of MuZero to create 6 functions that incorporate communications and expands how predictions are made.  The paper then focuses on efficient Monte Carlo tree search (MCTS) so that the planning complexity is reasonable. Numerical studies are carried out in Starcraft multi-agent challenge (SMAC) environments.  The experiments compare to model-free and model-based methods as baseline.  The results generally show improved learning efficiency for CDTE execution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is a logical extension of MuZero to the multi-agent case, and builds on those ideas to create six neural network functions that underly the model.  The writing is clear and the various cost functions and parameters are well laid out.  T\n\nThe experiments push the MARL problem in terms of action space complexity, and show that the MCTS method is effective in providing good performance with reduced search.  The primary contribution of the paper is to use prediction along with the reduced search in the multi-agent setting. \n\nIt appears that the method is generally applicable to Dec-POMDP problems under the assumptions in the paper, in particular the CDTE assumption. \n\nThe advantage-weighted policy optimization (AWPO) is an interesting way to balance cloning loss and the reduced tree search (the optimistic value)."
                },
                "weaknesses": {
                    "value": "Ultimately, the method gains in learning efficiency for the game studied, which is an important contribution, although it isn\u2019t clear that there is any performance gain compared to other CDTE methods. \n\nThe method seems to require global reward information at each agent during execution."
                },
                "questions": {
                    "value": "It seems that the method assumes that the agents have access to the global reward at each step during inference?  \n\nIn Figure 6, please clarify the difference between communication and sharing.  \n\nSection 3.1, the discussion about the degree to which a homogeneous solution is a good one is interesting and certainly depends on the particular scenario, but it isn\u2019t clear how the MAZero fits into this.\n\nCould you say more about the Shared Individual Dynamic Network g ?  Perhaps it is just the terminology but the idea seems confusing.  \n\nSection B.1, could you say more about how to \u201cuse positional encoding to distinguish agents in homogeneous settings\u201d?\n\nAppendix C, why do you think Adam is more effective than stochastic gradient descent?  Isn't this well known?\n\nSome small items:  Perhaps \u201cingenious\u201d and \u201caudacious\u201d are terms better left for the reader to decide for themselves?  The paper refers to \u201creal world\u201d cases but ultimately this is about a game environment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3293/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890804562,
            "cdate": 1698890804562,
            "tmdate": 1699636278046,
            "mdate": 1699636278046,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BYXAAY8Jrx",
                "forum": "CpnKq3UJwp",
                "replyto": "th69FZTPq1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3293/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your positive recommendation of our work and your insightful comments. Following are our responses to your concerns.\n\n**Q1: It seems that the method assumes that the agents have access to the global reward at each step during inference?**\n\nThe MAZero algorithm is designed under the CTDE framework, which means the global reward allows the agents to learn and optimize their policies collectively during centralized training. The predicted global reward is used in MCTS planning (inference) to search for a better policy based on the network prior. Ablation study in Appendix C, Table 8 shows that agents maintain comparable performance using the final model without global reward, communication or planning.\n\n**Q2: In Figure 6, please clarify the difference between communication and sharing.**\n\nWe have provided a clearer explanation of the differences between communication and sharing in Figure 6.\n\n- Communication: This refers to the exchange of information between agents to facilitate cooperation. In the MAZero algorithm, communication is achieved through an additional communication block using the attention mechanism. This allows agents to share their individual latent states and promote cooperation during the model unrolling process. \u201cNo communication\u201d means the Centralized Dynamic Block in Figure 2 only contains the Shared Individual Dynamic Network g.\n- Sharing: This refers to the parameter sharing among agents in the centralized-value, individual-dynamic model, which is a common technique for multi-agent reinforcement learning. By sharing parameters, the model can capture the homogenous behavior of agents and improve learning efficiency. \u201cNo sharing\u201d means there is no parameter sharing in representation function h, dynamic function g and policy prediction function P in Equation (4), i.e., agents learn network individually.\n\n**Q3: Section 3.1, the discussion about the degree to which a homogeneous solution is a good one is interesting and certainly depends on the particular scenario, but it isn\u2019t clear how the MAZero fits into this.**\n\nMAZero fits into the discussion of homogeneous solutions through parameter sharing, which has underscored the huge success in model-free MARL methods. More specifically, agents share representation network, policy prediction network and individual dynamic network, thus exhibit homogeneous behavior when facing the same situation or input similar observations.\n\n**Q4: Could you say more about the Shared Individual Dynamic Network $g_\\theta$ ? Perhaps it is just the terminology but the idea seems confusing.**\n\nWe have offered additional clarification on the Shared Individual Dynamic Network $g_\\theta$ and its role in the model in the revised version. The Shared Individual Dynamic Network $g_\\theta$ is responsible for deriving the subsequent local latent state $s_{t,k+1}^i$ of each agent $i$ based on its individual current latent state $s_{t,k}^i$ , individual action $a_{t,k}^i$, and communication feature $e_{t,k}^i$. The term \"Shared Individual Dynamic\" refers to the combination of shared parameters among agents (to capture homogenous behavior) and individual dynamic modeling for each agent (to capture local information input).\n\n**Q5: Section B.1, could you say more about how to \u201cuse positional encoding to distinguish agents in homogeneous settings\u201d?**\n\nPositional encoding is a detailed implementation of the self-attention block in the Communication network $e$. Using positional encoding to distinguish agents in homogeneous settings means that the algorithm incorporates the relative positions of agents in the environment to differentiate between them. This helps the model to learn distinct communication features for each agent, even when their actions or observations are similar.\n\n**Q6: Appendix C, why do you think Adam is more effective than stochastic gradient descent? Isn't this well known?**\n\nWhile it is well-known that Adam often performs better than stochastic gradient descent (SGD) in many cases, especially for multi-agent reinforcement learning[1], muzero-based algorithms tend to use SGD optimizer[2-4]. So we provide the ablation study to demonstrate the effectiveness of Adam optimizer specifically for our MAZero algorithm. The results show that Adam leads to superior performance and more consistent stability in the experiments.\n\n[1] Yu, Chao, et al. \"The surprising effectiveness of ppo in cooperative multi-agent games.\"\u00a0*Advances in Neural Information Processing Systems*\u00a035 (2022): 24611-24624.\n\n[2] Schrittwieser, Julian, et al. \"Mastering atari, go, chess and shogi by planning with a learned model.\"\u00a0*Nature*\u00a0588.7839 (2020): 604-609.\n\n[3] Hubert, Thomas, et al. \"Learning and planning in complex action spaces.\"\u00a0*International Conference on Machine Learning*. PMLR, 2021.\n\n[4] Ye, Weirui, et al. \"Mastering atari games with limited data.\"\u00a0*Advances in Neural Information Processing Systems*\u00a034 (2021): 25476-25488."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3293/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656305457,
                "cdate": 1700656305457,
                "tmdate": 1700656305457,
                "mdate": 1700656305457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]