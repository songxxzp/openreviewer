[
    {
        "title": "Efficient Unsupervised Knowledge Distillation with Space Similarity"
    },
    {
        "review": {
            "id": "Xt7y1V5qBM",
            "forum": "QHVTxso1Is",
            "replyto": "QHVTxso1Is",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
            ],
            "content": {
                "summary": {
                    "value": "In this study, the authors discuss boosting performance of knowledge distillation without use of ground-truth labels. Knowledge distillation usually uses combinations of human annotations and features/embeddings extracted from teacher models, but this study discusses knowledge distillation in an unsupervised setting and introduces cosine similarity and space similarity (CoSS) training objective to help student models learn to mimic teacher models' embedding structure. The proposed approach is numerically assessed mainly for ResNet-18 and EfficientNet-b0 in image classification tasks and image/video instance segmentation tasks. The evaluation also involves GPU memory requirement per method, and the result seems comparable to that of SEED baseline. Based on those results, the authors confirm the improvements by the proposed approach in many scenarios (models x methods x tasks) in efficient manners."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Given that majority of the existing studies on knowledge distillation requires human annotations to improve model performance, the reviewer sees some originality of this study. There are not many existing studies that discuss knowledge distillation in a pure unsupervised learning setting for target tasks.\n\nThe strongest point of this paper may be a lot of experiments the authors conducted such as multiple image classification tasks and image/video object segmentation tasks. Even though each of the experiments is not well described (e.g., lacking justification of baselines and hyperparameter tuning for those methods), the reviewer wants to recognize the attempt to show how the proposed method generalizes.\n\nThis study provides not only quantitative assessments but qualitative assessments e.g., Figs. 3 - 5. While Figs. 4 and 5 seem not much inconclusive, specifically with respect to BINGO+, the provided examples helped the reviewer see the representations learned with CoSS is more similar to that of teacher than SEED."
                },
                "weaknesses": {
                    "value": "Even though the reviewer appreciates many experiments for various tasks, this paper lacks descriptions and justifications of the baselines and how tuned the baseline methods are. \n\nThe reviewer also believes that it is unfair to use supervised methods without supervised signals (human annotations) as baselines unless their hyperparameters are tuned without the annotations, but neither hyperparameter tuning nor choices is described in this paper. \n\nFor the same reason, the reviewer also has a concern about reproducibility of this work. Appendix A.1 is not detailed enough (how teacher model is trained, architectures of additional trainable layers, baseline hyperparameters, etc) and does not cover all the experiments (at least) in the main body.\n\n\nThis paper also lacks clarity and needs improvement in writing.\n- The reviewer needs more clarifications in the description of the additional evaluation with kNN as it's not convincing. While Section 5.1 explains it is because the approach allows them to evaluate the methods directly without the need of tuning parameters, it is still dependent on the choice of $k$, which is not justified but heuristic.\n- In Section 1, the reduction of GPU memory requirement is emphasized, but the GPU memory requirement is not defined in this paper. How was it measured? If the teacher's embeddings are pre-computed and cached, it maybe easily save GPU memory usage as much as the proposed method does.\n- There are many typos and grammatical errors:\n  - \"distilled student\" should be replaced with \"trained student\", as (knowledge of) teachers are distilled into students, and students are not distilled\n  - and Attention transfer -> and attention transfer\n  - \"(i) feature queues (ii) contrastive objectives (iii) heavy augmentations (iv) and custom batch composition\" -> \"(i) feature queues, (ii) contrastive objectives, (iii) heavy augmentations, and (iv) custom batch composition\"\n  - Some notations are not defined where used. e.g., $\\widehat{A_t^T}$ and $\\widehat{A_s^T}$ in Eq. (4) $\\lambda$ in Eq. (5)\n  - DisCo vs. DISCO\n  - section vs. Section\n  - Table vs table\n  - Figure vs. figure\n  - In the Appendix -> In Appendix\n  - \"We compute nearest neighbour in the size 10 neighbourhood of the sample\" -> \"We choose 10 nearest neighbours from the training samples\"\n  - Ericsson et. al. Ericsson et al. (2021) -> Ericsson et al. (2021)\n  - ImageNet vs. imagenet vs. Imagenet\n  - \"two settings 1. traditional ... 2. Following\" -> \"two settings. 1) Traditional ... 2) Following\"\n  - \"as reported in 4\" -> \"as reported in Table 4\"\n  - We'd like -> We would like (not an error, but suggested)"
                },
                "questions": {
                    "value": "Questions\n- How did the authors reduce embedding space to make plots in Fig. 3 (c) and (d)?\n- What is the difference between $\\hat{A}_t$ in Eq. (3) and $\\widehat{A_t^T}$ in Eq. (4)? (same for $\\hat{A}_s$ vs. $\\widehat{A_s^T}$)\n- Why is SSKD (Xu et al., 2020) as referred as part of unsupervised distillation (Section 2.2)? SSKD does use human annotations.\n\nSuggestions\n\nHuman-annotation-free knowledge distillation is also discussed in the NLP community, and the authors may want to discuss the proposed \n or similar approach for NLP tasks in future work. For instance,\n- Embeddings-based KD: Reimers and Gurevych (2020) [\"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\"](https://aclanthology.org/2020.emnlp-main.365/)\n- Logits-based KD: Gupta et al. (2023) [\"Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages\"](https://aclanthology.org/2023.findings-acl.885/)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697525854411,
            "cdate": 1697525854411,
            "tmdate": 1699637143505,
            "mdate": 1699637143505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gLGns90csQ",
                "forum": "QHVTxso1Is",
                "replyto": "Xt7y1V5qBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "**We express our gratitude to the reviewer for investing their valuable time in reviewing our work. Their insightful observations have been duly noted, and we address them sequentially below.**\n\n> Even though the reviewer appreciates many experiments for various tasks, this paper lacks descriptions and justifications of the baselines and how tuned the baseline methods are. \n\nThe baselines provided in the paper are introduced and discussed in the related section. Regarding the tuning, as highlighted in A.1, we utilise their official implementations for reproducing results (along with official hyper-parameters). SEED, DisCo, BINGO all use identical settings for training and thus, by following SEED, we are able to make a fair comparison with other methods as well. More importantly, we follow SEED\u2019s training hyper-parameters such as augmentations, epoch, batch-size, optimiser etc for our work thus, making the results in sec 5.1 meaningful and valid. For all the subsequent experiments 5.3 and onwards, we employ the backbone obtained from sec 5.1. For sec 5.2, the hyper-param sweep is performed for all models (from 5.1) for all datasets following Ericsson et al. If there are any more specific details that the reviewer would like to be reported, we would be happy to incorporate it. \n\n> The reviewer also believes that it is unfair to use supervised methods without supervised signals (human annotations) as baselines unless their hyperparameters are tuned without the annotations, but neither hyperparameter tuning nor choices is described in this paper. \n\nIf the reviewer is referring to supervised methods in experiment 5.7, we would like to apologise for the oversight. As mentioned in the paper, we followed the official implementation of CRD for generating these results. For a fair comparison, for each baseline, we scaled the recommended loss multiplier $\\beta$ by factors of **{0.5, 1, 2, 4, 8 ,16}** and selected the value which yielded the highest top-1 for the **Resnet56/20** teacher-student pair. Note that the contribution of supervision and logics based KD is set to 0. Keeping this $\\beta$ fixed, we then performed distillation on all the other teacher-student pairs and reported the results. For CoSS, we performed the hyper-param search for **Resnet56/20** and fixed the obtained $\\beta=140$  for all other teacher-student pair as well. We will be adding the above mentioned information along with shortlisted $\\beta$ for each baseline to A.1 in the newer version. If there is any other additional that the reviewer would like us to cover, we will happily oblige. \n\n> For the same reason, the reviewer also has a concern about reproducibility of this work. Appendix A.1 is not detailed enough (how teacher model is trained, architectures of additional trainable layers, baseline hyperparameters, etc) and does not cover all the experiments (at least) in the main body.\n\nWe would like to clarify the reproducibility aspect of our work. As mentioned in A.1, we have adopted SEED\u2019s training hyper-parameters (teacher, scheduling, epochs, optimizer, augmentations etc) for the ImageNet distillation experiments. The cited paper thoroughly covers the necessary information. However, we would like to apologise for omitting the overall loss scaling value which was learnt from the CIFAR experiments as mentioned in the previous response. We directly apply the scaling factor ($\\beta$=140) learnt from CIFAR to the ImageNet experiments as well. Moreover, post decision deadline, we will be sharing the official code for our work.\n\n> The reviewer needs more clarifications in the description of the additional evaluation with kNN as it's not convincing. While Section 5.1 explains it is because the approach allows them to evaluate the methods directly without the need of tuning parameters, it is still dependent on the choice of k, which is not justified but heuristic.\n\nThank you for the insightful suggestion. kNN performance indeed depends on the choice of the neighbourhood. The choice of $k$, as reported in Section 5.1, is kept to be in-line with the evaluation setting adopted in SEED. The authors of SEED chose the value of $k=10$ in their evaluation. Though this justifies as to why we have used $k=10$, it does not explain as to why only $k=10$ should be preferred. To circumvent this issue, we have reported kNN with k={1,51,101} in A.2.2, where we see a similar trend across different k."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699693362186,
                "cdate": 1699693362186,
                "tmdate": 1699705152634,
                "mdate": 1699705152634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t0ufaQZaqG",
                "forum": "QHVTxso1Is",
                "replyto": "Xt7y1V5qBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (2/2)"
                    },
                    "comment": {
                        "value": "> In Section 1, the reduction of GPU memory requirement is emphasised, but the GPU memory requirement is not defined in this paper. How was it measured? If the teacher's embeddings are pre-computed and cached, it maybe easily save GPU memory usage as much as the proposed method does.\n\nThank you for pointing this out. We measured GPU memory as the maximum GPU space occupied by the method while processing an input batch. We reported the mean computed over the first 100 training batches. Training batch size for all methods is the same (as per implementation notes). Caching the representations will only allow us to not load the teacher model onto the GPU for computing embeddings. However, there are few downsides with this.\n\n (1) It will only save the GPU memory required by the teacher, since the teacher embeddings are required to be on GPU for loss (gradient) computation and hence does not free up GPU space required for embedding themselves.\n\n(2) Teacher Resnet-50 utilises considerably less GPU space (~100MB) when loaded only for inference/embedding generation. Additionally, the cost of a forward pass for this computation is negligible compared to the overall time some methods take.\n\n(3) Each method (including CoSS) loads the teacher onto the GPU, hence if an optimisation can help other methods, it should also help CoSS.\n\n(4) Usage of heavy augmentations like CutMix as in BINGO, makes it hard to pre-process and store all the teacher embeddings and hence requires an online computation of embeddings.     \n\nWe will be adding these omitted details to the appendix.\n\n> How did the authors reduce embedding space to make plots in Fig. 3 (c) and (d)?\n\nThank you for highlighting this aspect. The MLP architecture selected for this task performs the mapping 2->4->8->4->2->2. The 2D input is mapped via hidden layers to the 2D embedding layer which subsequently feeds into the 2D classification layer. We have visualised the 2D embedding space as learnt by the teacher and student models coming from the penultimate layer in Fig 3. (b,c,d). We will be adding this information along with more details to reproduce the plots in the appendix.\n\n> What is the difference between $\\hat{A}_t$ and $\\widehat{A^T_t}$ in Eq. (3) and in Eq. (4)?\n\nWe have used $\\hat{.}$ to denote the l2 normalisation of a matrix along the last dimension (columns). This means that $\\hat{A}_t$ is the batch of L2 normalised teacher\u2019s embeddings stacked along rows. $\\widehat{A^T_t}$ on the other hand first applies the matrix-transpose operation and then performs the l2 normalisation. Effectively, what we obtain is, l2 normalization along the space direction.  \n\n> Why is SSKD (Xu et al., 2020) referred to as part of unsupervised distillation (Section 2.2)? SSKD does use human annotations.\n\nWe mentioned SSKD along with CompRes and SEED to highlight a high degree of similarity in the KD objectives of these methods though they are for supervised and unsupervised tasks respectively. We can now understand the confusion our phrasing might create, and duly apologise for it. To rectify this, we have added a short intro to SSKD in the supervised section and reiterated on the fact that SSKD is supervised when mentioned in the unsupervised section.\n\n$\\textbf{Typos}$: We once again thank the reviewer for providing extremely thorough feedback on our work. We have addressed and fixed the grammatical errors and typos in our newer draft. We will be uploading it once we have incorporated suggestions from other reviewers as well.  \n\n> Human-annotation-free knowledge distillation is also discussed in the NLP community, and the authors may want to discuss the proposed or similar approach for NLP tasks in future work\n\nThe reviewer has provided a noteworthy perspective. We will be adding a discussion to highlight, compare and contrast unsupervised distillation methods across domains."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699693410074,
                "cdate": 1699693410074,
                "tmdate": 1699695568942,
                "mdate": 1699695568942,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SBptBtAWUz",
                "forum": "QHVTxso1Is",
                "replyto": "t0ufaQZaqG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the authors for their clarifications. Some of the reviewer's concerns were addressed.\n\n> We measured GPU memory as the maximum GPU space occupied by the method while processing an input batch. We reported the mean computed over the first 100 training batches. Training batch size for all methods is the same (as per implementation notes).\n\nThis is still a concern for the reviewer. Taking an average of peaked GPU spaces over batches should be noisy as only one data point is extracted at each batch, and the reviewer suggests discussing the GPU requirements using a more practical metric such as (how much GPU memory is consumed by a method) * (how long the method takes to complete the training)\n\nAlso, the reviewer highly recommends using different notations for multiple A hats. Those seem like very confusing."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291293042,
                "cdate": 1700291293042,
                "tmdate": 1700291293042,
                "mdate": 1700291293042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fncOenCCW6",
                "forum": "QHVTxso1Is",
                "replyto": "M8eGa330BQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "content": {
                    "comment": {
                        "value": "> We thank the reviewer for their continued engagement. We\u2019d like to highlight that the GPU space occupied is computed at each iteration and peak memory is often used as a reliable metric[1]. Though it may be noisy, by reporting the mean over the first 100 iterations we reduce its impact. We\u2019d also like to thank the reviewer for the suggested metric. We will be adding it to our results under Total-Training-Resources (TTR). Moreover, based on [1], we will also be providing the trend of (GPU, time) resources consumed per iteration over the course of training.\n\n> References [1] Cui, J., Wang, R., Si, S., & Hsieh, C. J. (2023, July). Scaling up dataset distillation to imagenet-1k with constant memory. In International Conference on Machine Learning (pp. 6565-6590). PMLR.\n\nWhile the authors claim that the peak memory is often used as a reliable metric in [1], the reviewer couldn't shortly find any such claims in the paper. Where can we find such claims and data points in [1]?\n\nThe reviewer is still concerned about the resource usage assessment since it uses only the first 100 iterations (not 100 epochs), which is largely affected by the batch size. Thus, the reviewer suggests removing GPU (GB) columns. \nBesides those, the reviewer wants to know how the runtime in this study was measured. Is it per sample? or per batch?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442081587,
                "cdate": 1700442081587,
                "tmdate": 1700442081587,
                "mdate": 1700442081587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eYMsPFVnwn",
                "forum": "QHVTxso1Is",
                "replyto": "P1ygmkQbVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks for the authors' follow-up comments. \n\n> > We\u2019d like to highlight that the GPU space occupied is computed at each iteration and peak memory is often used as a reliable metric[1]. \n\n> In the main paper, they don't cover what their metric 'GPU memory' corresponds to, but, in the appendix Tables 4,5 they specify the usage of peak GPU consumption as the metric. These numerical results correspond to the Figures 2,3 in their main paper (their section 5.3 highlights this fact) . Based on this, we believe that throughout their paper, the memory they refer to is indeed peak GPU memory consumed.\n\nIt does not support the initial statement \"peak memory is often used as a reliable metric[1]\", but sounds more like the authors' opinion.\n\n\nIt is good to learn that the authors use the same batch size across methods. The reviewer suggests adding the clarifications to the paper.\n\nLastly, the reviewer wonders why this work does not have Hinton et al.'s standard KD (but without the cross entropy term), which will be a strong baseline in terms of TTR (and peak GPU memory though the reviewer feels the metric is no longer necessary because of TTR)"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627893562,
                "cdate": 1700627893562,
                "tmdate": 1700627893562,
                "mdate": 1700627893562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "btEchfz40I",
                "forum": "QHVTxso1Is",
                "replyto": "Xt7y1V5qBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for their consistent and valuable feedback.\n\n> **It does not support the initial statement \"peak memory is often used as a reliable metric[1]\", but sounds more like the authors' opinion.**\n\nWe appreciate the reviewer feedback and subsequently have removed the peak-memory usage from Table 2. \n\n\n> **It is good to learn that the authors use the same batch size across methods. The reviewer suggests adding the clarifications to the paper.** \n\nWe appreciate the reviewer's acknowledgement. We will incorporate the suggested clarifications into the paper.\n\n> **Lastly, the reviewer wonders why this work does not have Hinton et al.'s standard KD (but without the cross entropy term), which will be a strong baseline in terms of TTR**\n\nWe followed prior works (SEED, BINGO etc.) in the domain of unsupervised distillation for setting up our baselines. \n\n[**Edited**] We agree that standard KD [1] (without CE) would be a strong baseline in terms of TTR, but, we also suggest that this baseline may not be a valid baseline for knowledge distillation. Standard KD formulates the distillation process as KL(Q||P), where $Q$ and $P$ corresponds to the teacher and student probabilities over the output space respectively. Without the CE term, it would be $\\sum_{x \\in \\mathcal{X}} Q(x) log(Q(x))$ which is independent of the student. Also, with the alternative formulation which many implementations follow (such as CRD[2]), we have $KL(P||Q)$ and in this case we have a training objective independent of the teacher. \n\nSince, the student will not learn anything from the teacher, we suggest that adding a baseline which diverges from the core methodology of knowledge distillation may not add value.\n\n[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015).\n\n[2] Tian, Yonglong, Dilip Krishnan, and Phillip Isola. \"Contrastive representation distillation.\" ICLR (2020)."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650224691,
                "cdate": 1700650224691,
                "tmdate": 1700671814313,
                "mdate": 1700671814313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8ap4kgaCqt",
                "forum": "QHVTxso1Is",
                "replyto": "btEchfz40I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_u1Un"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer is afraid that there are some misunderstandings regarding Hinton et al.'s KD.\nIts loss function is a linear combination of cross entropy and KL divergence terms, called soft and hard targets respectively.\nWithout the cross entropy term, it will be only KL divergence between softened class probabilities from teacher and student models. Thus, the student can learn from the teacher. In fact, one of the papers the reviewer suggested above shows that student models learned from teacher models without cross entropy term.\n\n> Logits-based KD: Gupta et al. (2023) [\"Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages\"](https://aclanthology.org/2023.findings-acl.885/)\n\nThe reviewer believes this is the formulation defined Hinton et al.'s KD paper (using a relative weight for the linear combination), and even early KD studies such as FitNets (2015) https://arxiv.org/abs/1412.6550 use the formulation while CRD paper was published in 2020.\n\nFor these reasons, the reviewer disagrees with the authors on the statement that student models won't learn anything from teachers by minimizing a standard KD loss without cross entropy term. This also means that the reviewer still believes that it is a strong baseline in terms of 1) TTR since there are no additional layers/modules to use other than the original student and teacher models and 2) accuracy when the temperature term is well-tuned."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672797606,
                "cdate": 1700672797606,
                "tmdate": 1700672797606,
                "mdate": 1700672797606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yDCALkSu28",
                "forum": "QHVTxso1Is",
                "replyto": "Xt7y1V5qBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing clarification. \n\nTo answer reviewer's query with the standard KD loss (with only KL), we are sharing results below for the ImageNet (ResNet-18, EfficientNet-b0), CIFAR-100 tasks with the aforementioned loss. Moreover, we also share the runtime and TTR for the ResNet-18 distillation on ImageNet. \n\n**ImageNet**\n\n| **Method** | **ResNet-18 (top-1)** | **ResNet-18 (kNN)** | **Eff-b0 (top-1)**   | **Eff-b0 (kNN)**  |\n|--------|-------------|----------------|-----------|-----------|\n| KD |   58.16   |   54.31 |   61.49  |  56.10  | \n| CoSS   |  **59.24**  | **55.04**  |  **63.55**  | **58.32**   |\n\n**CIFAR-100**\n\n| **Method** | **Resnet20/56** | **Resnet8x4/32x4** | **VGG8/13**   | **WRN16/40**  |\n|--------|-------------|----------------|-----------|-----------|\n| KD |    68.69   |   72.52    |  73.33    | 73.01     |\n| CoSS   | **71.11**   | **73.90**      | **74.58** | **74.65**     |\n\n**Runtime**\n\n| **Method** | **Runtime** | **TTR** | \n|--------|-------------|---|\n| KD |  0.19  |   322.26 |\n| CoSS   | 0.19 | 322.26 |\n\nThe difference in runtime was in micro-scale which was ironed out due to rounding off. CoSS and KD utilise the least amount of TTR compared to others. \n\nThe temperatures we used are $\\tau_{student}=0.1$ and $\\tau_{teacher}=0.07$ following the SSL adaptation of standard KD by [1]. \n\nWe agree with reviewer's feedback regarding KD being a strong baseline hence, we will be adding these results to the paper. \n\n**References:**\n\n    [1] Caron, Mathilde, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. \"Emerging properties in self-supervised vision transformers.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9650-9660. 2021."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677766719,
                "cdate": 1700677766719,
                "tmdate": 1700679482248,
                "mdate": 1700679482248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vxdyvQLPpa",
            "forum": "QHVTxso1Is",
            "replyto": "QHVTxso1Is",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_sCnR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_sCnR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an approach to unsupervised knowledge distillation that avoids reliance on a queue or contrastive loss. It identifies and addresses the non-homeomorphic issue in cosine similarity by enhancing both the Cosine similarity and Space Similarity between the student and teacher models. Compared to existing methods, this approach demonstrates significant improvement and reduces both training time and GPU memory usage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The proposed approach reduces training time and memory usage for unsupervised knowledge distillation, addressing the non-homeomorphic problem in cosine similarity by adding Space Similarity. \n\n2) Students trained by the proposed method have strong transferability and remain robust even in the face of input distribution shifts.\n\n3) This paper is easy to understand and implement."
                },
                "weaknesses": {
                    "value": "1) The contribution of the method is limited. A similar idea has already been discussed in [1]. For each pair of prediction vectors from the student and teacher within a batch $A^s, A^t \\in \\mathbb{R}^{b\\times d}$, [1] proposed inter-relation loss $\\frac{1}{b}\\sum^b_{i=1}g(A^s_{i,:}, A^t_{i,:})$, intra-relation loss $\\frac{1}{d}\\sum^d_{j=1}g(A^s_{:,j}, A^t_{:,j})$ where $g(\\cdot,\\cdot)$ is a distance function, $b$ is batch size and $d$ is the feature dimensions. [1] employ those loss functions on the logits vectors where $d$ is the number of classes. According to Figure 2, the proposed method is similar to [1]. The difference is the proposed method employs those loss functions on the feature vectors.\n\n2) The proposed method is behind some existing methods in the large dataset (ImageNet 1K) in Table 1. There is also an absence of a comparison with state-of-the-art methods, such as SMD[2].\n\n[1] Huang, T., You, S., Wang, F., Qian, C., & Xu, C. (2022). Knowledge distillation from a stronger teacher. Advances in Neural Information Processing Systems, 35, 33716-33727.\n\n[2] Liu, H., & Ye, M. (2022, October). Improving Self-supervised Lightweight Model Learning via Hard-Aware Metric Distillation. In European Conference on Computer Vision (pp. 295-311). Cham: Springer Nature Switzerland."
                },
                "questions": {
                    "value": "3) I am confused about $L_{ss}$ and $L_{co}$. For the representations matrix $\\hat{A_t}$, $\\hat{A_s}$ $\\in R^{b\\times d}$. What is the dimensions of the $A_{I}$ and $A_{II}$ in the Equation 3 and Equation 4? As mentioned in the paper, $A^i_s$ is only compared with\n$A^i_t$. If $L_{co}$ calculating the cosine similarity for each pair of features in the input batch, the dimensions of ${A_I}$ should be $R^{b}$\n\n4) There is a lack of ablation study for $L_{ss}$, $L_{co}$. Is the Space Similarity sensitive to the batch size? Could the authors conduct experiments using various batch sizes? How does the performance of the proposed method compare to SMD?\n\n5) Can authors compare the proposed method with the method mentioned in reference [1]?\n\n6) Can the authors provide additional details about ViT training in Tables 1 and 6? Were all ViTs trained from scratch?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647786583,
            "cdate": 1698647786583,
            "tmdate": 1699637143371,
            "mdate": 1699637143371,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zNWb8TZS3Y",
                "forum": "QHVTxso1Is",
                "replyto": "vxdyvQLPpa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "**We would like to express our sincere gratitude to the reviewer for their time to evaluate and provide feedback on our work. In particular, we would like to thank the reviewer for recognizing the simplicity and efficiency of our approach. We are also grateful for other suggestions. We address each of the noteworthy points they raised.**\n\n**Note: We will be uploading the updated version of the paper once we incorporate the changes proposed by all the reviewers. We thank the reviewer for their patience in this regard.**\n\n> **W1**: The contribution of the method is limited. A similar idea has already been discussed in [1]...\n\nThank you for bringing this to our notice. Indeed, [1] has a similar objective where they consider intra-class responses for their supervised setting. One subtle yet important difference in implementations of the two is their treatment of intra-class responses prior to the similarity computation. In [1], **the authors normalise the features prior to computing their inter and intra class losses**. We do not perform this step so as to avoid the loss of original manifold information which a normalisation operation induces. Another important difference is in the motivation and subsequently the setting to which these two strategies are applied. [1] utilises the class-wise scores as means to capture (class) prior from the teacher which consequently is modelled as a correlation maximisation problem. For this purpose, it makes sense to pre-normalize the logits to probability over classes before computing the intra-class loss. On the other hand, we have focused on modelling the manifold space of a teacher for which the space-wise similarity allows us to rectify the loss of information caused by the normalised feature-wise similarity. For this, we do not normalise features prior to computing space similarity. Lastly, we hope to remind the reviewer that unsupervised vs supervised is also usually considered as a distinction. We have added this discussion with [1] in the related work of our new draft.\n\n\n> **W2.A**: The proposed method is behind some existing methods in the large dataset (ImageNet 1K) in Table 1.\n \nThank you for noticing this, we agree that the proposed method is behind other methods in only terms of top-1 in Table 1. A gentle reminder is that we can achieve this with significantly less computing efforts. For example, BINGO which performs 0.7% better on (Resnet-18) top-1 requires roughly **9x more time** to train. And, for DisCO in case of efficientnet-b0, the GPU compute and training time required increases **~5x**. More importantly, additional experiments in 5.2 and onwards showed that the model from  the proposed method can still serve as an alternative backbone for various tasks where it often performs better than other models. \n\n > **W2.B**: There is also an absence of a comparison with state-of-the-art methods, such as SMD[2]\n\nThanks for the suggestion, we didn\u2019t compare SMD because of its different settings. For example, BINGO, DisCo, and SEED are using the same evaluation protocol, while the one SMD uses is different. \n\nWe are currently running distillation on ImageNet using SMD and will report the results in the coming days. Below are the preliminary results on CIFAR-100 for SMD compared with CoSS.\n\n| **Method** | **Resnet20/56** | **Resnet8x4/32x4** | **VGG8/13**   | **WRN16/40**  |\n|--------|-------------|----------------|-----------|-----------|\n| SMD[2]    | 70.33       | 71.41          | 74.10     | **74.93** |\n| Ours   | **71.11**   | **73.90**      | **74.58** | 74.65     |\n\n> **Q3**: I am confused about L_ss and L_co. For the representations \u2026\n\nThe dimension for $A_{I}$ and $A_{II}$ is $R^{b \\times d}$ similar to those of $\\hat{A}_t \\in R^{b\\times d}$ and $\\hat{A}_s \\in R^{b\\times d}$. This is because $A_I = \\hat{A}_t \\odot \\hat{A}_s$, here $\\odot$ is the point operation performing element wise multiplication. During loss computation, we then perform summation along the columns which yields the dot product.\n \n> **Q4.A**: There is a lack of ablation study for L_ss, Lco. Is the Space Similarity sensitive to the batch size?\n\nThank you for this suggestion. Below we report the performance of CoSS on different input batch sizes analogous to Table 7 in appendix A.2. As we can note, the performance of CoSS remains performant at small batch sizes well. For larger batches, there is a slight drop in performance. \n\n\n| **Method** | $b=32$ | $b=64$ | $b=128$   | $b=256$  | $b=512$ | $b=1024$ |\n|--------|-------------|----------------|-----------|-----------|-----------|-----------|\n| Resnet8x4/32x4   |   73.21    |  73.62        |  73.90     | 73.35     | 72.57 |  72.09  |\n| Resnet20/56   |   70.34    |  71.32        |  71.11     | 71.05     | 70.59 | 70.11"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699788697880,
                "cdate": 1699788697880,
                "tmdate": 1699788920953,
                "mdate": 1699788920953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uP4IAS0Icz",
                "forum": "QHVTxso1Is",
                "replyto": "vxdyvQLPpa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (2/2)"
                    },
                    "comment": {
                        "value": "> **Q4.B**: How does the performance of the proposed method compare to SMD?\n\nThe experiments are underway and we will be adding results in the next few days. Please refer to W1.1 for further updates\n\n> **Q5**: Can authors compare the proposed method with the method mentioned in reference [1]?\n\nThank you for the suggestion. We have listed below the performance of [1] when adapted for an unsupervised distillation setting on CIFAR-100. We compute the losses of [1] over the embeddings features. To highlight the differences to our method, [1] applies the normalisation to the features **before** computing inter and intra category similarities. This pre-normalisation has the same effect of L2 normalisation which causes a loss in original manifold information. \n\n| **Method** | **Resnet20/56** | **Resnet8x4/32x4** | **VGG8/13**   | **WRN16/40**  |\n|--------|-------------|----------------|-----------|-----------|\n| DIST[1] | 67.13       |  67.67        | 73.40     | 72.85     |\n| Ours   | **71.11**   | **73.90**      | **74.58** | **74.65**     |\n\n\n> **Q6**: Can the authors provide additional details about ViT training in Tables 1 and 6? Were all ViTs trained from scratch?\n\nYes, all (student) models (including ViTs) are trained from scratch. Teacher models are obtained off-the-shelf (more details in SEED). For ImageNet, ViT-tiny is the smallest network in the family. We use the patch_size=12, embedding dimension=192, depth=12, num_heads=3. This architecture definition is also supported by the popular library Timm [3]. For CIFAR-100, we scaled down the teacher and student networks to account for the relatively smaller dataset and image size of CIFAR-100. For CIFAR-100, the ViTs have patch size of 8, embedding size=384, num_heads = 12 for teacher and 3 for student and depth=7 for teacher 3 for student. They are trained following the hyper-params as used for other ImageNet and CIFAR training respectively. We have added this information to the appendix as well. Also, we will be sharing the official repository upon acceptance to reproduce **all** of our results including pre-trained models.\n\nReferences:\n\n[1] Huang, T., You, S., Wang, F., Qian, C., & Xu, C. (2022). Knowledge distillation from a stronger teacher. Advances in Neural Information Processing Systems, 35, 33716-33727.\n\n[2] Liu, H., & Ye, M. (2022, October). Improving Self-supervised Lightweight Model Learning via Hard-Aware Metric Distillation. In European Conference on Computer Vision (pp. 295-311). Cham: Springer Nature Switzerland.\n\n[3]: Ross Whitman (2019). Pytorch Image Models. https://github.com/huggingface/pytorch-image-models"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699788908416,
                "cdate": 1699788908416,
                "tmdate": 1699788908416,
                "mdate": 1699788908416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C5BGbBsqJ6",
                "forum": "QHVTxso1Is",
                "replyto": "vxdyvQLPpa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparisons with SMD (ImageNet experiment) (3/2)"
                    },
                    "comment": {
                        "value": "In addition to the CIFAR-100 evaluation (provided in the previous response), we share below results for the task of unsupervised distillation from the moco-v2 teacher (from our and other baseline's setting) to a ResNet-18 utilising the official implementation of SMD. We used SMD's official source code with official hyper-parameters for this experiment.\n\n|      |   Top-1   |   KNN-10  |\n|------|:---------:|:---------:|\n| SMD  | **59.56** | 49.69     |\n| Ours | 59.24     | **55.04** |\n\nThe results indicate an expected trend as we have seen in the case of other baselines. We are competitive in terms of top-1 with SMD but perform substantially better on the KNN metric. \n\n**Transfer Learning** \n\nHere, we report the transfer learning performance of SMD compared to ours. To recall, we learn the classification hyper-parameters for each model on each dataset independently. We observe that the CoSS student outperforms SMD by a large margin.\n\n|      |  CIFAR-10 | CIFAR-100 | STL-10    | Caltech-101 | Pets      | Flowers   | DTD       |\n|------|:---------:|:---------:|-----------|-------------|-----------|-----------|-----------|\n| SMD  | 86.47     | 64.42     | 94.24     | 80.59       | 74.59     | 78.97     | 69.31     |\n| Ours | **89.84** | **70.03** | **95.31** | **87.06**   | **80.31** | **87.04** | **71.54** |\n\n**OOD Robustness**\n\nFor brevity, we report consolidated results of the OOD experiments. For a detailed breakdown, we request the reviewer to view the (soon to be uploaded) new version of the paper. \n\n|      | ImageNet-v2 | ImageNet-S | ImageNet-C |\n|------|:-----------:|:----------:|------------|\n| SMD  | 44.38       | 10.27      | 33.89      |\n| Ours | **48.67**   | **12.85**  | **39.81**  |\n\n**Image Segmentation**\n\n|      | CamVid (Acc$_p$) | CamVid (IoU$_m$) | Cityscapes (Acc$_p$) | Cityscapes (IoU$_m$) |\n|------|:----------------:|:----------------:|----------------------|----------------------|\n| SMD  | 75.67            | 0.1620           | 82.38                | 0.2774               |\n| Ours | **88.00**        | **0.2855**       | **84.39**            | **0.3115**           |\n\nFor the SMD distillation on the remaining architectures, we have started the trainings but unfortunately they will not be finishing before the rebuttal deadline. We will be adding them to our work upon completion. \n\n**Lastly, we also would like to sincerely thank the reviewer for their suggestions which has allowed us to strengthen our manuscript.**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389412666,
                "cdate": 1700389412666,
                "tmdate": 1700389412666,
                "mdate": 1700389412666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j83M7JQULP",
                "forum": "QHVTxso1Is",
                "replyto": "vxdyvQLPpa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_sCnR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_sCnR"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer sCnR"
                    },
                    "comment": {
                        "value": "Thanks to the author for their engagement in the ablation study.\nThere is still a lack of the ablation study on $L_{ss}$ and $L_{co}$. Can authors conduct the experiment without $L_{co}$?\n\nFor the comparison with DIST, Did you use Pearson\u2019s distance for DIST?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487097702,
                "cdate": 1700487097702,
                "tmdate": 1700490197643,
                "mdate": 1700490197643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iSaNXU3Pkd",
            "forum": "QHVTxso1Is",
            "replyto": "QHVTxso1Is",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes the CoSS for efficient unsupervised knowledge distillation. Previous works rely on a large feature queue to compute the teacher knowledge, which consumes large memory and computation. CoSS can perform unsuperivsed knowledge distillation on a mini-batch. Specifically, they extract the embedding from the penultimate layer of the network to form a embedding matrix. Then, CoSS minimizes the feature similarity and space similarity between teacher and student. Experiments on various downstream tasks and backbones showcases its performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method uses a smaller embedding queue for unsupervised KD.\n2. The designed loss, feature similarity and space similarity, is easy to follow.\n3. The authors conduct extensive experiments to validate their method."
                },
                "weaknesses": {
                    "value": "1. The proposed feature similarity and space similarity is neither well-explained nor intuitive. The authors discuss the reason why they apply normalization on the embedding matrix and use the cosine similiaity instead of the L2 distance in Section 4. The author *treat the embedded manifold as a topological manifold*, and then introduce an argument based on Homeomorphism. Such a conclusion **assumes** that the unsupervised learning methods learn a low dimensional manifold and the manifold is locally euclidean. However, there is a lack of references or theoretical analysis to support their point.\n2. The designed loss is analogous to contrastive learning, which computes the cosine similarity between two normalized feature. However,\nthe discussion does not explain why they only consider the positive samples while neglect the negative samples. I think this may be the key difference from other methods."
                },
                "questions": {
                    "value": "1. Although the method is designed for unsupervised KD, it seems that the method can be used for supervised KD based on their argument.\n2. It is unclear why the introduced losses discard the negative samples.\n3. In the comparison to CRD, I guess the authors implement the negative contrastive learning. What is the performance of CRD when negative samples are removed?\n4. Also, for other comparison method, what is the performance if the negative samples are removed?\n5. What is the performance of the proposed model if adding negative samples?\n6. The paper claims efficiency as their advantage. Is it possible to improve the performance using a larger batch size?\n7. Please add more discussion regarding Section 4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9077/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2",
                        "ICLR.cc/2024/Conference/Submission9077/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672647431,
            "cdate": 1698672647431,
            "tmdate": 1700652020522,
            "mdate": 1700652020522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "issPilboyD",
                "forum": "QHVTxso1Is",
                "replyto": "iSaNXU3Pkd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for recognising the effort we put into conducting extensive experiments. Their acknowledgment of our work is truly appreciated. We value the opportunity to engage in a meaningful discussion about our submission, and the reviewer\u2019s comments contribute significantly to that dialogue.** \n\n**We will be uploading the new version of our work once we address the concerns and suggestions of all reviewers. We appreciate the reviewer\u2019s patience in this regard.**\n\n> **W1.A**: The proposed feature similarity and space similarity is neither well-explained nor intuitive. The authors discuss the reason why they apply normalisation on the embedding matrix and use the cosine similarity instead of the L2 distance in Section 4 \n\nIn section 4, we highlight that normalising the features (L2) alone erases the information present in the pre-normalization embedding manifold. A simple argument for this we presented is that all points lying on a ray through the origin will coincide on the hypersphere. With these collapsed representations we cannot model the teacher's original (unnormalized) manifold which was our original goal. For this purpose, we introduce space similarity. For two points now coinciding on the d-hypersphere will have different contributions to the space similarity loss. The student will learn how data is distributed along different dimensions (up to some scalar). We provide a toy example to help visualise this aspect. In the figure 3 of the paper, we showed that the baseline SEED, though it learns well separated embeddings, is not able to capture the teacher\u2019s embedding local shape as effectively as SEED. \n\nLastly, we hope to clarify that for (unit) normalised features, L2 distance and cosine similarity capture similar information. We do not prefer one over the other in our formulation. This can be demonstrated by the following derivation. The Euclidean distance between two vectors $\\mathbf{u}$ and $\\mathbf{v}$ is given by:\n$$D_{\\text{Euclidean}}(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2 $$\n\nFor normalized vectors, we have:\n$$ \\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{(\\mathbf{u} - \\mathbf{v})^T (\\mathbf{u} - \\mathbf{v})} $$\n$$ = \\sqrt{\\|\\mathbf{u}\\|_2^2 - 2 \\mathbf{u}^T \\mathbf{v} + \\|\\mathbf{v}\\|_2^2} $$\n$$= \\sqrt{2 - 2 \\cos(\\theta)} $$\n\nHere, $\\theta$ is the angle between the two vectors which is also captured by cosine similarity. Hence, we can either aim to minimise the distance or maximise the cosine similarity.\n\n> **W1.B**: The author treat the embedded manifold as a topological manifold, and then introduce an argument based on Homeomorphism. Such a conclusion assumes that the unsupervised learning methods learn a low dimensional manifold and the manifold is locally euclidean. However, there is a lack of references or theoretical analysis to support their point.\n\nThank you for highlighting this issue. Many manifold learning techniques compute local neighbourhood distance with the assumption that the space is locally euclidean[1,2,3]. Explicit statements about the assumption of a locally Euclidean manifold in the context of unsupervised learning can be relatively rare, as this assumption is often fundamental to the underlying methods without being explicitly articulated in the papers. For example, many unsupervised learning methods employ manifold learning based data visualisations which implies that the learnt manifold is locally euclidean [4,5]. We have now added this discussion to Section 4 to strengthen our assumption.\n\n> **W2**: The designed loss is analogous to contrastive learning, which computes the cosine similarity between two normalised features. However, the discussion does not explain why they only consider the positive samples while neglecting the negative samples. I think this may be the key difference from other methods.\n\nThank you for providing valuable insights. As noted by the reviewer, our methodology markedly deviates from baselines employing a contrastive learning framework for distillation. While we appreciate the acknowledgment of this distinction, we respectfully disagree with the assertion that our alignment objectives closely resemble conventional contrastive learning techniques. Rather, our approach shares greater affinity with methods which in the reviewer's categorization, exclusively relies on only positive samples [6,7]. An advantage of focusing on positive samples lies in the observation that comparisons with negative samples (as done by the baselines) lead to increase in training time and GPU compute. In summary, our objective focusing only on positive samples is not only more effective, but also more efficient, particularly in scenarios where resources may be constrained."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700135948373,
                "cdate": 1700135948373,
                "tmdate": 1700135948373,
                "mdate": 1700135948373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BLN7gkEbrD",
                "forum": "QHVTxso1Is",
                "replyto": "iSaNXU3Pkd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (2/2)"
                    },
                    "comment": {
                        "value": "> **Q1**: Although the method is designed for unsupervised KD, it seems that the method can be used for supervised KD based on their argument.\n\nYes, our method can be augmented with any existing supervised learning method. We mentioned this possibility in the discussion section.\n\n> **Q2**: It is unclear why the introduced losses discard the negative samples.\n\nThe response is provided in **W2** above. \n\n> **Q3**: In the comparison to CRD, I guess the authors implement the negative contrastive learning. What is the performance of CRD when negative samples are removed?\n\nYes, we utilised the complete objective of CRD which incorporates positives and negatives in the loss computation. Without the contribution of the negatives, CRD loss failed to optimise in our recent experiments achieving classification score of 1\\% equivalent to that of random classification. \n\n> **Q4**: Also, for other comparison methods, what is the performance if the negative samples are removed?\n\nGiven that the other methods are also contrastive learning based and removing negatives had a drastic impact on the CRD training. We are currently running a training with SEED\u2019s objective with negative comparisons removed and will provide an update shortly.\n\n> **Q5**: What is the performance of the proposed model if adding negative samples?\n\nAs alluded to earlier in our responses, our solution follows an alternative route to that of contrastive learning. We cannot add negatives (samples to increase dissimilarity from)  in the space similarity as this would change the entire paradigm under which we operate. Our intention is not to increase dissimilarity between positive-negative dimensions which is the core idea of contrastive learning. \n\n> **Q6**: The paper claims efficiency as their advantage. Is it possible to improve the performance using a larger batch size?\n\nThe ablation over the batch size does not indicate that the performance can be increased by increasing the batch size directly. Please note that we did not perform any hyperparameter optimisation for this study. \n\n| **Method** | $b=32$ | $b=64$ | $b=128$   | $b=256$  | $b=512$ | $b=1024$ |\n|--------|-------------|----------------|-----------|-----------|-----------|-----------|\n| Resnet8x4/32x4   |   73.21    |  73.62        |  73.90     | 73.35     | 72.57 |  72.09  |\n| Resnet20/56   |   70.34    |  71.32        |  71.11     | 71.05     | 70.59 | 70.11\n\n> **Q7**: Please add more discussion regarding Section 4.\n\nThank you for the suggestion. Based on the discussions highlighted in the review, we have added more discussions in section 4 to support our assumptions and intuition. We have also included details about the setup used for the visualisations in Figure 3.\n\n**References**\n\n[1] van der Maaten, L. & Hinton, G. (2008). \u201cVisualizing Data using t-SNE\u201d . Journal of Machine Learning Research, 9, 2579--2605. \n\n[2] Cayton, Lawrence. \u201cAlgorithms for manifold learning\u201d. eScholarship, University of California, 2008.\n\n[3] Hinton, Geoffrey E., and Sam Roweis. \"Stochastic neighbor embedding.\" Advances in neural information processing systems 15 (2002).\n\n[4] Oord, A. V. D., Li, Y., & Vinyals, O. (2018). \u201cRepresentation learning with contrastive predictive coding\u201d. arXiv preprint arXiv:1807.03748.\n\n[5] Zhuang, Weiming, et al. \"Collaborative unsupervised visual representation learning from decentralized data.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[6] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, et al.. Bootstrap Your Own Latent: A new approach to self-supervised learning. Neural Information Processing Systems, 2020.\n\n[7] Xinlei Chen and Kaiming He. Exploring Simple Siamese Representation Learning. CVPR (2021)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136101541,
                "cdate": 1700136101541,
                "tmdate": 1700136101541,
                "mdate": 1700136101541,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qCOpiEKn3h",
                "forum": "QHVTxso1Is",
                "replyto": "iSaNXU3Pkd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks four your reply.\n\n- The clarification about positive samples is important to understand the difference from other constrative learning methods. Here is a reference [1] may be helpful to you.  \n\n- Regarding Sec. 4, it would be helpful if you add these discussions.\n\nI have a concern that in Sec. 5.7 (page 8), the authors claim that *''The authors\nhighlighted that CRD can be used for unsupervised scenario, however, we find its performance to be\nlacking compared to CoSS''*. However, I note that the authors try to prove this argument in a limited scenarior as described in **Methodology**, which is not rigorous.\n\nRegards,\n\n\n[1] Understanding self-supervised learning dynamics without contrastive pairs. Yuandong Tian \u00b7 Xinlei Chen \u00b7 Surya Ganguli. ICML 2021"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534099847,
                "cdate": 1700534099847,
                "tmdate": 1700534138821,
                "mdate": 1700534138821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7fwfh31NIa",
                "forum": "QHVTxso1Is",
                "replyto": "qKQCc7Nlax",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_VMp2"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your response.\n\nAfter reading the rebuttal, I think it is safe to improve the rating.\n\nThe authors adress most of my concerns and correspondingly provide a revised manuscript.\n\nAlthough Reviewer sCnR thinks the formulation of the proposed loss is similiar with other work, which is also my concern at another point, I think the novelty is that CoSS proposes to preserve the learned manifold of the teacher network.\n\nRegards,"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652345426,
                "cdate": 1700652345426,
                "tmdate": 1700652345426,
                "mdate": 1700652345426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hOKegXaI9u",
            "forum": "QHVTxso1Is",
            "replyto": "QHVTxso1Is",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_ahHP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_ahHP"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about knowledge distillation without ground-truth labels. A method named CoSS is proposed. A loss based on space similarity loss is introduced alongside with normalized cosine similarity. Specially, each dimension of the student feature space is required be similar to the corresponding dimension of the teacher. Experiments are done to compare with other methods on computer vision tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The CoSS method is simple and does not require importance sampling.\n2. The paper is in general well to follow."
                },
                "weaknesses": {
                    "value": "Please check the questions part.\n1. Comparison over baseline is lacked.  \n2. Performance on CNN and ViT needs more reasonable analysis.\n3. Sensitivity of hyperparameter.\n4. More details on the computational efficiency are required."
                },
                "questions": {
                    "value": "1. The space similarity is like the traditional cosine similarity loss. But the comparison over the cosine similarity is lacked. It is difficult to evaluate the effects of so-called space similarity.\n2. CoSS performs worse on CNN->CNN distillation (Table 1). Could the authors provide more analysis on why the method works well on ViT, but not so good on CNNs?\n3. The hyperparameter of lambda is somewhat sensitive to different architectures and datasets. How to choose the appropriate lambda needs further discussion. If hyperparameter search is required, additional training cost is required.\n4. The authors claims that the CoSS is faster and more efficient. Yet the comparative details on computational efficiency is not provided. The analysis on how the method is of high efficiency is also lacked."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9077/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9077/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9077/Reviewer_ahHP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769208884,
            "cdate": 1698769208884,
            "tmdate": 1699637143148,
            "mdate": 1699637143148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3kmYlG1TEi",
                "forum": "QHVTxso1Is",
                "replyto": "hOKegXaI9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/1)"
                    },
                    "comment": {
                        "value": "**We\u2019d like to thank the reviewer for dedicating their valuable time to review our work. In particular, we are pleased to find that the reviewer appreciated the simplicity and presentation of our work. They have raised interesting points, which we aim to address below.**\n\n**We will be uploading the new version of our paper once we address the concerns and suggestions of all reviewers. We appreciate the reviewer\u2019s patience in this regard.**\n\n> **Q1 & W1**: The space similarity is like the traditional cosine similarity loss. But the comparison over the cosine similarity is lacked. It is difficult to evaluate the effects of so-called space similarity.\n\nThank you for the suggestion. We would like to take this opportunity to highlight that we provide an ablation study over $\\lambda$ in section A2.1. $\\lambda=0$ represents the case of only cosine similarity of features. New experiments on ImageNet ResNet-18 & Eff-b0 with ($\\lambda=0$) reveal that we gain roughly $0.9\\%$ and $0.8\\%$ top-1 by employing space similarity over the baseline. We have now added these results to the appendix.\n\n> **Q2 & W2**: CoSS performs worse on CNN->CNN distillation (Table 1). Could the authors provide more analysis on why the method works well on ViT, but not so good on CNNs?\n\nThanks for raising this interesting perspective. Underperforming due to differences in architecture is a widely observed phenomenon in the context of supervised distillation [1]. A reason for the drop is owed to different inductive biases present in the different teacher-student architectures. Contrative objectives encourage the model to learn discriminative features. W.r.t distillation, contrastive objectives encourage the student to learn discriminative features as learnt by their teachers. Given different network capacities and inductive biases, we suspect that these are stronger conditions than our alignment based objective. \n\n> **Q3 & W3**: The hyperparameter of lambda is somewhat sensitive to different architectures and datasets. How to choose the appropriate lambda needs further discussion. If hyperparameter search is required, additional training cost is required.\n\nThank you for the insightful suggestion. Indeed, ideal value for $\\lambda$ varies across architectures. We suspect that this is due to differing inductive biases of these architectures. In order to choose the best $\\lambda$ for each student-teacher pair, unfortunately, one has to evaluate multiple configurations. However, based on our experiments, $\\lambda=0.5$ provides strong results which also extends readily to ImageNet like datasets and for different architectures (Resnet, EfficientNets, ViTs). In addition, we hope to highlight that the additional cost of hyperparameter search is pertinent to all methods. But, given the speed advantage that our method possesses, this search will be relatively faster than baselines\u2019.\n\n> **Q4 & W4**: The authors claims that the CoSS is faster and more efficient. Yet the comparative details on computational efficiency is not provided. The analysis on how the method is of high efficiency is also lacked.\n\nThank you for the feedback. We have now also added the table reporting the absolute values for GPU utilisation and training time per batch to the appendix. They reflect the values presented in Fig 1 of our paper. Moreover, we have also added relative speed and GPU utilisation (compared with CoSS) to Table 1 as well.\n\n**EDIT:** We will also be reporting total training resources, which is (how much GPU memory is consumed by a method) * (how long the method takes to complete the training). Alongside we will also report the trend of memory and time consumption over the course of training in our paper.\n\n **References**:\n\n[1] Tian, Yonglong, Dilip Krishnan, and Phillip Isola. \"Contrastive Representation Distillation.\" In International Conference on Learning Representations. 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140182772,
                "cdate": 1700140182772,
                "tmdate": 1700391289376,
                "mdate": 1700391289376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uHT6Yuz2Uq",
            "forum": "QHVTxso1Is",
            "replyto": "QHVTxso1Is",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_jpJi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9077/Reviewer_jpJi"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles self-supervised distillation, specifically targeting the challenge of existing methods requiring extensive sample queues. To address this, the authors have innovatively introduced a loss based on dimension-specific spatial similarity. The novel CoSS supervision framework is composed of conventional sample-based similarities in conjunction with space similarities, thereby effectively emulating the semantic and structural attributes of the data manifold as captured by teacher models. Many experimental validations demonstrate that CoSS not only achieves performance on par with existing methods but also with enhanced efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The math formulation of the proposed space similarity is elegantly concise, suggesting an inherent capability to capture and learn the manifold's structure effectively.\n\n2. The diversity of experimental settings presented allows readers to gain a thorough understanding of the proposed CoSS loss's capabilities and performance.\n\n3. The organization of this paper benefits from a clear and logical progression that facilitates comprehension of the material presented."
                },
                "weaknesses": {
                    "value": "1. The Methods section could be enhanced by incorporating a simplified example or illustrative figure to show the concept of space similarity.\n\n2. The discussion on topological spaces in Section 4 is commendable. However, It should be noted that L2 normalization in previous approaches is designed to conform the semantic manifold to a hyperspherical space, thereby constraining the metric within the bounds of the cosine similarity. That means the manifold of teacher feature space is already a cosine space and the similarity is determined by the inner product between two hyper-sphere spaces. In light of this, the paper would benefit from a rigorous comparison demonstrating the superiority, if any, of a d-dimensional Euclidean manifold over a d-dimensional hypersphere for the learning tasks at hand."
                },
                "questions": {
                    "value": "1. This paper could be strengthened by investigating the connection between batch normalization and the proposed CoSS. Can the authors provide insights or very simple results about how BN might influence or interact with CoSS?\n\n2. In my view, to some degree, the joint constraint in CoSS appears to share conceptual relations with the optimal transport (Sinkhorn function). Could the authors elaborate on any theoretical underpinnings or empirical evidence that supports this connection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9077/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698872648014,
            "cdate": 1698872648014,
            "tmdate": 1699637143043,
            "mdate": 1699637143043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Om1YcOzkOk",
                "forum": "QHVTxso1Is",
                "replyto": "uHT6Yuz2Uq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "**We extend our sincere gratitude to the reviewer for investing their valuable time and expertise in reviewing our work. We're thrilled to hear their positive assessment of our work, particularly the elegant formulation of the proposed space similarity and the diverse experimental settings presented. We value the reviewer\u2019s feedback and have carefully considered each suggestion to enhance the quality of our manuscript.** \n\n**We will be uploading the new version of our paper once we address the concerns and suggestions of all reviewers. We appreciate the reviewer\u2019s patience in this regard.**\n\n> **W1:** The Methods section could be enhanced by incorporating a simplified example or illustrative figure to show the concept of space similarity.\n\nThank you for the suggestion. Currently, to enhance clarity on the concept of feature and space similarity, we've included an illustrative figure in Figure 2. We've now incorporated a reference to this figure in our method section to more effectively convey our ideas.\n\n> **W2:** The discussion on topological spaces in Section 4 is commendable. However, It should be noted that L2 normalization in previous approaches is designed to conform the semantic manifold to a hyperspherical space, thereby constraining the metric within the bounds of the cosine similarity. That means the manifold of the teacher feature space is already a cosine space and the similarity is determined by the inner product between two hyper-sphere spaces. In light of this, the paper would benefit from a rigorous comparison demonstrating the superiority, if any, of a d-dimensional Euclidean manifold over a d-dimensional hypersphere for the learning tasks at hand.\n\nThank you for pointing this out. As highlighted by the reviewer, similarity matching using only the feature happens in the d-hypersphere with projected embeddings from the student and teachers. We address the issue of information loss from the teacher's unnormalized manifold to a d-hypersphere. This matching on the d-hypersphere does not account for the structure of the original unnormalized manifolds of teacher and student. Through our empirical evaluations presented in the paper, we aimed to highlight the benefit of capturing the original teacher\u2019s manifold information. CoSS is able to yield competitive results, moreover, lagging in top-1 does not hinder its performance on robustness and various transfer learning tasks.\n\n> **Q1:** This paper could be strengthened by investigating the connection between batch normalization and the proposed CoSS. Can the authors provide insights or very simple results about how BN might influence or interact with CoSS?\n\nWe thank the reviewer for providing an interesting perspective. Indeed, we can draw a connection between Batch Normalisation (BN) [1] and our final objective.\n\nTo recall, during training, BN operates on a batch of input data, $X \\in R^{b\\times d}$, where batch size is $b$ and $d$ is the feature dimension. It first performs standardisation along each spatial (channel) dimension. $\\hat{X_{:,I}} = \\frac{X_{i} - \\mu_i}{ \\sigma_i}$ \n\nwhere, $\\mu_i$, $\\sigma_i$ are the mean and variances respectively for the $i^{th}$ spatial dimension. The normalized values are then scaled by trainable parameters $\\gamma$ and $\\beta$ as: $Z_{:,i} = \\gamma_i \\hat{X}_{:,i} + \\beta_i$\n\nHere, $\\gamma \\in R^d$ and $\\beta \\in R^d$ operate on each spatial dimension. These can be interpreted as affine transformations working on each dimension. We can potentially utilise it to map the standardised student embeddings to the teacher's un-normalized embedding space. The corresponding loss can be defined as follows: $\\mathcal{L}= \\frac{1}{b}\\sum_{i=0}^{i<b}\\mathcal{D}(Z^s_i, X^t_i) $\n\nwhere, $X^t_i$ is the teacher's embedding for the $i^{th}$ sample and $Z^s_i$ corresponds to the BatchNormalized student's embeddings for the same sample. Below we share empirical findings with the BatchNorm formulation on CIFAR-100.\n \n| **Method** | **VGG8/13** | **Resnet8x/32x** | **WRN16/40**  |\n|--------|-------------|----------------|-----------|\n| BN         |  74.01      | 72.22            |  73.42         |\n| Ours       | 74.58       | 73.90            | 74.65         |\n\n> **Q2:** In my view, to some degree, the joint constraint in CoSS appears to share conceptual relations with the optimal transport (Sinkhorn function). Could the authors elaborate on any theoretical underpinnings or empirical evidence that supports this connection?\n\nWe thank the reviewer for the valuable suggestion. Unfortunately, due to limited familiarity with the topic, we are unable to provide any meaningful insights on the question raised by the reviewer. However, we are willing to explore this direction in the future.\n\n**References:**\n\n[1] Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358020253,
                "cdate": 1700358020253,
                "tmdate": 1700379809445,
                "mdate": 1700379809445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fGwrYTZXNP",
                "forum": "QHVTxso1Is",
                "replyto": "uHT6Yuz2Uq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for dedicating your time to reviewing our paper and offering valuable insights and suggestions. As today marks the final day of discussion, we're curious if you've had an opportunity to review our responses to your queries. We're more than willing to provide further explanations or address any additional questions you may have. Your feedback is greatly appreciated."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684540443,
                "cdate": 1700684540443,
                "tmdate": 1700684540443,
                "mdate": 1700684540443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DFuI6ealBR",
                "forum": "QHVTxso1Is",
                "replyto": "fGwrYTZXNP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_jpJi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9077/Reviewer_jpJi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your explanations of my concerns. I don't have any further questions at present. I will consider my final review accordingly."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9077/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687643848,
                "cdate": 1700687643848,
                "tmdate": 1700687643848,
                "mdate": 1700687643848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]