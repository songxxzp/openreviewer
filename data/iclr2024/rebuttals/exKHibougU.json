[
    {
        "title": "LLM-grounded Video Diffusion Models"
    },
    {
        "review": {
            "id": "1BDSDpzPWa",
            "forum": "exKHibougU",
            "replyto": "exKHibougU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a new text-to-video generation pipeline called LLM-grounded Video Diffusion (LVD). In particular, it first uses LLM to generate the layouts of the video and then uses the generated layout to guide a pre-trained video diffusion model. The whole process does not update the weights of both the LLM and video diffusion model. Besides, the authors show that LLMs\u2019 can generate spatiotemporal dynamics aligned with text prompts in a few-shot setting. Qualitative results and quantitative results show that LVD generates higher quality videos that also align more with text."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-organized and easy to follow. The figures and tables are informative.\n\n- The finding that LLMs can generate good spatiotemporal dynamics with only three examples is interesting and well supported by the experiments. The exploration of physical properties contained in LLM is also inspiring and deserves further research. \n \n- The results generated by LVD are promising compared to the baseline, ModelScope."
                },
                "weaknesses": {
                    "value": "- The idea of using LLM to generate layout is already explored in LayoutGPT (Feng et al., 2023) and LMD (Lian et al., 2023). LMD also adapts in a training-free manner. It is beneficial for the authors to include a more detailed comparison.\n\n- The technical contribution is limited. The first layout generation part is similar to LMD, and the second part is a straightforward application of other training-free layout-to-image methods in the video domain."
                },
                "questions": {
                    "value": "- From Table 3, we can see that LVD improves the video quality. What causes the improvement?\n\n- Is LLM able to generate reliable layouts using text without direction information, such as \u201ca walking dog\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808278835,
            "cdate": 1698808278835,
            "tmdate": 1699636163279,
            "mdate": 1699636163279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uukvDbQ9ix",
                "forum": "exKHibougU",
                "replyto": "1BDSDpzPWa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your encouraging review! Here we address your comments individually:\n\n> The idea of using LLM to generate layout is already explored in LayoutGPT (Feng et al., 2023) and LMD (Lian et al., 2023).\n\n> The first layout generation part is similar to LMD, and the second part is a straightforward application of other training-free layout-to-image methods in the video domain.\n\nThanks for the comment. However, we would like to point out a few differences in terms of findings and the proposed methods between our work and LMD.\n\nAs for the LLM-grounded dynamic scene layout generation stage, **there are two noteworthy distinctions between our work and previous works, as explained in detail in the General Rebuttal \\[Q2\\]**.\n1. Most of text-to-image-layout generation tasks introduced in previous works (LayoutGPT and LMD) can be completed without in-depth understanding of the properties of the objects in the prompt. In contrast, even a simple generation of dynamic scene layouts of \"a falling object\" requires understanding several world properties, object properties, and camera properties, which is **highly non-trivial for an LLM and not tested by previous works**.\n2. More importantly, previous works either only evaluate properties that are **already seen in in-context examples** (e.g., the 4 main tasks that LMD showcases are already demonstrated in the in-context examples in Table 10 of the LMD paper), or **retrieve related layouts from a layout database** as in-context examples (LayoutGPT). **Neither work demonstrates whether LLMs have the knowledge for generalization without relevant in-context examples.** In contrast, in Section 3 (Discoveries), we show that LLMs can generalize to physical properties or object properties that are **unseen in the in-context examples** (e.g., inferring the property of a paper airplane, while similar concepts are never mentioned in the in-context examples). This implies LLMs have an intrinsic understanding of the physical world instead of just emulating from in-context examples.\n\nAs for the dynamic scene layout to video generation stage, **as explained in the General Rebuttal \\[Q1\\],** we propose a temporal energy term that provides guidance on the positions and velocities of center-of-mass of each object, which significantly improves our generation accuracy, especially on the temporal tasks. **Since previous layout-to-image methods only focus on generating one image (i.e., do not take temporal consistency into accounts), we believe our introduction of temporal center-of-mass energy term sets our method further apart from previous image layout conditioning methods that you mentioned.** We have updated our paper PDF to include the descriptions to this energy term.\n\n**We hope the reviewer can agree with us on these significant distinctions that are either not applicable to previous LLM-grounded text-to-image works or are not demonstrated by them.** We will update the manuscript accordingly to express these distinctions more clearly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383593652,
                "cdate": 1700383593652,
                "tmdate": 1700383753936,
                "mdate": 1700383753936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gDnCpMz15N",
                "forum": "exKHibougU",
                "replyto": "1BDSDpzPWa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> From Table 3, we can see that LVD improves the video quality. What causes the improvement?\n\nOur method indeed improves on the FVD scores on both UCF-101 and MSR-VTT datasets over the base video diffusion models **without additional training**, as shown in Table 3.\n\nFVD measures the alignment between the generated videos and reference videos, which in turn reflects both video-text alignment and video quality. Two factors together contributed to our improvements in the FVD score:\n\n1. Our method, with an LLM processing input text prompts to generate dynamic scene layouts, has improved ability to understand the input text prompt. This is reflected in the improved video-text alignment.\n2. Although improving video quality is not our main goal, our method also improves the quality of generated videos by using the intermediate dynamic scene layouts for planning, which in turn leads to better attribute binding and spatial layouts that are more reasonable.\n\n**For example,** in the first row of Fig. 1 in our paper, baseline ModelScope confuses the semantics of the bear and the pikachu mentioned in the prompt and generates an object that looks neither like a bear or like a pikachu. By giving spatial-temporal guidance to the video diffusion model, our method can successfully generate a bear and a pikachu with high visual quality, as required by the prompt. The ability for high-quality video generation in cases like this leads to our improved FVD score.\n\n> Is LLM able to generate reliable layouts using text without direction information, such as \u201ca walking dog\u201d.\n\nThat is a great point! **As your requested, we ask the LLM to generate a walking dog without specifying the direction information, as shown in Figure 8(c\\) in the appendix of the updated paper PDF.** We observe that the LLM is not only able to generate dynamic scene layouts for a walking dog but also **provide assumptions that it makes about the walking directions in the reasoning statement**. This shows that our LLM is indeed able to make reasonable guesses for ambiguous user prompts.\n\n```\nReasoning: A dog walking would have a smooth, continuous motion across the frame. **Assuming it is walking from left to right**, its x-coordinate should gradually increase. The dog's y-coordinate should remain relatively consistent, with minor fluctuations to represent the natural motion of walking. The size of the bounding box should remain constant since the dog's size doesn't change.\n```\n\nIf the assumption does not meet the user's expectation (e.g., the user wants a dog walking from the right instead), the user can reply to the LLM, and the LLM can incorporate additional information to satisfy the user's needs.\n\nGiven the successful generation of the dynamic scene layouts, our layout-conditioned video generation method also successfully generates a video of a walking dog. We have added this example in the appendix of our updated manuscript. **Please see Sec A.1 and Fig. 8 (a) in our updated PDF for the video generation.** Thanks again for the suggestion!\n\nIn light of this, we would like to kindly ask if there is anything to explain to make you consider raising your score. Additionally, we are open to addressing any additional concerns or answering any questions you may have!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383604031,
                "cdate": 1700383604031,
                "tmdate": 1700383604031,
                "mdate": 1700383604031,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eXpzPQ9A4S",
                "forum": "exKHibougU",
                "replyto": "1BDSDpzPWa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder: one day left for the author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear ZTWf,\n\nWe appreciate your positive feedback on our work! We\u2019re reaching out to see if you've had the opportunity to review our responses to your concerns about the novelty and contribution of our work, improvements in video quality, and clarification of ambiguous prompts. **We've conducted additional experiments and generated new videos based on your suggestions.**\n\nCould you kindly let us know if our responses have adequately addressed your comments? If there is any additional information or experiments that you believe would be beneficial, please don't hesitate to let us know. We're more than willing to provide further details during the author-reviewer discussion.\n\nThank you so much!\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611950267,
                "cdate": 1700611950267,
                "tmdate": 1700611950267,
                "mdate": 1700611950267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EtC93mU7Km",
                "forum": "exKHibougU",
                "replyto": "gDnCpMz15N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "After reading the authors' response, most of my concerns are solved, including the difference from the previous methods, results of ambiguous input, and technical novelty. I will maintain my rating at 6."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651677633,
                "cdate": 1700651677633,
                "tmdate": 1700651677633,
                "mdate": 1700651677633,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xbIDQ3ilfA",
            "forum": "exKHibougU",
            "replyto": "exKHibougU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
            ],
            "content": {
                "summary": {
                    "value": "Grounded Text-to-image generation has been studied by several papers recently. However, text-to-video geneartion with layout control is still unexplored. This paper tackles this task by proposing a training-free method by adding layout information through adjusting the attention maps of the diffusion UNet. Speficically, this paper first utilizes LLMs (GPT-4) to generate a multi-frame object layouts, then designs a layout-grounded video generator that encourages the cross-attention map to concentrate more on the bounding box areas. Extensive experiments for spatiotemporal dynamics evaluation have demonstrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written and easy to follow.\n\n- The proposed method is training-free, which avoid the need for costly training with image/video data.\n\n- Using LLM-generated layouts for videos is relatively unexplored. And it's natural to use the knowledge embedded in LLMs to general layouts for downstream video generation."
                },
                "weaknesses": {
                    "value": "- Even though the proposed method is training free, it takes longer time during inference to generate videos due to the optimization steps needed for the energy function.\n\n- Training-free layout control already exists in previous literatures [1, 2]. Therefore, the design of the energy function and backward guidance is not that novel. \n\n- Ablation study of the model design is not given (e.g., number of DSL guidance steps, energy function design).\n\n\n[1] Chen, Minghao, Iro Laina, and Andrea Vedaldi. \"Training-free layout control with cross-attention guidance.\" arXiv preprint arXiv:2304.03373 (2023)\n\n[2] Lian, Long, et al. \"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.\" arXiv e-prints (2023): arXiv-2305"
                },
                "questions": {
                    "value": "- Could the authors provide some reasoning why they report video-text similarity metric in Bain et al., 2021? It would be nice to also report CLIPScore, since its widely reported in other text-to-video generation baseilnes.\n\n- The examples provided in the paper are with non-overlapping bounding boxes. Will the proposed method work well with overlapping layouts?\n\n- If there are multiple objects, is the final energy function summing over the energy function corresponding to each object?\n\n- It seems that the background areas of the generated images with proposed method are quite static (Fig1, 7, 8, 9). Is this because the model encourages static background, or becuase the visualized images happens to have relatively static background? \n\n- Based on my understanding, another concurrent work, VideoDirectorGPT [1], is also for text-to-video generation with layout guidance. Even though the technical routes are different from this paper, it would be nice to have some discussions and comparison in the related work section. \n\n[1] Lin, Han, et al. \"Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning.\" arXiv preprint arXiv:2309.15091 (2023)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823445926,
            "cdate": 1698823445926,
            "tmdate": 1699636163209,
            "mdate": 1699636163209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BXDOXz7SpY",
                "forum": "exKHibougU",
                "replyto": "xbIDQ3ilfA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your encouraging review and suggestions! Here we address each point of your comments:\n\n> Even though the proposed method is training free, it takes longer time during inference to generate videos due to the optimization steps needed for the energy function.\n\nThanks for the great point! All previous methods based on backward guidance for training-free guidance (e.g., LMD, BoxDiff, Layout Guidance, Self-Guidance, and our method) take more inference time. The reason is that such methods need to find a way to leverage the knowledge inside the pretrained model during inference, without supervision from any external box-video pairs. \n\nHowever, training-free methods not only are free of training costs (time and compute requirements) but also have **no requirements on external data or human annotations (e.g., box annotations on frames)**. For example, our method can be applied off-the-shelf on video diffusion models such as ModelScope without external annotations. Moreover, training-free allows exploring a wide range of formulations without training them each time.\n\nFurthermore, even though inference time is usually not the main focus of this line of work (e.g., LMD, LayoutGPT, BoxDiff, etc.), **if low inference time is needed for the production environment, our method can be distilled into a conditioned generation model** so that the inference-time optimization of energy function is no longer required. Specifically, the generation of our method can potentially be distilled into a model with additional input tokens (e.g., ReCo) or attention adapters (e.g., GLIGEN), thus achieving fast generation without external datasets or additional human annotations. **This way, we get the advantages of being both free of external annotated data and having fast inference, enjoying the benefits of existing training-free and training-based methods.** We leave the distillation to future work as this is a general enhancement applicable to many training-free guidance methods.\n\n> Training-free layout control already exists in previous literatures [1, 2]. Therefore, the design of the energy function and backward guidance is not that novel.\n\nThe energy function proposed in our pre-rebuttal manuscript, which applies spatial control per frame, is indeed a little similar to the energy function used in previous works. However, **as explained in the General Rebuttal \\[Q1\\],** we proposed a temporal energy term that provides guidance on the positions and velocities of center-of-mass of each object, which significantly improves our generation accuracy, especially on the temporal tasks. Since previous layout-to-image methods only focus on generating one image (i.e., do not take temporal consistency into accounts), **we believe our introduction of temporal center-of-mass energy term sets our method further apart from previous image layout conditioning methods mentioned by the reviewer.** We will update the manuscript accordingly. Thanks for the good point!\n\n> Ablation study of the model design is not given (e.g., number of DSL guidance steps, energy function design).\n\nThank you for the suggestion. We have conducted ablation studies on the CoM energy function design as well as the number of DSL guidance steps in LVD. **Please refer to the General Rebuttal [Question 4] for more details.**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383518974,
                "cdate": 1700383518974,
                "tmdate": 1700383518974,
                "mdate": 1700383518974,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GvigAym0U3",
                "forum": "exKHibougU",
                "replyto": "xbIDQ3ilfA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Could the authors provide some reasoning why they report video-text similarity metric in Bain et al., 2021? It would be nice to also report CLIPScore, since its widely reported in other text-to-video generation baseilnes.\n\nGood question! CLIPSIM, the video-text alignment score obtained from CLIP, is computed per-frame and then averaged across the feames ([reference](https://arxiv.org/abs/2104.14806)). This leads to its temporal ambiguity. **For example,** it could not distinguish a barrel floating from the left to the right vs from the right to the left. Instead, we use Frozen-In-Time, a CLIP-like model that takes in the whole video sequence and outputs a video-text alignment, which does not treat a video as a bag of frames and can potentially model object motion in the alignment score.\n\n**Per your request, we also evaluate our model and report the CLIPSIM metric compared to the baseline**, following a similar setting as Tab. 4 (Tab. 5 in the updated PDF) except we change the alignment model to a per-frame CLIP.\n\n|   | ModelScope | LVD on ModelScope |\n|:---:|:---:|:---:|\n| CLIPSIM | 0.2947 | **0.3001** |\n\nOur method gives better CLIPSIM compared to baseline ModelScope, indicating better text-to-video alignment.\n\nHowever, we would like to mention that previous benchmarks, such as FVD or similarity-based metrics (e.g., metrics based on CLIP or Bain et al., 2021), do not perform detailed checks for correctness in aspects such as spatial dynamics and generative numeracy. While these methods mainly focus on the semantic alignment and generation quality, **our proposed benchmark in Tab. 2 pioneers in quantitatively assessing the correctness in prompt understanding in video generation, opening up a direction to explore future video generation research.**\n\n\n> The examples provided in the paper are with non-overlapping bounding boxes. Will the proposed method work well with overlapping layouts?\n\n\nYes. **We have added an example with overlapping layouts in Figure 8(b) in the paper appendix (Section A.1 in our updated PDF). Generally we find that the LVD also works with overlapping boxes.** Our energy function, with its `topk` operator, simply selects the most appropriate place to encourage object appearance, and two layout boxes can select different places due to the different topk selection. Therefore, two or more overlapping boxes will not conflict with each other.\n\n> If there are multiple objects, is the final energy function summing over the energy function corresponding to each object?\n\nYou are right! The final energy function is the sum over the one for each object. We will make it clear in the manuscript.\n\n> It seems that the background areas of the generated images with proposed method are quite static (Fig1, 7, 8, 9). Is this because the model encourages static background, or becuase the visualized images happens to have relatively static background?\n\n\nGood point! Empirically we found that our base model, ModelScope, often tends to generate videos with small background variations, potentially because it uses a lot of static images in training. **To ensure that our model retains the ability to generate non-static background, we let it generate `a bear taking a shower in a lake, lightning strikes` as a simple example, which we show in Figure 8(c\\) in the paper appendix (Sec A.1 in the updated PDF).** Although the background motion is not extreme, the model indeed generates rain dropping on the water and the lightning, showing that our method does not impede the ability to generate non-static background.\n\n> Based on my understanding, another concurrent work, VideoDirectorGPT [1], is also for text-to-video generation with layout guidance. Even though the technical routes are different from this paper, it would be nice to have some discussions and comparison in the related work section.\n\nThanks for the suggestion! **We have included a reference to [1] and updated a comparison in the related work section in the paper.** Since this work was released on arXiv only a few days before the submission deadline, **[1] is a concurrent work that should not be used to devalue our work**.\n\nOur work has a different focus compared to [1]:\n1. Our work is training-free and focuses on inference-time algorithms, whereas [1] trains adapters with human annotated images.\n2. Our work focuses more on in-depth analysis on the capabilities of LLMs to generalize to unseen properties and to use knowledge in the weights rather than mimick in-context examples. \n\n**In light of the completion of several requested experiments and comparisons, we would like to kindly ask if there are any other concerns or additional questions that we can respond to or if you are willing to consider increasing your score!**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383539596,
                "cdate": 1700383539596,
                "tmdate": 1700383539596,
                "mdate": 1700383539596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "54eRcp0HHH",
                "forum": "exKHibougU",
                "replyto": "xbIDQ3ilfA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
                ],
                "content": {
                    "comment": {
                        "value": "Really appreciate for your detailed explanations to address my concern about CLIPSIM and overlaping bounding boxes!\n\nBut I still have concern about the reasoning of static background you mentioned above. ModelScope is also trained on webvid dataset, and a lot of videos in webvid have obvious camera motions. So I'm not very convinced by the statement \"ModelScope often tends to generate videos with small background variations\". \n\nIn the additional example you generated (Fig 8(c)), it seems that the background is still very static, even though there are lightings. But the lightings can be regarded as \"foreground\" objects rather than the background. Therefore, it would be nice if the authors can provide some examples with obvious background motion.\n\nAnd another minor question is about the resolution of videos generated from ModelScopeT2V. As described in their technical report, their model is trained and evaluated on resolution of 256\\*256. In LVD, the videos are of resolution 512\\*512. Do the authors observe some negative visual quality change after increasing the resolution?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502997152,
                "cdate": 1700502997152,
                "tmdate": 1700503534210,
                "mdate": 1700503534210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5kWVWOWSSA",
                "forum": "exKHibougU",
                "replyto": "xbIDQ3ilfA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer 34cZ,\n\nWe appreciate your positive and insightful feedback on our work! We would like to check again to see if you've had the opportunity to review our responses to your additional questions. We've conducted additional experiments to generate videos with background motion that arises from camera motion according to your request.\n\n**In light of our updates to our work addressing your concerns, completion of many requested experiments, and discussions about related work mentioned in the review, we would like to ask if you are willing to reconsider your score, and also if there are any additional questions that we can respond to during the last day of the author-reviewer discussion period!**\n\nThank you so much!\n\nBest Regards,\n\nAuthors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679044133,
                "cdate": 1700679044133,
                "tmdate": 1700679044133,
                "mdate": 1700679044133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jK48B3Rm4A",
                "forum": "exKHibougU",
                "replyto": "5kWVWOWSSA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThe additional video examples you provided addressed my concern about static background, so thanks again for your detailed explanation and additional efforts!\n\nA last question just out of curiosity: I noticed that the foreground objects in these new video examples are quite static, which means that the layouts (bounding boxes) generated by LLMs probably do not have large moments. Will this make LVD easier to generate videos with better temporal coherence? And have the authors observed a decrease in temporal coherence across frames in videos when there are significant layout changes?"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719794736,
                "cdate": 1700719794736,
                "tmdate": 1700719794736,
                "mdate": 1700719794736,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DkaqDWnnxV",
            "forum": "exKHibougU",
            "replyto": "exKHibougU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to text-conditioned video generation that seeks to address the limitations of current models, which struggle with complex spatiotemporal prompts and often produce videos with restricted or incorrect motion patterns. The key contribution is the LLM-grounded Video Diffusion (LVD) model that separates the video generation task into two steps: (1) using a Large Language Model (LLM) to generate dynamic scene layouts (DSLs) from text inputs, and (2) using these layouts to guide a diffusion model in generating the video. The approach is described as training-free and can be integrated with existing video diffusion models that allow for classifier guidance. Moreover, they introduce a benchmark for evaluating the alignment between input prompts and generated videos."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposal of a training-free approach presents a pipeline that is well-suited for the application of off-the-shelf LLMs and diffusion models. Its simplicity yet effectiveness stands out as a notable strength.\n- The discovery that LLMs can generate spatiotemporal layouts from text with only a limited number of in-context examples is noteworthy. It highlights the potential for a straightforward integration of LLM reasoning into text-to-video tasks."
                },
                "weaknesses": {
                    "value": "- The idea of guidance via energy functions and cross-attention maps seems to be basically derived from BoxDiff (Xie et al., 2023;) and Chen et al. 2023a;. It is unclear how much of this work is based on previous research and how much is new. Since they are dealing with video generation using layouts, it would have been nice to see the authors' contribution in extending to the temporal axis, but this is not evident, which is disappointing.\n- I am concerned that the scale of the sample size for the proposed DSL benchmark may be too small to conduct a sufficiently robust evaluation.\n- The paper's contribution appears to lack novelty. There is existing work in text-to-image generation that has already established the capability of LLMs to create layouts, and this research seems to merely extend that to assess whether the same capability applies to temporal understanding. I didn't perceive any novel ideas stemming from the temporal aspect of the problem that would distinguish this work significantly from its predecessors.\n- The paper seems to lack a detailed analysis or ablation studies concerning the prompts given to the LLM for generating Dynamic Scene Layouts (DSLs). Such investigations are crucial to understand how different prompts affect the LLM's output and the subsequent quality of the video generation. Further exploration in this area could significantly bolster the robustness of the presented approach.\n- The paper's current framework could indeed benefit from additional ablation studies or analytical experiments to demonstrate the effectiveness of using DSLs for training-free guidance of text-to-video diffusion models. Moreover, a theoretical explanation of why this particular approach is effective would be valuable. It's important for the research to not only present the method but also to thoroughly validate and explain why certain choices were made and how they contribute to the overall performance and reliability of the model."
                },
                "questions": {
                    "value": "- Can the authors elaborate on how the model performs with ambiguous or complex text prompts that might yield multiple valid interpretations in terms of spatial and temporal dynamics?\n- Could the authors discuss any observed limitations or failure modes of the LVD approach, particularly in cases where the LLM might generate less accurate DSLs?\n- (Minor point) Typographical error in Section 4, second paragraph. The sentence in question should indeed conclude with \"feature map\" instead of \"feature ma.\" A revision is recommended for accuracy and clarity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA",
                        "ICLR.cc/2024/Conference/Submission2307/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698874944551,
            "cdate": 1698874944551,
            "tmdate": 1700548666916,
            "mdate": 1700548666916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A4zHw4RMyA",
                "forum": "exKHibougU",
                "replyto": "DkaqDWnnxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed review and highly-valuable suggestions! **Accordingly, we improved our approach with a temporal energy function, added additional studies on a benchmark of a much larger scale, conducted key ablation studies to analyze our method in response to your comments.** We address each of your comments in detail:\n\n> Since they are dealing with video generation using layouts, it would have been nice to see the authors' contribution in extending to the temporal axis, but this is not evident, which is disappointing.\n\nThanks for the suggestion! Indeed, although our method already greatly outperformed the baseline with the current energy function, innovation on temporal axis should further boost the temporal consistency of the generated objects. Therefore, **as introduced in the General Rebuttal \\[Q1\\]**, we added a temporal center-of-mass term, which significantly improves our generation accuracy by 10%, especially on the temporal dynamics and sequential movements, while reducing the mismatches between the generated objects and the dynamic scene layout input. **Our introduction of center-of-mass energy term sets our method further apart from previous image layout conditioning methods that do not deal with the temporal axis. We hope this novel energy term meets your expectation for novelty related to the temporal axis! Thanks for your suggestion again!**\n\n> I am concerned that the scale of the sample size for the proposed DSL benchmark may be too small to conduct a sufficiently robust evaluation.\n\nThis point is totally valid! Taking your suggestions, we propose an enlarged version of our benchmark that is 10x larger compared to the previous version. Specifically, each task now has 5x more prompts in a similar structure as the previous version, with 500 prompts in total. For each prompt, we generate two videos from dynamic scene layout, with **1000 videos per benchmark run in total**.\n\nWe run both the baseline and our method twice with the enlarged benchmark and **we observe evaluation results with small overall variation across the runs**.\n\n|   | Numeracy | Attribution | Visibility | Dynamics | Sequential | Average | **Variation** | Previous smaller benchmark |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|---|---|\n| ModelScope Baseline (run 1/run 2) | 7.5%/5% | 65.5%/66% | 1.5%/1% | 18.5%/23.5% | 0%/0% | 18.6%/19.1% | **0.5%** | 16% |\n| Ours (run 1/run2) | 59.5%/55.5% | 94%/88.5% | 40%/50.5% | 65.5%/67.5% | 34%/32% | 58.6%/58.8% | **0.2%** | 58% |\n\n**Furthermore, our enlarged benchmark still results in very similar results compared to the previous smaller benchmark, which also validates the findings obtained with the previous benchmark. We hope the robustness in our benchmark results can address your concerns!**\n\nFinally, we would like to note that while previous benchmarks (e.g., CLIPSim and FVD) focus on generation quality and semantics, **our benchmark is a pioneer for correctness in spatial-temporal alignment in the literature,** which is of great significance for future research, especially given that the results justify the robustness of our benchmark.\n\nFor the final version of the paper, we plan to run all our ablation studies on this enlarged benchmark and include the results in our work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383426111,
                "cdate": 1700383426111,
                "tmdate": 1700383426111,
                "mdate": 1700383426111,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rT728ywMUi",
                "forum": "exKHibougU",
                "replyto": "DkaqDWnnxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The paper's contribution appears to lack novelty. There is existing work in text-to-image generation that has already established the capability of LLMs to create layouts, and this research seems to merely extend that to assess whether the same capability applies to temporal understanding. I didn't perceive any novel ideas stemming from the temporal aspect of the problem that would distinguish this work significantly from its predecessors.\n\nThanks for the comment. There may be some misconceptions on the capabilities justified by previous works that use LLMs for image layout generation. **There are two noteworthy distinctions between our work and previous works, which we briefly explain here, and we provide more detailed clarifications in the General Rebuttal \\[Q2\\]**.\n1. Most of text-to-image-layout generation tasks introduced in previous works (LayoutGPT and LMD) can be completed without understanding the properties of the objects in the prompt in depth. In contrast, even a simple generation of dynamic scene layouts of \"a falling ball\" requires understanding of complex properties of the physical world, such as physical property (e.g., gravity which decides the trajectory of the ball), object interaction (e.g., ball hitting the ground), object properties (e.g., elasticity of ball which decides how much the ball will bounce), and camera properties (e.g., perspective geometry which decides if the size of the ball will change based on its distance to the camera), which are all **highly non-trivial for an LLM and have not been assessed, demonstrated, or evaluated by previous works**. **In contrast, we show that LLMs demonstrate the understanding of these properties, as illustrated in Figure 4 in the paper.**\n2. Previous works either only evaluate properties that are **already seen in in-context examples** (e.g., the 4 main tasks that LMD showcases are already demonstrated in the in-context examples in Table 6 of [the LMD paper](https://arxiv.org/pdf/2305.13655.pdf)), or **retrieve related layouts from a layout database** as in-context examples (LayoutGPT). **Neither work demonstrates whether LLMs have the knowledge for generalization without relevant in-context examples.** In contrast, in Section 3 (Discoveries part), we show that LLMs can generalize to physical properties or object properties that are **unseen in the in-context examples** (e.g., as shown in Figure 4 (c\\) in the paper, when generating layouts of throwing a paper airplane, it can figure out the airplane will slide in the air due to its weight and aerodynamics, while similar concepts are never mentioned in the in-context examples). This implies LLMs have an intrinsic understanding of the physical world instead of just emulating from in-context examples.\n\nWe will clarify these two points further in our manuscript. Thanks again for the valuable feedback!\n\n> The paper seems to lack a detailed analysis or ablation studies concerning the prompts given to the LLM for generating Dynamic Scene Layouts (DSLs). Such investigations are crucial to understand how different prompts affect the LLM's output and the subsequent quality of the video generation. Further exploration in this area could significantly bolster the robustness of the presented approach.\n\nThank you for the suggestion! We have added ablation studies on how the number, content, and format of prompted in-context examples will affect the generation quality. **Please refer to the General Rebuttal \\[Q3\\],** in which we show that 3 in-context examples already result in high-quality dynamic scene layout generation and LLM is robust to different in-context examples in both content and format. \n\n> The paper's current framework could indeed benefit from additional ablation studies or analytical experiments to demonstrate the effectiveness of using DSLs for training-free guidance of text-to-video diffusion models.\n\nThank you for the comment. To analyze different design choices of the DSL-to-video stage in LVD, we conduct ablation studies on the CoM energy function design as well as the number of guidance steps in the DSL-grounded video generation pipieline. **Please refer to the General Rebuttal \\[Q4\\] for more details.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383458961,
                "cdate": 1700383458961,
                "tmdate": 1700383458961,
                "mdate": 1700383458961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KMsIZSYlu5",
                "forum": "exKHibougU",
                "replyto": "DkaqDWnnxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Can the authors elaborate on how the model performs with ambiguous or complex text prompts that might yield multiple valid interpretations in terms of spatial and temporal dynamics?\n\nThat is a good point! The potential ambiguity in the user-provided prompts is indeed worth exploring. We find that LLMs can oftentimes perform reasonable guesses if the spatial-temporal information is missing in the user-given prompt.\n\nFor example, with a simple prompt as `a walking dog`, the LLM gives the following reasoning statement, with the assumption clearly written in it:\n```\nReasoning: A dog walking would have a smooth, continuous motion across the frame. **Assuming it is walking from left to right**, its x-coordinate should gradually increase. The dog's y-coordinate should remain relatively consistent, with minor fluctuations to represent the natural motion of walking. The size of the bounding box should remain constant since the dog's size doesn't change.\n```\n\nThis allows the subsequent layout generation to be consistent with respect to the assumption of walking from the left to the right. **We show the generated video of this example in Figure 8(a) in appendix of the updated paper PDF.**\n\n\nAdditionally, if the assumption does not meet the user's expectation (e.g., the user wants a dog walking from the right instead), the user can reply to the LLM, and the LLM will incorporate additional information to satisfy the user's needs.\n\n> Could the authors discuss any observed limitations or failure modes of the LVD approach, particularly in cases where the LLM might generate less accurate DSLs?\n\nThank you for the suggestion. We show two failure cases in the DSL-to-video stage in Figure 10 and Figure 11 of the updated paper PDF. For the text-to-DSL stage, although the LLM-generated DSLs have a high accuracy in following the prompt (98% accuracy in Table 1 in the paper), it still has some failure cases. The failures mainly come from generating complex dynamics such as sequential movements. One failure case, which we added in the updated paper PDF, is shown as below and also demonstrated in Figure 9 in Section A.6 of the paper appendix:\n\n*prompt*: A realistic lively video of a top-down viewed scene in which a ball initially on the lower left of the scene. It first moves to the upper left of the scene and then moves to the upper right of the scene, outdoor background.\n\n*generated boxes* (visualized in Figure 9 of the updated paper PDF):\n```\nFrame 0: (0.097, 0.878, 0.195, 0.976) \nFrame 1: (0.097, 0.683, 0.195, 0.781) \nFrame 2: (0.097, 0.488, 0.195, 0.585)\nFrame 3: (0.292, 0.488, 0.390, 0.585) \nFrame 4: (0.488, 0.488, 0.585, 0.585) \nFrame 5: (0.683, 0.488, 0.781, 0.585)\n```\n\nNote that for each frame, the box is represented by 4 float numbers, where the first two are the x and y coordinates of the upper left corner of the box, and the last two are the x and y coordinates of the lower right corner of the box.\n\nIn this case, although the prompt indicates the ball moves to the upper left corner of the scene, the LLM does not generate the box in the upper left corner, but instead a box slightly above the middle of the whole scene. The failure is probably because the sequential movements contain multiple stages and LLM has a higher probability to oversee or misunderstand one of the stages. We will add these discussions on the failure cases to the paper.\n\n\n> (Minor point) Thank you for pointing out this typographical error in Section 4, second paragraph. The sentence in question should indeed conclude with \"feature map\" instead of \"feature ma.\" A revision is recommended for accuracy and clarity.\n\nThanks for the good point! We have fixed this in our manuscript.\n\n**In light of the additional improvements to our methods and benchmarks we've provided, as well as the completion of numerous requested experiments, we would like to kindly ask if you might be open to increasing your assessment score, and if there are any new concerns or further questions that we can address for you!**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383476309,
                "cdate": 1700383476309,
                "tmdate": 1700383476309,
                "mdate": 1700383476309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sU2VHlERJH",
                "forum": "exKHibougU",
                "replyto": "ftsH4QzZej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough responses to the concerns I raised. All issues have been resolved. The strengthened benchmark contributions through our discussions are particularly noteworthy and hold potential impact for the field. Consequently, I am adjusting my score towards an accept."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548677252,
                "cdate": 1700548677252,
                "tmdate": 1700548677252,
                "mdate": 1700548677252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]