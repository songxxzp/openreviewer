[
    {
        "title": "Bayesian Coreset Optimization for Personalized Federated Learning"
    },
    {
        "review": {
            "id": "OBgnw6LrcV",
            "forum": "uz7d2N2zul",
            "replyto": "uz7d2N2zul",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9034/Reviewer_FxK1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9034/Reviewer_FxK1"
            ],
            "content": {
                "summary": {
                    "value": "The work incorporated granular-level bayesian coresets optimization in Federated Learning. The proposed approach gave minimax convergence rate and showed good performance in empirical studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of incorporating coreset optimization in FL is new and well-motivated.\n2. Solid theoretical results are given.\n3. Some optimistic empirical studies are presented."
                },
                "weaknesses": {
                    "value": "1. The major weakness is the lack of convergence comparison in the empirical part. One of the major concerns in FL is the communication cost. Thus the number of iteration rounds is crucial in FL. The reviewer suggests not only including the comparison of the final accuracy under (maybe different levels, not only 50%) of sample complexity, but also including the convergence speed, i.e., the communication cost comparison.\n\n2. How expensive it is to calculate the coreset samples/weights? Is there any empirical runtime results?\n\n3. How is \\hat{\\pi} defined in Eq. (3) and (4)?\n\n4. Some typo: first sentence in section 3.2 is incomplete. Different places for \\hat notation in q^i(\\theta, w), on q, or q^i, or q^i(theta, w)."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9034/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9034/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9034/Reviewer_FxK1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9034/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755482373,
            "cdate": 1698755482373,
            "tmdate": 1700451699024,
            "mdate": 1700451699024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b2QPlGnK04",
                "forum": "uz7d2N2zul",
                "replyto": "OBgnw6LrcV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 1/2 to the Reviewer FxK1"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's time and effort in reviewing our paper on employing Bayesian coresets in federated learning. Their insightful feedback is invaluable, and we are thankful for the constructive comments provided.\n\nWe greatly appreciate the reviewer's positive evaluation of our work, particularly their recognition of the well-motivated approach in incorporating coreset optimization into federated learning. The reviewer's acknowledgment of the our proposed solid theoretical results and the optimism reflected in our empirical studies is truly motivating.\n\n**Weakness**\n\n    The major weakness is the lack of convergence comparison in the empirical part. One of the major concerns in FL is the communication cost. Thus the number of iteration rounds is crucial in FL. The reviewer suggests not only including the comparison of the final accuracy under (maybe different levels, not only 50%) of sample complexity, but also including the convergence speed, i.e., the communication cost comparison.\n\n\n We appreciate the reviewer's attention to the critical aspect of convergence comparison and its relevance to the communication cost in Federated Learning (FL). While we may have already provided in our theoretical results the convergence rate achieved in the case of our method which is in order of logarithmic bounds of the coreset size $k$ and thereby much faster than full data training,  we also get better convergence as compared to random sampling (as indicated in Fig 3). In addition to these, we update on our empirical results to include different subset sample sizes along with their communication cost (total no. of communication rounds) as follows.\n\n\nIn Appendix Section 10.2 we are including new results to show case the total number of communication rounds (where under each setting where we are considering different coreset subsample sizes k=50,30,15,10) which is indicative of the convergence of our proposed method. Empirical results shows that with significant lesser coreset subsample sizes we achieve faster convergence with less number of communication rounds , but however with lesser accuracy than that if we used a larger coreset size or full data training which is as expected. \n\n\n\n**Table: Comparative results of test accuracies across different coreset sample complexity**\n\n| Method (Percentage = sampling fraction) | MNIST                       |                             | FashionMNIST                    |                             | CIFAR                       |                             |\n|-----------------------------------------|-------------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|\n|                                         | Test Accuracy                | Communication Rounds       | Test Accuracy                    | Communication Rounds       | Test Accuracy                | Communication Rounds       |\n|-----------------------------------------|-------------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|\n| $pFedBayes$ (Full)                      | 98.79                         | 194                         | 93.01                             | 215                         | 83.46                       | 266                         |\n| $pFedBayes$ with random sampling (50%)                      | 80.2                          | 135                         | 87.12                             | 172                         | 48.31                       | 183                         |\n| $Our Method$ (k = 50%)                  | 92.48                         | 98                          | 89.55                             | 93                          | 69.66                       | 112                         |\n| $Our Method$ (k = 30%)                  | 90.17                         | 84                          | 88.16                             | 72                          | 59.12                       | 70                          |\n| $Our Method$ (k = 15%)                  | 88.75                         | 62                          | 85.15                             | 38                          | 55.66                       | 32                          |\n| $Our Method$ (k = 10%)                  | 85.43                         | 32                          | 82.64                             | 24                          | 48.25                       | 16                          |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277280969,
                "cdate": 1700277280969,
                "tmdate": 1700277468749,
                "mdate": 1700277468749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g11TDXWS4O",
                "forum": "uz7d2N2zul",
                "replyto": "OBgnw6LrcV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 2/2 to the Reviewer FxK1"
                    },
                    "comment": {
                        "value": "How expensive it is to calculate the coreset samples/weights? Is there any empirical runtime results?\n\nOur approach to computing coreset samples/weights is exceptionally cost-effective, thanks to the utilization of accelerated iterative hard thresholding in the coreset\u2014an incredibly efficient method for rapidly determining coreset weights. Although we haven't conducted explicit runtime experiments for coreset sample weights, we've assessed the overhead cost in terms of computation, revealing an average of only 0.001 seconds per round with a maximum of 10 iterations for coreset computation (which is ideal due to early convergance in coreset weights)  per round. Over 200 rounds, this results in an additional overhead cost of just 0.2 seconds compared to random sampling. \n\nWe look forward to suggestions regarding details to any additional specific runtime experiments that the reviewer would like us to try out.\n\n     How is $\\hat{\\pi}$ defined in Eq. (3) and (4)?\n\nWe have defined $\\hat{\\pi}$ as the weighting distribution that has the same support under an embedding Hilbert space as the true posterior $\\pi$, for which we consider an L2 norm as the distance metric between the client's overall data likelihood and its corresponding coreset weighted data likelihood. \n\n\n     Some typo: first sentence in section 3.2 is incomplete. Different places for \\hat notation in $q^i(\\theta, w)$, on $q$, or $q^i$, or $q^i(theta, w)$.\n\nThank you for your careful inspection. We have identified this and similar places if any and have fixed the same in our revised version. Additionally regarding the notation for hat notation, $q^i(\\theta, w)$ we have chosen to keep it consistent to $\\hat{q^i}$. We have identified the sections where there is this notation inconsistency like in Section 4 and Section 6 of the main paper and have fixed them in our final revision. There are currently some portions in the supplementary which we are converting to a consistent notation for the hat operator and which will updated in our upcoming rebuttal revision.\n\n\n---------------------------------------------------------------\n\nWe hope that the rebuttal clarifies the questions raised by the reviewer. We would be happy to discuss any further questions about the work, and would appreciate an appropriate increase in the score if the reviewer\u2019s concerns are adequately addressed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278382715,
                "cdate": 1700278382715,
                "tmdate": 1700278436175,
                "mdate": 1700278436175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UDuxaAgTeK",
                "forum": "uz7d2N2zul",
                "replyto": "g11TDXWS4O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_FxK1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_FxK1"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "The reviewer has read the feedback given by the authors. The reviewer believe the added experimental results has addressed most of my concerns, thus raising the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451682850,
                "cdate": 1700451682850,
                "tmdate": 1700451682850,
                "mdate": 1700451682850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hrVRMkW0ql",
                "forum": "uz7d2N2zul",
                "replyto": "OBgnw6LrcV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FxK1"
                    },
                    "comment": {
                        "value": "We are deeply grateful to the reviewer for the positive support for our paper and the increment in score. We would appreciate the reviewer's support for acceptance of the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550928428,
                "cdate": 1700550928428,
                "tmdate": 1700550988070,
                "mdate": 1700550988070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mgCGLlJuXp",
            "forum": "uz7d2N2zul",
            "replyto": "uz7d2N2zul",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an optimization framework for personalized federated learning by incorporating Bayesian coresets into the model proposed in [1]. The author want to ensure that the accuracy performance does not deteriorate when applying coresets. To achieve this, they have made modifications to the common coreset objective. Furthermore, they provide proof of the convergence rate of generalization error using their approach and evaluate the effectiveness of their method on a range of datasets.\n\n[1] Xu Zhang, Yinchuan Li, Wenpeng Li, Kaiyang Guo, and Yunfeng Shao. Personalized federated learning via variational bayesian inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The integration of Bayesian coresets with federated learning is innovative.\n- In the context of personalized federated learning, this work presents new ideas and considerations for defining the objective in coreset computation, which differs from the commonly used coreset definition."
                },
                "weaknesses": {
                    "value": "- The paper's content is a bit bloated, and the use of notations can be messy. For instance, sections 3.2 and 4 could be condensed to make them more concise. Additionally, there is potential to simplify the formulaic aspect.\n- It would be beneficial if the author could emphasize their novel contribution, distinguishing it from the techniques previously proposed by others. Currently, these ideas seem to be mixed within the intricate details of the interpretations.\n- The overall architecture, as well as certain smaller techniques and theoretical analysis methods, seem to be largely derived from previous work.\n- The contribution on the coreset construction is limited. Although the authors introduce a new coreset objective, they do not provide sufficient NEW optimization techniques for the new objective. I could only identify some techniques borrowed from previous work.\n- In my opinion, the primary contribution of this paper is the modified objective (eq. 9) tailored to personalized federated learning. However, the advantages of this modified objective are not adequately elucidated in the current presentation.\n\nsome minor problems\n\n- In section 3, there is a confusion of n and N. For example, n in Fig 1 should be N. \n- In section 3.2 , it should be $ g_j = \\mathcal{P}_\\theta(\\mathcal{D}_j^i) = E_{\\theta\\sim \\hat{\\pi}} P_\\theta(\\mathcal{D}_j^i) $.\n- The subscript of the bold variable should not be bolded if it is a scalar.\n- many other typos, e.g. missing equation references and confusing sentence like \u201cFor the first term in Equation 1, the authors we use a minibatch stochastic gradient descent \u2026\u201d"
                },
                "questions": {
                    "value": "- What is the benefits to apply coreset in the personalized federated learning? I think one of the most important is that it can reduce the communication complexity. It would be valuable to investigate and quantify the extent to which the coreset approach reduces communication complexity in the specific optimization task addressed in this work. This can be done theoretically, by providing a complexity formula, and practically, by presenting numerical results from experiments that show the reduction in communication complexity achieved.\n- the intuition behind the new objective in eq. 9 is not very persuasive. If you could compute a coreset with a sufficiently small loss as defined in eq. 3, it is unecessary to add the term representing the \u201cdistance\u201d between $\\hat{q}^i(\\theta, w)$ and $\\hat{q}^i(\\theta)$ since $\\hat{q}^i(\\theta, w)$ and $\\hat{q}^i(\\theta)$ will lead to closed losses; On the other hand, if you couldn\u2019t make it under the constraint $\\| w \\|_0 \\leq k$, which means there is no such small coreset with ideal error, the coreset method could not work well. It would be beneficial to clarify the merits of the new objective, such as its robustness or any other advantages it offers. Experiments that demonstrate the effectiveness of the new objective would greatly strengthen the argument.\n- Does the modifications of eq. 6 consist of the following two parts: i) use the weighted likelihood. ii) replace prior distribution with global distritution. I am not sure for that.\n- is there any strategy for choosing the value of k in practice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9034/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83",
                        "ICLR.cc/2024/Conference/Submission9034/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9034/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764660296,
            "cdate": 1698764660296,
            "tmdate": 1700673152253,
            "mdate": 1700673152253,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5fpQLyR04v",
                "forum": "uz7d2N2zul",
                "replyto": "mgCGLlJuXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 1/4 to the Reviewer QB83"
                    },
                    "comment": {
                        "value": "We extend our gratitude to the reviewer for dedicating time and effort to assess our paper on the utilization of Bayesian coresets in federated learning. \n\nWe appreciate the reviewer's recognition of the innovation in integrating Bayesian coresets framework within the federated learning setting in our paper. The reviewer's positive assessment of this aspect is encouraging, and we are pleased that they find our approach to be innovative.\nWe are equally appreciative of the reviewer's acknowledgement for our novel research proposal and new ideas in this space which we believe will be significantly beneficial to the community.\n\nNow, we go forward to clarify some of the questions raised by the reviewer.\n\n**Weaknesses**\n\n    The paper's content is a bit bloated, and the use of notations can be messy. For instance, sections 3.2 and 4 could be condensed to make them more concise. Additionally, there is potential to simplify the formulaic aspect.\n\nWe thank the reviewer for their valuable feedback and insights on the paper's content and notations. In response, we have streamlined and condensed sections 3.2 and 4 to enhance conciseness without compromising the clarity of our presentation, which has been reflected in our updated rebuttal version. We look forward to the reviewer's suggestions in understanding which other specific portions can be further simplified to improve upon our current draft.\n\n    It would be beneficial if the author could emphasize their novel contribution, distinguishing it from the techniques previously proposed by others. Currently, these ideas seem to be mixed within the intricate details of the interpretations.\n\nOur **primary objective** in this proposal is to **enhance the efficiency in Federated Learning training via utilizing a fraction of the original data at each client** while maintaining **near-optimal accuracy**, along with **minimally possible communication cost**. Our proposed theoretical findings showcase that **model trained on a client's coreset sample size weighted data show faster convergence rate** than in the case when trained fully on the entire training data. \n\nBoth Bayesian Coreset Optimization and Federated Learning Optimization problems are **two different orthogonal and independent problems**. Considering their both distinct challenges, we aim to infuse the two optimization framework in one single unified cohesive framework such that the coreset optimization problem can **strategically pick data points for each client** in a **model dependent manner**, i.e. without facing too much penalty in the model training under Federated Learning setup. As of now, to the best of our knowledge, this specific problem **remains largely unexplored**. Remarkably, there is a significant gap in the existing research landscape concerning subset selection or coreset-based strategies within the context of Federated Learning, with a conspicuous absence of theoretical frameworks in this domain.\n\nCrucially, **our work is unique in addressing the absence of research on subset selection or coreset-based strategies within Federated Learning setup** and theoretical contributions along with empirical results in terms of near optimal performance under data scarcity setting accompany our contributions.\n\nLastly we also **showcase its applicability in medical domain scenario** via medical datasets where **data scarcity remains a huge problem** and how our proposal can alleviate the same.\n\n     The overall architecture, as well as certain smaller techniques and theoretical analysis methods, seem to be largely derived from previous work.\n\nWe thank for the reviewer's feedback in this aspect. However, we respectfully assert a differing viewpoint. While building upon prior research to establish a robust foundation, our **contribution is distinctive in its innovative synthesis and adaptation of Bayesian Coresets and Personalized Federated Learning frameworks**. The fusion of these domains creates a novel architecture of its own as it can be seen in Algorithm 1 with unique optimization objectives, thereby further leading to new theoretical formulations and findings, as evidenced in our Theoretical Analysis and extensive novel formulations in the Supplementary (Section 9 Lemma 1, Proposition 1, Proposition 3, Proposition 4. ).  We further refine and extend these objectives to enhance performance and efficiency. We firmly believe that the synergy of existing knowledge with our novel contributions significantly strengthens the overall impact and applicability of our work. We aim to further clarify certain aspects of the above thread with specific instances as we go forward."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288081564,
                "cdate": 1700288081564,
                "tmdate": 1700290675464,
                "mdate": 1700290675464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EkG0lpZGk1",
                "forum": "uz7d2N2zul",
                "replyto": "mgCGLlJuXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 2/4 to the Reviewer QB83"
                    },
                    "comment": {
                        "value": "The contribution on the coreset construction is limited. Although the authors introduce a new coreset objective, they do not provide sufficient NEW optimization techniques for the new objective. I could only identify some techniques borrowed from previous work.\n\nWe appreciate the reviewer's feedback. It is essential to emphasize that our primary contribution lies not in introducing new optimization techniques for the Bayesian Coreset construction but instead, our innovation centers around **proposing a comprehensive algorithm and corresponding optimization strategies for a common unifying framework** that governs two distinct optimization problems at hand. This approach leads to a **model-dependent coreset sampling technique** (in this case the model being trained under a personalized federated learning setup) i.e. along with the model training for federated learning, the coreset weights are also learnt dynamically for each client, which is a novel aspect of our contribution. While drawing inspiration from previous work is a common practice, our work's strength lies in the integration and adaptation of these techniques to address the challenges posed by our unique framework. In particular for bayesian coreset optimization problem, we **propose a novel objective loss in Eq 9/8** on top of which we apply Accelerated Iterative Hard Thresholding. It's noteworthy that while AIHT is an efficient optimization strategy in its own right, our emphasis is not on its intrinsic properties. Rather our **proposed optimization objective stands out for its considerable complexity** and to address the same, we **employ optimization strategies tailored to its nuances**, such as the use of AIHT. Detailed insights into this choice are provided in the Supplementary section proofs, affirming our confidence in the appropriateness of this approach.\n\n\n      In my opinion, the primary contribution of this paper is the modified objective (eq. 9) tailored to personalized federated learning. However, the advantages of this modified objective are not adequately elucidated in the current presentation.\n\n\nWe appreciate your recognition of the main contribution being the modified objective function (Eq. 9) tailored for personalized federated learning. Your feedback on the need for a more thorough elucidation of the advantages is duly noted. In Section 3.2, we delve into the rationale and motivation behind our proposed approach, offering a more comprehensive understanding of the modified objective's benefits. Additionally, we have conducted new experiments and updated our explanations in response to Question 2 under **Questions**, specifically addressing the advantages of the modified objective. This enhancement aims to provide a clearer and more detailed exposition of the strengths associated with our proposed contribution."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288738271,
                "cdate": 1700288738271,
                "tmdate": 1700290664151,
                "mdate": 1700290664151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dkzGM5rRfF",
                "forum": "uz7d2N2zul",
                "replyto": "mgCGLlJuXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 3/4 to the Reviewer QB83"
                    },
                    "comment": {
                        "value": "**Questions**\n\n      What is the benefits to apply coreset in the personalized federated learning? I think one of the most important is that it can reduce the communication complexity. It would be valuable to investigate and quantify the extent to which the coreset approach reduces communication complexity in the specific optimization task addressed in this work. This can be done theoretically, by providing a complexity formula, and practically, by presenting numerical results from experiments that show the reduction in communication complexity achieved.\n\nWe value the reviewer's input on comprehending the computational complexity and the effectiveness of our approach in this context. Consequently, we perform an analysis of communication complexity to assess the total number of communication rounds required for full convergence across various coreset subsample sizes. We present our results as follows (Additionally we have included these results under Supplementary section 10.2)\n\n\n| Method (Percentage = sampling fraction) | MNIST                       |                             | FashionMNIST                    |                             | CIFAR                       |                             |\n|-----------------------------------------|-------------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|\n|                                         | Test Accuracy                | Communication Rounds       | Test Accuracy                    | Communication Rounds       | Test Accuracy                | Communication Rounds       |\n|-----------------------------------------|-------------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|\n| $pFedBayes$ (Full)                      | 98.79                         | 194                         | 93.01                             | 215                         | 83.46                       | 266                         |\n| $pFedBayes$ with random sampling (50%)                      | 80.2                          | 135                         | 87.12                             | 172                         | 48.31                       | 183                         |\n| $Our Method$ (k = 50%)                  | 92.48                         | 98                          | 89.55                             | 93                          | 69.66                       | 112                         |\n| $Our Method$ (k = 30%)                  | 90.17                         | 84                          | 88.16                             | 72                          | 59.12                       | 70                          |\n| $Our Method$ (k = 15%)                  | 88.75                         | 62                          | 85.15                             | 38                          | 55.66                       | 32                          |\n| $Our Method$ (k = 10%)                  | 85.43                         | 32                          | 82.64                             | 24                          | 48.25                       | 16                          |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290214490,
                "cdate": 1700290214490,
                "tmdate": 1700290649251,
                "mdate": 1700290649251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6c42dksJ0h",
                "forum": "uz7d2N2zul",
                "replyto": "mgCGLlJuXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. I believe that the coreset optimization offers advantages in terms of the communication complexity. And the additional experiments indicate that the coreset has better performance than random sampling.  However, the comparision to the full data is not very competitive since the sampling size is of the same magnitude as the full data, which is also mentioned by other reviewers. The authors point out that the coreset method leads to smaller communication rounds. Therefore, i would like to see the performances of the full data and coresets of varying sizes in the case where the number of communication rounds is limited to different thresholds. It would be valuable to assess the advantages of coresets in scenarios where only a limited number of communication rounds are allowed. If such advantages are demonstrated, I would be inclined to raise my rating accordingly."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585915953,
                "cdate": 1700585915953,
                "tmdate": 1700586014433,
                "mdate": 1700586014433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LK6xTL2V5K",
                "forum": "uz7d2N2zul",
                "replyto": "mgCGLlJuXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you Reviewer QB83 for your response and helpful suggestions"
                    },
                    "comment": {
                        "value": "Dear Reviewer QB83,\n\nWe appreciate your helpful feedback and suggestion towards the importance of testing across multiple communication rounds threshold. We are therefore reporting new results on Test accuracy coresponding to the settings where only a limited number of communication rounds are now allowed (limited communication rounds settings 15,25,40 and 70 across all 3 datasets and for different sample subset size threshold).\n\nThis is in addition to the previous results as requested https://openreview.net/forum?id=uz7d2N2zul&noteId=dkzGM5rRfF where we are not considering any limitation on the number of communication rounds.\n\n| Method (Percentage = sampling fraction) |   MNIST                    |                             |                             |                             |   FashionMNIST               |                             |                             |                             |   CIFAR                     |                             |                             |                             |\n|-----------------------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|\n|                                         | Communication Rounds (at 15)| Communication Rounds (at 25)| Communication Rounds (at 40)| Communication Rounds (at 70)| Communication Rounds (at 15)| Communication Rounds (at 25)| Communication Rounds (at 40)| Communication Rounds (at 70)| Communication Rounds (at 15)| Communication Rounds (at 25)| Communication Rounds (at 40)| Communication Rounds (at 70)|\n|-----------------------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|-----------------------------|\n| $pFedBayes$ (Full)                      | 92.44                           | 93.17                           | 94.02                           | 94.77                           | 88.91                        | 89.20                           | 89.27                           | 90.33                           | 53.94                           | 59.54                           | 68.13                           | 72.77                           |\n| $Our Method$ (k = 50%)                  | 87.13                          | 91.62                      | 91.47                          | 91.94                         | 80.26                          | 83.57                      | 86.22                          | 89.13                          | 54.66                          | 59.11                      | 61.04                          | 64.58                          |\n| $Our Method$ (k = 30%)                  | 86.45                          | 89.12                      | 89.36                          | 90.15                          | 80.01                          | 84.55                     | 85.81                          | 88.28                        | 52.43                          | 55.95                      | 57.38                          | 59.12                          |\n| $Our Method$ (k = 15%)                  | 78.14                          | 85.17                      | 87.81                          | 88.73                          | 79.72                          | 83.41                     | 85.15                          | 85.14                          | 48.91                          | 54.96                      | 55.63                          | 55.81                          |\n| $Our Method$ (k = 10%)                  | 73.68                          | 84.26                      | 85.43\t                          | 85.51                          | 75.13                          | 82.64                      | 82.69                          | 82.71                          | 48.25                          | 48.26                      | 48.22                           | 48.29                          |\n\nWe hope that the rebuttal clarifies the questions raised in this aspect."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630156562,
                "cdate": 1700630156562,
                "tmdate": 1700630322845,
                "mdate": 1700630322845,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c0jSx07mhd",
                "forum": "uz7d2N2zul",
                "replyto": "LK6xTL2V5K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_QB83"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have decided to increase my score for the following reasons:\n\ni) The modifications made to the presentation.\n\nii) I believe that the coreset method shows potential advantages, particularly in terms of communication complexity, in the personalized federated learning.\n\niii) The experiments conducted in the paper demonstrate that the proposed coreset method outperforms random sampling.\n\nHowever, I have not increased my score significantly because the experiments indicate that there is no particular need to apply the coreset method in this Bayesian-PFL framework. The size of the coreset is comparable to that of the full data, and the performance is worse than that of the full data when the numbers of communication rounds are the same."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673231106,
                "cdate": 1700673231106,
                "tmdate": 1700673231106,
                "mdate": 1700673231106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dPuCmVnirN",
                "forum": "uz7d2N2zul",
                "replyto": "mgCGLlJuXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you Reviewer QB83 for Helpful Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer QB83,\n\nWe deeply appreciate the increase in score and yours positive acknowledgement on the concerns raised earlier in terms of the motivation for our framework and experimental analysis in efficiency and outperformance over random sampling.\n\nWe would like to point our here as well, that when we considered around 50% of sample size, that leads to around half the chunk of the training data that is lost. We have further decremented it to 10% data (i.e. only considering 10% of the original training size). In any case when we train on only a few samples as opposed to the entire training set we would expect less accuracy.  10% , 25% and 40% and 50% data are significantly smaller chunks of our original training data and hence it is imperative that we have a low performance as compared to full data training. In all these cases the sample size is very much less in the order than that of training set (even in the case of 50%). \n\nOur main experimental contribution is to show case the utility of coreset optimization in the case of Bayesian-PFL framework over random sampling of training data points or even applying popular submodular function based subset selection strategies where we get very less significant drop in accuracy (achieve near optimal accuracy as compared to the baselines)."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699103923,
                "cdate": 1700699103923,
                "tmdate": 1700699170505,
                "mdate": 1700699170505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "01OnEveC0T",
            "forum": "uz7d2N2zul",
            "replyto": "uz7d2N2zul",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9034/Reviewer_YU5h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9034/Reviewer_YU5h"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a method to use Bayesian coresets for each individual client in a federated learning setting. Bayesian coreset can be used as proxy for full data at each individual client to estimate client-side distribution. The authors describe objective functions to incorporate the Bayesian coresets with federated learning setting. The authors give an algorithm and also give theoretical guarantees for the generalization error and its convergence. The authors support their theoretical claims with empirical results comapring their proposed approach with a number of baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is, for the most part, well written. There is not much work in terms of coresets for federated learning and as such the paper will be of interest to the community.\n2. The authors have compared their method with a variety of baselines consisting of both - federated learning algorithms and also sampling strategies that incorporate diversity.  Their method performs well in most of the cases.\n3. The algorithm is backed with theoretical guarantees. I did not check the proofs, but the statements appear sound."
                },
                "weaknesses": {
                    "value": "1. I am not sure what is the challenge in incorporating the Bayesian coreset framework in federated learning setting. It would be better to explain clearly why this is a significant contribution. Both the algorithm and proof techniques appear to be heavily inspired from Zhang 2022b. The only modification seems to be use of Bayesian coresets. \n\n2. There are minor grammatical errors. Please do a grammar check."
                },
                "questions": {
                    "value": "1. Why the prior $\\pi$ in equation 1 is replaced by $\\mathbf{z}$ in eq.6 - the modified client-side objective. Please clarify.\n\n2. The subsample size is 50%. Is it not quite large? Does it give significant computational time benefits when compared with full data? Other than figure 3, there are no experiments mentioning computational efficiency.\n\n3. Not a question but a suggestion. Algorithm 1 is not easy to follow for anyone unfamiliar with existing work or similar algorithms. How exactly is the coreset getting constructed? It would be good to give a high-level description of the same. \n\nOverall, the paper appears sound and I would be happy to raise my score once the doubts are cleared."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9034/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698944555904,
            "cdate": 1698944555904,
            "tmdate": 1699637138048,
            "mdate": 1699637138048,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fJwFLhx5eh",
                "forum": "uz7d2N2zul",
                "replyto": "01OnEveC0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 1/3 to the Reviewer YU5h"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort that the reviewer has invested in reviewing our paper on utilizing Bayesian coresets in a federated learning setting. The reviewer's insightful feedback is invaluable to us, and we are grateful for the constructive comments that they have provided.\n\nFirstly, we are pleased to hear that the reviewer has found the paper to be well-written and acknowledges its potential significance to the community given the lack of such work in this space.\nSecondly we are thankful to the reviewer for acknowledging the thoroughness of our comparative analysis and theoretically motivated framework and that they found our method's performance commendable across a variety of scenarios, especially when benchmarked against both federated learning algorithms and diverse sampling strategies. The reviewer's positive assessment encourages us and reaffirms the effectiveness of our proposed approach.\n\n\n**Weaknesses**:\n\n     I am not sure what is the challenge in incorporating the Bayesian coreset framework in federated learning setting. It would be better to explain clearly why this is a significant contribution. Both the algorithm and proof techniques appear to be heavily inspired from Zhang 2022b. The only modification seems to be use of Bayesian coresets.\n\nOne of the **core problems that we try to address** here is how to **achieve efficiency in training in a Federated Learning setup** **using only a fraction of the original data at each client** yet achieving **near optimal performance in terms of accuracy** (theoretical results on our logarithm bounded convergence rate using coreset indicates better convergence than full training while empirical results across a diverse set of baselines including diversity based subset selection techniques show promising results in terms of accuracy). Both the Bayesian Coreset optimization framework and Personalized Federated Learning framework  which in itself involves a  bilevel optimization problem are two separate entirely different orthogonal problems. \n\nOur goal in this proposal is thus to **bridge the two towards a common optimization framework** whereby we can utilise a Bayesian coreset framework for strategic selection of data points at each client level and at the same time we do not need to pay for a bigger penalty while solving the Federated Learning optimization problem. Both these problems represent independent optimization challenges that need to be addressed concurrently. The difficulty arises when striving to learn the coreset in an optimized manner that minimizes the overall penalty incurred while training within the Federated Learning setting. Hence our work focuses on seamlessly **interleaving these two optimization frameworks** into a **single, cohesive framework**, introducing **novel optimization formulations as evident in Eq 8 and 7** (updated equation numbers due to manuscript revision), along with corresponding strong theoretical guarantees. Importantly, our *contribution addresses a gap in the existing literature*. To the best of our knowledge, this specific problem has not been thoroughly explored. There is a notable absence of research on subset selection or coreset-based strategies within a Federated Learning context, let alone the formulation of theoretical frameworks in this domain. \n\n\nAdditionally, as an application setting we also showcase the usefulness and applicability of our method on **real-world medical datasets where in many cases there is a severe dearth of medical data**.\nTo address the challenges in infusing the two optimization frameworks together and propose new findings on top of it, we draw upon some of the proof techniques from papers like Polson\n& Ro\u02c7ckov \u0301a (2018), and others, one of them being Zhang 2022b, particularly due to the similarities observed in Eq. 1. We utilize certain existing lemmas and theorems, on top of which we provide novel theoretical formulations and new results as seen in theorem 1 and theorem 2, and their accompanied proofs in Supplementary (Section 9 Lemma 1, Proposition 1, Proposition 3, Proposition 4. ) which are our primary contributions.\n\n[1] Nicholas G Polson and Veronika Ro\u02c7ckov \u0301a. Posterior concentration for sparse deep learning. \n\n     There are minor grammatical errors. Please do a grammar check.\n\nWe thank the reviewer for their careful inspection. We have made a thorough proofread pass and have identified the specific areas and have fixed them in our recent revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272559298,
                "cdate": 1700272559298,
                "tmdate": 1700278892551,
                "mdate": 1700278892551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fVNZ94QONs",
                "forum": "uz7d2N2zul",
                "replyto": "01OnEveC0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 2/3 to the Reviewer YU5h"
                    },
                    "comment": {
                        "value": "**Questions**\n\n\n    Why the prior in equation 1 is replaced by *z* in eq.6 - the modified client-side objective. Please clarify.\n\nThank you for your inquiry regarding the substitution of the prior in Equation 1 with 'z' in Equation 6. In Equation 1, we initially introduce the concept of a prior distribution when formalizing the client-side objective. For the sake of clarity and ease of understanding, we denote this prior probability distribution as $\\pi$ as it represents a more nuanced notation to indicate probability and also as it encompasses additional terms such as $\\pi(\\theta|D)$. In Equation 6, we streamline and simplify the representation while retaining the essence of the prior distribution in a more concise form using *z*. We have also adequately defined after the equation to resolve any confusion for the same. \n\n    The subsample size is 50%. Is it not quite large? Does it give significant computational time benefits when compared with full data? Other than figure 3, there are no experiments mentioning computational efficiency.\n\nWe do agree that taking subsample size = 50% can be considered somewhat large, but at the same time dropping too many data points may also result in degradation in performance (accuracy) irrespective of what sampling strategies we use. Hence we thought 50% to be a respectful threshold where we can measure the effectiveness of our proposed approach for maintaining near-optimal performance as compared to other baselines. Keeping in consideration the importance of studying across different thresholds we have included some of the baseline comparisons on less percentage data as follows.\n\nIn Appendix Section 10.2 we are including new results to show case the total number of communication rounds (per setting where we consider different coreset subsample sizes k=50,30,15,10) which is indicative of the convergence of our proposed method. Empirical results shows that with significant lesser coreset subsample sizes we achieve faster convergence with less number of communication rounds but with lesser accuracy than that if we used a larger coreset size or full data training. \n**Table: Comparative results of test accuracies across different coreset sample complexity**\n\n| Method (Percentage = sampling fraction) | MNIST                       |                             | FashionMNIST                    |                             | CIFAR                       |                             |\n|-----------------------------------------|-------------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|\n|                                         | Test Accuracy                | Communication Rounds       | Test Accuracy                    | Communication Rounds       | Test Accuracy                | Communication Rounds       |\n|-----------------------------------------|-------------------------------|-----------------------------|-----------------------------------|-----------------------------|-----------------------------|-----------------------------|\n| $pFedBayes$ (Full)                      | 98.79                         | 194                         | 93.01                             | 215                         | 83.46                       | 266                         |\n| $pFedBayes$ with random sampling (50%)                      | 80.2                          | 135                         | 87.12                             | 172                         | 48.31                       | 183                         |\n| $Our Method$ (k = 50%)                  | 92.48                         | 98                          | 89.55                             | 93                          | 69.66                       | 112                         |\n| $Our Method$ (k = 30%)                  | 90.17                         | 84                          | 88.16                             | 72                          | 59.12                       | 70                          |\n| $Our Method$ (k = 15%)                  | 88.75                         | 62                          | 85.15                             | 38                          | 55.66                       | 32                          |\n| $Our Method$ (k = 10%)                  | 85.43                         | 32                          | 82.64                             | 24                          | 48.25                       | 16                          |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273990928,
                "cdate": 1700273990928,
                "tmdate": 1700276671718,
                "mdate": 1700276671718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OhLxUOsyjN",
                "forum": "uz7d2N2zul",
                "replyto": "01OnEveC0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response 3/3 to the Reviewer YU5h"
                    },
                    "comment": {
                        "value": "Not a question but a suggestion. Algorithm 1 is not easy to follow for anyone unfamiliar with existing work or similar algorithms. How exactly is the coreset getting constructed? It would be good to give a high-level description of the same.\n\nIn Algorithm 1, our approach involves evaluating the client side objective function both the full data-based likelihood and the coreset weighted likelihood for each client. Subsequently, we calculate the objective function using Equation 8 ( now changed Eq number due to revised manuscript). To compute the coreset samples, we employ the highly effective Accelerated IHT algorithm atop this objective function. Supplementary Section 10.5 provides comprehensive details on the Accelerated IHT algorithm for a deeper understanding of its application in our methodology. \n\n----------------------------------------------------------------------------------------------------------\n\nWe hope that the rebuttal clarifies the questions raised by the reviewer. We would be happy to discuss any further questions about the work, and would appreciate an appropriate increase in the score if the reviewer\u2019s concerns are adequately addressed."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276659109,
                "cdate": 1700276659109,
                "tmdate": 1700279704405,
                "mdate": 1700279704405,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YNxl20UW0N",
                "forum": "uz7d2N2zul",
                "replyto": "OhLxUOsyjN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_YU5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9034/Reviewer_YU5h"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I have read the reviews and response from the author and am slightly more positively inclined towards accepting the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548088840,
                "cdate": 1700548088840,
                "tmdate": 1700548088840,
                "mdate": 1700548088840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]