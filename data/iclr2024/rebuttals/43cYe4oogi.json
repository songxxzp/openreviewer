[
    {
        "title": "Understanding Expressivity of Neural KG Reasoning from Rule Structure Learning"
    },
    {
        "review": {
            "id": "8bUNL8msce",
            "forum": "43cYe4oogi",
            "replyto": "43cYe4oogi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_TfWh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_TfWh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate the expressivity of GNN-based KG reasoning methods. The paper provides terminology for rule structure in the KG reasoning task and proves the limitation of current methods in structure rules T(h,x) and U(h,x) in theory. For the method, In algo 1, the initial representation is assigned to the entities whose out-degree is larger than a threshold d. The proposition 5.1 and the empirical results in Table 1 show that the proposed method can help discover structure rules T(h,x) and U(h,x)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The novelty is great, this paper provides a systematic way for rule structures finding in GNN based KG reasoning task.\n- The theoretical part is sound, and the experimental study supports the theoretical result as well.\n- The method is simple that assigning the initial representation to the entities but effective based on experimental results."
                },
                "weaknesses": {
                    "value": "n/a"
                },
                "questions": {
                    "value": "In page 7, it mentioned \" the additional time complexity introduced by entity labeling is linear with respect to the number of entities in the graph,  which is marginal in comparison to QL-GNN\". I am not sure about why is marginal considering the KG is usually large. Could you provide the numeric value about time cost for the experiments or adding more discussion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635495344,
            "cdate": 1698635495344,
            "tmdate": 1699636794371,
            "mdate": 1699636794371,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vgJc7SkoXs",
                "forum": "43cYe4oogi",
                "replyto": "8bUNL8msce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TfWh"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and provide valuable feedbacks.\n\n### Q. I am not sure about why is marginal considering the KG is usually large. Could you provide the numeric value about time cost for the experiments or adding more discussion?\n\n**A.**\nThank you for your feedback.\nDespite KG being large, the additional time required for EL-GNN is insignificant compared to QL-GNN. This is because the extra time in EL-GNN arises from traversing all entities on KG, which is not the main factor affecting time cost. The primary contributor to time cost is message passing and other time-consuming operations in QL-GNN.\nTo make the time comparison clearer, we improve the writing in section 4 of revision.\nAlso, we compare the time costs (seconds of testing) of QL-GNN(NBFNet) and EL-GNN(EL-NBFNet) in various KGs in the following table (Table 11 in revision).\n\n| Methods | Family | Kinship | UMLS | WN18RR | FB15k-237 |\n|---------|--------|---------|------|--------|-----------|\n|EL-GNN|270.3   |14.0     |6.7   |35.6    |20.1       |\n|QL-GNN   |269.6   |13.5     |6.4   |34.3    |19.8       |\n\nThe time costs of experiments are consistent with our analysis in Section 4 of revision and indicate a negligible additional cost(~1s) for EL-GNN compared to QL-GNN."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576375631,
                "cdate": 1700576375631,
                "tmdate": 1700576375631,
                "mdate": 1700576375631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YTOFgTKE6E",
            "forum": "43cYe4oogi",
            "replyto": "43cYe4oogi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_S7Jh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_S7Jh"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the domain of Knowledge Graph (KG) reasoning, which involves deducing new facts from existing ones in a KG. While Graph Neural Networks (GNNs) with tail entity scoring have recently achieved state-of-the-art performance in KG reasoning, there's a gap in the theoretical understanding of these GNNs. This work aims to bridge this gap by unifying GNNs with tail entity scoring into a common framework and analyzing their expressivity in terms of the rule structures they can learn. The insights from this analysis lead to the proposal of a novel labeling strategy to further enhance rule structure learning in KG reasoning. Experimental results support the theoretical findings and demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1 The paper provides a thorough analysis of the expressivity of state-of-the-art GNNs used for KG reasoning. By unifying these GNNs into a common framework (QL-GNN), the authors offer a structured approach to understanding their capabilities and limitations in terms of rule structure learning.\nS2 The introduction of the QL-GNN framework and the subsequent EL-GNN model showcases the authors' innovative approach to addressing the gaps in the current understanding of GNNs for KG reasoning. The EL-GNN, in particular, is designed to learn rule structures beyond the capacity of existing methods, marking a significant advancement in the field.\nS3 The authors don't just rely on theoretical findings; they validate their claims with experiments on synthetic datasets. The consistency between the experimental results and theoretical insights adds credibility to their claims and demonstrates the practical applicability of their proposed methods."
                },
                "weaknesses": {
                    "value": "W1  While the EL-GNN model is introduced as an improvement over QL-GNN, there's limited discussion on its scalability. How does EL-GNN perform when applied to very large-scale KGs? Are there any computational constraints or challenges that users should be aware of? Moreover, the experiments in the paper employ relatively small datasets. It would greatly benefit the research to include larger datasets to demonstrate the effectiveness of the proposed methods on a more substantial scale.\n\nW2 The paper introduces a novel labeling strategy to enhance rule structure learning. Are there specific scenarios in which this labeling strategy may not yield effective results or encounter limitations?"
                },
                "questions": {
                    "value": "Q1 Can the models trained on one KG be adapted or fine-tuned for another KG? If so, are there any specific considerations or challenges in doing so?\n\nQ2 How robust are QL-GNN and EL-GNN to noisy or incomplete data in KGs? Have any tests been conducted to assess their performance under such conditions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733118613,
            "cdate": 1698733118613,
            "tmdate": 1699636794221,
            "mdate": 1699636794221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yBvzvvScqB",
                "forum": "43cYe4oogi",
                "replyto": "YTOFgTKE6E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S7Jh"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and provide valuable feedbacks.\n\n### W1. Experiments\n\n**W1.1** How does EL-GNN perform when applied to very large-scale KGs? Are there any computational constraints or challenges that users should be aware of?\n\n**R1.1** For large-scale KGs, EL-GNN has a similar scalability with QL-GNN, because the extra time cost of EL-GNN comes from traversing all entities on KG, which costs negligibly compared to QL-GNN. We compare the time cost (seconds of testing) of QL-GNN(NBFNet) and EL-GNN(EL-NBFNet) on different KGs in the following table (in Table 11 of revision).\nThe results show that EL-GNN has negligible extra cost (~1s) when compared to QL-GNN.\n\n| Methods | Family | Kinship | UMLS | WN18RR | FB15k-237 |\n|---------|--------|---------|------|--------|-----------|\n|EL-GNN   |270.3   |14.0     |6.7   |35.6    |20.1       |\n|QL-GNN   |269.6   |13.5     |6.4   |34.3    |19.8       |\n\n**W1.2** Experiments on large datasets\n\n**R1.2** Thank you for the suggestion. In the revision, we conducted experiments on FB15K-237, a dataset with more edges. The accuracies of QL-GNN and EL-GNN are shown in the following table.\nIn the following table, the accuracy of EL-GNN is higher than QL-GNN, which indicates that EL-GNN improves the performance on large-scale datasets.\n\n| EL-NBFNet | EL-RED-GNN | NBFNet | RED-GNN |\n|-----------| -----------|--------|---------|\n|0.332 ($\\uparrow$ 3.5\\%)      | 0.322($\\uparrow$ 10\\%)      | 0.321  | 0.284   |\n\n### W2. Scenarios in which this labeling strategy may not yield effective results or encounter limitations\n\n**R2.** One possible limitation of EL-GNN is the risk of over-fitting due to its larger number of parameters compared to QL-GNN. However, by carefully choosing a suitable degree threshold d, a balance can be achieved between the fitting ability and performance of EL-GNN."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575885356,
                "cdate": 1700575885356,
                "tmdate": 1700575885356,
                "mdate": 1700575885356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EUmFWLroD9",
                "forum": "43cYe4oogi",
                "replyto": "VqxtS4VMtd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6850/Reviewer_S7Jh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6850/Reviewer_S7Jh"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the thorough responses. My opinion of the paper remains unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632124553,
                "cdate": 1700632124553,
                "tmdate": 1700632124553,
                "mdate": 1700632124553,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WOAOZhAF3I",
            "forum": "43cYe4oogi",
            "replyto": "43cYe4oogi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_zRzb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_zRzb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel perspective to understand the expressivity of recent GNNs for KG reasoning based on rule structure learning. It identifies the types of rule structures that different GNNs can learn and analyzes their advantages and limitations. It also introduces a unified framework, QL-GNN, that encompasses two SOTA GNNs, RED-GNN and NBFNet. Moreover, it presents a new labeling strategy based on QL-GNN, called EL-GNN, that can learn more rule structures. The paper validates the theoretical analysis and the effectiveness of QL-GNN and EL-GNN through experiments on synthetic and real datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a novel approach to understanding RED-GNN and NBFNet from a rule-learning perspective.\n\n- The theoretical analysis reveals the advantages and limitations of existing popular GNNs in KG reasoning. The paper also provides experimental results to support the theoretical conclusion. \n\n- Furthermore, the paper proposes two GNNs for KG reasoning, which outperform state-of-the-art models on several datasets."
                },
                "weaknesses": {
                    "value": "- In my opinion, the datasets used in Section 6.2 appear to be well-suited for rule-based methods. However, the most popular link prediction dataset, FB15K-237, was not included in this experiment. Therefore, I believe that the experimental results of Section 6.2 are insufficient to evaluate the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "- What about the performance of the proposed GNNs on FB15K-237?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745569254,
            "cdate": 1698745569254,
            "tmdate": 1699636794054,
            "mdate": 1699636794054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tVIYYbCcKF",
                "forum": "43cYe4oogi",
                "replyto": "WOAOZhAF3I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zRzb"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and provide valuable feedbacks.\n\n### W1 & Q1. Performance on FB15K-237\n\n**R1.** Thank you for your suggestions. To fully show the effectiveness of EL-GNN, we conducted experiments on FB15k-235 and added the results to Table 2 of revision. Below is the accuracy of EL-GNN and QL-GNN on FB15k-237.\nFrom this table, we can see that EL-GNN outperforms QL-GNN on FB15k-237, indicating that EL-GNN can improve performance on large-scale datasets.\n\n\n| EL-NBFNet              | EL-RED-GNN | NBFNet | RED-GNN |\n| ---------------------- | ---------- | ------ | ------- |\n| 0.332 ($\\uparrow$ 3.5\\%) | 0.322($\\uparrow$ 10\\%)      | 0.321  | 0.284   |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574997457,
                "cdate": 1700574997457,
                "tmdate": 1700574997457,
                "mdate": 1700574997457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qiYdUFjgfs",
            "forum": "43cYe4oogi",
            "replyto": "43cYe4oogi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_NwzR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6850/Reviewer_NwzR"
            ],
            "content": {
                "summary": {
                    "value": "This paper which studies theoretical properties of GNN models includes:\n* Analysis of the expressiveness of GNN models in terms of a rule-learning formalism. \n* Presentation a simple yet effective labeling strategy based on their analysis that yields improvements. \n* Empirical analysis that supplements the theoretical contribution. The proposed approach instantiated with RED-GNN and NBFNet yields positive improvements across real and synthetic datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper leads a technical and thoughtful analysis to what kinds of relationships GNN-based models can effectively represent and effectively predict. Strengths of the paper include:\n* **Formalism** for link prediction in knowledge graphs using CML. This allows the authors to describe which kinds of rule structures each class of model is able to represent. It allows for the generalization of existing methods to represent broader classes of rules.\n* **Empirical Successes** are demonstrated across a wide variety of datasets. These seem to indicate different kinds of graph and entity / relation structures. \n* **Theoretical Analysis** appears to be rigorous and formally describe the different classes of rules and what models are effective for each."
                },
                "weaknesses": {
                    "value": "My main concern with this paper is the presentation / structure and the way in which that presentation and structure limits the reader from connecting both the clear theoretical advantages of the proposed class of GNN to both the limitations of other classes and the empirical successes. Please correct me if you think I have misinterpreted or misunderstood things or put emphasis points inappropriately. I am mentioning these presentation points because I think the paper has a number of very nice properties that I would like readers to be able to more easily grasp and benefit from.\n\nW1. **Defining Expressivity** I think that the definition of expressivity used in the paper should be defined much earlier in the manuscript. I say this not only for the sake of readers unfamiliar with definitions of expressivity for GNNs but also for the sake of familiar readers understanding differences between the choice of expressivity definition and the choices of past work. The related work section, which appears before definitions of expressivity so far as I understand, provides too high level a comparison between past work to be meaningful (in my opinion) for all but the most familiar readers of these methods (as an aside, I think that the related work section as it is now would be better suited later in the paper, say before experiments). In my opinion, readers would benefit from explicitly talking about the relationship between generalization and representation of graphs immediately. \n\nW2. **Connecting Formalism and Data** While the formalism used is based on past work and as I understand motivated and accepted in those works as a meaningful formalism, I think that paper would be greatly improved with many more motivating examples from real data that express why the rule based formalism is meaningful. For instance, the example in Figure 1 is great. I see how it connects to Figure 3 and Corollary 5.2. However, how often do such patterns appear in the real world empirical datasets? How much of the gains from the given methods correlate with the existence of the kinds of subgraphs described? \n\nW3. **Understanding Generalization** I think my main point of confusion, which I was not able to resolve in my reading of the paper is how to think about generalization vs representation. As I understand it depends on which rules the model can learn. But I am having a hard time understanding how this relates to things like number of parameters, number of training examples, choice of aggregation functions, graph size, number of entities, number of relationships, etc. I am missing something fundamental here? E.g. How do number of examples / graph size / parameters relate to the base theorem C.2? Or do those things not matter in the analysis? It seems it depends only on $L$ is that correct?\n\nW4. **Presentation of some of the theoretical results** As a more minor point, I think readers would take away more from the theoretical results if the authors provided more remarks about the limitations/take aways from the theorem statements. For instance, I was confused about Theorem 4.4. For instance, I think I would have appreciated more handholding as to the result: \"The structural rules in Figure 2 cannot be learned by CompGCN due to Theorem 4.4.\"\n\nMy concern is that I think we need:\n (1) how the proposed formalism allows us to better analyze the generalization capabilities of models to understand why we would expect empirical successes\n (2) how the proposed formalism is reflected in real world datasets (e.g., the kinds of rule patterns indeed show up)\n (3) an understanding of why the proposed formalism and analysis is better than other forms of analysis that one could do. \nThese are certainly addressed by the paper, but I think that they could made significantly more crisp in the way the paper and results are presented.\n\nMinor:\n* I think the first sentence is missing an \"A\", \"A knowledge graph (KG) ...\"\n\nOther related work: \n* [Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters.](https://dl.acm.org/doi/10.1145/3097983.3098054)\n* [Neighborhood Growth Determines Geometric Priors for Relational Representation Learning](https://proceedings.mlr.press/v108/weber20a.html)\n* [What relations are reliably embeddable in Euclidean space?](https://arxiv.org/abs/1903.05347)"
                },
                "questions": {
                    "value": "* Can you say more about how to think about generalization and expressivity in regards to the above comments?\n* Can you say more about Theorem 4.4 and \"The structural rules in Figure 2 cannot be learned by CompGCN due to Theorem 4.4.\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6850/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6850/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6850/Reviewer_NwzR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785880663,
            "cdate": 1698785880663,
            "tmdate": 1700706338517,
            "mdate": 1700706338517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bsC9cmR9Ss",
                "forum": "43cYe4oogi",
                "replyto": "qiYdUFjgfs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NwzR"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and provide constructive feedbacks. We have followed your suggestion to improve the structures and presentations of our paper.\n\nFirst, we emphasize the significance of expressivity for model's generalization. Expressivity refers to a model's ability to represent information, while generalization is the expressivity a model can reach in practice after thorough training process. The experiments in this paper show that QL-GNN can reach such expressivity with standard deep learning training methods. On the other hand, a consensus in deep learning is that more expressivity typically leads to better generalization(e.g., GNN[1] & Transformer[2]). Our experiments also demonstrate that EL-GNN, with stronger expressivity, indeed leads to stronger generalization. The generalization of GNN is also the topic we are most interested in. We are glad to have more discussion with you.\n\n### W1. Defining Expressivity.\n**W1.1** Definition of expressivity used in the paper should be defined much earlier.\n\n**R1.1** The expressivity in our paper use logic as tools to study the ability of GNN for learning rule structures rather than distinguishing structures with different triplet representations, such as [3, 4].\nActually, the expressivity in our paper is called ``logical expressivity`` in related fields. To ease your concern and make the definition clearer, we have made the following revisions:\n\n- we used the term ``logical expressivity`` to emphasize the difference of expressivity in our paper from previous work;\n\n- we added a few sentences in Section 3.1 to explain what ``logical expressivity`` is (i.e., logical expressivity of GNN is a measurement of the ability of GNN to learn logical formulas and is defined as the set of logical formulas that GNN can learn);\n\n- we revised the connection between logical expressivity and learning rule structures in Section 3.2.\n\n**W1.2** related work section has high level comparison & discuss about generalization and representation immediately\n\n**R1.2** Thank you for the suggestion. The related work has been moved to the place before the experiment section in the latest revision.\nIn addition, we emphasized the relationship between generalization and expressivity in the 3rd paragraph of the introduction section in revision by pointing out that generalization is the maximum expressivity that QL-GNN can generalize through training."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573525819,
                "cdate": 1700573525819,
                "tmdate": 1700573525819,
                "mdate": 1700573525819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yV3uoVtNF7",
                "forum": "43cYe4oogi",
                "replyto": "JwSrmBogGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6850/Reviewer_NwzR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6850/Reviewer_NwzR"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed reply"
                    },
                    "comment": {
                        "value": "Thank you authors for your detailed reply. I believe that the changes strengthen the paper. I have, as a result, modified my review score from 3->5. I would encourage the authors to think more about the presentation of generalization & expressivity; I think there is room to make the points discussed here and in the revised paper even more crisp."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706318515,
                "cdate": 1700706318515,
                "tmdate": 1700706318515,
                "mdate": 1700706318515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]