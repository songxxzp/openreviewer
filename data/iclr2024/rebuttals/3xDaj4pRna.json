[
    {
        "title": "Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning"
    },
    {
        "review": {
            "id": "gF8i8Z7Vxf",
            "forum": "3xDaj4pRna",
            "replyto": "3xDaj4pRna",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_Jhr2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_Jhr2"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that SAM can enhance the quality of features in datasets containing redundant or spurious features and this effect can't be explained by in-distribution alone. They show two possible mechanisms for this effect, namely (1) SAM will upweight examples that can't predict well using so-called 'hard features', and (2) the modified gradient in SAM optimizer will attribute more to 'hard features' instead of well-learned 'easy features'. They support the claims with a simple controlled toy experiment and also experiment on real datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* **Originality.** To the reviewer's knowledge, this paper is very novel. The phenomenon that SAM may learn more diverse features than SGD even when both reach nearly perfect generalization hasn't been reported before.\n\n* **Quality.** The experiments conducted are complete and thorough.\n\n* **Significance.** The reviewer believes that the finding will deepen the understanding of the mechanism of SAM."
                },
                "weaknesses": {
                    "value": "* **Clarity.** The reviewer feels that clarity requires improvement in the paper. For example, the legend of Figure 5 is hard to follow. Also, the equality (1) seems to contain a typo. \n\n* **Verification of mechanism.** A possible way to verify whether the effect pointed out in the paper really contributes to the phenomenon is to try designing different variants using the discovered mechanism. The paper now only verifies that the importance weighting mechanism exists in real-world experiments but hasn't connected it with the diversity of features."
                },
                "questions": {
                    "value": "* **Q1.** Is it possible to verify the effect of the found mechanisms? (see weaknesses)\n\n* **Q2.** Can the authors try disentangle the two mechanisms and verify which is more important for the phenomenon?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418679282,
            "cdate": 1698418679282,
            "tmdate": 1699636983763,
            "mdate": 1699636983763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G3Kuqh0cRo",
                "forum": "3xDaj4pRna",
                "replyto": "gF8i8Z7Vxf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review, we appreciate your feedback!\n\n> The reviewer feels that clarity requires improvement in the paper. For example, the legend of Figure 5 is hard to follow. Also, the equality (1) seems to contain a typo.\n\nThanks for pointing this out. We will address all issues of clarity in the revised manuscript. For reference here, would like to clarify the meaning of the legend of Figure 5:\n\nAs a preliminary, Figure 5 aims to show that points that are well fit by the easy feature are up-weighted, and points that are poorly fit by the complex feature are down-weighted, on average. As a measure of how well each point is fit by the easy feature, we define the \u201csigned easy feature component\u201d as $y v_{\\text{easy}} \\Phi_{\\text{easy}}$. This term represents the magnitude of the output of the easy feature component of the network signed by whether the prediction is correct. When this component is large (well fit by easy feature), we observe that points are generally up-weighted. When the corresponding component for the hard feature is small (poorly fit by hard feature), we observe that points are generally up-weighted.\n\nIn the figure caption, we will emphasize that we are plotting, on the x-axis, how well each feature is fit, and on the y-axis, how much the corresponding point is weighted (on average).\n\n> Q1. Is it possible to verify the effect of the found mechanisms?\n\n> Q2. Can the authors try disentangle the two mechanisms and verify which is more important for the phenomenon?\n\nWe agree with this concern, and we present this as a limitation in our conclusion section. We want to clarify the challenges associated with verifying and quantifying the effects in practice. In particular, we want to explain the challenges in establishing a causal relationship between each effect and the improvement in feature diversity. Further, we argue that this challenge is precisely why we must rely on a toy model for understanding.\n\nFor the toy example, we are able to determine that both effects cause improvement in feature quality through the experiment presented in Figure 6. The exact experiment is explained precisely in Appendix C, but we summarize here. To isolate the learning rate effect, we train with SGD, but where the feature gradients term\u2014associated with the learning rate effect\u2014is computed with artificially balanced features. \n\nIdeally, we would like to perform the same experiment on realistic data. However, in order to artificially balance features, we would have to understand precisely how each feature is represented throughout the neural network, in order to make the appropriate perturbation. This is challenging because the community does not currently understand how features are represented throughout the network. \n\nWe can approximate this effect by taking advantage of our observation that a single SAM step will balance features. We can compute the feature gradients term under a SAM perturbation, which approximates the term if features were balanced. However, when training with SAM normally, the benefits of both the importance weighting effect and the feature learning effect are compounded. By approximating only one of these effects at every step means that we cannot take advantage of the cumulative balancing step of both terms, and thus such an experiment will underestimate the effect of each term individually.\n\nDue to the challenge of directly verifying the mechanisms on real data, we need an alternative way to verify our hypothesis. This is precisely the reason that establishing an interpretable toy setup that models the behavior of SAM is an important tool for understanding."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248251696,
                "cdate": 1700248251696,
                "tmdate": 1700248251696,
                "mdate": 1700248251696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8WSqbeDqAS",
                "forum": "3xDaj4pRna",
                "replyto": "G3Kuqh0cRo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_Jhr2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_Jhr2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the reply. I will keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728247441,
                "cdate": 1700728247441,
                "tmdate": 1700728247441,
                "mdate": 1700728247441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FXhdPz19u6",
            "forum": "3xDaj4pRna",
            "replyto": "3xDaj4pRna",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_qpzp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_qpzp"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the effectiveness of Sharpness-Aware Minimization (SAM).\nThe authors claims that SAM can enhance downstream generalization through feature diversity.\nEmpirical analysis of SAM trues to demonstrate that SAM offers feature-diversifying benefit."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper provides clear motivation.\n- The paper provides several experimental results from various perspectives."
                },
                "weaknesses": {
                    "value": "- Although the paper introduces several concepts including feature-probing error or phantom parameter, but it is still unclear why SAM can improve feature diversity.\n- What is the meaning of the ratio $v_{\\text{hard}}/v_{\\text{easy}}$? Why we can say the high ratio means re-balancing effect?\n- Why do we need to perturb the last layer only? It would be nice to describe in detail."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7986/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7986/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7986/Reviewer_qpzp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827846752,
            "cdate": 1698827846752,
            "tmdate": 1700647150919,
            "mdate": 1700647150919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K7IItgpPE5",
                "forum": "3xDaj4pRna",
                "replyto": "FXhdPz19u6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review, we appreciate your feedback!\n\nWe appreciate the suggestions to improve clarity and we hope to address them below. In our revised version, we will address each point explicitly in the paper. \n\n> Although the paper introduces several concepts including feature-probing error or phantom parameter, but it is still unclear why SAM can improve feature diversity.\n\nWe\u2019d like to ask for further clarification on what you mean that \u201cit is still unclear why SAM can improve feature diversity\u201d.\nIf you are concerned with our definition, we define feature diversity to measure how well each algorithm can achieve small linear probing error for all available features. More formally, we define a measure of feature diversity as the worst case (maximal) ProErr (see Equation 1) over the features. The lower the error, the more diverse the features. We will certainly clarify this precisely in the revisions.\nIf you are concerned with the methodology of the paper, please let us know if there is any particular issue with our argument: in general, we construct a toy setup where we show that when multiple features are present, SAM can promote the learning of features that SGD performs poorly on. We confirm that SAM improves feature diversity on real-world datasets, and we validate our conclusions from the toy setup.\n\n> What is the meaning of the ratio $v_{\\text{hard}} / v_{\\text{easy}}$?\n\nThe quantities $v_{\\text{hard}}$ and $v_{\\text{easy}}$ refer to the last-layer parameters of the architecture (specified in Equation 4 and following text). They affect the gradient\u2014and therefore update step\u2014of the model by placing weight on the hard and easy features, respectively, as described in Equation 6. Thus, the ratio $v_{\\text{hard}} / v_{\\text{easy}}$ is important because it determines the relative weight placed on the hard feature compared to the easy feature during the update step. For SGD, the easy feature tends to dominate the update step, and thus by placing more weight on the hard feature will \u201cbalance\u201d the feature contributions. This corresponds with increasing the ratio $v_{\\text{hard}} / v_{\\text{easy}}$.\n\n> Why do we need to perturb the last layer only?\n\nThe toy setup (LSAM) uses a last-layer-only perturbation for simplicity and for interpretability. Despite its simplicity, LSAM exhibits many of the core behaviors of SAM, such as improving feature diversity via the balancing effect. In addition to presenting an understanding of LSAM in the toy setup, we verify that the conclusions we make from the toy setup generalize to SAM---where we perturb every layer---on real data. Please refer to Section 6.3, \u201cVerification on Real-world datasets\u201d."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248216342,
                "cdate": 1700248216342,
                "tmdate": 1700248216342,
                "mdate": 1700248216342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VxrINK7jak",
                "forum": "3xDaj4pRna",
                "replyto": "K7IItgpPE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_qpzp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_qpzp"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the kind reponse."
                    },
                    "comment": {
                        "value": "I have read the author\u2019s response, and authors address my concerns in terms of the methodology. I have raised my score to 6, but I think there is still room for improvement in presentation."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647676190,
                "cdate": 1700647676190,
                "tmdate": 1700647676190,
                "mdate": 1700647676190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4GHqkzpeL0",
            "forum": "3xDaj4pRna",
            "replyto": "3xDaj4pRna",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_KbQq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_KbQq"
            ],
            "content": {
                "summary": {
                    "value": "Sharpness-aware minimization (SAM) is an alternative method to the traditional stochastic gradient descent (SGD) for training neural networks. The core idea behind SAM is to guide models towards \"flatter\" minima, believed to enhance generalization. However, recent works revealed that flatness is not the only reason for good generalization. This study proposes an additional advantage of SAM: it improves the quality of dataset features, especially in datasets with redundant or unnecessary features. SAM achieves this by suppressing already well-learned features, allowing lesser-known features to be recognized. This process leads to a more balanced model feature update. Consequently, SAM enhances representation learning by promoting feature diversity. This sheds light on SAM's effectiveness in out-of-distribution performance, offering a new perspective on SAM's benefits."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Generally, I think this paper provides an interesting view of SAM. \n\n- This paper provides a new point of view that the SAM algorithm can boost feature diversity. In detail, SGD may be biased towards learning the strongest features from the training data, while SAM promotes the learning of a diverse set of features. Such an explanation is different from the view of the sharpness-based or Hessian-based analysis. \n\n- The new view indicates that the SAM can not only benefit in-distribution tasks but also downstream or out-of-distribution tasks. The empirical study supports their results. The findings may inspire the community to develop new algorithms to improve feature learning.\n\n- This paper is well-organized and presented with comprehensive experiments and interesting illustration figures."
                },
                "weaknesses": {
                    "value": "- Incomplete Understanding of Mechanisms: The discovery that SAM can result in a variety of features is interesting. However, the underlying mechanisms of this finding are not entirely comprehended, particularly in the theoretical aspects.\n\n- Clarity in Data Model Description: The study explores SAM from a feature learning perspective. Nevertheless, the data model described in the paper lacks clarity. It would enhance the paper's comprehensibility if the authors could explicitly present the formula of the toy model (perhaps in the Appendix).\n\n- Elaboration on Feature Measurement: The paper introduces a method to measure strong and weak features. I think it would be an independent significant contribution. An in-depth elaboration on this measure would further strengthen the paper's impact."
                },
                "questions": {
                    "value": "- In experiments such as CIFAR-MINIST, do all the data have a strong feature and a weak feature, or only some of the data have a strong feature while others do not? In equation 6, the authors divide the gradient into the direction of the easy feature and the hard feature, so it seems that each data has both strong and weak features.\n\n- A recent theoretical paper [1] shows that SAM can prevent noise memorization by deactivating the neurons that align with noise and, therefore, can get better in-distribution performance. The authors provide a novel view of the strong-weak features but haven't considered the noise in the model. So, I am wondering how the noises (which are independent of the label ) influence the SAM and SGD's out-of-distribution performance. \n\n-  The batch size $m$ of SAM updates plays an important role [2, 3, 4]. It is observed and proved that m-SAM (SAM with batch size $m$) can have different implicit biases and, therefore, get different performances. It would be better if the author could include the batch size in the methodology and have some discussions. \n\n- I checked the appendix in detail and found that the authors calculate the ratio of importance weighting terms for different neural network architectures. The authors only calculate the gradient for deep linear networks and leave the general networks blank. I agree that the general network's gradient is complex and hard to compute. However, adding some discussion for the two-layer neural networks can help the authors better understand the intuition of this paper,  which is still simple and easy to calculate [1, 4]. \n\n[1] Chen et al. \"Why Does Sharpness-Aware Minimization Generalize Better Than SGD?\" arXiv preprint arXiv:2310.07269, 2023.\n\n[2] Foret et al. \"Sharpness-aware minimization for efficiently improving generalization.\" In ICLR, 2021.\n\n[3]  Andriushchenko et al. \"Towards understanding sharpness-aware minimization.\" In International Conference on Machine Learning, pp. 639\u2013668. PMLR, 2022.\n\n[4] Wen et al. \"Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.\" arXiv preprint arXiv:2307.11007, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698862533456,
            "cdate": 1698862533456,
            "tmdate": 1699636983519,
            "mdate": 1699636983519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ByURmllKes",
                "forum": "3xDaj4pRna",
                "replyto": "4GHqkzpeL0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review, we appreciate your feedback!\n\n> It would enhance the paper's comprehensibility if the authors could explicitly present the formula of the toy model.\n\nOf course! We have added this to the revised paper. Please see Appendix E in the revised copy.\n\n> An in-depth elaboration on [the linear probing error] measure would further strengthen the paper's impact. \n\nWe will add discussion of this measure in detail in the appendix in the form of extended related work, in which we will discuss the following. The linear probing measure\u2014in which we measure feature quality by measuring the error of the linear classifier on top of the representation layer that minimizes error of the desired feature\u2014has been applied previously.  Previous work has shown that the last layer representations often encode features that are useful for out-of-distribution generalization even when the neural network performs poorly OOD (without linear probing) [1]. Surprisingly, linear probing error is often close to the error as if the neural network was finetuned (with all parameters) on the downstream data [2], which makes it a strong measure of feature quality. In addition, linear probing is often itself used for downstream generalization when there is only limited downstream data available, or in compute constrained environments, notably for [3]. \n\n> In experiments such as CIFAR-MINIST, do all the data have a strong feature and a weak feature, or only some of the data have a strong feature while others do not?\n\nIn all cases, every data point has both features: for example, in CIFAR-MNIST, the \u201chard\u201d CIFAR feature was {truck, airplane}, and an image of a truck or an image of an airplane would be present, and similarly for MNIST, {0, 1}. This is also true of every other dataset that we consider. \n\nWe have added experiments to the toy setting but where each feature is not necessarily present in every training example. Consistent with our results, we find that in this setting, SAM improves performance. Please see our \u201cGeneral Response\u201d for specific details.\n\n>  So, I am wondering how the noises (which are independent of the label ) influence the SAM and SGD's out-of-distribution performance. \n\n> It would be better if the author could include the batch size in the methodology and have some discussions.\n\nWe have added experiments to test various types of noise and varied batch size in our toy setting. Consistent with our results, we find that in both of these settings, SAM improves performance. When we vary batch size, consistent with prior work, small batch size tends to improve performance with SAM. Please see our \u201cGeneral Response\u201d for specific details.\n\n> Adding some discussion for the two-layer neural networks can help the authors better understand the intuition of this paper.\n\nWe have added the analysis of the importance weighting ratio for a multi-layer ReLU network (MLP) to the revised paper (Appendix D. Example 4). Similar to the multi-layer linear case, the importance weighting ratio is a weighted sum of the intermediate layer outputs. However, unlike the multi-layer linear case, these weights can depend on the inputs.\n\nPlease let us know if you have any additional questions or want further clarification!\n\n[1] https://openreview.net/forum?id=Zb6c8A-Fghk#\n\n[2] https://arxiv.org/abs/2202.06856\n\n[3] https://arxiv.org/abs/2212.07143"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248161318,
                "cdate": 1700248161318,
                "tmdate": 1700248161318,
                "mdate": 1700248161318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "23Auk2tF1M",
            "forum": "3xDaj4pRna",
            "replyto": "3xDaj4pRna",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_BgWc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_BgWc"
            ],
            "content": {
                "summary": {
                    "value": "This paper empirically studies the mechanism of Sharpness-Aware Minimization (SAM), which improves generalization beyond in-distribution data (e.g., on downstream or out-of-distribution tasks). The authors demonstrate that SAM has a significant impact on improving representation quality through feature diversity, i.e., promoting a neural network to learn both easy and hard features. Through toy experiments, the authors isolate two core mechanisms within SAM that lead to better feature quality: 1) importance sampling, and 2) learning rate scaling. The former one is further verified on real-world datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Explaining the effectiveness of SAM from the perspective of feature quality is novel.\n- The authors identify that SAM\u2019s feature improvement cannot be solely explained by in-distribution improvements.\n- The authors carefully design a toy experiment, based on which two reasonable mechanisms are isolated to explain why SAM can learn the hard-to-learn features."
                },
                "weaknesses": {
                    "value": "- The authors argue that due to the ability to learn diverse features (both easy and hard), SAM can improve the generalizability beyond in-distribution. However, this raises a question: are all these features beneficial to the out-of-distribution generalization or downstream tasks? For example, in Table 1, the hard-to-learn feature in CelebA is the \"gender feature\" while the task is to predict the \"hair color\". In this case, the easy features are correctly related to the labels, and there exists spurious-correlation between the hard \"gender feature\" and the labels. The SAM indeed decreases both the probe errors for easy and hard features, but the latter is harmful for ood generalization. Thus, it seems contradictive between \"SAM learns diversity features\" and \"SAM improved generalizability beyond in-distribution\". The authors should provide more explanations.\n- While the authors identify two underlying effects to explain why SAM learns diversity features with better quality, the learning rate effect is hard to verify in real-world datasets.\n- Minor: In Sec3.1, Eq 1, replace the $\\phi_\\theta$ to $\\Phi_\\theta$."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871356588,
            "cdate": 1698871356588,
            "tmdate": 1699636983417,
            "mdate": 1699636983417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hLM7XjzAv8",
                "forum": "3xDaj4pRna",
                "replyto": "23Auk2tF1M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review, we appreciate your feedback!\n\n> Are all these features beneficial to the out-of-distribution generalization or downstream tasks?\n\n> [...] It seems contradictive between \"SAM learns diversity features\" and \"SAM improved generalizability beyond in-distribution\".\n\nWe apologize for any confusion and clarify that indeed feature diversity is beneficial to OOD/downstream tasks. There is no contradiction here. In fact, feature diversity is key for better OOD/downstream performance. We explain why below.\n\nPrior works that study Waterbirds and CelebA typically treat these settings as having a  \u201ccore/good\u201d feature (e.g., hair) and a \u201cspurious/bad\u201d feature (e.g., gender) which is merely correlated with the label. These works only care about distribution shifts where the spurious feature in particular is no longer predictive of the label. However, our work takes a more general view where we want to tackle distributions where either one of the features (hair or gender) could possibly become unpredictive of the label.  Thus, we argue that ideally we want our learned representation to have both the \u201chair\u201d and \u201cgender\u201d feature: this is because a downstream task may require using any one of these features (either hair or gender). If both features are learned, then a linear probe can successfully pick up whichever feature is relevant downstream. We clarify that this does not contradict the \u201cspurious correlation\u201d literature, where the nomenclature can be somewhat misleading: there, the practice is to call the easier feature the \u201cspurious feature\u201d since their interest is specifically in learning the feature that SGD cannot learn (the non-spurious one). However, in the real-world, where any feature (either the easy or the hard one) can be \u201cspurious\u201d in that its correlation with the label may disappear in the downstream task.\n\nTo confirm that the features learned are useful for OOD/downstream tasks, we have added additional experiments. We show that SAM improves domain-transfer performance on DomainBed, a realistic dataset of domain shifts. Please see our \u201cGeneral Response\u201d for more specific details about both additions.\n\n\n> While the authors identify two underlying effects to explain why SAM learns diversity features with better quality, the learning rate effect is hard to verify in real-world datasets\n\nWe agree with the concern that it is still unclear how the learning rate effect generalizes to a realistic setup. We present this as a limitation in our conclusion section. The key challenge in achieving a similar observation of the learning-rate effect on real data is understanding precisely how features are represented. Since SAM perturbs the weights at every layer, we would need to understand how each feature is represented at every layer of the neural network. Based on the general principle that SAM perturbs weights as a function of how much they affect the output, we suspect that well-learned features will be inhibited by SAM at every layer. \n\nNonetheless, due to the challenge of directly verifying the mechanisms on real data, we need an alternative way to verify our hypothesis. This is precisely the reason that establishing an interpretable toy setup where the latent features are explicitly disentangled\u2014here we can understand the behavior of SAM.\n\nPlease let us know if you have any additional questions or want further clarification!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248112282,
                "cdate": 1700248112282,
                "tmdate": 1700248112282,
                "mdate": 1700248112282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AxOSsbm3S9",
                "forum": "3xDaj4pRna",
                "replyto": "hLM7XjzAv8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_BgWc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_BgWc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. I keep my score after reading the authors\u2019 response and other reviewers' comments."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641632067,
                "cdate": 1700641632067,
                "tmdate": 1700641632067,
                "mdate": 1700641632067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vJcB47ouQ3",
            "forum": "3xDaj4pRna",
            "replyto": "3xDaj4pRna",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_NKN8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_NKN8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies why Sharpness-Aware Minimization (SAM) can generalize well. They provide an explanation based on the weight that SAM/SGD put on different features, explaining SAM can up-weight features that are harder to learn."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think the idea of balancing features is interesting, and it makes sense to study SAM from this perspective. I agree that sharpness of the loss cannot alone explain the differences of SAM/SGD, and I appreciate this work's attempt to go beyond."
                },
                "weaknesses": {
                    "value": "The paper is inherently more on the empirical side, which I think is fine. However, I think some simplified theory could increase the contribution of the work. Based on this, I'd rate the paper as borderline accept."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7986/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7986/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7986/Reviewer_NKN8"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699050847805,
            "cdate": 1699050847805,
            "tmdate": 1699636983312,
            "mdate": 1699636983312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g3QLhb4b5M",
                "forum": "3xDaj4pRna",
                "replyto": "vJcB47ouQ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your support of our contributions. We are excited to present work that moves beyond the notion that sharpness can explain generalization. We hope that our empirical but principled insights can serve as useful guidance for future theoretical characterizations of SAM, and can motivate the development of principled algorithms."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248078421,
                "cdate": 1700248078421,
                "tmdate": 1700248078421,
                "mdate": 1700248078421,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rSG4PfG68u",
            "forum": "3xDaj4pRna",
            "replyto": "3xDaj4pRna",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_LFgC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7986/Reviewer_LFgC"
            ],
            "content": {
                "summary": {
                    "value": "Several learning theories, e.g., PAC Bayes, explain that flatter minima generalize better.\nThus, SAM is formulted to find flatter minima, and is known to improve in-distribution\ngeneralization.\nHowever, the beyond in-distribution performance is not well understood.\nThis paper focuses on  a feature-diversifying effect of SAM that is relevant for downstream generalization.\nAs seen in simplicity bias or gradient starvation,SGD exhibits a bias toward learning simple/easy-to-fit feature, even if the training data has multiple latent features that are all useful for prediction.\nIn contrust, this paper explans that SAM also learn the hard-to-fit features.\nThe authors shows this property of SAM by constructing a toy setting consisting of an easy-to-fit and a hard-to-fit features."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper seems to be the first to show the relationship between easy-to-fit and hard-to-fit features and learning by SAM. \nThis perspective is useful in that it opens up the possibility that SAMs can solve problems that SGD has, such as gradient starvation."
                },
                "weaknesses": {
                    "value": "While the analysis of the toy setting is interesting, it is unclear to what extent the analysis is applicable to learning with real images such as the winter bird and CelebA datasets."
                },
                "questions": {
                    "value": "Q1.What does it take in the future to extend the theory from toy setting to reality image identification? Please explain how difficult it would be. \nQ2: As a question related to Q1, in the toy setting, easy-to-fit and hard-to-fit are separated as latent features, but this may not necessarily be the case in real-world data. For example, is it possible that at the time of the feature extractor, the SGD is trained in such a way that the signals of hard-to-fit features do not reach the output layer in the first place? Is there any way to know that easy-to-fit and hard-to-fit features are disentangled in latent features in a real-world problem?\nQ3: How much is the influence of the terms after the second order in Eq.(20), which can be ignored since the hessian trace is small for the purpose of finding the fat minima? In this case, what is the effect during the learning process?\nQ4. The word \"diversity\" is used in this paper; however, it actually refers only to two characteristics, easy-to-fit and hard-to-fit.\nIn terms of beyond-in-distribution setting,  transferabiliity is also important. For example, robust/non-robust features in the analysis of adversarial examples, etc. could be considered. Please explain the relationship with other features.\nQ5. There are oftern noisy features as well as easy-to-fit and hard-to-fit features in real images. Is it possible to analyze the effect of SAM on noisy features that are typically hard-to-fit."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699230420452,
            "cdate": 1699230420452,
            "tmdate": 1699636983202,
            "mdate": 1699636983202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "203BXtCnxW",
                "forum": "3xDaj4pRna",
                "replyto": "rSG4PfG68u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We agree that there is currently a gap between the assumptions of the toy setting and a more realistic setting. We hope to clarify the specific challenges that we face in generalizing our results to more realistic settings. However, we want to emphasize that we do not require all of these assumptions to be held strictly, and we observe improvements in SAM even when these assumptions are broken. All discussed below:\n\n> It is unclear to what extent the analysis is applicable to learning with real images\n> What does it take in the future to extend the theory from toy setting to reality image identification?\n> Is it possible that the signals do not reach the output layer in the first place.\n\nWe present analysis to verify the learning rate effect in real data in Section 6.3. To summarize, we verify the two main predictions from the toy: (1) the distribution over importance weights becomes more uniform under SAM (Figure 4), and (2) SAM up-weights points that are well-fit by the easy feature and poorly fit by the complex feature (Figure 5).\n\nWe agree with the concern that it is still unclear how the learning rate effect\u2014in which we argue that SAM balances the step size in the direction of each feature\u2014generalizes to a realistic setup. The key challenge in achieving a similar observation of the learning-rate effect on real data is understanding precisely how features are represented. Since SAM perturbs the weights at every layer, we would need to understand how each feature is represented at every layer of the neural network. Based on the general principle that SAM perturbs weights as a function of how much they affect the output, we suspect that well-learned features will be inhibited by SAM at every layer. We expect that if the signal of a particular feature does not reach the output layer, SAM will promote this feature.\n\nThis challenge is precisely the reason that establishing an interpretable toy setup is an important tool for understanding. \n\n> In the toy setting, easy-to-fit and hard-to-fit are separated as latent features, but this may not necessarily be the case in real-world data\n\nWe agree, as you mention, that features may not be entirely disentangled in the latent space. Nonetheless, we observe the benefits of feature learning with SAM and can identify the importance weighting effect with realistic data. This suggests that our analysis holds even when the features are not entirely separated as latent features.\n\n> Is there any way to know that easy-to-fit and hard-to-fit features are disentangled in latent features in a real-world problem.\n\nAs mentioned above, the community does not have a solid understanding of how features are represented by neural networks. Thus it may be difficult, in general, to determine if features are disentangled. Recent work suggests that features are often at least partially disentangled (see Appendix B.4 of [1]). \n\nHowever, We suspect, for the reasons enumerated above, that SAM should promote harder features even when they are not entirely disentangled.\n\n> How much is the influence of the terms after the second order in Eq.(20), which can be ignored since the hessian trace is small for the purpose of finding the fat minima?\n\nExactly as you mention, since SAM is optimizing for minima with a small spectral norm and since $\\rho$ is relatively small (in practice, we find that performance is typically maximized for $\\rho$ around 0.05 or so), we suspect that the second order term is unlikely to contribute meaningfully to the dynamics.\n\n > The word \"diversity\" is used in this paper; however, it actually refers only to two characteristics, easy-to-fit and hard-to-fit. [...] For example, robust/non-robust features.\n\nWe call a feature \u201ceasy-to-fit\u201d or \u201chard-to-fit\u201d relative to another depending on which feature SGD learns better. In any real-world scenario, such preferential learning is bound to happen in the presence of multiple features. In your robustness example, non-robust features are known to be preferentially learned by SGD over robust features [2], thus our insights would apply there too. Consistent with this, SAM has been shown to improve adversarial robustness [3].  Besides, our results on DomainBed show that SAM indeed helps learn more transferable features in a realistic setting.\n\n> Is it possible to analyze the effect of SAM on noisy features that are typically hard-to-fit.\n\nGreat point! We have added new experiments based on this suggestion. We find that SAM improves domain-transfer performance on DomainBed, a realistic dataset of domain shifts. In addition, we have added experiments that explore variants of the toy in which we add various types of noise to the features. Consistent with our results, we find that LSAM improves performance in this setting. Please see our \u201cGeneral Response\u201d for more specific details about both additions.\n\n\n[1] https://openreview.net/pdf?id=Zb6c8A-Fghk\n\n[2] https://arxiv.org/abs/2006.07710\n\n[3] https://arxiv.org/abs/2305.05392"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248601006,
                "cdate": 1700248601006,
                "tmdate": 1700248601006,
                "mdate": 1700248601006,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LmM9YTUuIm",
                "forum": "3xDaj4pRna",
                "replyto": "203BXtCnxW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_LFgC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7986/Reviewer_LFgC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for answering many questions.\nI wanted to raise the score, however this time I found that the score 6 is next to 8 in the system.\nIt still seems to me that there is a gap between the theory and the real data setting if I were to raise the score to 8.\nTherefore, I would like to keep the score at 6 but note that it is more positive than 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7986/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645761484,
                "cdate": 1700645761484,
                "tmdate": 1700645761484,
                "mdate": 1700645761484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]