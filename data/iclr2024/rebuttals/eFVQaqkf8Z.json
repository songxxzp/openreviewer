[
    {
        "title": "Revisiting Non-separable Binary Classification and its Applications in Anomaly Detection"
    },
    {
        "review": {
            "id": "6e0YBIVyjJ",
            "forum": "eFVQaqkf8Z",
            "replyto": "eFVQaqkf8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8167/Reviewer_7KS5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8167/Reviewer_7KS5"
            ],
            "content": {
                "summary": {
                    "value": "The standard linear SVM fails in the classification of the XOR problem.  To resolve this problem, the authors proposed a new paradigm, equality separation. Additionally, they integrated the idea of equality separation into the neural network and applied the proposed method to supervised anomaly detection tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The authors studied the VC dimension of the proposed equality separation. The idea of equality separation can also be applied in the neural network.\n* The authors introduced the notion of closing numbers to quantify the difficulty of forming a closed decision boundary."
                },
                "weaknesses": {
                    "value": "* Many times, I got overwhelmed and distracted by the narration and the layout of the manuscript. For examples,\n  * The sentence \"Equality separators use the distance from a learnt hyperplane as the classification metric\" is confusing: A distance is measured between two objects, but here the authors only mention one object, i.e., the learnt hyperplane.\n  * I feel the authors spent too much tutorial-like narration for VC dimension, closing number, and locality, in the main manuscript. In particular, what is the purpose of the over-detailed VC dimension? What useful information can we conclude in this section? Did the authors want to use the VC dimension to give some theoretical bound of generalization errors?\n  * In section 2.2, it is difficult to follow the mixed descriptions. The authors may use bullet points to describe case-by-case and use some plots to support the narration, if necessary.\n  * In section 3, the popped sentence \"The utility of equality separators becomes evident in AD setting\" is confusing since there is no particular interpretation of the utility of the equality separator in Anomaly Detection in the previous section after the introduction. As for the \"Anomaly detection\" in the introduction, it looks more like related works, maybe the authors could consider moving that part into Section 3.\n\n* For the equality separator, what is the necessity of this proposed method? \n  * Even though linear SVM fails to solve the XOR classification while equality separator can, why do not use kernel SVMs?\n  * In Figure 3(e), what if the unseen classes fall into the purple region but are far away from the brown points? Will they be classified as brown classes when using $\\epsilon$-error separator?  What if the brown class is surrounded by the blue class which consists of several cohorts? In this case, does $\\epsilon$-error separator work?\n  * When considering the toy example in Figure 3, the authors also use the kernel to improve the shallow equality separator. Does this imply that the proposed equality separator (even though it is simple and linear) in general is not proper without kernel or activation?\n\n* The decision of  $\\epsilon$-error separator depends on the value of $\\epsilon$, but I cannot see any discussion on the choice or computation for the value of  $\\epsilon$.\n\n* Since the paper is titled \"in Anomaly Detection\", it should contain more well-established anomaly detection benchmarks (http://odds.cs.stonybrook.edu)\n\n* There is no discussion on Deep One-Class Classification [1] or a comparison with it. This related work also targets anomaly detection by forming a circle boundary to the normal classes.\n\n* Is that possible to graphically show the closed decision boundaries on other examples formed by the proposed method? \n\n[1] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M\u00fcller, Marius Kloft Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4393-4402, 2018."
                },
                "questions": {
                    "value": "* Multiple minor issues:\n  * Line 1, Page 3: do you mean $\\mathbb{R}^+\\cup\\{0\\}$?\n  * Theorem 2.3.: separators $\\mathcal{H}$ \"in\" Def.\n  * Corollary 2.4: do you mean $\\mathcal{H}_\\epsilon$?\n\n* \"modeling a halfspace separator \u2026. with an equality separator requires non-linearity like ReLU\":  could the authors explain more about how the ReLU reflects the modeling for a halfspace with an equality separator?\n\n* \"equality separators yield a different optimal hyperplane compared to SVMs in binary classification\": could the authors articulate the \"optimality\" here?\n\n* \"where equality separation is more conservative in classifying the positive brown class\": What do you mean by \"more conservative\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698423765604,
            "cdate": 1698423765604,
            "tmdate": 1699637012426,
            "mdate": 1699637012426,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lAfOFgXN7y",
                "forum": "eFVQaqkf8Z",
                "replyto": "6e0YBIVyjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifying questions and other questions"
                    },
                    "comment": {
                        "value": "We thank this reviewer for the feedback on how to improve our paper. We updated the paper to increase its clarity based on the feedback given (e.g. on distance, bullet points, related works section, small typos). We proceed to answer other questions.\n\n# Why VC dimension?\n\nAs mentioned in the global response, VC dimension analysis shows that equality separation (ES) is more expressive than halfspace separation (HS), not just a linear solution to XOR. The generalization bound can be directly obtained with VC dimension if desired.\n\n# Tutorial-like narration\n\nWe explain what locality is to motivate the need for quantitative metrics (closing numbers). It was our goal to explain to a wider audience who may not be familiar with both learning theory and anomaly detection (AD) literature. If this presentation is problematic, we ask this reviewer to suggest how they think we should edit the presentation.\n\n# Why ES? Why not kernel SVMs? Can ES work without kernels?\n\nXOR is our starting point: we point out that the widely known \u201cfact\" that XOR cannot be classified with linear models is indeed false. No kernels needed.\n\nIn general, the motivation of using any classifier depends on the use case. In our work, we observe interesting properties of ES for AD and corroborate this in our experiments. For instance, we observe that kernel ES perform much better than kernel SVMs in our synthetic supervised AD experiment \u2013 it is ES that performs well, the kernel merely does feature embedding.\n\nWe do not over-claim that (linear) ES works for every task: raw data may not be linear classifiable. We use standard methods to embed raw data into feature space before linear classification: RBF kernels and neural networks in tabular datasets, and a pre-trained computer vision model (DINOv2) for MVTec defect dataset.  What we claim is that the _paradigm_ of ES is better for AD, that we should impose one class (the normal class) to have more structure than the other (the anomaly class). We show this theoretically with closing numbers and empirically with supervised AD experiments.\n\n# Failure modes? E.g. Fig 1e.\n\nIf unseen anomalies fall in the margin, they will indeed be misclassified. Hence, the need for finite-volumed decision regions is important, and we may not be able to achieve that with just 1 ES. This observation motivates our introduction of closing numbers (especially when paired with neural networks, where neurons in a hidden layer work together), which balances the learnability and the openness of the decision region (Figure 2).\n \nAlthough ES may not classify everything correctly, we observe that it can classify the brown class _better_ than the corresponding HS: the set of misclassified non-brown points from ES is a proper subset of misclassified non-brown points by HS.\n\n# Discussion on $\\epsilon$?\n\nThe margin width is $\\frac{2\\epsilon}{||\\mathbf w||_2}$. In Appendix F1 and F3 (Figure 8), we discuss with respect to the reparameterization of $\\epsilon$ to $\\sigma$ in the bump function.\n\nOur main goal was to assess the separation of normal data from anomalies \u2013 if there is no/little separation, choosing a threshold would be moot. Hence, we use AUPR as our metric. During inference, the (output) threshold depends on the tolerance on the false positive rate. Since the\ntolerance is domain-dependent, we do not prescribe a threshold in our general framework.\n\n# AD Benchmarks\n\nNSL-KDD is an improved version of the KDDCUP99 dataset mentioned in the website given by this reviewer \u2014 it is a standard cyber-security benchmark. In this revision, we included the standard MVTec defect dataset used in AD (for instance, in Explainable Deep One-Class Classification [1]) and thyroid dataset from the suggested website.\n\n# Deep One-Class Classification (Deep SVDD) discussion?\n\nIn our paper, we use the improved Deep Semi-supervised Anomaly Detection (SAD) as a SOTA benchmark, which is Deep SVDD that can use labels. This discussion is in section 3.1 and 4.2 e.g. \u201cSAD is \u2026 an RBF separator\u2026\u201d.\n\n# Possible to graphically show other examples?\n\nWe are unsure of what this reviewer means and would like to ask for clarification. \n\n# Modeling halfspace with ES and ReLU?\n\nAs per section 2.2, ES acts as a HS by splitting the space into the positive and the zero parts of the ReLU.\n\n# Optimality of 2 ES for HS?\n\nWe answer this in the last 2 sentences of section 2.2 from a robust perspective. Support vectors may be outliers far from high density regions, so using them may not be as robust as fitting a hyperplane for each class (which depends more on the center of mass).\n\n# Meaning of \u201cconservative\u201d?\n\nA more conservative classifier is less likely to classify a datum as positive, in line with density level set estimation. Closing numbers is a property of a class of classifiers that quantify conservativeness, motivated by using these classifiers in neural networks.\n\n[1] Liznersk et al., Explainable Deep One-Class Classification. ICLR 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197276699,
                "cdate": 1700197276699,
                "tmdate": 1700197276699,
                "mdate": 1700197276699,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cs01kXvZNe",
            "forum": "eFVQaqkf8Z",
            "replyto": "eFVQaqkf8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8167/Reviewer_NTiL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8167/Reviewer_NTiL"
            ],
            "content": {
                "summary": {
                    "value": "The authors explore the space of halfspace separator. In this manuscript they explore the equality separator. Instead of dividing the space into two halves, all instances that fall on the hyperplane (or are near to it) are part of one class while the rest belong to the other class. The others then calculate the VC dimension of this equality separator. Furthermore, they also introduce the bump activation function to be used in NNs which is a smoothed version of the equality separator. They propose using this separator for anomaly detection. Finally, they show the efficacy of the proposed method in the experimental section."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed equality separator is very interesting. Even though for epsilon-separator is related to SVMs there is still other novel aspects to this. Furthermore the theoretical analysis shown here for VC dimension shows the advantage of the proposed method over regular linear separators.\n2. The results for anomaly detection is promising specially on the synthetic data set.\n3. The paper is very well written. All required information is provided in a clear manner and explained properly."
                },
                "weaknesses": {
                    "value": "1. As mentioned above, the anomaly detection results in this paper are promising. However, the gain on the NSL-KDD dataset is not always positive. This limits the application of the proposed method.\n2. The authors performed thorough experiments on the NSL-KDD dataset. However, further datasets should also be included in the experimentation to show the efficacy of the proposed method."
                },
                "questions": {
                    "value": "1. This is related to concern regarding weakness 1. What are the authors intuition regarding the equality separator not always outperforming the other baseline methods for NSL-KDD.\n2. I noticed that in Table 3, for DOS, HS-NS result is in bold. Why is that? I though ES-NS performs the best here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798567706,
            "cdate": 1698798567706,
            "tmdate": 1699637012290,
            "mdate": 1699637012290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qsUFHCYTeC",
                "forum": "eFVQaqkf8Z",
                "replyto": "Cs01kXvZNe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question on Performance"
                    },
                    "comment": {
                        "value": "We thank this reviewer for affirming the creativity of equality separation, the theoretical analysis provided and the communication of our ideas. We acknowledge this reviewer\u2019s request for more datasets in our global response (adding MVTec and thyroid dataset results), and thank this reviewer for noting the thorough experimentation on NSL-KDD dataset. We address other concerns below. \n\n# Why don\u2019t equality separators have the best performance for NSL-KDD dataset in all metrics, especially over (unsupervised) baseline methods? Are there intuitions why?\n\nIn short, equality separation is an attempt to merge the benefits of unsupervised methods into supervised methods. We observe a _significant_ improvement over supervised methods for unseen anomalies while maintaining supervised-level performance on seen anomalies.\n\nWe do not claim that equality separation is or should be the _final_ solution to supervised anomaly detection (AD). We do claim that, among methods that use label information via standard empirical risk minimization, equality separation is more suitable, especially to detect unknown anomalies. The experiments support this claim: for instance, in regular binary classification, equality separation _significantly_ improves AUPR on detecting unseen anomalies (privilege and remote access attacks) by 3-fold compared to halfspace separation.\n\nComparing equality separation to unsupervised AD baselines, we especially see that OCSVM beats all other methods in detecting unseen anomalies (privilege and remote access attacks). **However**, unsupervised methods like OCSVM cannot capitalize on labels, hence our comment in footnote 5 that unsupervised methods are not our main apples-to-apples comparison. The inability to use labels impacts detection on seen anomalies (DoS attacks). This disadvantage is in addition to the computational load of computing kernels (e.g. for OCSVM) for large datasets like NSL-KDD. Furthermore, equality separation outperforms unsupervised baselines Isolation Forest and Local Outlier Factor on the unseen remote access attacks, while other regular binary classifiers are far from it. We conclude that equality separation is a step in the right direction to use limited anomaly labels to detect **both seen and unseen** anomalies.\n\nOur intuition on why equality separation may not be beating the OCSVM baseline in detecting unseen anomalies relates mainly to implementation details \u2013 given that equality separation had excellent performance in our synthetic experiments, we posit that higher dimensional data amplify certain difficulties. We observe that optimization may be more difficult because of more sensitivity in initializations and vanishing gradients. We are currently working on understanding how the activation function, loss function and optimization scheme (including initializations) work together to form closed decision regions, which we posit may be the key to further improvements.\n \n# Bold Results for HS-NS in Table 3?\n\nWe consider both the mean and standard deviation when determining which is the best. This is to account for variability in running the optimization that arises from different initializations. To avoid confusion, we account for this consideration in our discussion and appendix instead and remove the double bold text in the table in the main body."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197414658,
                "cdate": 1700197414658,
                "tmdate": 1700197414658,
                "mdate": 1700197414658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WeJiAvtSW1",
                "forum": "eFVQaqkf8Z",
                "replyto": "qsUFHCYTeC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8167/Reviewer_NTiL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8167/Reviewer_NTiL"
                ],
                "content": {
                    "title": {
                        "value": "Response to author rebuttals"
                    },
                    "comment": {
                        "value": "I appreciate the authors for providing the rebuttal. However, after reading the rebuttal and the other reviews, I still think that there are some weaknesses in this manuscript. Thus I have decided not to update my score at this time."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634082782,
                "cdate": 1700634082782,
                "tmdate": 1700634082782,
                "mdate": 1700634082782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tvPqmXS2dK",
            "forum": "eFVQaqkf8Z",
            "replyto": "eFVQaqkf8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8167/Reviewer_FBaF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8167/Reviewer_FBaF"
            ],
            "content": {
                "summary": {
                    "value": "This work discusses a novel approach to linearly classify the XOR problem, challenging the conventional wisdom that it cannot be done with linear methods. The authors propose \"equality separation\" as an alternative to traditional halfspace separation, adapting the SVM objective to distinguish data within or outside the margin. They integrate this classifier into neural network pipelines, highlighting its potential for anomaly detection and demonstrating its effectiveness in supervised anomaly detection experiments, including detecting both seen and unseen anomalies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The introduction of an 'equality separator' to address the XOR problem is indeed an intriguing and innovative concept.\n- The introductory section is well-structured and easily comprehensible, complemented by the informative Figure 1.\n- All the theoretical assertions are substantiated with precise definitions and rigorous proofs."
                },
                "weaknesses": {
                    "value": "- In order to enhance the accessibility and comprehensibility of the content, it would be advisable to incorporate critical discussions and analyses that are currently relegated to the appendix into the main body of the manuscript.\n- The proposed design has exclusively undergone experimentation on toy datasets or relatively straightforward real-world datasets. Consequently, there is uncertainty surrounding the effectiveness of the proposed method when confronted with more intricate, real-world datasets."
                },
                "questions": {
                    "value": "- Is this network design extensible to more intricate datasets, such as image data?\n- Isn't there a gradient vanishing problem with that bump activation design when the layers of the neural network are deep?\n- What are the advantages of doubling the VC dimension in contemporary neural network architecture?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865630391,
            "cdate": 1698865630391,
            "tmdate": 1699637012176,
            "mdate": 1699637012176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZJcu1hP5Bc",
                "forum": "eFVQaqkf8Z",
                "replyto": "tvPqmXS2dK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions on Dataset, Gradients and VC Dimension"
                    },
                    "comment": {
                        "value": "We thank this reviewer for appreciating the innovation of equality separation, effective visualizations and theoretically sound analysis. We address this reviewer\u2019s question on general applicability (to other problem domains like computer vision) in our global response (adding MVTec and thyroid dataset results). We address other questions here.\n\n# There are some useful sections in the appendix that would benefit to be in the main body.\n\nWe thank this reviewer for reading through the details in the appendix. We are willing to move relevant sections of the appendix to the main paper. Which sections does this reviewer think are relevant to move to the main body?\n\n# The proposed method is tested only on straightforward datasets. Can it be generalized to other datasets such as images?\n\nAlthough we use the toy dataset for enhanced visualizations and understanding (both in the main body and appendix), we use NSL-KDD as a real cyber-attack dataset. NSL-KDD is an (improved) edited version of the KDDCUP99 dataset, which was part of USA DARPA\u2019s Intrusion Detection System evaluation program, with \u201cabout 4 gigabytes of compressed raw (binary) tcpdump data of 7 weeks of network traffic, which can be processed into about 5 million connection records, each with about 100 bytes. The two weeks of test data have around 2 million connection records\u201d [1]. Real cyber-attack datasets allow us to test methods against adversaries who are actively trying to circumvent and break the system.\n\nOur method can indeed be generalized to other domains \u2013 there is nothing specific to tabular or cyber-attack data. We have included results for MVTec image defect dataset, showing better AUPR in 5/6 objects tested. We also experiment with the thyroid dataset, which focuses on a medical application. Our method once again shows better performance in detecting seen and unseen anomalies.\n\n# Will gradients vanish in deep networks?\n\nGradients will vanish outside the margin (proportional to $\\frac{1}{||\\mathbf w||_2}$) as plotted in Figure 4 (Appendix E1), similar to vanishing gradients in sigmoid or the dead neuron problem in ReLU. However, like sigmoid, gradients are the greatest in the regions close to the boundary between positive and negative (i.e. regions of aleatoric uncertainty).\n\nThere are 2 sides of this coin. The conventional perspective is that gradients that vanish inhibit learning, and we mention in Section 4 and Appendix F1 that we can mitigate this by initializing $\\sigma$ to be large (e.g. $\\sigma=10$). This decreases the sensitivity in weight initialization. \n\nOn the flip side, there is a self-regularizing effect, where the neuronal activation is very selective. This is indeed what we desire in anomaly detection, where unseen inputs should not activate the network. In Appendix F3 (Figure 8), we show in Figure 8 how the margin width changes across hidden layers (grows across time in earlier layers for more information flow and shrinks in later layers for selectivity of information/features).\n\n# Advantages of doubled VC dimension in contemporary neural network architecture?\n\nAs mentioned in our global response, the VC dimension analysis was not aimed at a neural network analysis. It shows that equality separation is indeed more expressive than halfspace separation (i.e. not that equality separation works for XOR and nothing else).\n\nIn neural networks trained for anomaly detection, we champion for using the equality separation paradigm, which has an inductive bias towards closed decision regions while maintaining the capacity to learn. We think of closing numbers to anomaly detection and forming closed decision regions as VC dimension is to risk minimization \u2013 closing numbers is a dimension-like combinatorial number describing the capacity of a hypothesis class to minimize open space risk. Closing numbers also have a direct interpretation in a neural network setting, as we described in section 3.3. \n\n\n[1] M. Tavallaee, E. Bagheri, W. Lu and A. A. Ghorbani, \"A detailed analysis of the KDD CUP 99 data set,\" 2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications, Ottawa, ON, Canada, 2009, pp. 1-6, doi: 10.1109/CISDA.2009.5356528."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198076247,
                "cdate": 1700198076247,
                "tmdate": 1700198076247,
                "mdate": 1700198076247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]