[
    {
        "title": "Splicing Up Your Predictions with RNA Contrastive Learning"
    },
    {
        "review": {
            "id": "iPcAM0yZf4",
            "forum": "wRkfniZIBl",
            "replyto": "wRkfniZIBl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8080/Reviewer_ZWCi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8080/Reviewer_ZWCi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a contrastive learning approach to learn RNA representations that can be employed in downstream tasks. Based on functional similarity between a) alternatively spliced RNA and b) homologous gene, the authors build positive sample pairs through these two similarities. A contrastive training objective adapted from SimCLR is then used to train an RNA encoder, along with projection modules. Experiments are conducted on 3 downstream tasks: 1) RNA half-life and 2) Mean ribosomal load and Gene ontology prediction by training a layer on top of frozen representation. The authors showed that their approach outperform other pretrained represrentations in these tasks and is especially effective in the low data regime."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+Strong empirical performance of approach versus baselines\n+The paper is easy to read and follow\n+Potential application of approache in biomedicine"
                },
                "weaknesses": {
                    "value": "The key novelty of the paper seems to hinge on using functional similarity between a) alternatively spliced RNA and b) homologous genes to create positive sample pairs for contrastive learning. There is little innovation in the machine learning aspect (i.e. novel algorithm or significant change from SimCLR), which might make this more suitable for a biomedicine-focused audience rather than the general machine learning community."
                },
                "questions": {
                    "value": "What is the key difference between this approach and existing contrastive objective such as SimCLR apart from exploiting the functional similarity between a) alternatively spliced RNA and b) homologous genes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808051314,
            "cdate": 1698808051314,
            "tmdate": 1699637000730,
            "mdate": 1699637000730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tLToYpHW8r",
                "forum": "wRkfniZIBl",
                "replyto": "iPcAM0yZf4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for commending the clarity of our paper and relative improvement over baseline methods. We have summarized our thoughts about novelty in the top-level comment and highlighted the information weighting of samples proportional to their informativeness in the absence of a view-generating function. We respectfully disagree that this is not of interest to the ICLR community given the growing interest in ML applications for biological sciences but appreciate the suggestion and can aim to describe our approach in more general terms that can be relatable to researchers in related domains."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668179283,
                "cdate": 1700668179283,
                "tmdate": 1700668179283,
                "mdate": 1700668179283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e5Vhqmgpyt",
            "forum": "wRkfniZIBl",
            "replyto": "wRkfniZIBl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8080/Reviewer_qo8f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8080/Reviewer_qo8f"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a contrastive learning-based method for an RNA pre-trained model. It proposes to utilize functional similarities between sequences generated through alternative splicing and gene duplication as positive samples for contrastive learning and train models on it. It performs linear probing on 3 datasets to show that the proposed method performs well against baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of utilizing functional similarities between sequences generated through alternative splicing and gene duplication as positive samples for contrastive learning is sound.\n- The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The empirical analysis is not convincing.\n    - The number of datasets is limited. Only three datasets are used, among which the performance on Gene ontology is only partially reported in the Appendix.\n    - What's the point of learning good RNA representations if all your downstream tasks can be solved with standard fine-tuning? In NLP, better sentence embedding can directly be used to amplify retrieval-based QA systems or retrieval-based text generation. But in your application, I do not see the better embeddings helping your downstream applications.\n    - If I understand correctly, the proposed method works comparably with `Saluki` in full fine-tuning while performing worse in linear probing than it. It also has more parameters than it. What is the benefit of your model over `Saluki`?\n    - How do you use the pre-trained DNA models to solve the RNA prediction tasks? Do you replace all the `U` with `T` and feed them to the DNA pre-trained models? If this is the case, the comparison between the contrastive training target and MLM/NTP is unfair since the models are trained on different corpus.\n    - If I understand correctly, the proposed model and `Saluki` utilize more information than the sequence itself. If this is true, then the comparison with the baselines is unfair.\n    - Why do you present the results on linear probing with models like RNA-FM, NT, and DNA-BERT2 while skipping them in the model fine-tuning? If they can generate embeddings for linear probing on the datasets, you should be able to fine-tune them on the tasks, too. The models, except for NT-2.5b, are not too large to be fine-tuned on consumer GPUs."
                },
                "questions": {
                    "value": "Please see the Weaknesses section for my questions. Thanks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8080/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8080/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8080/Reviewer_qo8f"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815387642,
            "cdate": 1698815387642,
            "tmdate": 1700682429997,
            "mdate": 1700682429997,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tZbsBxpDRD",
                "forum": "wRkfniZIBl",
                "replyto": "e5Vhqmgpyt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for engaging in a thorough evaluation of our paper.\n\n1. > The number of datasets is limited\n\nWe aim to improve on this in a further iteration, however, we believe the substantial improvement observed in linear probing results over previous methods has merit.\n\n2. > What's the point of learning good RNA representations if all your downstream tasks can be solved with standard fine-tuning?\n\nFirst, we\u2019d like to point the reviewer to Figure 3b in which we demonstrate IsoCLR\u2019s significant improvements in low data availability prediction setting over supervised methods. In biological sciences lower throughput experiments are more common compared to other domains such as language and vision underscoring the utility of this result.\n\nIn addition, we demonstrate IsoCLR\u2019s improvement over supervised models with matched architectures in Table 3. These results demonstrate the general utility of our methods highlighting the potential for improvement across a diversity of architectures. \n\n3. > What is the benefit of your model over Saluki?\n\nOur linear probing results in Table 2 / Figure 1 compare IsoCLR\u2019s linear probing results to full model fine-tuning. For linear probing we do not have to train the full model, optimizing only the last layer to achieve 85% of the Saluki full fine-tuning performance.\n\nThis is interpreted as a successful finding in transfer-learning as the original SimCLR paper also does not outperform the fully supervised ResNet in linear probing regime (they match the performance of the 4x smaller model when restricting training to 90 epochs vs 1000 epochs https://arxiv.org/pdf/2002.05709.pdf figure 7). \n\nIn addition, we\u2019d like to point out that we perform significantly better than the existing self-supervised methods.\n\n4. > Pre-trained DNA models \u2026 are trained on different corpus\n\nIndeed, for DNA pre-trained models we replace U->T to create a shared vocabulary between RNA and DNA models. While that in itself does not alter the training distribution (since no information is lost), the DNA models are trained on a superset of our corpus. The distribution shift occurs due to a lack of introns sequences separating exonic regions. We can further clarify this practice in a future revision, underlying the need for RNA-specific models.\n\nWe do compare against RNA-FM and find that we outperform the existing RNA self-supervised method.\n\nThis observation indicating the difficulty of utilizing DNA models for RNA property prediction tasks underlines the importance of training an RNA-specific self-supervised model.\n\n5. > The proposed model and Saluki utilize more information than the sequence itself\n\nMost of the methods we compare against are designed for DNA tasks and so do not possess this RNA-relevant information. We did consider this and wanted to understand whether our performance stems from additional information or the pre-training procedure so we conducted an ablation study. We confirm that our smallest model outperforms all the other self-supervised learning methods on both RNA half-life tasks and Gene ontology and can be found under Splice + Homology + Mask row in Table 4. We outperform the other models on the mean ribosome load task as well which we can add to the appendix.\n\n6. > Why do you present the results on linear probing .. while skipping them in the model fine-tuning\n\nThe existing models have a truncated context window relative to the length of RNA sequences. Most methods have 512-1024 context window length whereas to capture 95% of RNA sequences our context length is 12,000 nucleotides in length. During the evaluation of linear probing, we perform a concatenation of average pooling and max pooling to address this issue. It is unclear how to perform this in full-model fine-tuning as the existing context window is between 5-10% of the overall sequence length. We did not perform this experiment since we expected poor results given our margin of performance relative to HyenaDNA. We\u2019ve noted the challenges associated with short context length models in section 4.2 but we will aim to further clarify this in text.\n\nThank you for engaging with us in the review process, and we would appreciate knowing if our responses clarified your questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668140756,
                "cdate": 1700668140756,
                "tmdate": 1700668140756,
                "mdate": 1700668140756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9pwYvYmyax",
                "forum": "wRkfniZIBl",
                "replyto": "tZbsBxpDRD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8080/Reviewer_qo8f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8080/Reviewer_qo8f"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response. It solves some of my concerns so I raise my score from 3 to 5. Though this manuscript is not ready for publish for now IMO, I think it\u2019s on a decent direction and will be an impactful work after more interactions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682408328,
                "cdate": 1700682408328,
                "tmdate": 1700682408328,
                "mdate": 1700682408328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cnajGy5jkR",
            "forum": "wRkfniZIBl",
            "replyto": "wRkfniZIBl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8080/Reviewer_rpSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8080/Reviewer_rpSM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes IsoCLR to learn RNA representations by contrastive learning. Splicing as the augmentation is identified as the key for the success."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to follow.\n- The method is simple, yet effective in the studied tasks."
                },
                "weaknesses": {
                    "value": "- The work has limited novelty and contributions. It simply applies the contrastive objective to the RNA sequence learning, with the major contribution as identifying an effective augmentation method.\n- The work narrows down to learn representations of RNA sequences that mainly can be used for property prediction, making the work less interest and has less impact.\n- For example, RNA-FM (which was compared to isoCLR in experiments) demonstrate its effectiveness in structural-related prediction (secondary structure predictions, RNA contact predictions, 3D distances, etc.), which is a more crucial aspect in the related field. The current submission is mostly focused on relatively easier tasks, which is not sufficient to verify the effectiveness of learned representations."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8080/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8080/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8080/Reviewer_rpSM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835332683,
            "cdate": 1698835332683,
            "tmdate": 1699637000425,
            "mdate": 1699637000425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bG4tIP8srx",
                "forum": "wRkfniZIBl",
                "replyto": "cnajGy5jkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for a detail-oriented review. Given the overall reviewer sentiment, we have opted not to run experiments for RNA structure prediction for this submission. We agree that including structural prediction tasks will make our evaluations more comprehensive."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667864366,
                "cdate": 1700667864366,
                "tmdate": 1700667864366,
                "mdate": 1700667864366,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]