[
    {
        "title": "MuSc : Zero-Shot Anomaly Classification and Segmentation by Mutual Scoring of the Unlabeled Images"
    },
    {
        "review": {
            "id": "y0kysomv1q",
            "forum": "AHgc5SMdtd",
            "replyto": "AHgc5SMdtd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission19/Reviewer_H5pV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission19/Reviewer_H5pV"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets industrial zero-shot anomaly detection. Leveraging mutual scoring on unlabeled data, the proposed method achieves SOTA performance on several well-known industrial anomaly detection benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper is well written, with clear architecture.\n2.\tThe motivation is insightful, with clear ablation studies to show its effectiveness. The motivation and the proposed method are well-matched."
                },
                "weaknesses": {
                    "value": "1.\tThis paper employs a methodology that utilizes unlabeled test images to collectively measure anomaly scores, differing somewhat from the traditional zero-shot recognition setting. In the conventional approach, each image is independently evaluated, such as WinCLIP. This difference in evaluation methodologies can result in a somewhat unfair comparison.\n2.\tThe underlying concept is reminiscent of [a], which presumes homogeneous input texture and identifies image regions that disrupt this homogeneity as anomalies. In other words, if a patch significantly differs from its neighboring areas, it's deemed an anomaly.\n3.\tThe approach of jointly measuring anomaly scores across entire datasets aligns closely with [b]. While not mandatory, it would be beneficial for the authors to discuss or draw comparisons with [b].\n\n[a] Aota et al. \"Zero-shot versus Many-shot: Unsupervised Texture Anomaly Detection.\" In WACV 2023.\n\n[b] Li et al. \"Zero-Shot Batch-Level Anomaly Detection.\" arXiv, February 2023."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission19/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648612162,
            "cdate": 1698648612162,
            "tmdate": 1699635925396,
            "mdate": 1699635925396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QghMxPoMnJ",
                "forum": "AHgc5SMdtd",
                "replyto": "y0kysomv1q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the insightful comments.\n\n**Q1: This paper employs a methodology that utilizes unlabeled test images to collectively measure anomaly scores, differing somewhat from the traditional zero-shot recognition setting. This difference in evaluation methodologies can result in a somewhat unfair comparison.**\n\nA1: We agree that our approach is different from several existing approaches, e.g., winCLIP, in using the unlabeled test images, which may lead to a potentially unfair comparison. Therefore we have added the comparison to ACR [b]\\[c] in Tab.1.  Similar to our approach, ACR assumes access to a set of unlabeled test images together during inference, the experimental results demonstrate that MuSc outperforms ACR by a large margin. We hope such a comparison is more fair.\n\n**Q2: The underlying concept is reminiscent of [a], which presumes homogeneous input texture and identifies image regions that disrupt this homogeneity as anomalies.**\n\nA2: Thanks for your valuable suggestion! We have carefully studied [a] and added the discussion in the Related Work section. [a] is based on the assumption that the input texture is homogeneous, and the image regions that break the homogeneity are detected as anomalies. [a] explores the relationship between the patches inside one test texture image, while our method leverages the relationship in a set of unlabeled images.\n\n**Q3: The approach of jointly measuring anomaly scores across entire datasets aligns closely with [b]. While not mandatory, it would be beneficial for the authors to discuss or draw comparisons with [b].**\n\nA3: Thanks for your valuable suggestion! We have carefully studied ACR [b]\\[c] and added the comparison with it in Tab. 1 and the discussion in the Related Works section. We found that ACR is an excellent zero-shot AD approach, we agree that our approach aligns closely with ACR, since both of us jointly measure the anomaly scores across the entire dataset or a set of unlabeled test images, while we use the unlabeled images differently, another difference is that ACR contains a training process, while our approach does not need any training or prompt.\n\nWe give the classification and segmentation performance of our method and [b]\\[c] on the MVTec AD dataset in the following table. \n\n|   Method    | AUROC-cls | AUROC-segm | PRO-segm |\n| :---------: | :-------: | :--------: | :------: |\n|   ACR [b]   |   85.8    |    92.5    |   72.7   |\n| MuSc (ours) |   97.8    |    97.3    |   93.8   |\n\n> [a] Aota et al. \"Zero-shot versus Many-shot: Unsupervised Texture Anomaly Detection.\" In WACV 2023.\n>\n> [b] Li et al. \"Zero-Shot Batch-Level Anomaly Detection.\" arXiv, February 2023.\n>\n> [c] Li et al. \"Zero-Shot Anomaly Detection via Batch Normalization.\" In NeurIPS 2023.\n>"
                    },
                    "title": {
                        "value": "Response to Reviewer H5pV"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission19/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684172520,
                "cdate": 1700684172520,
                "tmdate": 1700724811706,
                "mdate": 1700724811706,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Uk9PdZmZYz",
            "forum": "AHgc5SMdtd",
            "replyto": "AHgc5SMdtd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission19/Reviewer_dCfU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission19/Reviewer_dCfU"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses zero-shot anomaly classification (AC) and segmentation (AS) with following contributions:\n1) Using unlabeled test images for AC/AS.\n2) A new mutual scoring mechanism for identification of abnormal patches.\n3) SOTA performance, significantly outperforming existing zero-shot methods"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The claimed contributions summarized above."
                },
                "weaknesses": {
                    "value": "1) This paper makes the assumption that access to the entire test dataset is available. This allows for a direct application of the proposed mutual scoring mechanism. However, for zero-shot settings, such an assumption is too restrictive.  Access to test data is mostly very limited in real-world scenarios, especially for zero-shot.\n\n2) The method heavily relies on many heuristic design choices. It consists of a three-step pipeline which depends on many hyperparameters for feature representation, mutual scoring estimation, and classification rescoring. The paper poorly presents sensitivity of performance to optimizing all these parameters.\n\n3) Overall technical novelty seems incremental, since the method incorporates well-established multiscale features (Sec 3.1), norm-2 distance (Sec 3.2), and clustering (Sec 3.3). \n\n4) Since the method lacks a training phase and directly generates results based on estimating the test-set statistics, inference is likely to be much slower than in previous approaches. I was not able to find a report/comparison of the inference times."
                },
                "questions": {
                    "value": "1) Could your approach handle a setting when a test dataset lacks ground truth? What is performance when a test dataset lacks ground truth for optimizing the proposed heuristic procedure? \n\n2) One of the claimed contributions, the mutual scoring mechanism, appears to depend significantly on the relative positions of feature patches. This limits the application of this approach in real settings where orientations and scales are not consistent. What is performance when test images exhibit inconsistent orientations or scales?\n\n3) Could your method be extended to (or readily address) the few-shot setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission19/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680197799,
            "cdate": 1698680197799,
            "tmdate": 1699635925316,
            "mdate": 1699635925316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uvqAKstcVu",
                "forum": "AHgc5SMdtd",
                "replyto": "Uk9PdZmZYz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dCfU (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and thoughtful questions.\n\n**Q1: This paper makes the assumption that access to the entire test dataset is available. For zero-shot settings, such an assumption is too restrictive. Access to test data is mostly very limited in real-world scenarios, especially for zero-shot.**\n\nA1: Thanks for your comment! As mentioned by Reviewer H5pV, our approach aligns closely with ACR [a]\\[b], an excellent zero-shot AD approach published in NeurIPS2023, both of us jointly measure the anomaly scores across either the entire dataset or a sub-set of unlabeled test images. Therefore, we hope the comparison with ACR makes our approach more fair. \n\n**Q2: The method heavily relies on many heuristic design choices. It consists of a three-step pipeline which depends on many hyperparameters. The paper poorly presents the sensitivity of performance to optimizing all these parameters.**\n\nA2: Our method contains three steps, and each step has only one hyperparameter, i.e., the aggregation degree $r$ in LNAMD, Interval Average (IA) on the minimum $X\\%$, and window mask size in RsCIN. \n\nThe setting of aggregation degree $r$ indeed impacts\u00a0the result as it is\u00a0related to the size of anomalies. As shown in Tab. 3,\u00a0in multiple settings, the model's classification AUROC on the MVTec AD dataset is reduced by up to 3.6%, and segmentation AUROC is reduced by up to 3%.\u00a0On the VisA dataset, the use of $r$={5} results in a 10.4% reduction in classification results compared to $r$={1,3,5}, due to more small defects on VisA, making it unsuitable to use only the large aggregation degree.\u00a0For better practicality, we use the same setting {1,3,5} in all the datasets to obtain a good segmentation of anomalies\u00a0with various sizes. \n\nThe $X\\%$ in Interval Average is an insensitive hyperparameter. As shown in Fig. 7, we\u00a0compare the models using different percentage ranges of the Interval Average in the MSM module. The change of this parameter has a maximum effect of 3% on the image-AUROC and 1% on the pixel-AUROC.\n\nThe window mask size in RsCIN is also an insensitive hyperparameter, which is proved by the experiments in Fig. 8. Observably, on the MVTec AD data set, F1-max-score fluctuates between 97.1\u00a0and 97.5, with the amplitude not exceeding 0.4. On the VisA data set, F1-max-score floats between 87.8 and 89.5, and the amplitude does not exceed 1.7.\u00a0Furthermore, we also verify the sensitivity of the RsCIN module to different methods, as shown in Tab.\u00a011 of the appendix. All methods maintain the same parameter settings of the RsCIN module as ours. Almost all methods achieve performance improvements after applying the RsCIN module, except DRAEM only. This experiment proves that the window mask size in RsCIN is insensitive for different methods.\n\nIn summary, the aggregation degree $r$ is a sensitive parameter, while the other two hyperparameters are insensitive.\n\nIn addition, we show the performance of our method on the BTAD dataset, and the parameter settings are consistent with those in our experiments. Specific settings include aggregation degree $r$ as {1, 3, 5}, \u00a0$X$ as 30, and window mask size as {2, 3}. The following table shows that the classification and segmentation performance of our method on the BTAD dataset still exceeds that of zero/few-shot methods and even some full-shot methods. Such an experimental result shows that although the performance of MuSc is sensitive to the setting of aggregation degree $r$, an appropriate choice of $r$ can achieve good results in different datasets simultaneously, which proves its universality.\n\n|   Method    |  Setting  | Image-AUROC | Pixel-AUROC |\n| :---------: | :-------: | :---------: | :---------: |\n|   VT-ADL    | full-shot |    83.7     |    90.0     |\n|   P-SVDD    | full-shot |    83.3     |    92.1     |\n|    SPADE    | full-shot |    87.6     |    96.9     |\n|    PaDiM    | full-shot |    93.7     |    97.3     |\n| PyramidFlow | full-shot |    95.8     |    97.7     |\n|  PatchCore  |  4-shot   |    91.4     |    96.3     |\n|    RegAD    |  4-shot   |    91.0     |    97.5     |\n|  APRIL-GAN  |  4-shot   |    91.3     |    92.4     |\n|  APRIL-GAN  |  0-shot   |    72.3     |    89.6     |\n| MuSc(ours)  |  0-shot   |    94.8     |    97.3     |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission19/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685712267,
                "cdate": 1700685712267,
                "tmdate": 1700685712267,
                "mdate": 1700685712267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qpYwDOXK6H",
                "forum": "AHgc5SMdtd",
                "replyto": "Uk9PdZmZYz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dCfU (Part 3)"
                    },
                    "comment": {
                        "value": "**Q5: Could your approach handle a setting when a test dataset lacks ground truth? What is performance when a test dataset lacks ground truth for optimizing the proposed heuristic procedure?**\n\nA5: Thanks for your comment. As we mentioned in the response to Q2 of reviewer\u00a0dCfU, our approach contains three hyperparameters. Our performance is sensitive to the aggregation degree $r$ but insensitive to the other two hyperparameters. In addition, the experimental results on the new BTAD dataset demonstrate that the fixed hyperparameter setting could achieve good experimental results in different datasets. Therefore, we do not need to use the ground truth to optimize our approach. It is\u00a0clarified that all the steps of our approach do not use the ground truth\u00a0actually.\n\n**Q6: One of the claimed contributions, the mutual scoring mechanism, appears to depend significantly on the relative positions of feature patches. This limits the application of this approach in real settings where orientations and scales are not consistent. What is performance when test images exhibit inconsistent orientations or scales?**\n\nA6: Thanks for your comment! In the mutual scoring mechanism, we leverage each image in {$D_{test}$ \\\\ $I_i$} to assign an anomaly score for each aggregated patch token of a test image $I_i$ in stage one as follows\uff0c\n\n$$\na_{i,l}^{m,r}(I_j) = \\\\min_{n} \\\\Vert \\\\hat{p}_{i,l}^{m,r}-\\\\hat{p}^{n,r}\\_{j,l} \\\\Vert_2\n$$\n\nwhere $m$ and $n$ are the index of the image patches, $r$ is the aggregation degree, $i$ and $j$ are the index of the unlabeled images, and $l$ indicates the stage $l$ of ViT. We clarify that $m$ is irrelevant to the position of $n$,  since Eq.1 first computes the distance between the  $m$-th patch (in image $I_i$) and all the patches in image $I_j$, and then performs min operation to select the most similar image patch from image  $I_j$. Accordingly, the anomaly score only depends on the aggregated patch token and is irrelevant to the relative positions of feature patches. \n\nIn addition, according to your insightful comment, we select the categories with inconsistent orientations or scales from MVTec AD and VisA datasets for the additional experimental evaluation, which is given in Tab. 12 in the paper and also given in the following table. We compare our method with WinCLIP and APRIL-GAN and show the image-AUROC (AC)/pixel-AUROC (AS) in this table. We observe that our approach and WinCLIP are both\u00a0influenced by inconsistent orientations or scales. The reason is that both of us use the fixed pre-trained vision transformer as the feature extractor. As introduced in [c], during the pre-training process, the vision transformer only performs minor data augmentation on orientations and scales, and hence both of us cannot well address the inconsistent orientations or scales. APRIL-GAN also achieves a decreased AC score, while it obtains a better AS score. We guess that\u00a0it is due to\u00a0its additional training set for optimizing\u00a0AS. In addition, we should emphasize that our approach still outperforms WinCLIP and APRIL-GAN in inconsistent orientations or scale cases.\n\nIn our future works, we plan to\u00a0design rotation and scale invariance features to alleviate such a limitation.\n\n| Category  |    WinCLIP    |   APRIL-GAN   |  MuSc(ours)   |\n| :-------: | :-----------: | :-----------: | :-----------: |\n|           |     AC/AS     |     AC/AS     |     AC/AS     |\n|   screw   |   83.3/89.6   |   84.9/97.8   |   83.5/98.9   |\n| hazelnut  |   93.9/94.3   |   89.6/96.1   |   99.6/99.4   |\n| metal_nut |   97.1/61.0   |   68.4/65.4   |   96.3/86.0   |\n| capsules  |   85.0/81.6   |   61.2/97.5   |   88.8/98.8   |\n| macaroni2 |   63.7/59.3   |   64.6/97.8   |   69.9/97.2   |\n| **mean**  | **84.6/77.2** | **73.7/90.9** | **87.6/96.1** |\n| mean-ALL  |   91.8/85.1   |   86.1/87.6   |   97.8/97.3   |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission19/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687897690,
                "cdate": 1700687897690,
                "tmdate": 1700688095708,
                "mdate": 1700688095708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nd4rOlaFoy",
            "forum": "AHgc5SMdtd",
            "replyto": "AHgc5SMdtd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission19/Reviewer_idjR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission19/Reviewer_idjR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces MuSc, a novel zero-shot framework for industrial anomaly classification and segmentation. It utilizes cues from unlabeled test images and combines local patch tokens with a mutual scoring mechanism. The method notably outperforms existing zero-shot approaches and rivals many few-shot and full-shot methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The MuSc framework introduces a novel approach to zero-shot anomaly classification and segmentation, particularly in the industrial domain. The method of leveraging implicit cues from unlabeled test images for anomaly detection is an innovative concept.\n\n2. The empirical results demonstrate a substantial improvement over existing zero-shot approaches and competitiveness with few-shot and full-shot methods.\n\n3. The approach holds significant potential for industrial applications, where anomaly detection is crucial but training data is often scarce or expensive to obtain."
                },
                "weaknesses": {
                    "value": "In Table 3, the ablation study of LNAMD with different aggregation degrees \\(r\\) raises a question regarding the effectiveness of combining aggregation degrees. Specifically, it's unclear why the combination of \\({3, 5}\\) performs worse in the anomaly classification (AC) task than using \\({3}\\) alone. This observation seems to contradict the paper's claim that using all aggregated patch tokens with different degrees is beneficial for detecting anomalies of various sizes. Based on this claim, one would expect the combination of \\({3, 5}\\) to outperform either \\({3}\\) or \\({5}\\) individually. This inconsistency warrants further clarification or investigation to reconcile the results with the stated claims."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission19/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805736672,
            "cdate": 1698805736672,
            "tmdate": 1699635925241,
            "mdate": 1699635925241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tg5JFxyUHQ",
                "forum": "AHgc5SMdtd",
                "replyto": "nd4rOlaFoy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer idjR"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments.\n\n**Q1: It's unclear why the combination of ({3, 5}) performs worse in the anomaly classification (AC) task than using ({3}) alone.**\n\nA1: Thanks for your comment. We have explained this phenomenon deeply in the revised Appendix A.2.2. As we written\u00a0in Section 3.1, aggregation degree ({5}) is suitable for detecting large defects, but it in turn smoothes small defects to a lower score.\u00a0We show a classic example in Fig. 9 of the revised manuscript. When directly summing the anomaly scores of the two aggregation degrees ({3,5}), the score within the small anomaly region is pulled down by the low score generated by the aggregation degree ({5}), which results in false negative. Consequently, the combination of ({3,5}) performs worse in the anomaly classification (AC) task than using ({3}) alone.\n\nDue to the above reasons, we use the aggregation degrees ({1,3,5}) to take into account various-size anomalies in real industrial scenarios. As displayed in Fig 9, when we add a large anomaly score with aggregation degree ({1}), the anomaly score within the black rectangle in (e) increases and false negatives reduce."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission19/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685617545,
                "cdate": 1700685617545,
                "tmdate": 1700685617545,
                "mdate": 1700685617545,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yinO0jnVvj",
            "forum": "AHgc5SMdtd",
            "replyto": "AHgc5SMdtd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission19/Reviewer_52S5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission19/Reviewer_52S5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an anomaly detection and segmentation method utilizing unlabeled images from a test set. It is assumed that a set of images with some form of anomaly is available for inference and it is claimed to be zero shot. \nThe method starts with computing a representation for every image patch through aggregating (pooling) token features generated by ViT at different layers. An anomaly score for each patch is then computed by comparing the aggregated patch token with that from each image in the *test* set. Further heuristic tricks are applied to refine this score and the final pixel level anomaly score is given by the max of the refined score. \nThe method then produces an image level classification by defining a weighted graph over *test* set images where weights are defined by the class token generated by ViT. The image level classification score is the determined by by some graph operations that were not explained well. The method was tested on public datasets and compared with solid baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I am afraid I was not able to find a notable strength of this paper."
                },
                "weaknesses": {
                    "value": "1. Technical soundness: The method relies heavily on availability of a set of images in order to compute the anomaly scores. I dont think this emulates the practical scenario of anomaly detection in industry. The more realistic scenario is a method is required to classify and segment the anomaly given one image. Using test set images to produce the output on the same set of images also does not conform with the scientific procedure. I dont think this is the definition of a zero shot method either. The setting sounds completely unreasonable to me.\n\n2. Contribution: The algorithm appears to be a set of heuristic tricks applied in a sequence. There is not a solid technique that is novel, elegant, theoretically justified and of broad interest.\n\n3. Clarity: None of the techniques were adequately explained as to why they are being performed and why it makes sense (intuitively or conceptually) to apply them. Just stating the process does not qualify as a good scientific exposition."
                },
                "questions": {
                    "value": "..."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission19/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809673236,
            "cdate": 1698809673236,
            "tmdate": 1699635925147,
            "mdate": 1699635925147,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xgptmKzpYd",
                "forum": "AHgc5SMdtd",
                "replyto": "yinO0jnVvj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 52S5 (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your comments, we want to first give the following clarifications.\n\n> \u201cIt is assumed that a set of images with some form of anomaly is available for inference\u201d.\n>\n\nHere, it needs to be clarified that our approach leverages a set of unlabeled images for anomaly AC/AS but does not assume that the anomaly has the same form.\n\n**Q1: The method relies heavily on availability of a set of images in order to compute the anomaly scores. I dont think this emulates the practical scenario of anomaly detection in industry.**\n\nA1: Thanks for your comment. Since we have long-term collaboration with several well-known companies, our approach is designed according to the practical industry requirements.  Let us take a famous anonymous company as an example. In general, a production line (AMOLED/LCD) has a daily output of 1500 pieces. Typically, a production line is equipped with\u00a0at least 3 parallel quality inspection equipment, and each furnished with\u00a0at least 2 cameras. The image resolution of each camera exceeds 4000\u00d75000, and a typical solution is to crop a series of 512\u00d7512 images from high-resolution images by sliding windows with 10% overlap.\u00a0The above production line needs to inspect nearly **a million product images per day**. As far as we know, such a production quality inspection requirement exists in a large number of companies. This example illustrates that unlabeled images are abundant in real industrial scenarios and easy to collect.\n\nIn addition, it needs to be clarified that\u00a0MuSc did not rely heavily on a large number of unlabeled images, which can be demonstrated by Tab. 7 of the paper. MuSc obtains decent AUROCs under the test sets of different amounts, even when the test set only has 14-56 images. Referring to the example above, we are confident that such a number of images can be easily achieved in practical scenarios. \n\n**Q2\uff1aThe more realistic scenario is a method is required to classify and segment the anomaly given one image. Using test set images to produce the output on the same set of images also does not conform with the scientific procedure. I dont think this is the definition of a zero shot method either.**\n\nA2: Thanks for your comment. We agree that it is a widely-used practice to classify and segment anomalies in one image. However, this does not negate the scientific validity of using a set of test images to produce the output on the same set of images. As mentioned by Reviewer H5pV, our approach aligns closely with ACR [b]\\[c], an excellent zero-shot AD approach published in NeurIPS2023. Both of us jointly measure the anomaly scores across either the entire test dataset or a sub-set of unlabeled test images. Therefore, we hope the comparison with ACR makes our approach more fair and dispels your doubts.\n\n**Q3: The algorithm appears to be a set of heuristic tricks applied in a sequence. There is not a solid technique that is novel, elegant, theoretically justified and of broad interest.**\n\nA3: Thanks for your comment! We clarify our technique novel as follows.\n\nFirstly, we find that the patch-level features extracted by ViT have the limitation of detecting industrial anomalies of varying sizes. Swin transformer has large patches in some stages leading to the patch-level features extracted by it not working well. The proposed LNAMD module addresses this issue in ViT, as far as we know, such a design is a new technique to optimize the patch-level features of ViT in industrial vision.\n\nSecondly, in MSM, the mutual scoring operation in Eq. (1) is inspired by the observation of normal and abnormal patches. It uses the minimum similarity between the patch $m$ of image $I_i$ and all the patches of image $I_j$ to score the patch $m$ of image $I_i$. This is a new technique to score the patch by unlabeled images. The interval average operation in Eq. (2) is based on our observation of the false positives, so we designed this operation to reduce the false positives. To the best of our knowledge, the Mutual Scoring Mechanism is a new mechanism for the zero-shot AC/AS community.\n\nThirdly, in RsCIN, we observe that due to the high smoothness of $\\textbf{C}$, the traditional manifold learning approach may be influenced by a large number of images. Therefore, we propose the Multi-window Mask Operation (MMO) to constrain the image number, which makes each image only influenced by a small number of neighborhood images. RsCIN was first proposed in the field of anomaly classification and segmentation. Meanwhile, we experimentally proved that RsCIN can be further employed to improve the AC accuracy of existing approaches in Appendix A.2.4."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission19/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685468655,
                "cdate": 1700685468655,
                "tmdate": 1700701367779,
                "mdate": 1700701367779,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p5amrLZXIi",
            "forum": "AHgc5SMdtd",
            "replyto": "AHgc5SMdtd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission19/Reviewer_NLA8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission19/Reviewer_NLA8"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a MuSc model for zero-shot AC/AS. The method exploits the normal and abnormal information implicit in unlabeled test images without any training or prompts, A Mutual Scoring Mechanism (MSM) is proposed to assign abnormal scores to each other using unlabeled test images, and an optimization method based on constrained image-level neighborhoods (RsCIN) for image-level anomaly classification. The method shows better performance on MVTec AD and VisA datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea that normal image patches can be found in a relatively large number of similar patches in other unlabeled images, while abnormal image patches have only a small number of similar patches, is novel. The authors were able to accomplish this task simply by using a test dataset and utilizing the test images to score each other without any training or prompting.\n\n2. The authors considered the size of the anomalies in different datasets and used patch tokens with multiple aggregation degrees to obtain high-quality anomaly scores even when using a simple distance measure.\n\n3. The authors found that the image-level features satisfy the conditions of high-dimensional manifolds, and designed RsCIN based on manifold learning to optimize the pixel-level anomaly classification results, and experimentally verified the effectiveness of the module, proving that the proposed RsCIN module can further improve the performance of the existing methods."
                },
                "weaknesses": {
                    "value": "Even without training or prompts, the method requires long inference times and high memory costs. While the authors provide a solution to increase speed and reduce memory by dividing the test set into subsets, this also reduces performance by a small margin. Are there any other approaches that could have been considered to solve the problem?\n\nThe authors should provide more detailed theories for MSM and RsCIN."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission19/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698901355281,
            "cdate": 1698901355281,
            "tmdate": 1699635925074,
            "mdate": 1699635925074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AHEB7UMUiz",
                "forum": "AHgc5SMdtd",
                "replyto": "p5amrLZXIi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission19/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NLA8 (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments.\n\n**Q1: Even without training or prompts, the method requires long inference times and high memory costs. While the authors provide a solution to increase speed and reduce memory by dividing the test set into subsets, this also reduces performance by a small margin. Are there any other approaches that could have been considered to solve the problem?**\n\nA1:  Driven by your suggestion, we have examed the MuSc's code implementation thoroughly and found two issues that slowed down the model and increased the number of parameters. For now, both issues have been solved. After the paper is accepted, we will release the source code to prove that both issues are indeed addressed and the parameter amount and speed are optimized. Below, we describe these two issues and the corresponding countermeasures respectively.\n\n**First issue: A time-consuming operation in LNAMD.** In the original implementation of the LNAMD, we have used the \"for\" loop operation to process a tensor with dimensions [1369, 1024], i.e.,  \"features\", resulting in a lot of time-consuming.\n\n`return [x.detach().cpu().numpy() for x in features]`\n\nConsequently, this inefficient implementation costs 95.2ms under a specific aggregation degree r, consuming more than 70% of the time in LNAMD which takes 129.9ms only. We have\u00a0addressed\u00a0this issue by\u00a0the following code:\n\n`return features.detach().cpu().numpy()`\n\n**Second issue: Additional GPU memory usage in LNAMD.**\u00a0The original implementation of the LNAMD module unexpectedly loaded the backbone network an extra time, which was neglected when we modified the code structure. For now, we have\u00a0removed the extra\u00a0process of loading the backbone, saving 1186MB and 620MB of memory respectively when using ViT-L-14-336 and ViT-B-16-plus-240.\n\nAfter modifying the code of LNAMD, we have conducted\u00a0a **comprehensive efficiency comparison**, which is listed\u00a0in the following table. Since there is no official source code of WinCLIP, its GPU cost is unknown (marked by \u201c-\u201d). In addition, the author of APRIL-GAN does not provide the pre-trained model using ViT-B-16-plus-240 (our backbone), so its AUROC and AUPRO\u00a0are unknown (marked by \u201c-\u201d).\n\nIn terms of inference time, when using the same backbone as WinCLIP and setting $s$ to 3, our inference time is 238.3ms per image, which is 150.7ms shorter than WinCLIP, and 71.1ms shorter than RegAD. Additionally, APRIL-GAN has the shortest inference time (64.7ms) among these methods. In terms of memory cost, when using the same backbone as WinCLIP, our GPU memory cost is 3002MB, significantly lower than RegAD (8920MB) and 224MB less than APRIL-GAN (3226MB). The detailed comparisons of inference time and memory cost have been added to Appendix A.4.\n\n|      Method       |     Backbone      | Training | Inference time (ms) | GPU (MB) | Image-AUROC | AUPRO |\n| :---------------: | :---------------: | :------: | :-----------------: | :------: | :---------: | :---: |\n|   RegAD(4-shot)   |     ResNet18      |   yes    |        309.4        |   8920   |    89.1     | 88.0  |\n| APRIL-GAN(0-shot) |   ViT-L-14-336    |   yes    |        100.2        |   4996   |    86.1     | 44.0  |\n|    MuSc($s=1$)    |   ViT-L-14-336    |    no    |        998.8        |   7168   |    97.8     | 93.8  |\n|    MuSc($s=2$)    |   ViT-L-14-336    |    no    |        605.8        |   5666   |    97.1     | 93.8  |\n|    MuSc($s=3$)    |   ViT-L-14-336    |    no    |        513.5        |   5026   |    96.7     | 93.7  |\n| APRIL-GAN(0-shot) | ViT-B-16-plus-240 |   yes    |        64.7         |   3226   |      -      |   -   |\n|  WinCLIP(0-shot)  | ViT-B-16-plus-240 |    no    |         389         |    -     |    91.8     | 64.6  |\n|    MuSc($s=1$)    | ViT-B-16-plus-240 |    no    |        455.3        |   3002   |    95.4     | 91.9  |\n|    MuSc($s=2$)    | ViT-B-16-plus-240 |    no    |        285.7        |   2740   |    95.4     | 91.5  |\n|    MuSc($s=3$)    | ViT-B-16-plus-240 |    no    |        238.3        |   2684   |    95.2     | 91.4  |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission19/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685278508,
                "cdate": 1700685278508,
                "tmdate": 1700685278508,
                "mdate": 1700685278508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]