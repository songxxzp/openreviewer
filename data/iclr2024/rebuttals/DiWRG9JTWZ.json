[
    {
        "title": "MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation"
    },
    {
        "review": {
            "id": "PO6kZicdgD",
            "forum": "DiWRG9JTWZ",
            "replyto": "DiWRG9JTWZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7015/Reviewer_xD6Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7015/Reviewer_xD6Q"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a large-scale, diverse, and realistic environment benchmark for spurious-correlation few-shot classification. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious correlation shifts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and neatly presented. \n2. Exploring the spurious-correlation few-shot classification is a very interesting task.\n3. Sufficient experimental results show that spurious correlation shifts damage the model performance."
                },
                "weaknesses": {
                    "value": "1. Contributions are not clearly and accurately stated.\n2. The explanation for the spurious-correlation few-shot classification is not easy to understand."
                },
                "questions": {
                    "value": "1. In the experiment, why not use the most recent methods for comparison? I have only listed some references for 2021-2022, and the authors can use other recent methods rather than the enumerated ones.  I'm just curious about how recent methods perform on the MetaCoCo dataset.\n2. How to understand the conceptual and contextual information mentioned in the manuscript?\n3. The network structure of ResNet-50 is not explained in EXPERIMENTAL SETUP, it is suggested that the author explain it in backbone architectures.\n\n[1]  Yang, Zhanyuan, Jinghua Wang, and Yingying Zhu. \"Few-shot classification with contrastive learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[2]  Wertheimer, Davis, Luming Tang, and Bharath Hariharan. \"Few-shot classification with feature map reconstruction networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n[3]  Wang, Heng, et al. \"Revisit Finetuning strategy for Few-Shot Learning to Transfer the Emdeddings.\" The Eleventh International Conference on Learning Representations. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7015/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7015/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7015/Reviewer_xD6Q"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583733145,
            "cdate": 1698583733145,
            "tmdate": 1700660292107,
            "mdate": 1700660292107,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TfYac6rdCy",
                "forum": "DiWRG9JTWZ",
                "replyto": "PO6kZicdgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 1): Added contribution summary and further concept clarifications"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1] Contributions are not clearly and accurately stated.**\n\n**Response:** We thank the reviewer for pointing out this issue. **We have added a concise and clear contribution summary of our paper in the \u201cIntroduction\u201d section.** In a nutshell, our main contributions can be summarized as follows:\n\n-\tWe propose a large-scale few-shot classification benchmark with spurious-correlation shifts arising from various contexts in real-world scenarios, named MetaCoCo, in which each class (or concept) consists of different backgrounds (or contexts).\n\n-\tWe further propose a metric by using a pre-trained vision-language model to quantify\nand compare the extent of spurious correlations on MetaCoCo and other benchmarks.\n\n-\tWe conduct extensive experiments on MetaCoCo to evaluate and compare most recent methods in few-shot classification, domain shifts, overfitting, and self-supervised learning.\n\n> **[W2] How to understand the conceptual and contextual information mentioned in the manuscript?**\n\n**Response:** Thank you for the comment. In this paper, we follow the existing literature addressing out-of-distribution problems in computer vision [1, 2, 3] to adopt the terminologies \u201cconceptual information\u201d and \u201ccontextual information\u201d. Specifically, **the conceptual information in an image refers to the object of the class,** while **the contextual information in an image refers to the background or the environment.** For example, consider an image with a dog in green grass, and the true class label is \"dog\". Then \"dog's nose\" can be regarded as a conceptual information, while \"green grass\" can be regarded as a contextual information. As shown in Figure 1 (b), the conceptual information includes \u201cdog\u201d, \u201cbicycle\u201d, \u201ctrain\u201d, \u201ccat\u201d, etc., whereas the contextual information includes \u201cautumn\u201d, \u201csnow\u201d, \u201crock\u201d, \u201cwater\u201d, etc.\n\n> **[W3] The explanation for the spurious-correlation few-shot classification is not easy to understand.**\n\n**Response:** We continue with our response to W2 to further explain the spurious-correlation in few-shot classification. For the aforementioned example, that is, an image with a dog in green grass, a well-trained classifier may capture both conceptual and contextual information: \"dog's nose\" and \"green grass\". However, given that the true label of the image is \u201cdog\u201d, we know that **the classifier produces a spurious correlation between the contextual information \"green grass\" and the image label \u201cdog\u201d.**\n\nDifferent from the widely known Meta-dataset with cross-domain shifts shown in Figure 1 (a), the proposed MetaCoCo as shown in Figure 1 (b) further emphasizes the existence of spurious-correlations in the real-world scenarios. For example, in base classes we have an image \"dog in green grass\" (and the true class label is \"dog\"), while in novel classes we instead have an image \"cat in green grass\" (and the true class label is \"cat\"). **Thus, we need the classifier to be able to distinguish between these conceptual and contextual information.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558430195,
                "cdate": 1700558430195,
                "tmdate": 1700560958709,
                "mdate": 1700560958709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oFXD90gFnI",
                "forum": "DiWRG9JTWZ",
                "replyto": "PO6kZicdgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 2): Added extensive experiments comparing the most recent few-shot classification methods"
                    },
                    "comment": {
                        "value": "> **[W4] How recent methods perform on the MetaCoCo dataset?**\n\n**Response:** We thank the useful suggestions from the reviewer. **We added more experiments to compare the performance of the 8 most recent few-shot classification methods on the proposed MetaCoCo.** The experimental results are shown as below.\n\n| Method | Conference | Backbone | Type | GL | LL | TT | $1$-shot | $5$-shot |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| CDKT+ML [4] | NeurIPS 2023 | ResNet18 | Meta |  | &check; | &check; | 44.86 | 61.42  |\n| CDKT+PL [4] | NeurIPS 2023 | ResNet18 | Meta |  | &check; | &check; |43.21 | 59.87 |\n| DeepBDC [5] | CVPR 2022 | ResNet12 | Metric |  | &check; | &check; |46.78 | 62.54 |\n| TSA+DETA [6] | ICCV 2023 | ResNet18 | Metric | | &check; | &check; | 51.42 | 61.58 | \n| PUTM [7] | ICCV 2023 | ResNet18 | Metric | | &check; | &check; | 60.23 | 72.36 |\n| FRN [8] | CVPR 2021 | ResNet12 | Fine-tuning | &check; | | &check; | 50.23 | 60.56 |\n| Yang et al [9] | ECCV 2022 | ResNet12 | Fine-tuning | &check; | | &check; | 58.01 | 69.32 |\n| LP-FT-FB [10] | ICLR 2023 | ResNet12 | Fine-tuning | &check; | &check; | &check; | 56.21 | 70.21 |\n| | | | | | | | | | \n\nFrom the table, we find that: (1) The performance of these methods is significantly reduced on our proposed MetaCoCo dataset. (2) The performance of fine-tuning based methods is better than other methods, such as LP-FT-FB [10]. \n\n**In addition to the methods mentioned by the reviewer, we also verify diverse methods including vision-language pre-trained methods.**\n\n| Method | Conference | Backbone | Type | Base (1-shot) | New (1-shot) | HM (1-shot) | Base (16-shot) | New (16-shot) | HM (16-shot) |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| CLIP [11] | ICML 2021 | ResNet50 | Zero-shot CLIP | 68.12 | 72.19 | 70.10 | 68.12 | 72.19 | 70.10 |\n| CoOp [12] | IJCV 2022 | ResNet50 | Prompt-tuning CLIP | 61.22 | 63.62 | 62.40 | 47.55 | 40.90 | 43.98 |\n| CoCoOp [13] | CVPR 2022 | ResNet50 | Prompt-tuning CLIP | 59.86 | 63.88 | 61.80 | 36.88 | 46.10 | 40.98 |\n| KgCoOp [14] | CVPR 2023 | ResNet50 | Prompt-tuning CLIP | 67.95 | 72.05 | 69.94 | 49.18 | 48.78 | 48.98 |\n| LFA [15] | ICCV 2023 | ResNet50 | Prompt-tuning CLIP | 66.87 | 73.01 | 69.81 | 49.23 | 48.12 | 48.67 |\n| ProGrad [16] | ICCV 2023 | ResNet50 |Prompt-tuning CLIP | 65.23 | 69.39 | 67.25 | 49.14 | 47.36 | 48.23 |\n| | | | | | | | | | | \n| CLIP [11] | ICML 2021 | ViT-B/16 | Zero-shot CLIP | 73.19 | 75.29 | 74.23 | 73.19 | 75.29 | 74.23  |\n| CoOp [12] | IJCV 2022 | ViT-B/16 | Prompt-tuning CLIP | 64.95 | 68.87 | 66.85 |  49.98 | 45.40 | 47.58 |\n| CoCoOp [13] | CVPR 2022 | ViT-B/16 | Prompt-tuning CLIP | 65.60 | 69.50 | 67.49 | 49.35 | 47.84 | 48.58 |\n| KgCoOp [14] | CVPR 2023 | ViT-B/16 | Prompt-tuning CLIP | 71.22 | 74.74 | 72.94 | 51.21 | 50.99 | 51.10 |\n| LFA [15] | ICCV 2023 | ViT-B/16 | Prompt-tuning CLIP | 71.56 | 74.98 | 73.23 | 52.04 | 49.65 | 50.82 |\nProGrad [16] | ICCV 2023 | ViT-B/16 | Prompt-tuning CLIP | 70.81 | 73.27 | 72.02 | 51.48 | 48.02 | 49.69 |\n| MaPLe [17] | CVPR 2023 | ViT-B/16 | Prompt-tuning CLIP | 60.31 | 68.94 | 64.34 | 48.04 | 49.47 | 48.74 |\n| | | | | | | | | | | \n\nThe above table shows the experimental results of VLMs on MetaCoCo, where base and new represent the accuracy of the base and new classes, respectively. HM is the harmonic mean of accuracy on base and new data. From the above table, we find two interesting phenomenons: (1) The more samples are used in the model prompt-tuning phase, the worse the performance of the model will be. For example, the performance drops from 62.40% with 1-shot to 43.98% with 16-shot in CoOp with ResNet50. This further indicates that the more samples used, the greater the shifts in spurious correlations introduced, resulting in reduced model performance. (2) The performance of prompt-tuning methods is lower than zero-shot CLIP in all settings. The possible reason is that prompt-tuning methods overfit to spurious information, therefore, affecting the generalization ability of the model."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558595400,
                "cdate": 1700558595400,
                "tmdate": 1700561353994,
                "mdate": 1700561353994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eKa5lgSL43",
                "forum": "DiWRG9JTWZ",
                "replyto": "PO6kZicdgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 3): Experimental details including backbone architectures and parameter tuning"
                    },
                    "comment": {
                        "value": "> **[W5] The network structure of ResNet-50 is not explained in EXPERIMENTAL SETUP, it is suggested that the author explain it in backbone architectures.**\n\n**Response:** We thank the useful suggestions from the reviewer. For **network structure,** we follow [18] to implement ResNet50 using 50 layers, including 48 convolutional layers and 2 fully connected layers, with the detailed network structure summarized in below.\n\n| Layer name | output size | ResNet50 |\n|:---------| :---------: | :---------: |\n| conv1 | 112x112 | 7x7, 64, stride 2 |\n| conv2_x | 56x56 | 3x3 max pool, stride 2 |\n| conv2_x | 56x56 | 1x1, 64 |\n| conv2_x | 56x56 | 3x3, 64 |\n| conv2_x | 56x56 | 1x1, 256 |\n| conv3_x | 28x28 | 1x1, 128 |\n| conv3_x | 28x28 | 3x3, 128 |\n| conv3_x | 28x28 | 1x1, 512 |\n| conv4_x | 14x14 | 1x1, 256 |\n| conv4_x | 14x14 | 3x3, 256 |\n| conv4_x | 14x14 | 1x1, 1024 |\n| conv5_x | 7x7 | 1x1, 512 |\n| conv5_x | 7x7 | 3x3, 512 |\n| conv5_x | 7x7 | 1x1, 2048 |\n| | 1x1 | average pool, 1000-d fc, softmax |\n| | | | \n\nSpecifically, each layer in conv2_x and conv5_x are repeated with 3 times, each layer in conv3_x are repeated with 4 times, and each layer in conv4_x are repeated with 6 times. The building block of ResNet50 is the residual block, which typically contains two convolutional layers along with the skip connection. The network architecture includes global average pooling and a softmax layer for classification in the final layers.\n\nFor all experiments, we use the A100 80G GPU as the computational resource. We tune batch size in $\\{128, 256, 512\\}$. For learning rate, optimizer, and weight decay, **we follow [20,21] as a comprehensive library for few-shot learning to implement using fixed parameter combinations as summarized in below.**\n\n| Method | Conference | Backbone | Type | Learning Rate | Optimizer | Decay |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| MoCo [20] | CVPR 2020 | ResNet50 | Self-supervised learning | 0.03 | SGD | Cosine |\n| SimCLR [21] | ICML 2020 | ResNet50 |  Self-supervised learning | 0.001 |  Adam | Cosine |\n| | | | | | | | \n\n**For ease of reproducibility, we also added Table 4 in Appendix C showing the optimal parameter combinations when implementing the other 17 methods using ResNet12 as the backbone model following [19].**\n\n***\n\n**We hope the above discussion will fully address your concerns about our work, and we would really appreciate it if you could be generous in raising your score.** We look forward to your insightful and constructive responses to further help us improve the quality of our work. Thank you!\n\n***\n\n> **Reference**\n\n[1] Haotian Wang et al. Out-of-distribution generalization with causal feature separation. TKDE 2023.\n\n[2] Xingxuan Zhang et al. Deep Stable Learning for Out Of Distribution Generalization. CVPR 2021.\n\n[3] Mingjun Xu et al. Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains. CVPR 2023.\n\n[4] Tianjun Ke et al. Revisiting Logistic-softmax Likelihood in Bayesian Meta-learning for Few-shot Classification. NeurIPS 2023.\n\n[5] Jiangtao Xie et al. Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification. CVPR 2022.\n\n[6] Ji Zhang et al. DETA: Denoised Task Adaptation for Few-Shot Learning. ICCV 2023. \n\n[7] Long Tian et al. Prototypes-oriented Transductive Few-shot Learning with Conditional Transport. ICCV 2023.\n\n[8] Wertheimer Davis et al. Few-shot classification with feature map reconstruction networks. CVPR 2021. \n\n[9] Zhanyuan Yang et al. Few-shot classification with contrastive learning. ECCV 2022. \n\n[10] Heng Wang et al. Revisit Finetuning strategy for Few-Shot Learning to Transfer the Emdeddings. ICLR 2023.\n\n[11] Radford Alec et al. Learning transferable visual models from natural language supervision. ICML 2021.\n\n[12] Kaiyang Zhou et al. Learning to prompt for vision-language models. IJCV 2022.\n\n[13] Kaiyang Zhou et al. Conditional prompt learning for vision-language models. CVPR 2022.\n\n[14] Haotao Yao et al. Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. CVPR 2023.\n\n[15] Yassine Ouali et al. Black Box Few-Shot Adaptation for Vision-Language models. ICCV 2023.\n\n[16] Beier Zhu et al. Prompt-aligned Gradient for Prompt Tuning. ICCV 2023.\n\n[17] Muhammad Uzair Khattak et al. MaPLe: Multi-modal Prompt Learning. CVPR 2023.\n\n[18] Kaiming He et al. Deep Residual Learning for Image Recognition. CVPR 2016.\n\n[19] Wenbin Li et al. Libfewshot: A comprehensive library for few-shot learning. TPAMI 2023.\n\n[20] Kaiming He et al. Momentum contrast for unsupervised visual representation learning. CVPR 2020.\n\n[21] Ting Chen et al. A simple framework for contrastive learning of visual representation. ICML 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558929542,
                "cdate": 1700558929542,
                "tmdate": 1700561515157,
                "mdate": 1700561515157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qIV0yQJPbi",
                "forum": "DiWRG9JTWZ",
                "replyto": "PO6kZicdgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Reviewer_xD6Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Reviewer_xD6Q"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the replies!"
                    },
                    "comment": {
                        "value": "The replies are very detailed and mostly addressed my concerns. I've raised my score to 8."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660254658,
                "cdate": 1700660254658,
                "tmdate": 1700660331235,
                "mdate": 1700660331235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EGNELG5Cnp",
            "forum": "DiWRG9JTWZ",
            "replyto": "DiWRG9JTWZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7015/Reviewer_QLBK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7015/Reviewer_QLBK"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel dataset, called METACoCo. The dataset targets at one research void that existing public datasets cannot evaluate \u2013 the spurious-correlation problem in few-shot classification. One metric is proposed to evaluate the spurious-correlation shifts. Experiments are conducted to evaluate the performance of existing few-shot classification models etc. The results shows that the evaluated models degrade significantly in the presence of spurious-correlation shifts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main contribution of this paper is the created dataset. The dataset enables researchers to study spurious-correlation problem in few shot learning. The conducted experimental analysis is persuasive."
                },
                "weaknesses": {
                    "value": "One of my concerns is the technical contribution of this work. Although this paper points out spurious-correlation that have not been well addressed by existing methods and a dataset is proposed, the technical contribution of this work would be more significant if a model can be provided to address the problem."
                },
                "questions": {
                    "value": "What is the difference between spurious-correlation studied in this paper and overfitting in few shot learning? Can we view spurious-correlation as one type of model overfitting? If this is true, it would be interesting to have a comprehensive study of overfitting problem in few-shot learning, in addition to the spurious correlation studied in this paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730449606,
            "cdate": 1698730449606,
            "tmdate": 1699636822045,
            "mdate": 1699636822045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6hN4THeAcB",
                "forum": "DiWRG9JTWZ",
                "replyto": "EGNELG5Cnp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors (Part 1): Added contribution summary and experiments comparing methods for addressing the overfitting problem"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1] The technical contribution of this work would be more significant if a model can be provided to address the problem.**\n\n**Response:** We thank the reviewer for pointing out this issue. Nonetheless, we kindly remind our paper is submitted to the **\u201cdatasets and benchmarks\u201d track** in ICLR 24, and we agree with the reviewer that **the main contribution is the proposed large-scale few-shot classification benchmark collected from real-world scenarios, rather than a specific method.** To the best of our knowledge, **this is the first benchmark for studying the spurious-correlation shifts in few-shot classification.** Moreover, we further propose a novel metric by using a pre-trained vision-language model to measure the extent of spurious correlations.\n\nTo make our contributions more recognizable, **we have added a concise and clear contribution summary of our paper in the \u201cIntroduction\u201d section.** In a nutshell, our main contributions can be summarized as follows:\n\n-\tWe propose a large-scale few-shot classification benchmark with spurious-correlation shifts arising from various contexts in real-world scenarios, named MetaCoCo, in which each class (or concept) consists of different backgrounds (or contexts).\n\n-\tWe further propose a metric by using a pre-trained vision-language model to quantify\nand compare the extent of spurious correlations on MetaCoCo and other benchmarks.\n\n-\tWe conduct extensive experiments on MetaCoCo to evaluate and compare most recent methods in few-shot classification, domain shifts, overfitting, and self-supervised learning.\n\n\n> **[W2] What is the difference between spurious-correlation studied in this paper and overfitting in few shot learning?**\n\n**Response:** Thank you for raising such an interesting and insightful concern. In our opinion, we consider overfitting to be a broader concept than spurious-correlation, that is, **spurious-correlation in the training data can cause model overfitting when naively using empirical risk minimization, but not necessary.** In fact, **many other factors can also lead to model overfitting, such as long-tailed data [1], noisy labels [2], cross-domain shifts [3, 4], etc.**\n\n> **[W3] Can we view spurious-correlation as one type of model overfitting?**\n\n**Response:** Yes, please kindly refer to our response to W2 that **the spurious-correlation problem is one of the causes resulting in model overfitting.**\n\n> **[W4] If this is true, it would be interesting to have a comprehensive study of overfitting problem in few-shot learning, in addition to the spurious correlation studied in this paper.**\n\n**Response:** As suggested by the reviewer, **we added a comprehensive study comparing 9 most recent methods for addressing the overfitting problem** in few-shot learning on MetaCoCo. The experimental results are shown as below.\n\n|Method | Conference | Backbone | Type | GL | LL | TT | 1-shot | 5-shot | \n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| MTL [5] | CVPR 2019 | ResNet12 | Meta | &check; | &check; | &check; | 44.23 | 58.04 |\n|ProtoNet [6] | NeurIPS 2017 | ResNet12 |Metric |  | &check; | &check; | 42.69 | 59.50 |\n| BF3S [7] | ICCV 2019 | WRN-28-10 | Metric | | &check; | &check; | 43.78 | 57.64 |\n| FEAT [8] | CVPR 2020 | ResNet18 | Metric | &check; | &check; | &check; | 50.23 | 64.12 | \n|DSN [9] | CVPR 2020 | ResNet12 | Metric | | &check; | &check; | 46.21 | 58.01 |\n| SVM with DC [10]| TPAMI 2021 | ResNet18 | Metric | &check; | &check; | &check; | 43.56 | 58.96 |\n| SMKD with Prototype [11] | CVPR 2023 | ViT-S | Metric | &check; | &check; | &check; | 56.89 | 69.54 |\n| FGFL [12] | ICCV 2023 | ResNet12 | Metric |  | &check; | &check; | 46.78 | 64.32 | \n|LP-FT-FB [13] | ICLR 2023 | ResNet12 | Fine-tuning | &check; | &check; | &check; | 56.21 | 70.21 |\n|  |  |  |  |  |  |  |  | \n\nFrom the table, we find that these methods do not show excellent performance on MetaCoCo with spurious-correlation. One possible reason is that **these methods aim to address the overfitting problem of base classes in the few-shot training phase,** but not for the spurious-correlation problem, **making the trained model tend to overfit to the context information,** *e.g.*, **background knowledge.**\n\n***\n**We hope the above discussion will fully address your concerns about our work, and we would really appreciate it if you could be generous in raising your score.** We look forward to your insightful and constructive responses to further help us improve the quality of our work. Thank you!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558110009,
                "cdate": 1700558110009,
                "tmdate": 1700559744804,
                "mdate": 1700559744804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wb6buI2QGq",
                "forum": "DiWRG9JTWZ",
                "replyto": "EGNELG5Cnp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed and insightful comments, and we would like to address your concerns and questions."
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and insightful comments. We kindly remind you that the author-reviewer discussion period is about to close and we hope that the response in the rebuttal could address all your concerns. Specifically, we added **a concise and clear contribution summary** of our paper (in Introduction, Section 1) and **a comprehensive study comparing the 9 most recent methods for addressing the overfitting problem** in few-shot learning on MetaCoCo (in Appendix D) to better address your concerns. We also added extensive experiments comparing the **9 most recent few-shot classification methods** on MetaCoCo (in Table 2, Section 5), as well as the **7 most recent vision-language pre-trained models (VLMs)** using ResNet50 and ViT-B/16 as the vision backbone, and **further clarified our experimental details** in terms of the backbone architectures and parameter tunings (in Appendix C). We hope the above discussion will fully address your concerns about our work.\n\nWe are anticipating any further advice, inquiries or remaining concerns you may have. We will make our best effort to handle them. Thank you!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637769967,
                "cdate": 1700637769967,
                "tmdate": 1700664472802,
                "mdate": 1700664472802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vB6RfKZEW7",
            "forum": "DiWRG9JTWZ",
            "replyto": "DiWRG9JTWZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7015/Reviewer_F7nA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7015/Reviewer_F7nA"
            ],
            "content": {
                "summary": {
                    "value": "Out-of-distribution problems in few-shot classification are important and challenging, which considerably degrades the performance of deep learning models deployed in real-world applications. However, although cross-domain shifts and spurious-correlation shifts are two important real-world distributions, the lack of spurious dataset has left a gap in this important research area. In this paper, the authors present a new benchmark to accelerate the development of the spurious-correlation few-shot classification problem. Overall, I think it is important to study the spurious-correlation few-shot classification and the authors have done a great job to this area."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: The paper is well-motivated to propose a benchmark with spurious-correlation shifts collected from real-world scenarios. The presentation of this paper is fairly clear, and the details of the dataset construction are also explained thoroughly.\n\nS2: The authors further proposed to use of a pre-training vision-language model CLIP for measuring the quantifying and comparing the extent of spurious correlations on MetaCoCo and other FSC benchmarks, which is quite impressive.\n\nS3: Extensive experiments are conducted to investigate the ability of current methods for addressing real-world spurious correlations, with the baselines including a wide range of existing representative approaches."
                },
                "weaknesses": {
                    "value": "W1: Currently, many vision-language pre-training models, such as CLIP [1] and CoOp [2], evaluate the performance in few-shot setting. I hope to see the performance of pre-trained large models like CLIP on the MetaCoCo with spurious-correlation shifts.\n\n[1] Radford, Alec et al. Learning transferable visual models from natural language supervision, ICML.\n\n[2] Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei, Learning to prompt for vision-language models, IJCV."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838225581,
            "cdate": 1698838225581,
            "tmdate": 1699636821928,
            "mdate": 1699636821928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "48fTnXGV0h",
                "forum": "DiWRG9JTWZ",
                "replyto": "vB6RfKZEW7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses by Authors: Added experiments for evaluating the vision-language pre-trained models"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1] The performance of pre-trained large models like CLIP on the proposed MetaCoCo dataset.**\n\n**Response:** We thank the reviewer for pointing out this issue. **We added extensive experiments for evaluating the performance of vision-language pre-trained models (VLMs) on our proposed MetaCoCo dataset.** These VLMs are broadly categorized into zero-shot CLIP and prompt-tuning CLIP. We follow the original papers to implement using ResNet50 and ViT-B/16 as backbones with 1-shot and 16-shot samples, and the experimental results are shown as below.\n\n| Method | Conference | Backbone | Type | Base (1-shot) | New (1-shot) | HM (1-shot) | Base (16-shot) | New (16-shot) | HM (16-shot) |\n|:---------| :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| CLIP [1] | ICML 2021 | ResNet50 | Zero-shot CLIP | 68.12 | 72.19 | 70.10 | 68.12 | 72.19 | 70.10 |\n| CoOp [2] | IJCV 2022 | ResNet50 | Prompt-tuning CLIP | 61.22 | 63.62 | 62.40 | 47.55 | 40.90 | 43.98 |\n| CoCoOp [3] | CVPR 2022 | ResNet50 | Prompt-tuning CLIP | 59.86 | 63.88 | 61.80 | 36.88 | 46.10 | 40.98 |\n| KgCoOp [4] | CVPR 2023 | ResNet50 | Prompt-tuning CLIP | 67.95 | 72.05 | 69.94 | 49.18 | 48.78 | 48.98 |\n| LFA [5] | ICCV 2023 | ResNet50 | Prompt-tuning CLIP | 66.87 | 73.01 | 69.81 | 49.23 | 48.12 | 48.67 |\n| ProGrad [6] | ICCV 2023 | ResNet50 |Prompt-tuning CLIP | 65.23 | 69.39 | 67.25 | 49.14 | 47.36 | 48.23 |\n|| |  |  | | | | | | |\n| CLIP [1] | ICML 2021 | ViT-B/16 | Zero-shot CLIP | 73.19 | 75.29 | 74.23 | 73.19 | 75.29 | 74.23  |\n| CoOp [2] | IJCV 2022 | ViT-B/16 | Prompt-tuning CLIP | 64.95 | 68.87 | 66.85 |  49.98 | 45.40 | 47.58 |\n| CoCoOp [3] | CVPR 2022 | ViT-B/16 | Prompt-tuning CLIP | 65.60 | 69.50 | 67.49 | 49.35 | 47.84 | 48.58 |\n| KgCoOp [4] | CVPR 2023 | ViT-B/16 | Prompt-tuning CLIP | 71.22 | 74.74 | 72.94 | 51.21 | 50.99 | 51.10 |\n| LFA [5] | ICCV 2023 | ViT-B/16 | Prompt-tuning CLIP | 71.56 | 74.98 | 73.23 | 52.04 | 49.65 | 50.82 |\nProGrad [6] | ICCV 2023 | ViT-B/16 | Prompt-tuning CLIP | 70.81 | 73.27 | 72.02 | 51.48 | 48.02 | 49.69 |\n| MaPLe [7] | CVPR 2023 | ViT-B/16 | Prompt-tuning CLIP | 60.31 | 68.94 | 64.34 | 48.04 | 49.47 | 48.74 |\n|| |  |  | | | | | | |\n\n\nIn the above table, \u201cbase\u201d and \u201cnew\u201d represent the accuracy of the base and new classes, respectively, and HM is the harmonic mean of accuracy on \u201cbase\u201d and \u201cnew\u201d data. From the table, we find two interesting phenomenons: (1) The more samples are used in the model prompt-tuning phase, the worse the performance of the model will be. For example, the performance drops from 62.40% with 1-shot to 43.98% with 16-shot in CoOp with ResNet50. This further indicates that the more samples used, the greater the shifts in spurious correlations introduced, resulting in reduced model performance. (2) The performance of prompt-tuning methods is lower than zero-shot CLIP in all settings. The possible reason is that prompt-tuning methods overfit to spurious information, therefore, affecting the generalization ability of the model.\n\n***\n**We hope the above discussion will fully address your concerns about our work.** We look forward to your insightful and constructive responses to further help us improve the quality of our work. Thank you!\n***\n\n> **References**\n\n[1] Radford Alec et al. Learning transferable visual models from natural language supervision. ICML 2021.\n\n[2] Kaiyang Zhou et al. Learning to prompt for vision-language models. IJCV 2022.\n\n[3] Kaiyang Zhou et al. Conditional prompt learning for vision-language models. CVPR 2022.\n\n[4] Haotao Yao et al. Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. CVPR 2023.\n\n[5] Yassine Ouali et al. Black Box Few-Shot Adaptation for Vision-Language models. ICCV 2023.\n\n[6] Beier Zhu et al. Prompt-aligned Gradient for Prompt Tuning. ICCV 2023.\n\n[7] Muhammad Uzair Khattak et al. MaPLe: Multi-modal Prompt Learning. CVPR 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557541734,
                "cdate": 1700557541734,
                "tmdate": 1700560459623,
                "mdate": 1700560459623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4VBtJlObL6",
                "forum": "DiWRG9JTWZ",
                "replyto": "vB6RfKZEW7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed and insightful comments, and we would like to address your concerns and questions."
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and insightful comments. We kindly remind you that the author-reviewer discussion period is about to close and we hope that the response in the rebuttal could address all your concerns. **Specifically, we added experiments comparing 7 most recent vision-language pre-trained models (VLMs) using ResNet50 and ViT-B/16 as the vision backbone, respectively.** We hope the above discussion will fully address your concerns about our work. \n\nWe are anticipating any further advice, inquiries or remaining concerns you may have. We will make our best effort to handle them. Thank you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637659869,
                "cdate": 1700637659869,
                "tmdate": 1700637679142,
                "mdate": 1700637679142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mewa9B6JKM",
                "forum": "DiWRG9JTWZ",
                "replyto": "4VBtJlObL6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Reviewer_F7nA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Reviewer_F7nA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the authors' replies, and my positive recommendation remains unchanged."
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their great efforts during the rebuttal period, the added comparison experiments with VLMs enhanced the quality of this paper. My positive recommendation remains unchanged."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663502832,
                "cdate": 1700663502832,
                "tmdate": 1700663502832,
                "mdate": 1700663502832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rkLp0aV8Zs",
                "forum": "DiWRG9JTWZ",
                "replyto": "vB6RfKZEW7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive recommendation!"
                    },
                    "comment": {
                        "value": "We are glad to know that your concerns have been effectively addressed. We are very grateful for your constructive comments and questions, which helped improve the clarity and quality of our paper. Thanks again!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690488114,
                "cdate": 1700690488114,
                "tmdate": 1700690512949,
                "mdate": 1700690512949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]