[
    {
        "title": "Text-Aware Diffusion Policies"
    },
    {
        "review": {
            "id": "spJdi0O3m7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
            ],
            "forum": "YhPUSofMgr",
            "replyto": "YhPUSofMgr",
            "content": {
                "summary": {
                    "value": "This paper presents a policy learning method that uses text-visual alignment as a reward signal. Diffusion models are used to compute the matching scores between text and images. The authors then conduct experiments on a variety of locomotion experiments, and the results show that the proposed method can learn to perform following the text while performing the motion as autonomous agents.\n\n---\n\nPost rebuttal: the authors provided extensively more experiments, which not look very good but it's reasonable. As a result, I upgrade my score from five to six."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of adopting diffusion models as a reward model is novel in my feeling. I think this idea may have other applications and this is very insightful.\n2. I love the provided video demos. They demonstrate that the proposed method can work in several environments.\n3. The proposed approach outperforms a CLIP-based method, which is promising and it indicates that diffusion models may be a better measurement model."
                },
                "weaknesses": {
                    "value": "I have a few concerns in my mind. However, I'll re-rate this paper after the rebuttal and after seeing other reviews.\n# The soundness of this work.\n\n1. The game environments are not natural images but the diffusion models are trained on natural images. How will this work in a game environment? I doubt the effectiveness of diffusion models in such a scenario. Can the authors show some generated images in this locomotion environment? Otherwise, I doubt the effectiveness of using diffusion models here.\n\n2. If the video is not used, why doesn't the model get stuck into some best-matching frames? Since policy learning often involves some non-trivial procedures, it is not convincing if the authors only use text-to-image techniques but not video encoding methods. The matching should be conducted in the temporal space.\n\n3. I see that the provided video demos do not have a very complex procedure. Most videos are nearly static and do not move much. This problem may be due to the use of images in the reward calculation.\n\n# Comparisons and experiments.\n\n1. Why don't the authors compare their methods in other language-guided tasks such as language-guided navigation?\n\n2. The results shown in Table 1-2 are not very surprising. The authors are recommended to try harder problems. Also, comparing it to pure RL methods is a must."
                },
                "questions": {
                    "value": "1. Why do the authors construct a large model for shaping rewards? This requires a good explanation.\n\n2. How reliable the noise is to calculate the reward? Sometimes the noise may not mean much thing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697199757377,
            "cdate": 1697199757377,
            "tmdate": 1700723663144,
            "mdate": 1700723663144,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P9ZQCF3g2P",
                "forum": "YhPUSofMgr",
                "replyto": "spJdi0O3m7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer JZP2"
                    },
                    "comment": {
                        "value": "We thank Reviewer JZP2 for their thorough review.  We seek to address their listed considerations below:\n\n- On soundness: the reviewer raises the question of how diffusion models can be applied to frames produced by game environments, if the diffusion models are trained on natural images.  Indeed, the diffusion models are trained on natural images; however, they are also trained on cartoon images, paintings, movie posters, and more.  We believe that the large-scale, pretrained diffusion model is able to encode the essence of the environment.\n\n- On avoiding getting stuck: we provide analysis into why leveraging a text-to-image generative model as a reward signal does not cause the resulting policy to get stuck in achieving some best-matching frames in Appendix A.1.  On the other hand, this is a common and natural pitfall when using alignment models such as CLIP as the supervisory reward signal to train a policy, as we both quantitatively and qualitatively demonstrate throughout the paper (in particular Walker, as well as the new Dog experiments).  We believe that the stochasticity stemming from the source noise causes the pretrained, frozen diffusion model to constantly change its current belief of what the \"best-matching frame\" is.  However, this does not result in a wildly unstable reward signal; as the diffusion model has learned a conditional *distribution*, these changing \"best-matching frame\" beliefs will always have high alignment with the provided text prompt.  Ultimately, for continuous locomotion prompts, where there is a large set of aligned images (e.g. for \"walking\" there are many valid poses of different configurations of swinging arms and legs), we find that TADPol learns to perform coherent, text-aligned motion.  On the other hand, as CLIP is deterministic, there is inherently a singular frame achievable by the policy that will have the highest alignment score with the text prompt.  Then, a policy optimized using a pretrained CLIP checkpoint will learn to seek and get stuck maintain that best-matching frame.\n\n- On using video models: we would like to mention that we do compare against video methods: we perform experimentation of our approach against Vid-TADPol, which uses a text-to-video diffusion model in place of a text-to-image one for TADPol.  However, we surprisingly find that Vid-TADPol achieves comparable performance with TADPol in our experiments.  One potential explanation might be that learning to walk in the OpenAI Gym environment is relatively insensitive to the priors encoded in the text-to-video model beyond those found in a text-to-image model.  Another potential explanation is that Vid-TADPol is limited by the quality of currently existing text-to-video solutions.  Addressing this finding and exploring how natural videos can be used to supervise the learning of novel text-conditioned policies is exciting future work.\n\n- On the complexity of video demonstrations: we provide additional results demonstrating TADPol on the complex Dog environment from the DeepMind Control Suite.  In these experiments, we show that TADPol is able to learn complex procedures (such as having the Dog stand on its hind legs or chase its tail).  Furthermore, we showcase in our updated experiments how TADPol can learn motion policies.\n\n- On comparisons to language-guided tasks: in this work we focus our efforts on learning text-conditioned continuous locomotion capabilities, and we show evidence suggesting TADPol is a promising approach towards this objective.  Text-conditioned continuous locomotion is difficult compared to goal-achieving tasks (including language-guided navigation) in that there is no canonical pose or goal frame that, if reached, would denote successful achievement of the task.  For example, there is no specific pose for \u201crunning\u201d that when reached, would successfully represent completion of the \u201crunning\u201d behavior; instead, agents must perform a coherent continuous sequence of poses over time (such as alternating stepping with the legs and swinging the arms) to convincingly demonstrate accomplishment of the task.  To our knowledge, there are no prior works that seek to tackle this objective.  Furthermore, the environments of many language-guided tasks are egocentric in nature; we leave it as interesting future work to explore whether or not pretrained diffusion models naturally understand how intermediate egocentric frames should be aligned with a desired text instruction."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583382207,
                "cdate": 1700583382207,
                "tmdate": 1700583382207,
                "mdate": 1700583382207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mYaRPlKThW",
                "forum": "YhPUSofMgr",
                "replyto": "y6by2XsVba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response."
                    },
                    "comment": {
                        "value": "The provided experiments on dogs do not look very good. Those dogs move very awkwardly, which differs from realistic dogs very much. Also, the stochasticity cannot fully account for the good motion, because the direction of stochasticity cannot be controlled well."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614710310,
                "cdate": 1700614710310,
                "tmdate": 1700614710310,
                "mdate": 1700614710310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hjaVgzIfsh",
                "forum": "YhPUSofMgr",
                "replyto": "VG9jXA7IJL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response again."
                    },
                    "comment": {
                        "value": "1. Try to pick some good demos. Although some envs may not be very satisfying, the proposed approach should do well in some other environments. \n\n2. The authors should compare to others that adopt diffusion as policy, not the methods running without diffusions. These are good baselines (mentioned by another reviewer): https://arxiv.org/abs/2310.12921 https://arxiv.org/abs/2203.12601 https://arxiv.org/abs/2210.00030. Also, I suggest the authors compare to the concurrent work https://openreview.net/forum?id=N0I2RtD8je. I read your comments that you said that the settings are somewhat different. Yes, you experimented in different setups, but their setups were making more sense (e.g. with imitation learning and diffusions for exploration). So comparisons were still needed. It's not only an issue of choice of baselines, I think moving to their setups is also benefit for current manuscript."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637798570,
                "cdate": 1700637798570,
                "tmdate": 1700637798570,
                "mdate": 1700637798570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xz8VhYcPmh",
                "forum": "YhPUSofMgr",
                "replyto": "W0y3GxBkMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the additional feedback."
                    },
                    "comment": {
                        "value": "1. Authors say, \"Dog is a known environment in RL space\". It's better to provide some reference. I haven't seen any papers in the RL community using this environment. It's better to use some robotic environments instead.\n\n2. About baselines: CLIP-SimPol is not that important in my eyes because it's not a standard policy learning method. The crucial point is to compare against three lines of work: RL, imitation learning, and unsupervised learning. Currently, CLIP-SimPol can fall into the third category, but results of standard RL and imitation learning are missing. It's interesting to see whether the proposed approach can benefit RL and imitation learning methods. Authors criticize that  \"this assumes the existence of an expert trajectory demonstration already.\" I don't agree with such a criticism because imitation learning is a vast domain, and it can often address problems that RL cannot address. Also, learning from a big data is a good general direction and I think imitation learning will have a good future."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700296423,
                "cdate": 1700700296423,
                "tmdate": 1700700296423,
                "mdate": 1700700296423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QCDWCWHcvA",
                "forum": "YhPUSofMgr",
                "replyto": "spJdi0O3m7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the additional comments."
                    },
                    "comment": {
                        "value": "We thank Reviewer JZP2 for the prompt and informative replies, particularly this late in the discussion period, and seek to address their concerns.\n\n**On the difficulty of the Dog environment:** we provide two references, [1] and [2], that testify to the difficulty and complexity of the Dog environment, particularly due to its large 38-dimensional action space and its complex transition dynamics.  Concurrent work (https://openreview.net/forum?id=Oxh5CstDJU) demonstrate that SAC and DreamerV3, powerful model-free and model-based approaches established in the field, are unable to learn any of the Dog tasks despite training on the ground-truth reward ([Figure 5](https://openreview.net/pdf?id=Oxh5CstDJU)).  We selected the Dog environment to demonstrate TADPol's capabilities because beyond being complex, it naturally enables us to flexibly demonstrate a variety of goal-conditioned *and* continuous locomotion behaviors.  We agree that robotic environments are similarly worthwhile and interesting to apply TADPol to, and leave it as interesting future work.\n\n**On comparisons with RL:** We agree with the reviewer that our method is RL that uses external supervision, and we do indeed compare against other externally-supervised RL techniques (the authors believe that \"unsupervised RL\" as a term might more accurately describe curiosity-based or intrinsic motivation-based RL, where the agent generates its own reward signals).  We also agree with the reviewer that comparison with standard/ground-truth RL is valuable; and indeed we do already provide such numerical reports in Column 2 of Table 1 of the paper.  For further reference, in the updated experiments on the Dog, the ground-truth TD-MPC model achieves about 150 reward, whereas our hybrid TADPol Dog agent achieves a comparable reward of 138; the rest of the ground-truth rewards achieved by externally-supervised approaches are listed in Table 3.  Quantitatively, we show that TADPol is able to achieve high ground-truth reward despite using an external text-conditioned reward as a supervisory signal; qualitatively, we show that it can indeed recreate the behaviors described by the original ground truth reward function.  We thus believe TADPol is comparable to ground-truth RL techniques.  We also note that ground-truth RL techniques require detailed ad-hoc design of the reward function - in the absence of such ground truth reward functions, such as making a Dog stand on its hind legs or chase its tail, we demonstrate TADPol's superior performance against other externally-supervised techniques such as LIV, CLIP-SimPol, and Text2Reward.\n\n**On comparisons with IL:** We would like to clarify that we wholly support the development of imitation learning as a field; it is certainly an exciting, interesting, and valuable direction of study.  However, we are unable to apply imitation learning techniques for the Dog environment, as there are no ground truth demonstrations for the Dog tasks (such as for chasing its tail or standing on its hind legs).  In these cases, without an appropriate dataset of expert demonstrations, imitation learning cannot be performed and compared against.\n\nWe believe that when expert demonstration datasets are available, prescribing imitation learning as a solution is appropriate and desirable.  We are also interested in further developments in this field, and agree that it has a bright future.  On the other hand, RL approaches are complementary to this direction, as they are able to be applied when demonstrations are scarce or nonexistent; in such cases, we demonstrate that TADPol can be applied to learn novel behaviors conditioned flexibly on natural language.\n\n[1] Yi Zhao et al. Simplified Temporal Consistency Reinforcement Learning. ICML, 2023.\n\n[2] Nicklas Hansen et al. Temporal difference learning for model predictive control. ICML, 2022."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720854227,
                "cdate": 1700720854227,
                "tmdate": 1700720879919,
                "mdate": 1700720879919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aUJg3kckrh",
                "forum": "YhPUSofMgr",
                "replyto": "spJdi0O3m7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_JZP2"
                ],
                "content": {
                    "title": {
                        "value": "I'll update my score."
                    },
                    "comment": {
                        "value": "Thanks for your response. I also understand the effort of the authors to publish this paper in time. Some of my concerns are addressed, and the response reads reasonable. So I'll upgrade my score by one."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723595341,
                "cdate": 1700723595341,
                "tmdate": 1700723676197,
                "mdate": 1700723676197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dAqCVHJ0cB",
            "forum": "YhPUSofMgr",
            "replyto": "YhPUSofMgr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_gAGE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_gAGE"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose Text-Aware Diffusion Policies (TADPols), which attempt to learn the text-aware policy through reward function with the help of the generative prior contained in pre-trained text-image models. In particular, the reward function measures the alignment of provided instructions and images rendered of agent actions. The authors compared the proposed framework with baselines that use a reward function defined by CLIP similarity or using a text-to-video diffusion model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-organized and easy to follow. \n\n- The idea is intuitive and straightforward. The authors defined an explicit and simple reward function to optimize the policy."
                },
                "weaknesses": {
                    "value": "- The authors need to compare with other works such as LangLfP, Text-Conditioned Decision Transformer, or Hiveformer.\n\n- As mentioned in section 4, the choice of noise step might be sensitive to the performance of the proposed method. It would be helpful if the authors could provide some experiments on the choice of noise step or even try to sample a range of noise steps to make the training more stable. Besides, the function k(t) in the defined function is not described clearly. Is the method sensitive to the choice of function k(t)?\n\n- The method is only tested on a simulated environment within only three scenarios. It is hard to evaluate the method's effectiveness without testing on real-world scenarios."
                },
                "questions": {
                    "value": "- Why is the stick not always in the air if we provide instructions with the verb \u201cjump up\u201d since the rendered images will align more with the states in the air? \n\n- The results with the velocity metric is a bit confusing. Why the diffusion-based ones are faster than the clip-based one? The reward function is to match the text descriptions, which does not imply the velocity.\n\n- How will the model perform if provided with descriptions like \u201cmove forward\u201d and \u201cmove backward\u201d. Will they generate different policies?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Reviewer_gAGE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790294838,
            "cdate": 1698790294838,
            "tmdate": 1699636616456,
            "mdate": 1699636616456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JBTqiFl5Gq",
                "forum": "YhPUSofMgr",
                "replyto": "dAqCVHJ0cB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer gAGE"
                    },
                    "comment": {
                        "value": "We thank Reviewer gAGE for their thorough review.  We seek to address their listed considerations below:\n\n- On comparison with text-conditioned work: we appreciate the reviewer for providing a list of related work.  We would like to mention that these works: LangLfP, Text-Conditioned Decision Transformer, and Hiveformer, all require training on datasets of trajectories that have been labeled with natural language.  Not only are such labeled datasets expensive to create, involving substantial human effort, but the resulting trained policies do not naturally transfer across environments.  Indeed, a labeled dataset of trajectories must be created for each novel environment, and across a large number of behaviors and associated text prompts within the environment.  In contrast, TADPol enables the learning of text-conditioned policies irrespective of visual environment, and without requiring any pretraining dataset of demonstrations or labeling whatsoever.  We do agree with the high-level sentiment of the reviewer, however, that additional comparisons with benchmarks would strengthen the work.  In the updated experiments, the results of which can be found in both the updated manuscript and website, we provide comparisons against CLIP-SimPol (Rocamonde et al. arXiv:2310.12921 [a]), LIV (Ma et al., ICML 2023 [b]), and Text2Reward (Xie et al., arXiv:2309.11489 [c]).  Such approaches leverage large-scale pretrained models to generate dense rewards for novel behaviors conditioned on text, and we apply them out-of-the-box to arbitrary RL environments.  We demonstrate that in the Dog environment from DeepMind Control Suite that TADPol is able to outperform the other methods in learning both goal-achieving behaviors as well as continuous locomotion behaviors.\n\n- On the sensitivity to noise steps:  we agree with the reviewer that the choice of noise step might be sensitive to the performance of TADPol. In the updated experiments for the Dog experiment, we thoroughly investigate design choices surrounding the source noise in Appendix B.3.  We explore how the frequency of re-sampling the source noise affects the behavior: having a global source noise for all episodes results in more stable, CLIP-like behavior, having a source noise resampled for every frame results in more a more unstable reward signal and therefore policy, and having a source noise re-sampled for each episode is currently proposed as a happy medium.  Furthermore, we demonstrate that choice of noise level also affects the resulting policy; too high a noise level (such as using step 750 out of 1000), and the reward signal is no longer meaningfully understanding alignment with the generated frame.  We discover a \u201csweet spot\u201d for the noise level exists around step 450 out of 1000 that is able to extract meaningful priors from the pretrained diffusion model with respect to the generated frame, while avoiding overcorruption.  We visualize the results of these ablation experiments in the updated website link.\n\n- On the $k(t)$ function: for our experiments we use $k(t) = \\frac{1}{2} (1 - \\prod_{i \\leq t} \\alpha_i )^2$; however, we note that in our experiments $k(t)$ can be treated as a constant, since we only evaluate with a constant t (where $t=450$ in our experiments).  However, we include a $k(t)$ term in the TADPol equation to keep it general, allowing it to flexibly change with respect to $t$; in experiments where $t$ is resampled during training, this term provides the option to flexibly adjust the reward computation accordingly.\n\n- On testing on more advanced environments: we take note of the reviewer\u2019s wish to see experimentation on more advanced environments.  We therefore report additional experiments on the Dog locomotion environment from the DeepMind Control Suite.  The Dog environment is complex in that it has a high degree of freedom of motion, with no task-specific design decisions, allowing the modeling of very flexible and complex behaviors.  We demonstrate that TADPol is able to learn goal-achieving behaviors (such as standing on its hind legs or chasing its tail), as well as continuous locomotion movements (such as walking) conditioned on natural language inputs, and invite the reviewer to look at the updated website for the latest visualization results.  The background visual environment of the Dog has been modified to appear more natural, and we take these promising results as positive signals for TADPol\u2019s effectiveness in real-world scenarios."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583011015,
                "cdate": 1700583011015,
                "tmdate": 1700583011015,
                "mdate": 1700583011015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1WuZMejusB",
            "forum": "YhPUSofMgr",
            "replyto": "YhPUSofMgr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_fbDb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_fbDb"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes TextAware Diffusion Policies (TADPols) to leverage  text-to-image diffusion models to generate the reward signals for RL policies, which is from the prediction error between the diffusion model and the rendered images. The experiments show that the TADPols have comparable performances to the baselines with original rewards in some tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method is novel and interesting."
                },
                "weaknesses": {
                    "value": "(1) The writing of this paper requires improvement in a great deal. \n\n(2) The experimental setting is not sufficient. \n- There are few baselines for introducing diffusion models for policy learning and other methods (not diffusions) for reward generations.\n- The tasks are simple. What about some tasks with the DMControl suite, as other works do.\n\n(3) The reward signal lacks of motion information or temporal information. For example, the diffusion model can not distinguish whether the walker is walking forward or backward. How to identify rewards in these scenarios?"
                },
                "questions": {
                    "value": "(1) Because the image is generated by the frozen diffusion model, it must be blurry and quite different from the current scene. In this case, the reward noise will be large, and RL is sensitive to the rewards. So I am wondering why the generated rewards can share comparable performances to the vanilla ones. Can you do some visualization to explain this phenomenon?\n\n(2) The inference of the diffusion model is at a quite low speed. I am wondering how computation-efficient of this work is. Is it necessary to generate rewards every step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Reviewer_fbDb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805030392,
            "cdate": 1698805030392,
            "tmdate": 1699636616350,
            "mdate": 1699636616350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E6gFilLB0f",
                "forum": "YhPUSofMgr",
                "replyto": "1WuZMejusB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer fbDb"
                    },
                    "comment": {
                        "value": "We thank Reviewer fbDb for their review, and seek to address their listed questions and feedback:\n\n- On writing: we appreciate the reviewer\u2019s encouragement to improve the writing of this paper.  We update our draft to demonstrate and analyze new experiments to substantially reinforce the merits of our proposed approach.  We look forward to the reviewer\u2019s helpful comments on the latest version of the work to improve the clarity of our paper.\n\n- On baseline experiments: In our updated experiments (Section B in appendix) on the Dog environment, we evaluate TADPol against numerous other reward generation methods.  We compare against CLIP-SimPol (Rocamonde et al. arXiv:2310.12921 [a]), LIV (Ma et al., ICML 2023 [b]), and Text2Reward (Xie et al., arXiv:2309.11489 [c]), and demonstrate that TADPol is able to achieve superior performance in not only goal-achieving behaviors but also continuous locomotion.\n\n- On the simplicity of tasks: we agree with the reviewer that the merits of TADPol would be better showcased in complex environments such as DMControl.  We therefore provide additional experimental results on the Dog environment, a complex, flexible continuous control environment from the Deepmind Control Suite.  This environment is complex in that it has a high degree of freedom of motion, and no task-specific reset conditions, allowing the modeling of very flexible and complex behaviors.  We further split our analysis across two types of tasks: goal-achievement and continuous locomotion.  For goal-achieving tasks, we demonstrate that TADPol learns to successfully achieve a desired pose, such as \"standing\", \"standing on hind legs\", or \"chasing its tail\".  For continuous locomotion, we show how TADPol outperforms other approaches both qualitatively and quantitatively in learning motion. \n\n- On motion and temporal information: as mentioned in Section 4.3 we agree with the reviewer that by generating the reward signal solely from individual frames\u2019 alignment with natural language, a fundamental limitation is that there is indeed no conceptual way to determine temporal-extended properties such as direction or speed.  In this work we propose two ways to address this problem; firstly, by proposing and comparing against a text-to-video diffusion model.  Text-to-video models would be able to distinguish if the entire rolled-out trajectory indeed moves according to a desired direction or speed specification.  However, we discover that current text-to-video models do not outperform text-to-image models under the TADPol framework, which is an interesting future direction to explore.  Secondly, as stated in Appendix A.2, we are aware that it is often desirable to learn policies that consider non-visual factors as well, such as minimizing control costs or the energy expended by the agent as it interacts.  Such information may not be naturally extractable from visual signals, even if videos are used.  In such cases, we believe TADPol can be augmented with other available reward signals that consider other factors beyond the visual domain.  In our updated rebuttal experiments, explained in the new Appendix B.4 section, we demonstrate that TADPol can be augmented easily and effectively - when adding a simple direction-agnostic speed reward to the TADPol reward for the prompt of \"a dog walking\", the resulting learned Dog agent indeed is able to move quickly while moving in a natural walking motion.  We believe further investigative efforts into how TADPol can be made aware of and utilize motion, temporal, and non-visual information is interesting future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582849755,
                "cdate": 1700582849755,
                "tmdate": 1700582849755,
                "mdate": 1700582849755,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7f8qcOrXVu",
            "forum": "YhPUSofMgr",
            "replyto": "YhPUSofMgr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_mJca"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_mJca"
            ],
            "content": {
                "summary": {
                    "value": "While previously text-to-video models are trained using text-video pairs. This paper instead proposes to do text-to-video generation, using an existing physics simulator. This helps in preventing the text-to-video model from modelling low-level pixels, physics etc.\nInstead now the model learns on how to act in the physics simulator, given a description of the task. The model proposes to use text-to-image diffusion models as reward signals to learn a policy that can act in the physics simulator such that it can generate a video of the text description."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Proposes a novel way of rendering videos.\n- Uses a generative model for getting a reward function to learn new behaviours, to my knowledge previous works have mainly use discriminative models such as CLIP for this.\n- Shows results indicating that on a some environments they achieve rewards similar to the ground truth reward."
                },
                "weaknesses": {
                    "value": "- the motivation of using simulator to render videos is unclear to me. Like in what realistic scenarios would such a method be useful? As to the best of my knowledge current simulators are not realistic in terms of the RGB they render aka sim2real gap. It's unclear to me in what end use cases would they be useful, also given that we have millions of videos available on the web widely.\n\n- there are no comparisions with existing video rendering methods, thus making it unclear in what cases would they get better results. This point  is linked with the above point.\n \n- The other motivation of the paper is learning robot behaviours, however there are mainly approaches previously proposed that do so such as : https://arxiv.org/abs/2310.12921 https://arxiv.org/abs/2203.12601 https://arxiv.org/abs/2210.00030. The paper fails to compare against any of them."
                },
                "questions": {
                    "value": "Any answers to address the three points above would help me make the final decision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819995865,
            "cdate": 1698819995865,
            "tmdate": 1699636616226,
            "mdate": 1699636616226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U2a2DzHS6D",
                "forum": "YhPUSofMgr",
                "replyto": "7f8qcOrXVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer mJca"
                    },
                    "comment": {
                        "value": "We thank Reviewer mJca for supplying useful feedback in response to our work, as well as relevant prior work, which we address:\n\n- On simulator-rendered videos: we agree with the reviewer that there are limited realistic scenarios that would utilize the output of a simulator-rendered video as a meaningful video, beyond some complex behavior demonstrations that might be too expensive or dangerous to perform and record in real life (e.g. how to evacuate a failing spacecraft, or how to perform a high-stakes surgical procedure).  We would like to clarify the motivation of using a simulator to render videos as a natural way to treat policies as a form of visual generative model.  By interpreting the policy as a model that can generate coherent images/frames (rendered through an environment), we can connect it with diffusion models, which also generate images but with additional text-conditioning capabilities.  TADPol can then be motivated as a way to distill these capabilities, acquired from large-scale data and architecture, from a pretrained diffusion model into a policy, to naturally unlock the learning of text-conditioned behavior.  We do so by training the policy to generate rendered frames that align with the text prompt as supervised by the priors of a pre-trained frozen diffusion model.  Using a simulator to render videos of a policy is therefore the motivating bridge that connects policy learning with existing large-scale pretrained models, and enables the learning of novel text-conditioned behaviors.\n\n- On comparisons with existing video rendering methods: tied with the previous response on the practical usage of simulator-rendered videos, we agree that our goal is not really to render high-fidelity, natural-looking videos for downstream purposes.  Indeed, the visual quality of the policy is naturally limited by the strength of the environmental renderer.  Aware of these limitations, and as it is also not our main objective, we therefore do not compare TADPol against video rendering methods.  Instead, we believe the contribution of highlighting the inherent video-rendering capabilities of a policy in an environment is valuable as a paradigm shift; it exposes a natural bridge through which benefits of scale can be introduced to reinforcement learning, which has generally been siloed off from the scaling benefits observed in other domains such as vision and language.  In this work we demonstrate the benefits of incorporating large-scale data pretraining and architectures into reinforcement learning through TADPol, unlocking the learning of policies that are flexibly and accurately conditioned on natural language inputs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582633578,
                "cdate": 1700582633578,
                "tmdate": 1700582633578,
                "mdate": 1700582633578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F23z1k0tcm",
            "forum": "YhPUSofMgr",
            "replyto": "YhPUSofMgr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_LDs4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5835/Reviewer_LDs4"
            ],
            "content": {
                "summary": {
                    "value": "The authors present TADPol, a method that leverages a pretrained text-to-image diffusion model to provide a dense reward signal for control tasks based on only a text prompt. They demonstrate success on several locomotion tasks, show improved performance over a CLIP-based baseline, and also evaluate a video-diffusion-based version of their method."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is overall well-written, clearly presented, and very easy to follow. The idea of using a diffusion model to provide a dense reward signal is interesting and novel (as far as I know). On the environments that are tested, the method is successful and performs better than CLIP, which is a natural baseline. The result of successfully performing locomotion tasks in zero-shot from text prompts is impressive."
                },
                "weaknesses": {
                    "value": "While the idea of the paper is sound and interesting, I think the experiments are insufficient to demonstrate the efficacy of the method, particularly due to the lack of baselines.\n\n- The experiments are overall very thin. The tested tasks are very simple and not very numerous, which does not really convince me that TADPol is generally applicable or works consistently.\n- Baselines are lacking, with a CLIP-based reward being the only baseline. There are other methods use various pretrained models to provide a dense reward: e.g., LIV, which is mentioned in the related works section. Comparison to some other reward learning methods would be appropriate.\n- While the paper is overall fairly clear, I feel that the title is a bit of a misnomer and the narrative constructed at the beginning of the paper caused some confusion. The method does not involve a diffusion policy at all; it leverages a text-to-image diffusion model, but only to provide rewards in a policy-agnostic way. I also found all of the talk about the policy as an implicit video-generating model distracting, as well as the discussion of video-generating diffusion models in the related work. I don't think these are fundamentally related to TADPol, since the end goal of the method has nothing to do with generative modeling, but is instead just about achieving good performance in traditional RL tasks. I really think the authors should edit the paper to make sure it does not claim to perform any sort of video synthesis since this is a gross overstatement."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5835/Reviewer_LDs4",
                        "ICLR.cc/2024/Conference/Submission5835/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070566205,
            "cdate": 1699070566205,
            "tmdate": 1700680205006,
            "mdate": 1700680205006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zqjBGEBp2J",
                "forum": "YhPUSofMgr",
                "replyto": "F23z1k0tcm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LDs4"
                    },
                    "comment": {
                        "value": "We thank Reviewer LDs4 for the in-depth, constructive feedback.  Below we seek to address the reviewer\u2019s concerns:\n\n- On experimentation: we take note of the reviewer\u2019s wish to see more thorough experimentation across more complex tasks, and present additional results on the Dog locomotion environment from the DeepMind Control Suite.  The Dog environment is complex in that it has a high degree of freedom of motion, and no task-specific reset conditions, allowing the modeling of very flexible and complex behaviors.  We demonstrate that TADPol is able to learn goal-achieving behaviors (such as standing on its hind legs or chasing its tail), as well as continuous locomotion movements (such as walking) conditioned on natural language inputs, and invite the reviewer to look at the updated website for the latest visualization results.\n\n- On comparison with existing benchmarks: we perform and report additional comparisons against existing approaches that seek to generate dense rewards from natural language specification.  In particular, we benchmark our approach against LIV (Ma et al., ICML 2023 [b]), as well as Text2Reward (Xie et al., arXiv:2309.11489 [c]).  We demonstrated that using a pre-trained LIV out of the box as a dense reward signal is unable to learn meaningful goal-reaching or continuous locomotion capabilities on the Dog agent.  Whereas this may be expected, given that the experiments LIV is trained for are goal-reaching robotics demonstrations, fine-tuning LIV for the Dog task requires explicit ground-truth text-labeling of trajectories, which TADPol does not require.  Furthermore, we report comparisons against a reward function generated from text descriptions, as proposed by Text2Reward.  In these experiments, we prompt ChatGPT with a similar template as proposed in Text2Reward for Hopper, with the details of the particular Dog agent specified instead of the Hopper agent.  We discover that the reward signal generated purely from text and code conditioning is indeed able to promote related behavior in the Dog agent, but at a lower quantitative and quality compared to TADPol despite having access to real-time state signals such as speed and direction.\n\n- On the usage of \"diffusion policy\": the reviewer is correct that the policy does not synthesize predicted actions using a diffusion process; however, in this work we call our framework a \"Text-Aware Diffusion Policy\" because it is a policy optimized to respect a text-aware diffusion model.  In essence, this policy is a distillation of a pre-trained text-to-image diffusion model, transferring all the priors within the diffusion model relevant to the prompt and visual environment into a policy network.  Just as a diffusion model generates images from text prompts, so too does the resulting policy continually \"generate\" frames (by selecting actions and rendering the results through the environment) conditioned on the input text, in accordance with the text-to-image diffusion model it is distilled from.  As it is a diffusion model essentially converted into a policy network, we call it a \"Text-Aware Diffusion Policy\"; however, we are amenable to clarifying this name with an alternative, such as \"Text-Aware Diffusion-Distilled Policy\", and are open to further suggestions from the reviewer.\n\n- On video generation and reinforcement learning: we agree with the reviewer that the end goal of TADPol is about achieving good performance in traditional RL tasks, and that video synthesis is not the main contribution or story of the work.  However, we would like to defend the perspective and highlight the value in interpreting policies as implicit video-generators through an environment with rendering capabilities.  By treating the policy as a model that can generate coherent images/frames (where visual quality is admittedly at the mercy of the environment), it naturally provides a connection with diffusion models which also generate images but with the additional benefit of having excellent text conditioning properties.  This shared perspective inspires the TADPol framework, where we seek to distill the priors and text-alignment captured within a pre-trained text-to-image diffusion model, such that the resulting policy also generates frames that are now strongly aligned with human-provided text.  Whereas we do not treat policies as video renderers for the sake of using the output videos, it is precisely because we *treat* policies as a form of video generator that provides a clear motivation and intuition into TADPol.  Indeed, it is through this perspective that a path forward to incorporating the benefits of large-scale pretrained data and models into reinforcement learning, which has generally been siloed off from such scale, becomes promising; and in this work we preliminarily demonstrate such benefits, unlocking the learning of policies that are flexibly and accurately conditioned on natural language inputs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582171075,
                "cdate": 1700582171075,
                "tmdate": 1700582171075,
                "mdate": 1700582171075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjojtt5ZXG",
                "forum": "YhPUSofMgr",
                "replyto": "zqjBGEBp2J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_LDs4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5835/Reviewer_LDs4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the update and extensive additional experiments. The dog results are impressive and have convinced me of the method's robustness much more than the original experiments. The addition of LIV and Text2Reward are important baselines that also flesh out the experiments section.\n\nOverall, it seems like this paper is still a bit of a work in progress. However, zero-shot text-conditioned reward shaping is a hot area right now and I understand the authors' desire to get something out there. The authors' proposed method is novel, interesting, and appears to work at least as well as competing methods. I also appreciate the authors' attention to detail and ability to produce high-quality work. As a combination of these factors, I am slightly positive about accepting the paper, and have raised my score to a 6.\n\nI do think the biggest remaining weakness is the writing structure and narrative, which made the paper much more difficult to digest (for me as well as other reviewers, it seems). The first issue is the title: having anything close to \"diffusion policy\" immediately evokes the existing idea of [diffusion policies](https://diffusion-policy.cs.columbia.edu/), which are quite popular in the robotics community and completely unrelated to TADPol. The second issue is the framing of the paper, at least in the introduction, as being about \"video generation\". This immediately evokes video generation work from the generative modeling literature, and sets expectations accordingly, only for them to be crushed when the reader realizes that the \"video generator\" is a MuJoCo renderer. While I appreciate the authors' perspective on the connection to video synthesis, and this is an interesting point, I think it would be possible to more gently introduce the reader to this perspective without misleading (and potentially upsetting) them.\n\nMy overall point here is that the authors should be very up-front and transparent about what the paper does at its core, which from my perspective, is primarily about *using a generative diffusion model in a discriminative manner to provide text-conditioned rewards*. When framed this way, I personally still think the idea and results are pretty cool. This also makes it much easier for the reader to categorize the paper and know what comparisons to expect, i.e., comparisons to other reward-shaping papers such as LIV."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680189544,
                "cdate": 1700680189544,
                "tmdate": 1700680189544,
                "mdate": 1700680189544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]