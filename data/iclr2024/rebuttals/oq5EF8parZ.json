[
    {
        "title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models"
    },
    {
        "review": {
            "id": "J45FZRpDWo",
            "forum": "oq5EF8parZ",
            "replyto": "oq5EF8parZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_qmnH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_qmnH"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the open multimodal dialogue following user instruction in the conversations of multiple turns with multiple images. This work achieves this from three directions by proposing a model (SparklesChat), a dataset (SparklesDialogue), and a benchmark (SparklesEval). It also performs experiments on SparklesEval, the BISON binary image selection, and the NLVR2 visual reasoning task, on which SparklesChat outperforms MiniGPT-4 significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is one of the first works that studies multiple turns with multiple images for the open multimodal dialogue. Thus, it can additionally evaluate cross-image and cross-turn coherence and completeness of responses.\n\n2. It contributes a novel dataset named SparklesDialogue leveraging GPT-4.\n\n3. This work also proposes GPT-assisted evaluation named SparklesEval that can automate quantitative evaluation of a model\u2019s conversation across multiple images and dialogue turns.\n\n4. The Appendix and the supplementary material is helpful and very thorough."
                },
                "weaknesses": {
                    "value": "1. It only consider two images per context, which could be too structure with little diversity.\n\n2. The SparklesChat model is not novel in that it is just an instruction tuned miniGPT-4. It could be removed from contributions. \n\n3. As described in Table 1, SparklesDialogue is not large-scale. \n\n4. Each conversation seems to have a very typical pattern with two images as described in section 2.  \n\u201cIn the first turn, the user initiates a reasonable and creative message regarding some images. In response, the assistant generates detailed answers that include comprehensive reasoning regarding the visual content. In the second turn, the user introduces a new image for further discussion, referencing both the new and previous images.\u201d\n\n5. Only the miniGPT-4 is compared as a baseline."
                },
                "questions": {
                    "value": "1. Why only the two datasets - BISON and NLVR2 are chosen? Is there any other dataset to use?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498402609,
            "cdate": 1698498402609,
            "tmdate": 1699636607999,
            "mdate": 1699636607999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zWw1Nx0Yw7",
                "forum": "oq5EF8parZ",
                "replyto": "J45FZRpDWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qmnH"
                    },
                    "comment": {
                        "value": "We deeply appreciate your comprehensive review and are grateful for your positive remarks about our novel dataset, benchmark, and the thoroughness of our supplementary materials. We acknowledge your concerns and would like to address them and your question in the following response.\n\n---\n\n**1: Regarding Image Numbers and Dataset Diversity**\n\nAs shown in Table 1, our dataset spans one to four images across one or two dialogue turns and is not limited to two images. Additionally, the model can generalize to scenarios with more than four images or two turns. Additionally, **we include examples with four or five images in Appendix F of the revised manuscript to demonstrate this expanded capability.** We acknowledge your point on diversifying conversation patterns and plan to introduce additional templates and user-initiated conversation structures to expand the range of dialogue scenarios.\n\n**2: Clarifying Model Contribution**\n\nOur contribution to the model is the open-source SparklesChat to support chats across multiple images and dialogue turns. This capability has been a frequent request in issue trackers that remain unresolved since the release of popular multimodal instruction-following models such as MiniGPT-4 and LLaVA (see [issues 1](https://github.com/haotian-liu/LLaVA/issues/197), [2](https://github.com/Vision-CAIR/MiniGPT-4/issues/180), and [3](https://github.com/Vision-CAIR/MiniGPT-4/pull/232). We believe SparkelsChat is contributing to the community by pushing the boundaries of existing architectures to address novel challenges.\n\n**3: Scale of SparklesDialogue Dataset**\n\nThe instructional capability of SparklesChat primarily stems from an instruction-tuned LLM (Vicuna). SparklesDialogue is designed to provide dialogue data to learn alignments involving multiple images and turns. With a total of 12K dialogue turn samples (4.5K x 2 + 2K x 2), our dataset is more than triple the size of MiniGPT-4's fine-tuning set (3.5K image-description pairs). We are committed to further expanding the dataset and enhancing its diversity to serve as a training and evaluation resource for multimodal dialogue models.\n\n**4: Broadening Baseline Models**: \n\nSparklesChat is built upon the MiniGPT-4 and adapted to accommodate multiple images due to its relevance and prominence in the field. We **extended our approach to more advanced models such as LLaVA-v1.5**to offer a broader view of our approach's effectiveness. LLaVA-v1.5 has improved its image input resolution from 224 pixels to 336 pixels. It is trained on a diverse dataset comprising about 665K instruction-following data. Additionally, LLaVA-v1.5* is fine-tuned on SparklesDialogue (about 6.5K) using the low-resource technique LoRA to conserve memory and time. The results are shown below and have **updated in the revised manuscript in Section 5.1 and Table 2.**\n\n| Model        | Instruction Data| BISON | NLVR2 | SparklesEval |\n|--------------|-----------------|-------|-------|-------|\n| GPT-4        | -               | -     | -     | 9.26  |\n| MiniGPT-4    | description     | 46.0% | 51.3% | 3.91  |\n| MiniGPT-4*   | description     | 51.3% | 46.7% | 3.50  |\n| LLaVA-v1.5   | Mixture (665K ) | 52.7% | 53.3% | 2.75  |\n| LLaVA-v1.5*  | **+SparklesDialogue**| 65.3% | 56.7% | 7.93  |\n| SparklesChat | description     | 52.0% | 48.0% | 3.06  |\n| SparklesChat | reasoning       | 52.7% | 54.0% | 6.71  |\n| SparklesChat | SparklesDialogue| 56.7% | 58.0% | 8.56  |\n\nAccording to the results, despite LLaVA-v1.5's advantages of higher image resolution (336 vs. 224 pixels) and a larger training set (665K vs. 6.5K), SparklesChat significantly outperforms LLaVA-v1.5 in three evaluation sets involving multiple images. While LLaVA-v1.5 outperforms MiniGPT-4 on BISON and NLVR2, it shows weaker results on SparklesEval. This may be due to LLaVA-v1.5's training data primarily focusing on closed-set multimodal tasks such as VQA, TextCaps, and RefCOCO, while lacking in open-ended dialogue training. After fine-tuning with our SparklesDialogue, LLaVA-v1.5* not only significantly improved in open-ended dialogue tasks but enhanced traditional multimodal tasks. These results validate the adaptability of our method in unlocking chats across multiple images for multimodal instruction-following models with minimal additional training cost.\n\n\n**5: Choice of Evaluation Dataset such as BISON and NLVR2**\n\nNLVR and BISON were selected for their use of natural images and the feasibility of automatic evaluation with unambiguous answers (e.g., TRUE/FALSE, single-choice problems). Based on your feedback, we aim to include broader evaluation benchmarks, incorporating images from diverse domains (e.g., documents, fashion, cartoons) and a wider range of evaluation metrics beyond accuracy measures."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733857416,
                "cdate": 1700733857416,
                "tmdate": 1700733857416,
                "mdate": 1700733857416,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WbH03mw5Tg",
            "forum": "oq5EF8parZ",
            "replyto": "oq5EF8parZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_eU8x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_eU8x"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SparklesChat, a multimodal instruction following model for open-ended dialogues across multiple images. This is MiniGPT4 fine-tuned on the machine-generated dialogue dataset released in the paper called SparklesDialogue. This contains word-level interleaved multi-image and text interactions with up to 3 images during the first turn and 1 image during the second turn. SparklesDialogue consists of two subsets: 1) SparklesDialogueCC which contain images from CC3M and captions generated by MiniGPT4 2) SparklesDialogueVG which contain images from Visual Genome and descriptions from GPT-4, based on human-annotated captions, objects, and regions. SparklesEval is a new GPT-assisted benchmark with 150 dialogs, introduced to assess conversational competence across multiple images and dialogue turns, through criteria such as Image Understanding & Reasoning, Cross-Image & Cross-Turn Coherence, and Relevance & Completeness of Responses. SparklesChat outperforms MiniGPT-4 and gets marginally close GPT-4 on binary image selection task and the NLVR2 visual reasoning task. The paper contains ablation study on the effect of dialog turns and SparklesDialogue subsets during training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tNew dataset SparklesDialogue for word-level interleaved multi-image and text interactions\n2.\tNew benchmark SparklesEval for word-level interleaved multi-image and text interactions\n3.\tDemonstration of improved performance over MiniGPT4"
                },
                "weaknesses": {
                    "value": "1.\tSparklesDialogue contains subset SparklesDialogueVG, which was generated using GPT-4. The paper compares with performance of GPT-4 (method used to create the data set is also being evaluated on), while still performing worse although SparklesChat uses much richer image embedding. \n2.\tNo contribution in terms of novelty architecture. Main contribution is in the data set. \n3.\tOnly two turns per sample in the dataset. Longer sessions are probably more practical than more images per turn and limiting to just 2 turns. Dataset (that too, machine-generated) being the highlight of this paper, would have expected more.\n4.     Not clear how this extends to other approaches such as LLaVA. Results are shown only for Min-GPT4 extension."
                },
                "questions": {
                    "value": "Q1) Section 5.2 mentions, SparklesDialogueVG and SparklesEval use the same sources of images and captions. This is suspected to be one of the reasons why model trained on SparklesDialogueVG performs better than model trained on SparklesDialogueCC. Isnlt this a serious issue, especially since SparklesDialogueVG is claimed to be the high quality subset?\n\nMinor typo\n1.\tTable 2: Column title should be A2 under \u201cTurn two\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper used GPT4 and MiniGPT-4 to create the datasets without any human review. It is unclear how safe the dataset is. Also, not clear about RAI."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635408289,
            "cdate": 1698635408289,
            "tmdate": 1699636607893,
            "mdate": 1699636607893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "unYcnIdYa9",
                "forum": "oq5EF8parZ",
                "replyto": "WbH03mw5Tg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eU8x"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review and the valuable insights! We appreciate your recognition of our new dataset and benchmark. In response to your concerns, we would like to provide the following clarifications and address the questions raised.\n\n---\n\n**1. Comparison with GPT-4**\n\nSparklesChat underperforms compared to GPT-4, despite using richer image representations, because GPT-4 operates based on detailed image annotations and the evaluation set was generated by GPT-4. This gives GPT-4 an inherent advantage in image understanding tasks, as it does not face the same challenges in interpreting images.\n\n**2. Model Contribution**\n\nThank you for pointing out that the dataset is our main contribution. Our contribution to the model is the open-source SparklesChat to support chats across multiple images and dialogue turns. This capability has been a frequent request in issue trackers that remain unresolved since the release of popular multimodal instruction-following models such as MiniGPT-4 and LLaVA (see [issues 1](https://github.com/haotian-liu/LLaVA/issues/197), [2](https://github.com/Vision-CAIR/MiniGPT-4/issues/180), and [3](https://github.com/Vision-CAIR/MiniGPT-4/pull/232)). We believe SparkelsChat is contributing to the community by pushing the boundaries of existing architectures to address novel challenges.\n\n**3. Adapt to More than Two-Turn Dialogue**\n\nWe initially created two-turn dialogues to validate our ideas in unlocking chats across multiple images and turns, which is adaptable to more turns, as shown in Figure 8. Additionally, we acknowledge the practicality of longer sessions and plan to expand the dataset accordingly. This can be achieved by incorporating more turns of conversation based on the current two-turn dialogues or exploring new scenarios for images and text from diverse domains (e.g., documents, fashion, cartoons).\n\n**4. Extension to Other Approaches such as LLaVA**\n\nIn light of your feedback, we **extended our approach to more advanced models such as LLaVA-v1.5**to offer a broader view of our approach's effectiveness. LLaVA-v1.5 has improved its image input resolution from 224 pixels to 336 pixels. It is trained on a diverse dataset comprising about 665K instruction-following data. Additionally, LLaVA-v1.5* is fine-tuned on SparklesDialogue (about 6.5K) using the low-resource technique LoRA to conserve memory and time. The results are shown below and have **updated in the revised manuscript in Section 5.1 and Table 2.**\n\n| Model        | Instruction Data| BISON | NLVR2 | SparklesEval |\n|--------------|-----------------|-------|-------|-------|\n| GPT-4        | -               | -     | -     | 9.26  |\n| MiniGPT-4    | description     | 46.0% | 51.3% | 3.91  |\n| MiniGPT-4*   | description     | 51.3% | 46.7% | 3.50  |\n| LLaVA-v1.5   | Mixture (665K ) | 52.7% | 53.3% | 2.75  |\n| LLaVA-v1.5*  | **+SparklesDialogue**| 65.3% | 56.7% | 7.93  |\n| SparklesChat | description     | 52.0% | 48.0% | 3.06  |\n| SparklesChat | reasoning       | 52.7% | 54.0% | 6.71  |\n| SparklesChat | SparklesDialogue| 56.7% | 58.0% | 8.56  |\n\nAccording to the results, despite LLaVA-v1.5's advantages of higher image resolution (336 vs. 224 pixels) and a larger training set (665K vs. 6.5K), SparklesChat significantly outperforms LLaVA-v1.5 in three evaluation sets involving multiple images. While LLaVA-v1.5 outperforms MiniGPT-4 on BISON and NLVR2, it shows weaker results on SparklesEval. This may be due to LLaVA-v1.5's training data primarily focusing on closed-set multimodal tasks such as VQA, TextCaps, and RefCOCO, while lacking in open-ended dialogue training. After fine-tuning with our SparklesDialogue, LLaVA-v1.5* not only significantly improved in open-ended dialogue tasks but enhanced traditional multimodal tasks. These results validate the adaptability of our method in unlocking chats across multiple images for multimodal instruction-following models with minimal additional training cost.\n\n**5. Similarity of Sources in SparklesDialogueVG and SparklesEval**\n\nAlthough there are similarities in image sources between SparklesDialogueVG and SparklesEval, the test set features unique combinations (e.g., 2-2 image pairings not present in training). The test set is carefully curated to ensure diversity, allowing for a comprehensive evaluation of multi-image dialogue capabilities. We acknowledge the similarity in image sources as a concern and plan to introduce additional data sources to provide a more balanced assessment.\n\n**6. Ethics Concerns**\n\nOur dataset and evaluation process has involved initial human review, including manual initiation and selection of dialogue demonstrations and rigorous quality checks, as detailed in Section 4.1 of our paper. We acknowledge the importance of thorough human review to ensure the safety and ethical soundness of the data.\n\n**7. Correction Acknowledged**\n\nThank you for pointing out the typo. We have corrected it."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733998527,
                "cdate": 1700733998527,
                "tmdate": 1700734307560,
                "mdate": 1700734307560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "55it6TPm98",
            "forum": "oq5EF8parZ",
            "replyto": "oq5EF8parZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_viMW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_viMW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. It introduces SparklesDialogue, a specialized machine-generated dialogue dataset, and achieves superior performance compared to MiniGPT-4 on vision-language benchmarks. SparklesChat's effectiveness is further demonstrated by its high score on SparklesEval, a benchmark for assessing conversational competence across multiple images and dialogue turns."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses a key limitation in the field by introducing SparklesChat, a multimodal instruction-following model that integrates multiple images at the word level. This fine-grained integration of images and text is a novel approach that mimics natural human communication more closely.\n\n2. The paper presents SparklesDialogue, the first machine-generated dialogue dataset designed for word-level interleaved multi-image and text interactions. The dataset is constructed from different image and description sources, ensuring greater robustness and diversity. Additionally, the paper introduces SparklesEval, a comprehensive scoring system that quantitatively evaluates the model's conversational competence in multimodal, open-ended dialogues.\n\n3. The SparklesEval benchmark shows that SparklesChat's conversational competence significantly surpasses MiniGPT-4 and approaches the performance of GPT-4. These results highlight the potential of SparklesChat in real-world scenarios."
                },
                "weaknesses": {
                    "value": "Considering the current status of single-image comprehension, which still requires further advancements, it appears that addressing scenarios involving multiple images may not be an immediate priority. Additionally, when considering the data construction approach described in the paper, it becomes evident that the model's capabilities are still constrained by the limitations of single-image understanding.\n\nIn my personal opinion, focusing on improving single-image comprehension would be more beneficial at this stage. Once single-image understanding is well-established, the demonstrated ability to handle multiple images, as showcased in the paper, should not pose significant challenges. It is crucial to ensure a solid foundation in single-image comprehension before delving into more complex scenarios involving multiple images."
                },
                "questions": {
                    "value": "1. How do the Dialogue Demonstrations contribute to the data quality and diversity?\n2. Considering the impressive performance of GPT-4 with ground truth (gt) annotation, could the authors provide a baseline using a strong caption model with an instruction-tuned Language Model to address the challenges raised in the paper?\n3. Does the model in the paper have the capability to handle scenarios with more than two images, considering that the paper only showcases examples with two images?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5780/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5780/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5780/Reviewer_viMW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656640919,
            "cdate": 1698656640919,
            "tmdate": 1699636607778,
            "mdate": 1699636607778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5BoXrloGEK",
                "forum": "oq5EF8parZ",
                "replyto": "55it6TPm98",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer viMW"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and constructive review of our work. We are encouraged by your recognition of the novelty and potential of our approach. We would like to address your concerns regarding the prioritization of single-image versus multi-image comprehension and other aspects related to our dataset and model capabilities.\n\n---\n\n1. **Prioritization of Single vs. Multi-Image Comprehension**\n    - We acknowledge the importance of single-image understanding in visual comprehension. However, our focus on multi-image dialogues stems from real-world complexity, where individuals often encounter and discuss multiple images simultaneously.\n    - Since the release of existing multimodal models such as MiniGPT-4 and LLaVA, there have been requests for multi-image capabilities in issue trackers that remain unresolved (see [issues 1](https://github.com/haotian-liu/LLaVA/issues/197), [2](https://github.com/Vision-CAIR/MiniGPT-4/issues/180), and [3](https://github.com/Vision-CAIR/MiniGPT-4/pull/232)). Our work aims to address these demands with our dataset and methodology. \n    - What's more, advancing multi-image dialogue capabilities complements single-image comprehension. Tackling complex scenarios with multiple images helps uncover limitations in single-image understanding and drives the field forward.\n\n2. **Dialogue Demonstrations' Contribution to Data Quality and Diversity**: \n    - Thank you for your question! We **add examples and analysis in Appendix K**, to show that Dialogue Demonstrations act as contextual learning examples, guiding GPT-4 to produce well-formatted and diverse responses. \n    - To demonstrate this effect, we modified the original prompt used for GPT-assisted Multiple Dialogues Generation, as detailed in Table 9, by removing content relating to demonstration dialogues. We then employed the same Candidate Image Descriptions as in Figure 18 to create a new prompt and generate a response. \n    - The resulting response was inferior in quality, failing to meet the desired formatting criteria, such as assigning image IDs, specifying the number of images per dialogue turn, and incorporating new images in subsequent turns. Furthermore, the response lacked the diversity that its dialogues typically ask for more detailed descriptions of images but not specifying particular aspects. In conclusion, dialogue demonstrations are crucial not only for enhancing data quality by providing formatting guidelines but also for increasing diversity by conditioning different demonstrations.\n\n3. **Baseline with Strong Caption Model and Instruction-Tuned LM**\n   We appreciate your suggestion to consider a baseline using a strong caption model combined with an instruction-tuned language model. We analyze the advantages of multimodal models over this baseline lie in that (1) some images are challenging to describe solely through language, as visual elements can be more expressive than words, and (2) multimodal models can interpret and discuss visual elements in specific contexts. In light of your suggestion, we plan to experiment with this in future iterations to provide a more comprehensive comparison.\n\n4. **Capability with More than Two Images**:\n   - SparklesChat is capable of handling dialogues involving multiple images, not limited to two. **We include examples with four or five images in Appendix F of the revised manuscript to demonstrate this expanded capability.**\n    - To analyze SparklesChat's ability to generalize in scenarios involving a larger number of images, we consolidated several questions from a dialogue into a single query. This was done to generate responses for queries involving four or five images, as shown in **Figure 9 and Figure 10** in the updated manuscript, where the model effectively distinguishes between the images and adheres to complex queries. In the case of Figure 10, where three questions involving five images are concatenated into one query, the model tends to ignore the final question and only responds to the first two. We believe this limitation arises from the absence of similar patterns in the training data. A potential solution could involve incorporating multiple turns into each training dialogue to enhance the model's ability to handle such complex scenarios."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733710387,
                "cdate": 1700733710387,
                "tmdate": 1700733710387,
                "mdate": 1700733710387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xcA7tDthRc",
            "forum": "oq5EF8parZ",
            "replyto": "oq5EF8parZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_9M6X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5780/Reviewer_9M6X"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces SparklesChat, a multimodal instruction-tuned model designed to effectively engage in dialogues that encompass multiple images. Additionally, the constructed multi-image dialogue dataset and an evaluation benchmark are introduced."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work focuses on a new scenario that is not well-explored by current large multimodal models, i.e. multi-image multimodal dialogue. \n\nThis work propose new training data, evaluation benchmark, and model for this scenario, which exhibit better performance than MiniGPT-4."
                },
                "weaknesses": {
                    "value": "**1. The data construction process seems too trivial and not sound.** \n\nIn the data construction process to generate visual dialogue with multiple images, you provide multiple image-text pairs and ask GPT-4 to link them together, which I think is the simplest way to construct multi-image dialogues. \n\nBesides, this simple approach fails to yield effective samples. In Figure 3, the response from GPT-4 seems too naive, *i.e.*, in image #1, we see ..., in image #2, we witness..... This is just a concatenation of descriptions of two images.\n\n**2. Insufficient experiments.**\n\nI think current experiments cannot form a strong foundation to support the effectiveness of your model and training data.\n\n* Baselines. You compare your method only with MiniGPT-4, which in my understanding is an embarassingly weak and simple model & dataset. More comparisons are definitely needed.\n\n* Evaluation benchmarks. You use three benchmarks for evaluation, BISON, NLVR2, and your own evaluation data. Among them, BISON and NLVR2 are not commonly used benchmarks now. Besides, on your own evaluation data, you claim your performance apporach GPT-4. However, your self-constructed training data could share similar distribution to you eval data. To this end, I think this claim cannot well establish."
                },
                "questions": {
                    "value": "More solid experiments could be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5780/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5780/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5780/Reviewer_9M6X"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754393791,
            "cdate": 1698754393791,
            "tmdate": 1699636607680,
            "mdate": 1699636607680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "juWa4Rtjjk",
                "forum": "oq5EF8parZ",
                "replyto": "xcA7tDthRc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9M6X"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We appreciate your recognition of the novelty of our scenario and our contributions to the field in introducing new training data and benchmarks. Below, we address your concerns regarding the data construction process and our experimental setup.\n\n---\n\n1. **Data Construction Process:**\n   - Our approach to generating visual dialogue with multiple images extends beyond simple image-text pair linking or descriptions. We instruct GPT-4 to simulate realistic user-assistant interactions as introduced in Section 4.1, and not to directly describe images with the prompt *\"Please make sure not to reveal the content of the images or describe the images in the user messages\"* as shown in Table 8. Thus, the generated dialogues are not mere concatenations of descriptions but are contextually rich and coherent conversations. \n   - Sometimes, a detailed image description is part of the image understanding and reasoning process. The case in Figure 3 is picked to show a response that clearly distinguishes between multiple images. Meanwhile, Figure 19 (original Figure 17) presents a more complex example of generating a promotional text inspired by several images but not describing them naively.\n   - The statistics in Figure 4 further illustrate the diversity of SparklesDialogue, showcasing its range from generating text materials to seeking advice or discussing relationships between images. This involves reasoning about the content and context of the images, connecting different visual elements, and generating responses that reflect a deep understanding of the images about the user's queries.\n\n2. **Expanded Experiments and Benchmarks:**\n\n    Thanks for your constructive suggestion! \n\n    **Expanded Baselines.** SparklesChat is built upon the MiniGPT-4 and adapted to accommodate multiple images due to its relevance and prominence in the field. We **extended our approach to more advanced models such as LLaVA-v1.5** to offer a broader view of our approach's effectiveness. LLaVA-v1.5 has improved its image input resolution from 224 pixels to 336 pixels. It is trained on a diverse dataset comprising about 665K instruction-following data. Additionally, LLaVA-v1.5* is fine-tuned on SparklesDialogue (about 6.5K) using the low-resource technique LoRA to conserve memory and time. The results are shown below and have **updated in the revised manuscript in Section 5.1 and Table 2.**\n\n    | Model        | Instruction Data| BISON | NLVR2 | SparklesEval |\n    |--------------|-----------------|-------|-------|-------|\n    | GPT-4        | -               | -     | -     | 9.26  |\n    | MiniGPT-4    | description     | 46.0% | 51.3% | 3.91  |\n    | MiniGPT-4*   | description     | 51.3% | 46.7% | 3.50  |\n    | LLaVA-v1.5   | Mixture (665K ) | 52.7% | 53.3% | 2.75  |\n    | LLaVA-v1.5*  | **+SparklesDialogue**| 65.3% | 56.7% | 7.93  |\n    | SparklesChat | description     | 52.0% | 48.0% | 3.06  |\n    | SparklesChat | reasoning       | 52.7% | 54.0% | 6.71  |\n    | SparklesChat | SparklesDialogue| 56.7% | 58.0% | 8.56  |\n\n    According to the results, despite LLaVA-v1.5's advantages of higher image resolution (336 vs. 224 pixels) and a larger training set (665K vs. 6.5K), SparklesChat significantly outperforms LLaVA-v1.5 in three evaluation sets involving multiple images. While LLaVA-v1.5 outperforms MiniGPT-4 on BISON and NLVR2, it shows weaker results on SparklesEval. This may be due to LLaVA-v1.5's training data primarily focusing on closed-set multimodal tasks such as VQA, TextCaps, and RefCOCO, while lacking in open-ended dialogue training. After fine-tuning with our SparklesDialogue, LLaVA-v1.5* not only significantly improved in open-ended dialogue tasks but enhanced traditional multimodal tasks. These results validate the adaptability of our method in unlocking chats across multiple images for multimodal instruction-following models with minimal additional training cost.\n\n    **Evaluation Benchmarks.** Our SparklesEval benchmark is designed to evaluate open-ended dialogues involving multiple images and turns, using LLMs as judges. Additionally, NLVR and BISON were specifically chosen for their use of natural images and the feasibility of automatic evaluation with unambiguous answers, such as TRUE/FALSE or single-choice problems. In response to your suggestion, we are expanding our experimental framework to include broader evaluation benchmarks, incorporating images from diverse domains (e.g., documents, fashion, cartoons) and a wider range of evaluation metrics beyond accuracy measures. Additionally, we plan to diversify SparklesEval with different distributions of images and text to reduce potential biases."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733693207,
                "cdate": 1700733693207,
                "tmdate": 1700733693207,
                "mdate": 1700733693207,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]