[
    {
        "title": "MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data"
    },
    {
        "review": {
            "id": "pUJwSD2x5F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7570/Reviewer_8h9M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7570/Reviewer_8h9M"
            ],
            "forum": "lNZJyEDxy4",
            "replyto": "lNZJyEDxy4",
            "content": {
                "summary": {
                    "value": "This paper adapts masked cell modelling for anomaly detection in tabular data, with learnable and ensembled masks for further performance improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "MLM for anomaly detection is relatively under-explored and for tabular data it is a novel avenue for research. \n\nThe results are promising and comprehensive, including the ablation study."
                },
                "weaknesses": {
                    "value": "Some aspects of the methodology and decisions are not entirely clear. Please see questions for further details."
                },
                "questions": {
                    "value": "What is the importance of using soft masks instead of binary masks? It appears that data transformation through soft masking could cause the sample to break from the normal patterns and actually be an anomaly itself, but be treated like a normal sample in the training process. Could this hinder model training?\n\nWhy is learnable masking so important for AD, when random masks are fine for other tasks?\n\nThe batch size is very large, and is an important parameter as it affects the dimensionality of M. How does the methodology perform with a smaller batch size? Also, can the methodology still work if the batch size changes between training and test sets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Reviewer_8h9M"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697382905458,
            "cdate": 1697382905458,
            "tmdate": 1699636916948,
            "mdate": 1699636916948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P7IcM3nH5m",
                "forum": "lNZJyEDxy4",
                "replyto": "pUJwSD2x5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8h9M"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and address your concerns as follows:\n\n**Q1**: What is the importance of using soft masks instead of binary masks? It appears that data transformation through soft masking could cause the sample to break from the normal patterns and actually be an anomaly itself, but be treated like a normal sample in the training process. Could this hinder model training?\n\n**A1**: It worth noting that the anomaly score of one sample is the average reconstruction error of its each masked version. So the masked data itself is not important, it can be of any pattern, we only focus on whether the masked data can be reconstructed well. Similarly, the model is trained to generate masks under which the masked data can be reconstructed well for normal data. Therefore, under generated masks, normal data can be reconstructed better than anomalies, such that anomalies can be detected. To sum up, the pattern of masked data will not hinder model training. When applying soft masks, the model can not only choose which features to mask, but also the degree of masking. This brings more flexibility for the model and is conductive to learning diverse and optimal masks under which the masked normal data can be reconstructed better than anomalies.\n\n\n\n**Q2**: Why is learnable masking so important for AD, when random masks are fine for other tasks?\n\n**A2**: This is mainly due to the difference in **data type** and  **the purpose of masking**. In CV domain, a simple random masking strategy with a high masking ratio works quite well [1]. Because all the features of image data are pixels, which are **homogeneous and highly relevant**, no matter where to mask, predicting a large proportion of masked patches with just a small proportion of unmasked patches is meaningful for the model to **understand an image**, such that useful representations for downstream tasks can be learnt. On the contrary, the features of tabular data are **heterogeneous**, each feature has a different meaning, some of them are totally **irrelevant**. Our purpose is to **capture intrinstic correlations** in normal data by finding using which unmasked features can reconstruct the left masked features well. Random masking strategy is easy to produce meaningless masks for this purpose. But by learnable masking strategy, the mask generator can generate optimal masks for our purpose. Thus, correlations that can characterize normal data will be captured and anomalies can by judged by whether deviating from such correlations.\n\n[1] Masked autoencoders are scalable vision learners. CVPR 2022\n\n\n\n**Q3**: The batch size is very large, and is an important parameter as it affects the dimensionality of M. How does the methodology perform with a smaller batch size? Also, can the methodology still work if the batch size changes between training and test sets?\n\n**A3**: In our method, there are only two matrix operations involving M, the first is element-wise product between M and the original data to produce masked data, the second is the calculation of diversity loss. Both of them are calculated independently for each sample and do not involve calculations between different data. Therefore, although batch size affects the dimensionality of M, it does not affect the overall time and complexity of matrix operations. So a large batch size is quite acceptable, especially considering that the dimensions of tabular data are mostly relatively low. However, we still supplement the parameter sensitivity experiments for batch size in **Table 12** and **Table 13** in the latest version. As is shown, the average AUC-PR on the 20 benchmark datasets with batch size of **{64, 128, 256, 512}** are **{0.7052, 0.7108, 0.7155, 0.7486}**. The performance does decrease as the batch size decreases, but even with a batch size of 64, our method can still surpass all  the compared methods and achieve SOTA performance. Moreover, as we illustrated above, our method is calculated for each data separately, so the batch size of the test set can be any value. Of course, it can also be different from the batch size of the training set, and it will not affect the final results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205348298,
                "cdate": 1700205348298,
                "tmdate": 1700205348298,
                "mdate": 1700205348298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xX9BL1M4gI",
            "forum": "lNZJyEDxy4",
            "replyto": "lNZJyEDxy4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7570/Reviewer_yu8Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7570/Reviewer_yu8Y"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a new method for anomaly detection in tabular data. The proposed method detects anomalies based on the reconstruction error of appropriately trained autoencoder. In the proposed training procedure, a tabular input is first masked according to diverse masking strategies to produce a batch of masked inputs. Diverse masks are produced by a set of models trained to output masks of high variety. The obtained masked input is then fed to an autoencoder which reconstructs the initial input. The corresponding optimization objective consists of a reconstruction loss and a loss which enforces the diversity between the generated masks. During inference, inputs are marked as anomalies if they are unsuccessfully reconstructed which is indicated by the high reconstruction loss. The proposed method achieves strong experimental results under considered setups which include real and synthetic test anomalies.\n\nThe manuscript claims the following contributions:\n\nC1. Masked modeling framework for anomaly detection in tabular data.\n\nC2. Masking strategy which captures underlying high-level correlations in the normal data.\n \nC3. Strong experimental results on tabular data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The manuscript is well-written and easy to follow.\n\nS2. Masked modeling for tabular data makes sense. \n\nS3. The proposed data-dependent masking strategy outperforms other more naive masking strategies.\n\nS4. The presented empirical results are indeed strong and ablations are extensive."
                },
                "weaknesses": {
                    "value": "W1. Diversity loss (Eq. 5) has log-sum-exp form. The motivation for this design choice is not clear since other formulations may also be effective. Also, ablating different designs of diversity loss are missing."
                },
                "questions": {
                    "value": "Q1. Are there any limitations of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Reviewer_yu8Y"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698412827045,
            "cdate": 1698412827045,
            "tmdate": 1699636916840,
            "mdate": 1699636916840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5xyNfxaZcQ",
                "forum": "lNZJyEDxy4",
                "replyto": "xX9BL1M4gI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yu8Y"
                    },
                    "comment": {
                        "value": "We appreciate the kind and constructive comments and answer your question as follows:\n\n**Q1** Are there any limitations of the proposed method?\n\n**A1**: In the current field of deep learning, most attention is paid to general large models that can be trained on various datasets and applied to downstream tasks. Although our network architecture and parameters almost do not need to change for different datasets, our method still requires training a model from scratch for each dataset, which can not combine knowledge from different datasets. In the future, we will try to change our network to a Transformer-like architecture, train one general model to learn knowledge from different datasets, and address various tabular AD tasks of different dimensions and areas by just using one model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205304691,
                "cdate": 1700205304691,
                "tmdate": 1700205304691,
                "mdate": 1700205304691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T6Grge2ikl",
                "forum": "lNZJyEDxy4",
                "replyto": "5xyNfxaZcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7570/Reviewer_yu8Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7570/Reviewer_yu8Y"
                ],
                "content": {
                    "title": {
                        "value": "Weaknesses not addressed?"
                    },
                    "comment": {
                        "value": "The authors still did not respond to (W1.)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640077171,
                "cdate": 1700640077171,
                "tmdate": 1700640077171,
                "mdate": 1700640077171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UJiVyfydMM",
            "forum": "lNZJyEDxy4",
            "replyto": "lNZJyEDxy4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7570/Reviewer_cK19"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7570/Reviewer_cK19"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for improving anomaly detection via self-supervised learning with masking modeling for tabular data. The key idea is to train a network on the clean input to generate the mask, and then apply the mask into these inputs before feeding them through the auto-encoder to reconstruct the clean inputs. The mask matrices are designed as soft masks via sigmoid function. To avoid the collapse of the generator in generating trivial and redundant mask matrices, the authors regularize the reconstruction loss with diversity loss which penalizes the masking matrices to be far away. The experiments are conducted with 20 benchmark datasets and compared with 9 baselines. The experimental results are encouraging compared to baselines on benchmark datasets. The paper also shows a number of ablation studies to understand the impacts of each individual proposed component and of masking strategy, and how it detects different types of anomalies. In addition, the authors also perform the analysis on robustness of number of masking matrices and contamination and some other studies relating to the feature correlation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is sensible and novel to me and the results are quite encouraging. The experiments and ablation study are quite extensive. Potentially, this idea of learning the masking matrices might be applicable to other domains although there might be some difficulties regarding computation when dealing with higher-dimensional data."
                },
                "weaknesses": {
                    "value": "In Table 2, the ensemble appears not to contribute a lot to performance. Would it be beneficial to have another option similar to Task E but Ensemble is turned off?\n\nTable 3, have the authors studied the impact of parameter $\\lambda$ on the overall performance?\n\nIt would be beneficial to show the mask of normal data as well would be beneficial in Fig. 5, and also the variety of masks generated by the mask generator?\n\nWould it be beneficial to investigate the model parameters and latency of training/inference of compared methods? The diversity loss is useful but the computation cost is the concern as it has to deal with matrices. Has the author studied the scalability of the proposed method in terms of data dimension, e.g., how the proposed method handles with tabular data scaled at very high dimension?\n\nWhat does x-axis represent in Fig. 6?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7570/Reviewer_cK19"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786764057,
            "cdate": 1698786764057,
            "tmdate": 1699636916731,
            "mdate": 1699636916731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5n7SczvBDO",
                "forum": "lNZJyEDxy4",
                "replyto": "UJiVyfydMM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cK19 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and address your concerns as follows:\n\n **Q1**: In Table 2, the ensemble appears not to contribute a lot to performance. Would it be beneficial to have another option similar to Task E but Ensemble is turned off? \n\n **A1**: **First**, diversity loss is used to encourage the diversity of different masks, when ensemble is turned off, diversity loss is useless because there is only one mask. Therefore, this option is the same as Task C. **Second**, compared to Task C, Task D applies ensemble learning but contributes little, which suggests that simply increasing the number of masks doesn\u2019t work, as the model tends to generate redundant masks. **Third**, compared to Task C, Task E applies ensemble learning with diversity loss and achieves a remarkable performance boost. This illustrates the significance of the interaction between ensemble and diversity, which is necessary to capture diverse correlations that can characterize normal data better.\n\n \n\n**Q2**: Table 3, have the authors studied the impact of parameter \u03bb on the overall performance?\n\n**A2**:  In the original version, we have studied the impact of \u03bb on several datasets, but have not studied the overall performance. We agree with you and add the experiments with \u03bb taking **1, 5, 10, 20, 50** and **100** on all the datasets to study the impact of \u03bb on the overall performance. The average AUCPR are **0.6928, 0.7105, 0.7135, 0.7255, 0.7089** and **0.7062**, respectively. The detailed results of each dataset are provided in **Table 10** and **Table 11** of the Appendix in the new version. These results show that our method is not sensitive to \u03bb, a fixed value of 20 can achieve superior performance and outperform other masking strategies in Table 3 by a large margin. \n\nMoreover, in comparison to that \u03bb is adjusted individually for each dataset, the average AUCPR of a fixed \u03bb only decreased by 0.023, which still surpasses all baselines. Considering the only two parameters tuned for different datasets in our method are learning rate and \u03bb, above results further prove that our method is very insensitive to parameters. \n\n\n\n**Q3**: It would be beneficial to show the mask of normal data as well in Fig. 5, and also the variety of masks generated by the mask generator?\n\n**A3**: Thank you for your suggestion. We add the masks of one normal sample in **Fig.9**. Moreover, to show the variety of masks generated by the mask generator, we select one sample in Breastw dataset and draw the 15 masks generated when training with and without diversity loss. As shown in **Fig.10**, without diversity loss, more than half masks are identical and redundant, other masks are also relatively similar to each other. Differently, when applying diversity loss, the diversity of generated masks is significantly increased."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205190486,
                "cdate": 1700205190486,
                "tmdate": 1700205190486,
                "mdate": 1700205190486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DLbIJMkOCt",
                "forum": "lNZJyEDxy4",
                "replyto": "UJiVyfydMM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7570/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cK19 (Part 2)"
                    },
                    "comment": {
                        "value": "**Q4**: Would it be beneficial to investigate the model parameters and latency of training/inference of compared methods? The diversity loss is useful but the computation cost is the concern as it has to deal with matrices. Has the author studied the scalability of the proposed method in terms of data dimension, e.g., how the proposed method handles with tabular data scaled at very high dimension?\n\n**A4**: **First**,  we conduct experiments on optdigits dataset which contains 5216 samples of 64 dimensions, and compare our method with two latest self-supervised learning based methods NeutralAD [1] and ICL [2]. The parameters of NeutralAD, ICL, and our proposed MCM are **0.30M, 0.02M, and 0.41M**. The time required to train one epoch is **0.01s, 3.51s, and 3.76s**, while the inference times are **0.79s, 0.28s, and 0.20s**, respectively (on one Tesla V100 GPU). As is shown, these three models have their own advantages, our method performs better in inference time, which is more critical in practical applications. It is also worth mentioning that in the current large models era, compared to mainstream models in the fields of NLP and CV (such as 340M BERT-L and 307M ViT-L), the parameters and training/testing time of our model are several orders of magnitude smaller, and the differences between methods in our area are quite marginal.\n\n**Second**, we use $n$, $t$, $d$ to denote batch size, the number of masks, and data dimension. Although diversity loss has to deal with matrices, it calculates the similarity between masks of each independent data and does not involve calculations between different data. The complexity of one similarity calculation is $O(d)$. For each data, $t(t-1)/2$ times will be calculated. So the computational complexity of diversity loss is $O(n$$t^2$$d)$, which is linear with the data dimension $d$ and the cost is affordable with the increasing of $d$. \n\n**Third**, we generate some data with dimensions ranging from 10 to 10,000 to evaluate the scalability of the proposed method and add the results in **Table 16**. This table reveals two important points: (i) For the dimensions in the range of 10-1000, where the vast majority of tabular data belongs to, the growth of time with dimensions is lower than the linear growth rate. (ii) When dealing with data of 10,000 dimensions, although this dimension has far exceeded the general tabular data, the training and inference times for a batch are less than 1s (on one Tesla V100), which is quite acceptable. The reason is that except for the input and output layers, our network architecture does not change for data of different dimensions. Therefore, the time only increases in the first and the last layers as the dimension increases.\n\n[1] Neural transformation learning for deep anomaly detection beyond images. ICML 2021 \n\n[2] Anomaly detection for tabular data with internal contrastive learning. ICLR 2022\n\n \n**Q5**: What does the x-axis represent in Fig. 6?\n\n**A5**: In **Fig.6**, the x-axis represents the value of the feature, and we have added this information to Fig.6 in the new version. Thanks for the heads up."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205258777,
                "cdate": 1700205258777,
                "tmdate": 1700673451450,
                "mdate": 1700673451450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]