[
    {
        "title": "Stochastic Unrolled Federated Learning"
    },
    {
        "review": {
            "id": "OecoxqMxDt",
            "forum": "68k0KcHFrW",
            "replyto": "68k0KcHFrW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6498/Reviewer_DuLq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6498/Reviewer_DuLq"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Stochastic UnRolled Federated learning (SURF), a novel approach that applies algorithm unrolling, a learning-based optimization paradigm, to the server-free federated learning scenario. The authors aim to leverage these benefits to address the challenges faced by low-end devices in collaborative deep model training. The paper identifies two main challenges in applying algorithm unrolling to federated learning: the necessity of feeding whole datasets to unrolled optimizers and the decentralized nature of federated learning. The authors propose solutions to these challenges by introducing stochastic mini-batches and a graph neural network (GNN)-based unrolled architecture, respectively. The stochastic mini-batches address the data feeding issue, while the GNN-based architecture preserves the decentralized nature of federated learning. The authors also provide theoretical proof of the convergence of their proposed unrolled optimizer and demonstrate its efficacy through numerical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Originality: The paper introduces a novel approach. Algorithm unrolling is a learning-based optimization paradigm where iterative algorithms are unfolded into trainable neural networks, leading to faster convergence. Federated learning, on the other hand, is a distributed learning paradigm where multiple devices collaboratively train a global model. The originality of the paper lies in its integration of these two concepts, addressing specific challenges in server-free federated learning such as the need for whole datasets in unrolled optimizers and the decentralized nature of the learning process.\n\n2. Clarity: The paper is well-structured and presents its ideas in a clear and concise manner. \n\n3. Algorithm Simplicity and Neatness: Despite addressing complex challenges in federated learning, the algorithm proposed in the paper is simple and neat. The use of stochastic mini-batches and a GNN-based architecture provides a straightforward yet effective solution. The simplicity of the algorithm makes it accessible and easy to implement."
                },
                "weaknesses": {
                    "value": "1. Vulnerability of Assumption 1: The paper assumes convexity in its problem formulation, which might not align with the real-world scenarios where deep learning models, predominantly used in Federated Learning (FL), are non-convex. This assumption is quite vulnerable as it oversimplifies the complexity of the learning models, potentially leading to over-optimistic results and conclusions. In practice, dealing with non-convex optimization problems is more challenging, and the algorithms need to be robust enough to handle such complexities.\n\n2. Practicality of Assumption 2: The assumption that  g=f and g=\u2223\u2223\u2207f\u2223\u2223 (f=\u2223\u2223\u2207f\u2223\u2223) is very rare to satisfy in real-world applications. These conditions impose strict requirements on the relationship.\n\n3. Local Minima and Convergence: In non-convex optimization problems, the paper should consider replacing the goal of reaching local minima with finding stationary points, which are points where the gradient is close to zero. This adjustment would provide a more accurate representation of the convergence behavior in non-convex settings, since two neural nets are involved.\n\n4. Heterogeneity of Local Models and Fair Comparison: The paper adopts the heterogeneity of local models and data distribution in federated learning settings. However, the comparison of SURF with FedAvg-type methods might not be entirely fair due to this heterogeneity. To address this issue, the paper should conduct more extensive experiments, comparing SURF with a broader range of personalized federated learning methods that are designed to handle heterogeneity more effectively. Some of the methods that could be considered for comparison:\n\npFedMe: Personalized Federated Learning with Moreau Envelopes Dinh et al., 2020\nPerFedAvg: Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach Fallah et al., 2020\nAPFL: Adaptive Personalized Federated Learning Deng et al., 2020\nDitto: Fair and Robust Federated Learning Through Personalization Li et al., 2022\nMobilizing Personalized Federated Learning in Infrastructure-Less and Heterogeneous Environments via Random Walk Stochastic ADMM\n, Parsons et al., 2023"
                },
                "questions": {
                    "value": "1. Given that the assumption of convexity might not hold in many real-world deep learning scenarios, how does this affect the applicability of SURF, and are there plans to extend SURF to non-convex settings?\n2. How can we ensure that the conditions g=f and g=\u2223\u2223\u2207f\u2223\u2223 are met?\n3. How much does the heterogeneity of local models and data distribution in federated learning environments affect the performance of SURF?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6498/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437659413,
            "cdate": 1698437659413,
            "tmdate": 1699636728687,
            "mdate": 1699636728687,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8scyjxVORg",
                "forum": "68k0KcHFrW",
                "replyto": "OecoxqMxDt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful review, and for their positive evaluation of our work. We would like to address the reviewer's comments and questions combined as follows:\n\n1. We thank the reviewer for their deliberate reading of our paper and theoretical analysis. The reviewer is indeed correct about the fact that the convexity assumption does not hold in practice. We acknowledge that we made an unintentional mistake in our write-up. Assumptions 1-5 are inherited from the constrained learning theory (Theorem 1 in [1]). This theorem requires strong duality of a version of (SURF) that learns a continuous function $\\Phi$ instead of the parameterized one. The strong duality holds if $f$ is convex. Another sufficient conditions for the strong duality to hold is by satisfying Assumptions 3 and 5, as proved in Proposition 3.2 in [1]. In our case, $f$ is definitely non-convex and therefore we had Assumptions 3 and 5 in place. These two assumptions are mild and hold for modern DL models.  We have corrected the mistake in Assumption 1 in our revised manuscript and we thank the reviewer again for their deliberate reading of our paper.\n3. Assumption 2 identifies the sample complexity required to approximate the actual expectations with sample averages in both the objective function and the constraints of (SURF). Therefore, Assumption 2 only suggests that we need $Q$ realizations to approximate both $\\mathbb{E}[f]$ and $\\mathbb{E}[|\\nabla f|]$ with at least $\\zeta$ precision where $\\zeta$ monotonically decreases with $Q$. This is a typical assumption in deep learning. **We, therefore, do not require $f = \\|\\nabla f\\|$**. We meant that the sample complexity holds for the two cases. To prevent this confusion, we split this assumption into two parts, each of which handles one case separately.\n4. We agree with the reviewer. The reviewer's suggestion is exactly what we propose in our paper. We force the unrolled network to converge to a stationary point by imposing the descending constraints in (SURF). These constraints force the gradients to decrease over the layers. We acknowledge that the notation in Theorem 2 would imply otherwise. We have adjusted our notation in the revised manuscript to make it clear that we converge to a region where the gradient of $f$ is upper bounded by a small value that is close to zero. This definitely implies that we converged to a stationary point. \n5. While related, our method does not directly belong to the personalized FL class of algorithms. The (FL) problem we solve imposes consensus constraints between the agents. These constraints force all the agents to learn the same model despite the disparity between the agents. DGD was proved in [2] to converge to a solution where all $w_i$'s coincide. Unrolled DGD that we propose is an accelerated version of DGD that can converge to the same solution in less iterations. Therefore, without additional changes, we do not expect our method to be able to compete with personalized FL methods over performance measures. We emphasize that the advantage of our method is in the fast convergence and not the performance. Indeed, The FL baselines depicted in Figure 1 will reach the same performance level of SURF eventually when we run more iterations. Therefore, we believe that comparisons with personalized FL is not fair to our method. Having said that, we agree that heterogeneity is a challenging  issue in FL that we aim to consider in a future work.\n\n[1] L. Chamon et al., Constrained learning with non-convex losses, IEEE Transactions on Information Theory, 2022.\n[2] A. Nedic et al., Distributed subgradient methods for multi-agent optimization, IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6498/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520811749,
                "cdate": 1700520811749,
                "tmdate": 1700520811749,
                "mdate": 1700520811749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aytQXxl8Ci",
            "forum": "68k0KcHFrW",
            "replyto": "68k0KcHFrW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6498/Reviewer_WDdi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6498/Reviewer_WDdi"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework named SURF, focusing on stochastic algorithm unrolling in federated learning contexts. The authors specifically employ descending constraints on the outputs of unrolled layers to make sure convergence. They also leverage the Lagrangian dual problem for optimization, with empirical validation on Graph Neural Networks (GNN)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The SURF framework stands out for its innovative method of implementing stochastic algorithm unrolling in federated learning. This novel approach, particularly the use of duality and gradient descent ascent in solving the Lagrangian dual problem, is a significant departure from traditional federated learning methodologies.\n2. The paper provides a mathematical analysis of the convergence bound of SURF, indicating thorough theoretical underpinning. Also, the key technique of imposing descending constraints on the outputs of the unrolled layers to ensure convergence appears novel to me."
                },
                "weaknesses": {
                    "value": "1. **Strong Assumptions**: The assumption of convexity in Assumption 1 is a significant limitation, given that many real-world scenarios involve non-convex functions. This assumption could restrict the applicability of the SURF framework in broader federated learning contexts.\n\n2. **Lack of Comparative Analysis**: The paper does not provide an upper bound for the number of communication rounds needed to converge to a certain precision $\\varepsilon$. This omission makes it difficult to compare SURF with other federated learning works, raising questions about the significance and practicality of the contribution."
                },
                "questions": {
                    "value": "1. In the (SURF), there is no explicit representation of $\\mathbf{W}_L = \\boldsymbol{\\Phi}(\\boldsymbol{\\vartheta}; \\boldsymbol{\\theta})$. Is this an intentional choice?\n2. What is the complete formulation of the function $f$ in Assumption 2? Since the parameter of $f$ seems to depends not only on $\\theta$ but also on other factors like $l, w_0$ etc., a clear definition is necessary.\n3. Given that the fomula in (5) is based on expectations without explicit randomness, why does Theorem 2 require that (5) holds with a certain probability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6498/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614264555,
            "cdate": 1698614264555,
            "tmdate": 1699636728559,
            "mdate": 1699636728559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1vSTmkrTYO",
                "forum": "68k0KcHFrW",
                "replyto": "aytQXxl8Ci",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful review and suggestions, and for their positive evaluation of our work.\n\n1. We thank the reviewer for their deliberate reading of our paper and theoretical analysis. The reviewer is indeed correct about the fact that the convexity assumption does not hold in practice. We acknowledge that we made an unintentional mistake in our write-up. Assumptions 1-5 are inherited from the constrained learning theory (Theorem 1 in [1]). This theorem requires strong duality of a version of (SURF) that learns a continuous function $\\Phi$ instead of a parameterized one. The strong duality holds if $f$ is convex. Another sufficient condition for the strong duality to hold is by satisfying Assumptions 3 and 5, as proved in Proposition 3.2 in [1]. In our case, $f$ is definitely non-convex and, therefore, we had Assumptions 3 and 5 in place. These two assumptions are mild and hold for modern deep learning models. We have corrected the mistake in Assumption 1 in our revised manuscript and we thank the reviewer again for their deliberate reading of our paper.\n3. We also would like to thank the reviewer for this insightful suggestion. We addressed this point in Theorem 3. The new theorem finds an upper bound of the gradient norm after a finite number of layers $L$. This allows us to find an upper bound on the number of layers required to achieve certain precision. This upper bound is derived in Appendix A.3.\n\nQuestions:\n1. The constraints in (Optimizer) are *implicit* constraints that are forced in the design of the unrolled netowrk $\\Phi$. We wrote them explicitly in (Optimizer) only to help the comprehension of the idea behind unrolling. Observe that the two constraints were omitted from the Lagrangian function in (1).\n2. We clarified Assumption 2 in our revised manuscript.\n3. Theorem 2 does not require (5) to hold with probability. It is Theorem 1 that only guarantees that (5) holds with probability. Therefore, we had to incorporate this induced randomness in Theorem 2.\n\n[1] L. Chamon et al., Constrained learning with non-convex losses, IEEE Transactions on Information Theory, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6498/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520526501,
                "cdate": 1700520526501,
                "tmdate": 1700520526501,
                "mdate": 1700520526501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xkrNLMPXMN",
            "forum": "68k0KcHFrW",
            "replyto": "68k0KcHFrW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6498/Reviewer_94iK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6498/Reviewer_94iK"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to accelerate FL convergence in a server-less setting. This is achieved via incorporating descending constraints on unrolled architectures. The proposed approach SURF is theoretically (Theorem 2) and empirically (Figure 2) substantiated. These findings demonstrate that an unrolled optimizer trained with SURF converges to a region close to optimality, ensuring its ability to generalize effectively to datasets within the distribution."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and the problem is well motivated. I find the descent constraints to arrive at a convergence guarantee very clever. As far as I'm aware, this method is novel, although I am not quite familiar with the L2O/unrolled algorithm literature so I can't say for sure.\nExperiments are quite basic, but show some promising results."
                },
                "weaknesses": {
                    "value": "There have been some existing works on serverless FL. For example, I find the following paper \"FedLess: Secure and Scalable Federated Learning Using Serverless Computing\" (Grafberger et al., 2021). I would suggest the authors to compare to some of these methods rather than standard FL approaches.\n\nI also do not understand how the SURF method is limited to serverless FL. Can it be applied to standard FL instead? \n\nIt feels quite strange seeing that the accuracy curves of all other methods are very similar. Could it be due to this setting?\n\n I think Fig. 2 does not say anything about your convergence. What do accuracy and loss value at one point have to do with convergence guarantee? If anything, it would be Fig. 1, but it feels quite amazing to achieve perfect accuracy on CIFAR10 with only 20 training epochs. Can you please elaborate on what is happening in one communication round here?\n\nSome of the experiment setup descriptions are quite vague. Could you elaborate on the following points:\n- How were the other FL baselines modified to account for the serverless setting? \n- What does it mean by \"randomly-chosen agents are asynchronous with the rest of the agents\". What is being asynchronous here, and how do you simulate it?\n- What exactly is happening in one communication round?"
                },
                "questions": {
                    "value": "I have put my concerns in question form above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6498/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700108740,
            "cdate": 1698700108740,
            "tmdate": 1699636728440,
            "mdate": 1699636728440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n8ZHAHpbLx",
                "forum": "68k0KcHFrW",
                "replyto": "xkrNLMPXMN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful review and suggestions.\n\nWe start by addressing the reviewer's comments:\n1. We thank the reviewer for bringing up this insightful point. Our method, SURF, is not limited to serverless federated learning (FL) as the reviewer suggested. Standard FL is a special case of our setting when we consider a star graph, where a central node (i.e., server) is connected to all the other nodes and no communication is allowed between non-central nodes. Inspired by the reviewer's comment, we conducted extra experiments over star graphs and added them to the Appendix. For the sake of completeness, we also attached to the Appendix additional experiments over random graphs. We observe that regardless of the graph topology, we still achieve fast convergence compared to DGD and the FL benchmarks. However, our current implementation of the star-graph experiment only considers a homogeneous case, where the central agent is similar to the other agents, i.e., it learns a model based on its local data beside to its role as a server. To implement a classic FL with SURF, our algorithm should be modified in two ways: i) a different model update should be implemented in the server node, with the server having no data and just aggregating agents\u2019 models, i.e.,\n$\n{\\bf w}_{i,l} = [{\\bf H}\\_{l}({\\bf W}\\_{l-1})]_i\n$\n(Note the difference between this update rule and the one in (U-DGD)),\nand ii) a different test pipeline should be used, where the final server model is evaluated on a central test data. \nAdditional experiments on this full classic FL setup are currently underway and we will add it to the camera-ready version of the manuscript. We expect the results to be similar to those in the star topology case, reported in Appendix B, since the difference between the two cases is subtle. We again thank the reviewer for his comment that inspired us to generalize our work to classic FL and we will update Section 5 in the camera-ready version with a new subsection on the extension to classic FL showing the update rule mentioned above.\n2. Figure 1 does not imply that the other methods are quite similar. Zooming in on the benchmark curves would show a variability of +/-3% in the performance of these methods across the communication rounds. This variability is significant in real-world applications. Therefore, the figure only implies that the variability in the performance of the FL benchmarks diminishes when compared to **our method** that **is radically different**.\n3. Figure 2 shows the effect of discarding the descending constraints. When the constraints are omitted as in standard unrolling, neither the objective/loss function $f$ nor the accuracy decreases monotonically over the layers. As shown in the figure, the final estimate has even jumped from 0% accuracy to 96% in one step raising doubts about the role of the previous layers. The figure shows that by adding the descending constraints, the estimates are **guaranteed** to descend over the layers. Figure 1, on the other hand, shows the convergence speed compared to other methods. \n4. The remarkable fast convergence is attributed to the core idea of algorithm unrolling, which is to use data to learn **accelerated** descending directions. To describe the operations in one communication round, we emphasize that each agent aggregates its $K$-hop neighbors' models into a new model and then updates it with a local mini-batch. This process resembles one layer in our unrolled model. However, aggregating the models from each hop neighbor requires one communication round and therefore one layer requires $K$ communication rounds. In summary, each agent sends and receives data from $K$-hop neighbors in $K$ communication rounds and updates their own model once at every $K$ communication rounds. It is worth noting that this update rate is lower than the ones used in FL benchmarks, which update the model at every round. The fast convergence we experience in SURF is, therefore, due to the fact that we learn from training data how to weigh the neighbors' models efficiently and how to find a sequence of descending steps that converges quickly. This is not unique to our method, SURF, as similar fast convergence results have been reported for many other applications (see our Related Work Section and the references therein)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6498/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521544214,
                "cdate": 1700521544214,
                "tmdate": 1700524896909,
                "mdate": 1700524896909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CJbxutJwUs",
                "forum": "68k0KcHFrW",
                "replyto": "n8ZHAHpbLx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6498/Reviewer_94iK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6498/Reviewer_94iK"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "Thanks for replying to my questions.\n\n- Since model synchronization is not frequent (once every K comm rounds), I suspect your framework might suffer from client drift in a heterogeneous data setting. What do you think about this?\n- I strongly believe FL without the client drift issue is just distributed learning (and in your case decentralized learning). Testing on client data simulated by a Dirichlet distribution has become a rather standard practice, so it would be quite hard to accept any FL method that did not demonstrate robustness against client drift.\n- Regarding \"The FL baselines were not modified to account for the serverless setting\" -- This is why I suggested comparing against existing works on serverless FL, but it seems like my concern was not addressed.\n\nOverall, I don't think I am ready to accept your paper due to a lack of demonstration and will most likely stay my score. I however do think that the idea is promising and hope to see a future revision of it."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6498/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582806951,
                "cdate": 1700582806951,
                "tmdate": 1700582806951,
                "mdate": 1700582806951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jXhUTvmEmi",
                "forum": "68k0KcHFrW",
                "replyto": "xkrNLMPXMN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6498/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We updated the manuscript with new experiments"
                    },
                    "comment": {
                        "value": "To follow up on our response, we have conducted the experiments under a Dirichlet distribution and compared our results to decentralized federated learning benchmarks, as requested by the reviewer. The results were added to Appendix B.3. The figure shows that our method is more robust to heterogeneity/client drift. SURF still converges fast, and the deterioration in the performance with the Dirichlet concentration parameter $\\alpha$ is much slower than all the other benchmarks. This is the case since U-DGD, during its meta-training, learns to converge faster while balancing between the aggregated models received from the agents' neighbors and the local updates. This shows another strength of our method."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6498/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670397298,
                "cdate": 1700670397298,
                "tmdate": 1700670462552,
                "mdate": 1700670462552,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]