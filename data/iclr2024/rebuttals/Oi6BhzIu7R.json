[
    {
        "title": "REAL: Rectified Adversarial Sample via Max-Min Entropy for Test-Time Defense"
    },
    {
        "review": {
            "id": "7O5gymps37",
            "forum": "Oi6BhzIu7R",
            "replyto": "Oi6BhzIu7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7541/Reviewer_7jdE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7541/Reviewer_7jdE"
            ],
            "content": {
                "summary": {
                    "value": "Improving adversarial robustness against adversarial attacks is an important but challenging task. This paper presents a critical problem that generalizing to numerous unseen adversarial attacks is difficult but paid less attention in the community, and proposes a new concept, i.e. generalizable robustness. Inspired by test-time adaptation, this paper proposes a new test-time defense methodology in robustness and overcomes the non-reasonable prediction entropy assumption in defense, and designs a two-stage rectification approach, i.e, REAL, through a max-min entropy optimization with attack-aware weighting mechanism. This submission brings some new perspectives that will promote adversarial robustness against unknown attacks. Experiments on benchmark datasets show the proposed REAL greatly improves the robustness of previous sample rectification models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The idea of REAL is interesting, rational, and novel. First, it is valuable to explore new generalizable adversarial defense approaches against unseen attacks. Second, considering that adversarial perturbations are diverse and unseen in real applications, it is rational to establish a test-time training paradigm for removing perturbations. Third, since such a paradigm is still seldom studied in this field, the proposed test-time adversarial sample rectification approach is novel in multiple aspects, which may produce a high impact in related fields.\n2. The proposed max-min entropy optimization strategy is new. The authors clearly claim that in conventional test-time adaptation for natural image classification, the entropy loss is commonly used for training unlabeled target data, and successfully reveal that this is not appropriate for adversarial samples as shown in Fig.1. Therefore, the authors propose a natural and novel idea to maximize the entropy of adversarial samples (stage 1) instead of minimization. For the final objective of accurate recognition, stage 2 is formulated for minimizing the entropy of rectified adversarial samples. The two-stage rectification paradigm achieves test-time defense on the fly.\n3. Another merit of this paper is the proposed attack-aware weighting mechanism, which is simple but useful. The intuition behind this is clear because each sample should be treated unequally due to the differences in their attacking power. This paper contributes a simple metric of attack strengths by assessing samples' prediction entropy.\n4. Experiments on benchmark datasets fully prove the superiority of the proposed REAL method, by plugging and playing in the classical SOAP model."
                },
                "weaknesses": {
                    "value": "1. Since the auxiliary task is leveraged during test-time optimization, the authors could discuss some choices of different auxiliary tasks, although one may not decide which one is better without empirical observation.\n2. Since this work aims to improve generalization, a more complete setting toward attack generalization can be implemented in the future and produce a higher impact.\n3. It is better to clarify which module is frozen and noted in Figure 2."
                },
                "questions": {
                    "value": "1. Is the code of the proposed REAL approach available? This is also important to improve the impact on the open-source community.  \n2. In Fig.2, are the C and E frozen during test-time max-min optimization? \n3. In Eq. 3b and 4b, what is \"S\" ? which is not defined. I guess this is a typo error, which may be \"C\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698460249346,
            "cdate": 1698460249346,
            "tmdate": 1699636911498,
            "mdate": 1699636911498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kdEq9GyTuc",
                "forum": "Oi6BhzIu7R",
                "replyto": "7O5gymps37",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 7jdE"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and will respond to your questions and concerns one by one.\n\n**Q1: The code**\n\n**A1:** As our paper is currently under review and considering the review process and requirements, we have not provided a code link at the moment. In the future, we plan to organize our code and make it publicly available on GitHub.\n\n**Q2: The model in test time**\n\n**A2:** In practice, our approach involves updating samples during the test time process using pre-defined fixed models. Therefore, during the test time stage, models A, C, and E have all been frozen, and only the samples are rectified using max-min entropy optimization to achieve defense objectives. The update process of sample is same as the process of generating adversarial samples, which is to update sample at the pixel level by backpropagating the gradient of a loss. And for a better understanding, we modify Fig.2 in the article and annotate the model in the testing stage.\n\n**Q3: The Eq 3b, 4b**  \n\n**A3:** Thank you for bringing up this question. There is a mistake in the formula editing in Eq.3b and Eq.4b. In fact, here S should represent A and C. To understand these formulas, $L_{\\text {mask}}$ involves two parts of the loss, namely $L_{\\text {aux}}$ and$L_{\\text {ent}}$. The computation of $L_{\\text {aux}}$ involves networks A and E, while the computation of $L_{\\text {ent}}$ involves networks C and E. Therefore, it should be represented as $L_{\\text {mask}}(x_{adv}+\\delta; A, C, E)$ in this context, and similarly in Eq4. We have corrected this error in the original text."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237472831,
                "cdate": 1700237472831,
                "tmdate": 1700237472831,
                "mdate": 1700237472831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dgNKuIKA7L",
            "forum": "Oi6BhzIu7R",
            "replyto": "Oi6BhzIu7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7541/Reviewer_nJ3p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7541/Reviewer_nJ3p"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a test-time adversarial defense that uses a combination of auxiliary task loss thresholds, entropy thresholds, and two sets of self-adversarial rectification rounds. The method is applied to adversarial defense on MNIST, CIFAR-10, and CIFAR-100, and it is observed that the method can provide robustness to an unsecured classifier."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The method investigates an ambitious and relevant task of test-time adversarial defense.\n* A wide variety of ideas from many different defenses are integrated into a novel method."
                },
                "weaknesses": {
                    "value": "* The presentation of the method and the motivation of the difference aspects is somewhat difficult to follow. \n* The primary weakness is that the method appears to be built upon a broken defense, namely the SOAP model. The work [a] reports breaking the SOAP defense using BPDA. I expect that a similar attack could be used against this method. Although this work does present an adaptive attack, from what I can tell the attack does not differentiate through the purification. BPDA provides an efficient way to do this approximately. Re-evaluation of this defense using the methodology in [a] is essential, especially given that this methodology has broken the SOAP defense this work is based on.\n* The method does not compare with recent diffusion-based purification defenses such as [b], which generally obtain stronger results than those reported in this work.\n\n[a] https://arxiv.org/pdf/2202.13711.pdf (ICML 2022)\n\n[b] https://arxiv.org/pdf/2205.07460.pdf (ICML 2022)"
                },
                "questions": {
                    "value": "Can the authors re-evaluate their defense using the adaptive attack used against SOAP in [a]?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834911921,
            "cdate": 1698834911921,
            "tmdate": 1699636911344,
            "mdate": 1699636911344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S094Dmns5x",
                "forum": "Oi6BhzIu7R",
                "replyto": "dgNKuIKA7L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your reviews and will respond to your questions and concerns one by one.\n\n**Q1:** The presentation of  method and motivation of the difference.\n\n**A1:** For a clearer understanding, we will briefly summarize the presentation of our motivation and method here.\n\n### 1. A summary of our approach\nThe core of our approach lies in exploring the properties of adversarial samples, specifically by investigating a way to utilize the entropy of adversarial sample properties\u2014the max-min entropy optimization mechanism. Additionally, we propose an attack-aware adaptive weighting scheme. We present our proposed method as a plugin that can be embedded into the auxiliary task learning framework (ATL) to achieve defense objectives by rectifying samples during test time. Additionally, due to the introduction of a max-min entropy optimization mechanism, the predicted values of samples oscillate between confident and uncertain predictions during the optimization process, making the sample optimization more challenging. Considering the potential issue of insufficient optimization and to avoid over-optimization, we propose a combination algorithm to assist in achieving more stable sample rectification.\n\n### 2. The differences from previous methods.\nIn the past, there have been methods attempting sample rectification through the ATL framework, but they predominantly focus on the design and innovation of the auxiliary network within the ATL framework and do not adequately leverage the classification information inherent in the main task. In this paper, we explore the utilization of adversarial sample entropy, enabling the extraction of classification information from the main task during the test stage, even in the absence of labels. This, in turn, assists in better sample rectification."
                    },
                    "title": {
                        "value": "To Reviewer nJ3p"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233271581,
                "cdate": 1700233271581,
                "tmdate": 1700233305930,
                "mdate": 1700233305930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Z9KqwAJlE",
                "forum": "Oi6BhzIu7R",
                "replyto": "dgNKuIKA7L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer nJ3p"
                    },
                    "comment": {
                        "value": "**Q2:** The BPDA+APGD attack effect on our method\n\n**A2:** Here, we first introduce the implementation principle of BPDA+APGD and then report the test results of BPDA+APGD.\n### 1. A brief explanation for BPDA+APGD attack\n\nWe analyze the implementation principle of BPDA+APGD in work [a]. The specific implementation of this attack can be briefly understood as follows: during the attack process, instead of using only the gradient of the model with respect to the final purified image as the update direction (sign), the average gradient of the intermediate iterations generated during the rectification process is taken. Intuitively, this directs all intermediate images towards misclassification, making the attack more effective. And in the attack, this iteration is run 1000 times.\n\n### 2. Implement and test BPDA+APGD on our method\n\nWe attempt to apply the same BPDA+APGD strategy to our rectification method following the settings in [a] and obtain the results as shown in the following table.\n\n**Overview of main experimental contents:**\n\nWe conduct attacks on both the rectification process of SOAP and our rectification method, obtaining two sets of adversarial samples denoted as `SOAP_BPDA` and `ours_BPDA`. The difference between the two sets of adversarial samples lies in attacking different rectification processes. Subsequently, we test both sets of adversarial samples using SOAP defense and our rectification method. Following the settings in [a], tests are conducted on CIFAR-10 dataset using ResNet18 architecture for label consistency auxiliary task.\n\n#Table1 BPDA+APGD on CIFAR10 \n\n|            |  SOAP_BPDA |  ours_BPDA |\n|:----------:|:----------:|:----------:|\n| NO_defense |    4.00%   |    8.00%   |\n|    SOAP    |    3.60%   | **22.00%** |\n|    ours    | **23.40%** |   18.70%   |\n\n**Performance on SOAP_BPDA adversarial examples:**\n\nFrom Table 1, it can be observed that `SOAP_BPDA` adversarial samples achieve a 4% success rate against the base model without rectification. However, after SOAP rectification defense, the test results do not improve; instead, there is a slight decrease to 3.6%. This is consistent with the findings reported in [a] and indicates that for methods utilizing auxiliary tasks for sample rectification through an Auxiliary Task Learning (ATL) framework for defense, BPDA+APGD could be a potent attack strategy.\n\nNext, we employ our rectification method to defend against the `SOAP_BPDA` adversarial samples, resulting in a test success rate of 31.6%, demonstrating improvement. This result can be considered as a manifestation of the effectiveness of our method against black-box attacks, hence the observed enhancement is a reasonable outcome.\n\n**Performance on Ours_BPDA adversarial examples:**\n\nThen, to explore a more powerful attack, we introduce a second type of adversarial samples, `ours_BPDA`, where the attack directly targets our rectification process. From Table 1, it can be observed that the `ours_BPDA` adversarial samples, without any defense and at the same iteration count of 1000, exhibit an improvement in test accuracy on the base model from 4% to 8%. This shows that it is more difficult for an attacker to completely attack our rectification process.\n\nAdditionally, when we apply rectification defense to `ours_BPDA` adversarial samples using our method, we still achieve a considerable improvement, obtaining a test accuracy of 18.7%. This implies that even if attackers have complete knowledge of our rectification process and specifically target it during the attack, our method can still provide a certain level of defense.\n\n**Cause Analysis:**\n\nAnalyzing the underlying reasons, firstly, our defense process incorporates a max-min entropy optimization mechanism, making it harder for attackers to identify the gradient direction in such attacks, thus rendering the attack more difficult. Secondly, our defense method introduces an attack-aware weighting mechanism that dynamically adjusts weights with input variations, achieving real-time adaptability during test time and making it difficult to fully trace our rectification process.\n\n**Summary:**\n\nFurthermore, we conduct experiments on CIFAR-100 dataset, and it can be observed that our method can resist the BPDA+APGD attack designed in [a] to a certain extent.\n\n#Table2 BPDA+APGD on CIFAR100 \n\n|            | SOAP_BPDA  | ours_BPDA |\n|------------|------------|-----------|\n| NO_defense | 1.70%      | 2.10%     |\n| SOAP       | 1.30%      | **9.20%** |\n| ours       | **10.00%** | 5.00%     |\n\n\\[a\\] https://arxiv.org/pdf/2202.13711.pdf (ICML2022)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234071565,
                "cdate": 1700234071565,
                "tmdate": 1700234363781,
                "mdate": 1700234363781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jdCJg52cRN",
                "forum": "Oi6BhzIu7R",
                "replyto": "RakfKyPTT1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7541/Reviewer_nJ3p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7541/Reviewer_nJ3p"
                ],
                "content": {
                    "title": {
                        "value": "New results might not be strong enough"
                    },
                    "comment": {
                        "value": "Thanks so much to the authors for their efforts to investigate stronger attacks against their model. If my understanding is correct, using a strong adaptive attack significantly lowers the reported adversarial robustness, for example from ~60% to ~20% on CIFAR-10. Is that an accurate description of the new tables? If so, I am not sure that the results of this defense are close enough to established methods like DiffPure, even if the method is much faster."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635668426,
                "cdate": 1700635668426,
                "tmdate": 1700635668426,
                "mdate": 1700635668426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tY7Xf6jd1H",
            "forum": "Oi6BhzIu7R",
            "replyto": "Oi6BhzIu7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7541/Reviewer_f1E9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7541/Reviewer_f1E9"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the properties of prediction entropy in adversarial samples and presents several strategies for adversarial defense. Specifically, it introduces a max-min entropy optimization scheme and an attack-aware weighting mechanism. The results demonstrate that these approaches are compatible with existing models and exhibit strong performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The starting point is novel and the proposed attack-aware weighting mechanism is technically sound.\n- The paper is generally well-written and easy to follow. The authors have illustrated their settings and motivations using bullet points to provide a clear understanding of their objectives."
                },
                "weaknesses": {
                    "value": "- The limitation of selecting the detection threshold and auxiliary tasks is crucial yet challenging. Besides, in numerous real-world attack scenarios, the specific attack methods are often unknown.\n- The motivation behind employing a max-min optimization scheme is unclear. Why is a mask loss necessary in this context?\n- Additionally, the experiments conducted seem insufficient, and it would be beneficial to observe more results obtained on ImageNet.\n- In the text, $L_{ent}$ and $L_{cls}$ are not consistent. \n- In the preliminary, you'd better provide more introduction and explain about the $\\delta$."
                },
                "questions": {
                    "value": "- If entropy is related to error rate, what about mutual information or signal-to-noise ratio (SNR)? Do they have a similar effect?\n- The effectiveness of the max-min optimization scheme lacks convincing evidence. Could you please include a comparison of the robust accuracy of $x_{mask}$ to support your claim further?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699461866637,
            "cdate": 1699461866637,
            "tmdate": 1699636911212,
            "mdate": 1699636911212,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SyW2324U2a",
                "forum": "Oi6BhzIu7R",
                "replyto": "tY7Xf6jd1H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer f1E9"
                    },
                    "comment": {
                        "value": "We thank you for your reviews and will respond to your questions and concerns one by one.\n\n**Q1:  the limitations of auxiliary tasks and threshold selection**\n\n**A1:**\nWe reiterate the focus of our approach: our approach focuses on exploring the utilization of adversarial sample entropy and combines it into the process of using the auxiliary network for sample rectification.\n\nFirstly, for the auxiliary task designed in the combined sample rectification process, we have maintained an open choice. Different auxiliary tasks will exhibit varying effects due to their inherent correlation with the main task. The better the correlation between the auxiliary task and the features learned by the main task, the more favorable the final rectification results will be. Our focus was not on exploring the auxiliary tasks themselves but rather on attempting to enhance the rectification effects on different auxiliary tasks by incorporating the entropy optimization mechanism we proposed.\n\nSecondly, we need to clarify the relationship between the selection of the threshold and the auxiliary task. The primary purpose of this threshold is to distinguish between clean and adversarial samples, preventing over-rectification and avoiding the issues of insufficient optimization. In the specific operation, we drew inspiration from [a], utilizing auxiliary task loss for detection to separate clean samples from adversarial samples. The reason for this approach is based on an assumption that auxiliary tasks will exhibit different characteristics on clean/adversarial samples. In our case, we further leverage this dissimilarity for detection. Furthermore, this assumption is indeed the underlying premise and foundation of all methods that utilize semi-supervised auxiliary tasks for sample purification. We believe that the principle behind this detection is reasonable.\n\nHowever, in practical applications, some auxiliary tasks may not strictly adhere to this ideal assumption, leading to the issues mentioned in the paper where some auxiliary tasks do not perform well in detection. This may be attributed to the joint learning between auxiliary and main tasks, leading to negative transfer, as mentioned in [b]. In other words, there is a conflicting optimization between the joint feature distributions learned by auxiliary and main tasks. Indeed, our experimental results confirm this observation. For instance, as shown in Table 2 and 3, when jointly training with auxiliary tasks, the classification accuracy on clean samples for the main task decreases. Moreover, the drop is more pronounced in the reconstruction task, indicating a more significant negative transfer effect from the reconstruction task to the main task due to lower correlation.\n\nThe mismatch between the auxiliary task and the main task results in a significant performance gap for samples in both tasks, thereby affecting our detection phase. However, we believe this issue is addressable. Alleviating conflicts between the main task and auxiliary task, ensuring their learning consistency, and employing learning strategies like [b] during the training phase can assist in improving our detection performance.\n\nCurrently, in the original ATL framework, we can still gain benefits by incorporating some auxiliary tasks with less apparent effectiveness, such as the reconstruction task used in the paper.\n\nFinally, we would like to provide a detailed explanation of our threshold selection strategy. In the actual selection, we follow the prior statistical data of natural clean samples. The underlying reason is that, during the training phase, our method exclusively focuses on clean samples, thus assuming that the statistical characteristics of the auxiliary task differ between clean and adversarial samples. In this case, when facing various complex unknown attacks in real-world scenarios, it does not affect the choice of the threshold.\n\nIn summary, our main focus is on exploring the utilization of adversarial sample entropy and integrating it into methods relying on auxiliary tasks for sample rectification. Some auxiliary tasks may have limitations due to their inability to well distinguish between clean and adversarial samples. We believe that this issue can be mitigated through certain ATL methods, and currently, even when the auxiliary network is limited, we can still attain some improvements.\n\n[a] [Test-time defense against adversarial attacks: Detection and reconstruction of adversarial examples via masked autoencoder](https://arxiv.org/abs/2303.12848)\n[b] [ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning](https://arxiv.org/abs/2301.12618)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206311824,
                "cdate": 1700206311824,
                "tmdate": 1700206311824,
                "mdate": 1700206311824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WLXPSZJ6K8",
                "forum": "Oi6BhzIu7R",
                "replyto": "tY7Xf6jd1H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer f1E9"
                    },
                    "comment": {
                        "value": "**Q3: more experiments**\n\n**A3:**\nDue to time and resource constraints, in order to showcase results on additional datasets, we attempted experiments on the TinyImageNet-200 dataset. TinyImageNet-200 is designed for image classification tasks. It consists of 120,000 training images, 10,000 validation images, and 10,000 test images, with each image sized at 64x64 pixels. Results are shown in the tables below.\nWe use **bold and italic** for the optimal value, **bold** for the suboptimal value.\n\n#TinyImageNet (resnet18)\n\n#TinyImageNet (resnet18)\n\n|            |      cln     |     FGSM     |     PGD      |      CW      |      DF      |\n|:----------:|:------------:|:------------:|:------------:|:------------:|:------------:|\n|    None    | **_51.01%_** |    2.00%     |    1.58%     |    0.00%     |    11.85%    |\n|  AT(FGSM)  |    29.00%    |  **13.45%**  |    12.29%    |    8.42%     |    18.19%    |\n|   AT(PGD)  |    28.49%    | **_13.70%_** |  **12.98%**  |    8.42%     |    17.96%    |\n| NO_defense |  **44.45%**  |    1.64%     |    0.00%     |    0.00%     |    12.34%    |\n|  SOAP(LC)  |    41.97%    |    7.97%     |    5.69%     | **_40.29%_** | **_40.65%_** |\n|  SOAP+ours |    30.90%    |    11.14%    | **_13.51%_** |  **39.65%**  |  **36.24%**  |\n\n##TinyImageNet (widresnet28-10)\n\n|            |      cln     |     FGSM     |      PGD     |      CW      |      DF      |\n|:----------:|:------------:|:------------:|:------------:|:------------:|:------------:|\n|    None    | **_65.89%_** |     8.96%    |     0.26%    |     0.00%    |     9.66%    |\n|  AT(FGSM)  |    50.78%    |    24.00%    |    19.97%    |    10.25%    |    24.65%    |\n|   AT(PGD)  |    49.45%    | **_22.99%_** |    19.18%    |     8.95%    |    26.87%    |\n| NO_defense |  **64.32%**  |    15.70%    |     1.12%    |     0.00%    |     9.66%    |\n|  SOAP(LC)  |    47.23%    |    21.20%    |  **25.09%**  |  **44.12%**  | **_42.88%_** |\n|  SOAP+ours |    50.80%    |  **22.55%**  | **_29.45%_** | **_44.45%_** |  **42.20%**  |\n\n**Q4: The consistency of $L_{cls}$ and  $L_{ent}$ **\n\n**A4:** Thank you for your proposal. We check the position of the two losses in the article but no inconsistencies are found. In order to avoid misunderstanding, we will re-describe the two losses here. \n\n$L_{\\text{cls}}(x, y)=-\\sum_i y_i \\log \\left(p_i\\right)$\n\n$L_{cls}$ refers to the cross-entropy loss for sample classification, calculating the correlation between the predicted values and the correct labels. We examine its occurrences in the text, and it consistently represents the cross-entropy loss for sample classification. \n\n$L_{\\text{ent}}(x)=-\\sum_i p_i \\log \\left(p_i\\right)$\n\n$L_{ent}$ , on the other hand, refers to the predictive information entropy. Higher values indicate higher predictive entropy and lower confidence in predictions, and its calculation process does not require correct labels. In the text, when computing the weighting coefficients, we normalize the information entropy ${V}_{ent}$  by dividing it by log(N) (where N is the number of classes). This normalization operation might be overlooked, leading to potential misunderstandings.\n\n**Q5: Supplement about $\\delta$**\n\n**A5:** We add $\\delta$ content in the revision.  \n\n**Q6: the effect of SNR and mutual information**\n\n**A6:** We consider the introduction of this problem to be highly meaningful. Regarding the exploration of entropy and error rate, corresponding experimental results are provided in [c]. Additionally, the calculation process of entropy is unsupervised, making it applicable in the test phase without labeled supervision. Therefore, we follow existing viewpoints and delved into the entropy properties of adversarial samples.\n\nRegarding the relationship between mutual information and error rate, mutual information is primarily used to measure the degree of mutual dependence between two random variables. The mutual information here can be seen as the correlation between features, and intuitively, correctly classified samples and misclassified samples may exhibit different properties. Currently, there is no exploration of the relationship between mutual information and error rate that I am aware of, and it is worth further investigation.\n\nFurthermore, regarding the relationship between signal-to-noise ratio (SNR) and error rate, in the field of communication systems, SNR does indeed have a certain correlation with error rates. However, when applied to the field of images, further exploration is still needed. Additionally, because the calculation process of image signal-to-noise ratio requires obtaining noise and the original image, when applied to the calculation process of adversarial samples, the corresponding participation of clean samples in supervision is needed. Therefore, its subsequent application may not be suitable for the test time stage, and careful consideration is required for its application.\n\n[c] https://arxiv.org/abs/2006.10726"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209642277,
                "cdate": 1700209642277,
                "tmdate": 1700237965922,
                "mdate": 1700237965922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]