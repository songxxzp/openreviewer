[
    {
        "title": "Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective"
    },
    {
        "review": {
            "id": "rBe8llQFQn",
            "forum": "uUELOzcTk8",
            "replyto": "uUELOzcTk8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_sPYy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_sPYy"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to generate adversarial examples while avoid compromising semantic information. To achieve this goal, they propose a semantic distance which replaces the geometrical distance. Then, they utilize the technique of Langevin Monte Carlo to search the adversarial examples. Instead of generate adversarial examples in a geometrical constraint, they transition to a trainable, data-driven distance distribution, which can incorporate personal comprehension of semantics into the model. The generated adversarial examples shown in Experiment seem more natural and untouched than the tradition PGD-generated examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem is interesting to me. Although adversarial examples are widely known as imperceptible to human eyes, some adversarial examples are vague compared to the original images. This work generates more clear adversarial examples, which are harder for humans to detect.\n* The algorithm is theoretically supported and empirically efficient."
                },
                "weaknesses": {
                    "value": "* Although the generated adversarial examples are indeed elusive to human eyes, it is unknown whether these examples are harder for the defense methods to detect.\n* It would be better if the authors can also validate the efficacy of the proposed method on larger network such as ResNet-50, and more difficult task such as CIFAR-100 and ImageNet."
                },
                "questions": {
                    "value": "What is the perturbation radius used in Figure 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698373380136,
            "cdate": 1698373380136,
            "tmdate": 1699636387535,
            "mdate": 1699636387535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ECrVc3zUaL",
                "forum": "uUELOzcTk8",
                "replyto": "rBe8llQFQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sPYy"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work. Here is our response:\n\n> some adversarial examples are vague compared to the original images. \n\nYes, indeed, this phenomenon is particularly evident in adversarially trained classifiers, which are notably challenging to attack.\n\n> ..., it is unknown whether these examples are harder for the defense methods to detect.\n\nThrough reject sampling and sample refinement, the samples presented to human annotators are already capable of deceiving the victim classifier. Due to the length constraints of our submission, we detailed reject sampling and sample refinement in Appendix D, which might have led to some misunderstanding. We apologize for any confusion this may have caused.\n\n> It would be better if the authors can also validate the efficacy of the proposed method on larger network such as ResNet-50, and more difficult task such as CIFAR-100 and ImageNet.\n\nWe have observed a notable decline in classification performance in multi-class classifiers with a large number of classes after adversarial training, and the improvement in defense is not substantial. Consequently, we did not select CIFAR-100 as our target, following the precedent set by works [1] and [2]. To showcase the capability of our method with larger images, we are currently experimenting with ResNet-50 on a restricted version of ImageNet, aligning with the methodology in [1]. Updates on the progress of this additional experiment will be provided as soon as it is complete.\n\n> What is the perturbation radius used in Figure 1?\n\nIn Figure 1, the perturbation radius ($\\epsilon$) for PGD with $L_{\\infty}$ norm are set to 0.4, 0.15, and 0.1 for MNIST, SVHN, and CIFAR10, respectively. For PGD with the $L_2$ norm, the corresponding $\\epsilon$ values are 5, 4, and 3 for the same datasets.\n\n[1] Tsipras, Dimitris, et al. \"Robustness may be at odds with accuracy.\" arXiv preprint arXiv:1805.12152 (2018).\n\n[2] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699837996631,
                "cdate": 1699837996631,
                "tmdate": 1700585636146,
                "mdate": 1700585636146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kJsjV9D4D5",
                "forum": "uUELOzcTk8",
                "replyto": "rBe8llQFQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ImageNet Update"
                    },
                    "comment": {
                        "value": "The results of the ImageNet experiment have been compiled. Kindly refer to Appendix N in the present version of the PDF, located at the document's end. As I am in the process of revising the document, I will inform you should there be any changes to this reference.\n\nPlease review the ImageNet results at your earliest convenience and do not hesitate to reach out with any questions you might have. Thank you!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584367015,
                "cdate": 1700584367015,
                "tmdate": 1700584367015,
                "mdate": 1700584367015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QMPZD3bHZm",
            "forum": "uUELOzcTk8",
            "replyto": "uUELOzcTk8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_DtN5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_DtN5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces generating adversarial examples from a probabilistic perspective. The geometric constraints of adversarial examples are interpreted as distributions, thus facilitating the transition from geometric constraints to data-driven semantic constraints. The paper introduces relevant background knowledge in detail and introduces four techniques that enhance the performance of our proposed method in generating high-quality adversarial examples.  The effectiveness of the proposed method was verified on some simple dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It provides detailed background knowledge on paper writing and is very reader-friendly.\n2. The author models adversarial examples from a probabilistic perspective and proposes an energy model-based adversarial example generation method."
                },
                "weaknesses": {
                    "value": "1.  Although the author models the generation process of adversarial examples through the energy model, there is no essential difference between the previous adversarial example generation processes (they all use gradients to find optimization targets\uff0cthe objective function used in this paper is still the C&W attack.). \n2. The method has only been verified on some small datasets( e.g., MNIST and CIFAR), and the effectiveness of the method needs to be verified on larger-scale and more complex semantic datasets(e.g., ImageNet). Although the authors discuss this weaknesses."
                },
                "questions": {
                    "value": "Some questions\uff1a\n1:  Generating adversarial samples through the energy model requires multiple data transformations. The author discussed changes in semantic distribution and geometric distribution. Can defenders train a classifier to filter adversarial samples generated based on the energy model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698485618559,
            "cdate": 1698485618559,
            "tmdate": 1699636387448,
            "mdate": 1699636387448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cCpwtd2ujF",
                "forum": "uUELOzcTk8",
                "replyto": "QMPZD3bHZm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DtN5"
                    },
                    "comment": {
                        "value": "Thank you for your review. Below is our response:\n\n> It provides detailed background knowledge on paper writing and is very reader-friendly.\n\nI am greatly pleased to hear your acknowledgment. We have put effort into making this section clear, as our goal is to bridge the gap in understanding between those focused on probabilistic modeling and those specializing in adversarial attack applications.\n\n> there is no essential difference between the previous adversarial example generation processes (they all use gradients to find optimization targets\uff0cthe objective function used in this paper is still the C&W attack.).\n\nWe respectfully disagree with this point. As detailed in formula (1), the objective function comprises two components: $\\mathcal{D}$ and $f$. While it's true that we selected $f_{CW}$ as a common choice for $f$, the unique contribution of our probabilistic approach lies in the data-driven aspect of $\\mathcal{D}$. This means that $\\mathcal{D}$ is not just a geometric distance anymore. The distance distribution $p_{\\text{dis}}$ can be modeled by any probabilistic generative model. In our case, we found that the Energy-Based Model (EBM) provides an elegant mathematical formulation, allowing us to use minus energy to represent distance. We believe we have already highlighted this point in Section 4 of our original submission.\n\nMoreover, we believe that the version you mentioned is identified as the 'probabilistic CW attack' (prob CW), wherein $\\mathcal{D}$ is the L2 norm and $f$ aligns with $f_{CW}$, as depicted in Figure 3 (c). The primary distinction between prob CW and the conventional CW attack is the implementation of Langevin dynamics for the optimization process.\n\n> The method has only been verified on some small datasets.\n\nWe have tested our proposed method on the MNIST, SVHN, and CIFAR10 datasets, which is consistent with the experimental scale in [1]. As highlighted in Section 8, attacking digit datasets, despite their lower resolution, is practically more challenging. We believe the scope of our current experiments sufficiently demonstrates the strengths of our method. The primary limitation in extending to higher resolution images stems from the capabilities of the Energy-Based Model (EBM), which we used as one possible representation of $ p_{\\text{dis}}$. It's plausible that choosing a different $p_{\\text{dis}}$ capable of handling higher resolution images could overcome this limitation. In this study, we chose EBM for its elegant mathematical form in the probabilistic context, presenting it as a foundational implementation while considering alternative $p_{\\text{dis}}$ for future research.\n\nNevertheless, we will endeavor to include higher resolution experiments in this rebuttal and will keep you updated with any new experimental results.\n\n> Can defenders train a classifier to filter adversarial samples generated based on the energy model?\n\nCertainly, if defenders know about the set of transformations $\\mathcal{T}$ beforehand, they might integrate these into their training as data augmentation. However, our study is focused on victim classifiers that have undergone standard adversarial training, a prevalent method consistent with the approach in [1]. We haven't tested attacks on classifiers trained with such transformations, operating under the assumption that defenders are not privy to the attacker\u2019s specific transformations in advance.\n\n[1] Song, Yang, et al. \"Constructing unrestricted adversarial examples with generative models.\" Advances in Neural Information Processing Systems 31 (2018)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699831486627,
                "cdate": 1699831486627,
                "tmdate": 1699831486627,
                "mdate": 1699831486627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y1NQgI4PwV",
                "forum": "uUELOzcTk8",
                "replyto": "AcPujPsPdD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_DtN5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_DtN5"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author"
                    },
                    "comment": {
                        "value": "Thanks to the author for the response,  I read the rebuttal and some of my concerns were addressed. However, It is unacceptable that an attack method currently cannot be applied to large-resolution images, especially in 2023. \nIn the supplementary experiment, the author still only conducted experiments at 128 resolution, and did not provide the attack success rate. The author showed some attacked images, which was not convincing.\n\n\nI keep my rating!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722566084,
                "cdate": 1700722566084,
                "tmdate": 1700722566084,
                "mdate": 1700722566084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kahRz9I9zE",
                "forum": "uUELOzcTk8",
                "replyto": "QMPZD3bHZm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply!\n\n> However, It is unacceptable that an attack method currently cannot be applied to large-resolution images, especially in 2023. \n\nMy understanding is that the primary goal of academic conferences is to propose and exchange innovative ideas, rather than to engage in benchmarking or competition. I encourage you to focus on the originality and innovation of our ideas. It's unreasonable to expect the first steam locomotive to outperform horse-drawn carriages in every performance aspect. Similarly, a data-driven $p_{dis}$ represents a novel concept with considerable flexibility and future potential, particularly as probabilistic generative models continue to evolve.\n\nFurthermore, considering that the popular resolution for ImageNet classification is 224x224, we believe that our resolution of 128x128 is comparable in scale.\n\n> ... and did not provide the attack success rate. The author showed some attacked images, which was not convincing.\n\nAs previously mentioned, all the images generated by our method successfully deceive the classifier, achieving a 100% success rate. However, the challenge with unrestricted adversarial attacks is that they may be perceptible to human observers, as they are not assessed under an objective geometric distance. Therefore, a human subjective evaluation becomes crucial. We recognize, as demonstrated in Appendix N, that while our attack method generates adversarial examples with less noticeable tampering traces, the images tend to be relatively blurred. This blurring effect could be attributed to the current limitations in the implementation of energy-based models."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736449156,
                "cdate": 1700736449156,
                "tmdate": 1700741499468,
                "mdate": 1700741499468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8E7yChfZvk",
            "forum": "uUELOzcTk8",
            "replyto": "uUELOzcTk8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a probabilistic approach to construct semantically meaningful adversarial examples by interpreting geometric perturbations as distributions. The approach relies on training an energy based model (EBM) to emulate such distributions. The approach is evaluated on a variety of image classification datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The proposed approach is a principled take on the semantic adversarial attacks where in the authors derive an EBM-style formulation to generate geometric adversarial examples.\n\n2. The algorithm presents high attack success rates on adversarially trained models."
                },
                "weaknesses": {
                    "value": "1. The paper is missing several references, and is relatively sparse with regards to related work. Similar works include Spatially Transformed Adversarial Examples (Xiao et al., ICLR 2018), Semantic Adversarial Attacks (Joshi et al, 2019, ICCV 2019), Semantic Adversarial examples (Hosseini et al, CVPRW, 2018) and Unrestricted adversarial examples (Bhattad et al, ICLR 2020). Several of these works use generative models to generate semantic adversarial examples and should be discussed and contrasted in this work.\n\n2. The experiments are limited to just two adversarially trained networks. Furthermore, the adversarially trained models do not appear to have been trained under the semantic attack threat model and hence unlikely to be robust to such attacks. \n\n3. The attack also relies on adversarially trained models learning semantically meaningful features. However, the images shown in the figures appear to be distorted which could be a consequence of the models overfitting on a subset of semantic features.\n\n**Update**: The authors have addressed some of my concerns and clarified certain points of misunderstanding. I am therefore increasing my score to 5 to reflect this."
                },
                "questions": {
                    "value": "Could the authors clarify the training setup for the adverarially trained models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4207/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4207/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723350044,
            "cdate": 1698723350044,
            "tmdate": 1700586096592,
            "mdate": 1700586096592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s6t5Yq8the",
                "forum": "uUELOzcTk8",
                "replyto": "8E7yChfZvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer URQr"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review my submission. Below are our detailed responses to your comments. \n\n> The paper presents a probabilistic approach to construct semantically meaningful adversarial examples by interpreting geometric perturbations as distributions. The approach relies on training an energy based model (EBM) to emulate such distributions.\n\nIn addition to the concrete approach, we also want to highlight the significance of our probabilistic perspective presented in Section 3. The distance distribution $p_{dis}$ can be modeled using various probabilistic generative models. This probabilistic framework holds considerable potential for extension, with Energy-Based Models (EBM) being just one of many possible options. In this paper, we opted for EBM because its mathematical formulation aligns perfectly with Langevin dynamics, resulting in clear and concise derivations.\n\n> The paper is missing several references, and is relatively sparse with regards to related work. Similar works include ...\n\nWe contend that, with the probabilistic modeling perspective, the works cited are not similar to our approach. While some of these studies may employ Generative Adversarial Networks (GANs), it's important to note that GANs do not explicitly model the probability distribution $p(x)$. Our work is pioneering in rigorously deriving the distribution $p_{adv}$ from which adversarial samples are generated.\n\nWe acknowledge that from an application perspective, these works bear similarities to ours. In fact, under this viewpoint, any study not employing geometric constraints on the original image could be considered similar to our approach. However, our focus is on the novelty of the probabilistic perspective and modelling. Our contribution lies in introducing new concepts to the academic community, rather than solely advancing application benchmarks.\n\nThank you for introducing these related works to us. We will ensure to incorporate them into the revised version of our paper.\n\n> The experiments are limited to just two adversarially trained networks.\n \nIn the main text, we discuss three adversarially trained networks: two are presented in Table 1 for MNIST and SVHN, and another is detailed in Section 6.2 for CIFAR10. Furthermore, Appendix F features 12 additional networks, demonstrating our model's transferability. We limited the number of structures in the main text for two reasons: firstly, to maintain alignment with the settings in [1], and secondly, to efficiently illustrate our method's advantages without unnecessarily utilizing human annotators.\n\n> Furthermore, the adversarially trained models do not appear to have been trained under the semantic attack threat model and hence unlikely to be robust to such attacks.\n\nThe objective of our experiment is to demonstrate the effectiveness of our attack method against adversarially trained models. While some studies in this field have incorporated experiments where generated adversarial examples are used in training robust classifiers, we have not pursued this approach. We believe our current experimental setup sufficiently highlights the strengths of our attack method, in line with [1] and [3], and we aim to avoid unnecessary use of human annotators.\n\n> The attack also relies on adversarially trained models learning semantically meaningful features.\n\nThis is not correct. I encourage a careful review of the draft, particularly Figure 2 (a) and (b). These figures demonstrate that when the victim classifier is adversarially trained, $p_{vic}$ possesses generative capabilities, meaning it tends to produce images resembling those from the target class, which in turn makes an adversarial attack more challenging. Our proposed attack method is not dependent on adversarially trained models. In fact, attacking a victim classifier that is not adversarially trained would be significantly easier.\n\n> However, the images shown in the figures appear to be distorted which could be a consequence of the models overfitting on a subset of semantic features.\n\nCould you please specify which distortion you're referring to and in which figure it occurs?\n\n> Could the authors clarify the training setup for the adverarially trained models?\n\nWe use the setting introduced in [2], as introduced in Section 2.2. \n\n[1] Song, Yang, et al. \"Constructing unrestricted adversarial examples with generative models.\" Advances in Neural Information Processing Systems 31 (2018).\n\n[2] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n\n[3] Xiao, Chaowei, et al. \"Spatially transformed adversarial examples.\" arXiv preprint arXiv:1801.02612 (2018)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699682907131,
                "cdate": 1699682907131,
                "tmdate": 1699682907131,
                "mdate": 1699682907131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TrXVGfOxhv",
                "forum": "uUELOzcTk8",
                "replyto": "s6t5Yq8the",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for promptly responding to my review and appreciate the detailed response.\n\n1.  Firstly, I appreciate the clarification that the EBMs used to model the geometric distribution can be replaced with any probabilistic generative model. However, the paper only presents results with EBMs and therefore it is hard to evaluate the effectiveness of the attack for better generative models like diffusion or variational autoencoders. \n\n2. In terms of comparisons, while I do agree with the authors that their contribution of explicitly modelling the distribution is important, it is equally important to compare with similar approaches in order to evaluate the proposed attack. An important additional evaluation metric would be to analyse how accurately do EBMs model the adversarial distribution in comparion to GANs or even uniform distributions over the transformations (perhaps by studying the number of adversarial samples). This would provide more support to the theoretical argument.\n\n3. I thank the authors for pointing me to the additional experiments. I suggest including them in the main draft rather than the appendix (by perhaps reducing the background discussion).\n\n4. It is well known that adversarially trained models are not robust to attacks generated under different threat models, (for example [1](https://openreview.net/pdf?id=Sy8WeUJPf).). As such, it would be unfair to evaluate semantic adversarial attacks on models trained to be robust to $\\ell_\\infty$ attacks. A more principled would be to actually use the geometric adversarial examples generated to train a robust model, and then evaluate the attack to see if it can still find an adversarial distribution under the given threat model. \n\n5. I thank the authors for clarifying my understanding of fig. 2. I see now that the adversarial distribution would have more support in case of non-adversarially trained models.\n\n6. Both Fig 4 and Fig 10 show recognizably different images from the source, and do not look like numbers sourced from MNIST. I referred to these examples as distorted. Specifically almost all images show broken strokes and grey patches which while may be adding adversarial features, do not look like numbers that are part of the MNIST dataset. \n\nI again thank the authors and hope to continue the discussion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512948013,
                "cdate": 1700512948013,
                "tmdate": 1700512948013,
                "mdate": 1700512948013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sUWvYDe0Jl",
                "forum": "uUELOzcTk8",
                "replyto": "8E7yChfZvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply! This response will be divided into two text boxes.\n\n### 1 \n\nIn Section 3, we present the abstract form of the probabilistic perspective on adversarial attacks. The $p_{\\text{dis}}$ can represent any probabilistic generative model. Given the abstract nature of this framework, it's not essential to exhaustively explore all possible choices for $p_{\\text{dis}}$. Instead, we view it as a promising research direction for future exploration. In this work, we use the EBM as a case to demonstrate the framework's potential. Our current draft aims to introduce a new perspective and a model based on it, rather than achieving the ultimate performance in this task.\n\n### 2\n\n>  it is equally important to compare with similar approaches in order to evaluate the proposed attack.\n\nWe agree that comparing with similar approaches is crucial for evaluating our proposed attack. However, aligning comparisons in unrestricted adversarial attacks is challenging. In restricted attacks, we can standardize comparisons by fixing the L-p distance and comparing success rates. For unrestricted attacks, while we can achieve a 100% success rate in deceiving the classifier, it's unclear whether the adversarial examples deviate significantly from the original image in human perception. Therefore, we rely on human annotators to determine if the original class of the image is recognizable. This method, aligned with [1], is reflected in our Table 1 results.\n\nContrastingly, [2] proposes a method that doesn't guarantee a 100% success rate in deceiving the victim classifier. Consequently, they employ a different evaluation method, assuming their adversarial examples are imperceptible to humans. Their success rate is thus based on the classifier's deception rate. To validate their assumption, they conducted an A/B test assessing human perception, focusing on ImageNet. However, this method may lack rigor, as indicated by the discernibility of their MNIST adversarial samples in Figure 2 of [2], which might be more noticeable to humans than suggested.\n\n[3] similarly assumes that their method produces adversarial examples undetectable by humans and compares the accuracy of the victim classifier, where a lower accuracy indicates a more effective attack. However, upon examining their figures, I find their assumption about human indistinguishability unconvincing.\n\n[4], being a workshop paper, lacks comparative analysis.\n\n[5] assumes that alterations in color and texture remain undetected by humans. Subsequently, they assess their attack's success rate in comparison with conventional methods.\n\nIn summary, **our approach and [1], guarantee that our adversarial examples successfully deceive the victim classifier, with human annotators then evaluating their distinguishability. In contrast, [2], [3], and [5] proceed on the assumption that their generated adversarial examples are indistinguishable to humans, focusing instead on comparing the success rates in compromising the victim classifier.**\n\nTherefore, in our submission, we limit our comparison to [1]. In the revised version, we will discuss [2], [3], [4], and [5], but will not include them in our comparison.\n\n> how accurately do EBMs model the adversarial distribution in comparion to GANs\n\n[1]'s approach uses GANs to implicitly model the distribution of adversarial examples, whereas our method explicitly models this distribution. Direct comparison of these distributions is challenging, leading us to select human annotators' success rate as our sole evaluation metric.\n\n> even uniform distributions over the transformations\n\nI may have misunderstood your question, but if we were to use a uniform distribution for $p_{\\text{dis}}$, then $p_{\\text{adv}}$ would essentially reduce to $p_{\\text{vic}}$, as depicted in Figure 2 (a) and (b).\n\n### 3\n\nThank you for your suggestion. We will relocate the training and sampling details of the EBM to the Appendix and shift the table showing transferability results to the main text.\n\n### Reference\n\n[1] Song, Yang, et al. \"Constructing unrestricted adversarial examples with generative models.\" Advances in Neural Information Processing Systems 31 (2018).\n\n[2] Xiao, Chaowei, et al. \"Spatially transformed adversarial examples.\" arXiv preprint arXiv:1801.02612 (2018).\n\n[3] Joshi, Ameya, et al. \"Semantic adversarial attacks: Parametric transformations that fool deep classifiers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019.\n\n[4] Hosseini, Hossein, and Radha Poovendran. \"Semantic adversarial examples.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018.\n\n[5] Bhattad, Anand, et al. \"Unrestricted adversarial examples via semantic manipulation.\" arXiv preprint arXiv:1904.06347 (2019)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527863663,
                "cdate": 1700527863663,
                "tmdate": 1700560506832,
                "mdate": 1700560506832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLhXeyrKmz",
                "forum": "uUELOzcTk8",
                "replyto": "8E7yChfZvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I appreciate the quick reply and the clarifications as well as the new Imagenet results. However, I still disagree with the authors on comparisons. Perhaps it is harder to exhaustively compare all semantic adversarial methods fairly due to the varying setups. However, [1] provides a very similar threat model. Also, while the authors claim that [1] does not have a 100% success rate and therefore cannot be compared, Figs 3 and 4 from the main paper show similar failures. It might be useful to compare the failure modes to evaluate how semantic divergence is a better approach. In addition, given the significant progress in our understanding of adversarial robustness, and the standard evaluation approaches, evaluating a robust model under an attack with a different threat model does not make sense to me. Perhaps the other reviewers could chime in and present their opinions on this.\n\nHowever, I do find the idea of learning an adversarial distribution interesting by itself as well as of interest to the community if it can be scaled to larger models and complex datasets. As of now, I am willing to increase my score to 5 (marginally below the acceptance threshold). I am also open to changing my mind if the other reviewers or authors have additional thoughts to support the paper.\n\n[1] Xiao, Chaowei, et al. \"Spatially transformed adversarial examples.\""
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585946267,
                "cdate": 1700585946267,
                "tmdate": 1700585983266,
                "mdate": 1700585983266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aFF52yvJTL",
                "forum": "uUELOzcTk8",
                "replyto": "8E7yChfZvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "StAdv Replicated"
                    },
                    "comment": {
                        "value": "Thank you for your suggestion! We have successfully replicated the StAdv method as proposed by [1], and the detailed results are now included in Appendix O (**Update: now is P**) of the latest version of our document.\n\n[1] Xiao, Chaowei, et al. \"Spatially transformed adversarial examples.\""
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664091488,
                "cdate": 1700664091488,
                "tmdate": 1700717786861,
                "mdate": 1700717786861,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r2cZmR9xFL",
                "forum": "uUELOzcTk8",
                "replyto": "aFF52yvJTL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Reviewer_URQr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the update"
                    },
                    "comment": {
                        "value": "Thanks for the update. Could you also share the success rates of stAdv vs your approach?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667870974,
                "cdate": 1700667870974,
                "tmdate": 1700667870974,
                "mdate": 1700667870974,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lsxRMyjMY2",
            "forum": "uUELOzcTk8",
            "replyto": "uUELOzcTk8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_CGw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4207/Reviewer_CGw2"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a novel and simple methods for constructing semantic-aware adversarial examples. The proposed frame work use a probabilistic perspective method to generate semantics-aware adversarial examples.  As depicted in the figure of this paper, it seems this attack successfully attack classifiers while retaining the original image's semantic information."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The method for generating semantics-aware adversarial examples is very intuitive.\n* This paper is well-written. The problem this paper focuses is important, and the proposed method is interesting."
                },
                "weaknesses": {
                    "value": "Please correct me if I have some misunderstanding of the paper.\n\n1. Since this paper proposes a semantics-aware method, the human evaluation is important to verify the validity of adversarial examples.\n\n2. It lacks a comparison on the number of queries.\n\n3. There is a lack of an ablation experiment. In $p_{adv}$, what will happen if $p_{dis}$ uses L2 or other methods to calculate Figure 1 or Table 1?"
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4207/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4207/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4207/Reviewer_CGw2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762272826,
            "cdate": 1698762272826,
            "tmdate": 1699636387287,
            "mdate": 1699636387287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZUhD3jOKBm",
                "forum": "uUELOzcTk8",
                "replyto": "lsxRMyjMY2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CGw2"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review and for recognizing the contribution of our paper.\n\nHere are our responses to your comments:\n\n> Since this paper proposes a semantics-aware method, the human evaluation is important to verify the validity of adversarial examples.\n\nYes, human evaluation is crucial. Our experiments conform to the evaluation methods detailed in [1].\n\n> It lacks a comparison on the number of queries.\n\nCould you clarify what is the number of queries you're referring to?\n\n> There is a lack of an ablation experiment. In $p_\\{\\text{adv}}$, what will happen if $p_\\{\\text{dis}}$ uses L2 or other methods to calculate Figure 1 or Table 1?\n\nRefer to Figure 3 (c), showcasing the Prob CW (probabilistic CW attack). \n\nFigure 3 (c) reveals that the adversarial examples represent intersections between the original image and the target class, making them easily identifiable by humans. Therefore, we omitted them from Table 1 to conserve human annotation efforts. In response to your suggestion, we will incorporate adversarial examples generated by the prob CW attack in the format of Figure 1 into our revised draft and will notify you once it's complete.\n\n[1] Song, Yang, et al. \"Constructing unrestricted adversarial examples with generative models.\" Advances in Neural Information Processing Systems 31 (2018)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699646966772,
                "cdate": 1699646966772,
                "tmdate": 1699647233832,
                "mdate": 1699647233832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]