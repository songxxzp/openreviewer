[
    {
        "title": "Multisize Dataset Condensation"
    },
    {
        "review": {
            "id": "njdOms4iMc",
            "forum": "FVhmnvqnsI",
            "replyto": "FVhmnvqnsI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission801/Reviewer_DoMB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission801/Reviewer_DoMB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method to compress the condensation process into one process. It is different from the model compression or dataset compression, and this topic sound new to me. The definition of \u201csubset degradation problem\u201d is important in this domain. It will help the researchers to consider the problem. The experiments validate the effectiveness of the propose method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The figures are beautiful and easy to understand.\n2. The idea is novel. The process compression sound new to me since it is different from the model compression or dataset compression. \n3. The \u201csubset degradation problem\u201d is practical. Although I have find similar pattern in experiments, it is good to see it is officially and properly presented.\n4. The experiment results are promising. It save the computational cost by N times."
                },
                "weaknesses": {
                    "value": "1. It's not clear what's the purpose of baseline B. It looks like the results are only compared to baseline A and C.\n2. It's not clear why the freezing is used in MLS selection. If adaptive is good, why not just use adaptive method to choose the subset?\n3. Will the additional loss bring extra computational cost?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418469705,
            "cdate": 1698418469705,
            "tmdate": 1699636007241,
            "mdate": 1699636007241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YSgfSLxwkq",
                "forum": "FVhmnvqnsI",
                "replyto": "njdOms4iMc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive feedback.\nWe will address the listed questions one by one.\n\n> 1. It's not clear what's the purpose of baseline B. It looks like the results are only compared to baseline A and C.\n\nBefore explaining the purpose of Baseline B, here is the recap of the all three Baselines. We will use examples of 10 images per class (IPC=10).\n- Baseline A: \n\t- The ultimate goal of this task, allowing **all subset** to have the same performance as directly condense to a dataset with specific IPC (Image Per Class).\n\t- **Approach**: conduct 10 separate condensations and obtain 10 different datasets.\n\t- The **purpose** is the provide an ''upper bound'' of the experimental result.\n- Baseline B: \n\t- **Approach**: Condense to 10 different datasets with IPC=1, construct large synthetic dataset using small IPC datasets.\n\t- **Purpose**: this approach **ensures** that the accuracy of the **smallest IPC** is preserved as opposed to preserving the accuracy of the largest IPC (Baseline C). The baseline B illustrates that combining many small datasets do not give comparable results on large IPC.\n- Baseline C:\n\t- **Approach**: traditional condensation approach.\n\t- **Purpose**: show the subset dagradation problem.\n\n The main reason we introduce Baseline-B is that it has the **same storage** as Baseline-C while addressing the subset degradation problem (accuracy of IPC1 is preserved). However, as illustrated in the experimental results, Baseline-B introduces another problem: using many small synthetic images to construct a big datasets does not give comparable results. **That it, Baseline-B has good performance when IPC is small, but fails when IPC is large.\n Baseline-C is the exact opposite, achieving good performance of large IPC, but fails at small one.**\n Therefore, the proposed method effectively address the problem from both sides, achieving good performance when both IPC is small and large.\n\n > 2. It's not clear why the freezing is used in MLS selection. If adaptive is good, why not just use adaptive method to choose the subset?\n\nAdaptive selection and Freezing serve for different purposes.\n- **adaptive selection**: it is not practical to incorporate the information of all IPC at the same time due to incurring more computations or slowing learning process. We use adaptive selection to find **one** IPC that is the most learnable. By adding information of **one** IPC at a time, the mentioned problem is addressed.\n- **freezing**: freezing aims to prevent the ''already learned\" images from being overwritten by information of large IPC. By extracting a smaller subset, ideally speaking, the subset should not contain information of larger IPCs, otherwise the performance will be affected.\n\n\n| Calculate | Compare | Freeze | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     | 10    | Avg.  |\n|-----------|---------|--------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| -         | -       | -      | 27.49 | 38.50 | 45.29 | 50.85 | 53.60 | 57.98 | 60.99 | 63.60 | 65.71 | 67.50 | 53.15 |\n| \u2713         | -       | -      | 49.35 | 48.27 | 50.00 | 52.30 | 54.20 | 58.29 | 60.90 | **63.63** | **65.90** | **67.63** | 57.08 |\n| \u2713         | \u2713       | -      | 40.12 | **54.91** | 56.02 | 56.12 | 56.18 | 59.74 | 61.68 | 63.41 | 65.56 | 67.01 | 58.08 |\n| \u2713         | \u2713       | \u2713      | **49.55** | 53.75 | **56.39** | **59.33** | **58.13** | **60.62** | **62.06** | 63.59 | 65.25 | 66.79 | **59.55** |\n\nThis is the Tab. 2 from our paper, it shows the difference of condensation with (row 4) and without (row 3) freezing.\nRow 3 delivers a poorer performance, especially IPC=1 when compared to Row 4. \nThe issue is that performance of IPC1 is affected by large IPC since it is **not frozen** (keep receiving pixel updates according to larger IPC).\n\n> 3. Will the additional loss bring extra computational cost?\n\nYes, we provide an example in `Section. 4.3 More Analysis: Redcued Training Time Needed`.\nThe complete training of our method requires 30% more training time (**15.1 hrs** vs 11.5 hrs) on CIFAR-10 IPC10.\nHowever, to reach the average accuracy of the traditional approach, we only requires **0.6 hrs** compared to 11.5hrs (traditional approach).\nFig. 4 in the paper presents a visualization of the comparison."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308656129,
                "cdate": 1700308656129,
                "tmdate": 1700308656129,
                "mdate": 1700308656129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Fdl67EeuM",
                "forum": "FVhmnvqnsI",
                "replyto": "YSgfSLxwkq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission801/Reviewer_DoMB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission801/Reviewer_DoMB"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer DoMB"
                    },
                    "comment": {
                        "value": "Thanks for the author's response. My concerns have been well addressed. I would keep my rating as accept."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662973198,
                "cdate": 1700662973198,
                "tmdate": 1700662973198,
                "mdate": 1700662973198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OauQ9CLUkg",
            "forum": "FVhmnvqnsI",
            "replyto": "FVhmnvqnsI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission801/Reviewer_6Dsd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission801/Reviewer_6Dsd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Multisize Dataset Condensation (MDC) method, aiming to address challenges associated with dataset condensation in on-device processing scenarios. The main innovation lies in the compression of multiple condensation processes into a single process to produce datasets of varying sizes. The authors combat the \"subset degradation problem\" with an \"adaptive subset loss,\" ultimately enhancing the representation quality of condensed subsets. Experiments spanning various networks and datasets showcase the method's effectiveness."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: This paper offers a unique approach to dataset condensation, aiming to cater to the specific needs of on-device scenarios. The proposal to compress N condensation processes into one is innovative.\nQuality: The \"adaptive subset loss\" is a novel concept, targeting the \"subset degradation problem.\" The method to select the Most Learnable Subset (MLS) is well-thought-out and complex.\nClarity: The paper is organized logically, and concepts are explained clearly. The use of terms like \"adaptive subset loss\" and \"subset degradation problem\" helps the reader understand the core issues being addressed.\nSignificance: The problem space being tackled (on-device training with dynamic computational resources) is relevant. Solving this issue can have substantial implications for real-world applications."
                },
                "weaknesses": {
                    "value": "The paper explains three baselines for comparison. Compared to baseline A, the accuracy is not higher. Please explain the reason.\nIs it possible to reach Baseline A's accuracies? \nEquation 7 is not that clear. How to calculate the distance between the full dataset and subset?"
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission801/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission801/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission801/Reviewer_6Dsd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698540534985,
            "cdate": 1698540534985,
            "tmdate": 1699636007173,
            "mdate": 1699636007173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pHC5iUlzDE",
                "forum": "FVhmnvqnsI",
                "replyto": "OauQ9CLUkg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive rating. The main question provided has 3 small question, and we will address each of them accordingly.\n\n> 1. Compared to baseline A, the accuracy is not higher. Please explain the reason.\n\nTake IPC10 as an example, the Baseline-A is composed of 10 separate condensation processes and 10 different datasets.\nDuring the evaluation of the Baseline-A, we evaluate each synthetic dataset separately. Therefore, there is **NO INTERFERENCE** between datasets.\nCompare to the propose method, it requires 5x more storage.\nOur method, on the other hand, does have the inference between different subsets. For instance, IPC1 should not have information of large IPCs (IPC>1) during the evaluation of IPC1. However, during the evaluation of larger IPCs, we actually expect IPC1 to contain information of larger IPCs. Therefore, there is a conflict of interests and trade-offs should be made.\n\n> 2. Is it possible to reach Baseline A's accuracies?\n\nWe believe it is possible to reach a similar average performance (a very close one), but we don't have the proof at this stage.\nOne interesting observation is that: at larger IPCs, the performance of ImageNet outperforms Baseline-A. The below table is a taken from Tab. 1 (c) in our paper.\n\n| Dataset             |      |  10    | 15    | 20    |\n|---------------------|------|--------|-------|-------|\n| ImageNet-10         | A    |  **72.80** | 75.50 | 76.60 |\n|                     | B    |  63.60 | 62.73 | 64.13 |\n|                     | C    |  73.00 | 74.47 | 75.73 |\n|                     | Ours |  71.13 | **76.00** | **79.20** |\n\nThe performance of IPC15 and IPC20 indeed outperforms the Baseline-A, and it exceeds by a noticeable margin.\nThis gives us a sign that updating the subsets (smaller IPCs) may eventually help larger IPCs.\n\n> 3. Equation 7 is not that clear. How to calculate the distance between the full dataset and subset?\n\nThe feature distance comparison is possible even if number of images are different.\nSince both real and synthetic images are fed into the network, the output feature only differs in the number of images.\nFor example, if we sample 40 real images from full datasets and forward them, the output dimension is (40, 4, 4, 512), where 40 is the number of images, 4 is the feature size, and 512 is the number of channels. Lets say if we forward 20 synthetic images, the output will have shape like (20, 4, 4, 512), where the difference is on the first dimension. \nWe then average the feature along the first dimension, we will then obtain two averaged features of shape (4, 4, 512). Therefore we can compare. A similar approach is used by Wang etal. [1].\n\n[1] K. Wang _et al._, \u201cCAFE: Learning to Condense Dataset by Aligning Features,\u201d in _CVPR_, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308701130,
                "cdate": 1700308701130,
                "tmdate": 1700308701130,
                "mdate": 1700308701130,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j5GOEM11ZJ",
            "forum": "FVhmnvqnsI",
            "replyto": "FVhmnvqnsI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission801/Reviewer_ZxGV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission801/Reviewer_ZxGV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called Multisize Dataset Condensation (MDC) to compress multiple-size dataset condensation processes into a single process. The goal is to obtain a small synthetic dataset that is equally effective but much smaller in size. The authors introduce the concept of the Most Learnable Subset (MLS) and propose an adaptive subset loss to mitigate the \"subset degradation problem\" in traditional dataset condensation. The MDC method can reduce the condensing process and lower the storage consumption. The MDC achieves state-of-the-art performance on various models and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed Multisize Dataset Condensation (MDC) method can effectively condense the N condensation processes into a single condensation process with lower storage and addresses the \u201csubset degradation problem\u201d.\n2. The adaptive subset loss in the MDC method helps mitigate the \u201csubset degradation problem\u201d and improves the accuracy of the condensed dataset compared to the Baseline-C.\n3. The concept of the rate of change of feature distance as a substitute for the computationally expensive \u201cgradient distance\u201d reduces computational overhead while capturing essential characteristics among subsets."
                },
                "weaknesses": {
                    "value": "1. When the IPC (Inter-Process Communication) is small, there still exists a large accuracy gap between the proposed model and Baseline-A as shown in Figure 2 and Table 1.\n2. The impact of the calculation interval (\u2206t) on the performance of the MDC method needs to be further analyzed to determine the optimal interval size."
                },
                "questions": {
                    "value": "1. Can you provide the computational resource consumption and algorithmic complexity compared to Baseline-A, B, C, and other SOTA methods? It can help authors better understand the effects of algorithms in devices with limited computational resources.\n2. Can you provide the values of hyperparameters such as \u03bb and \u03b7 in Formula 2?\n3. The section on Visualization of MLS is currently difficult to understand. It would be helpful to provide more detailed and accessible explanations to ensure a clear understanding for readers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829601415,
            "cdate": 1698829601415,
            "tmdate": 1699636007077,
            "mdate": 1699636007077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kGVmkg8V7D",
                "forum": "FVhmnvqnsI",
                "replyto": "j5GOEM11ZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for bringing up the questions and concerns. We try to address them one by one.\n\n> 1. When the IPC (Inter-Process Communication) is small, there still exists a large accuracy gap between the proposed model and Baseline-A as shown in Figure 2 and Table 1.\n\nThe IPC we use stands for Image Per Class, indicating how many images are for each class.\nThe Baseline-A uses much more resources compared to the proposed method. It is the **ideal setting** if the resources being used is the same as our method.\nTo the best of our knowledge, there is no method could achieve Baseline-A using the given resources (i.e., one condensation process with one dataset).\n\n> 2. The impact of the calculation interval (\u2206t) on the performance of the MDC method needs to be further analyzed to determine the optimal interval size.\n\nOptimizing the calculate interval $\\Delta t$ will further improve the performance, and we admit finding the optimal one will not be easy, especially this hyper-parameter can vary from dataset to dataset.\nWe will leave this problem for future exploration.\nNevertheless, Tab. 7 shows that the choice of $\\Delta t$ fall a wide range, and each of them brings a considerable improvement.\n\n> 3. Can you provide the computational resource consumption and algorithmic complexity compared to Baseline-A, B, C, and other SOTA methods? It can help authors better understand the effects of algorithms in devices with limited computational resources.\n\nTab. 2 (b) compares the resources for different baselines. (A simplified version is shown below.)\n\n|      | Condense | Storage         |\n| ---- | -------- | --------------- |\n| A    | N        | 1 + 2 + ... + N |\n| B    | N        | N               |\n| C    | 1        | N               |\n| Ours | 1        | N               |\n\nTake CIFAR-10 as an example, condensing using IDC takes roughly 11 hours on average for IPC1-10. Note the time used to condense to different IPCs is different, but will not be extremely different, e.g., IPC1 ~ 10 hrs, IPC10 ~ 12 hrs.\nHere are the resources needed for each baseline:\n- Baseline-A: 110 hours + 550 images (storage)\n- Baseline-B: ~100 hours + 100 images\n- Baseline-C: 11.5 hours + 100 images\n- Ours (+0.0%): 0.6 hours + 100 images\n- Ours (+6.4%): 15.1 hours + 100 images\n\nIn conclusion, our method incurs 30% more training cost to attain 6.4% increase of average accuracy. To attain the same accuracy as Baseline-C, **0.6** hrs is enough. The visualization is presented in Fig. 4 in the paper. \n\n**Comparing to other SOTA.** We did not conduct experiments on other SOTA except for IDC-based method, but the expected additional cost is 30% - 50% (capped at 100%). The reason is that the selected IPCs are usually not large, incurring small additional cost (as shown in Fig. 5).\n\n> 4. Can you provide the values of hyperparameters such as \u03bb and \u03b7 in Formula 2?\n\nThe $\\lambda$ and $\\eta$ are learning rates using in dataset distillation frameworks, and we did not modify this.\nWe follow the same setting used in IDC [1] (Details can be found in Appendix C.1 in [1]).\n\n[1] J.-H. Kim _et al._, \u201cDataset Condensation via Efficient Synthetic-Data Parameterization,\u201d in _ICML_, 2022.\n\n> 5. The section on Visualization of MLS is currently difficult to understand. It would be helpful to provide more detailed and accessible explanations to ensure a clear understanding for readers.\n\nThank you for the feedback, we will try to adjust the figure for a clearer presentation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308767839,
                "cdate": 1700308767839,
                "tmdate": 1700308767839,
                "mdate": 1700308767839,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MoRPUvVnjl",
                "forum": "FVhmnvqnsI",
                "replyto": "kGVmkg8V7D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission801/Reviewer_ZxGV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission801/Reviewer_ZxGV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response. My concerns have been essentially addressed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661330097,
                "cdate": 1700661330097,
                "tmdate": 1700661330097,
                "mdate": 1700661330097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ry6BmDb0kU",
            "forum": "FVhmnvqnsI",
            "replyto": "FVhmnvqnsI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission801/Reviewer_CWAb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission801/Reviewer_CWAb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Multisize Dataset Condensation problem that can derive multiple subsets from the condensed images for supporting on-device scenarios. The authors identify \u201csubset degradation problem\u201d where the performance of a subset from condensed images is lower than directly condensing the full dataset to the target size. Subsequently, the authors propose \u201cadaptive subset loss\u201d where the most learnable subset is selected to update the subset, to alleviate the \u201csubset degradation problem\u201d for all subsets. Experimental results demonstrate that MDC works well for various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents a solution for DC named Multisize Dataset Condensation which is crucial for on-device scenarios. The proposed method outperforms baseline C significantly."
                },
                "weaknesses": {
                    "value": "1. The synthetic samples within the subset seem to be fixed, which may not reflect \u201cMultisize Dataset Condensation\u201d correctly."
                },
                "questions": {
                    "value": "I have several questions:\n1. In Fig 2c, for baseline C, how to select subsets to calculate accuracy? Is it random? Let\u2019s assume we have a subset of 2 images. Do we select 2 images from the condensed data randomly?\n2. In basic condensation training (Sec 4.1), for each initialization the network is trained for 100 epochs. Is it the inner loop E?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission801/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission801/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission801/Reviewer_CWAb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699083307329,
            "cdate": 1699083307329,
            "tmdate": 1699636006985,
            "mdate": 1699636006985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Damh6pSmwx",
                "forum": "FVhmnvqnsI",
                "replyto": "ry6BmDb0kU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. We will response point by point.\n\n> 1. The synthetic samples within the subset seem to be fixed, which may not reflect \u201cMultisize Dataset Condensation\u201d correctly.\n\nThe keyword (**multisize**) in our title means the condensed dataset can be treated as a multi-sized dataset.\nThat is, the dataset of $IPC=N$ can be used as dataset of $IPC={1, 2, ..., N}$ without requiring additional condensation process.\nThis is currently unattainable using traditional approach, since traditional one condenses a synthetic dataset to a **specific IPC**. For example, dataset with $IPC=N$ will perform poorly on smaller $IPC$ ones, and we take the rights to name the phenomenon \"Subset Degradation Problem\". Our method addresses the problem, turning the dataset available for multiple size of IPCs.\n\n> 2. In Fig 2c, for baseline C, how to select subsets to calculate accuracy? Is it random? Let\u2019s assume we have a subset of 2 images. Do we select 2 images from the condensed data randomly?\n\nFor baseline C, all subsets are selected using the $N$ images. For example, if there are $10$ images in the class, we take the first image as IPC=1, and first two images as IPC=2.\n\nTo address the concern, here is the result of randomly sampled images (see below table). We do not see an obvious advantage with one method over the other.\n\n|          | 1        | 2        | 3        | 4        | 5        | 6    | 7    | 8        | 9        | 10    | avg  |\n| -------- | -------- | -------- | -------- | -------- | -------- | ---- | ---- | -------- | -------- | ----- | ---- |\n| First N  | 27.5     | 38.5     | **45.3** | 50.9     | **53.6** | 58.0 | 61.0 | 63.6     | **65.7** | 67.50 | 53.2 |\n| Random N | **28.4** | **39.3** | 44.3     | **51.1** | 53.2     | 58.0 | 61.0 | **63.8** | 65.6     | 67.50 | 53.2 |\n\n> 3. In basic condensation training (Sec 4.1), for each initialization the network is trained for 100 epochs. Is it the inner loop E?\n\nYes. The 100 epoch is exactly the inner loop E depicted in Fig. 3.\n\nWe hope the response addresses your concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308812367,
                "cdate": 1700308812367,
                "tmdate": 1700308812367,
                "mdate": 1700308812367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]