[
    {
        "title": "The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning"
    },
    {
        "review": {
            "id": "LhxH9IY9kz",
            "forum": "ldJXXxPE0L",
            "replyto": "ldJXXxPE0L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_B31e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_B31e"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the effects of pruning and down-scaling large language models (LLMs) on the model capabilities. Specifically, the authors focus on two abilities of modern LLMs: (1) the ability to process information stored in the weights (fact recall) and (2) the ability to process information that is available in context. To evaluate down-scaled models on these capabilities, they use a suite of benchmarks covering four tasks, open-book QA, closed-book QA, overriding QA, and learning tasks (i.e., model needs to understand underlying function based on examples given as in-context learning). Experiments on 6 base LLMs, each with 9 different sparsity levels demonstrate different model behavior in terms of the two capabilities. Model ability to process information in weights degrades with moderate level of pruning (>30%), while model ability to process information in context does not really degrade even with aggressive pruning (up to 70%)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall, I think the paper discusses an important question regarding the trade-offs of having smaller-scaled models and its impact to model capabilities. The experiments are well-thought, with the use of different benchmark tasks to isolate different model capabilities being tested and the use of different base LLMs to see that the effects are similar across different model families. I think the main findings of this paper will be useful for future work in this area. The paper is well-written."
                },
                "weaknesses": {
                    "value": "- Although down-scaling and pruning are the main topic of the paper, the technical details on methods used is very limited (even in the Appendix too). If space is an issue, I would suggest to cut down the paper motivation which is repeated multiple times throughout the paper.\n- Relatedly, there is very little discussion regarding down-scaling vs. pruning. For general readers it would be helpful to understand what are the difference between the two, and is one a specific version of the other?"
                },
                "questions": {
                    "value": "- For learning tasks evaluation, why only consider task with scalar values as labels? I understand this needs to be something that model can generalize through the examples, but if we focus on language capability of the model, I would expect that a natural language task is used instead.\n- For ICL results, it seems the performance drops significantly (not gradually) from 70% above, do you have intuition why?\n- As the paper only use some particular pruning methods, do you have any opinion on whether the same findings will hold for other pruning methods? It would be good to have a short discussion on the difference between them.\n\n**Things to improve the paper**\n- Section 5 is hard to follow without examples, there are multiple notations without explanation, e.g. K (page 7), x, D=4, N=32, etc\n- Would be good to release the version of datasets that are used for benchmarking"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620645629,
            "cdate": 1698620645629,
            "tmdate": 1699636311852,
            "mdate": 1699636311852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x7LMxaJcSm",
                "forum": "ldJXXxPE0L",
                "replyto": "LhxH9IY9kz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful feedback and questions!\n\n> Although down-scaling and pruning are the main topic of the paper, the technical details on methods used is very limited (even in the Appendix too). If space is an issue, I would suggest to cut down the paper motivation which is repeated multiple times throughout the paper.\n\nWe are working on including more details about the pruning techniques to make our work self-contained. Will update once we apply the edits.\n\n> Relatedly, there is very little discussion regarding down-scaling vs. pruning. For general readers it would be helpful to understand what are the difference between the two, and is one a specific version of the other?\n\nWe can view dense down-scaling as a form of structured pruning. While SparseGPT prunes at the granularity of individual weight, we can view dense-downscaling as pruning at the granularity of entire neurons and entire layers. Some important differences between dense-scaling and pruning include:\n- The order of weights removal and training \u2013 viewing dense downscaling as structured pruning, weights removal happens before training. However, for pruning, weight removal happens after training (and is followed by repair).\n- Granularity of weights removal \u2013 dense downscaling removes entire neurons & layers, whereas pruning removes individual weight.\n\n> For learning tasks evaluation, why only consider task with scalar values as labels? I understand this needs to be something that model can generalize through the examples, but if we focus on language capability of the model, I would expect that a natural language task is used instead.\n\nPlease refer to our global response, additional task evaluation section. We provide additional results on the LAMBADA benchmark, a natural language based ICL task. \nOur focus on synthetic ICL tasks (thus scalar/synthetic labels) is driven by our goal to disentangle fact recall from ICL tasks. Many existing natural language based ICL tasks run the risk of relying on the model's parametric knowledge. For example, one of the ICL examples in GPT3 paper is language translation, which clearly heavily depends on pre-trained knowledge of source and target language. We already know that such pre-trained knowledge is impaired in our QA experiments. Synthetic tasks, due to their synthetic nature, are largely free of such issues.\n\n>For ICL results, it seems the performance drops significantly (not gradually) from 70% above, do you have intuition why?\n\nWe conjecture that ICL learning ability depends on a small set of weights that act as the in-context gradient descent module [1]. At 70% and above, SparseGPT may eventually decide to remove this small set of weights, causing sudden drop of accuracy.\n\n> As the paper only use some particular pruning methods, do you have any opinion on whether the same findings will hold for other pruning methods? It would be good to have a short discussion on the difference between them.\n\nWe believe our findings will generalize to other pruning methods. This is because our findings not only apply to pruning (SparseGPT, Wanda), but also to dense down-scaling. The consistency of our results between sparse and dense down-scaling provides evidence that our observation is fundamentally about model size reduction in general, measured in the number of parameters. \n\n> Section 5 is hard to follow without examples, there are multiple notations without explanation, e.g. K (page 7), x, D=4, N=32, etc\n\nWe will make sure to improve our writing and let you know once we finish updating the paper!\n\n> Would be good to release the version of datasets that are used for benchmarking\n\nYes we will release the datasets we use!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204269526,
                "cdate": 1700204269526,
                "tmdate": 1700204269526,
                "mdate": 1700204269526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IgDHerI2sU",
            "forum": "ldJXXxPE0L",
            "replyto": "ldJXXxPE0L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_2TYK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_2TYK"
            ],
            "content": {
                "summary": {
                    "value": "Edit - the authors have addressed concerns within my review and also contrasted the novelty of their work with previous pruning/ICL studies.  I'm increasing my score accordingly.\n\nContinuing off recently proposed LLM pruning methods (i.e., SparseGPT and Wanda), the authors explore the effect of LLM pruning on both parametric knowledge (i.e., knowledge memorized by the model) and knowledge learned via ICL.  Towards this end, the authors explore the effect of pruning for pretrained OPT-13B/30B and LLamA-13B/33B models.  The authors main contributions are experiments evaluating the pruned-sparsity levels of the aforementioned models versus (a) parametric knowledge and (b) ICL knowledge, as measured via several Q&A tests; parametric knowledge is measured using a closed-book Q&A test, while ICL knowledge is measured using an open-book Q&A test where the in-context prompt is contains a counter-factual answer compared to the training data.  Furthermore, the authors also test ICL knowledge by providing in-context information detailing parametric functions and measure the models predictive accuracy in this case."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic is important and interesting and, at a high-level, the experimental design make sense.  Furthermore, as LLM parameter sizes continue to grow, the question of how new pruning methods (i.e., LLM-Pruner, SparseGPT, and Wanda) affect a model's parametric and ICL knowledge is important.  However, more care is required to ensure model performance is accurately being measured."
                },
                "weaknesses": {
                    "value": "# Evaluation\nThe major weakness of the paper is the effective testing of LLM parametric and ICL knowledge.  In particular, how do the authors verify that the in-context evidence contradicts a fact present in training data?  Although LLama was trained on\npublicly available data, it is not a simple matter to verify that the answers in the Q&A datasets align or misalign with the massive dataset used to train LLamA (note that, the dataset itself was never released, so all public datasets would have to be evaluated in their entirety for verification).  In this case, it is not clear how Verifying that \"answers do not exist in the pre-training corpus\" is possible.  While the authors discuss previous work which has explored LLMs' abilities to override in-memory/parametric knowledge, such works set up measurement guardrails to do so through fine-tuning, e.g.:\n- \"DissentQA. Neeman et al. (2022) constructed the DissentQA dataset from the NaturalQuestions dataset. It contains pairs of\nquestions and evidence for a made-up answer that is different from the factual one. It assesses\nwhether the model can override its memory formed during pre-training with new context.\"\n\"Given that the anticipated made-up answers are randomized and\ndifferent from the factual ones, the model cannot depend on memorization from its pre-training\ndata to generate responses. This evaluation framework rigorously assesses the model\u2019s ability to\noverride its pre-training data with new, context-specific information.\" <- Neeman et al. (2022) fine-tune their evaluated T5 models, thus ensuring that the parameterized answers are learned and relevant questions and answers are actually counter-factual (relatedly, gold passages are considered in Longpre et al. (2021)). This work does not fine-tune the evaluated LLMs, thus relating to the earlier criticism on the validity of the presented results.\n\nFurthermore, the exact measurement of accuracy used in the paper is potentially incorrect and too conservative for recently release instruction-tuned LLMs like LLaMA and OPT.  From the text:\n- \"Answers are the model\u2019s prompt completions produced by greedy decoding. We report the\npercentage of answers that exactly match ground truth.\" <- Two important remarks: greedy is known to be extremely suboptimal for recent\ninstruction-tuned LLMs, and an exact match is not necessarily a fair metric.  Such chat models are known to be extremely wordy, so if the model produces some lead up text followed by the correct answer, this metric discounts such a correct response.  For the former, it makes sense as a fair, reproducible benchmark across different sparsity percentages per model (e.g., nucleus sampling would produce differing results between runs), please include in the text why greedy is used.  However, note that the latter is an extremely important problem which biases all related results.\n\n# Claims\nSeveral claims require revision or further discussion.  In general, wrt to key contributions, it is necessary to discuss how the presented methodology differs from previous work.  E.g., the SparseGPT paper itself reports zero-shot performance for different datasets at different sparsity levels (which effectively tests parametric knowledge), how does the presented benchmark differ from this?  Why does the presented work differ in conclusions wrt parametric knowledge compared to the SparseGPT paper, i.e., SparseGPT showed high sparsity while retraining zero-shot performance.  Why is this not the case in the presented work?  These types of questions, and their ensuing answers/justifications, require significant discussion.  More examples from the paper:\n- \"From work on image classification, however, we know that down-scaling neural networks affects more than just top-line metrics or task accuracy. Pruning, for example, can introduce biases (Hooker et al., 2019) or disproportionate affects on certain subsets of the data (Jin et al., 2022).\" <- This claim is too strong, it makes it seem as though it is a certainty that such effects occur given down-scaling.  However, a significant amount of work has shown that pruning is an effective tool for vision models.\n- \"It is difficult to assess these abilities in isolation, as a standard downstream task needs to process the\ninformation provided in context as well as access the information stored in weights.\" <- Please contrast related work which has previously explored zero-to-many shot ICL performance (across different target applications); see the following for an extensive overview:\nDong et al, \"A Survey on In-context Learning\", https://arxiv.org/pdf/2301.00234.pdf\n- \"Improve inference efficiency. Our work reveals that scaling down model size alone has little impact\non tasks demanding processing information in the LLM\u2019s context. Practitioners may thus use our\nfindings to identify scenarios where decisions could be routed to a smaller model instead of a larger\none without hurting task performance (Chen et al., 2023; Dohan et al., 2022).\" <- The latter work already explores how the parameter size affects performance.  In particular, the Wanda paper already tackles the question of how pruning affects ICL performance (and compares to SparseGPT)\n- \"Our work differs from prior scaling studies in two ways: while prior work (Kaplan et al., 2020b) studies\njoint scaling of both pre-training corpus size and model size, we focus on scaling model size alone.\nFurthermore, instead of measuring task performance, we focus on foundational capabilities of LLMs\u2013fact recall and ICL. These capabilities drive the success for many real world applications of LLMs\" <- This is wrong for a number of reasons.  Firstly, \"we focus on scaling model size along\" is not a valid contribution, as this would, by definition, be provided in the study of \"joint scaling of both pre-training corpus size and model size.\"  Secondly, the work of Kaplan does not study pruning, but rather LLM model size->training->resulting performance.  It is necessary to demarcate the difference between these two paradigms.\n-\"In-weight versus in-context learning\" <- Please explain how the considered work differs from Longpre et al 2022, which extensively explores In-weight versus in-context learning.\n- \"the versatility of LLMs calls for a different approach to assessing pruned models. Our work begins to fill this gap, proposing to evaluate pruning\u2019s effect on fact recall and ICL.\" <- As previously mentioned, what the authors define as fact recall is equivalent to the task of zero-shot question answering; the effect of pruning on various tasks has been explored, e.g., within the papers of the pruners specifically used within this work (SparseGPT and Wanda), as well as in the LLM-Pruner paper.  Furthermore, the effect of pruning an LLM to various sparsity levels on ICL was extensively explored in the Wanda paper.  Please revise your contributions, and position them within the context of previous works.\n-\" In all the above settings, as a simple point of comparison, we measure the effect of downscaling on perplexity\" <- Please note in the paper that this was previously considered in both the SparseGPT and Wanda papers.\n-\"  we focus on foundational capabilities of LLMs \u2013 fact recall and ICL\" <- Fact recall is zero-shot Q&A, which may be thought of as a specific task.  Please adjust this claim.\n\n# Presentation\nOverall, the writing and presentation of the discussed work could be significantly improved.  E.g.:\n- \"removing more than 30% of weights leads to significant (> 5%, relative) accuracy degradation on fact recall related tasks (Figure 1, left). Fact recall suffers similarly from dense down-scaling.\" <- Please have some text which segways from the first paragraph (page 2) to the list of 3 bold-faced items.  The intro currently reads like a collection of text/paragraphs which do not blend into one another.  E.g., combine all the bold-faced-starting paragraph in page 2 into a single paragraph, which: -States the paper shows the following dichotomy wrt pruning LLMs.  For fact recall/parametricknowldge, minimal pruning significantly degrades performance.  [insert your bold-faced-starting text here] In stark contrast, large-scale  pruning does not significantly degrade ICL performance. [insert your second bold-faced-starting text here]. [insert your third bold-faced-starting text here]\n- Same comment for italicized-starting-text, which proceed bold-starting-text; please segway the various paragraphs together.  It is very difficult for a reader to understand the point that is trying to be made when sentences exist independently."
                },
                "questions": {
                    "value": "-Why did the authors not consider the GINC dataset for ICL, from Xie et al's \"An Explanation of In-context Learning as Implicit Bayesian Inference?\"\n\n-\"From the OPT family, we evaluate the two largest models that fit in our hardware\nsetup\" <- Please state the hardware setup\n\n-In Table 1, please define what is meant by \"Context Type\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3569/Reviewer_2TYK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702107514,
            "cdate": 1698702107514,
            "tmdate": 1700604489862,
            "mdate": 1700604489862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CvNKYLnOGF",
                "forum": "ldJXXxPE0L",
                "replyto": "IgDHerI2sU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - Evaluation"
                    },
                    "comment": {
                        "value": "We appreciate the efforts that go into providing us with so much feedback! We strive to address them fully, and we look forward to hearing back!\n\n> The major weakness of the paper is the effective testing of LLM parametric and ICL knowledge. In particular, how do the authors verify that the in-context evidence contradicts a fact present in training data? Although LLama was trained on publicly available data, it is not a simple matter to verify that the answers in the Q&A datasets align or misalign with the massive dataset used to train LLamA (note that, the dataset itself was never released, so all public datasets would have to be evaluated in their entirety for verification). In this case, it is not clear how Verifying that \"answers do not exist in the pre-training corpus\" is possible. While the authors discuss previous work which has explored LLMs' abilities to override in-memory/parametric knowledge, such works set up measurement guardrails to do so through fine-tuning\n\nWe agree with your point and took actions to address your concern. It is indeed nearly impossible to verify that counter-factual answers do not exist in pre-training corpus. We amended the phrasing of our experiment design to avoid making absolute statement about dataset contamination. Furthermore, we quantify the extent to which they exist in the pre-training corpus, by running the counterfactual QA evaluation using dense models without the counterfactual context \u2013 we count the model\u2019s answer as correct if and only if it matches the counterfactual answer. If the training dataset of LLaMA is contaminated with the counterfactual question/answers, we should expect to see reasonably high accuracy under this setup. Instead, we observe near-zero accuracy for both LLaMA-13 and LLaMA-33B model (0.1% for both LLaMA-13B and LLaMA-33B).\n\n>Furthermore, the exact measurement of accuracy used in the paper is potentially incorrect and too conservative for recently release instruction-tuned LLMs like LLaMA and OPT. From the text:\n\"Answers are the model\u2019s prompt completions produced by greedy decoding. We report the percentage of answers that exactly match ground truth.\" <- Two important remarks: greedy is known to be extremely suboptimal for recent instruction-tuned LLMs, and an exact match is not necessarily a fair metric. Such chat models are known to be extremely wordy, so if the model produces some lead up text followed by the correct answer, this metric discounts such a correct response. For the former, it makes sense as a fair, reproducible benchmark across different sparsity percentages per model (e.g., nucleus sampling would produce differing results between runs), please include in the text why greedy is used. However, note that the latter is an extremely important problem which biases all related results.\n\nTo begin with, both LLaMA[4] and Pythia[5] models (which we study) use greedy decoding and exact match during evaluation. So our experimental design is standard as is. Furthermore, we do not use instruction-tuned models, so verbosity should not be a concern either. Nevertheless, we collected extra results to address your concern about decoding and wordiness; and show that our conclusion holds up to additional scrutiny. We re-run key subset of our experiments using a more advanced sampling scheme and more tolerant matching criteria according to your proposal. Specifically, we repeat our LLaMA-13B and LLaMA-33B experiments on TriviaQA benchmark with and without context. We use beam search with beam size of 3 (some paper in the past, e.g., OPT[6], GPT3[7] used beam search during evaluation). To get an answer from the model, we generate 32 tokens from the model, and check whether the answer appears anywhere within the generated tokens. The average number of tokens for TriviaQA answer is 4.7, so a model can generate extra tokens irrelevant to the answer whilst still correctly answering the question. Here\u2019re our results:\n- Our conclusion still holds in this setting: using our current experimental design, averaging over two LLaMA models, one can prune 30% and 40% weights on Closebook and Openbook TriviaQA task, respectively. Using beam search, one can prune 30% and 50% weights on Closebook and Openbook TriviaQA tasks, respectively. The ability to retrieve answers from provided context remains more resilient to pruning than the ability to recall information learnt during pre-training. \n- Additionally, we observe a noticeable accuracy boost likely due to better decoding strategy using beam search (about 10-20% accuracy improvement). This is expected, as beam search with beam size of 3 costs ~3x the compute of greedy decoding.\n- Please see full pruning curves and comparison with greedy decoding in Appendix L of our newly updated paper draft."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203226350,
                "cdate": 1700203226350,
                "tmdate": 1700203382473,
                "mdate": 1700203382473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JEzv8kla7O",
                "forum": "ldJXXxPE0L",
                "replyto": "IgDHerI2sU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - Claims (1/2)"
                    },
                    "comment": {
                        "value": "> \"From work on image classification, however, we know that down-scaling neural networks affects more than just top-line metrics or task accuracy. Pruning, for example, can introduce biases (Hooker et al., 2019) or disproportionate affects on certain subsets of the data (Jin et al., 2022).\" <- This claim is too strong, it makes it seem as though it is a certainty that such effects occur given down-scaling. However, a significant amount of work has shown that pruning is an effective tool for vision models.\n\nWe agree. Our use of the word \u201ccan\u201d intends to convey the uncertainty. If you think this is insufficient, we are happy to be more precise about the precondition of the said phenomenon \u2013 \u201cPruning to very high sparsity, for example, can introduce biases\u2026\u201d\n\n> \"It is difficult to assess these abilities in isolation, as a standard downstream task needs to process the information provided in context as well as access the information stored in weights.\" <- Please contrast related work which has previously explored zero-to-many shot ICL performance (across different target applications); see the following for an extensive overview: Dong et al, \"A Survey on In-context Learning\", https://arxiv.org/pdf/2301.00234.pdf\n\nThank you for sharing the survey on ICL. Our definition of ICL is consistent with the one provided in the survey. The survey summarizes work on various aspects of ICL (algorithmic mechanisms behind it, evaluation of ICL, benchmarks, improving ICL, ICL in smaller models via distillation, etc.), but does not mention any work attempting to disentangle ICL and fact recall, or evaluating how pruning affects ICL, thus providing further evidence on the novelty of our contribution.\n\n> \"Improve inference efficiency. Our work reveals that scaling down model size alone has little impact on tasks demanding processing information in the LLM\u2019s context. Practitioners may thus use our findings to identify scenarios where decisions could be routed to a smaller model instead of a larger one without hurting task performance (Chen et al., 2023; Dohan et al., 2022).\" <- The latter work already explores how the parameter size affects performance. In particular, the Wanda paper already tackles the question of how pruning affects ICL performance (and compares to SparseGPT)\n\nPlease also refer to our global response for a discussion of our contribution within the context of existing pruning work.\n\nFrugalGPT[8] (Chen et al.,), SparseGPT[1] and Wanda[2] indeed all investigated the task performance-size/cost trade-off of LLMs. However, when a new task emerges outside of those examined by these prior studies, how can a practitioner know if it is a good idea to route it to a smaller model? Our work provides insight in this scenario: practitioners can assess how much the said task relies on parametric knowledge versus context processing capability. The more a task relies on context processing, the more likely such a task can be routed to a smaller model.\n\n> \"Our work differs from prior scaling studies in two ways: while prior work (Kaplan et al., 2020b) studies joint scaling of both pre-training corpus size and model size, we focus on scaling model size alone. Furthermore, instead of measuring task performance, we focus on foundational capabilities of LLMs\u2013fact recall and ICL. These capabilities drive the success for many real world applications of LLMs\" <- This is wrong for a number of reasons. Firstly, \"we focus on scaling model size along\" is not a valid contribution, as this would, by definition, be provided in the study of \"joint scaling of both pre-training corpus size and model size.\" \n\nWe use this statement to highlight a difference in methodological choice \u2013 we want to be upfront about the more focused and thus narrower scope of our study. To be clear, we use this statement to disambiguate, not to present a contribution.\n\n> Secondly, the work of Kaplan does not study pruning, but rather LLM model size->training->resulting performance. It is necessary to demarcate the difference between these two paradigms.\n\nWe agree. The difference is important \u2013 1). In addition to dense scaling (Sec 6), which follows the paradigm of Kaplan et al, we also studied pruning. 2). Instead of perplexity, we assess LLMs on its ability to recall information from pretraining vs process contextual information. Both differences are also our key contributions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203596481,
                "cdate": 1700203596481,
                "tmdate": 1700203776750,
                "mdate": 1700203776750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LoISLMvuOK",
                "forum": "ldJXXxPE0L",
                "replyto": "IgDHerI2sU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - Claims - 2/2"
                    },
                    "comment": {
                        "value": "> \"In-weight versus in-context learning\" <- Please explain how the considered work differs from Longpre et al 2022, which extensively explores In-weight versus in-context learning.\n\nThis paper focuses on constructing experiments that contradict existing knowledge in a variety of ways, including changing the answer to an alias, changing the type of the answer. Alongside DisentQA they provide key methodological foundations to our work (specifically in the counterfactual QA setup). Our work differs from this in that 1). Our work studies pruning, and 2). Our work investigates context-processing abilities beyond answering counter-factual questions \u2013 we also construct in-context learning tasks to learn parametric functions to stress test a model\u2019s ability to process information in context.\n\n> \"the versatility of LLMs calls for a different approach to assessing pruned models. Our work begins to fill this gap, proposing to evaluate pruning\u2019s effect on fact recall and ICL.\" <- As previously mentioned, what the authors define as fact recall is equivalent to the task of zero-shot question answering; the effect of pruning on various tasks has been explored, e.g., within the papers of the pruners specifically used within this work (SparseGPT and Wanda), as well as in the LLM-Pruner paper. Furthermore, the effect of pruning an LLM to various sparsity levels on ICL was extensively explored in the Wanda paper. Please revise your contributions, and position them within the context of previous works. \n\n> we focus on foundational capabilities of LLMs \u2013 fact recall and ICL\" <- Fact recall is zero-shot Q&A, which may be thought of as a specific task. Please adjust this claim.\n\nWe emphasize that fact recall is not equivalent to zero-shot closebook question answering (correct us if we\u2019re wrong, by zero-shot QA we believe you mean zero-shot **closebook** QA). The model may rely on the ability to recall parametric knowledge even in openbook QA settings. Hence we argue that no single task exclusively test the model\u2019s ability to recall information from pretraining versus its ability to process information in context. Hence a primary contribution of ours is to disentangle the assessment of both abilities to the best of our abilities. Please refer to our global response for the discussion of our contribution in the context of prior pruning work.\n\nNevertheless, we are happy to better acknowledge the work done by prior pruning research, and revised our phrasing to the following: \u2026 approach to assessing pruned models. Prior work does a commendable job to assess model size and task accuracy trade-off. Our work continues to expand our toolkits for empirical assessment, proposing to evaluate pruning\u2019s effect on fact recall and ICL.\n\n> \" In all the above settings, as a simple point of comparison, we measure the effect of downscaling on perplexity\" <- Please note in the paper that this was previously considered in both the SparseGPT and Wanda papers. -\" \n\nAgree. Revised to credit this to prior pruning papers."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203750621,
                "cdate": 1700203750621,
                "tmdate": 1700204781433,
                "mdate": 1700204781433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JGlQbtmbR2",
                "forum": "ldJXXxPE0L",
                "replyto": "T5qa2B9fq8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Reviewer_2TYK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Reviewer_2TYK"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "I've read all reviews and responses.  I thank the authors for their extensive answer to all my questions/remarks and increased my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604180242,
                "cdate": 1700604180242,
                "tmdate": 1700604180242,
                "mdate": 1700604180242,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N371zoQP03",
            "forum": "ldJXXxPE0L",
            "replyto": "ldJXXxPE0L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_xD3m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_xD3m"
            ],
            "content": {
                "summary": {
                    "value": "Pruning parameters from large language models can affect aspects of model performance differently. The authors strive to characterize these effects by separating fact recall from in context learning. They explore the relative impact of pruning on several different tasks using several base models and multiple pruning techniques. Overall they find that even moderate pruning can degrade fact recall settings, here in-context learning seems more robust."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The settings for evaluating fact recall and in context learning seem useful in general.\n\nMultiple settings for pruning to push for more robust result interpretations\n\nMultiple model families were used in evaluation.\n\nA range of tasks were presented."
                },
                "weaknesses": {
                    "value": "Fact Recall and In Context Learning are some reasonable aspects, but the authors could have considered more. Detailed Instruction Following, and Heavy Reasoning feel like other key aspects, as well as the ability to learn from Few Shot inline. I would have loved to see some more details.\n\nAre all In Context Learning tasks equally difficult? Could a few more gradations be helpful here?\n\nAre there a few more settings that one could use for evaluating model performance? The set of tasks seems rather small.\n\nI'm assuming that pruning is primarily used to increase inference speed, right? If that's the case, I'd like to see tradeoffs between accuracy and inference speed be presented here."
                },
                "questions": {
                    "value": "It seems that Dense Pruning of 30B -> ~13B underperforms the unpruned 13B param model, right? I'd love to see more discussion here about what is going on there."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824831774,
            "cdate": 1698824831774,
            "tmdate": 1699636311662,
            "mdate": 1699636311662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ou4vA9pPtI",
                "forum": "ldJXXxPE0L",
                "replyto": "N371zoQP03",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your helpful feedback! Here's our response to your comments.\n\n> Fact Recall and In Context Learning are some reasonable aspects, but the authors could have considered more. Detailed Instruction Following, and Heavy Reasoning feel like other key aspects, as well as the ability to learn from Few Shot inline. I would have loved to see some more details.\n> Are there a few more settings that one could use for evaluating model performance? The set of tasks seems rather small.\n\nPlease see the additional task evaluation section of our global response. Specifically, we present additional ICL task evaluation to learn algorithmic solutions to problems in context. The solution thus requires some algorithmic reasoning. For the ability to learn from few-shots inline, we emphasize that our ICL task evaluation setup examines exactly this ability. Model learns to perform novel tasks based on few-shot examples we provide in-context.\n\nAs for detailed instruction following, since all six models we examine are pre-trained models without instruction-tuning, we feel like this may be out of scope for our current work.\n\n> Are all In Context Learning tasks equally difficult? Could a few more gradations be helpful here?\n\nIn Appendix E, we varied the difficulty of in-context learning tasks by changing the input dimensionality, and observed that ICL tasks nevertheless remain highly resilient to pruning. This observation corroborates our main conclusion.\n\n> I'm assuming that pruning is primarily used to increase inference speed, right? If that's the case, I'd like to see tradeoffs between accuracy and inference speed be presented here.\n\nIt is true that the ultimate goal of pruning is to increase inference speed. However, pruning studies usually appear before the relevant hardware support is available. For example, seminal work done by Han et al. showed the first promising results of applying pruning techniques to deep neural network models in 2015. However it is not until 2020 that nVidia rolled out the first GPU with hardware support for sparsity. Our work focuses on unstructured sparsity, which is not supported natively by all current generations of GPUs and TPUs; nonetheless, we believe our results will facilitate the real-world adoption of pruning techniques like SparseGPT and Wanda."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202846642,
                "cdate": 1700202846642,
                "tmdate": 1700204340709,
                "mdate": 1700204340709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5piCQJh0Sc",
            "forum": "ldJXXxPE0L",
            "replyto": "ldJXXxPE0L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_ecKe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3569/Reviewer_ecKe"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the effects of weight pruning, a popular technique for reducing model size, on the two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in context. They find that existing pruning techniques affect these two abilities of LLMs quite differently. The paper presents a detailed analysis of the experimental results, which show that the effects of down-scaling LLMs depend on the specific pruning technique used. The authors conclude that there is a trade-off between model size and performance, and that the optimal model size depends on the specific task and dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper investigates the impact of down-scaling large language models on their capabilities, which is an important topic in the field of natural language processing. \n- The authors provide a detailed analysis of the experimental results, which can help researchers and practitioners better understand the trade-offs between model size and performance. \n- The paper provides insights into the development of more efficient language models, which are becoming increasingly important for a wide range of natural language processing tasks."
                },
                "weaknesses": {
                    "value": "- The paper is empirical in nature, and the authors acknowledge that their observations may not generalize to the full spectrum of tasks and large language models. \n- The study focuses on evaluating two pruning algorithms that are unstructured pruning, evaluation on structured pruning methods are expected. \n- The study could include other types of tasks, like NLI, classification, summarization, to make the study more solid."
                },
                "questions": {
                    "value": "How the structured pruning methods, e.g., LLM-Pruner, performs on these two LLM capabilities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699015964569,
            "cdate": 1699015964569,
            "tmdate": 1699636311550,
            "mdate": 1699636311550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QnUwbUVjJg",
                "forum": "ldJXXxPE0L",
                "replyto": "5piCQJh0Sc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3569/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your helpful feedback!\n\n> The study focuses on evaluating two pruning algorithms that are unstructured pruning, evaluation on structured pruning methods are expected.\n> How the structured pruning methods, e.g., LLM-Pruner, performs on these two LLM capabilities?\n\nWe are working on providing some structured pruning results. We will get back to you once results become available.\n\n> The study could include other types of tasks, like NLI, classification, summarization, to make the study more solid.\n\nPlease see additional task evaluation in our global rebuttal response. We present 4 additional task evaluations including lambada, translation and additional ICL classification tasks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202660699,
                "cdate": 1700202660699,
                "tmdate": 1700202660699,
                "mdate": 1700202660699,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]