[
    {
        "title": "Speech language models lack important brain-relevant semantics"
    },
    {
        "review": {
            "id": "wzIchj5NZC",
            "forum": "eoB6JmdmVf",
            "replyto": "eoB6JmdmVf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_jzFj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_jzFj"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a detailed analysis predicting fMRI activity from the activations of text-based and speech-based neural network models. The fMRI dataset studied contains data obtained during both reading and listening to the same story, allowing the authors to directly compare how each type of model can predict a given modality. The authors build a regression model between low-level features and the model activations and use the residuals of this model to also predict the neural responses, which will evaluate how much of the prediction is due to correlations of the stimuli with low-level features encoded by the model. The authors demonstrate that although early visual areas can be predicted by the responses of such language networks, this is primarily driven by low-level cues. Similarly, although early auditory areas are somewhat predicted by the text-based model features, this is due to the low-level cues."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The dataset that the authors use provides a compelling way to get at the question of whether the features of language models trained on one modality (speech or text input) are able to predict the other modality). The authors claim that prior work presented puzzling evidence of language models predicting early sensory regions of a modality that they were not trained on, and this work provides evidence that this is primarily due to features that are correlated between the two modalities. The methodology is also very clearly described and well-documented."
                },
                "weaknesses": {
                    "value": "W1: The paper lacks a general discussion about correlated features in natural stimuli, which, in my view, is directly what the authors are trying to test. Citing some past work along these lines would be helpful (for instance, Norman-Haignere et al. 2018, Groen et al. 2018). \n\nW2: In various parts of the paper, there seems to be a bit of a conflation with correlation and causation. For instance, the authors state: \u201cThis raises questions about what types of information text-based language models capture that is relevant to early auditory processing.\u201d, but I think the authors mean something like \u201cThis raises questions about what types of information text-based language models capture that is **correlated with features** relevant to early auditory processing\u201d. Another instance: \u201cThis is possibly because the language models do not process visual features such as edges.\u201d->\u201cThis is possibly because the **features in language models are not correlated with visual stimulus features** such as edges.\u201d There are other places like this in the paper that I encourage the authors to fix. \n\nW3: The title of the paper seems a bit limited in scope and focuses on a result in the paper that is a bit of a straw man. The speech-based models that were tested (wave2vec2.0 and whisper) are not trained to capture semantic information, so it doesn\u2019t seem surprising that they fail to capture semantic-driven responses."
                },
                "questions": {
                    "value": "Q1: In the first paragraph of the introduction, which of the cited papers claims that text-trained language models predict early sensory cortex activities? This is currently unclear (many of the cited works listed under \u201cspeech\u201d only study models with a waveform or spectrogram input), and seems very important to distinguish as is the main motivation for the study. \n\n\nQ2: Are there some baseline models that the authors could include to better contextualize the results? For instance, how well does a random-feature baseline do (useful to see a \u201clower bound\u201d)? How well does a classic primary-area model such as the motion energy model, or a spectrotemporal filter bank do at capturing the primary area responses? For the classic model comparison, if neither language model is predicting the voxels better than these baseline models, then should we be considering the variance they are explaining as significant at all? \n\n\nQ3: Is ridge regression necessary when fitting the low-level features to the neural model representations? It seems like there is nearly infinite available data for this fit (I believe the low-level features are automatically extracted) and there is no noise, so I\u2019m just wondering what the intuition is for using a ridge parameter here.  \n\n\nQ4: Are the visual words and the onset of auditory words aligned in time in the dataset? That seems particularly important for this comparison, as one of the overall features encoded by both models may be \u201cwhen a new thing starts\u201d. \n\nMinor points: \n\n* The ROIs are described as \u201clanguage relevant\u201d but early visual and early auditory areas are included. It seems non-standard to refer to these as \u201clanguage relevant\u201d. \n\n* There is a sentence about the estimated noise ceiling being based on an assumption of a perfect model. Is the \u201cperfect model\u201d referred to here the regression model used for the participant-particiant regression? Or is this \u201cperfect model\u201d one of the candidate encoding models from DNN predictions? Clarifying what this sentence means would help the reader."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627500181,
            "cdate": 1698627500181,
            "tmdate": 1699636683423,
            "mdate": 1699636683423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aQ119YSFOd",
                "forum": "eoB6JmdmVf",
                "replyto": "wzIchj5NZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We thank the reviewer for their insightful and valuable comments and suggestions which are crucial for further strengthening our manuscript.*\n\n**Q1: In the first paragraph of the introduction, which of the cited papers claims that text-trained language models predict early sensory cortex activities? This is currently unclear (many of the cited works listed under \u201cspeech\u201d only study models with a waveform or spectrogram input), and seems very important to distinguish as is the main motivation for the study.**\n\nThank you for the question. \n* We want to clarify that all cited papers related to text-based language models predict early sensory activity to an impressive degree. It is important to note that these studies used secondary auditory areas that are also part of language regions. \n* In case of speech-based language models, [Vaidya et al. 2022 ICML, Millet et al. 2022 NeurIPS] have shown that all SSL based models predict the early auditory cortex (A1 region) to an impressive degree and trails behind simple word embeddings across the rest of the cortex. \n\n*[Vaidya et al. 2022] Self-supervised models of audio effectively explain human cortical responses to speech, ICML-2022*\n\n*[Millet et al. 2022] Toward a realistic model of speech processing in the brain with self-supervised learning. NeurIPS-2022*\n\n**Q2: Are there some baseline models that the authors could include to better contextualize the results?\nHow well does a classic primary-area model such as the motion energy model, or a spectrotemporal filter bank do at capturing the primary area responses? For the classic model comparison, if neither language model is predicting the voxels better than these baseline models, then should we be considering the variance they are explaining as significant at all?**\n\n* Based on the reviewer\u2019s suggestion, we now plot the average normalized brain alignment for classical models, such as low-level stimulus features (textual, speech, and visual) during both reading and listening in the early sensory areas (early visual and A1), as shown in **Fig. 19 (see section G in Appendix)**. \n* Additionally, we report results for the individual low-level stimulus features as baseline models, including the number of letters, PowSpec, phonological features, and motion energy features, particularly in early sensory processing regions. It appears that both text-based and speech-based language models meet the baselines and show improvement in early sensory processing regions, particularly early visual areas in reading and A1 areas during listening. \n* Among low-level stimulus features, motion energy features have better normalized brain alignment during reading in the early visual area and phonological features have better brain alignment during listening in the A1 region.\n\n* Overall, in the context of classic model comparison, language model representations predict better than baseline models and the variance explained by these models is significant.\n\n**Q3. Is ridge regression necessary when fitting the low-level features to the neural model representations?**\n\n* We fit the removal method using the stimulus text, which is limited by the fMRI dataset. This is why we use ridge regression for the fitting procedure to minimize possible overfitting. \n* The reviewer\u2019s suggestion to fit a more general removal approach using additional text data is very interesting to explore in future work\n\n**Q4. Are the visual words and the onset of auditory words aligned in time in the dataset? That seems particularly important for this comparison, as one of the overall features encoded by both models may be \u201cwhen a new thing starts\u201d.**\n\n* The same stories from listening sessions were used for reading sessions. Praat\u2019s word representation for each story (W, t) was used for generating the reading stimuli. The words of each story were presented one-by-one at the center of the screen using a rapid serial visual presentation (RSVP) procedure. \n* During reading, each word was presented for a duration precisely equal to the duration of that word in the spoken story. RSVP reading is different from natural reading because during RSVP the reader has no control over which word to read at each point in time. \n* Therefore, to make listening and reading more comparable, the authors matched the timing of the words presented during RSVP to the rate at which the words occurred during listening."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665445033,
                "cdate": 1700665445033,
                "tmdate": 1700665445033,
                "mdate": 1700665445033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uTFUn8f9SU",
                "forum": "eoB6JmdmVf",
                "replyto": "XcUyA291rI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_jzFj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_jzFj"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications and the text changes. After reading the responses and other reviewers' concerns I am deciding to keep my score as-is. This is partially due to the noise-ceiling discussion, which was also my reasoning for asking about the baseline models. Its overall difficult to interpret what the main results are. \n\nFrom reading the responses, one thing I'm noticing is that there seems to be a disagreement about what a 'language model' is. This is why I pointed out that the title of the paper seems like a straw man, and (at least from what I could tell) not the main focus of the paper (which seems to be asking questions about text-based models as well)? Calling wav2vec a language model definitely seems incorrect to me. As the authors note, whisper is a bit different because it does have an audio-conditional language model as a decoder, but it is still typically considered an ASR model so its just a bit non-standard, and I think it is an open question if that decoder captures semantic information. \n\nOverall, I think that the analyses presented in the paper seem interesting and promising, but need a bit more polishing to fully guide readers through their significance and also validate that the analysis methods are correct for the setting."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699170429,
                "cdate": 1700699170429,
                "tmdate": 1700699170429,
                "mdate": 1700699170429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sokl9KhZZT",
            "forum": "eoB6JmdmVf",
            "replyto": "eoB6JmdmVf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_cgRE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_cgRE"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a re-analysis of two datasets of fMRI responses of a story from the Moth Radio Hour. In one cases, subjects listened to the story and in the other case the subjects read the story. The authors attempt to predict responses to these stories using text-based, large language models (BERT, GPT2, FLAN-T5) and audio-based, speech models (wav2vec2.0 and Whisper) using standard, regression-based voxelwise encoding models. They compare the prediction accuracy of these models with variants where they have regressed out the contribution from text-based features, audio- and speech-based features, and low-level visual features. They find that text- and speech-based models show similar overall prediction accuracy in early visual and early auditory regions, while text-based models show superior performance in putatively higher-level visual and language regions. They find that the performance of text-based models is relatively robust in higher-level language regions, maintaining relatively good performance when controlling for text, audio/speech, and visual features, consistent with a response to higher-level linguistic properties. In early sensory areas, there is a greater impact of controlling for these features suggesting that these lower-level features predict more of the response variance, as expected. The trends for the speech model are mostly similar, with the biggest difference being that the controlling for audio and speech features hurts performance more in high-level, language regions, suggesting that these models are not predicting high-level, linguistic properties."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think directly comparing text and speech models is a valuable contribution to the literature.\n\nThe dataset investigated, with matched responses to reading and listening, is interesting and relevant to the questions being addressed. \n\nI think there is value in highlighting the problem of feature correlations, which this paper does well.\n\nThey test a large set of control features. Their controls are more comprehensive than most papers I have seen."
                },
                "weaknesses": {
                    "value": "The conclusion that lower-level features explain a large portion of the variance in lower-level sensory areas is not surprising. The fact that high-level language regions are more robust to lower-level features in the context of text-based language models is also not surprising, since the response is higher-level and the model does not have any visual or auditory information baked into it. The fact that speech models are impacted by including audio and speech features is not as surprising as the authors suggest, since the models are taking audio as input and the representations are only being derived from 2-second stimuli and thus lose much of the higher-level language information that is present in a text-based model. This paper is essentially confirmatory and should be framed as such, in my opinion. \n\nI don\u2019t see the benefit of the \u201cdirect\u201d approach compared with measuring the unique variance of different feature sets using some kind of a variance partitioning framework such as that promoted by the Gallant and Huth labs. Conceptually, the main thing one wants to know is what fraction of the neural response variance can be uniquely explained by a particular model and how much is shared with other models. The direct approach seems like an indirect way to address that question. I also find the term \u201cdirect\u201d unclear? What is direct about it? What would the indirect approach be?\n\nThere is no attempt to understand whether text- and speech-based models account for shared or unique variance from each other, which seems important and natural in the context of this paper. \n\nThere needs to be more detail on the speech models. Minimally, there needs to be a summary of the tasks (e.g., masked token prediction) they were trained on and the maximum possible temporal extent that the models are able to consider. If possible, the authors should extend the window they consider to go beyond 2 seconds to allow the models to potentially incorporate longer timescale linguistic information.\n\nAveraging performance across models is suboptimal because some of the models might be performing quite well, which would be valuable to know. For example, Whisper has been trained on a much broader range of tasks than wav2vec2.0 and it would be useful to know whether it performs better as a consequence. A better choice would be to select the best performing model for the main figure and to put the performance of all models in the appendix. Model selection could be done on training or validation data to prevent overfitting.\n\nIt is unclear how activations from different layers were handled. Were they all combined together? Typically, one selects the best-performing layer in a model using the training or validation set. \n\nThe authors need more detail about the stimuli. They should specify the total duration of the story(ies) in the listening and reading conditions, how many words there were, how the words were presented, and the rate they were presented at. For example, for listening, was this a natural story with a variable word rate? Or were the words presented artificially using a fixed ISI? If they used a variable word rate, how does this impact how the features were calculated? Downsampling does not seem straightforward in this case. For the reading condition, how was the text presented? Was there a word presented every few hundred milliseconds or was a whole sentence presented at once? Similarly how does this impact the feature design?\n\nThe language ROIs includes many regions of the STG that I would consider high-level auditory regions (e.g., respond similarly to native and foreign speech). I would recommend repeating the analyses with the language parcels released by the Fedorenko lab, or at least limiting yourself to the STS. For the early auditory analysis, I think it would be worth repeating these analyses with just the A1 parcel to be more conservative. \n\nThe visual word form area is quite small and challenging to localize:\nhttps://www.pnas.org/doi/abs/10.1073/pnas.0703300104\n\nI suspect the results here reflect what one would see from a generic high-level fusiform visual region. The authors could test this by selecting another nearby region and seeing if the results differ. If the results are similar, I think it is misleading to describe the results as specific to the visual word form area, despite the label provided by the atlas.\n\nI could not follow how the noise ceiling is calculated. What is done with the results from all of the different subsamples? Is there some attempt to extend the results to infinite samples? I am skeptical about calculating a noise ceiling in V1 or A1 for the non-preferred modality. I would expect the noise ceiling to be very close to 0. How was this handled? When possible, it would be preferable to plot both the raw scores and the noise ceiling on the same figure so that you can see both. When you average across voxels for ROI analyses, do you average the noise-corrected values or do you separately average the raw and noise ceiling values and then divide these two numbers?\n\nFor Figure 3, it would be preferable to group by the listening/reading as was done in later figures. The performance between the modalities is not really comparable as these are totally different stimuli (and I am skeptical of the noise ceiling calculation). \n\nThe equations in the section title \u201cRemoval of low-level features from language model representations\u201d make it seem like there is a single regularization term for all of the model features. It seems preferable to do what was done for the neural analyses and to fit a banded ridge model separately on every model feature. The different low-level features have very different dimensionality, so there should be some discussion of how this was handled when concatenating the features. If you z-score each feature than features that have higher-dimensionality will have much more influence. It was also not clear to me how cross-validation was handled here. Did you train and validate on a subset of stimuli and then remove the predicted response on test? How many folds were there? This information about cross-validation should also be specified in the voxel-wise encoding model section. For the banded ridge regression, were the lambdas specified separately varied for each feature set? How do we know that this range is sufficient? It is highly sensitive to the scale of the features. How fine was the grid search?"
                },
                "questions": {
                    "value": "In most cases, I found it easier to include my questions in the weaknesses section. See above. \n\nWhat was the reason for constraining the text window to 20 words? How are the results impacted by this choice?\n\nWhat is the reason for not removing the control features from the neural responses as well? How would doing so impact the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786319119,
            "cdate": 1698786319119,
            "tmdate": 1699636683271,
            "mdate": 1699636683271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z84GkBKpU2",
                "forum": "eoB6JmdmVf",
                "replyto": "sokl9KhZZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We thank the reviewer for their insightful and valuable comments and suggestions which are crucial for further strengthening our manuscript.*\n\n**Q. The influence of low-level features on early sensory processing regions and the resilience of high-level language regions in text-based models, are unsurprising. Similarly, the impact on speech models when incorporating audio features aligns with expectations, making the paper essentially confirmatory in nature.**\n\nIt\u2019s not clear which results about the speech models align with the reviewer\u2019s expectations. We find the following quite surprising and noteworthy:\n* The impressive predictive performance of speech models in the early auditory cortex is NOT entirely accounted for by low-level features. As the reviewer kindly pointed out, we investigate a comprehensive set of low-level features that are thought to relate to processing in the early auditory cortex. We show that, surprisingly, 40-50% of the explainable variance in the early auditory cortex is left to explain after removing the low-level features we consider. This shows that speech-based models capture additional features that are important for early auditory cortex and understanding these further can help us better understand this part of the brain.\n* Another surprising result is that, in contrast to the alignment of speech-based models in the early auditory cortex, the alignment in late language regions is almost entirely due to low-level features. The surprise is coming from the fact that the predictive performance of the late language regions is still good and almost on par with text-based models, and also from the fact that several recent works have proposed speech-based models for good models of language in the brain [Yuanning Li et al. 2023 Nature Neuroscience, Chen et al. 2023 Arxiv]. Our work clearly demonstrates the importance of careful consideration of the reasons behind the alignment that is observed between models and human brain recordings.\n\n*[Yuanning Li et al. 2023] Dissecting neural computations in the human auditory pathway using deep neural networks for speech, Nature Neuroscience*\n\n*[Chen et al. 2023] Do self-supervised speech and language models extract similar representations as human brain?*\n\n**Q. I also find the term \u201cdirect\u201d unclear? What is direct about it? What would the indirect approach be?**\n\nWe thank the reviewer for this question and we will clarify this in the main paper.\n\n* An indirect approach is one which first relates model representations to the human brain, followed by independent examination of the related model to some task performance or behavioral output. For instance, [Schrimpf et al. 2021 PNAS] test the computations of a language model that may underlie human language understanding. This is accomplished by an independent examination of the relationship between the models\u2019 ability to predict an upcoming word and their brain predictivity. Similarly, [Goldstein et al. 2022 Nature Neuroscience] provides empirical evidence that both the human brain and language model engage in continuous next-word prediction before word onset. \n\n* In contrast, the approach we use directly estimates the impact of a specific feature on the alignment between the model and the brain recordings by observing the difference in alignment before and after the specific feature is computationally removed from the model representations.  This is why we refer to this approach as direct.\n\n[Schrimpf et al. 2021]  The neural architecture of language: Integrative modeling converges on predictive processing. PNAS, 2021\n\n[Goldstein et al. 2022]  Shared computational principles for language processing in humans and deep language models. Nature neuroscience, 2022\n\n**Q. I don\u2019t see the benefit of the \u201cdirect\u201d approach compared with measuring the unique variance of different feature sets using some kind of a variance partitioning framework such as that promoted by the Gallant and Huth labs.**\n\nWe hope that the response to the previous question clarified what we mean by a direct approach. The method we use to remove the linear contribution of a feature to a model\u2019s representation is one way to implement such a direct approach. Another method was investigated by previous work [Oota et al. 2023 NeurIPS] and was shown to yield very similar results. Other methods can also be used. One can imagine devising a similar approach based on variance partitioning; however it is not immediately clear to us how to best take into account the estimate of the noise ceiling in the variance partitioning calculation. \n\n[Oota et al. 2023 NeurIPS] Joint processing of linguistic properties in brains and language models. NeurIPS-2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657728401,
                "cdate": 1700657728401,
                "tmdate": 1700657789992,
                "mdate": 1700657789992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GNG27DEuY4",
                "forum": "eoB6JmdmVf",
                "replyto": "sokl9KhZZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q. Whether text- and speech-based models account for shared or unique variance from each other?**\n\n* The direct comparison of text-based and speech-based language models in terms of their brain alignment is not straightforward to do in a rigorous way with models that are currently publicly available. \n* There are many differences between these types of models beyond the data modality, such as the amount of their training data and objective function. Instead of comparing these models that differ in many ways directly, we opt for comparing a model only to itself after removal of a particular feature.  This way we know exactly what a difference in brain alignment is due to: the removal of the specific feature. \n* We agree that directly comparing speech- vs text-based models is an exciting avenue for future work and we hope that it will be possible to do in a scientifically rigorous way soon. We summarize this discussion in the updated manuscript (see Appendix section H).\n\n**There needs to be more detail on the speech models. If possible, the authors should extend the window they consider to go beyond 2 seconds to allow the models to potentially incorporate longer timescale linguistic information.**\n\nThank you for the question. \n* Previous work that considered longer windows (~64 seconds) for speech-based language models found that speech-based representations perform worse at predicting brain recordings outside of the primary auditory area than simple text-based word embeddings (without any context) [Vaidya et al. 2022 ICML, Millet et. al. 2022 NeurIPS]. \n* This implies that even when longer timescales are considered, the speech-based representations do not substantially improve their ability to predict later language regions.  Still, there is room to examine the effect of context window using a direct approach similar to ours, and this would make a nice direction for future work.\n\nThe details of two speech models as follow: \n|Model|Input|Training data size|Loss type|\n|-------|-------|-------|-------|\n|Wav2Vec2.0-base|Waveform|250K hours of raw speech|Masked contrastive loss|\n|Whisper-base|Log Mel spectrogram|680K hours of speech (raw speech+speech tasks)|Masked dynamic loss|\n\n[Vaidya et al. 2022] Self-supervised models of audio effectively explain human cortical responses to speech, ICML-2022\n\n[Millet et al. 2022] Toward a realistic model of speech processing in the brain with self-supervised learning. NeurIPS-2022\n\n**Q. Averaging performance across models is suboptimal because some of the models might be performing quite well: Wav2Vec2.0 vs. Whisper ?**\n\nWe now show the results per model in the updated **Appendix F (see Fig. 17)**. \n* During listening, specifically for the A1 region, we observe that both speech models, Wav2Vec2.0 and Whisper, exhibit high normalized brain alignment. Additionally, the elimination of low-level textual and speech features results in a significant performance decline in both models. However, it is important to note that these language models have additional information other than low-level features that need to be explored to further explain the early auditory region.\n* Similar to the A1 region, we observed that both Wav2Vec2.0 and Whisper exhibit similar normalized brain alignment in late language regions. Moreover, the removal of low-level textual and speech features results in a significant performance decline in both models."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658412953,
                "cdate": 1700658412953,
                "tmdate": 1700658448882,
                "mdate": 1700658448882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JEylBrHfSi",
                "forum": "eoB6JmdmVf",
                "replyto": "21b74FRcAg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_cgRE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_cgRE"
                ],
                "content": {
                    "title": {
                        "value": "Comments on Contribution"
                    },
                    "comment": {
                        "value": "Many thanks for the detailed and thoughtful reply to my comments.\n\nSome additional comments and questions. \n\nI felt I was pretty clear about what results aligned with my expectations:\n\nThe conclusion that lower-level features explain a large portion of the variance in lower-level sensory areas is not surprising. \n\nThe fact that high-level language regions are more robust to lower-level features in the context of text-based language models is also not surprising, since the response is higher-level and the model does not have any visual or auditory information baked into it. \n\nThe fact that speech models are impacted by including audio and speech features is not as surprising as the authors suggest, since the models are taking audio as input and the representations are only being derived from 2-second stimuli and thus lose much of the higher-level language information that is present in a text-based model. \n\nWhat was unclear?\n\nYou state in your rebuttal two key contributions:\n\nThe impressive predictive performance of speech models in the early auditory cortex is NOT entirely accounted for by low-level features. As the reviewer kindly pointed out, we investigate a comprehensive set of low-level features that are thought to relate to processing in the early auditory cortex. We show that, surprisingly, 40-50% of the explainable variance in the early auditory cortex is left to explain after removing the low-level features we consider. This shows that speech-based models capture additional features that are important for early auditory cortex and understanding these further can help us better understand this part of the brain.\n\nI agree this is a potentially valuable contribution although it is not mentioned at all in the abstract. In fact, the abstract seems to emphasize the opposite conclusion:\n\nUsing our direct approach, we find that both text-based and speech-based models align well with early sensory areas due to shared low-level features.\n\nThis finding is not entirely new. For example, the Li et al. (2013) paper cited in their response shows something similar. If this is going to be a major focus of the paper it needs to be motivated and described as such and contrasted with the prior work in terms of what is new.\n\nThe second contribution flagged is: Another surprising result is that, in contrast to the alignment of speech-based models in the early auditory cortex, the alignment in late language regions is almost entirely due to low-level features. The surprise is coming from the fact that the predictive performance of the late language regions is still good and almost on par with text-based models, and also from the fact that several recent works have proposed speech-based models for good models of language in the brain [Yuanning Li et al. 2023 Nature Neuroscience, Chen et al. 2023 Arxiv]. Our work clearly demonstrates the importance of careful consideration of the reasons behind the alignment that is observed between models and human brain recordings.\n\nThe DNN audio models tested here only see 2-second stimuli and are audio-based, so the fact that including audio controls has a greater impact in high-level language regions strikes me as confirmatory. The Li paper is focused on characterizing responses to speech in the STG again mostly focused on short timescales, not high-level language regions in the STS and frontal cortex, like those canonically associated with high-level language regions. I think extending the window beyond 2 seconds would be valuable even if it doesn\u2019t improve prediction accuracy, since that provides a stronger demonstration of the model\u2019s failure and a stronger contrast with the text-based models. \n\nEven though the finding is not terribly surprising in my opinion, I still find it a useful contribution if framed appropriately, since it demonstrates a limitation of existing audio models that needs to be addressed in future work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678859926,
                "cdate": 1700678859926,
                "tmdate": 1700678859926,
                "mdate": 1700678859926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5kcpPC3PAp",
                "forum": "eoB6JmdmVf",
                "replyto": "JEylBrHfSi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_cgRE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_cgRE"
                ],
                "content": {
                    "title": {
                        "value": "Comments on Methods"
                    },
                    "comment": {
                        "value": "Downsampling is typically applied to evenly spaced samples. What is unclear to me is how you handle the fact that the words are not evenly spaced? \n\nI do not see an appendix section J.\n\nA proper noise ceiling estimates the variance explained in the absence of noise. It is not clear why the procedure they describe estimates this quantity. They have a small number of subjects and each subject has noisy data and so the predictions for the left-out subject are likely to be themselves highly imperfect. The fact that this was used in a prior paper does not mean that the procedure is appropriate. One solution would be to call it something else, e.g. \u201cCross-subject prediction accuracy\u201d and then explain clearly in the text what the quantity is and how it differs from a noise ceiling. \n\nI remain highly skeptical about calculating a noise ceiling for visually presented text in A1 (and vice versa). I have a hard time believing that you have any reliable signal. Moreover, the voxel selection procedure you are using is almost certainly going to substantially bias the noise ceiling upwards since you are explicitly selecting voxels with a reliable value. What would happen if you selected voxels in one half of your dataset and then measured the noise ceiling in another set? What would the variance be? Could you even sensibly distinguish the noise ceiling from 0?\n\nI still don\u2019t totally follow what this means: \u201cTo clarify, we first cross-validated on the training data set stimuli. Then for each fold in the banded ridge regression cross-validation, we trained the ridge regression model to remove low-level features to that particular fold. Overall, we performed a 10-fold cross-validation scheme as the training dataset stimuli consists of 10 different stories.\u201d Are you saying that you learned the weights on the training set, selecting the regularization parameter via nested cross-validation on the training set, and then you applied the learned weights on test to derive a prediction and subtracted out the prediction?\n\nYou do not answer this question: For the banded ridge regression, were the lambdas specified separately varied for each feature set? How do we know that this range is sufficient? It is highly sensitive to the scale of the features. How fine was the grid search? You repeat what was said in the methods and say it has been \u201cthoroughly tested\u201d.\n\nThere are several places where the authors provide additional clarifications in their response to me, but do not indicate if and how the manuscript will be changed to improve clarity."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678882436,
                "cdate": 1700678882436,
                "tmdate": 1700678882436,
                "mdate": 1700678882436,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5Kv3aBBbzF",
            "forum": "eoB6JmdmVf",
            "replyto": "eoB6JmdmVf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_1bg5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_1bg5"
            ],
            "content": {
                "summary": {
                    "value": "A research paper submitted by the authors raises concerns about the quality and validity of the study. The reviewer argues that the study is questionable because it is based on fMRI recordings obtained from only six subjects, which is too small a sample size for any publication. Moreover, the methodology used in the study is not new, and the results are obvious, merely showing that LLMs are predictable to the human brain. \n\nWhile the reviewer acknowledges that they may have missed something, they emphasize that the study lacks methodological novelty and scientific rigor. In other words, the research does not bring anything new to the field and fails to meet the basic standards of scientific research. \n\nOverall, the reviewer's critique suggests that the authors need to revisit their study and address the concerns raised by the reviewer in order to produce a more compelling and scientifically robust piece of research."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "It is difficult to determine the strength of the paper, as the contribution appears weak. Furthermore, the use of a small dataset downloaded from the internet, combined with existing analysis methods, fails to provide any novelty. The authors have not even attempted to persuade the reader that such a study makes sense. It is evident that language modeling algorithms (LLMs) are trained to replicate human language. Therefore, it is likely that human brain activity would follow language that sounds or looks natural, just as it would if delivered by a human."
                },
                "weaknesses": {
                    "value": "Lack of methodological novelty; small dataset from another study without validation of its relevance for the authors' analysis; and, most importantly, a lack of argumentation justifying the study."
                },
                "questions": {
                    "value": "1. Why is such a small fMRI dataset used? \n2. What are the technical, methodological, and scientific contributions that would interest the ICLR audience?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699489588384,
            "cdate": 1699489588384,
            "tmdate": 1699636683135,
            "mdate": 1699636683135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qgp4sLUVwa",
                "forum": "eoB6JmdmVf",
                "replyto": "5Kv3aBBbzF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We thank the reviewer for their valuable comments and suggestions. We address the reviewer's concerns point by point below. We hope that the reviewer can reevaluate our work in light of these clarifications.*\n\n**Size of fMRI dataset**\n\nWe thank the reviewer for this suggestion.\n\n* The fMRI dataset we use is a well-known public dataset that has previously been used in many publications in both ML and Neuroscience venues (ML: [Jain & Huth 2018 NeurIPS, Jain et al. 2020 NeurIPS, Antonello et al. 2021 NeurIPS, Lamarre et al. 2022 EMNLP, Vaidya et al. 2022 ICML;], Neuroscience: [Huth et al. 2016 Nature, Huth et al. 2017 Journal of Neuroscience, Deniz et al. 2019 Journal of Neuroscience]).\n* For the kind of analyses that we do and that are common in this area of research, the number of samples per participant is more important than the number of subjects because the predictive models are trained independently for each participant. So, having more samples per participant helps us learn a better predictive model.\n* This dataset is one of the largest datasets in terms of samples per participant (~4000 samples), which is the main reason for its frequent use.\n* Our results also clearly show that this dataset is sufficient to learn a good predictive model, as we show that we can predict up to 75% of the explainable variance for held-out brain recordings that were not used for training (e.g., Fig. 3).\n\n*[Huth et al. 2016], Natural speech reveals the semantic maps that tile human cerebral cortex. In Nature Neuroscience, 2016*\n\n*[De Heer et al. 2017], The hierarchical cortical organization of human speech processing, Journal of Neuroscience, 2017*\n\n*[Jain & Huth, 2018], Incorporating context into language encoding models for fmri. In NIPS-2018*\n\n*[Deniz et al. 2019], The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience, 2019*\n\n*[Jain et al. 2020], Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech, In NeurIPS-2020*\n\n*[Antonello et al. 2021], Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses, In NeurIPS-2021*\n\n*[Lamarre et al. 2022] Attention weights accurately predict language representations in the brain, EMNLP-2022*\n\n*[Vaidya et al. 2022] Self-supervised models of audio effectively explain human cortical responses to speech, ICML-2022*\n\n**Contributions and implications of our work**\n\nOur main contributions are the fine-grained analysis approach of the contributions of different types of information to the brain alignment of speech-based and text-based language models, and the scientific findings of this analysis. We summarize the implications of our findings below and include these in the updated manuscript (**see Appendix section H**).\n\n* Our findings have direct implications for both machine learning and cognitive neuroscience. \n* First, we show that even during speech-evoked brain activity (i.e., listening), the alignment of speech-based models trails behind that of text-based models in the late language regions. More importantly, our results demonstrate that the alignment of speech-based models with these regions is almost entirely explained by low-level stimulus features. Since these regions are purported to represent semantic information, this finding implies that contemporary speech-based models lack brain-relevant semantics. This suggests that new machine learning approaches are needed to improve speech-based models. Furthermore, these results imply that observed similarities between speech-based models and brain recordings in the past [Vaidya et al. 2022 ICML, Millet et al. 2022 NeurIPS] are largely due to low-level information and not semantics, which is important to take into account when interpreting the similarity between language representations in speech-based models and the brain.\n\n* Second, we observe that phonological features explain the most variance during listening for speech-based models, whereas ``number of letters\" explains the most variance during reading for text-based models. This result offers us a glimpse into the mechanisms underlying language processing in the brain. \n\n* Third, we demonstrate a direct residual approach to identify the contribution of specific features to brain alignment. To our knowledge, there is no better alternative to selectively remove information from language models to probe their impact on brain alignment. Using this approach, it is possible to investigate how the human brain processes language during both reading and listening at a finer scale than before. \n\n*[Vaidya et al. 2022] Self-supervised models of audio effectively explain human cortical responses to speech, ICML-2022*\n\n*[Millet et al. 2022] Toward a realistic model of speech processing in the brain with self-supervised learning. NeurIPS-2022*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617185538,
                "cdate": 1700617185538,
                "tmdate": 1700617185538,
                "mdate": 1700617185538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a6NTVu9oVs",
                "forum": "eoB6JmdmVf",
                "replyto": "qgp4sLUVwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_1bg5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Reviewer_1bg5"
                ],
                "content": {
                    "title": {
                        "value": "No change in decision"
                    },
                    "comment": {
                        "value": "The reviewer appreciates the author's feedback. However, despite the long recordings and machine learning training settings that have been optimized for the small subject sample, the results must be validated on a larger participant sample. The current dataset may be popular, but it is important to ensure the validity of the findings. The authors should also provide a more robust defense of the novelty of their approach, and clarify their contribution to the field of cognitive neuroscience."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639707741,
                "cdate": 1700639707741,
                "tmdate": 1700639707741,
                "mdate": 1700639707741,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GxOnT72pH4",
            "forum": "eoB6JmdmVf",
            "replyto": "eoB6JmdmVf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_5at5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6248/Reviewer_5at5"
            ],
            "content": {
                "summary": {
                    "value": "The researchers are trying to understand what kind of information that text-based and speech-based language models are actually predicting about brain activity.  They are using a controlled experimental setup with a known fMRI dataset to systematically investigate brain alignment of language models by eliminating specific low-level textual, speech, and visual features from model representations. Finding reveal that both text and speech-based models align with the brain's early sensory areas due to common low-level features. But when these features were removed, text-based models still aligned well with brain regions involved in language processing, while speech-based models did not. This was unexpected and suggests that speech-based models might need improvement to better mimic brain-like language processing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The findings of this research are important in computational linguistics and neuroscience. \n\nThe paper provides insights into the potential of text-based language models to capture deep linguistic semantics by showing the models can predict brain activity in language-processing regions without low-level features. The observation regarding speech-based models suggests a possible direction to further improve their capability.\n\nThe controlled experimental design and statistical approach seem rigorous to me. The clarity of the paper is good and easy to follow."
                },
                "weaknesses": {
                    "value": "I\u2019m not familiar with the six-participate dataset and not certain if the limited scope of the datasets could affect the generalizability of the results. It would be nice if the author could discuss how to translate the findings translate to different languages, models, and datasets.\n\nWhile the author describes details regarding their experiment setup, it would benefit the community if the author could publish their implementation, especially the low-level feature removal and data preprocessing. These aspects are critical for ensuring reproducibility and are not entirely clear to me. Making this information available would significantly enhance the paper's utility and impact."
                },
                "questions": {
                    "value": "I'm seeking to better understand the application of ridge regression in the context of your study. Specifically, when you remove low-level feature vectors from pre-trained features, could this process potentially alter or diminish the representation of higher-level features? In other words, might the removal of these low-level signals inadvertently affect the model's ability to process more complex, abstract linguistic features that are also captured in these representations?\n\nI would be interested to see more discussion for Figure 10."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6248/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6248/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6248/Reviewer_5at5"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699579536339,
            "cdate": 1699579536339,
            "tmdate": 1699636683004,
            "mdate": 1699636683004,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "whAyyiUSfk",
                "forum": "eoB6JmdmVf",
                "replyto": "GxOnT72pH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6248/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We thank the reviewer for their encouraging words. We address the reviewer's concerns point by point below.*\n\n**I\u2019m not familiar with the six-participate dataset and not certain if the limited scope of the datasets could affect the generalizability of the results. It would be nice if the author could discuss how to translate the findings translate to different languages, models, and datasets.**\n\nWe thank the reviewer for this suggestion. \n* The fMRI dataset we use is a well-known public dataset that has previously been used in many publications in both ML and Neuroscience venues (ML: [Jain & Huth 2018 NeurIPS, Jain et al. 2020 NeurIPS, Antonello et al. 2021 NeurIPS, Lamarre et al. 2022 EMNLP, Vaidya et al. 2022 ICML;], Neuroscience: [Huth et al. 2016 Nature, Huth et al. 2017 Journal of Neuroscience, Deniz et al. 2019 Journal of Neuroscience]). \n* For the kind of analyses that we do and that are common in this area of research, the number of samples per participant is more important than the number of subjects because the predictive models are trained independently for each participant. So, having more samples per participant helps us learn a better predictive model. \n* This dataset is one of the largest datasets in terms of samples per participant (~4000 samples), which is the main reason for its frequent use. \n* Our results also clearly show that this dataset is sufficient to learn a good predictive model, as we show that we can predict up to 75% of the explainable variance for held-out brain recordings that were not used for training (e.g., Fig. 3).\n\nWe expect the main results of the current study to persist even if a similar experiment and analysis were to be conducted in a different language, model, or a dataset. There is, however, value in actually testing this hypothesis and confirming this prediction. We hope to see future work replicating our findings across different languages and datasets. To our knowledge, there are very few naturalistic datasets in different languages. Therefore, to be able to test this prediction it is necessary to collect neuroimaging data using different languages.\n\n*[Huth et al. 2016], Natural speech reveals the semantic maps that tile human cerebral cortex. In Nature Neuroscience, 2016*\n\n*[De Heer et al. 2017], The hierarchical cortical organization of human speech processing, Journal of Neuroscience, 2017*\n\n*[Jain & Huth, 2018], Incorporating context into language encoding models for fmri. In NIPS-2018*\n\n*[Deniz et al. 2019], The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience, 2019*\n\n*[Jain et al. 2020], Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech, In NeurIPS-2020*\n\n*[Antonello et al. 2021], Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses, In NeurIPS-2021*\n\n*[Lamarre et al. 2022] Attention weights accurately predict language representations in the brain, EMNLP-2022*\n\n*[Vaidya et al. 2022] Self-supervised models of audio effectively explain human cortical responses to speech, ICML-2022*\n\n**While the author describes details regarding their experiment setup, it would benefit the community if the author could publish their implementation, especially the low-level feature removal and data preprocessing. These aspects are critical for ensuring reproducibility and are not entirely clear to me. Making this information available would significantly enhance the paper's utility and impact.**\n\nWe thank the reviewer for pointing out open science practices, which are very important to us. We agree with the reviewer that it is critical to make our code openly available so that others can reproduce our results and easily use our approach for their own research questions. Therefore, we are going to publish our implementation, including how we remove low-level features. In this study, we used an openly available fMRI dataset which includes preprocessed BOLD responses. The details of data preprocessing can be found in the original article [Deniz et al. 2019 Journal of Neuroscience]. \n\n*[Deniz et al. 2019], The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience, 2019*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615273637,
                "cdate": 1700615273637,
                "tmdate": 1700617374122,
                "mdate": 1700617374122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]