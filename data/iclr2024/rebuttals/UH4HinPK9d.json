[
    {
        "title": "Provably Accurate ODE Forecasting Through Explicit Trajectory Optimization"
    },
    {
        "review": {
            "id": "7k5FKGkXze",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_U4Us"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_U4Us"
            ],
            "forum": "UH4HinPK9d",
            "replyto": "UH4HinPK9d",
            "content": {
                "summary": {
                    "value": "The paper presents a method for forecasting time series governed by ordinary differential equations (ODEs), focusing on future trajectories. The authors showed that the solution space of such ODEs is a finite-dimensional Riemannian manifold, allowing for applying statistical objectives like maximum likelihood and minimum mean squared error directly in the feasible ODE solutions space. This approach aims to reduce forecasting errors compared to traditional methods not based on the solution manifold, further supported by numerical examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper showed the space of trajectories on a finite time interval I, with a finite degree of freedom, is a finite-dimensional manifold embedded in the space of a square-integrable function.\n\n2. The paper summarized several classic estimation methods for trajectory forecasting and numerically compared their effects on the Lotka--Volterra equation.\n\n3. The paper is very clearly written."
                },
                "weaknesses": {
                    "value": "1. I find the paper presents an overly simple idea with an unrealistic assumption. When the space of parameters $\\Theta$, the space for the inputs $\\mathcal{U}$ are both finite-dimensional, finding the unknown dynamics is, of course, a finite-dimensional problem. However, most of the time,  the space of parameters $\\Theta$/the input space $\\mathcal{U}$ are not finite-dimensional. This is why forecasting dynamics is hard. For example, suppose the initial condition is not known exactly but on an open interval, which is no longer finite-dimensional. In that case, we do not know what type of chaotic behavior will occur in the long-time horizon. The same thing applies to the parameter space. General variable coefficients, even for the smoothest functions in $C^\\infty$, are in an infinite-dimensional space before further assumptions.\n\n2. Even if we settle for finite-dimensional spaces for the initial condition, the parameters, the inputs, etc. (i.e., all the places where one can change the dynamics), the overall dimension can be large. Indeed, $C_{f,I}$ is the range of the forward problem, containing all possible trajectories. It is not possible to characterize the space analytically, so it will have to be done through the finite number of parameters mentioned above. In that sense, (13) becomes the standard parameterized minimization problem, like those we see in regressions. If we know a priori that the dynamics are only subject to a finite number of parameters, no one will use (12), which does not utilize this prior/expert knowledge.\n\n3. The numerical examples are simple, and the white noise assumption for the data, as shown in (17), is also too simple. The proposed method will perform poorly if the noise can be fit into the trajectory manifold $C_{f,I}$.\n\n4. The paper titled \"**Provably accurate** ODE forecasting through explicit trajectory optimization\". I don't see where the \"provably accurate\" is based. Both Theorem 1 and Proposition 1 are results with little to do with ODE forecasting. Theorem 1 is a consequence of the \"finite dimension\" assumption, and Proposition 1 is a property from differential manifold."
                },
                "questions": {
                    "value": "1. The metric tensor K is diagonal, in the sense that it is zero if the time points don't match. This is very special and does not accommodate correlations in time. Why do the authors want to choose this?\n\n2. Is the norm $ || \\cdot||$ in (12) the same as in (13)? If so, are you not giving the manifold a particular metric but using the one for $L^2(I)$?\n\n3. It was $\\hat{x}$ in (13), but became $\\tilde{x}$ in (16)?\n\n4. Figure 2 is a bit confusing. For some, we need to look for the maximum, while for some, we need to look for the minimum. It is better that they are all for maximum or all for minimum. Otherwise, it is confusing and hard to compare across different figures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Reviewer_U4Us"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697393667403,
            "cdate": 1697393667403,
            "tmdate": 1699636619007,
            "mdate": 1699636619007,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d1Noa7TCAL",
                "forum": "UH4HinPK9d",
                "replyto": "7k5FKGkXze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to U4Us, Part 1"
                    },
                    "comment": {
                        "value": "> \"I find the paper presents an overly simple idea with an unrealistic assumption. When the space of parameters $\\Theta$, the space for the inputs $\\mathcal{U}$ are both finite-dimensional, finding the unknown dynamics is, of course, a finite-dimensional problem. However, most of the time, the space of parameters $\\Theta$/the input space $\\mathcal{U}$ are not finite-dimensional. This is why forecasting dynamics is hard. For example, suppose the initial condition is not known exactly but on an open interval, which is no longer finite-dimensional. In that case, we do not know what type of chaotic behavior will occur in the long-time horizon. The same thing applies to the parameter space. General variable coefficients, even for the smoothest functions in $C^\\infty$, are in an infinite-dimensional space before further assumptions.\"\n\nWe suspect there may be some confusion in this concern regarding the meaning of finite-dimensional spaces.\nThe example provided, \"the initial condition is not known exactly but on an open interval,\" is one-dimensional as it is a continuous subset of the real line, a one-dimensional vector space.\nGeneral variable coefficients are indeed allowed by the finite-dimensionality assumption.\n\nThe finite-dimensionality assumption is commonly satisfied by many real-world systems such as robotics or circuits.\nThese come from practical engineering constraints, where construction of arbitrary input functions is infeasible due to physical constraints such as damping in a system.\n\n> \"Even if we settle for finite-dimensional spaces for the initial condition, the parameters, the inputs, etc. (i.e., all the places where one can change the dynamics), the overall dimension can be large. Indeed, $\\mathcal{C}_{f,I}$ is the range of the forward problem, containing all possible trajectories. It is not possible to characterize the space analytically, so it will have to be done through the finite number of parameters mentioned above.\n\nIt is true that the space of solutions cannot be characterized analytically, as even simple ODEs do not have analytical solutions, though the lack of existence of analytical solutions does not cause any issues with inference.\nA lack of analytical solutions does not prohibit the usage of the smooth structure in optimization problems, allowing gradient-based methods to be applied.\n\n> \"In that sense, (13) becomes the standard parameterized minimization problem, like those we see in regressions. If we know a priori that the dynamics are only subject to a finite number of parameters, no one will use (12), which does not utilize this prior/expert knowledge.\"\n\nEquation (13) is *not* the current approach today, and we hope that other researchers begin to adopt it after reading this manuscript.\nWe suspect this is because it was not previously known that the objective is well-behaved enough to solve.\nThis guarantee comes from the core contribution of this work.\n\nNeural ODEs and related work (Chen et al. 2018; Dupont et al., 2019; etc.) fit a trajectory to the observations, similar to a classical regression method.\nIn Bayesian statistics, Equation (12) represents the minimum mean squared error estimator, and is one of the main objectives of MCMC sampling.\n\n> \"The numerical examples are simple, and the white noise assumption for the data, as shown in (17), is also too simple.\"\n\nWe selected the Lotka-Volterra equation because it provides a clear illustration of the difference between the state estimation problem and the forecasting problem. As trajectories of the system form trains of short pulses with period dependent on the initial conditions, successful forecasting is highly dependent on predicting the locations of the pulses. This feature allows clear visualization of a trade-off in estimation: improve the accuracy of the shape of the initial pulse, or choose an initial condition that better captures the timings.\n\nThere is no fundamental dependence on white noise in this work, and any stochastic observation process could be chosen.\nThe core contribution of this work is the theoretical advancement and the guarantee for optimality, irrespective of the noise process.\n\n\n> \"The proposed method will perform poorly if the noise can be fit into the trajectory manifold $\\mathcal{C}_{f,I}$\"\n\nUnless we are misunderstanding the claim, the proposed noise process would result in the forecasting problem being impossible in general, not just for the our technique.\nIf the noise is designed such that different valid solutions of the underlying model are indistinguishable from only the observables (i.e. the distribution of observables is independent of the true process), then the problem would be impossible for any statistical method."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688264686,
                "cdate": 1700688264686,
                "tmdate": 1700688264686,
                "mdate": 1700688264686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wPWIbhACeu",
                "forum": "UH4HinPK9d",
                "replyto": "7k5FKGkXze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to U4Us, Part 2"
                    },
                    "comment": {
                        "value": "> \"The paper titled 'Provably accurate ODE forecasting through explicit trajectory optimization'. I don't see where the 'provably accurate' is based. Both Theorem 1 and Proposition 1 are results with little to do with ODE forecasting. Theorem 1 is a consequence of the 'finite dimension' assumption, and Proposition 1 is a property from differential manifold.\"\n\nTheorem 1 and Proposition 1 enable provably optimal forecasting within a model family.\nThis contribution is distinct from other approaches for which the objective function is dependent explicitly on the parameters rather than on the future predictions.\n\n> \"The metric tensor K is diagonal, in the sense that it is zero if the time points don't match. This is very special and does not accommodate correlations in time. Why do the authors want to choose this?\"\n\nThe choice of a diagonal metric tensor in Equation (6) does not represent a limitation to this work, and the more general form in Equation (5) is applicable.\nAs stated in the manuscript, the diagonal structure in Equation (6) was chosen to enable a re-weighting of the importance of different time horizons, a common requirement in forecasting problems.\nIt represents a useful example which later has a fundamental implication for Section 4.3, where it implies point-wise optimality for the MMSE estimation in the ambient space."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688310024,
                "cdate": 1700688310024,
                "tmdate": 1700688310024,
                "mdate": 1700688310024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IouJsGqUxA",
            "forum": "UH4HinPK9d",
            "replyto": "UH4HinPK9d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_YW1N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_YW1N"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses a new approach for the estimation of controlled ODE solutions based on the geometric properties of flow maps. The authors first provide an overview of the problem and present their main theoretical results. Then, they discuss how the theory translates into practical algorithms and finally show performance on a Lotka-Volterra ODE. Other ODEs are studied in the appendix. In the appendix, one can also find a detailed discussion of some interesting elements, such at tolerance and stepsize selection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is rigorous, very well-written, and interesting. For me, truly a joy to read. I learned a lot from this paper and find the idea very nice. I especially like the fact that the authors guide the reader through proofs and through the literature. This is very helpful. I am arguably not an expert on ODE estimation, but I worked on this a bit and find the contribution quite relevant for applications."
                },
                "weaknesses": {
                    "value": "While I generally like the paper there are some points that need revision and better clarity.\n\n1) Experiments: They work pretty well, but two things are not clear to me (a) assumptions for computation of $\\det D\\psi$ and (b) what is the input to the training pipeline. (a) From the appendix, it seems you compute $\\det D\\psi$ assuming access to the exact solver. From what I understood in the paper, it instead seems you have access only to noisy measurements. (b) this is related. seems from the paper you assume access to a single noisy trajectory, but I guess you actually use more than one. Can you make this more clear to me? \n\n2) Comparisons with other methods: This is totally missing. Only variants of the proposed methods are discussed. While the discussion is still interesting, I would like the authors to show comparisons with other alternatives for solving the problem. In terms of Accuracy, Speed, Assumptions. I am totally happy to revise this score if you are able to show this.\n\nTypos (minor): \n- In Abb B, I would remind the reader of the definition of $\\psi$. Also, in the proof of Lemma 1, $\\forall x_0\\in\\mathcal{X}$, not $x$. \n- After formula 9, I would perhaps explain better why \"this potential concern is unfounded\". I would also tone down the sentence."
                },
                "questions": {
                    "value": "I have a question.\n\n3) In the main paper assumptions, you have that $\\mathcal{U}$ is a manifold. However, in App. B, you assume the inputs have a finite-sum structure. Hence, it seems to me the assumption on the input set is a bit stronger than what you claim. Am I right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753083146,
            "cdate": 1698753083146,
            "tmdate": 1699636618899,
            "mdate": 1699636618899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v0d3fadYDV",
                "forum": "UH4HinPK9d",
                "replyto": "IouJsGqUxA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> \"Experiments: They work pretty well, but two things are not clear to me (a) assumptions for computation of and (b) what is the input to the training pipeline. (a) From the appendix, it seems you compute assuming access to the exact solver. From what I understood in the paper, it instead seems you have access only to noisy measurements. (b) this is related. seems from the paper you assume access to a single noisy trajectory, but I guess you actually use more than one. Can you make this more clear to me?\"\n\nPart (a):\nWe assume knowledge that true model exists in some known parameterized family of ODEs.\nWe then use a numerical solver to evaluate the associated transformations in this model family.\nThese transformations are deterministic functions of the model family, and are not dependent on the observations.\n\nPart (b):\nWe observe a single noisy trajectory.\nThis noisy trajectory induces the probability density on which we operate with the transformations from part (a).\n\n\n> \"Comparisons with other methods: This is totally missing. Only variants of the proposed methods are discussed. While the discussion is still interesting, I would like the authors to show comparisons with other alternatives for solving the problem. In terms of Accuracy, Speed, Assumptions. I am totally happy to revise this score if you are able to show this.\"\n\nWe included simulations for the methods that are most relevant to this work.\nThe core contribution in this work is the recognition that statistical cost functions are well-behaved on the space of solutions of a differential equation.\nIt is for this reason that we compared the optimal solutions to these costs to their counterparts in the state estimation problem.\n\nMethods with weaker assumptions are unable to capture long-term structure of the trajectory (as seen in Appendix A.2).\nFurthermore, other methods emphasizing speed with similar assumptions typically aim to construct approximate solutions to the state estimation problem.\nThus, we did not include them as we felt a more thorough characterization of the difference between the optimal solutions would be more salient.\n\n> \"In the main paper assumptions, you have that is a manifold. However, in App. B, you assume the inputs have a finite-sum structure. Hence, it seems to me the assumption on the input set is a bit stronger than what you claim. Am I right?\"\n\nWe appreciate your careful reading of the appendices.\nYou are correct regarding the mismatch between the paragraph at the beginning of Appendix B and the claim.\n\nWe will revise the paragraph to state that the parameter space can be augmented as the product manifold of $\\mathcal{U}$ and $\\Theta$, rather than attempting to illustrate the process concretely through a linear subspace."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688185192,
                "cdate": 1700688185192,
                "tmdate": 1700688185192,
                "mdate": 1700688185192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wVFPRDxGu2",
                "forum": "UH4HinPK9d",
                "replyto": "v0d3fadYDV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Reviewer_YW1N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Reviewer_YW1N"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the comments! I cannot see a revision of the manuscript posted, are you sure you included this?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694656503,
                "cdate": 1700694656503,
                "tmdate": 1700694656503,
                "mdate": 1700694656503,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Pxq6uFrJEW",
            "forum": "UH4HinPK9d",
            "replyto": "UH4HinPK9d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_Rdcm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_Rdcm"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework for predicting the behavior of ordinary differential equations. The research is centered on the study of smooth Lipschitz dynamical systems over a finite time interval and establishes that the set of possible trajectories forms a finite-dimensional Riemannian manifold. Through the integration of established estimation techniques, such as maximum likelihood, maximum a posteriori, and minimum mean squared error estimation, the authors introduce methods for computing the best-estimated trajectories. The paper also delves into the properties and conducts numerical experiments to illustrate specific estimations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The model formulation is rigorous, and the theoretical proofs are robust.\n\n- After introducing the abstract framework, the paper discusses common estimation objectives and presents practical methods for computing trajectory estimations."
                },
                "weaknesses": {
                    "value": "- The commonalities and differences between the author's approach and related methods are not thoroughly discussed, as indicated in the first question.\n\n- While the authors assert that using objectives explicitly dependent on the system's trajectory is superior to point estimation, there is a lack of theoretical guarantees provided to substantiate this claim formally."
                },
                "questions": {
                    "value": "- The commonalities and differences between the author's approach and related work in Section 1.1 are not clearly delineated. For instance:\n  - Is the method proposed in this paper a variant of the Neural ODE method?\n  - What advantages does the proposed method offer over existing Neural ODE methods in the context of time-series forecasting problems?\n  - The relevance of discussing regularization methods for training neural networks to solve differential equations is not entirely clear.\n\n- When the parameter space $\\Theta$ s derived from a neural network, is it possible to verify whether $f$ is a smooth function, satisfying assumption 1?\n\n- Could you please provide the definition of the function $\\varphi$ (in page 4, line 15) and the notation $\\varphi^{\\gamma_i}(x_0)$ (in page 5, line 24)?\n\n- What is the meaning of the bracketed term $[D\\varphi^{\\gamma_i}]$ in the equation (14)?\n\n- Could you offer a comparison of the computational complexity between the proposed methods for computing trajectory estimations and the estimations of the initial condition?\n\n- Regarding Figure 2, it appears to be somewhat confusing. \n  - Is the caption of each subfigure indicating the names of methods? How do they correspond to the six different forecasting objectives used in simulations?\n  - Could you provide further insights into which surfaces of objective functions yield better results and which do not? Are these outcomes affected by changes in the initialization point?\n  - For the statement 'the trajectory MSE illustrates a valley of initializations ... structure not captured by any competing technique',  it seems the State MSE also exhibits a valley structure. Could you elaborate on this observation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Reviewer_Rdcm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773644072,
            "cdate": 1698773644072,
            "tmdate": 1699636618795,
            "mdate": 1699636618795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Er3KXsxDQY",
                "forum": "UH4HinPK9d",
                "replyto": "Pxq6uFrJEW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rdcm, Part 1"
                    },
                    "comment": {
                        "value": "> \"Is the method proposed in this paper a variant of the Neural ODE method?\"\n\nThe proposed method is broadly applicable, both to Neural ODEs and more general system parameter estimation problems.\nThe proposed method is best thought of as an improved cost function when the chosen model family is represented by differential equations, and can used as a replacement for the typical cost function.\n\n> \"What advantages does the proposed method offer over existing Neural ODE methods in the context of time-series forecasting problems?\"\n\nOur approach is guaranteed to be optimal in minimizing a chosen forecasting cost for a particular model family.\nNeural ODE methods typically use a likelihood approach as a surrogate objective.\nIn doing so, competing methods fit parameters based only on the observation horizon, rather than the forecasting horizon.\n\n> \"The relevance of discussing regularization methods for training neural networks to solve differential equations is not entirely clear.\"\n\nOur proposed method and existing regularization methods seek to resolve the same issue: generalization of a model to the out-of-sample regime.\nWhile regularization is commonly used to increase model stability and reduce overfitting to any given sample, our proposed method instead transforms the cost to explicitly be in the regime of interest.\n\n> \"When the parameter space $\\Theta$s derived from a neural network, is it possible to verify whether $f$ is a smooth function, satisfying assumption 1?\"\n\nYes, the smoothness property requires the usage of smooth activation functions.\nThe smoothness assumption is common, and is additionally required in Neural ODEs and related work (Chen et al. 2018; Dupont et al., 2019; etc.).\n\n> \"Could you please provide the definition of the function (in page 4, line 15) and the notation (in page 5, line 24)?\"\n\nThe collection of functions $\\{\\varphi^\\tau\\}$ advance time in the system.\nThey are defined such that $\\mathbf{x}_{\\tau + t} = \\varphi^\\tau(\\mathbf{x}_t)$ for all $t$.\n\n> \"What is the meaning of the bracketed term in the Equation (14)?\"\n\nSimilar to Equation (7), the $D$ notation represents the matrix of partial derivatives in local coordinates.\nThus,\n$$D\\varphi^{\\tau_i} = \\begin{bmatrix}\n\\rule[.5ex]{2.5ex}{0.5pt} & D\\_{\\mathbf{v}_1}|\\_{\\mathbf{x}_0} \\varphi^{\\tau_i}& \\rule[.5ex]{2.5ex}{0.5pt} \\\\\\\\\n\\rule[.5ex]{2.5ex}{0.5pt} & D\\_{\\mathbf{v}_2}|\\_{\\mathbf{x}_0} \\varphi^{\\tau_i} & \\rule[.5ex]{2.5ex}{0.5pt} \\\\\\\\\n & \\vdots &\n\\end{bmatrix},$$\nwhere $\\{\\mathbf{v}_i\\}$ is a basis of the local coordinates on the state space around $\\mathbf{x}_0$.\n\n> \"Could you offer a comparison of the computational complexity between the proposed methods for computing trajectory estimations and the estimations of the initial condition?\"\n\nWe believe that such a comparison would only be informative for the particular pair of differential equation and ODE solver being investigated.\nThe primary source of the difference is in requiring the solutions of the differential equation to be computed over the entire forecasting horizon rather than the observation horizon.\nThe relative amount of time required is dependent on the dynamically chosen step sizes in the ODE solver in the observation regime and the forecasting regime.\nThese step sizes are dependent on the ODE solver, the differential equation, and the initial conditions, making it difficult to generalize statements on the computational cost.\n\n\n> \"Is the caption of each subfigure indicating the names of methods? How do they correspond to the six different forecasting objectives used in simulations?\"\n\nThe title of each subfigure is the function in the plot, e.g., \"Likelihood\" represents the likelihood function of the parameters, \"State Posterior\" represents the posterior distribution of the state variable, etc.\nWe will rework the caption to improve clarity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697400182,
                "cdate": 1700697400182,
                "tmdate": 1700697400182,
                "mdate": 1700697400182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5DcqzcDli3",
                "forum": "UH4HinPK9d",
                "replyto": "Pxq6uFrJEW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rdcm, Part 2"
                    },
                    "comment": {
                        "value": "> \"Could you provide further insights into which surfaces of objective functions yield better results and which do not?\"\n\nThe objective function relating directly to prediction performance should yield better results in prediction performance.\nFigure 2 was intended to illustrate the qualitative differences between costs in the space of solutions and costs in the space of initial conditions, demonstrating that the \"valley of initialization which lead to similar trajectories\" is not captured by other objectives.\n\n> \"Are these outcomes affected by changes in the initialization point?\"\n\nThis work does not propose a specific optimization algorithm, but rather a new cost function dependent directly on the forecasting performance.\nAs such, there is no initialization point in our proposed method.\n\n> \"For the statement 'the trajectory MSE illustrates a valley of initializations ... structure not captured by any competing technique', it seems the State MSE also exhibits a valley structure. Could you elaborate on this observation?\"\n\nThe key part of the sentence was \"which lead to similar trajectories along the interval.\"\nWe are referring to the fact that the proposed cost function meaningfully associates similar forecasts, rather than similar initializations.\nThe state MSE figure would suggest that a significant reduction in the initial prey population would be possible without sacrificing performance, whereas the trajectory MSE plot demonstrates this is not the case, and that the path is extremely sensitive to the initial prey population."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697423061,
                "cdate": 1700697423061,
                "tmdate": 1700697423061,
                "mdate": 1700697423061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DMzHPYsD6H",
            "forum": "UH4HinPK9d",
            "replyto": "UH4HinPK9d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_kKvz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5850/Reviewer_kKvz"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates statistical estimation of finite-dimensional parameter via maximum likelihood (ML), maximum a posteriori (MAP), and minimum mean squared error (MMSE) in the space of feasible ODE solutions when ODE is known and the only thing to be estimated is the unknown initial condition. The main insight is based on the classical results on the existence and uniqueness of the solutions of an ODE with Lipschitz continuous vector field with a continuously differentiable derivative, in which case there exists diffeomorphism  between the (finite-dimensional) state space and manifold of trajectiories with a finite time-horizon. Using the diffeomorphism, estimation is seen as a supervised learning problem for which ML, MAP and MMSE estimation can be formulated and solved. \n\nUsing standard argument, this setting is directly extended to estimating unknown initial condition $x_0\\in{\\mathcal X}$, parameters $\\theta\\in\\Theta$ and input functions $u\\in{\\mathcal U}$, whenever $\\mathcal X$, $\\Theta$ and $\\mathcal U$ are finite dimensional.\n\nOne illustrative example of the Lotka-Voltera predator-pray system is presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Paper has a clear story-line. Statistical estimation of the ODE parameters from the observed trajectories is an important scientific problem."
                },
                "weaknesses": {
                    "value": "While I find the story-line of the paper nice and useful for readers interested in learning dynamical systems, my overall impression is that the submission is this form is not appropriate for acceptance for the ICLR. The main reasons are the following:\n\n1) __Theoretical aspect.__ Main results of the paper seam, at best, based on well-known arguments, if not already present is the same form in the existing literature. How the paper is presented, my impression is that novelty is highly overstated. Giving proper references for all arguments based on classical ODE theory is necessary. \n\n2) __Methodological aspect.__  In related works many references study different problem - estimating dynamics when ODE is not known, which is not the problem that this paper studies. On the other hand, existing literature on the ML MAP and MMSE estimation of the initial conditions of a known ODE is not properly reviewed, and the novelty of the considered methodology lacks perspective. \n\n3) __Experimental aspect.__ One toy ODE model is by far bellow ICLR standard. No broader context presented. Such discussion is insufficient to draw any kind of reliable conclusions. Additional material in the Appendix is minor and in part (Section A.2) obvious.\n\n\n__Minor issues:__\n\n1) $\\varphi$ is nowhere properly defined\n2) Bellow Eq. (6) $R^{K\\times K}$ should read $R^{N\\times N}$\n3) In Eq. (8) inner product is in $L^2(I)$ not an RKHS. Proposition with an RKHS should be stated, at least in the Appendix.\n4)  Eq. (12) should be rewritten to avoid confusion. What is written now is that the empirical estimate equals the regression function."
                },
                "questions": {
                    "value": "When introducing kernel for the trajectories, due to the change of geometries between RKHS and  $L^2(I)$ spaces, the existence and the properties of the diffeomorphism should be at least commented. Also, can you please clarify when the optimization is done in the RKSH norm and when in $L^2$ norm. To me it seams that aspects of the statistical learning theory of kernel methods are not addressed properly. In particular, regression function in Eq. (12) may or may not belong to the RKHS defined by the kernel, and the minimisation is typically not done in $L^2$ but in RKSH.  In particular, can you please elaborate on \"_MMSE trajectory estimate is optimal for any desired weighting of time horizons by the construction of Equation (6)._\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5850/Reviewer_kKvz"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5850/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828558659,
            "cdate": 1698828558659,
            "tmdate": 1699636618690,
            "mdate": 1699636618690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YtYvcJfPPw",
                "forum": "UH4HinPK9d",
                "replyto": "DMzHPYsD6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to kKvz, Part 1"
                    },
                    "comment": {
                        "value": "> \"Main results of the paper seam, at best, based on well-known arguments, if not already present is the same form in the existing literature. How the paper is presented, my impression is that novelty is highly overstated.\"\n\nTo our knowledge, the provided structural result is novel.\nOther related results either require significantly stronger system constraints (e.g. Quotient manifold theorem (Lee, 2013)) or fail to provide sufficient structure for practical applications (e.g. non-Hausdorff manifold of solutions discussed by Souriau (1997)*. Without the Hausdorff structure, many common tools break down due to lack of uniqueness of limits).\n\nThe novelty is in identifying the appropriate set of constraints to induce sufficient structure for statistical analysis.\nWhile remaining broadly applicable, the set of constraints enables the ability to directly optimize forecasts over the entire time horizon of interest.\n\n*: J.-M. Souriau, _Structure of Dynamical Systems: a symplectic view of physics._ Springer Science+Business Media, New York, NY, 1997.\n\n\n> \"Giving proper references for all arguments based on classical ODE theory is necessary.\"\n\nWe believe that we have included proper references.\nWhen appropriate throughout the manuscript, we include references to \"Nonlinear Systems\" by Hassan Khalil, a standard book on the analysis of nonlinear ODEs.\nWhen discussing geometric approaches, we additionally reference \"Introduction to Smooth Manifolds\" by John Lee.\nWe are unclear as to which arguments you feel are missing references; we could additionally include a real analysis book for the epsilon-delta proofs.\n\n> \"In related works many references study different problem - estimating dynamics when ODE is not known, which is not the problem that this paper studies.\"\n\nWe are unclear as to which references you feel study fundamentally different problems, as our work relies on the same assumptions as neural ODEs and related work (Chen et al. 2018; Dupont et al., 2019; etc.), is most comparable to physics-informed ML (Raissi & Karniadakis, 2018;\nRaissi et al., 2019), and represents a change in cost function similar to regularization methods.\nFurthermore, we believe that hierarchical forecasting  (Rangapuram et al., 2021; 2023) is implicitly operating under a similar structural assumption.\n\nThe problem of estimating dynamics and making predictions are intrinsically linked.\nThe benefits of algorithms such as neural ODEs are in their predictive power, not in capturing a interpretable representation of some fundamental physical principle.\nFurthermore, we do not assume a fully known ODE, but rather that the ODE exists in some parameterized family of candidate models, the same underlying assumption in such a work.\n\n> \"On the other hand, existing literature on the ML MAP and MMSE estimation of the initial conditions of a known ODE is not properly reviewed, and the novelty of the considered methodology lacks perspective.\"\n\nWe do not believe that existing literature on ML, MAP, and MMSE estimation in dynamic systems addresses the problem of interest in this work.\nExisting literature on specific objective functions largely focus on computationally efficient algorithms and approximations.\n\n> \"One toy ODE model is by far bellow ICLR standard. No broader context presented. Such discussion is insufficient to draw any kind of reliable conclusions.\"\n\nThe reliable conclusion in the manuscript is in the theoretical foundations of the work, not the numerical aspect.\nThe proposed analysis guarantees optimality of the forecast within the model family.\nSimulations are included largely to characterize situations in which the performance gap may be meaningful (e.g., large time horizons in Figure 3).\n\n> \"Additional material in the Appendix is minor\"\n\nWe believe that we have included extensive additional material throughout the appendix.\nAppendix A includes simulations for three additional systems and two competing techniques.\nAppendix B and Appendix D include full proofs for the main results.\nAppendix C includes a review of key properties in Riemannian geometry.\nAppendix E provides some analysis on tolerance selection for the ODE solvers used in the work.\n\n> \"in part (Section A.2) obvious.\"\n\nWhile we agree that Section A.2 is well-known to many in this field, in past publications comparisons with these models have been specifically requested by reviewers.\nThus, we have included the additional figures for those for whom it is not obvious."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687891904,
                "cdate": 1700687891904,
                "tmdate": 1700687891904,
                "mdate": 1700687891904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ReyNauypu",
                "forum": "UH4HinPK9d",
                "replyto": "DMzHPYsD6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to kKvz, Part 2"
                    },
                    "comment": {
                        "value": "> \"When introducing kernel for the trajectories, due to the change of geometries between RKHS and $L^2(I)$ spaces, the existence and the properties of the diffeomorphism should be at least commented.  Also, can you please clarify when the optimization is done in the RKSH norm and when in $L^2$ norm. To me it seams that aspects of the statistical learning theory of kernel methods are not addressed properly. In particular, regression function in Eq. (12) may or may not belong to the RKHS defined by the kernel, and the minimisation is typically not done in $L^2$ but in RKSH.\"\n\nWhile it can be shown that the solutions of the ODE exist in an RKHS directly based on continuity of the evaluation functional, we do not change from $L^2(I)$ into an RKHS by introducing a linear operator.\nIt is true that positive definite integral operators induce an RKHS by Mercer's theorem, but we do not require the RKHS structure and thus feel it would serve as a distraction from the main contributions.\nThe core contribution in this work is to show that the space of solutions is a Riemannian manifold, not an arbitrary structure in an RKHS, nor $L^2(I)$.\nThe embedding in $L^2(I)$ is used only to enable naturally occurring cost functions through a restriction onto the embedded manifold and to inherit a metric.\n\n> \"In particular, can you please elaborate on 'MMSE trajectory estimate is optimal for any desired weighting of time horizons by the construction of Equation (6).'\"\n\nThe cost function in Equation (6) is constructed through the application of an invertible transformation to the solution space.\nAs the MMSE estimator commutes with linear transformations, the result is invariant to the choice of integral kernel.\nThe integral kernel determines the weighting of time horizons in the cost.\n\nWe appreciate the feedback and will expand the line in the manuscript to make this point more explicit."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687926662,
                "cdate": 1700687926662,
                "tmdate": 1700687926662,
                "mdate": 1700687926662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cfy3CLTWep",
                "forum": "UH4HinPK9d",
                "replyto": "0ReyNauypu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5850/Reviewer_kKvz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5850/Reviewer_kKvz"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgment of the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their reply. I have read it, but, due to short remaining time, I cannot provide detailed discussion at this point. However, I will consider the entire rebuttal to base my final opinion during AC-reviewers discussion period that is starting soon."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5850/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724977836,
                "cdate": 1700724977836,
                "tmdate": 1700724977836,
                "mdate": 1700724977836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]