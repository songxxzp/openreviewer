[
    {
        "title": "Understanding Unfairness via Training Concept Influence"
    },
    {
        "review": {
            "id": "WaTYAUKq5C",
            "forum": "dwademPdV1",
            "replyto": "dwademPdV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to identify the causes of a model\u2019s unfairness by leveraging the method of influence function. Instead of only detecting the training samples that induce unfairness directly and passively, the article takes a step further and tries actively to replace the \u201chigh-influence/biased\u201d samples with new counterfactual samples generated by designing an algorithm. This solution is interesting and intuitive. In addition, the framework proposed in this paper is general and has the potential to be applied to various new scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. this paper is well-written; \n2. the topic is both intriguing and of practical significance.\n3. the main idea is intuitive and easy to understand; \n4. the algorithm design is reasonable."
                },
                "weaknesses": {
                    "value": "1. There are no theoretical guarantees about the methods used to generate counterfactual samples."
                },
                "questions": {
                    "value": "I do not have any major concerns about this work in technical details or presentation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749150377,
            "cdate": 1698749150377,
            "tmdate": 1699636354265,
            "mdate": 1699636354265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "75xIDUGFpw",
                "forum": "dwademPdV1",
                "replyto": "WaTYAUKq5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3936/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for considering our work to be \"well-written,\" \"intriguing,\" \"practical,\" \"intuitive,\" \"easy to understand\", and  \"reasonable design.\"\n\nWe address the following concern.\n\n### **1. Theoretical Understanding**\nWe have dedicated an entire section on the theoretical insights in Appendix D. We did not include it in the main paper due to the page limit.\n\nWe briefly summarize here why intervening training data attributes using CIF framework can improve fairness. For simplicity, we focus on accuracy disparity as the fairness measure.\n\nWe base the analysis on the data generation model adopted in [1] to capture the impact of data patterns generated with different frequencies and the impact of label errors. This setup is a good fit for understanding how counterfactual data interventions can change the data frequency of different groups (majority group with higher frequency vs. minority group with lower frequency) and provides insights for CIF. \n\nIntervening labels $Y$ is relatively straightforward. If we are able to intervene on a training label of a disadvantaged group from a wrong label to a correct one, we can effectively improve the performance of the model for this group. Therefore the label intervention can reduce the accuracy disparities. Our analysis also hints that \nthe influence function is more likely to identify samples from the disadvantaged group with a lower presence in the data and mislabeled samples. This is because, for a minority group, a single label change would incur a relatively larger change in the influence value.\n\nIntervening sensitive attributes $A$ improves fairness by ``balancing\" the data distribution. In the experiments, we show that the influence function often identifies the data from the majority group and recommends them to be intervened to the minority group. In the analysis, we also show that this intervention incurs positive changes in the accuracy disparities between the two groups and therefore improves fairness. \n\nPlease let us know if there is still anything unclear.\n\n\n[1] Feldman, Vitaly. \"Does learning require memorization? a short tale about a long tail.\" Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing. 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700012256205,
                "cdate": 1700012256205,
                "tmdate": 1700012267446,
                "mdate": 1700012267446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nhKjJgGfzI",
                "forum": "dwademPdV1",
                "replyto": "75xIDUGFpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response. I have no further questions and tend to remain the score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400771535,
                "cdate": 1700400771535,
                "tmdate": 1700400771535,
                "mdate": 1700400771535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "shC8wZgiTD",
            "forum": "dwademPdV1",
            "replyto": "dwademPdV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3936/Reviewer_rxAc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3936/Reviewer_rxAc"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces tools from the influence function literature to quantify algorithmic fairness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See below for contextual discussion of strengths."
                },
                "weaknesses": {
                    "value": "This paper uses influence functions to quantify fairness. The topic is one of the paper's strengths; although influence functions and counterfactual fairness models are well-established, the connection between the two is (to my knowledge) less developed. At least for me, for a paper combining two well-worn topics, it is important that the paper does so in an exceptionally clear or general way. The paper has potential for doing so but may be somewhat lacking in its current framing (at least according to my current understanding of the paper and contextual literature). For example, much of the discussion seems to assume a relatively limited class of ML problems (e.g., binary classification). Also, the relationship between ''Concepts'' and ''Fairness'' could (in my view) use a clearer elaboration. ''Transforming the sample based on certain concepts of training data'' seems far more general (this could concern any number of modifications). This framing somewhat complicates (at least this readers') analysis of the paper. I would consider revisions that streamline some of this. The introduction of the override operator also wasn't particularly obvious to me (e.g., ''override() operator counterfactually sets the value of the concept variable to a different c0.'' sounds like the do() operation, although I see an attempt to distinguish). \n\nOverall, I see promise in the paper, but (at least in my opinion), certain limitations exist in terms of clarity. Also, the methods don't seem to have a comparison benchmark (even a naive one) which would help readers understand the contribution over any existing approaches in this context. \n\nA few more focused comments. \n\n(1) The paper uses the term ''fairness'' throughout. It would be helpful to provide an explicit definition early on. At times, it seems like the term is more qualitative (e.g., ''Influence Functions for Fairness''; ''They can all contribute to unfairness''). At other times, the term seems to be in some sense quantitative (i.e., ''fairness can improve''; ''fairness becomes worse''). Although the meaning may be clear to readers embedded in the fair ML literature, I think it would help to fix ideas early on for readers. Having said that, what constitutes ''fair'' (in at least some of its meanings) is, of course, subject to social values. \n\n(2) It could help to discuss some of the challenges of computing the Hessian vector product (e.g., in high dimensions) and to emphasize why bypassing the Hessian calculation via the HVP can be useful. Would the influence function approach break down, e.g., when the parameter number ranges in the billions? More generally, based on the paper's existing framing, I had a hard time understanding some of its limitations, which are not particularly emphasized in the paper (the ''Conclusions and Limitations'' section is 10 lines long.)"
                },
                "questions": {
                    "value": "(1) Could the authors clarify what is meant by, ''we show that the influence function often identifies the data from the majority group and recommends them to be changed to the minority group''? Presumably, this refers to re-weighting of the training data, but it reads almost like the direct changing/manipulation of observed data values. \n\n(2) Do all of the performance metrics have definitions in the paper? (I don't see one, for example, for the ''Equality of Odds'' metric). If the metrics could be compiled into a table with equations, this reader may have a better understanding of them and the results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No major ethnics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Reviewer_rxAc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803299385,
            "cdate": 1698803299385,
            "tmdate": 1700748549735,
            "mdate": 1700748549735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KKwcXQEKmS",
                "forum": "dwademPdV1",
                "replyto": "shC8wZgiTD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3936/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for considering our problem to be important. We address the concerns one by one.\n\n### **1. Applicability**\nWe thank the reviewer for the concern. Although we test binary classification models in our experiments, our method is flexible enough to be extended to multi-class fairness - the only difference is the definition of fairness, which is easy to plug in.\n\nWe admit that extending to the regression or generative models would require significant change, but almost all works in influence function (we mean general influence function, not even the influence function for fairness) only target the classification models, and therefore we think our scope is reasonable.\n\n### **2. Relationship between \"Concepts\" and \"Fairness\"**\nWe thank the reviewer for asking for clarification.\n\nIn our case, the concept can be sensitive attribute $A$, feature $X$, or label $Y$. Consider group fairness like Demographic Parity (DP): $\\big| \\mathbb{P}(h_\\theta(X) = 1 | A = 0)-\\mathbb{P}(h_\\theta(X) = 1 | A = 1) \\big|$, we can see it depends on feature $X$ and sensitive attribute $A$, and therefore the concept $X$ and $A$ are closely related to the fairness. Or consider Equality of Opportunity (EOP): $\\big| \\mathbb{P}(h_\\theta(X) = 1 | A = 0, Y=1)-\\mathbb{P}(h_\\theta(X) = 1 | A = 1, Y=1) \\big|$, it depends on all $X$, $Y$, $A$.\n\nHowever, the relationship cannot be derived precisely since, for example, the change of a training sample's $A$ would impact the trained model weights $\\theta$ and then impact the test fairness on $\\theta$. How $A$ would impact the final fairness is complex and therefore we need the influence function and our whole approach of generating counterfactual samples to estimate it.\n\n### **3. \"override(.)\" Operator**\n\nWe thank the reviewer for the clarification. As the reviewer already noticed our explanation in Section 2.3, we will try our best to further clarify.\n\nThe \"override(.)\" operator is conceptually similar to the do(.) operator in the causal inference; but it is broader. In general, it represents the change of data attributes (feature $X$, label $Y$, or sensitive attribute $A$) of training samples. Since $X$, $Y$, and $Y$ are interdependent, we need to find out, for example, how the change of $A$ would impact $X$. If we already know the underlying causal relationship between variables, then it is the same as the do(.) operator. However, it would only work on synthetic data.\n\nIn practice, given non-synthetic data, the causal relationship is unclear (and discovering it is a notoriously hard problem in causal inference), which is our focus where the traditional causal methods are not applicable. To this end, we propose a set of empirical methods to estimate it (i.e. our W-GANs).\n\n### **4. Baseline Comparison**\nWe thank the reviewer for raising the concern. \n\nTo the best of our knowledge, the only prior works that study influence function on fairness are [1, 2]. However, both are not comparable to our work because they only consider removing or reweighing whole training samples while we decompose the fairness influence into features, labels, and sensitive attributes. We do not know other works that also study the fairness influence function of modifying features, labels, and sensitive attributes *independently*. If the reviewer knows, please let us know.\n\nIn addition, the \"Removal\" baseline in our experiment considers the effect of removing a training sample, which can be thought of as the method commonly used in the literature [1,2].\n\nFurthermore, we have compared our mitigation to the baseline of in-processing mitigation with fair training [3] in Figure 5.  \n\nLast but not least, our work is mainly an explainable AI tool and the primary goal is to explain fairness to AI practitioners. The main objective is not to outperform other bias mitigation methods but rather attributing and explaining the observed fairness.\n\n[1] Wang, Jialu, Xin Eric Wang, and Yang Liu. \"Understanding instance-level impact of fairness constraints.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2] Li, Peizhao, and Hongfu Liu. \"Achieving fairness at no utility cost via data reweighing with influence.\" International Conference on Machine Learning. PMLR, 2022.\n\n[3] Agarwal, Alekh, et al. \"A reductions approach to fair classification.\" International conference on machine learning. PMLR, 2018."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700012100263,
                "cdate": 1700012100263,
                "tmdate": 1700012169758,
                "mdate": 1700012169758,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iPbUFpNkGU",
            "forum": "dwademPdV1",
            "replyto": "dwademPdV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3936/Reviewer_BzqW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3936/Reviewer_BzqW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CIF (concept influence for fairness) as a framework to identify potential fairness problems in training data when looking at 1) sensitive attributes 2) features and 3) labels by overriding concepts in each case. The authors provide details on how to change concepts, and then how to define the influence of those concept changes. The authors provide experimental results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides a framework that is very flexible to changing technologies."
                },
                "weaknesses": {
                    "value": "This paper seems incremental in its approach. For example, the authors state that the effectiveness of their solution depends on finding a proper transform() which is their work's focus. The then proposed transforms in \"generating counterfactual samples\" use techniques not novel to this paper and pulled from a number of different sources. Although this framework is generally useful, I don't believe it provides enough novelty for this conference."
                },
                "questions": {
                    "value": "Could the authors provide more details on the \"overriding X\" experiment w.r.t each of the datasets used. What kind of features where chosen in these cases?\n\nCould the authors please comment on novelty from above?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3936/Reviewer_BzqW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806397651,
            "cdate": 1698806397651,
            "tmdate": 1699636354085,
            "mdate": 1699636354085,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R4eE5YwyaG",
                "forum": "dwademPdV1",
                "replyto": "iPbUFpNkGU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3936/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for considering our work to be \"generally useful.\" We address the concerns one by one.\n\n### **1. Incremental Approach**\nWe thank the reviewer for the concern. We understand that it might look like our approach is incremental: influence function and W-GAN are adapted and modified from the prior works. However, our novelty does not lie in the details of measuring the influence of samples or generating counterfactual samples; it rather lies in the proposal of a general *framework* that explains and measures the training samples' influence on fairness.\n\nTo the best of our knowledge, almost no work in the area of influence function is based on a completely new technical approach. For example, influence function itself [1] is merely a revival of the concept in robust statistics that can be traced back to as early as 1974. \n\nIn addition, our framework is far from something technically trivial. None of the prior works can be directly applied in our setting. For example, the original influence function does not target fairness, and we adapt it to the fairness context. Furthermore, W-GAN in [2] only works on sensitive attributes, and we adapt it to also consider features and labels.\n\nMost importantly, the concept of our framework is entirely new. The limited prior works on understanding fairness via influence function [3,4] so far only focus on removing or reweighing whole training samples. Our framework decomposes the fairness influence into features, labels, and sensitive attributes. This distinction is vital in fairness because each attribute can significantly impact the underlying fairness. And it is feasible because of our novel framework by generating counterfactual samples.\n\nIn short, our framework is more flexible, powerful, and more suitable for the nature of AI fairness and can give practitioners a wider scope of understanding of fairness. It also leads to a wider range of applications, e.g. detecting mislabeling, fixing imbalanced representations, and detecting fairness-targeted poisoning attacks, as articulated in the paper.\n\nWe think if we all insist on an entirely new invention of all methodological details, then almost no work on fairness, influence function, or causal inference can be published on ICLR. We believe most of the papers in those areas do not propose a *completely* new technique.\n\nWe will clarify it in the paper. Please let us know if we still have not addressed the concern. If the reviewer disagrees, feel free to let us know the reasons as well as some concrete and technically detailed examples.\n\n[1] Koh, Pang Wei, and Percy Liang. \"Understanding black-box predictions via influence functions.\" International conference on machine learning. PMLR, 2017.\n\n[2] Black, Emily, Samuel Yeom, and Matt Fredrikson. \"Fliptest: fairness testing via optimal transport.\" Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 2020.\n\n[3] Wang, Jialu, Xin Eric Wang, and Yang Liu. \"Understanding instance-level impact of fairness constraints.\" International Conference on Machine Learning. PMLR, 2022.\n\n[4] Li, Peizhao, and Hongfu Liu. \"Achieving fairness at no utility cost via data reweighing with influence.\" International Conference on Machine Learning. PMLR, 2022.\n\n\n### **2. Details of \"Overriding X\" Experiments**\nWe thank the reviewer for asking for clarification. Due to the space limit, we have included the settings of overriding $X$ in our experiments in Appendix E.1. We understand that it is easy to be omitted. We recap it here.\n\n\"Overriding X\" means counterfactually changing samples as if their features were changed to different values, and then measuring how the underlying fairness would be impacted. For example, in an image task, if we want to understand how skin color would affect fairness (say gender fairness), we can generate counterfactual features (image pixels) as if they come from a different race, i.e. how would fairness change if those images change their racial group.\n\nIn COMPAS, $X$ is the binary feature that indicates age $> 45$ or not while the sensitive attribute is race. In Adult, $X$ is race while $A$ is sex. In CelebA, $X$ is the binary label ``Young'' (i.e. age) while $A$ is gender.\n\nPlease let us know if this is still not clear enough."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011297067,
                "cdate": 1700011297067,
                "tmdate": 1700019312492,
                "mdate": 1700019312492,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]