[
    {
        "title": "Can Transformers Capture Spatial Relations between Objects?"
    },
    {
        "review": {
            "id": "LzHjpDBO0W",
            "forum": "HgZUcwFhjr",
            "replyto": "HgZUcwFhjr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_ubwR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_ubwR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the task of spatial relation prediction. The paper observes that prior works have focused on semantic relationships that may not be physically grounded and which can be ambiguous. To alleviate this issue, they define more precise and physically grounded definitions for relationships and reannotate an existing dataset (SpatialSense) to provide an improved test bed. Furthermore, the propose a new architectural choice that performs well on both existing synthetic datasets as well as the newly annotated benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very well motivated with a clear explanation of how the proposed analysis and method fits within the larger scope of the field.\n- The paper identifies a very interesting and nuanced ambiguity in existing annotations and provides clear examples of such issues. \n- The paper provides a good explanation for the different subtasks that a model needs to do and the corresponding architectural components that do it in Sec 4.1. \n- I appreciated that the authors included the very strong performing naive baselines and elaborated on why they perform well. \n- The paper has a nice flow where the proposed architecture follows logically from the analysis and design considerations presented in the work."
                },
                "weaknesses": {
                    "value": "I found three weaknesses in the paper: (1) it is unclear if the newer annotation provide a better evaluation or a simpler task; (2) ablations are not very clear; (3) several statements are not supported by the results. I have listed the major concerns below. I also included some minor concerns that should be dealt with as extra suggestions, and do not need to be addressed in the rebuttal.\n\n\n- The paper very nicely motivates language ambiguity as an issue for obtaining spatial relations. However, it is unclear if the newer annotation scheme results in more accurate annotation or simply annotation that is better correlated with a relative location prior. \n    - Comparing Tables 4 and 11, one finds that all methods achieve a higher absolute score on the newer dataset, while the bbox baselines and the proposed method seeing stronger gains. Furthermore, Table 7 shows that the new annotation seems more specific regarding annotations being done in the labelers point of view or gravity to minimize ambiguity. As a result, while it is possible that the new definitions are more precise and a better evaluation, it is also possible that they are more strongly correlated with the relative locations in the image, which would introduce a strong prior. \n    - Some of the definitions in Table 7 support the point above. For example, \"A in front of B\" is defined as A is closer to the camera than B. While this is a less ambiguious definition, it is a much easier one than the typical usage of \"in front of\" which often also requires reasoning about the subject and object's relative orientation. For example, if two people are facing away from the camera, the further away person would be in front of the person closer to the camera. This is still unambiguous, but the prediction would be more difficult as it requires the model to both reason about relative depth as well as orientation of each person. \n    - Said differently, the newer annotation scheme reduces the ambiguity but it also makes the relationships less dependant on context for cases like front/behind where the RelatiViT shows greatest performance improvements on the baseline (Table 6). This can be shown by the great improvements in performance of the bbox-only baseline. It would be nice to provide some qualitative analysis or quantitative comparisons that address this. \n\n- The ablations are not very clear and the conclusions drawn do not seem supported by the results. \n    - It is unclear how the method works without Pair Interaction, Context Aggregation, or Feature Extraction. For example, pair interaction is stated as the MLP that combines the two features, how is the prediction made without it? Additionally, how does one make predictions without feature extraction? is it based on pixel values, patch embeddings, something else? I think it is very important to explain exactly what each ablated model looks like. \n    - The ablations are additive, so that the feature extraction ablation is applied to an already ablated model. While this is valid experiemntal choice, it makes it difficult to understand the most critical modules as they are not ablated independatly of each other. This is more important in this problem where we know that feature extract is not actually very important since bbox baselines perform very well, as shown in the paper. \n\n- Several statements or claims are not fully supported/substantiated.\n    - The statement `it is difficult for CNN backbones to extract relation-grounded representations` (sec 5.4) is strongly confounded by the use of positional embeddings. Specifically, based on the description, it seems that all methods (ViTs) have access to positional embeddings, while the CNN-Transformer baseline doesn't. While I understand that this is a common choice for CNNs, it seems like a strong confounder here due to the strong performance of bbox-only baseline. It is possible that ViTs outperform CNNs due to the explicit inclusion of positional embeddings rather than due to attention explicitly modeling pairwise relationships as suggested in Sec 4. While I think that attention is playing a strong role here, I think providing the CNN model with some positional embeddings, eg, through CoordConv (Liu et al, NeurIPS 2018), would help clairfy this point and explain whether CNNs are still poor feature extractors even if another model performs context aggregation afterwards (following the design axes in sec 4.1). \n    - The paper states that `[prior methods] are prone to simply predict the relationships based on the bounding box coordinates rather than learn the intended solutions by referring to the RGB input.` (Sec 5.4) but that statement is not supported by the results. Most methods are consistently outperformed by the bbox-only baseline sugguestion that they are probably doing something other than predicting relationships baseline on bounding box coordinates. While I agree that those models are likely learning shortcuts, it seems odd that this shortcut would be using the bounding box coordinates if they are all underperforming the bounding-box baseline. Without conducting some further analysis, it is difficult to make such statements about what those models are doing. \n    - The description under Table 6 doesn't match the results. The paper states `Bbox-only performs better on \u201cto the right of\u201d than RelatiViT because according to the relation definition proposed in Section 3.2` which is not true. All methods achieve the same mean performance on the `right` relationship, while bbox outperforms RelatiViT on `Next` and `Above`.\n\n- I believe that there is a typo in the equation in Sec 3.1. I think you want to minimize the negative log likelihood, while the current term minimizes the log likelihood, as for $y{=}1$ the term would be minimized by estimating $\\hat{y}$ to be 0. If I am wrong, could you please clarify why this is the desired objective?\n\n\n\n\n------------\n**Minor concerns and suggestions:** Those are just things that I thought would improve the paper or minor issues I noticed. You do not need to address any of them and I did not factor them into any ratings. \n\n- While the authors do not have to fully explain every prior method used as a baseline, I think it would have been good to explain DRNet as it is used as the other baselines in the detailed comparisons presented in Table 6. \n- I think the conclusion of `In summary, RelatiViT successfully extracts effective visual representations for SRP` (sec 5.5) doesn't seem well supported. While it is great that a method can outperform such baselines and I appreciate the authors making such baselines very prominent in the paper, the results further showcase that we still do not have any methods that perform well on this task as their performance is still very close to models that only get bounding boxes. I think the paper is a good step towards better modeling and evaluation, but it is unclear if we have any methods that extract good feature for SRP.\n- Table 4: I think that adding some columns to clarify the difference between the methods would be useful. Speficially, Sec 4.1 did a nice job at clarifying the main modules for this task, and I thought that clarifying them in Table 4 would improve it.\n- Figure 4: A little bit of white space or gap between the plots would make the figure a lot more easier to parse especially since the attention maps meld into each other."
                },
                "questions": {
                    "value": "- Are the newer annotations simplifying the task or providing a stronger evaluation? I would appreciate it if the authors commented on the points raised in the weaknesses and provided some quantitative or qualitative results supporting it. I want to note that I think the paper is still valuable even if it was simplifying the annotation, it is just a matter of better explaining the contributions and limitations of the proposed dataset as an evaluation for this task. \n- How are the ablations conducted? I would appreciate it if the authors could explain what the model looks like under each ablations. As noted above, I think it would be stronger if the ablations are conducted independantly. If this is not possible (due to some reliance on each other), it would be good to explain that.\n- Do positional embeddings explain the difference between CNN and Transformers in Table 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698351409729,
            "cdate": 1698351409729,
            "tmdate": 1699636125999,
            "mdate": 1699636125999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YsoqAQUePu",
                "forum": "HgZUcwFhjr",
                "replyto": "LzHjpDBO0W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the insightful suggestions and supportive comments. We hope the following response will address your concerns.\n\n**Q1:** It is unclear if the newer annotation provides a better evaluation or a simpler task. It is possible that the new annotations are more strongly correlated with the relative locations in the image, which would introduce a strong prior.\n\n**Answer:** More precise definition does indeed result in an easier task. Imprecise definitions and the resulting inconsistent annotations lead to \"irreducible errors\" and therefore worse performance metrics. More importantly, the resulting models are less useful, because it is not clear what meaning to assign to the models if the task is specified in this imprecise/inconsistent way.\n\n**Q2:** The newer annotation scheme reduces the ambiguity but it also makes the relationships less dependent on context. For example, the definition of \u201cin front of\u201d in the paper is a much easier one than the typical usage which often also requires reasoning about the subject and object's relative orientation.\n\n**Answer:** Thanks for this example. In our opinion, this is only true for objects with \"a canonical front\". Not all object categories have such properties -- most don't. Furthermore, note that this involves reasoning through semantics. We do believe there is potential value for such a task, but it is worthwhile to decompose the geometric and semantic components of the task. i.e., first use our geometric notion of spatial relationships, then identify the semantic spatial orientations of the objects, and then combine these inferences into a \"semantic spatial relationship\" such as \"human A is in front of human B\".\n\n**Q3:** It is unclear how the method works without Pair Interaction, Context Aggregation, or Feature Extraction.\n\n**Answer:** In our model, Pair Interaction is implemented by the attention mechanism between subject and object query tokens within the transformer. We ablate it by applying an attention mask on the attention weight between these two tokens. For Context Aggregation, similarly, we employ an attention mask on the attention weights between the query tokens and the image patch tokens. For Feature Extraction, we remove the ViT Encoder and relied solely on the patch embeddings of the image patches to predict spatial relations. We will describe the ablation experiment setup more clearly in our revision.\n\n**Q4:** Provide the CNN model with some positional embeddings, e.g., through CoordConv.\n\n**Answer:** Thanks for your valuable suggestion! Following your advice, we incorporate positional embeddings into the input tensor of the first layer of ResNet in both DRNet and RUNet. The results, as illustrated in the table below, indicating that the CNN-based models (DRNet and RUNet) did not benefit significantly from the addition of positional embeddings. Their performance remains inferior compared to RelatiViT. This outcome reinforces our hypothesis that RelatiViT\u2019s superior performance is largely due to its enhanced ability to capture spatial relationships through attention mechanisms, as opposed to the sliding convolution used in traditional CNNs.\n\n|  Method   |  %Avg. Acc.  | %F1  |\n| :----: | :----: | :----: |\n| DRNet-CoordConv  | $76.86\\pm0.96$ | $75.44\\pm0.90$ |\n| RUNet-CoordConv  | $75.91\\pm0.73$ | $74.68\\pm1.26$ |\n| RelatiViT (ours) | $80.02\\pm0.73$ | $78.32\\pm1.01$ |\n\n**Q5:** More analysis is required to make the statement that the prior methods tend to make predictions mainly according to bounding box coordinates.\n\n**Answer:** Thank you very much for this suggestion. To verify whether the models predominantly rely on bounding box coordinates over image content, we conducted an intervention experiment by replacing the input images with random ones during testing. As shown in the following table, we report the *prediction flipping ratio*, which measures the frequency of predictions changing from True to False, or vice versa, across all samples. A higher flipping ratio suggests greater reliance on image content, while a lower ratio indicates a tendency to base predictions on bounding box coordinates. We find that DRNet and RUNet exhibit significantly lower flipping ratios than RelatiViT, indicating that RelatiViT places more emphasis on visual cues to when predicting spatial relations, contrary to the baselines focusing mainly on bounding box coordinates.\n\n| Method | %Prediction flipping ratio |\n| :----: | :----: |\n| DRNet | $2.33 \\pm 2.11$ |\n| RUNET | $6.99 \\pm 2.26$ |\n| RelatiViT | $29.47 \\pm 1.88$ |\n\n**Q6:** Typo in loss function.\n\n**Answer:** Thank you very much for pointing this out. We will correct it in our revision.\n\nOnce again, we are grateful for your precious time, insightful suggestions, and supportive comments. We will revise our paper according to your suggestions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552204852,
                "cdate": 1700552204852,
                "tmdate": 1700552204852,
                "mdate": 1700552204852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Hg41YJxb4",
                "forum": "HgZUcwFhjr",
                "replyto": "LzHjpDBO0W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_ubwR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_ubwR"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for engaging with all my comments and points. I respond below to each of those points. Overall, the points raised in Q1/2/6 are all addressed. I would appreciate the authors response regarding Q 3,4, and 5. \n\nQ1/Q2: The point made here is the newer annotations are more precise, and while they may make the task easier, this is primarily by resolving ambiguity (A1) and relying more on geometric rather than semantic relationships (A2). Overall, I see your point and mostly agree with it. One extra point I will add is that this is not just geometric, it's geometric relationships defined in a primarily ego-centric frame of reference. This changes things a bit as you can have well-defined relationships that do not depend on the observer, and such relationships would have much weaker relationships to the bounding box locations as they would be less view-dependent. One suggestion, in the spirit of your work, would be to revise the naming in Table 7 as many of those relationships are defined in an object-centric frame of reference; eg, in-front-of is currently defined with respect to the object, however, the relationship is a function of both the viewer and the object and is completely independent of the object's pose, just its location. Anyway, I think any remaining disagreements are more subjective, as a result, I think those points have been addressed. \n\nQ3: Thanks for the clarification. For pair interaction, does this mean that your attention matrix simply becomes a block matrix (with 4 blocks, 2 on the diagonal and 2 off the diagonal), where off-diagonal blocks are set to 0? if so, isn't that just equivalent to self attention with shared weights followed by concatenation? \n\nQ4: I appreciate the additional experiments, however, I find it difficult to make fair comparisons as both DRNet and RUNet have specialized architectures and hence, adding CoordConv to them does not provide a clear comparison to your method. In my review, I suggested the impact of positional embeddings specifically on the results in Table 3 as this, to me, is the key results table in the paper as it very nicely tests the claims of the paper. The question I have pertains to adding positional embeddings before the RoI align in Figure 3b. The reason I think this is an important experiment is because it provides the transformer following the CNN encoder with positional embeddings, something that it natively gets in the ViT encoders. Furthermore, by comparing the model to the adapted CNN-Transformer Figure 3b, we can control for other effects such as the architectural choices of previous approaches potentially being counterproductive. \n\nQ5: I appreciate going through the extra work. Overall, I really liked the experimental design of this experiment as it tests for sensitivity in a very nice way. However, my critiques in the above point hold. DRNet and RUNet do not provide a clear comparison to RelatiViT that addresses the main hypothesis/question of the paper as stated by the title and elaborated on in section 4."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598793999,
                "cdate": 1700598793999,
                "tmdate": 1700598833321,
                "mdate": 1700598833321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fc3NWPD7N2",
                "forum": "HgZUcwFhjr",
                "replyto": "jSaPruluUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_ubwR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_ubwR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the clarifications and for engaging this much with the reviewing process. I really appreciate it. Overall, all my concerns have been addressed. \n\nQ3: this is very interesting. I appreciate the additional nuance in clarifying the different attentional components. I think the paper will be much stronger with the additional clarifications. \n\nQ4: This is also very interesting. I expected the performance here would be higher giving the bounding box performance being so high. It is kind of amazing that the bounding box can do so well when the CNN-transformer does this poorly. You do not have to do anything else for the reviews, but I think that some additional discussion of those results and potential suggestions for experiments to further explore this phenomena would make for a very insightful discussion section. \n\nQ5: Thanks for running the additional experiments and my apologies for not making the connection between your answer and the weakness I had raised. I was very focused on the comparisons in Table 3 as I thought it was the salient point, and overlooked the other weakness I pointed out in my first review. I think the new experiments really substantiate your work by indicating how much the CNN-Transformer model attends to the visual information as well as its relatively weaker performance. \n\n\n---------\nI first want to emphasize that you do not have to do this experiment; It is just an experiment that I thought might be interesting to consider as it tries to address something that I am still confused about. \n\nIt remains very puzzling to me how bad the CNN+transformer model is given the strong performance of the bbox baseline. The very nice experiment you report in Q5's answer sheds some light on this: the model is paying attention to visual features which may be less informative, and as a result, might be overfitting to specific visual patterns in the data and failing to generalize. \n\nOne experiment that could shed more light on this is replacing the CNN features with ONLY the positional embeddings for a PosEmbed+Transformer baseline. This baseline would not depend on any visual information and at a high-level, should be learning something equivalent to the bbox baseline as it is getting the same information with a different parameterization. \n\nIt would be very interesting to see its performance. If it performs as well as the bbox baseline, it would further support the claim the CNN's features are bad and potentially resulting in overfitting (this should be accompanied with CNN+transformer also having a better training set performance). On the other hand, it might be possible that this baseline does worse or as well as the CNN+transformer. In this case, it would be very interesting to understand what about the transformer and the positional-encoding parametrization is resulting in this issue. I might be missing some subtlety or detail that makes the PosEmbed+Transformer baseline different from the bbox baseline, but it seems at a high level that they should learn equivalent functions and hence achieve similar performance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634094278,
                "cdate": 1700634094278,
                "tmdate": 1700634094278,
                "mdate": 1700634094278,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ElMRUlhHrK",
            "forum": "HgZUcwFhjr",
            "replyto": "HgZUcwFhjr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_hAXs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_hAXs"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the ability of current computer vision systems to recognize physically grounded spatial relations between objects. The authors propose precise relation definitions that allow for consistent annotation of a benchmark dataset and find that existing approaches perform poorly on this task. They propose new approaches that exploit the long-range attention capabilities of transformers and evaluate key design principles. The authors identify a simple \"RelatiViT\" architecture that outperforms all current approaches and demonstrate that it is the first method to convincingly outperform naive baselines on spatial relation prediction in in-the-wild settings. The findings have important implications for the development of computer vision systems and their ability to recognize spatial relations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes precise relation definitions that permit consistently annotating a benchmark dataset. \n2. the paper identifies a simple \"RelatiViT\" architecture that outperforms all current approaches and convincingly outperforms naive baselines on spatial relation prediction in in-the-wild settings.\n3. The paper evaluates key design principles and provides insights into the effectiveness of different approaches for recognizing physically grounded spatial relations between objects."
                },
                "weaknesses": {
                    "value": "1. The spatial relationships examined in this paper are rather straightforward, as they do not encompass interactions between humans and objects or comparative relationships among multiple objects. This limitation might confine the applicability of the proposed method.\n2. This paper would benefit from a more extensive examination of the reasons behind the superior performance of \"RelatiViT\" compared to the \"bbox-only\" baseline. Current methods utilizing image features do not match the simplicity and effectiveness of the \"bbox-only\" baseline. What sets \"RelatiViT\" apart?\n3. As illustrated in Table 6, \"RelatiViT\" does not consistently outperform the \"bbox-only\" baseline across various categories of spatial relations. The potential advantages of \"RelatiViT\" might not be evident when taking into account the higher computational demands it imposes for image feature processing.\n4. The paper fails to elucidate the reasons why \"RelatiViT\" exhibits superior performance in the \"left\" category as opposed to the \"bbox-only\" baseline, while displaying subpar performance in the \"right\" category.\n5. Although it is good to unveil that the current methods and datasets have limitations on this new task, it is still imperative to do a thorough study on each proposed component to show why such a design is essential to the performance improvements. It is hard to be convinced about the significance of the contribution based on the current form."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Reviewer_hAXs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698466851797,
            "cdate": 1698466851797,
            "tmdate": 1699636125904,
            "mdate": 1699636125904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m3KjViPbKm",
                "forum": "HgZUcwFhjr",
                "replyto": "ElMRUlhHrK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable feedback. We hope the following response will address your concerns.\n\n**Q1:** The applicability of the proposed method is limited because the spatial relations do not include the human-object and multi-object relations.\n\n**Answer:** Human-object interaction and multi-object relations are indeed promising topics to investigate. However, we find the existing methods struggle to accurately resolve even the fundamental spatial relations addressed in our paper. In this paper, we aim to investigate this spatial relation prediction task to show the importance and challenges of this under-explored field. Our intention is to provide a foundational framework that can serve as a springboard for tackling more complex scenarios in future research. \n\nAs for the applicability, our method can be easily applied to the robot systems to query for spatial relations. This capability can be further leveraged for advanced applications such as PDDL-based planning or unsupervised reward learning, which will boost the performance of robot learning.\n\n**Q2:** This paper would benefit from more analysis of the superior performance of RelatiViT and a thorough study on each proposed component.\n\n**Answer:** Thanks for your kind suggestion and for recognizing the good performance of our method. As also mentioned by Reviewer XTRZ and Reviewer ubwR, we have provided extensive experiments to analyze the success of RelatiViT in Section 5.5, Appendix A5, A7 and A8. In Section 5.5, we provide an in-depth ablation study, revealing that both context aggregation and good feature extraction are key contributors to the performance. The attention maps in Section 5.5 and Appendix A8 demonstrate that RelatiViT attends to appropriate regions to aggregate the context information for accurate spatial relation prediction. And the comparisons on each relation category in Section 5.5 and Appendix A5 show that RelaTiViT significantly surpasses baselines (especially bbox-only) in categories where visual information is critical. This indicates that RelatiViT successfully extracts the visual representations containing rich spatial relation information. Additionally, the prediction visualizations in Appendix A7 showcase that RelatiViT predicts the relations based on the visual information, e.g., object contact, occlusion, depth and background context, etc. This capability fundamentally contributes to its superiority over bbox-only baselines.\n\n**Q3:** \"RelatiViT\" does not consistently outperform the \"bbox-only\" baseline across various categories of spatial relations.\n\n**Answer:** In Table 6, our results are either comparable or outperform the bbox-only baselines across most categories. Moreover, in Table 10, RelatiViT performs better than bbox-only on 29 out of 30 predicate categories, which is a significant improvement. \n\nIt is essential to recognize that while bbox-only models may achieve comparable accuracy in certain scenarios, such blind models, which do not leverage image inputs, possess inherent limits in further improvements. In contrast, our method leverages both visual data and advanced attention mechanisms, marking a notable advancement in predicting spatial relations. For the first time, our work demonstrates that vision-based methods can substantially exceed the bbox-only baseline. This achievement highlights the pivotal role of visual cues and relation modeling in this task. We believe that future research building upon our methodology will achieve even greater accuracy, far surpassing simpler baselines. \n\n**Q4:** The reasons why \"RelatiViT\" exhibits superior performance in \"left\" but worse in \u201cright\u201d.\n\n**Answer:** There might be some misunderstanding. According to Table 6, the performance of RelatiViT is comparable with bbox-only on \u201cleft\u201d and \u201cright\u201d. However, it's important to highlight that RelatiViT demonstrates significantly superior performance on more complex spatial relations such as \"behind\", \"in\", \"under\", and \"front\". These particular relations inherently require a more nuanced understanding of visual information.\n\nThanks again for your precious time. Please let us know if there are any additional questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551910904,
                "cdate": 1700551910904,
                "tmdate": 1700551910904,
                "mdate": 1700551910904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NlgJL2Nqf7",
                "forum": "HgZUcwFhjr",
                "replyto": "m3KjViPbKm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_hAXs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_hAXs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my raised issues. I decided to change my rating to marginally above the acceptance threshold."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649440914,
                "cdate": 1700649440914,
                "tmdate": 1700649440914,
                "mdate": 1700649440914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YOZlyPAqjB",
            "forum": "HgZUcwFhjr",
            "replyto": "HgZUcwFhjr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_MUvq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_MUvq"
            ],
            "content": {
                "summary": {
                    "value": "This paper sets out to study transformers' capability of understanding spatial relations in a scene. The authors strictly define spatial relations and refine previous datasets accordingly. The authors propose several transformer-based models to tackle spatial relation prediction task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It is interesting to delve into whether transformers could understand spatial relations in a scene instead of simply exploiting spatial biases.\n\n- The linguistic biases during annotation brought up by this paper are reasonable.\n\n- The proposed models are reported to be effective on refined datasets. The discussion on design axes is informative.\n\n- Attention map visualization is insightful as it shows that the proposed model captures meaningful contextual information when making a prediction."
                },
                "weaknesses": {
                    "value": "- Although resolving linguistic biases in annotations is a well-motivated aspect of this work, I am not sure if these issues are nontrivial. It would be great if there were a quantitative bias analysis of previous datasets.\n- Besides linguistic biases during the annotation process, the statistics of spatial relations are biased, which could lead spatial relation detection to simply object classification. Rel3D addressed it by employing minimally contrastive construction, I would like to see authors' discussion on SpatialSense+. \n- Despite being titled as \"Can Transformers Capture Spatial Relations between Objects\", all prior baselines are CNN-based methods, it would be great to see authors include more advanced transformer-based models [1].\n\n[1] SGTR: End-to-end Scene Graph Generation with Transformer, Li et al, 2021."
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Reviewer_MUvq",
                        "ICLR.cc/2024/Conference/Submission1945/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659627205,
            "cdate": 1698659627205,
            "tmdate": 1700722801916,
            "mdate": 1700722801916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LvpsjCJJDm",
                "forum": "HgZUcwFhjr",
                "replyto": "YOZlyPAqjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable suggestions. We hope the following response will address your concerns.\n\n**Q1:** Quantitative bias analysis of linguistic biases.\n\n**Answer:** To demonstrate quantitatively how linguistic bias affects annotation quality, we analyze the top 20 subject-predicate-object triplets with the most significant change in labels after our relabeling process in the SpatialSense dataset. We also provide the issue types that make the original annotation incorrect, including object-centric, polysemous words, and idiomatic expressions, i.e., the three types of errors as discussed in Section 3.2. Our findings reveal that idiomatic expressions account for 20% of the incorrectly labeled triplets, polysemous words contribute to 10% of these errors, and the remaining 70% is mixing up object-centric and viewer-centric relations. The percentages of idiomatic expressions and polysemous words highlight the typical failure cases caused by linguistic bias in the dataset.\n\n| Triplet | Label Flip Ratio | Type |\n| :----: | :----: | :----: |\n| window-in front of-chair | 1.0 | object-centric |\n| frame-on-wall | 1.0 | idiomatic expressions |\n| women-to the left of-women | 1.0 | object-centric |\n| girl-to the left of-man | 1.0 | object-centric |\n| bus-in-road | 1.0 | idiomatic expressions |\n| book-on-shelf | 0.95 | polysemous words |\n| books-on-cupboard | 0.8 | polysemous words |\n| woman-to the left of-man | 0.8 | object-centric |\n| man-to the left of-woman | 0.8 | object-centric |\n| woman-to the left of-woman | 0.78 | object-centric |\n| table-in front of-chair | 0.75 | object-centric |\n| boy-to the left of-girl | 0.75 | object-centric |\n| woman-to the right of-woman | 0.75 | object-centric |\n| chair-to the right of-chair | 0.69 | object-centric |\n| man-on-wall | 0.67 | idiomatic expressions |\n| tree-in-ground | 0.67 | idiomatic expressions |\n| chair-next to-desk | 0.67 | object-centric |\n| women-to the left of-men | 0.67 | object-centric |\n| building-next to-tree | 0.67 | incorrect definition |\n| hen-to the left of-hen | 0.67 | object-centric |\n\n\n**Q2:** Discussion about the statistics bias of spatial relations.\n\n**Answer:** Thanks for this insightful question. The original SpatialSense dataset is collected with adversarial crowdsourcing strategy. Specifically, the authors trained a simple model to predict spatial relations mainly based on the statistics, and then they asked crowdsourcing annotators to only label the samples that cannot be handled by this model. In this process, the statistics bias has been much alleviated. Our SpatialSense+ retains these samples and predicate categories from the original dataset. Consequently, we inherit this advantage of mitigating statistical bias.\n\n**Q3:** Compare with more advanced transformer-based models.\n\n**Answer:** Thank you for your insightful suggestions. We apologize for missing references such as SGTR [1] and RelTR [2]. Firstly, we want to clarify that these works are in different settings from ours: SGTR and RelTR aim to generate scene graphs and include an end-to-end detection process, but in our setting the detection is not required. Recognizing the distinct settings of our work, we make much effort to adapt RelTR to align with our experimental setting. This involved maintaining the same ResNet backbone as RelTR, encoding bounding box and object category information into learnable entity tokens, and modifying the relation prediction head to mirror the K-way binary classification head used in our models.\n\nUpon implementing these changes, we observed an average accuracy of 57.91% with RelTR, which is markedly lower than anticipated. We attribute this underperformance to two primary factors: \n\n(1) RelTR relies on the features from only the last layer of ResNet, which we believe results in a significant loss of spatial information. \n\n(2) RelTR leverages learnable tokens to establish correspondence between the queries and the feature in region of interests, and then aggregate the information. This is difficult to achieve by the supervision of a small training set. Differently, our model utilizing large-scale pretrained transformer features and design a mask image based query localization strategy, which is able to establish the correspondence with fewshot training samples.\n\nThis experiment further highlights the capability to capture spatial relationships of our model. We will definitely include these references in our revision.\n\n[1] Li R, Zhang S, He X. Sgtr: End-to-end scene graph generation with transformer[C]//proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 19486-19496.\n\n[2] Cong Y, Yang M Y, Rosenhahn B. Reltr: Relation transformer for scene graph generation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\n\nWe appreciate your precious time in reviewing our paper. Please let us know if you have additional questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551801355,
                "cdate": 1700551801355,
                "tmdate": 1700551801355,
                "mdate": 1700551801355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pagf0kSldN",
                "forum": "HgZUcwFhjr",
                "replyto": "YOZlyPAqjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Has our response addressed your concerns?"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThanks again for your precious time and insightful suggestions. We hope we have addressed your concerns comprehensively. As the deadline is approaching, with less than 8 hours remaining, we kindly request that you let us know if there are any further questions or clarifications needed.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716777313,
                "cdate": 1700716777313,
                "tmdate": 1700716777313,
                "mdate": 1700716777313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BGAyvRV8X8",
                "forum": "HgZUcwFhjr",
                "replyto": "YOZlyPAqjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_MUvq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_MUvq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses. I think my concerns have been addressed. I also found the discussions of **hAXs.1**, **ubwR.3**,  **ubwR.5**,  **Jenr.2**,  **Jenr.5** valuable. Thus, I would like to raise my score to 6 and would not object to accepting this paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722843194,
                "cdate": 1700722843194,
                "tmdate": 1700722843194,
                "mdate": 1700722843194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aWPMo135xx",
            "forum": "HgZUcwFhjr",
            "replyto": "HgZUcwFhjr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_XTRZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_XTRZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how to understand spatial relationship between objects within a single image. GIven that the current benchmarks are synthetic (Rel3D) or ambiguous (SpatialSense), it starts with a re-annotation of SpatialSense with precise relation definitions and calls the new dataset SpatialSense+. Since existing state-of-the-art methods perform poorly on this benchmark, the paper proposes a RelatiViT, which is a ViT based network to resolve the problem. This is the first approach which outperforms naive baseline significantly on multiple benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I think this is a solid paper, and recommend accepting the paper.\n\n- The paper discusses limitations of current spatial relationship benchmarks. That makes a lot of sense to me.\n- The paper did an extensive study of different network architectures to capture the spatial relationship between objects.\n- Experiments suggest the proposed RelatiViT has strong performance on multiple benchmarks.\n- The writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper mainly compared RelatiViT with ConvNet and ViT based networks. However, some of current multimodal Large language models (e.g. LLaVA [1], or even GPT-4V) seem be to capable of handling some spatial relationships. But I think it's acceptable to ignore these literature as they are very recent works and are not specially designed for this task.\n- The annotation of SpatialSense+ can be very hard to scale, since it requires annotators to understand precise definition of spatial relationships.\n\n\n[1] Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee. Visual Instruction Tuning. NeurIPS 2023."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1945/Reviewer_XTRZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779523278,
            "cdate": 1698779523278,
            "tmdate": 1699636125737,
            "mdate": 1699636125737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GQNcoQPle4",
                "forum": "HgZUcwFhjr",
                "replyto": "aWPMo135xx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the insightful suggestions and supportive comments. We hope the following response will address your concerns.\n\n**Q1:** Compare with large vision language models.\n\n**Answer:** Thank you for noting that our work precedes the publicly available large vision-language foundation models. It is indeed valuable to evaluate these models on spatial relationship prediction extensively, and we have performed some early tests on LLaVA. We evaluate the SpatialSense+ dataset on LLaVA latest version, i.e., LLaVA-v1.5-13b, and it only gets 50.27% overall accuracy, indicating that LLaVA   cannot handle this physically grounded spatial relation task. We think this result is very interesting and provides some inspiration for us about future work, so thank you very much again for this insightful suggestion.\n\nRegardless of the specific results, for the foreseeable future, computation and time costs make it impractical for these foundation models to be directly applicable in many scenarios of interest, such as robotics with size, weight, power, and real-time operation constraints. We believe that it is valuable both practically for these applications and scientifically to understand and maximize the computer vision capabilities of smaller models for a large variety of tasks including ours.\n\n**Q2:** The annotation of SpatialSense+ can be very hard to scale.\n\n**Answer:** Thanks for your insightful question. We do not expect the labeling process to scale up, but want to fix this issue by taking advantage of large-scale unsupervised data and few-shot labeled data (i.e., self-supervised pretraining and finetuning). We admit that annotating such complex tasks cannot easily scale up, because it would definitely be very difficult and cost a lot of labor and financial resources. However, with the recent progress of foundation models, finetuning from large pretrained models makes it possible to inherit rich prior knowledge to handle complex downstream tasks with few-shot training sets. This has been a common solution for difficult problems in deep learning nowadays. Motivated by this, in this paper, we aim to fine-tune a good relation prediction model from the pretrained models with a few samples. In the future, especially in the age of large foundation models, we expect the solution to be reading out the relation information from foundation models by few-shot finetuning or even zero-shot prompting. Therefore, we do not require a large-scale spatial relation dataset, but only need a good pretrained model and a well-designed finetuning architecture and strategy, which is actually what we investigate in this paper.\n\nOnce again, we are grateful for the valuable and supportive comments!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551268402,
                "cdate": 1700551268402,
                "tmdate": 1700551268402,
                "mdate": 1700551268402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NzMxQbwmOS",
                "forum": "HgZUcwFhjr",
                "replyto": "GQNcoQPle4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_XTRZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Reviewer_XTRZ"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors"
                    },
                    "comment": {
                        "value": "I appreciate your LLaVA experiments. I personally feel it sometimes gets the correct spatial relationship, but I have never tested it on any benchmark.\n\nI'm confident to keep my rating after reading other reviews and your response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630984441,
                "cdate": 1700630984441,
                "tmdate": 1700630984441,
                "mdate": 1700630984441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dev9JIBRAa",
            "forum": "HgZUcwFhjr",
            "replyto": "HgZUcwFhjr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_Jenr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1945/Reviewer_Jenr"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the task of spatial relation classification on a single 2D image. Given two objects bounding boxes, the authors uses the nine predicates proposed in the SpatialSense dataset and correct some of its ambiguous labels. To solve the task, the authors present different architectures and promote a simple transformer based model. Experiments are also conducted on REL3D dataset with ablation studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose a more accurate definition of the problem of spatial relation, although the limitations of this definition is not completely covered.\n\nThe motivation to use transformers to model object relation is sensible, as relation modelling this is one of the main motivations over CNNs baselines.\n\nThe proposed architecture is validated with different baselines and ablation studies. In particular, I appreciate that the author also report the results of naive baselines. Experiments confirmed the usefulness of the proposed transformer mechanism to model relations."
                },
                "weaknesses": {
                    "value": "The current superiority of backbone transformers over CNNs and the ability of transformers to model object relations are commonly admitted. So in this aspect, this downgrade the insights brought by the results of the paper. \n\nThe claim of the authors to provide \"a precise and unambiguous definition\" seems a bit exaggerated. There are natural ambiguities in the task of spatial relationships. For instance, even for humans, it is difficult to judge on same natural images the relations \"above\" if the objects are very distant and that it is not easy to judge if they are on the same plane. For instance, between a car and a mountain in the background (this is one of the examples provided in the supplementary material).\n\nThe definition provided by the authors include subjective (which seems naturally inevitable for this task) appreciations in the supplementary material. For instance, the relation \"behind\" is ignored if the objects are \"in different directions\" and the relation \"in front of\" is ignored \"if the distance is too large\". This definition is not provided by the author, and in practice, it might even depend on the application and the image context. In the case of robotic application, which is the motivation suggested in the introduction of the paper, the definition of \"next\" could applied to all objects belonging to the same group. \n\nLastly, the author reuse the same predicates as the previous dataset. This limits their contribution to making corrections on an existing dataset.\n\nIn general, the paper does not bring a lot of new insights about the method as the modelling power of transformer is well known. This work do bring an improvement to an existing dataset, enabling researchers to better evaluate their method in the task of spatial relations, but I am not sure whether this in itself would be of interest to the ICLR community. Maybe this paper would be a better fit for a more vision or robotic dedicated conference."
                },
                "questions": {
                    "value": "in section 5.4, the author wrote \"bbox-language cannot beat bbox-only, indicating that we have successfully removed the language bias in our benchmark.\"\n\nHowever, in Table 11 where there is an evaluation on the original SpatialSense, where the bbox-only is also superior to bbox-language. Then, how the author can conclude to the relations between these two baselines and the language bias ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835061669,
            "cdate": 1698835061669,
            "tmdate": 1699636125670,
            "mdate": 1699636125670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tSuPdGUFIN",
                "forum": "HgZUcwFhjr",
                "replyto": "dev9JIBRAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable suggestions. We hope the following response will address your concerns.\n\n**Q1:** The proposed transformer-based method does not bring a lot of new insights because the superiority of transformer is commonly admitted.\n\n**Answer:** Firstly, we want to restate that our argument in this paper is not only transformer is better than CNN, but also transformer architectures are not always optimal but need special design and correct visual prompts to read out the relation information, as we compared and discussed in  Section 5.3. Besides, transformers actually have limited improvements on common tasks, e.g., image classification (top1 acc: ImageNet ResNet101 80.67 V.S. ViT-Base 80.73) and detection (COCO AP: Faster RCNN 42.0 V.S. DETR 43.3). In contrast, our transformer architecture shows significant performance boosting on the spatial relation task, as also mentioned by Reviewer XTRZ, MUvq and ubwR. We believe the architectural design principles will provide insight for the community.\n\n**Q2:** There are natural ambiguities in the task of spatial relationships. So the claim of \"a precise and unambiguous definition\" seems a bit exaggerated.\n\n**Answer:** A precise definition of the spatial relationship is possible with complete knowledge of the scene. This is different from whether a 2D image of the scene always presents this information unambiguously. This is also true for other types of visual properties that we like to infer, such as object shape: humans may not always be able to infer full 3D shapes of objects from an image, but the 3D shape is of course well-defined. Therefore, we provide precise and unambiguous definitions according to the geometries to annotate the dataset. As for the capability of vision model to solve this challenging task based on 2D images where there might existing ambiguity, we think it can be solved by large-scale pretraining and special architecture design. In this paper, we demonstrate this potential solution by our ReltiViT and IBOT pretrained vision transformer. We think this is a good starting point.\n\n**Q3:** The definitions provided by the authors include subjective (which seems naturally inevitable for this task) appreciation.\n\n**Answer:** Thank you for raising this point. It is indeed more precise to separate an \"in-principle definition\" from an \"operationalizable definition\" that an annotator can use to provide consistent annotations. We will clarify that these definitions are intended to focus human labeling efforts on unambiguously true predicates. In our annotation process, a sample was annotated by several annotators and the final label was voted among these multiple annotations, which reduces this problem.\n\n(This response is continued in the next comment block.)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551074547,
                "cdate": 1700551074547,
                "tmdate": 1700551316519,
                "mdate": 1700551316519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "srK2YCnsva",
                "forum": "HgZUcwFhjr",
                "replyto": "dev9JIBRAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4:** The contribution to dataset is limited because the author reuses the same predicates as the previous dataset.\n\n**Answer:** We would like to clarify that the main problem with the original SpatialSense dataset for studying spatial relationship prediction is not that it lacks some predicates, or that it has an uninteresting image distribution, but that its predicate labels are not consistently annotated. This is naturally what we have aimed to fix in our paper. Our experiments show that the relabeled SpatialSense+ is a reliable benchmark to evaluate the spatial relation prediction models, indicating our contribution to the whole community.\n\n**Q5:** The conclusion that language bias is removed cannot be drawn from the experiment result that Bbox-only is better than Bbox+language.\n\n**Answer:** We are sorry for the incorrect statement. There are two kinds of language bias: \n\n(1) The language bias from commonsense: a computer is usually *on* a table, so the model will predict \u201con\u201d without adequately considering the image and bounding box information, even when the computer is actually under the table. Actually, this kind of language bias has been mitigated by the original SpatialSense dataset through *adversarial crowdsource* strategy. And our SpatialSense+ is relabeled based on the original dataset and thus inherits this property. Consequently, bbox+language performs worse than bbox-only in both SpatialSense+ and SpatialSense datasets. We will correct this point and ensure clarity in our revision.\n\n(2) The language bias from some idiomatic expressions, i.e., the problem we discussed in Section 3.2, which will lead to ambiguous and noisy annotation, e.g., the boy is sometimes labeled as *in* snow and sometimes *on snow*. To illustrate this, we show some examples in the tables below, where language biases impact labeling. For example, \u201cbook-on-shelf\u201d samples are actually False because the books were not placed *on* the top of the shelves, but *in* one of the tiers of the shelf. These samples cannot be predicted only according to language and only affect the annotation quality. During our relabeling process, these mistakes are corrected by our precision and physically grounded definitions. These examples underscore our success in eliminating idiomatic expression language bias. We will include this analysis in our revision.\n\n**On:**\n| Triplet | Label Flip Ratio |\n| :----: | :----: |\n| frame-on-wall | 1.0 |\n| book-on-shelf | 0.95 |\n| books-on-cupboard | 0.8 |\n| man-on-wall | 0.67 |\n| sign-on-building | 0.33 |\n\n**In:**\n| Triplet | Label Flip Ratio |\n| :----: | :----: |\n| bus-in-road | 1.0 |\n| tree-in-ground | 0.67 |\n| dog-in-water | 0.33 |\n| man-in-water | 0.33 |\n| wire-in-wall | 0.25 |\n\nWe appreciate your precious time. We are eager to engage in further discussions to clear out any confusion."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551157293,
                "cdate": 1700551157293,
                "tmdate": 1700551361856,
                "mdate": 1700551361856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1nkhH2Fn8G",
                "forum": "HgZUcwFhjr",
                "replyto": "dev9JIBRAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Has our response addressed your concerns?"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThanks again for your precious time and insightful suggestions. We hope we have addressed your concerns comprehensively. As the deadline is approaching, with less than 8 hours remaining, we kindly request that you let us know if there are any further questions or clarifications needed.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716732602,
                "cdate": 1700716732602,
                "tmdate": 1700716732602,
                "mdate": 1700716732602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]