[
    {
        "title": "Constrained Decoding for Cross-lingual Label Projection"
    },
    {
        "review": {
            "id": "8DdP4K6Weq",
            "forum": "DayPQKXaQk",
            "replyto": "DayPQKXaQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_NyaM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_NyaM"
            ],
            "content": {
                "summary": {
                    "value": "This work describes CODEC, a method of generating instances of data in new languages with fine-grained labels transferred from high-resource languages (i.e., English) to low-resource languages (e.g., Bambara). This work intelligently identifies that prior methods such as EasyProject have the drawback of non-natural markers (e.g., BIO tokens) degrading translation quality. To counter this, CODEC instead uses an unconstrained translation as a template and proposes a constrained decoding algorithm to reconcile the template with the annotated input. \n\nThis changes the formula of EN to BAM from:\n```\nEN -> add markers -> MT -> BAM + markers\n```\nto:\n```\nEN -> BAM,    EN -> add markers -> [EN+markers, BAM] -> MT w/ constrained decoding -> BAM + markers\n```\nThis removes the issue of MT errors near annotation tokens and provides some reference to check approximate validity during CODEC. This work applies CODEC to both the translate-train and translate-test scenarios of cross-lingual transfer to identify that CODEC has benefits nearly everywhere we can use silver-standard data in cross-lingual transfer. Experiments on NER and event argument extraction identify how CODEC benefits cross-lingual transfer across a wide range of low-resource languages. Ablations and analysis across multiple languages are honest and interpretable in discussing where CODEC is beneficial and does not improve."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This is a very original contribution with wide ranging impact to low-resource cross-lingual transfer. Provided a sufficiently user-friendly codebase, the contributions of CODEC to the field could be widespread. This work also provides a more holistic and thoughtful contribution to the problem than the concurrent https://arxiv.org/abs/2309.08943 . Overall, I think this paper absolutely should be accepted.\n\n- The improvement in both translate-train and translate-test scenarios identify the method as a strong new idea with wide applicabiility. Provided _some_ MT capability, this work helps mitigate the cross-lingual transfer gap to languages with very little study. The work smartly focuses on this scenario (i.e., through MasakhaNER) to support that CODEC works in (approx) the lowest resource scenarios available in modern NLP. \n\n- Succintely describing a constrained decoding method is not easy and this work smartly describes the method visually and mathematically for excellent clarity of the contribution. The frank discussion of complexity and the heuristic approximations for tractability are also honest with tradeoffs discussed in detail to inform future practice."
                },
                "weaknesses": {
                    "value": "- [Minor]: the work could be stronger if this could also be extended to larger models (e.g., >1-5B) and the discussion of applicability on more architectures (enc-only, enc-dec and dec-only) could be more details. \n\n- [Minor]: the paper could also be improved with more comparison against zero-shot results from larger multilingual LLMs. This would be hard on the given 1 48GB GPU setup, but could strengthen the vailidity of the improvement using CODEC. In essence, asking if CODEC works on a larger scale would make the findings more universal.\n\n- [Minor]: it would be enlightening to see CODEC across a benchmark such as XTREME-UP but this likely should be future work not included here."
                },
                "questions": {
                    "value": "- Constrained decoding is also a large topic in semantic parsing and the authors could acknowledge work such as https://arxiv.org/abs/2109.05093\n\n- The sentence \"The intuition is that, if we decode the translation template but conditioned on the marker sentence, at the position that needs to be inserted an opening marker, the model would give a high probability to this token, and thus assign a low probability to the token from the template, as illus- trated in Figure 2 (Step 1).\" is very long and hard to parse. Consider revising.\n\n- Math format mistake in F2 caption, k -> $k$\n\n- Consider a bulleted list at the end of the intro to make your contributions clearer.\n\n- I think the introduction of Bambara as a language from Africa undersells the low resource importance. Consider a more quantitative phrasing such as \"Bambara is a Manding language primarily from Mali with approximately 15 million speakers\", using information from WALS and Ethnologue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698073761563,
            "cdate": 1698073761563,
            "tmdate": 1699636689072,
            "mdate": 1699636689072,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9yKFNOMnEm",
                "forum": "DayPQKXaQk",
                "replyto": "8DdP4K6Weq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments. We have edited the paper to fix the typos and acknowledge the work you suggested. We have also updated the paper to cover two of the points you mentioned as below:\n\n> **[Minor]: the work could be stronger if this could also be extended to larger models (e.g., >1-5B) and the discussion of applicability on more architectures (enc-only, enc-dec and dec-only) could be more details.**\n\nThank you for the suggestion, we have conducted more experiments with different sizes of the MT model used for constrained decoding in Codec and update the results in the Appendix C.3 of the paper.\n\n> **[Minor]: the paper could also be improved with more comparison against zero-shot results from larger multilingual LLMs.**\n\nIn our original submission, we fine-tune mDebertaV3-base model on English data (FT_En) as a zero-shot baseline for the Cross-lingual NER task. Inspired by your comments, we have added two multilingual pre-trained models, which are more competitive for African languages, for the zero-shot baseline: AfroXLMR-large [1], a model that is adapted to 17 African languages, and Glot500 [2], a model which is pre-trained on 500+ languages including African languages. Below are results of models on the test set of MasakhaNER2.0. We have updated these results in the Appendix C.1 of the paper.\n\n|  | FT_En | FT_En | FT_En | Codec | Codec |\n|-----------|------------|----------------|---------|-----------------|----------------|\n| **Languages** | mDeBERTaV3 | AfroXLMR| Glot500 | Translate-train | Translate-test |\n| Bambara     | 37.1    | 42.8    | 53.0    | 45.8    | **55.6**    |\n| Ewe         | 75.3    | 73.1    | 75.4    | **79.1**    | **79.1**    |\n| Fon         | 49.6    | 54.6    | 59.4    | **65.5**    | 61.4    |\n| Hausa       | 71.7    | **74.8**    | 68.0    | 72.4    | 73.7    |\n| Igbo        | 59.3    | **74.4**    | 65.4    | 70.9    | 72.8    |\n| Kinyarwanda | 66.4    | 68.4    | 67.0    | 71.2    | **78.0**    |\n| Luganda     | 75.3    | 78.9    | 80.9    | 77.2    | **82.3**    |\n| Luo         | 35.8    | 40.4    | 42.0    | 49.6    | **52.9**    |\n| Mossi       | 45.0      | 45.3    | 55.5    | **55.6**    | 50.4    |\n| Chichewa    | 79.5    | **82.2**    | 73.8    | 76.8    | 76.8    |\n| chiShona    | 35.2    | 38.4    | 37.6    | 72.4    | **78.4**    |\n| Kiswahili   | 87.7    | **88.1**    | 84.7    | 83.1    | 81.5    |\n| Setswana    | 64.8    | 74.4    | 68.8    | 74.7    | **80.3**    |\n| Akan/Twi    | 50.1    | 41.9    | 57.9    | 64.6    | **73.5**    |\n| Wolof       | 44.2    | 49.0    | 64.5    | 63.1    | **67.2**    |\n| isiXhosa    | 24.0      | 26.8    | 27.8    | **70.4**    | 69.2    |\n| Yoruba      | 36.0      | 57.0    | 56.1    | 41.4    | **58.0**    |\n| isiZulu     | 43.9    | 47.3    | 46.5    | 74.8    | **76.9**    |\n| **Average**     | 54.5    | 58.8    | 60.2    | 67.1    | **70.4**    |\n\n[1] Alabi et al. Adapting pre-trained language models to African languages via multilingual adaptive fine-tuning. COLING 2022.\n\n[2] ImaniGooghari et al. \u201cGlot500: Scaling multilingual corpora and language models to 500 languages\u201d. ACL 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637352295,
                "cdate": 1700637352295,
                "tmdate": 1700637352295,
                "mdate": 1700637352295,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J1awxz4JTb",
            "forum": "DayPQKXaQk",
            "replyto": "DayPQKXaQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_VywY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_VywY"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a constrained-translation-based label projection method for the cross-lingual transfer of two mention extraction problems (named entity recognition and event argument extraction). Instead of directly translating marked sentences, the proposed the method adopts a two-stage approach: first translate the original source without markers, then perform constrained decoding with the marked source and the translation in the first pass. The decoding algorithm consists several interesting parts, including marker-position pruning, branch-and-bound searching and re-ranking. With evaluations on multiple target languages, the proposed method is shown to provide benefits over existing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n- The proposed method is intuitive and effective."
                },
                "weaknesses": {
                    "value": "- The approach relies on an external MT system, whose performance may influence the effectiveness of the label projection. It would be nice if there can be an analysis on the influence of translation quality.\n- In some cases, the proposed method does not perform well, for example, the NER results are worse for Chichewa and Kiswahili, and the EAE results seem close to the baselines. It would be much better if there can be more analysis on why these happen to provide some guidance on how to select the label-projection methods for a new language."
                },
                "questions": {
                    "value": "- It would be nice to discuss and measure the efficiency of different methods, especially considering the extra stages of the proposed method. This can be important for \u201ctranslate-test\u201c, and maybe also for \u201ctranslate-train\u201d if the cost difference is too much.\n- I\u2019m wondering whether it would still be effective to replace the searching algorithm with some simpler alternatives, such as greedy pruning (like in a QA-MRC model). Since the problem itself is inserting a pair of markers, the output space is much smaller than the translation.\n- For the event task, it seems that the event triggers are assumed already given? How about considering the full event structures? This might be straight-forward since only span-projection would be enough (it would also be very interesting to consider pairs of spans when projecting)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698287970196,
            "cdate": 1698287970196,
            "tmdate": 1699636688891,
            "mdate": 1699636688891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uIlH9OitRn",
                "forum": "DayPQKXaQk",
                "replyto": "J1awxz4JTb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. Please find below our response to each point of your comments and some updates in the paper based on your suggestions. \n\n>**The approach relies on an external MT system... It would be nice if there can be an analysis on the influence of translation quality.**\n\nThank you for your suggestion, we have conducted additional analysis about this topic and updated the paper with the new analysis in Appendix C.3. The update about this analysis is as follows. There are two places where MT systems are used in Codec: one is for generating the translation template, and the other one is for decoding. \n* We analyzed the impact of using different sizes of NLLB (600M, 1.3B, and 3.3B) to generate the translation templates, while fixed on NLLB-600M as the MT system for constrained decoding. We evaluate the methods in the translate-test setting on the MasakhaNER2.0 dataset, over 18 languages. Overall, the average F1 scores of methods using NLLB-600M, NLLB-1.3B and NLLB-3.3B as the template-translator are 68.6, 70.6 and 70.4 respectively. We observe the performance improves the most when changing from NLLB-600M to 1.3B, where NLLB\u2019s translation quality also improves the most [1].\n* Given the same translation templates (i.e., using NLLB-3.3B as the template-generator), we explored using Codec with the three model sizes of NLLB as the constrained decoding module. The average F1 score of methods using NLLB-600M, NLLB-1.3B, and NLLB-3.3B are 70.4, 70.1, and 70.1, respectively. \n\n[1] NLLB Team. No Language Left Behind: Scaling Human-Centered Machine Translation. https://arxiv.org/abs/2207.04672\n\n>**In some cases, the proposed method does not perform well, for example, the NER results are worse for Chichewa and Kiswahili, and the EAE results seem close to the baselines. It would be much better if there can be more analysis on why these happen to provide some guidance on how to select the label-projection methods for a new language.**\n\nFor the case of Chichewa and Kiswahili, we observe that, for a large portion of the data, the entities in the target language are exactly the same as their English form, therefore, the multilingual model fine-tuned only on English data does extremely well in these two languages , and outperforms all label projection methods. We have added a discussion about this in the Experiments section.\n>**I\u2019m wondering whether it would still be effective to replace the searching algorithm with some simpler alternatives, such as greedy pruning...**\n\nWe have implemented a simpler search algorithm, we dub it as Constrained-Space Beam Search (CSBS). In particular, this algorithm behaves similarly to the vanilla beam search, but we constrain the search space of it to contain only valid hypotheses - following the translation template and having the correct number of markers.  Similar to Codec, CSBS also returns up to 5 best hypotheses (equal to the beam size when the beam size is smaller than 5), and a re-ranking step is performed to choose the best hypothesis. Below is the performance of CSBS with different beam sizes and Codec on the MasakhaNER2.0 test set. The methods are evaluated in the translate-test setting (see below). We have updated these results in the Appendix E of the paper. Overall, a higher beam size will increase the performance of CSBS, however, even with a beam size of 16, CSBS\u2019s average F1 score still falls behind Codec with a big gap. \n\n|  | CSBS | CSBS | CSBS | CSBS | Codec |\n|-------------|--------|--------|--------|---------|-------|\n| **Languages**   | beam=2 | beam=4 | beam=8 | beam=16 |  |\n| Bambara     | 32.6   | 35.9   | 38.5   | 42.4    | 55.6  |\n| Ewe         | 54.8   | 61.0   | 66.8   | 70.6    | 79.1  |\n| Fon         | 21.3   | 24.2   | 29.0   | 32.7    | 61.4  |\n| Hausa       | 54.4   | 66.4   | 68.7   | 70.1    | 73.7  |\n| Igbo        | 46.2   | 52.3   | 56.5   | 60.3    | 72.8  |\n| Kinyarwanda | 61.5   | 70.7   | 72.7   | 74.7    | 78.0  |\n| Luganda     | 62.0   | 74.2   | 77.8   | 80.3    | 82.3  |\n| Luo         | 27.4   | 31.8   | 34.4   | 41.1    | 52.9  |\n| Mossi       | 22.2   | 25.3   | 29.4   | 34.5    | 50.4  |\n| Chichewa    | 51.9   | 59.0   | 62.5   | 67.8    | 76.8  |\n| chiShona    | 68.0   | 74.0   | 75.3   | 76.2    | 78.4  |\n| Kiswahili   | 57.8   | 73.5   | 75.8   | 77.3    | 81.5  |\n| Setswana    | 69.2   | 74.7   | 76.7   | 78.1    | 80.3  |\n| Akan/Twi    | 57.3   | 63.1   | 65.4   | 68.1    | 73.5  |\n| Wolof       | 39.1   | 45.8   | 50.7   | 54.7    | 67.2  |\n| isiXhosa    | 49.9   | 59.9   | 64.2   | 67.0    | 69.2  |\n| Yoruba      | 38.1   | 45.1   | 48.4   | 51.7    | 58.0  |\n| isiZulu     | 54.2   | 69.4   | 72.2   | 74.6    | 76.9  |\n| **Average**         | 48.2   | 55.9   | 59.2   | 62.3    | **70.4**  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636669531,
                "cdate": 1700636669531,
                "tmdate": 1700636669531,
                "mdate": 1700636669531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kl5cXjvxPn",
            "forum": "DayPQKXaQk",
            "replyto": "DayPQKXaQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on improving label projection for zero-shot cross-lingual transfer learning. They claim that existing label projection techniques cannot generate accurate translation and there for affect the downstream performance. They accordingly propose a constrained decoding to decide which positions to insert markers conditioned on a better translation template. Some heuristic tricks are presented to accelerate the search process. Experiments on NER and EAE show the potential of the proposed method.\n\n\n==== After response ====\nGiven that the authors promise that they will make the description clearer and add the translate-test results for EAE, I consider increasing the score."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing is clear.\n- The proposed method performs well on two tasks."
                },
                "weaknesses": {
                    "value": "- The author say that inserting markers would degrade the translation quality. However, although they provide a translation template to guide the model during translation, the proposed method still relies on markers, which is not completely solving this issue. \n- When searching, they mention the assumption:\n```\nIf we decode the translation template but conditioned on the marker sentence, at the position that needs to be inserted an opening marker, the model would give a high probability to this token, and thus assign a low probability to the token from the template.\n```\nThis assumption largely relies the translation model\u2019s ability to handle markers. Different translators may have different behaviors to handle the markers. I suggest the authors to report the results of different translators to show the stability of the proposed method.\n- The proposed method is based on some heuristic. It would be great if the authors can provide some theoretical bound to justify the heuristic.\n- It seems like that they follow the experimental setting of previous work (Chen et al. 2023a). However, they consider MasakhaNER 2.0 rather than WikiAnn (reported by Chen et al. 2023a) for NER without any explanation. Is it because that the proposed method works better for low-resource languages? I suggest to report the scores on WikiAnn as well for more comprehensive comparisons.\n- I am a little bit confused by the reason for not considering translation-test for EAE. The authors mention that the English gold trigger is needed but not feasible. However, this can be obtained by applying the proposed constraint label project from the target language to English. It would be interesting to study more about this."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792059010,
            "cdate": 1698792059010,
            "tmdate": 1700763948778,
            "mdate": 1700763948778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RzcltD25zc",
                "forum": "DayPQKXaQk",
                "replyto": "kl5cXjvxPn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. Please find below our response to each point of your comments and some updates in the paper based on your suggestions. \n\n> **The author say that inserting markers would degrade the translation quality. However, although they provide a translation template to guide the model during translation, the proposed method still relies on markers, which is not completely solving this issue.**\u201d\n\nThere seems to be some confusion here, our proposed method, Codec, does not include markers during the translation phase; so, the final translated text quality from Codec will be the same as any MT system, which is also the same as the word alignment-based label projection approaches. Only after this translation phase (i.e., after we generated this translation template), the markers are inserted through a second constrained decoding phase, in which the translation is constrained to be the same as the original translation template, while markers are being inserted. We have updated the Introduction of the paper to make this point clearer.\n\n>**This assumption largely relies the translation model\u2019s ability to handle markers... I suggest the authors to report the results of different translators to show the stability of the proposed method.**\u201d\n\nThank you for your suggestion. We have added experiments with two more different MT systems (M2M-100 and MBart), besides NLLB.\n\n* For the NER task, we explore using M2M-100 instead of NLLB for decoding. M2M-100 is chosen for NER as it supports 6 African languages in the MasakhaNER2.0 dataset. In addition, for this task, we also experiment with different model sizes of NLLB (i.e., 600M, 1.3B, and 3.3B) as the decoding module for Codec. See the table below (under \u201cCodec\u201d column) for results under the translate-test setting.  We have also included the results of these analyses in Appendix C of the paper. \n\n|     | FT_En | CSBS | CSBS | CSBS | CSBS | Codec | Codec | Codec | Codec |\n|--------------|-------|--------|--------|--------|---------|-----------|-----------|---------|--------|\n| Languages    |  | beam=2 | beam=4 | beam=8 | beam=16 | NLLB-600M | NLLB-1.3B | NLLB-3B | M2M-100 |\n| Bambara      | 37.1  | 32.6   | 35.9   | 38.5   | 42.4    | 55.6      | **56.0**      | 55.8    | -      |\n| Ewe          | 75.3  | 54.8   | 61.0     | 66.8   | 70.6    | 79.1      | **79.3**      | 77.7    | -      |\n| Fon          | 49.6  | 21.3   | 24.2   | 29.0   | 32.7    | 61.4      | **62.4**      | 60.2    | -      |\n| Hausa        | 71.7  | 54.4   | 66.4   | 68.7   | 70.1    | **73.7**      | 73.5      | 73.1    | 73.1   |\n| Igbo         | 59.3  | 46.2   | 52.3   | 56.5   | 60.3    | 72.8      | **72.9**      | 72.8    | 72.8   |\n| Kinyarwanda  | 66.4  | 61.5   | 70.7   | 72.7   | 74.7    | **78.0**      | 71.3      | 71.6    | -      |\n| Luganda      | 75.3  | 62.0     | 74.2   | 77.8   | 80.3    | 82.3      | **82.4**      | 81.7    | -      |\n| Luo          | 35.8  | 27.4   | 31.8   | 34.4   | 41.1    | 52.9      | **53.1**      | **53.1**    | -      |\n| Mossi        | 45.0  | 22.2   | 25.3   | 29.4   | 34.5    | 50.4      | **52.1**      | 50.7    | -      |\n| Chichewa     | 79.5  | 51.9   | 59.0   | 62.5   | 67.8    | 76.8      | 76.7      | **76.9**    | -      |\n| chiShona     | 35.2  | 68.0     | 74.0   | 75.3   | 76.2    | **78.4**      | 78.0      | 78.3    | -      |\n| Kiswahili    | 87.7  | 57.8   | 73.5   | 75.8   | 77.3    | 81.5      | 82.0      | 82.5    | **82.9**   |\n| Setswana     | 64.8  | 69.2   | 74.7   | 76.7   | 78.1    | 80.3      | 81.0      | **81.2**    | -      |\n| Akan/Twi     | 50.1  | 57.3   | 63.1   | 65.4   | 68.1    | 73.5      | 72.7      | **74.6**    | -      |\n| Wolof        | 44.2  | 39.1   | 45.8   | 50.7   | 54.7    | 67.2      | 66.2      | **67.5**    | -      |\n| isiXhosa     | 24.0    | 49.9   | 59.9   | 64.2   | 67.0    | 69.2      | **69.6**      | 69.1    | 67.9   |\n| Yoruba       | 36.0    | 38.1   | 45.1   | 48.4   | 51.7    | 58.0      | 56.5      | **58.7**    | 53.8   |\n| isiZulu      | 43.9  | 54.2   | 69.4   | 72.2   | 74.6    | **76.9**      | 76.6      | 76.6    | 75.7   |\n| **Average**          | 54.5  | 48.2   | 55.9   | 59.2   | 62.3    | **70.4**      | 70.1      | 70.1    |    -    |\n\n(\u2018-\u2019: these low-resource languages are not supported by the M2M-100 MT system)\n\n* For the EAE task, we have conducted additional experiments using MBart50-large (a competitive MT system for Chinese and Arabic) for both translation template generation and constrained decoding in Codec, as well as other comparison baselines. See table below for results. We have updated the paper (Appendix D.2) to include these results.\n\n Languages   | FT_En | Awes-align | EasyProject | Codec |\n|---|---|---|---|---|\n| Arabic  | 44.8  | 49.1  | 45.4  | **49.5** |\n| Chinese | 54.0  | 56.6 | **57.6** | 57.2 |\n| **Average**     | 49.4  | 52.9 | 51.5 | **53.4** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636189253,
                "cdate": 1700636189253,
                "tmdate": 1700636189253,
                "mdate": 1700636189253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4zHS8jAY9t",
                "forum": "DayPQKXaQk",
                "replyto": "RzcltD25zc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. Let me explain my thought about the first comment a little bit more.\n\nI understand that Codec does not include markers during the translation phase. However, what I want to say is that. For prior marker-based approaches, based on my understanding, their drawback can be summarized as \n\ninput with markers -> *output word probability* becomes bad (due to markers) -> translated sentence is bad\n\nI think adding markers makes the output word probability abnormal for certain degree and this is the key point why the translation quality degrades. For Codec, although it has a translated sentence being independent to the markers, the marker insertion still replies on *output word probability* (which is based on the input with markers) to decide which position we should insert the markers. That's why I say the proposed method still relies on markers. I agree that Codec uses a smarter way to generate markers, but it does not solve the problem that *output word probability* becomes inaccurate because of adding markers. Given this, I feel the paper is a bit over-claiming.\n\nAny discussion is welcome. Thanks!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644690198,
                "cdate": 1700644690198,
                "tmdate": 1700644690198,
                "mdate": 1700644690198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sjq4sV0hDC",
                "forum": "DayPQKXaQk",
                "replyto": "4zHS8jAY9t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Reviewer_kHv7"
                ],
                "content": {
                    "comment": {
                        "value": "Also, it would be great if you can report the translation-test results for EAE as well, since you emphasize the strength of Codec on translation-test in the introduction."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645048469,
                "cdate": 1700645048469,
                "tmdate": 1700645048469,
                "mdate": 1700645048469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S52f8rlk7q",
            "forum": "DayPQKXaQk",
            "replyto": "DayPQKXaQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_TbXV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6284/Reviewer_TbXV"
            ],
            "content": {
                "summary": {
                    "value": "=== AFTER THE RESPONSE ===\nI would like to thank the authors for taking the time to provide a very detailed response which clarified my main concerns and extra questions. I still think that the method could have been evaluated on a larger selection of tasks, but this doesn't invalidate the soundness of the proposed methodology, and I'm happy to increase my score\n===========================\n\nThis paper targets cross-lingual transfer for sequence labeling task, where the main problem in previous work has been projecting labeled spans from the source language to the correct spans in the target language, a problem sometimes referred to as the labeled span mismatch. Previous work typically solved this problem via two different approaches: 1) using external word aligners to do the label projection from source to target, or 2) inserting marker tokens directly into the input of a strong (N)MT system and basically conducting a standard translate-train approach (but with those extra markers). However, both prior approaches have issues as the former critically relies on the quality of the external word aligner, while the latter yields to degraded MT performance (due to the insertion of markers).\n\nThis paper basically proposes an extension to the latter approach, aiming to preserve the original quality of the MT system by bypassing the direct insertion of markers, and proposes a two-step approach where in Step 1) the original text can be translated (via translate-train or translate-test), and in Step 2) projection is added via constrained decoding, keeping the translation from Step 1 as a fixed template. The main technical contribution is then a computationally feasible technique for the constrained decoding, bypassing the need to conduct exhaustive brute force search while maintaining strong performance. The separation of the translation and marker insertion steps also allows the approach to be applied to the translate-test setting, and the results confirm the usefulness of the technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper clearly defines the problem, which is very concrete, and sets out to solve the problem following a clear line of thinking: from the conceptual level all the way to low-level technical execution aiming to improve the performance-versus-efficiency trade-off.\n- The idea of constrained decoding which fixes the entire sentence (instead of focusing only on lexical constraints during constraints) is quite new (at least to the best of my knowledge) and could have applications beyond cross-lingual transfer tasks discussed in this work.\n- The paper is well written and it is easy to link the main hypotheses to the concrete experiments and analyses. The core section of the paper on \"constrained decoding\" is also nicely described and easy to follow.\n- The results on the two tasks seem to support the main claims although the paper requires more experiments (see also under weaknesses)."
                },
                "weaknesses": {
                    "value": "- The main issue with the work is 1) the lack of recognition of other (recent and less recent) work on the same problem of cross-lingual label projection, which consequently leads to the 2) lack of more comprehensive comparisons to more baselines. The main baseline is definitely the EasyProject method and I agree with that, but I feel that not enough care has been provided to optimise the word alignment-based baselines which also shows reasonable performance, and is quite competitive in the EAE task.\n-- For instance, there has been some recent work on alignment correction for label projection (https://aclanthology.org/2023.law-1.24.pdf), and there are also other very relevant papers which should be cited and discussed (and ideally even compared against): https://aclanthology.org/2021.findings-acl.396.pdf or https://d-nb.info/1203127499/34, \n-- The number of evaluation tasks is slightly underwhelming and the paper should extend the scope of tasks to other sequence labeling tasks (e.g., slot labeling in dialogue, dependeny parsing or semantic role labeling) - NER with only 3 NE classes is a (relatively) simple task (from the perspective of its experimental setup), and the paper would have more impact with a wider experimental setup.\n-- I would also like to see a wider exploration of different MT systems and chosen encoder-decoder models and their impact on the performance of both alignment-based approaches as well as EasyProject and CODEC. For instance, how would larger variants of NLLB affect the performance? Would the scale of the NMT system recover for its deficiencies?"
                },
                "questions": {
                    "value": "A similar two-step idea, but which is not MT-based but encoder-based has been investigated here: https://arxiv.org/pdf/2305.13528.pdf (the idea there is slightly different and is based on classification - in the first step, the system just decides whether something should be a labeled span or not; in the second step, the actual label is added to each span detected as 'labeled span'. The paper should also discuss ideas like this one in related work and they seem highly relevant."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6284/Reviewer_TbXV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698912968063,
            "cdate": 1698912968063,
            "tmdate": 1700726556213,
            "mdate": 1700726556213,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S7NIpOku0c",
                "forum": "DayPQKXaQk",
                "replyto": "S52f8rlk7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. Please find below our response to each point of your comments and some updates in the paper based on your suggestions. \n\nIn response to your two main concerns:\n\n\n> **The main issue with the work is 1) the lack of recognition of others ... 2) lack of more comprehensive comparisons to more baselines. The main baseline is definitely the EasyProject method and I agree with that, but I feel that not enough care has been provided to optimize the word alignment-based baselines ...**\n\nFor 1), thank you for pointing this out. There is indeed a long history of research on cross-lingual label projection. We should cite as many papers as possible as we can fit in the page limit. We have updated our Related Work section to include all the papers you mentioned. \n\nFor 2), thank you for your suggestion. Our paper focuses on improving the marker-based approach for label projection (especially low-resource languages), thus EasyProject, the best marker-based approach is used as the most important baseline for comparison as you mentioned. In terms of comparison to the word alignment-based approaches, in our original submission, we followed the prior work [1] in choosing Awesome-align (one of the state-of-the-art word alignment models) with multilingual BERT as the baseline. In response to your comments, we have added two more word-alignment baselines that are optimized for African languages, namely Awesome-align with AfroXLMR-large [2], a model that is adapted to 17 African languages, and Awesome-align with Glot500 [3], a model which is pre-trained on 500+ languages including African languages. These are novel and very competitive word alignment-based baselines for the MasakhaNER2.0 dataset. \n\nThe table below compares the fine-tuning of English data baseline (FT_En), and Awesome-align with multilingual BERT (result in the main paper), Awesome-align with two aforementioned pre-trained models, and our proposed Codec in the translate-test setting, the results are reported on the test set of MasakhaNER2.0 dataset. To save space, in this table, we also include the experiments of using Codec with different MT models (NLLB and M2M-100) for decoding, which is to respond to your later comments. We have updated the paper (Appendix C.1) to include these results.\n\n| | FT_En | Awes-align | Awes-align | Awes-align | Codec | Codec|\n--- | --- | --- | --- | --- | ---|---|\n**Languages** || mBERT | AfroXLMR | Glot500 | NLLB | M2M-100\nBambara | 37.1 | 50.0 | 53.8 | 52.0 | **55.6** | -\nEwe | 75.3 | 72.5 | 78.0 | 76.8 | **79.1** | -\nFon | 49.6 | 62.8 | **66.0** | 61.2 | 61.4 | -\nHausa | 71.7 | 70.0 | **73.7** | 73.1 | **73.7** | 73.1\nIgbo | 59.3 | **77.2** | 71.7 | 71.2 | 72.8 | 72.8\nKinyarwanda | 66.4 | 64.9 | 68.2 | 68.1 | **78.0** | -\nLuganda | 75.3 | 82.4 | 82.1 | **82.5** | 82.3 | -\nLuo | 35.8 | 52.6 | 51.3 | 51.2 | **52.9** | -\nMossi | 45.0 | 48.4 | 49.1 | 48.7 | **50.4** | -\nChichewa | **79.5** | 78.0 | 76.7 | 77.3 | 76.8 | -\nchiShona | 35.2 | 67.0 | **78.6** | 75.6 | 78.4 | -\nKiswahili | **87.7** | 80.2 | 80.5 | 79.5 | 81.5 | 82.9\nSetswana | 64.8 | **81.4** | 80.7 | 80.5 | 80.3 | -\nAkan/Twi | 50.1 | 72.6 | 71.7 | **73.5** | **73.5** | -\nWolof | 44.2 | 58.1 | 59.0 | 57.1 | **67.2** | -\nisiXhosa | 24.0 | 52.7 | 67.6 | 63.9 | **69.2** | 67.9\nYoruba | 36.0 | 49.1 | 52.7 | 49.2 | **58.0** | 53.8\nisiZulu | 43.9 | 64.1 | 75.5 | 74.9 | **76.9** | 75.7\n**Average** | 54.5 | 65.8 | 68.7 | 67.6 | **70.4**| -\n\n(\u2018-\u2019 marks the low-resource languages that are not supported by the M2M MT system)\n\n[1] Chen et al. Frustratingly easy label projection for cross-lingual transfer. Findings ACL 2023.\n\n[2] Alabi et al. Adapting pre-trained language models to African languages via multilingual adaptive fine-tuning. COLING 2022.\n\n[3] ImaniGooghari et al. \u201cGlot500: Scaling multilingual corpora and language models to 500 languages\u201d. ACL 2023.\n\nIn response to your other comments:\n\n> **The number of evaluation tasks is slightly underwhelming and the paper should extend the scope of tasks ...**\n\nOur proposed work primarily targets low-resource languages and span-level NLP tasks, where human-annotated datasets (that are directly annotated on these languages, other than annotating the translated texts from existing English datasets) are in scarcity. MasakhaNER2.0, which covers 18 African languages we can experiment on, is one of the best benchmarks that is available. Besides the NER task, we also did experiments on the ACE 2005 dataset for the EAE task, which is a commonly used benchmark in the label projection literature. We politely argue that having conducted experiments in 20 languages and two tasks (with 3 different MT systems, and 4 competitive baselines -- see our updates) are sufficient to support our claimed contributions, while agreeing with you that it is always nice to include even more experiments if possible."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635337104,
                "cdate": 1700635337104,
                "tmdate": 1700635337104,
                "mdate": 1700635337104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u04Mn2tJ7k",
                "forum": "DayPQKXaQk",
                "replyto": "LeZc4NzmGp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6284/Reviewer_TbXV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6284/Reviewer_TbXV"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for providing a detailed response which helped me clarify some main concerns, and in turn this reflects in an increased recommendation score. While the paper might be more interesting for the NLP audience (given the set of chosen evaluations and the fact that it's based on MT systems), it might be interesting also for the wider ML/ICLR audience."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726664637,
                "cdate": 1700726664637,
                "tmdate": 1700726664637,
                "mdate": 1700726664637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]