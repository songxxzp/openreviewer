[
    {
        "title": "BDQL: Offline RL via Behavior Diffusion Q-learning without Policy Constraint"
    },
    {
        "review": {
            "id": "iyUXYKp3Zn",
            "forum": "gEdg9JvO8X",
            "replyto": "gEdg9JvO8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
            ],
            "content": {
                "summary": {
                    "value": "This paper discovers a good property of one-step RL (policy improvement with fixed behavior value function) equipped with an estimated diffusion-based behavior policy: due to accurate modeling of the behavior policy, one-step RL without any policy constraint can reach a strong enough performance before it suffers from OOD actions. To mitigate training fluctuation or collapse and stabilize the evaluation of learned policies, this paper introduces stochastic weight averaging of policy checkpoints. Experiments on MuJoCo tasks from D4RL demonstrate that the proposed BDQL-SWA provides good performance without policy constraints."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A good property of one-step RL with estimated diffusion behavior policies is discovered, which leads to a simple and clear algorithmic design.\n2. Detailed ablation studies illustrate the contribution of each component."
                },
                "weaknesses": {
                    "value": "1. The good property of two-stage training in BDQL is only validated in MuJoCo tasks, which are relatively simple. There is a lack of experiments on more complex datasets, such as AntMaze and Adroit domains from MuJoCo, or even heteroskedastic datasets [2]. Also, it is only validated empirically, without theoretical analysis.\n2. The performance of BDQL-SWA on MuJoCo tasks still trails behind modern offline RL methods.\n3. It is not the first time diffusion models have been utilized to model complex behavior distributions, which has been done by Chen et al. [1]. The authors should explicitly clarify this and sufficiently discuss the difference with them.\n\n[1] Offline reinforcement learning via high-fidelity generative behavior modeling\n\n[2] Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints"
                },
                "questions": {
                    "value": "1. Why 'The output of the diffusion policy is the deterministic action rather than the distribution of action'? If I understand correctly, the diffusion policy models a stochastic state-conditioned action distribution as Equation (3) rather than a single deterministic action. So Equation (8) should be $\\mathbb{E}_{s_t \\sim \\mathcal{D}, a \\sim \\pi_\\theta(s_t)} [Q_\\phi(s_t, a)]$? Does this one still follow the deterministic policy gradient (DPG) theorem?\n2. The name of 'theoretical performance' is inappropriate. Better names can be 'online validation performance,' 'best performance,' or 'ideal performance.'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7361/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697820082765,
            "cdate": 1697820082765,
            "tmdate": 1699636880777,
            "mdate": 1699636880777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AUYUCMmRRe",
                "forum": "gEdg9JvO8X",
                "replyto": "iyUXYKp3Zn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FiPY"
                    },
                    "comment": {
                        "value": "We express our gratitude for your comprehensive feedback. We've made careful considerations and addressed each of your concerns.\n\n**Q1: The good property of two-stage training in BDQL is only validated in MuJoCo tasks, which are relatively simple. There is a lack of experiments on more complex datasets, such as AntMaze and Adroit domains from MuJoCo, or even heteroskedastic datasets [2]. The performance of BDQL-SWA on MuJoCo tasks still trails behind modern offline RL methods.**\n\n**A1**: Although the results of BDQL are not outstanding, the experiments is achieved under the condition of \"**without constraint to overcome OOD**\". Combined with our newly added theoretical analysis, we comprehensively proves that BDQL is an algorithm capable of solving offline RL without the need for constraints.\n\n**Q2**: **Also, it is only validated empirically, without theoretical analysis.**\n\n**A2**: We have added some theory to better analyze the two-stage property of BDQL. In **Conclusion 1** of the new version of the paper, we find that the optimality of BDQL is based on the small enough distance (Total Varational distance between the estimated behavioral policy and the true behavior policy). Building on this conclusion, we divided the training stage based on the distance between the current training policy and the true behavior policy. We observed that the experiment perfectly matched this analysis.\n\n**Q3**: **It is not the first time diffusion models have been utilized to model complex behavior distributions, which has been done by Chen et al. [1]. The authors should explicitly clarify this and sufficiently discuss the difference with them.**\n\n**A3**: There is no apparent peculiarity in the way we use diffusion model. The crucial core is the unique role of diffusion policy. In the new version of the paper, we demonstrate through theoretical analysis (refer to **Conclusion 1** and the following description) that the theoretical framework of BDQL highly depends on the precise behavioral modeling provided by diffusion model.\n\n**Q4**: **Why 'The output of the diffusion policy is the deterministic action rather than the distribution of action'? If I understand correctly, the diffusion policy models a stochastic state-conditioned action distribution as Equation (3) rather than a single deterministic action. So Equation (8) should be $\\mathbb{E}*{s_t \\sim \\mathcal{D}, a \\sim \\pi*\\theta(s_t)} [Q_\\phi(s_t, a)]$? Does this one still follow the deterministic policy gradient (DPG) theorem?**\n\n**A4**: From the details of the diffusion model, this action is sampled from a distribution. However, from an external perspective, the diffusion model takes in a state and outputs an action. This aligns with the definition of a deterministic policy and can be updated using DPG. Similar approaches are also adopted by Diffusion Q-learning [1].\n\n[1] Wang, Z., Hunt, J. J., & Zhou, M. (2022). Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193.\n\n**Q5**: **The name of 'theoretical performance' is inappropriate. Better names can be 'online validation performance,' 'best performance,' or 'ideal performance.'**\n\n**A5**: We have used \"best result\" in our new paper.\n\nShould there be any misinterpretation on our part regarding your inquiries or if we have not sufficiently resolved your concerns, please bring it to our attention at your earliest convenience. We look forward to any additional advice or guidance you can provide."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573883471,
                "cdate": 1700573883471,
                "tmdate": 1700573883471,
                "mdate": 1700573883471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TDZY1UHIAB",
                "forum": "gEdg9JvO8X",
                "replyto": "AUYUCMmRRe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response and additional theoretical analysis!\n\nHowever, I am not quite convinced by the theoretical analysis due to some unclear details. \n\nQ1: In (9) and (10), is $Q_{\\pi_b}$ a ground-truth behavior Q function or just an estimated one? If it is the former, it is not the case in practical offline RL, since the training instability indeed comes from OOD action queries on estimated Q function. If it is the latter, why is there not any constraint in this offline objective (10) to avoid overfitting this estimated Q by learning OOD actions?\n\nQ2: When optimizing the objective (10), $\\pi_b$ and $Q_{\\pi_b}$ are both updated, right? But when optimizing the objective (9), $Q_{\\pi_b}$ remains pre-defined, right? Thus,  $Q_{\\pi_b}$ in the two are not the same one, do the derivation of Theorem 2 consider this? I highly recommend a more rigorous formulation of Section 4, elaborating on the detailed parameterization of each function (policy and Q),  and the parameters being optimized by each objective.\n\nQ3: This question is related to my weakness 1. There is no theoretical guarantee that a diffusion policy can approximate closely enough to the behavior policy, i.e., making $D_{T V}\\left(\\pi_\\theta \\| \\pi_{\\mathrm{b}}\\right)$ small enough. In my opinion, complex datasets, listed in my original review, can pose a significant challenge to this. Thus, **my major concern** is that, the most important contribution of this paper, namely it is possible to do policy improvement without any policy constraint, may not generalize to more difficult offline RL scenarios beyond D4RL-Mujoco.\n\nPlease correct me if there is any misunderstanding. Looking forward to the authors' response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633563653,
                "cdate": 1700633563653,
                "tmdate": 1700633563653,
                "mdate": 1700633563653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TK0e0Z8wxS",
                "forum": "gEdg9JvO8X",
                "replyto": "40K7LZgDuZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Reviewer_FiPY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for the response.\n\nSince I still cannot accurately grasp the theoretical analysis (for example, if $Q_b$ is fixed, what is optimized for objective (10), and what is the difference between (9) and (10)?), and there is no clear evidence addressing **my major concern** above,  I decide to keep my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725904916,
                "cdate": 1700725904916,
                "tmdate": 1700725904916,
                "mdate": 1700725904916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V3RUgmwJsW",
            "forum": "gEdg9JvO8X",
            "replyto": "gEdg9JvO8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7361/Reviewer_BEcH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7361/Reviewer_BEcH"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studies the offline RL problem without policy constraint by utilizing the diffusion model as the tool. The authors further suggests the Stochastic Weight Average (SWA) to mitigate the training fluctuation. The superiority of the method is validated from D4RL tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction of diffusion model is a good trial for the offline RL community.  Although some papers have combined diffusion model in offline RL, this paper also devotes some ideas in this area. \nThe presentation is clear, and the paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. First, from the experimental studies, the results of BDQL is not convincing. The performance of BDQL is close to other competitors and BDQL-SWA is even inferior to other competitors. \n2. From the authors' explanation, the Theoretical performance refers to the on-policy scenario, and the Practical performance refers to the offline scenarios. In offline scenario, the performance of BDQL-SWA is still not good enough. \n3. In SWA, the author mentions ``SWA averages the multiple checkpoints during the optimization''. Do you mean averaging the parameters of the models at different training time? So it is similar to the Target Actor/Critic network trick in most offline RL methods?"
                },
                "questions": {
                    "value": "1. Some offline-RL baselines, such as BEAR are missed in experiment, and the recent popular SPOT [1] method is not considered in experiments as well. It is suggested to consider the baselines in offline-RL methods.\n2. The author claims that the BDQL has sufficiently solves the offline RL problem (OOD issue), and the OOD issue only causes fluctuation in training. The authors tries to illustrate this point with some ablation studies. However, this claim seems weak. It is suggested to add some theoretical analysis supporting this claim.\n3. Actually, the diffusion model is time-costly in model inference. Will this issue also occur in BDQL? Some ablation studies on computation issues are suggested. \n4. The review will consider increase the rating when some concerns are replied and solved.\n\n[1] Supported Policy Optimization for Offline Reinforcement Learning. https://arxiv.org/abs/2202.06239"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7361/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7361/Reviewer_BEcH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698491133070,
            "cdate": 1698491133070,
            "tmdate": 1699636880597,
            "mdate": 1699636880597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "caUpEsoKOk",
                "forum": "gEdg9JvO8X",
                "replyto": "V3RUgmwJsW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BEcH"
                    },
                    "comment": {
                        "value": "Your detailed comments are greatly appreciated. We have carefully addressed each point, and your insights have contributed significantly to improving our work.\n\n**Q1**: **First, from the experimental studies, the results of BDQL is not convincing. The performance of BDQL is close to other competitors and BDQL-SWA is even inferior to other competitors**.\n\n**Q2**: **From the authors' explanation, the Theoretical performance refers to the on-policy scenario, and the Practical performance refers to the offline scenarios. In offline scenario, the performance of BDQL-SWA is still not good enough.**\n\n**A1&A2**: We address these two questions together. BDQL and its variants achieve results comparable to the strongest baseline (ensemble-based methods). While this may not be a  breakthrough,  it is important to note that this result is achieved **without introducing constraints and with OOD preserved**. This experiments, along with our newly added theoretical insights, demonstrate that offline RL may not necessarily require constraints to solve.\n\n**Q3**: **In SWA, the author mentions ``SWA averages the multiple checkpoints during the optimization''. Do you mean averaging the parameters of the models at different training time? So it is similar to the Target Actor/Critic network trick in most offline RL methods?**\n\n**A3**: Indeed, SWA averages different checkpoints during training, but it differs from the Target Actor/Critic network trick. The key distinction is that the Target Actor/Critic network actively participates in training, whereas SWA does not. SWA averages different checkpoints during training to stabilize fluctuations and avoid collapse.\n\n**Q4**: **Some offline-RL baselines, such as BEAR are missed in experiment, and the recent popular SPOT [1] method is not considered in experiments as well. It is suggested to consider the baselines in offline-RL methods**\n\n**A4**: The descriptions of the support constraint value function in BEAR and SPOT inspired our proof, and we have cited both of these papers. The instability of BEAR's performance is a major reason why we did not consider it in our baseline. The strength of the SPOT algorithm lies in its offline-to-online fine-tuning while the performance is comparable to TD3+BC in addressing offline RL (SPOT: 773/9 = 85.89, TD3+BC: 84.83). Meanwhile, our baseline also includes ensemble-based methods, which are among the strongest algorithms.\n\n**Q5**: **The author claims that the BDQL has sufficiently solves the offline RL problem (OOD issue), and the OOD issue only causes fluctuation in training. The authors tries to illustrate this point with some ablation studies. However, this claim seems weak. It is suggested to add some theoretical analysis supporting this claim.**\n\n**A5**: In the new version of the paper, we analyze the optimality of BDQL and its tenable conditions (please see Section 4). We find that when the estimated behavioral policy is close enough to the true behavioral policy, BDQL can converge to the optimal policy. Inspired by this conclusion, we divide the training process into two stages based on the distance between the current training policy and the true behavior policy. This leads to improved analytical results and better demonstrationl in our experiments (Figure 4).\n\n**Q6**: **Actually, the diffusion model is time-costly in model inference. Will this issue also occur in BDQL? Some ablation studies on computation issues are suggested.**\n\n**A6**: we compare the training time between BDQL and BQL. The training time of BDQL (1492 min) is approximately 2.5 times that of BQL (608 min). In future work, we will consider further optimizations in terms of algorithm and implementation to reduce the time overhead.\n\nIn case our understanding of your questions is not accurate or if your concerns remain unaddressed, we encourage you to inform us promptly. We are keenly awaiting any additional suggestions or guidance you might offer."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573418822,
                "cdate": 1700573418822,
                "tmdate": 1700573418822,
                "mdate": 1700573418822,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VDOfVrhsZ7",
                "forum": "gEdg9JvO8X",
                "replyto": "V3RUgmwJsW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to know if our response has addressed your concerns. Additionally, we are eager to hear your feedback on the newly added proof section. We look forward to your response and any further suggestions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727665160,
                "cdate": 1700727665160,
                "tmdate": 1700727665160,
                "mdate": 1700727665160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M2KMHinQA4",
            "forum": "gEdg9JvO8X",
            "replyto": "gEdg9JvO8X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7361/Reviewer_Chng"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7361/Reviewer_Chng"
            ],
            "content": {
                "summary": {
                    "value": "The current paper introduces a new offline RL algorithm using diffusion models for policy, named BDQL. The algorithm has three components: 1. it performs behavior cloning on the offline dataset, 2. critic learning by SARSA on the offline dataset, 3. policy improvement by deterministic policy gradient with the critic trained by SARSA, and stabilizing training with stochastic weight averaging. In the experiment, the paper compares BDQL with several baselines on the d4rl benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper lists important preliminaries so even readers who are not familiar with diffusion models can understand the context. \n\n2. The ablation study is throughout. \n\n3. The experiment compares with a variety of baselines."
                },
                "weaknesses": {
                    "value": "1. The significance of section 2.2 is unclear, since both methods are not used in the current paper. \n\n2. The SARSA update requires the assumption that the offline data is coming from trajectories, and the data collecting policy is a single stationary policy. \n\n3. The choice of using SARSA to train the critic is actually confusing. According to the proposed algorithm, in the statistically asymptotical case, optimization is done perfectly, and offline data has good coverage, the critic will converge to the Q-function of the behavior policy, which might not be a strong policy, and the diffusion policy is just the argmax policy according to the Q-function of the behavior policy, which might be better than the behavior policy, but the performance is still not guaranteed. So it is hard to see why even in the most ideal setting this algorithm would return a strong policy.\n\n4. The argument on regularization for ood from the current algorithm is not very convincing. It seems like an alternative way of regulizing with behavior cloning with a diminishing regularization coefficient. Using this perspective, one can also suspect if this varying objective is causing the instability in the practical performance. \n\n5. No concrete algorithm box is provided. \n\n6. The usage of the term \"theoretical performance\" in the experiment section is confusing."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722174786,
            "cdate": 1698722174786,
            "tmdate": 1699636880459,
            "mdate": 1699636880459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9zabiqFynA",
                "forum": "gEdg9JvO8X",
                "replyto": "M2KMHinQA4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Chng"
                    },
                    "comment": {
                        "value": "Thank you sincerely for your thorough and insightful comments. We have taken each question seriously and provided detailed responses.\n\n**Q1**: **The significance of section 2.2 is unclear, since both methods are not used in the current paper.**\n\n**A1**: In Section 2.2, we introduce on-policy and off-policy learning. The former is highly relevant to our algorithm since BDQL can be viewed as a form of on-policy learning in the context of offline RL. When introducing off-policy learning, we also highlight the challenges in offline RL. This section serves as essential groundwork for the subsequent discussions.\n\n**Q2**: **The SARSA update requires the assumption that the offline data is coming from trajectories, and the data collecting policy is a single stationary policy.**\n\n**A2**: SARSA can also be updated from transitions, not only trajectories. Offline datasets can be assumed to be collected by the behavior policy, which allows update by SARSA. This assumption has been adopted by on-policy style offline algorithms like Onestep RL [1] and BPPO [2].\n\n[1] Brandfonbrener, D., Whitney, W., Ranganath, R., & Bruna, J. (2021). Offline rl without off-policy evaluation. Advances in neural information processing systems, 34, 4933-4946.\n\n[2] Zhuang, Z., Kun, L. E. I., Liu, J., Wang, D., & Guo, Y. (2022, September). Behavior Proximal Policy Optimization. In The Eleventh International Conference on Learning Representations.\n\n**Q3**: **The choice of using SARSA to train the critic is actually confusing. According to the proposed algorithm, in the statistically asymptotical case, optimization is done perfectly, and offline data has good coverage, the critic will converge to the Q-function of the behavior policy, which might not be a strong policy, and the diffusion policy is just the argmax policy according to the Q-function of the behavior policy, which might be better than the behavior policy, but the performance is still not guaranteed. So it is hard to see why even in the most ideal setting this algorithm would return a strong policy.**\n\n**A3**: One of our innovations is updating the policy using only the critic **without introducing constraints**. Whether the learned policy is optimal or not can be addressed by our added proofs. In essence, when the policy estimated by BC is sufficiently close to the true behavior policy, the optimality of the policy is guaranteed.\n\n**Q4**: **The argument on regularization for ood from the current algorithm is not very convincing. It seems like an alternative way of regularizing with behavior cloning with a diminishing regularization coefficient. Using this perspective, one can also suspect if this varying objective is causing the instability in the practical performance.**\n\n**A4**: We may not fully understand this question and are unsure if the reviewer is asking if SWA is an alternative OOD regularization method. If so, our response is as follows; SWA does not participate in training and does not alter the original optimization objectives. This fundamentally distinguishes the role of SWA from other constraints. The training of the original BDQL is unstable, and SWA is used to eliminate this instability\n\n**Q5**: **No concrete algorithm box is provided.**\n\n**A5**: We have added the algorithm box in appendix and the Figure 1 can also clearly express the algorithm logic.\n\n**Q6**: **The usage of the term \"theoretical performance\" in the experiment section is confusing.**\n\n**A6**: We have replaced the \u201ctheoretical performance\u201d with \u201cbest result\u201d, representing the best result obtained from online evaluation.\n\nIf we have misunderstood your queries or failed to address your concerns adequately, please do let us know promptly. We eagerly anticipate any further suggestions or guidance you may provide."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573023592,
                "cdate": 1700573023592,
                "tmdate": 1700573023592,
                "mdate": 1700573023592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M7VX8I0qlY",
                "forum": "gEdg9JvO8X",
                "replyto": "9zabiqFynA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7361/Reviewer_Chng"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7361/Reviewer_Chng"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> A3: One of our innovations is updating the policy using only the critic without introducing constraints. Whether the learned policy is optimal or not can be addressed by our added proofs. In essence, when the policy estimated by BC is sufficiently close to the true behavior policy, the optimality of the policy is guaranteed.\n\nThe contribution of performing offline RL without introducing constraints **with additional assumptions** may not be strong. For example, one can simply run fitted q iteration (FQI) when the data coverage has good coverage. \n\nThe added theory part again seems to only provide a guarantee between the learned policy and the **behavior policy** (which again could be just a random policy), so it still seems unclear why this bound shows the optimality of the learned policy. \n\nI will keep my score at this point."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669684954,
                "cdate": 1700669684954,
                "tmdate": 1700669684954,
                "mdate": 1700669684954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]