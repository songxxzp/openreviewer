[
    {
        "title": "Learning energy-based models by self-normalising the likelihood"
    },
    {
        "review": {
            "id": "5KJg0MIfmE",
            "forum": "zrxlSviRqC",
            "replyto": "zrxlSviRqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5359/Reviewer_qYkm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5359/Reviewer_qYkm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the learning of the energy-based model (EBM). The typical MLE learning requires MCMC sampling of EBM to obtain the gradient of the normalizing function, which can be challenging in practice due to its instability and computational cost. The proposed self-normalized log-likelihood (SNL) method instead parameterizes the normalizing function via a linear variation formulation (Eq.8 in the paper), which does not involve the MCMC sampling but can train the EBM with the SNL estimate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The energy-based model serves as a foundational generative model, and the proposed learning algorithm is thus well-motivated.\n2. The paper is in general well-presented, especially the theoretical parts regarding the understanding of the proposed method.\n3. The proposed method seems to be flexible as the author extends it to multiple settings, such as prior of VAE and regression tasks (in a supervised scenario)."
                },
                "weaknesses": {
                    "value": "1. This paper has a well-motivated idea and contains comprehensive theoretical derivation for understanding the key idea. However, as mentioned by the author, the NCE method is related, it would be nice to have a deeper theoretical connection and comparison with the NCE method. For now, the major comparison is shown by empirical experiments. \n2. Many other prior works can be applied to some more challenging real data, such as CIFAR-10, CelebA-64, or even the high-resolution (CelebA-HQ-256), so what limited this learning algorithm for such dataset?\n3. As a novel learning method, it would be nice to have a practical learning algorithm to simplify and illustrate the main idea."
                },
                "questions": {
                    "value": "(1) How do we understand the unbiased estimate of SNL (Eq.15) while the last term is based on Jensen's (Eq.6)?\n\n(2) Some typos can be fixed (e.g., Eq.17 \\nabla_\\theta missing)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5359/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5359/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5359/Reviewer_qYkm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5359/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697907943450,
            "cdate": 1697907943450,
            "tmdate": 1699636540397,
            "mdate": 1699636540397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRdmkt5lbN",
                "forum": "zrxlSviRqC",
                "replyto": "5KJg0MIfmE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5359/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5359/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank reviewer qYkm for their careful analysis of our paper and suggestions for improvement.\n\n\n*1 - This paper has a well-motivated idea and contains comprehensive theoretical derivation for understanding the key idea. However, as mentioned by the author, the NCE method is related, it would be nice to have a deeper theoretical connection and comparison with the NCE method. For now, the major comparison is shown by empirical experiments.*\n\n\nThank you for suggesting an investigation of the relationship between the NCE and SNL objectives. We found that the SNL loss resembles one of the generalisations of NCE proposed in a theoretical paper on NCE [2]. We will discuss this paper in the revised version.\n\n\n\n*2 - Many other prior works can be applied to some more challenging real data, such as CIFAR-10, CelebA-64, or even the high-resolution (CelebA-HQ-256), so what limited this learning algorithm for such dataset?*\n\nThe goal of our paper is to introduce a new method for training Energy-Based Models. As it was done for other methods in the field, scaling such methods requires tuning and tricks that are a completely new avenue of research. For image modelling, we will require more complicated tuning and proposal than just a simple Gaussian maybe using a flow as a proposal. For instance [1] focuses on MCMC-based methods but scales it using a replay buffer. \n\n\n\n*3 - As a novel learning method, it would be nice to have a practical learning algorithm to simplify and illustrate the main idea.*\n\nWe added an algorithm description for our method in appendix E. Thank you for the suggestions.\n\n\n[1] Du, Y. and Mordatch, I. \"Implicit Generation and Modeling with Energy Based Models \"Advances in Neural Information Processing Systems 32 (2019)\n\n[2] Pihlaja et al. (2010), A family of computationally efficient and simple estimators for unnormalized statistical models, Uncertainty in Artificial Intelligence"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5359/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408356649,
                "cdate": 1700408356649,
                "tmdate": 1700408396189,
                "mdate": 1700408396189,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "guZ9Zev8fH",
                "forum": "zrxlSviRqC",
                "replyto": "SRdmkt5lbN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5359/Reviewer_qYkm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5359/Reviewer_qYkm"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their responses, but it seems that my concern is not well addressed in detail, such as the connection to the NCE method. \n\nI understand real image datasets can be challenging, but I honestly don't think this is a \"complete\" new avenue of research. [1] propose a \"replay buffer\" as an improvement for the performance and many other works (e.g., [a] using only short-run Langevin dynamics) still work without the replay buffer. Therefore, I do think applying the proposed method to some standard benchmark (e.g., CIFAR-10 32x32) can be better for illustrating the potential of the method and motivating future research of such an active field.\n\n\n\n\n[a] Nijkamp, Erik, et al. \"Learning non-convergent non-persistent short-run MCMC toward energy-based model.\" Advances in Neural Information Processing Systems 32 (2019)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5359/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666327726,
                "cdate": 1700666327726,
                "tmdate": 1700666327726,
                "mdate": 1700666327726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nToYmsL0jC",
            "forum": "zrxlSviRqC",
            "replyto": "zrxlSviRqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5359/Reviewer_3VY8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5359/Reviewer_3VY8"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel objective function for estimating Energy-Based Models (EBM), offering a promising alternative to the costly Markov Chain Monte Carlo (MCMC) sampling method. Their proposed objective function not only eliminates the need for MCMC but also provides an estimate of the log-normalizing constant as a byproduct. The paper showcases the effectiveness of this new approach by demonstrating its ability to recover the Maximum Likelihood (ML) estimate and its favorable performance within the exponential family of distributions. To validate their method, the authors conduct comprehensive empirical tests on both low-dimensional synthetic and real-world datasets, illustrating its efficacy and its superiority over Noise Contrastive Estimation (NCE). Additionally, the authors extend the application of their method to Variational Autoencoders (VAEs) with energy-based priors, broadening the scope of their contribution in the field of generative models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written\n- The idea of doing a variational approximation of the logarithm is elegant\n- The application to VAEs with energy-based priors is interesting"
                },
                "weaknesses": {
                    "value": "- The method seems very sensitive to the curse of dimensionality because of its IS component. This scaling issue is not investigated.\n- The proposed method is not compared against MCMC-based methods.\n- The sensitivity to the choice of proposal should be critical but it is only investigated in low-dimensional cases.\n- Most experiments are toy experiments or in a very low dimension."
                },
                "questions": {
                    "value": "1. Can you provide more real-world experiments ? For instance in generative modeling (without the VAE component) or out-of-distribution detection.\n\n2. As mentioned in the weaknesses, I would expect your method to be very sensitive to the design of $q$.\n\n(a) Did you run a sensitivity comparison with [1], [2] (which develop similar ideas) or NCE in higher dimensional settings (compared to Sec 4.2) ?\n\n(b) Did you try to learn $q$ as done in [3] for NCE ?\n\n3. In [4], the authors give a very similar result as your theorem 3.1 but for NCE. Is there more theoretical comparisons to be drawn against NCE ?\n\n4. As mentioned in the weaknesses, I think it would be nice to compare SNL against MCMC-based methods (at least Langevin based) with apple-to-apple computational budgets.\n\n[1] Will Grathwohl, Jacob Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky, & David Duvenaud. (2021). No MCMC for me: Amortized sampling for fast and stable training of energy-based models.\n\n[2] Hanjun Dai, Rishabh Singh, Bo Dai, Charles Sutton, & Dale Schuurmans. (2020). Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration.\n\n[3] Ruiqi Gao, Erik N\u0133kamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, & Ying Nian Wu. (2020). Flow Contrastive Estimation of Energy-Based Models.\n\n[4] Bingbin Liu, Elan Rosenfeld, Pradeep Ravikumar, & Andrej Risteski. (2021). Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5359/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698506918655,
            "cdate": 1698506918655,
            "tmdate": 1699636540270,
            "mdate": 1699636540270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KoK8vciwgO",
                "forum": "zrxlSviRqC",
                "replyto": "nToYmsL0jC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5359/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5359/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer 3VY8 for their study of our paper and the recommendation for future iterations of the paper.\n\n*1 - Can you provide more real-world experiments? For instance in generative modeling (without the VAE component) or out-of-distribution detection.*\n*2 - As mentioned in the weaknesses, I would expect your method to be very sensitive to the design of $q$.\n(a) Did you run a sensitivity comparison with [1], [2] (which develop similar ideas) or NCE in higher dimensional settings (compared to Sec 4.2)?\n(b) Did you try to learn as done in [3] for NCE ?*\n\nAnswer for *1* and *2*: We agree that the method will be sensitive to the proposal, especially when scaling it for image modelling. However, we feel that the work required for finding and tuning a correct proposal is a new avenue of work in itself. Indeed, all these papers are mostly focused on scaling existing methods by finding and tuning a proposal (noisy neural proposal in [1] using $l_{IS}$ in our paper or flow proposal in [3] for NCE). Note that we did compare $l_{IS}$ and $l_{SNL}$ with a Gaussian for the 2D toy experiment and the image regression experiments. In such cases, training the EBM was less stable than with our method or NCE.\n\n\n\n*3 - In [4], the authors give a very similar result as your theorem 3.1 but for NCE. Are there more theoretical comparisons to be drawn against NCE ?*\n\n\nThe paper you are mentioning also provides more theoretical insights into the loss landscape for NCE. Conducting such a study for SNL would require substantial work but is an interesting new avenue of work but we will integrate it to a revised version of the paper. Thank you for suggesting an investigation of the relationship between the NCE and SNL objectives. We do not know if there is a direct relationship between our loss and the NCE one. However, we found that the SNL loss resembles one of the generalisations of NCE proposed in a theoretical paper on NCE [5]. We will discuss this paper in the revised version.\n\n*4 - As mentioned in the weaknesses, I think it would be nice to compare SNL against MCMC-based methods (at least Langevin based) with apple-to-apple computational budgets.*\n\nDue to the lack of time required to implement and tune MCMC-based methods, we will provide a comparison of SNL with MCMC based method using similar computational budgets (ie number of call of a neural networks and time) upon acceptance or for a further iteration of the paper.\n\n[1] Will Grathwohl, Jacob Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky, & David Duvenaud. (2021). No MCMC for me: Amortized sampling for fast and stable training of energy-based models.\n\n[2] Hanjun Dai, Rishabh Singh, Bo Dai, Charles Sutton, & Dale Schuurmans. (2020). Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration.\n\n[3] Ruiqi Gao, Erik N\u0133kamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, & Ying Nian Wu. (2020). Flow Contrastive Estimation of Energy-Based Models.\n\n[4] Bingbin Liu, Elan Rosenfeld, Pradeep Ravikumar, & Andrej Risteski. (2021). Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation.\n\n[5] Pihlaja et al. (2010), A family of computationally efficient and simple estimators for unnormalized statistical models, Uncertainty in Artificial Intelligence"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5359/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408297632,
                "cdate": 1700408297632,
                "tmdate": 1700408297632,
                "mdate": 1700408297632,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "og13umrAL0",
            "forum": "zrxlSviRqC",
            "replyto": "zrxlSviRqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5359/Reviewer_CuDM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5359/Reviewer_CuDM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to change the loss function of energy based model based on a variational formulation. The proposed algorithm estimate an extra parameter b together with the energy function $E_\\theta(x)$. The author use importance sampling with base distribution $q(x)$ to estimate the normalizing term of the energy function. They carry out the experiments on toy example datasets and energy regression task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think this paper is well written and the idea is easy to follow. The reformulation trick, though simple, is interesting to me."
                },
                "weaknesses": {
                    "value": "However, I am not fully convinced by whether the proposed algorithm really works better in practice than original EBM training when modeling complex distributions. This lies in several aspects:\n\n1. From my own experience, the most challenging part when training the EBM is to get valid samples from the current fitted distribution to estimate the (gradient of) normalizing constant. Previous works try to solve this problem with different sampling techniques. While this work proposes a linear lower bound, it still needs to estimate the normalizing constant with Monte Carlo based method. Thus, it might not really alleviate the training difficulties.\n\n2. To do this Monte Carlo estimation, the work employs important sampling using a base distribution $q(x)$ and $q(x)$ are simple distributions like Gaussian. I suspect this algorithm works because the target distributions tested in this work are very simple, either toy distribution or conditional distribution $p_\\theta(y|X)$ where y is low dimensional. If we are modeling model complex distribution like unconditional distribution $p(x)$ on high dimensional data like images, then we still need Monte Carlo based methods and the previous diffculties are still there.\n\n3. The proposed algorithm introduces a variational parameter b, and it requires to update b together with the energy function iteratively. Then similar to the VAE case, whether there can be a mismatch between the estimate of b and the energy function $E_\\theta(X)$.  (Not sure whether the $\\exp^{-b}$ term will make the training more unstable if b is not well optimized.) Or in other words, how diffcult is it to design the schedule of updating b and energy function to make this algorithm work. \n\n4. As also mentioned in 2, the modeled distributions in the experiments are too simple to be convincing to me. The modeled experiments are either unconditional distribution on toy data or with image input but only models the conditional distribution on some low dimensional label. The VAE experiment in 5.3 models binary MNIST (which is also not very complex). And with the help of encoder and decoder, the latent space might be more simple. (Beside, what if we train the model VAE-EBM not with $l_{snl}$ but with plain MLE loss? There seems to be included as a baseline in Table 5.) I think in order to make the proposed algorithm more convincing, the authors need to demonstrate better results than pure MLE loss on more complex distributions like real image (face or cifar or SVHN).\n\n5. The review for EBM study seems to be insuffcient, may consider the following works:\n\n[1] Improved contrastive divergence training of energy-based models.\n\n[2] Learning energy-based models by diffusion recovery likelihood.\n\n[3] A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model."
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5359/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814566030,
            "cdate": 1698814566030,
            "tmdate": 1699636540115,
            "mdate": 1699636540115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8IvTzjTwvm",
                "forum": "zrxlSviRqC",
                "replyto": "og13umrAL0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5359/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5359/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer CuDM for their careful review of our paper and address their concern below.\n\n*1 - From my own experience, the most challenging part when training the EBM is to get valid samples from the current fitted distribution to estimate the (gradient of) normalizing constant. [...]*\n\nOur experience matches the one of reviewer CuDM, where the difficulty is to get good samples from the current fitted distribution. In MCMC-based methods training the EBM requires long chains or tricks (for instance, keeping a buffer in [1]) to avoid biased training. However, our method does not require sampling from the model to estimate gradients and that is why we believe this method is more flexible and avoids biases training loss.\n\n*2 - To do this Monte Carlo estimation, the work employs important sampling using a base distribution and simple distributions like Gaussian. I suspect this algorithm works because the target distributions tested in this work are very simple,[...] .*\n\nThough we agree that having a simple distribution like a Gaussian might not scale to more complicated high-dimensional data like images, it does not necessarily mean the need to resort to MCMC-based methods again. Indeed, having a more complex proposal distribution like a flow as a proposal or base distribution could be a way to handle this issue.\n\n\n *3 - The proposed algorithm introduces a variational parameter b, which requires updating b and the energy function iteratively. Then similar to the VAE case, whether there can be a mismatch between the estimate of b and the energy function. (Not sure whether the term will make the training more unstable if b is not well optimized.) Or in other words, how difficult is it to design the schedule of updating b and energy function to make this algorithm work?*\n \nIt is true that the quality of the gradient estimate depends on how close $b$ is to the normalization constant. This is clearly seen in equation (17) of the paper where we rewrite SNL gradients as the likelihood gradients with some negligible terms if $b$ verifies the aforementioned condition :\n$$\\begin{aligned}\n    \\nabla_{\\theta} \\ell_{\\text{snl}}(\\theta,b) &=  - \\frac{1}{n} \\sum_{i=1}^{n}  \\nabla_{\\theta} E_{\\theta}(x) - e^{-b+\\log{Z_{\\theta}}} \\nabla \\log{Z_{\\theta}} \\\\\n    & = \\nabla_{\\theta} \\ell_{\\theta} + \\nabla_{\\theta} \\log{Z_{\\theta}} (1-e^{-b+\\log{Z_{\\theta}}}). \n\\end{aligned}\n$$\nIn practice, we train both $\\theta$ and $b$ within the same iteration and very quickly during training, $b$ gets very close to $\\log{Z_{\\theta}}$ thus this did not require more tuning from our side. Note that we treat the extra parameter $b$ as the bias of the last layer of neural network and as such as any other parameter weight of the neural network, optimized using Adam.\n\n\n\n*4 - As also mentioned in 2, the modelled distributions in the experiments are too simple to be convincing to me. The modelled experiments are either unconditional distribution on toy data or with image input but only model the conditional distribution on some low dimensional label. The VAE experiment in 5.3 models binary MNIST (which is also not very complex).[...]*\n\nThough we agree that the experiments are low-dimensional, we disagree that they are not convincing enough. Indeed, to the best of our knowledge, this is the first time an EBM is used for density estimation (and actually provides an upper and lower bound of the likelihood) on the UCI datasets. These datasets usually require complicated networks (autoregressive flows for instance), whereas we reach similar results with much simpler networks and a simple Gaussian proposal. Similarly, we show state-of-the-art performance on EBMs for image regression which is in itself useful [3] for visual tracking or pose-estimation in computer vision. \nFinally, the point of the paper is to introduce a new method for training general EBMs, without a specific focus on high-dimensional data. As it was done for other methods in the field, scaling such methods requires tuning and tricks that are a completely new avenue of research. \n\n*4 bis - I think in order to make the proposed algorithm more convincing, the authors need to demonstrate better results than pure MLE loss on more complex distributions like real images (face or cifar or SVHN).*\n\nCould you specify what you call pure MLE loss? Is that just the standard VAE? \n\n\n\n\n*5-* We added the recommended paper to the references, we want to thank the reviewer for their valuable suggestions.\n \n\n \n\n[1] Du, Y. and Mordatch, I. \"Implicit Generation and Modeling with Energy Based Models \"Advances in Neural Information Processing Systems 32 (2019)\n\n[2] Will Grathwohl, et al. \"No MCMC for me: Amortized samplers for fast and stable training of energy-based models.\" (2021).\n\n[3]  Gustafsson, F. K., Danelljan, M., Timofte, R., & Sch\u00f6n, T. B. (2020). How to train your energy-based model for regression. arXiv preprint arXiv:2005.01698."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5359/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408211759,
                "cdate": 1700408211759,
                "tmdate": 1700408211759,
                "mdate": 1700408211759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]