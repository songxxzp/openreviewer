[
    {
        "title": "Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation"
    },
    {
        "review": {
            "id": "GjTCfyCjZr",
            "forum": "o4CLLlIaaH",
            "replyto": "o4CLLlIaaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_gQRV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_gQRV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to construct the generalizable neural field, called the generalizable neural Point Field (GPF), based on point-based rendering. This approach explicitly models by geometric priors and augments it with neural features to eliminate occlusions in feature-fetching. A nonuniform log sampling strategy is proposed to improve both rendering speed and reconstruction quality. Moreover, this paper presents a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries. Experiments show that the proposed model can deliver better geometries, view consistencies, and rendering quality on three datasets in both generalization and finetuning settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a Generalizable neural Point Field (GPF) for building generalizable NeRF based on point-based neural rendering. This paradigm outperforms existing image-based benchmarks and yields state-of-the-art performance on generic reconstructions.\n\nThis method explicitly models the visibilities by geometric priors and augments it with neural features, which are then used to guide the feature fetching procedure to better handle occlusions.\n\nA nonuniform log sampling strategy is proposed based on the point density prior, and perturbations to sampling parameters are imposed for robustness, which not only improves the reconstructed geometry but also accelerates the rendering speed.\n\nA spatially feature-augmented learnable kernel as feature aggregators is presented, which is proven to be effective for generic abilities and geometry reconstruction at drastically shape-varying areas."
                },
                "weaknesses": {
                    "value": "This reviewer has the following concerns.\n\nThe primary contribution claimed by this paper is the introduction of the first generalizable NeRF based on point-based neural rendering. An existing point-based method is PointNeRF, which is a per-scene optimization method.\nSection F.1 discusses the comparison between GPF and PointNeRF, which is helpful. What does the entry \"Ours\" represent in the table below Figure 17? If it refers to the model after fine-tuning, what are the results without fine-tuning?\n\nUpon closer examination of the comparison with PointNeRF, this paper states that the improvement is attributed to hierarchical fine-tuning strategies. I am wondering about the runtime required for fine-tuning. PointNeRF can be optimized from scratch in approximately 40 minutes. Additionally, is pretraining of the generalizable NeRF necessary? It seems that the primary advantage of this method over PointNeRF lies in its superior fine-tuning strategy.\n\nThe log sampling strategy is simply a handcrafted sampling distribution around the surface, which may not be considered a significant technical contribution. This strategy can only be applied if the depth prior is known.\n\nIt is important to discuss the comparison between the proposed method and the recently introduced efficient Gaussian-splatting representation. What are the advantages of the proposed method?"
                },
                "questions": {
                    "value": "Please clarify the comparison with PointNeRF and explain the key factors that make this method superior to PointNeRF.\n\nPlease discuss the comparison between this method and Gaussian-splatting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1941/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1941/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1941/Reviewer_gQRV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697996231642,
            "cdate": 1697996231642,
            "tmdate": 1700648249840,
            "mdate": 1700648249840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FVbRtZSbxD",
                "forum": "o4CLLlIaaH",
                "replyto": "GjTCfyCjZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 1**: The primary contribution claimed by this paper is the introduction of the first generalizable NeRF based on point-based neural rendering. An existing point-based method is PointNeRF, which is a per-scene optimization method. Section F.1 discusses the comparison between GPF and PointNeRF, which is helpful. What does the entry \"Ours\" represent in the table below Figure 17? If it refers to the model after fine-tuning, what are the results without fine-tuning?\n\nThanks for the reviewer pointing out this. We apologize for the lack of the caption of the Table below **Figure 17** in the original paper. All metrics in the Table are tested under the finetuning setting. Following the reviewer\u2019s suggestion, we additionally report the comparison of the results under the generalization settings of the three point-based rendering methods. We consolidate all the experimental results related to point-based methods into a single Table, i.e. **Table 9** in the revised Supplementary Material. For the convenience of the reviewer, we also provide the Table as follows:\n\n|Training  Setting&nbsp;| Methods   | NeRF Synthetic | DTU|\n|----------|---------------|-----------|----------|\n|           |    |PSNR\u2191         &nbsp; SSIM\u2191  &nbsp;  LPIPS\u2193   | PSNR\u2191    &nbsp;  SSIM\u2191   &nbsp; LPIPS\u2193       |\n|| PointNeRF &nbsp;| 6.12    &nbsp;  &nbsp; &nbsp; 0.18   &nbsp;  &nbsp; &nbsp; 0.88         | 23.18       &nbsp; &nbsp; 0.87       &nbsp; &nbsp; &nbsp; 0.21  |\n|Generalization | Point2Pix    |19.23 &nbsp; &nbsp; 0.787   &nbsp; &nbsp; 0.542  | 16.74     &nbsp; &nbsp; 0.655    &nbsp; &nbsp; 0.558 |\n||  **Ours**     |   **29.31**       &nbsp; &nbsp;   **0.960**   &nbsp; &nbsp;   **0.081**    |   **27.67**    &nbsp; &nbsp;   **0.945**       &nbsp; &nbsp;   **0.118**   |\n|                 | PointNeRF | 30.71    &nbsp; &nbsp; 0.961    &nbsp; &nbsp; 0.081      | 28.43  &nbsp; &nbsp; 0.929  &nbsp; &nbsp; 0.183  |\n|Finetuning| Point2Pix | 25.62      &nbsp; &nbsp; 0.915     &nbsp; &nbsp; 0.133       | 24.81  &nbsp; &nbsp; 0.894  &nbsp; &nbsp; 0.209  |\n|| **Ours**      |  **33.28**           &nbsp; &nbsp;  **0.983**           &nbsp; &nbsp;  **0.037**          |  **31.65**    &nbsp; &nbsp;  **0.970**    &nbsp; &nbsp;  **0.081** | \n\nThe results illustrate that the PointNeRF performs well on the DTU dataset but it is completely collapsed on the NeRF dataset because it is pre-trained on the DTU. This means that PointNeRF does not have any generalization ability to out-of-distribution data, as stated in **Figure 20** in the Appendix. In contrast, our model also trained on the DTU dataset, but can generalize very well on the out-of-distribution data. Moreover, Point2pix only produces blurry images."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506458868,
                "cdate": 1700506458868,
                "tmdate": 1700566577062,
                "mdate": 1700566577062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E5WeTUNEDO",
                "forum": "o4CLLlIaaH",
                "replyto": "GjTCfyCjZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 2**: Upon closer examination of the comparison with PointNeRF, this paper states that the improvement is attributed to hierarchical fine-tuning strategies. I am wondering about the runtime required for fine-tuning. PointNeRF can be optimized from scratch in approximately 40 minutes. Additionally, is pretraining of the generalizable NeRF necessary? It seems that the primary advantage of this method over PointNeRF lies in its superior fine-tuning strategy.  \n>**Question 1**: Please clarify the comparison with PointNeRF and explain the key factors that make this method superior to PointNeRF.\n\nThanks for this comment. We here explain the differences between the PointNeRF and our method in detail and answer the reviewer's issue. \n\nIn terms of optimization time, we would like to note that the duration of 40 minutes for PointNeRF does not refer to training the model from scratch. Rather, it corresponds to the fine-tuning process using a pretraining checkpoint obtained from the DTU dataset. This information can be verified by referring to the official code repository of PointNeRF (https://github.com/Xharlie/pointnerf). \n\nBelow are the results of our evaluation, where we compared the fine-tuning time of our method with that of PointNeRF. The test set used for this comparison was scan114 of the DTU dataset, specifically in the context of fine-tuning. \n\n| Step (PSNR) | 1k | 10k | 15k |\n|------------|:---------------:|:-----------:|:----------:|\n|PointNeRF  | 2min (28.43)   | 20min (30.01) | 30min (30.1) | \n|**Ours**       | 1.5min (29.67) | 15min (31.18) | 22.5min (31.7) |\n\nThe Table reveals that our approach exhibits faster convergence and attains higher upper bounds compared to PointNeRF. This outcome can be attributed to our log-sampling strategy, which introduces robust geometric priors during the training process and reduces the number of sampling points. As a result, our method benefits from stronger geometric constraints, leading to improved performance in terms of convergence speed and achieving higher-quality results. \n\n\nThe finetuning strategy is indeed a big difference. However, our approach encompasses other significant components that are dominant to our superior performance, especially the generalization ability. Point-based rendering methods can generally be divided into two main steps: \"image-to-point feature fetching\" and \"point-to-ray feature aggregation.\" We will outline their distinctions in terms of the two steps. \n\n**There are *6* distinct differences with PointNeRF, also with most other point-based methods.**  \nFor the \u201cimage to point\u201d step :   \n***1.***\tPointNeRF fetches features for points solely from a single image, but our method considers\nmultiple-view images simultaneously, incorporating a broader range of visual information.   \n***2.***\tMoreover, our method explicitly takes occlusions into account during feature fetching, which PointNeRF does not consider.   \n***3.***\tIn addition, hile PointNeRF employs a single latent vector as the neural feature for each point, our method utilizes a pair of latent vectors to separately represent shape and appearance.   \n***4.***\tOur method leverages low-level features to capture color information and high-level features for geometry regression, whereas PointNeRF only utilizes high-level features. \n\nFor the \u201cpoint to ray\u201d step:  \n***1.***\temploys uniform sampling to sample points, whereas our method fully utilizes geometry priors in the point cloud by employing a log sampling strategy, allowing our method to render faster and obtain better geometries.  \n***2.***\tPointNeRF simply aggregates features from the neural point cloud to the sampling points along rays using inverse distance weights. In contrast, our method introduces a learnable kernel that enables feature aggregation based on visibilities, improving the effectiveness of the aggregation process. \n\nEach of the aforementioned components plays a critical role in the success of the Generalizable Point Field approach. Taking into account the response provided to the reviewer, we have restructured **Section G1** in the revised supplementary material to enhance clarity and readability. The revised section aims to provide a clearer and more comprehensive explanation of the divergences between the two approaches, facilitating a better understanding of the proposed method."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506695576,
                "cdate": 1700506695576,
                "tmdate": 1700571940402,
                "mdate": 1700571940402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JT7wAWLZ9L",
                "forum": "o4CLLlIaaH",
                "replyto": "GjTCfyCjZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 3**: The log sampling strategy is simply a handcrafted sampling distribution around the surface, which may not be considered a significant technical contribution. This strategy can only be applied if the depth prior is known.\n\nAs the reviewer state that the log sampling strategy indeed relies on the depth prior. Therefore we propose a way to extract depth priors from the point cloud (introduced in **Section 3.1 Equation 1~4**) initialized by the MVS techniques and taking into account visibilities. To make optimal use of the extracted depth priors, we propose the implementation of log sampling. This technique offers dual benefits: it enhances rendering speed and elevates the quality of reconstructed geometries. \n\nThe efficacy of log sampling is substantiated through the presentation of results in **Figure 14**, **Figure 16**, and **Table 3**. **Figure 14** and **Table 3**describes the efficiency and rendering quality among different sampling strategies, **Figure 16** are enlarged details of renderings that affected by the sampling methods. The visual evidence in **Figure 14** demonstrates the acceleration achieved through log sampling, while **Figure 16** showcases the improved fidelity of the reconstructions. Furthermore, the quantitative analysis provided in **Table 3** solidifies the functional superiorities of log sampling.\n\nCurrently, our proposed method still relies on the initialization module, which employs MVS techniques to predict a point cloud from input images. However, to reduce this dependency and enhance the flexibility of our approach, we plan to introduce an alternative neural module to replace the MVS-based initialization. The alternative module could be jointly trained with our GPF to produce point initialization for the given scenes. Moreover, we present this prospect idea in detail in Section H of the revised manuscript and we hope this would address concerns of the reviewer regarding deep priors."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506800111,
                "cdate": 1700506800111,
                "tmdate": 1700572173431,
                "mdate": 1700572173431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UzpXPSVRl9",
                "forum": "o4CLLlIaaH",
                "replyto": "GjTCfyCjZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 4**:  It is important to discuss the comparison between the proposed method and the recently introduced efficient Gaussian-splatting representation. What are the advantages of the proposed method?  \n>**Question 2**:  Please discuss the comparison between this method and Gaussian-splatting.\n\nWe thank the reviewer for the comment. 3D Gaussian Splatting (3DGS) recently has gained lots of attention due to its promising progress. 3DGS is a rasterization technique described in [1] that allows real-time rendering of photorealistic scenes learned from small samples of images. To summarize, the 3DGS focuses on **per-scene** fast optimization and real-time rendering, whereas our approach pays attention to the **generalization** to unseen scenes without retraining. The abilities of per-scene optimization between the two methods are similar, for example, on the NeRF dataset, both our method and 3DGS achieve 33.3 PSNR. We properly mention the concurrent work in our Introduction and Related work. \n\n[1] Kerbl, Bernhard, et al. \"3D Gaussian Splatting for real-time radiance field rendering.\" ACM Transactions on Graphics (ToG) 42.4 (2023): 1-14."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506893076,
                "cdate": 1700506893076,
                "tmdate": 1700566636511,
                "mdate": 1700566636511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UJ96FP7W61",
                "forum": "o4CLLlIaaH",
                "replyto": "UzpXPSVRl9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Reviewer_gQRV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Reviewer_gQRV"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to express my gratitude to the authors for their comprehensive response. The new results and discussions effectively address most of my previous concerns. As a result, I will raise my rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648231669,
                "cdate": 1700648231669,
                "tmdate": 1700648231669,
                "mdate": 1700648231669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6lJsAb8vX8",
            "forum": "o4CLLlIaaH",
            "replyto": "o4CLLlIaaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_s3fi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_s3fi"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel paradigm in generalizable Neural Radiance Field (NeRF) research by introducing the Generalizable Neural Point Field (GPF). Unlike traditional NeRF methods that utilize image-based rendering, this work focuses on point-based rendering. The paper claims to address several prevalent issues with the existing image-based methods, including occlusion-related problems, artifacts, and performance drop-offs with varying view distances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Originality: The Generalizable Neural Point Field (GPF) is a fresh perspective in NeRF research, emphasizing point-based over image-based rendering.\n- Quality: The proposed methods exhibit high-quality research and innovation, from the nonuniform log sampling strategy to the feature-augmented learnable kernel.\n- Clarity: The paper's overall structure is clear, presenting a logical flow of ideas and discussions.\n- Significance: If the claims are validated further, this research could serve as a benchmark in the NeRF domain."
                },
                "weaknesses": {
                    "value": "- Lack of Detailed Explanations: Some sections, especially the technical components, could use more in-depth explanations or visual aids.\n- Dependency on Other Technologies: The initial dependency on PatchmatchMVS might limit the paper's approach from being a standalone solution.\n- Limited Experimentation: Testing on only three datasets might not showcase the full potential or limitations of the method.\n- Complexity: The approach's intricate nature might pose scalability or efficiency challenges that haven't been addressed comprehensively."
                },
                "questions": {
                    "value": "- Could the authors expand on the visibility-oriented feature fetching, possibly with diagrams, for better clarity? Figure (b) alone is not clear enough.\n- Given the reliance on PatchmatchMVS for the initial point scaffold, how does this affect the scalability or deployment of GPF in diverse scenarios? The authors also mention in the limitation section that they want to propose a NeRF-based initialization module that can be trained from scratch. Please comment on how you plan to achieve this.\n- Can the authors comment on potential efficiency challenges due to the method's complexity? For example, compare the rendering speed with E-NeRF."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698065914228,
            "cdate": 1698065914228,
            "tmdate": 1699636125324,
            "mdate": 1699636125324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XB85RdKfxK",
                "forum": "o4CLLlIaaH",
                "replyto": "6lJsAb8vX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 1**: Lack of Detailed Explanations: Some sections, especially the technical components, could use more in-depth explanations or visual aids.  \n>**Question 1**: Could the authors expand on the visibility-oriented feature fetching, possibly with diagrams, for better clarity? Figure (b) alone is not clear enough.\n\nWe greatly appreciate the valuable suggestions provided. Considering the reviewer's advice, we have incorporated comprehensive and detailed illustrations and clarifications, accompanied by visual aids, to enhance the readers' comprehension and facilitate their understanding. In summary, the additional components include an in-depth explanation of the physical meaning of the visibility scores (**Section A1 and Figure 9**), detailed illustrations of different sampling strategies (**Section E and Figure 14**), and the low- and high-level feature aggregations (**Section A2**), an explanation of the initialization module (**Section E4 and H**), etc. Other important revisions are highlighted in red in the revised manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505988723,
                "cdate": 1700505988723,
                "tmdate": 1700505988723,
                "mdate": 1700505988723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "drCyXenKiD",
                "forum": "o4CLLlIaaH",
                "replyto": "6lJsAb8vX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 2**: Dependency on Other Technologies: The initial dependency on PatchmatchMVS might limit the paper's approach from being a standalone solution.  \n>**Question 2**: Given the reliance on PatchmatchMVS for the initial point scaffold, how does this affect the scalability or deployment of GPF in diverse scenarios? The authors also mention in the limitation section that they want to propose a NeRF-based initialization module that can be trained from scratch. Please comment on how you plan to achieve this.\n\nWe thank the reviewer for this question. Our method indeed needs the module to initialize a point scaffold, here we use the PatchmatchMVS but are not limited to it. Other multi-view methods can also be used such as Colmap or OpenMVG because our approach is not sensitive to the initial point cloud. If the initial point cloud is imperfect, our point growing, pruning, and refinement modules can make it better by optimization. We also plan to propose an alternative module to replace the currently used initialization module, which can be jointly trained with our GPF method. Here we briefly introduce what we are going to do for that. \n\nThis module will be designed for initializing point clouds from multi-view images for reconstructing our generalizable point field. Therefore, we plan to use a feature-based method combined with a randomization process. We use a figure to visually aid in explaining this method, which is added to the Revised Appendix on **Page 23, Figure 22**. The process begins by uniformly sampling points from the given space. Next, we employ a hidden point removal algorithm [1] to roughly identify the points that are visible from the source viewpoints. During this step, only the spatial relationship between the viewpoints and the point set is considered, disregarding the scene content depicted in the images. Subsequently, we extract features from these images using a UNet-like network, as illustrated in **Figure 1(a)** and project these features onto their corresponding visible points.\n\n As a result, each point contains multiple feature vectors. Following this, we either train another network or employ suitable metrics (such as cross-correlation) to evaluate the similarity of these features. If the features exhibit sufficient similarity, we can conclude that the given point lies on the surface of the object. Importantly, the features used for feature matching can also be reused for the reconstruction of the GPF, such as the low- or high-level features. Consequently, this approach does not impose an additional memory burden or introduce extra parameters.  \n\n[1]  https://github.com/pdhimal1/HPR"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506191305,
                "cdate": 1700506191305,
                "tmdate": 1700506191305,
                "mdate": 1700506191305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2k8TAME1D5",
                "forum": "o4CLLlIaaH",
                "replyto": "6lJsAb8vX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 3**: Limited Experimentation: Testing on only three datasets might not showcase the full potential or limitations of the method.\n\nWe thank the reviewer for pointing out this issue. Following the suggestion of the reviewer, we conduct additional experiments for all baselines and our method on the LLFF dataset [1] for further comparisons. The qualitative and quantitative results are reported in Section C1 of the revised Appendix, including the new **Figure 14** and **Table 5**. To facilitate the reviewing process, we list the quantitative Table on the new dataset in the following:\n \n| Training Setting &nbsp;&nbsp; | Generalization                                 | Finetuning |\n|------------------|---------------------------------------------|--------------|\n| Methods          |  PSNR\u2191 &nbsp; SSIM\u2191 &nbsp; LPIPS\u2193 &nbsp; &nbsp;|  PSNR\u2191 &nbsp; SSIM\u2191 &nbsp; LPIPS\u2193 |\n| IBRNet           | 25.13  &nbsp; &nbsp;  0.817 &nbsp; &nbsp; 0.205 &nbsp; &nbsp;| 26.73 &nbsp; &nbsp;  0.851 &nbsp; &nbsp;  0.175  |\n| MVSNeRF          | 21.93  &nbsp; &nbsp; 0.795 &nbsp; &nbsp; 0.252  &nbsp; &nbsp;| 25.45 &nbsp; &nbsp;  0.877 &nbsp; &nbsp;  0.192  |\n| ENeRF            | 22.78 &nbsp; &nbsp;  0.808 &nbsp; &nbsp; 0.209 &nbsp; &nbsp;| 24.89 &nbsp; &nbsp;  0.865 &nbsp; &nbsp;  0.199  |\n| Neuray           | 25.35  &nbsp; &nbsp; 0.818 &nbsp; &nbsp; 0.198  &nbsp; &nbsp;| 27.06 &nbsp; &nbsp;  0.850 &nbsp; &nbsp;  0.172  |\n| **Ours**             | **26.01** &nbsp; &nbsp;  **0.829** &nbsp; &nbsp; **0.184**  &nbsp; &nbsp;|**27.79** &nbsp; &nbsp;  **0.872** &nbsp; &nbsp;  **0.171**  |\n\nThe obtained results clearly indicate that our approach maintains its superiority in both the generalization and fine-tuning settings. we have consistently outperformed the alternative methods across various evaluation metrics.\n\n[1] Mildenhall, Ben, et al. \"Local light field fusion: Practical view synthesis with prescriptive sampling guidelines.\" ACM Transactions on Graphics (TOG) 38.4 (2019): 1-14."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506240375,
                "cdate": 1700506240375,
                "tmdate": 1700566496279,
                "mdate": 1700566496279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "82mncGL3cC",
                "forum": "o4CLLlIaaH",
                "replyto": "6lJsAb8vX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 4**: Complexity: The approach's intricate nature might pose scalability or efficiency challenges that haven't been addressed comprehensively.  \n>**Question 3**: Can the authors comment on potential efficiency challenges due to the method's complexity? For example, compare the rendering speed with E-NeRF.\n\nThanks for this comment. We follow the reviewer\u2019s advice to report some analysis and metrics reflecting the efficiency. First, we would argue that even if our method needs to calculate the visibility maps before rendering, these maps can be precomputed and reused many times. As a result, the rendering speed remains unaffected. Additionally, the computation of visibility maps is well-suited for parallel processing, allowing for rapid execution.\n\nFurthermore, we measure the time for all baselines and our method of rendering a single image on the DTU dataset by using a single NVIDIA 3090 GPU. We also calculate the number of parameters of these models (Param in the Table below). These provide insights into the complexity and resource requirements of the proposed method.\nWe also provide the results and analysis on **Page 20** in the Highlighted Revision Version. Here for the convenience of the reviewer, we present the Table in the following as well:\n\nMethod &nbsp; | IBRNet   &nbsp;     | MVSNeRF &nbsp;  | ENeRF  &nbsp;   | Neuray &nbsp;    | Ours  &nbsp;    |\n|----------|---------------|-----------|----------|-----------|----------|\n|Param  &nbsp; | 8.95e6       | 4.68e5   | 4.36e5   | 2.85e7   | 7.92e4   |\n|Speed (s)&nbsp;   | 30.85        | 4.963    | 0.34     | 35.18    | 0.83   |\n\n\nIt can be seen that our method exhibits the second-fastest rendering speed, with only a slight difference compared to ENeRF. However, despite ENeRF having a larger parameter count, it achieves a slightly faster rendering speed. This is primarily due to ENeRF compressing the number of sampled points to an extreme reduction of 2, whereas our approach employs log sampling with 16 sampling points. The impact of this influencing factor can be observed in **Figure 14** in the Appendix. \n\nAnother reason is that the k-nearest search algorithm also consumes a few times. In addition, one advantage of our method is the ability to freely adjust between rendering speed and quality. Because the rendering speed is sensitive to the number of points due to the K nearest search operation, more points typically result in better rendering quality but slower rendering speed, and vice versa. In the above experiments, we maintain 200,000 points."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506341901,
                "cdate": 1700506341901,
                "tmdate": 1700572100586,
                "mdate": 1700572100586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tDOAMOAIJG",
                "forum": "o4CLLlIaaH",
                "replyto": "82mncGL3cC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Reviewer_s3fi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Reviewer_s3fi"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you so much for your efforts in addressing my concerns. I have no further questions and would like to stick to my initial positive opinion."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661831838,
                "cdate": 1700661831838,
                "tmdate": 1700661831838,
                "mdate": 1700661831838,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TYmh6q4vPX",
            "forum": "o4CLLlIaaH",
            "replyto": "o4CLLlIaaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_BQBs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_BQBs"
            ],
            "content": {
                "summary": {
                    "value": "The work propose a generalizable point-based NeRF for novel-view synthesis tasks. The point cloud is first initalized from classical MVS technique. A U-Net is trained to extract feature for the input views, which is aggregated into the points in a visibility-aware and learnable manner. To do volume rendering, query coordinates are sampled per the point density to improve efficiency. The query coordinates search K-nearest neighbor from the feature point cloud to form the query point feature, which is then map to density and color for volume rendering."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The quantitative improvement is solid. I believe many of the proposed modules can be plug into point-based rendering system to boost results."
                },
                "weaknesses": {
                    "value": "The visibility score in Eq4 is not well designed. The score actually decay more quickly for point ahead of the depth ($P_z < D_{i,xy}$). Consider two points with $P_z^{(back)} = D_{i,xy} + \\epsilon$ and $P_z^{(front)} = D_{i,xy} - \\epsilon$, their scores are:\n- $score^{(back)} = 1 - \\frac{|D_{i,xy} + \\epsilon - D_{i,xy}|}{D_{i,xy} + \\epsilon} = \\frac{D_{i,xy}}{D_{i,xy} + \\epsilon}$\n- $score^{(front)} = 1 - \\frac{|D_{i,xy} - \\epsilon - D_{i,xy}|}{D_{i,xy} - \\epsilon} = \\frac{D_{i,xy} - 2\\epsilon}{D_{i,xy} - \\epsilon}$\n\nWhen $\\epsilon > 0$, $score^{(front)} < score^{(back)}$. In addition, the score is claimed to be naturally constrainted in range of 0 and 1, but it is not the case when $P_z < 0.5 D_{i,xy}$ (become negative)."
                },
                "questions": {
                    "value": "I found some qualitative results from the baseline is better. In Fig3, the head of the ship and the sea in the ship scene, the table of the durian scene are better recovered by ENeRF. In Fig4, the reconstructed ground dirt in the BlendedMVS is overly smooth while the baseline ENeRF can recover more detail texture. What would be the reason of these? Is it because the dependent MVS point cloud? \n\nIs the proposed method sensitive to the initial points?\n\nPaper proofread:\n- Missing parentheses for the $\\exp$ in Sec3.3 last paragraph.\n- The reference to the Figure in Sec.4.2's 5th sentence is missing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698926268891,
            "cdate": 1698926268891,
            "tmdate": 1699636125245,
            "mdate": 1699636125245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZXm2zh1vug",
                "forum": "o4CLLlIaaH",
                "replyto": "TYmh6q4vPX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness**: The visibility score in Eq4 is not well designed. \n\nThanks for the comment. We sincerely apologize for misleading you due to our lack of detailed description. Actually, the $P_z$ in **Eq. 4** is naturally larger than $D_{i,xy}$ due to the definition of depth projection. In order to enhance clarity and facilitate comprehension of this matter, we have incorporated an additional **Figure 9** in the revised Supplementary Material to serve as an illustrative tool.  The variable $P_z$ denotes the z-value associated with a specific point in the camera coordinate system. It represents the length of the projection of the line connecting the point and the camera center onto the camera's optical axis ($P_{z,c}$ in the figure). The value of $D_{i,xy}$ is derived by projecting the point onto the depth map and subsequently employing bilinear interpolation. The depth map itself is computed by **Equation 1 to 3** in the main paper. These relationships are visually depicted in the aforementioned Figure. \n\nIf a point, such as point A in the figure, is\nvisible from the viewpoint, it implies that it lies on the object\u2019s surface. In this case, the z-value of\nthe point in the camera coordinate system should equal the projected depth value, $D_{i,xy}$.\nOn the contrary, if a point (e.g., point B) is not visible from the viewpoint, its z-coordinate can only\nbe greater than $D_{i,xy}$. There should not be any other points between $D_{i,xy}$, and the camera\u2019s center,\nas it would cause a change in the value of $D_{i,xy}$ accordingly. Consider another case where a point\nlies outside the viewing angle\u2019s frustum, and its z-value is smaller than the minimum depth on the\ndepth map, as illustrated by point C in the figure. In this case, the point would be projected outside\nthe image plane. However, during interpolation on the plane, we employ zero padding. Therefore,\nthe value of $D_{i,xy}$ for this point would be zero, which is smaller than $P_z$ as well.\n\n**To conclude, the $P_z$ is always greater than or equal to $D_{i,xy}$**. Therefore, the decay rate would be stable, and the score would be constraint to [0, 1]. We hope the above interpretaion could dispel the reviewer's concen. \n\nIn order to enhance the readability and comprehensibility of the manuscript, we have incorporated the aforementioned additional descriptions into **Section A1** of the revised Appendix."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505786689,
                "cdate": 1700505786689,
                "tmdate": 1700567843639,
                "mdate": 1700567843639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZCsT9qLBot",
                "forum": "o4CLLlIaaH",
                "replyto": "TYmh6q4vPX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Question**: I found some qualitative results from the baseline is better. In Fig3, the head of the ship and the sea in the ship scene, the table of the durian scene are better recovered by ENeRF. In Fig4, the reconstructed ground dirt in the BlendedMVS is overly smooth while the baseline ENeRF can recover more detail texture. What would be the reason of these? Is it because the dependent MVS point cloud?\nIs the proposed method sensitive to the initial points?\n\nWe appreciate the reviewer for bringing this to our attention. Indeed some examples generated by ENeRF exhibit clearer background objects, such as the sea in the \"ship\" example and the table in the \"durian\" example. We would like to provide two arguments in response to this observation. \n\nOn the one hand, the point scaffold of our approach is initialized by the depth fusion algorithm from the predicted depth maps. As a result, the density of the point cloud belongting to the primary objects (i.e., the ship and the durian themselves) tends to be higher compared to that of the background objects (i.e., the sea and the table). This is because the camera views primarily focus on capturing the main subject, while there might only be a few camera views that capture the background objects. Consequently, the rendering quality of the background objects might be slightly inferior to that of the main object in a given scene. Improving the density of the point clouds in these areas can enhance the corresponding qualities.\n\nOn the other hand, ENeRF and other image-based methods select source views that are spatially close to the target views for rendering. These source images contain a comparable amount of information regarding both the background and the main subject. Hence, sometimes they could obtain clearer background objects. This is also a reason why our approach can effectively recover the main objects in a scene for both the rendering quality and the geometry, while the baselines cannot faithfully reconstruct the main objects. \n\nIn addition, as for the over-smoothness of the \u201cexcavator\u201d scene in BlenededMVS, this can be attributed to the insufficient number of initial points used. In this case, we only utilize 200k points to initialize the scene, which is inadequate considering the vastness of the scene. Consequently, the point density in space becomes low, making it challenging to effectively represent high-frequency information in the scene and resulting in image smoothness. \n\nThis can be alleviated by more initial points or a smaller search radius in the finetuning setting to grow up more points.  While our images may appear smooth with a small number of points, our method successfully reconstructs more comprehensive geometric structures compared to the baseline methods, particularly in heavily occluded areas. \n\nBesides, we incorporate this valuable comment into our Appendix. We provide an additional example which is produced from 400k initial points and smaller search radius in the new **Figure 19** of the revised Appendix. This example successfully recovers most of the high-frequency information in the scene by using more initial points. We think this should be a good example to illustrate the effect of the number of points.  \n\nIn conclusion, we figure that our approach is **not** sensitive to the initialization of point clouds due to our point growing, point pruning, and point refinement modules, which can be seen in **Section 4.3** of the main paper. Although the initial point clouds are not so perfect, we can still optimize them with these techniques."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505881125,
                "cdate": 1700505881125,
                "tmdate": 1700566415569,
                "mdate": 1700566415569,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XKGtDO1SU9",
            "forum": "o4CLLlIaaH",
            "replyto": "o4CLLlIaaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_BFzR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1941/Reviewer_BFzR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel paradigm for constructing a generalizable neural field based on point-based rendering, which addresses the challenges of occlusions, distortions, and degradation in image-based representations. The proposed approach combines geometric priors and neural features to eliminate occlusions in feature-fetching, and a nonuniform log sampling strategy and a learnable kernel spatially augmented with features for improved rendering speed and reconstruction quality. The authors demonstrate the effectiveness of their approach on a variety of datasets, showing improved generalization and robustness to occlusions and distortions compared to previous methods. Overall, the paper presents a promising approach for learning robust and generalizable radiance fields."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall the paper is nicely presented and introduce several novel components including: \n\n- The proposed approach combines geometric priors and neural features to eliminate occlusions in feature-fetching explicitly in the rendering process, which is a novel contribution to the field.\n- The authors introduce a nonuniform log sampling strategy and a learnable kernel spatially augmented with features, which is a novel approach to improving rendering speed and reconstruction quality.\n\nThe method is properly evaluated with some ablation study."
                },
                "weaknesses": {
                    "value": "The paper proposed a new pipeline with several novel designs over the existing methods, however many of the designs are not validated and some of the claims are not strongly backed by their existing experiments:\n1. The exact definition of convergence speed is vague in Table 2, and is not explained in details. Making the results in this table questionable.\n2. Separating low level and high level features sounds intuitive but however is not validated and the necessity of such design is thus questionable.\n3. No quantitative validation on claims such as \"better geometry\" and \"occlusion awareness\".\n\nAlso the paper needs more work to reduce typos and fix the format."
                },
                "questions": {
                    "value": "1. Visualize the test PSNR curve instead of just stating \"Convergence Speed\" as in Table 2 would be more convincing, intuitive and easier to follow.\n2. Some modules are not well ablated and analyzed - \\eg high-low-level feature encoding.\n3. It would be nice to present some metric and quantitatively prove the quality in geometry (esp. occlusion awareness). At least depth error should be compared with MVSNeRF and NeuRay.\n4. In related works, Generalizable Neural Field. section: \"All the above can be seen as image-based neural rendering\". I think this might be inaccurate- I believe the finetuned/unfinetuned MVSNeRF / GeoNeRF / NeRFusion can aggregate multi view information and do not require original images for further use (though MVSNeRF fetches image color for rendering in some versions). Could you clarify on this? Also I believe the section is not extensive enough. The authors should also talk specifically about other point-based neural rendering methods, maybe in a dedicated section.\n5. Maybe: considering other point-based methods as ft baselines and include in the main paper.\n6. Typos and minor fixes:\n  - Table 2: Convergeuce -> Convergence; Missing/misplaced underline under 1.04s\n  - Citation format should be fixed throughout the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1941/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1941/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1941/Reviewer_BFzR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699141244612,
            "cdate": 1699141244612,
            "tmdate": 1700699301471,
            "mdate": 1700699301471,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1UgNcLMQAF",
                "forum": "o4CLLlIaaH",
                "replyto": "XKGtDO1SU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Weakness 1**: The exact definition of convergence speed is vague in Table 2, and is not explained in details. Making the results in this table questionable.  \n>**Question 1**: Visualize the test PSNR curve instead of just stating \"Convergence Speed\" as in Table 2 would be more convincing, intuitive and easier to follow.\n\nWe appreciate the reviewer for bringing this to our attention. We have taken the feedback into account and have made the necessary revisions. In the revised Appendix, we have included a new figure, **Figure 14**, to provide a comprehensive comparison of efficiency among all counterparts. The horizontal axis represents training time, the vertical axis represents PSNR, and the size of the circles represents the time required for rendering an 800*800 image. \n\nUpon examining this figure, it becomes evident that the log sampling method requires the least training time while achieving the highest PSNR. Furthermore, it exhibits the second-fastest rendering speed, with only a slight slow compared to the \"surface sampling\". To avoid any potential misunderstanding, we have also updated the \"convergence speed\" and \"rendering speed\" in **Table 3** (original Table 2) of the manuscript to \"training time\" and \"rendering time\" respectively."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504721606,
                "cdate": 1700504721606,
                "tmdate": 1700567135428,
                "mdate": 1700567135428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "npFrYTtvQx",
                "forum": "o4CLLlIaaH",
                "replyto": "XKGtDO1SU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 2**: Separating low level and high level features sounds intuitive but however is not validated and the necessity of such design is thus questionable.  \n>**Question 2**: Some modules are not well ablated and analyzed - \\eg high-low-level feature encoding.\n\nThanks for this constructive comment. The motivation behind separating low- and high-level features in our approach draws inspiration from the theory of convolution. In CNN, the initial layers tend to capture low-level features related to color and corners, while deeper layers focus on extracting high-level, semantic, and abstract features. This observation has guided our decision to differentiate the use of low-level and high-level features in our method. Furthermore, research related to MVS, such as MVSNET [1] and MVSNeRF [2], has demonstrated that leveraging high-level features in feature-matching algorithms can lead to more accurate and reliable correspondences. Building upon this insight, we utilize low-level features for color regression and high-level features for decoding the $\\sigma$ in NeRF.\n\n\nWe agree with the reviewer that this design requires further clarifications and ablations. In response to this, we conducted experiments using the DTU dataset to analyze the effects of different feature combinations on rendering quality and reconstructed geometries. The experiment settings include \"only low-level features used\", \"only high-level features used\", and \"the combinations of the low- and high-level features\". To provide an example, when referring to \"only low-level features,\" it means that both the color and density decoders receive low-level features as input to regress their respective outputs. This configuration allows us to specifically investigate the influence of utilizing solely low-level features on the rendered results. The ablation results are listed below:\n\n|Training Setting &nbsp; |Generalization    |Finetuning     | |\n|----------|---------------|-----------|----------|\n|Methods |PSNR\u2191   &nbsp;  SSIM\u2191   &nbsp; LPIPS\u2193   &nbsp;  &nbsp;  | PSNR\u2191    &nbsp;   SSIM\u2191   &nbsp;   LPIPS\u2193  &nbsp;  &nbsp;  | RMSE\u2193  &nbsp; &nbsp;  Acc.T\u2191    |\n|Only-low  | 24.15   &nbsp; &nbsp;  0.896   &nbsp; &nbsp;  0.178   | 30.16    &nbsp; &nbsp;  0.921    &nbsp; &nbsp;  0.133  |0.383  &nbsp; &nbsp;  0.865 |\n|Only-high  | 22.94   &nbsp; &nbsp;  0.852   &nbsp; &nbsp;  0.231  | 30.47    &nbsp; &nbsp;  0.928   &nbsp; &nbsp;  0.132   |0.155  &nbsp; &nbsp;  0.917   |\n|low-high  |   **27.67**    &nbsp; &nbsp;  **0.945**    &nbsp; &nbsp;   **0.118**   |   **31.65**     &nbsp; &nbsp;   **0.970**    &nbsp; &nbsp;     **0.081**     |     **0.122**   &nbsp; &nbsp;      **0.936** |    \n\nIt is noted that both of the counterparts suffer from performance degradation. In addition, even though the rendering quality of \u201conly low-level features\u201d is better than that of the \u201conly high-level features\u201d, it is worth noting that the reconstruction geometries in the former are comparatively worse. This insight reinforces the significance of incorporating both low-level and high-level features in our approach. We also add these additional ablations to **Section E2** of the revised Appendix. \n\n[1] Yao, Yao, et al. \"Mvsnet: Depth inference for unstructured multi-view stereo.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[2] Chen, Anpei, et al. \"Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo.\" Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505065001,
                "cdate": 1700505065001,
                "tmdate": 1700565743974,
                "mdate": 1700565743974,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3YWxDCaQcl",
                "forum": "o4CLLlIaaH",
                "replyto": "XKGtDO1SU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weakness 3**: No quantitative validation on claims such as \"better geometry\" and \"occlusion awareness\".   \n>**Question 3**: It would be nice to present some metric and quantitatively prove the quality in geometry (esp. occlusion awareness). At least depth error should be compared with MVSNeRF and NeuRay.\n\nWe thank the reviewer for pointing out this issue. We highly agree with the reviewer that the quantitative metric for reconstruction quality is essential. In the revised **Section D**, we provide the RMSE an Accuracy with Threshold metrics to assess the quality of reconstructed depths for both the baseline methods and our own. Furthermore, as a means of validating the efficacy of explicit occlusion modeling, we present an alternate version of our approach, denoted as $Ours_{wo/vis}$, which excludes the visibility scores outlined in **Equation 5** of the main paper, marked as $Ours_{wo/vis}$. Here we provide this Table for the convenience of reviewing. \n\n\n|Dataset|\t Method          | IBRNet   | ENeRF  | Neuray  | Ours$_{wo/vis }$  | **Ours**  |\n|----------|---------------|-----------|----------|-----------|----------|----------|\n|NeRF Synthetic | RMSE\u2193 | 0.677   | 0.527    | 0.547    | 0.294       | **0.161**    |\n|  \t\t      | Acc.T\u2191 | 0.380    | 0.159   | 0.119    | 0.596    | **0.787**   |\n|DTU          | RMSE\u2193  | 0.321  | 0.435   | 0.162    | 0.189   | **0.122**   |                         \n|        | Acc.T\u2191 | 0.896    | 0.741    | 0.911    | 0.905       | **0.936**     |\n\nThe Acc. T denotes the accuracy of the threshold, we set the threshold as $1.25^3$ for all experiments. In addition, all tests are under generalization settings because finetuning cannot faithfully reflect the real understanding of models to the realistic geometries. Our approach, in particular, demonstrates superior results, particularly on the NeRF dataset. This indicates that our method is capable of capturing plausible geometries even when dealing with out-of-distribution data. Additionally, it should be highlighted that removing the visibility score can result in a decrease in performance, which validate the highly positive effect of explicitly modeling visibility on the generalization to unseen scenarios."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505272396,
                "cdate": 1700505272396,
                "tmdate": 1700575512738,
                "mdate": 1700575512738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y9V5qu4KrI",
                "forum": "o4CLLlIaaH",
                "replyto": "XKGtDO1SU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Question 4**: In related works, Generalizable Neural Field. section: \"All the above can be seen as image-based neural rendering\". I think this might be inaccurate- I believe the finetuned/unfinetuned MVSNeRF / GeoNeRF / NeRFusion can aggregate multi view information and do not require original images for further use (though MVSNeRF fetches image color for rendering in some versions). Could you clarify on this? Also I believe the section is not extensive enough. The authors should also talk specifically about other point-based neural rendering methods, maybe in a dedicated section.\n\nWe apologize for the lack of clarity in our previous statement regarding \"image-based\" rendering. To clarify, the **unfinetuned** versions of MVSNeRF and GeoNeRF still require all source images to synthesize novel views. While MVSNeRF employs the neural encoding volume to render images, it still need to select source images that are spatially close to the target view and reconstruct the volume before each time of rendering. On the other hand, as indicated by **Eq 3** in GeoNeRF, it still requires full-resolution 2D feature maps to render novel views, which prevents it from discarding any source images. \n\nBy contrast, our proposed method is fully independent of the source images both in **both** generalization and finetuning settings. As stated by the reviewer, the **finetuned** MVSNeRF, GeoNeRF and NeRFusion are indeed independent of the source images when synthesizing novel views. However, it should be noted that these models rely on the neural volume, which is not directly accessible for manipulation, as we claimed in our Introduction. If one desires to manipulate and edit the neural volume, one would have to begin by manipulating the source images used for reconstructing the volume. To address this ambiguity, we have made the necessary modifications to the related statement on **Page 2** in the Highlighted Revision Version.\n\nFurthermore, we follow the reviewer's feedback to include the \"Point-based Rendering\" into our **Related Work** as a dedicated section on **Page 3** in the Highlighted Revision Version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505403242,
                "cdate": 1700505403242,
                "tmdate": 1700566167670,
                "mdate": 1700566167670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dwzVLF17Ln",
                "forum": "o4CLLlIaaH",
                "replyto": "XKGtDO1SU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Question 5**: Maybe: considering other point-based methods as ft baselines and include in the main paper.\n\nThank you for your comment. We have included qualitative and quantitative comparisons with two distinct point-based rendering methods, namely PointNeRF [1] and Point2Pix [2], in the revised supplementary materials, **Pages 20 to 21**. These comparisons contain both generalization and finetuning settings to provide a comprehensive evaluation. We can see that both of the two methods cannot generalize well to unseen scenarios, especially PointNeRF. It only achieves 6 PSNR on the out-of-distribution data in the NeRF dataset. However, it is worth noting that PointNeRF exhibits favorable performance on the per-scene optimization task. Therefore, we have taken the reviewer's advice to include the finetuning results as the baselines to our main paper on **Page 7**. \n\nTo make it easier for reviewing purposes, we have provided the table here.\n\n\n|Training  Setting&nbsp;| Methods   | NeRF Synthetic &nbsp;&nbsp;| DTU|\n|----------|---------------|-----------|----------|\n|           |    |PSNR\u2191         &nbsp; SSIM\u2191  &nbsp;  LPIPS\u2193   | PSNR\u2191    &nbsp;  SSIM\u2191   &nbsp; LPIPS\u2193       |\n|| PointNeRF &nbsp;| 6.12    &nbsp;  &nbsp; &nbsp; 0.18   &nbsp;  &nbsp; &nbsp; 0.88         | 23.18       &nbsp; &nbsp; 0.87       &nbsp; &nbsp; &nbsp; 0.21  |\n|Generalization | Point2Pix    |19.23 &nbsp; &nbsp; 0.787   &nbsp; &nbsp; 0.542  | 16.74     &nbsp; &nbsp; 0.655    &nbsp; &nbsp; 0.558 |\n||  **Ours**     |   **29.31**       &nbsp; &nbsp;   **0.960**   &nbsp; &nbsp;   **0.081**    |   **27.67**    &nbsp; &nbsp;   **0.945**       &nbsp; &nbsp;   **0.118**   |\n|                 | PointNeRF | 30.71    &nbsp; &nbsp; 0.961    &nbsp; &nbsp; 0.081      | 28.43  &nbsp; &nbsp; 0.929  &nbsp; &nbsp; 0.183  |\n|Finetuning| Point2Pix | 25.62      &nbsp; &nbsp; 0.915     &nbsp; &nbsp; 0.133       | 24.81  &nbsp; &nbsp; 0.894  &nbsp; &nbsp; 0.209  |\n|| **Ours**      |  **33.28**           &nbsp; &nbsp;  **0.983**           &nbsp; &nbsp;  **0.037**          |  **31.65**    &nbsp; &nbsp;  **0.970**    &nbsp; &nbsp;  **0.081** | \n\n[1] Xu, Qiangeng, et al. \"Point-nerf: Point-based neural radiance fields.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022.\n\n[2] Hu, Tao, et al. \"Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505645051,
                "cdate": 1700505645051,
                "tmdate": 1700566043771,
                "mdate": 1700566043771,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7EysIhaZ2V",
                "forum": "o4CLLlIaaH",
                "replyto": "dwzVLF17Ln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1941/Reviewer_BFzR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1941/Reviewer_BFzR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! I'm raising my score since all concerns have been clearly addressed."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699333392,
                "cdate": 1700699333392,
                "tmdate": 1700699333392,
                "mdate": 1700699333392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]