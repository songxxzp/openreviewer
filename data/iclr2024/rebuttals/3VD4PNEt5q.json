[
    {
        "title": "Fusion is Not Enough: Single Modal Attack on Fusion Models for 3D Object Detection"
    },
    {
        "review": {
            "id": "NdDu3iyiHy",
            "forum": "3VD4PNEt5q",
            "replyto": "3VD4PNEt5q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes a two-stage optimization strategy for camera-LiDAR fusion models via using adversarial patches in camera images. The paper explores single-modal attack on fusion models, an interesting yet under-explored problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well-written and explores an interesting topic. \n* The proposed method can decrease the model performance by a large margin, showing the possibility of dramatically deteriorating the system performance by only attacking single-modality."
                },
                "weaknesses": {
                    "value": "* The effectiveness of the proposed two-stage optimization approach needs further justifications. Only showing the performance drop on fusion models is not enough. Comparisons with other single-stage attacks are also needed to demonstrate the effectiveness. Without proper benchmarks and comparisons with other SOTA algorithms, it is hard to justify the effectiveness of the technical contributions. \n* How to ensure the feasibility of the adversarial patches? Since the gradient optimization may find patches in the undeployable areas e.g., sky, can the proposed approach ensure the attack is feasible in the real physical world? Also in the paper, the author assumes the lidar data would not be changed. Since the patch may influence the lidar intensity or introduce extra points, please provide justifications for this assumption."
                },
                "questions": {
                    "value": "* What is the sensitivity of single-modal methods vs multi-modal (LiDAR/camera) methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698432586813,
            "cdate": 1698432586813,
            "tmdate": 1700626554084,
            "mdate": 1700626554084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ngpYkcy037",
                "forum": "3VD4PNEt5q",
                "replyto": "NdDu3iyiHy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer nx8r"
                    },
                    "comment": {
                        "value": "We express our deepest gratitude for the reviewer\u2019s time and astute comments. It is our pleasure to address the primary concerns in the following sections.\n\n**Q1: The effectiveness of the proposed two-stage optimization approach needs further justifications.**\n\n**A1**: Thank you sincerely for providing valuable insights.  In our original manuscript, we took care to include a comprehensive comparative analysis, particularly with respect to other single-stage attacks, notably the state-of-the-art (SOTA) one-stage camera-only patch region optimization methods discussed in Appendix J. The results of this comparative study underscore the limitations of the SOTA method in generating physically deployable patches when considering the entire scene.  Additionally, in the motivation section, we presented an evaluation of traditional single-stage patch attacks, emphasizing their failure in achieving success if the predefined patch region does not align with a vulnerable region for the subject fusion model. Our two-stage attack framework, through its adept identification of vulnerable regions and utilization of distinct attack strategies based on sensitivity types, aims to generate both deployable and effective adversarial patches. This, we believe, substantiates the efficacy and versatility of our proposed method.\n\n**Q2: How to ensure the feasibility of the adversarial patches?**\n\n**A2**: Your thoughtful question is much appreciated. In contrast to the SOTA one-stage gradient-based patch optimization approach (as shown in appendix J), our two-stage approach has solved the problem of undeployable adversarial patches. Notably, the patch's location in our approach is not optimized with gradients concerning the entire scene; rather, it is determined by highly practical projection functions. The optimization with gradients in the second stage is exclusively applied to the patch texture. As detailed in Section 4, following the recognition of sensitivity types of the fusion model in the first stage, different attack strategies are deployed in the second stage based on sensitivity types. The patch's location on the image is contingent on the projection function (i.e., proj_x()) in specific attack strategies (See Figure 4). For scene-oriented attacks, we project the patch onto a predefined physical location on the ground, and for object-oriented attacks, we dynamically project the patch onto a target object using reverse-engineered projection parameters based on different 3D locations and orientations of the object from the driving footage. Consequently, the patch generated by our method is highly deployable and feasible in the real world.\n\nWe posit a reasonable assumption that our patch does not affect LiDAR since it is designed to be affixed to the ground (for scene-oriented attacks) or a target vehicle (for object-oriented attacks) during deployment. Our primary alteration occurs in the surface texture of the 3D scene, preserving the 3D structure captured by the LiDAR sensor. The material of the patch (e.g., paper) is common and has little influence on the LiDAR intensity. Hence the impact of our approach on the LiDAR data is minimized and it is also difficult to recognize the adversarial information using only the LiDAR sensor. \n\n\n**Q3: What is the sensitivity of single-modal methods vs multi-modal (LiDAR/camera) methods?**\n\n**A3**: Your question is both thoughtful and appreciated. Our attack framework extends its applicability to single-modal methods, as evidenced by our evaluation with a camera-only model (BEVFormer) in the original manuscript. The last row of Figure 5 shows the sensitivity heatmap of BEVFormer, which demonstrates higher sensitivity in object areas and the model is classified as object sensitivity. Similar to other object-sensitive models, BEVFormer is robust to scene-oriented attacks and vulnerable to object oriented attacks as evidenced in Table 1 and Table 2. Compared with multi-modal models, the single-modal model exhibits  worse benign performance and experiences more pronounced performance degradation under object-oriented attacks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276146522,
                "cdate": 1700276146522,
                "tmdate": 1700323336189,
                "mdate": 1700323336189,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NuuIlsPWw3",
                "forum": "3VD4PNEt5q",
                "replyto": "NdDu3iyiHy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed feedback.\n\nQ1: Appendix J only shows the qualitative visualizations while quantitative results are needed to justify the performance boost. \n\nQ2: How to ensure the projection always projects to the ground?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586548777,
                "cdate": 1700586548777,
                "tmdate": 1700586561265,
                "mdate": 1700586561265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JLG3z4ewAQ",
                "forum": "3VD4PNEt5q",
                "replyto": "NdDu3iyiHy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to the Follow-up Question"
                    },
                    "comment": {
                        "value": "Thanks a lot for your continuous help in improving our paper. We answer your new questions below.\n\n\n**Q1: quantitative results are needed to justify the performance boost.**\n\n\n**A1**: Your thoughtful comments are appreciated. Allow us to clarify that the contribution of our approach is not the boost of attack performance in the digital space, but the practicality and effectiveness of the patch attack in the physical world. The qualitative visualization in appendix J demonstrates that the patch generated by the SOTA method, which encompasses a one-stage optimization of the patch\u2019s content and location, spans over multiple physical objects and is not deployable. In contrast, our two-stage approach decouples the location and content optimization, and it is guaranteed to generate both deployable and effective patches for any scene (see Figure 12 and Figure 13).  Hence our approach is more versatile and universal.\n\n\n**Q2: How to ensure the projection always projects to the ground?**\n\n\n**A2**: For the scene\u2013oriented attacks, as shown in Figure 4a, we pre-define the 3D location of the patch with parameters like height $g$, distance $d$ and viewing angle $\\alpha$, in the coordinate system of the ego-vehicle's front camera. Then we project the 3D coordinates of the patch back onto the 2D scene image (step 2). Given that we know the camera's height $g$ above ground from the dataset, we ensure by definition that the patch is positioned on the ground when using this camera height."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592442909,
                "cdate": 1700592442909,
                "tmdate": 1700593155500,
                "mdate": 1700593155500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LjwQSETB6L",
                "forum": "3VD4PNEt5q",
                "replyto": "NdDu3iyiHy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed comments and the great efforts in rebuttal. All my questions are addressed. Considering the novel physically plausible attack methods and technical contributions for multi-modal sensor fusion adversarial attack, I would like to increase the ratings. The analysis and additional explanations in the rebuttal should be added in the final manuscript for better readability. Thank you!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626438939,
                "cdate": 1700626438939,
                "tmdate": 1700626645223,
                "mdate": 1700626645223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ji7vbx0VqJ",
            "forum": "3VD4PNEt5q",
            "replyto": "3VD4PNEt5q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an adversarial attack method, which could attack multi-modal 3D object detection methods from the image input, making the attack easy to implement.\nIt first recognizes the proper attack type and proposes different methods for each type.\nExperiments are conducted with multiple popular multi-modal detectors, showing promising performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation that attacks from image input is reasonable, which is practical real world.\n- The proposed method overall makes sense.\n- Authors provide a demo, which makes the application of the proposed method more clear.\n- Authors conducted extensive experiments with many SOTA the art detectors, making the results convincing."
                },
                "weaknesses": {
                    "value": "- Attack from only the image side has limited application. In particular, some methods do not conduct feature-level fusion between image and LiDAR like [1][2].  The two modalities are decoupled in these methods. Even if the image modality is totally failed, they can output reasonable results.\n- The writing is not clear, especially in page 4 and page 5. There are very long paragraphs and many notations without clear organization, making it hard to follow the detailed method. I list some detailed questions in the following question box. \n- The method is not well-motivated. The proposed method seems to be a general attacking method. Are there any special designs to solve problems in image-only attacking or autonomous driving scenes? What is the difficulty of image-only attacks?\n\n[1] Frustum PointNets for 3D Object Detection from RGB-D Data \\\n[2] Fully Sparse Fusion for 3D Object Detection"
                },
                "questions": {
                    "value": "- In attack strategies, is the noise patch shared by the whole dataset? I saw the patch keeping changing in object-level attacks but remaining unchanged in scene-level attacks.\n- Is the mask in Sensitivity Distribution Recognition shared by the whole dataset?\n- I do not understand the form of the proj_x in Eq. 6 and why it is necessary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486626747,
            "cdate": 1698486626747,
            "tmdate": 1700632730933,
            "mdate": 1700632730933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EEZriODqQ6",
                "forum": "3VD4PNEt5q",
                "replyto": "Ji7vbx0VqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Part 1/2] Official Response to Reviewer DXNR (Q1, Q2 & Q3)"
                    },
                    "comment": {
                        "value": "We express our deepest gratitude for the reviewer's invaluable time and astute comments. We have made every effort to address the principal concerns as follows, and we sincerely appreciate the opportunity to enhance our work through your insightful feedback.\n\n**Q1: Some methods do not conduct feature-level fusion between image and LiDAR. The two modalities are decoupled in these methods.**\n\n**A1**: We appreciate your insightful remarks. The methods you have brought up are in the category of data-level fusion. As expounded in Appendix B of our manuscript, for the data-level fusion strategy, some studies use the extracted image information to augment the LiDAR points. In the evaluation section, we demonstrated the effectiveness of our proposed attack on a data-level fusion model (i.e., PointAugmenting) that utilizes image features extracted by CNN to hence LiDAR data. In the first study you mentioned, the authors employ CNN-based 2D object detection to identify object regions in the image input, then use the extruded 3D viewing frustum to extract corresponding points from the LiDAR point cloud for bounding box regression. In the second study, the authors use semantic segmentation on the image input to generate the frustum. Both methods depend on information from the camera input (i.e., 2D object detection or semantic segmentation results) to augment point cloud-based object detection. Therefore, adversarial attacks on images can deceive  object detection or semantic segmentation as in [1-3], leading to the failure of the extruded 3D frustum in capturing the corresponding LiDAR points of each object. This would subsequently affect the 3D bounding box regression. Your insights are greatly appreciated, and a discussion on these studies has been included in Appendix B, with appropriate citations.\n\n**Q2: Is the noise patch shared by the whole dataset?**\n\n**A2**:  Thank you for your comments and queries. The adversarial patch is shared across various frames in the dataset, with each patch optimized for a scene (in scene-oriented attacks) or a target object (in object-oriented attacks). In the context of object-oriented attack strategies, we project the patch image onto the rear of the target vehicle in each scene image using varying projection parameters based on the target object's location in 3D space (see Figure 4b). The varying patch appearances in object-oriented attacks result from the changing projection parameters and the interpolation among pixels in each data frame.  We have included a detailed threat model in Appendix Q of our revised manuscript.\n\n**Q3: Is the mask in Sensitivity Distribution Recognition shared by the whole dataset?**\n\n**A3**: No, the mask is not shared. Each mask (i.e., sensitivity heatmap) is defined and optimized for a specific frame of data extracted from the dataset. For the sensitivity type classification, as per Equation 4, the average sensitivity of object areas and non-object areas is calculated across various data frame $x$\u2019s sensitivity heatmaps. We have refined the description and notations in the revised version for clarity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276027759,
                "cdate": 1700276027759,
                "tmdate": 1700276027759,
                "mdate": 1700276027759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uu7lJ88uLT",
                "forum": "3VD4PNEt5q",
                "replyto": "Ji7vbx0VqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Part 2/2] Official Response to Reviewer DXNR (Q4 & Q5)"
                    },
                    "comment": {
                        "value": "**Q4: I do not understand the form of the proj_x in Eq. 6 and why it is necessary.**\n\n**A4**: proj_x() signifies the synthesis of the original patch image (see the \u201c2D patch image\u201d in Figure 4a) onto a specific area of the scene image (see the \u201ccaptured image\u201d in the center of Figure 4a)  to simulate how the patch would look once it's physically deployed. This projection is necessary for enhancing the physical-world robustness of the generated patch and minimizing the disparity between digital space and the physical world. As outlined, equations 7-9 provide an expanded representation, where ($u^p$, $v^p$) denotes the coordinates of a pixel on the original patch image, and ($u^s$, $v^s$) represents the pixel's corresponding coordinates on the scene image. proj_x() establishes this mathematical connection based on projection parameters (e.g., lateral and longitudinal distance, and viewing angle of the patch in the physical world). Specifically, for object-oriented attacks, as the target object\u2019s location and orientation vary across data frames, the patch would move with the target object once deployed physically. Hence, the patch\u2019s projection parameters should differ across frames for a faithful simulation during patch training before deployment. We innovatively extracted those parameters dynamically from the detected 3D position of the target object as we stated in our methods. We have revised the presentation of our method in the updated manuscript to make it more clear.\n\n**Q5: Are there any special designs to solve problems in image-only attacking or autonomous driving scenes? What is the difficulty of image-only attacks?**\n\n**A5**: We acknowledge your concerns and would like to highlight the unique challenges inherent in image-only attacks that have guided our method's unique design. Most prominently, depending on model architectures and fusion strategies, the vulnerable regions change across different fusion models. As a result, directly attacking a specific area, akin to the SOTA patch attack [4], may fail when the targeted area is not a vulnerable region (e.g., attacking the road for object-sensitive models, as depicted in Figure 2). Hence, a unique challenge is to determine the vulnerable region, and then the corresponding attack strategy can be employed. Our sensitivity distribution recognition algorithm addresses this challenge. A straightforward extension to the SOTA attack would be to directly optimize some patch on entire scene images. However, the generated patch may span over multiple physical objects and is not deployable (see Appendix J). Our two-stage design ensures physical deployability, and its compatibility with SOTA camera attacks reinforces it as a more universal solution. Rather than viewing the generality of our attack framework as a limitation, we consider it a strength. Another addressed challenge is reverse engineering precise projection functions for different target object locations and orientations from driving footage, facilitating faithful scene synthesis during offline patch generation (as elucidated in Q4). \n\n**Reference**\n\n[1] Xie, Cihang, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. \"Adversarial examples for semantic segmentation and object detection.\" In ICCV, 2017.\n\n[2] Huang, Lifeng, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L. Yuille, Changqing Zou, and Ning Liu. \"Universal physical camouflage attacks on object detectors.\" In CVPR, 2020.\n\n[3] Nesti, Federico, Giulio Rossolini, Saasha Nair, Alessandro Biondi, and Giorgio Buttazzo. \"Evaluating the robustness of semantic segmentation for autonomous driving against real-world adversarial patch attacks.\" In WACV, 2022.\n\n[4] Cheng, Zhiyuan, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, and Xiangyu Zhang. \"Physical attack on monocular depth estimation with optimal adversarial patches.\" In ECCV, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276096648,
                "cdate": 1700276096648,
                "tmdate": 1700471436473,
                "mdate": 1700471436473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E4uQ3Sa477",
                "forum": "3VD4PNEt5q",
                "replyto": "uu7lJ88uLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up question to Q5"
                    },
                    "comment": {
                        "value": "I appreciate the authors' effort in the rebuttal, which addresses some of my concerns. However, I am still confused about Q5.\n\n- In the rebuttal, the authors said \"As a result, directly attacking a specific area may fail when the targeted area is not a vulnerable region\". However, from my personal view, it holds true for all attacking methods, not only the image-only attacking. Thus, it is not a challenge of image-only attack. \n\n- It remains unclear what the type of the proposed method is.  It is a general method to attack fusion models or a specific method for image-only attacking?  If it is a specific model, the unique designs for the image-only setting are not very clear, because I don't think the sensitivity recognition is rooted in the image-only setting, as mentioned above. If it is a general model, why do the authors emphasize the image-only setting? I encourage the authors to make it more clear, otherwise, the logic of this paper is not firmed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459710424,
                "cdate": 1700459710424,
                "tmdate": 1700459710424,
                "mdate": 1700459710424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JYymwUVDgo",
                "forum": "3VD4PNEt5q",
                "replyto": "BrdwHpFMxz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "content": {
                    "title": {
                        "value": "Two follow-up questions"
                    },
                    "comment": {
                        "value": "- The authors said \"addresses this issue from an image-centric standpoint\", I do not completely agree because the proposed method can be also adopted for LiDAR modality (e.g., generate sensitivity map of LiDAR BEV map or all points) from my point of view. I just want to if the author solved a unique problem of image-only attacking. If does, what is the problem?\n\n- Will the authors open-source the project?\n\nI will give an overall evaluation after the authors address the concerns above concisely."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557695037,
                "cdate": 1700557695037,
                "tmdate": 1700557695037,
                "mdate": 1700557695037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "skHYvB1owb",
                "forum": "3VD4PNEt5q",
                "replyto": "Ji7vbx0VqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to the Follow-up Question"
                    },
                    "comment": {
                        "value": "Thanks a lot for your continuous help in improving our paper. We answer your new questions below.\n\n**Q1: If the author solved a unique problem of image-only attacking. If so, what is the problem?**\n\n**A1**: Thank you for your comments and queries. We agree that the proposed method can be adapted for LiDAR modality, which we leave as a future direction. In this context, the unique problem of image-only attack that we solve in this work is to reverse-engineer precise projection functions for varying target object locations and orientations from driving footage, which facilitates faithful scene image synthesis during patch generation (as elucidated in our initial response to Q4). \n\n\n**Q2: Will the authors open-source the project?**\n\n**A2**: Yes, we will open-source the project after the anonymous period. At present, our code with detailed instructions to run are available in the supplementary materials."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560330334,
                "cdate": 1700560330334,
                "tmdate": 1700560409670,
                "mdate": 1700560409670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xYyw6sRXpX",
                "forum": "3VD4PNEt5q",
                "replyto": "skHYvB1owb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "content": {
                    "title": {
                        "value": "Questions on proj_x()"
                    },
                    "comment": {
                        "value": "I I understand correctly, the so-called \"reverse-engineer precise projection functions\" should be $proj_x()$. I have two questions about it.\n- Why the project step \u2460 in Figure 4 is necessary? For example, $d$, $g$, and $\\alpha$ are usually known numbers and will not be optimized. If so, why not directly optimize the patch in the 3D world?\n- $proj_x$ seems to be a basic spatial transformation and projection, so why is it a problem to be solved?\n\nFeel free to point it out if I have any misunderstandings."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563978313,
                "cdate": 1700563978313,
                "tmdate": 1700563978313,
                "mdate": 1700563978313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A1ysyWODf9",
                "forum": "3VD4PNEt5q",
                "replyto": "Ji7vbx0VqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to the Follow-up Question"
                    },
                    "comment": {
                        "value": "We are genuinely grateful for your thoughtful considerations, and we address the new questions below:\n\n**Q1: Why the project step \u2460 in Figure 4 is necessary?**\n\n**A1**: We agree that $d$, $g$ and $\\alpha$ (the so-called projection parameters) are numbers that will not be optimized, but these parameters are not the same for different scene images in the driving footage, especially for object-oriented attacks. In other words, $proj_x()$ is the projection function for a specific scene image $x$ and the projection parameters (e.g., $d_x$, $g_x$ and $\\alpha_x$ ) are also unique for $x$. However, the 2D patch image ($p$) optimized by us should be universally effective across frames, hence we need to use $proj_x()$ to project the 2D patch image $p$ onto each scene image $x$ sampled randomly from the dataset during optimization, simulating the patch's appearance on each scene image $x$ (i.e., $p_x = proj_x(p)$ in Equation 6). Step 1 is necessary to connect the 2D patch image $p$ optimized by us and the randomly sampled scene image $x$ that will be fed to the fusion model, as $p$ should be universally effective across various scene images. \n\n**Q2: why is it a problem to be solved?**\n\n**A2**: Despite the form of $proj_x()$ is a basic spatial transformation and projection, the projection parameters (e.g., $d_x$, $g_x$ and $\\alpha_x$) in $proj_x()$ for a specific scene image $x$ is dynamically extracted. We solved the problem of dynamically projecting the patch image $p$ onto the target object in randomly sampled scene images during the patch optimization process. This is accomplished by extracting these unique projection parameters for each scene image from the target object's 3D bounding box that is predicted by the fusion model in benign case.\n\nPlease kindly let us know if we have any misunderstanding about your questions. Thanks."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570610667,
                "cdate": 1700570610667,
                "tmdate": 1700571529849,
                "mdate": 1700571529849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZwZUK3zWHy",
                "forum": "3VD4PNEt5q",
                "replyto": "A1ysyWODf9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ],
                "content": {
                    "comment": {
                        "value": "The authors answer my question clearly. However, I still think \"extracting these unique projection parameters\" is trivial and can not be regarded as a technical issue.  The authors could make a defense for this.\n\nConsidering the image-only attacking is reasonable and possesses potential application value, I increased my rating to weak accept."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632685577,
                "cdate": 1700632685577,
                "tmdate": 1700632685577,
                "mdate": 1700632685577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gQ86WM58FY",
                "forum": "3VD4PNEt5q",
                "replyto": "Ji7vbx0VqJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank You for the Feedback"
                    },
                    "comment": {
                        "value": "We express our gratitude for your comments and reevaluation of our contribution. Although extracting those projection parameters may not be technically difficult, we think it is a necessary and novel step for the process of dynamic projection in our proposed optimization approach. Thank you for your valuable feedback and continuous help in making our work better."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645325802,
                "cdate": 1700645325802,
                "tmdate": 1700671164262,
                "mdate": 1700671164262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hMaycWLJmJ",
            "forum": "3VD4PNEt5q",
            "replyto": "3VD4PNEt5q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_QA2X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_QA2X"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach to attacking fusion models from the camera modality, which successfully compromises advanced camera-LiDAR fusion models and demonstrates the weaknesses of relying solely on fusion for defence against adversarial attacks. The proposed attack framework is based on a novel PointAug method that perturbs the point cloud data and generates realistic-looking adversarial examples. The experiments conducted in both simulated and physical-world environments show the practicality and effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.Sophisticated Camera-LiDAR Fusion Model: The proposed methodology exemplifies a commendable synthesis of advanced camera-LiDAR fusion models. This amalgamation not only taps into the inherent strengths of individual modalities but also crafts a synergistic fusion, ensuring that the combined system is more robust and efficient than its constituent parts in isolation.\n2.Efficacy of PointAug in Generating Adversarial Samples: An intrinsic highlight of the paper is the PointAug method, which adeptly fabricates realistic-looking adversarial examples. Such capability is pivotal, particularly in the realm of robust machine learning, as it enables researchers to thoroughly evaluate the resilience of models against potential adversarial threats.\n3.Rigorous Experimental Validation in Diverse Environments: The rigorousness and diversity of experiments set this work apart. By conducting evaluations in both simulated and real-world environments, the paper fortifies the assertion of the proposed approach's practicality and efficacy. This dual-pronged validation underscores the method's adaptability and reliability in a wide range of scenarios."
                },
                "weaknesses": {
                    "value": "1.Limited Fusion Model Efficacy: The methodology, while promising, seems to be narrowly tailored for a specific set of fusion models. This raises concerns about its universality. An in-depth exploration into its effectiveness against a broader spectrum of fusion models would have provided a more comprehensive perspective, allowing for a holistic understanding of its potential and pitfalls.\n2.Data Dependency and Generalizability Concerns: The experiments, predominantly based on a circumscribed dataset, cast doubts on the model's capacity to generalize across diverse scenarios. The exclusive reliance on a limited dataset can inadvertently introduce biases, thereby undermining the robustness of the approach when deployed in novel, real-world situations.\n3.Inadequate Security Analysis and Potential Resource Constraints: While the paper delves into several aspects of the proposed approach, it seems to sidestep a comprehensive analysis of its security implications. Given the pivotal role of security in such contexts, a detailed discourse would have been invaluable. Furthermore, the potentially substantial computational overhead required to generate adversarial samples may render the approach untenable for resource-constrained environments. The paper's omission of a thorough dissection of its inherent limitations further obscures the potential challenges one might encounter in its adoption."
                },
                "questions": {
                    "value": "1.Comparison with Pre-existing Methodologies: Given the emergence and evolution of methodologies targeting fusion models, how does the proposed approach position itself relative to these existing strategies? An analytical juxtaposition against established techniques would elucidate its uniqueness, advantages, and potential shortcomings.\n2.Defensive Countermeasures against the Approach: While the paper sheds light on an innovative adversarial approach, it begs the question: What are the viable defensive strategies that can be deployed to counteract its effects? Unveiling potential countermeasures not only underscores the resilience of the approach but also aids in the development of more robust fusion models.\n3.Extension and Scalability Concerns: Fusion models are diverse and multifaceted. How malleable is the proposed approach in its application to other fusion model variants? A deeper dive into its adaptability would provide insights into its scalability and flexibility across various fusion paradigms.\n4.Implications for Autonomous Vehicle Security: Given the pivotal role of fusion models in autonomous vehicle systems, the proposed adversarial approach inevitably raises safety and security concerns. How might these attacks compromise the integrity and reliability of autonomous driving systems? A comprehensive discussion on this would be crucial for stakeholders in the autonomous vehicle domain."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738028785,
            "cdate": 1698738028785,
            "tmdate": 1699636045678,
            "mdate": 1699636045678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z3DV3Zitkc",
                "forum": "3VD4PNEt5q",
                "replyto": "hMaycWLJmJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Part 1/2] Official Response to Reviewer QA2X (Q1, Q2, Q3 & Q4)"
                    },
                    "comment": {
                        "value": "We express our deepest gratitude for the reviewer\u2019s invaluable time and astute comments. It is our utmost pleasure to address the principal concerns as follows.\n\n**Q1: Limited Fusion Model Efficacy.**\n\n**A1**: Your thoughtful comments are appreciated. As we elaborated in Related Works, Discussion of Other Fusion Strategies (Appendix B) and Limitations (Appendix P), our research primarily focuses on data-level and feature-level fusion strategies. This focus is driven by the fact that advanced fusion models, which demonstrate superior detection performance, predominantly employ these fusion strategies [1]. These strategies are favored due to their enhanced feature extraction capabilities. The adoption of such early-fusion schemes is also a general trend with the increasing popularity of end-to-end autonomous driving [2-3]. Decision-level fusion, the other strategy, is less prevalent in recent fusion models. In such a setup, our camera-only attacks would only influence the output of the camera-based model. Therefore, the impact of such an attack is significantly dependent on the relative weights assigned to the two modalities during the fusion process. This dependency is a common limitation for single-modal attacks. While prior LiDAR-only attacks on Baidu Apollo have shown success [4], this can be attributed to Apollo's current higher weighting of LiDAR results. Enhancing the success rate of camera-only attacks against decision-level fusion models poses an open problem, and we acknowledge it as a promising area for future exploration and research. Thanks for your insightful comments, we have highlighted the fusion strategies we focused on in the introduction of our revised version and discussed more about decision-level fusion in Appendix B. \n\n**Q2: Data Dependency and Generalizability Concerns.**\n\n**A2**: We express gratitude for your valuable feedback. The Nuscenes dataset employed in our experiments is a real-world dataset encompassing diverse scenarios across driving conditions. Models trained on this dataset exhibit robust generalization to real-world and simulation scenarios, as scrutinized in the practicality section of our manuscript. The fusion model, trained with the Nuscenes dataset, successfully detects all objects near the victim vehicle in benign cases. This performance across the Nuscenes dataset, real-world conditions, and simulation environments underscores the generalizability and minimum data dependency of our fusion models. Furthermore, our attacks demonstrate success across three types of data, affirming the generalizability and robustness of our proposed attack approach.\n\n**Q3: Inadequate Security Analysis and Potential Resource Constraints.**\n\n**A3**: We appreciate your insightful comments and suggestions. In response, we have enriched the revised version of our manuscript with a more comprehensive analysis of autonomous vehicle security implications in Appendix R. To address potential resource constraints, we've introduced a detailed threat model for our attack in Appendix Q, outlining the assumptions made about attackers. Additionally, specific devices and the detailed computational overhead required for generating adversarial patches are disclosed in Appendix H, providing a thorough dissection of our approach's resource requirements and inherent limitations.\n\n**Q4: Comparison with Pre-existing Methodologies.**\n\n**A4**: Your comments are valued. In the Related Work section of the original manuscript, we have thoroughly discussed the relationship between our attack and prior attacks against fusion models. Succinctly, some previous attacks are multi-modal attacks that fool camera and LiDAR modalities either separately (with colors and shape) or concurrently (with shape). Others are single-modal attacks against only the LiDAR modality. In contrast, our approach pioneers a single-modal attack exclusively targeting the camera modality. As stated in the Introduction, prior LiDAR-related approaches pose implementation challenges, necessitating additional equipment like photodiodes, laser diodes, or industrial-grade 3D printers to manipulate LiDAR data, thereby increasing deployment costs for attackers. Our camera-only attacks, in comparison, are more affordable and easier to deploy, enabling attackers to print generated adversarial patches with home printers. This reduction in effort heightens the threat to autonomous vehicle security."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275894970,
                "cdate": 1700275894970,
                "tmdate": 1700275894970,
                "mdate": 1700275894970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rcMCkdVz9Y",
            "forum": "3VD4PNEt5q",
            "replyto": "3VD4PNEt5q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_qqbx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1194/Reviewer_qqbx"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the vulnerability of multi-sensor fusion to adversarial attacks in autonomous driving. The authors propose to leverage the adversarial patch to attack the camera modality in 3D object detection. Specifically, they propose an attack framework employing a two-stage optimization-based strategy that first evaluates vulnerable image areas under adversarial attacks, and then applies dedicated attack strategies for different fusion models to generate deployable patches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies an important concern in autonomous driving, i.e., the vulnerability of multi-sensor fusion to adversarial attacks.\n\n- Multiple feature-level fusion models are considered in this paper.\n\n- The performance of the proposed framework is evaluated through both simulated and real-world experiments."
                },
                "weaknesses": {
                    "value": "- This paper does not provide the threat model. What information is available to the attacker during the attack? How feasible is it for the attacker to access this information in a real-world setting? What are the attacker's capabilities?\n\n- The practicality and generalizability of the proposed attack are limited due to its ineffectiveness on decision-level fusion models, which are widely used in many autonomous driving systems, such as Baidu Apollo. Although the proposed attack can alter camera inputs, it fails to affect the outputs of decision-level fusion models. Moreover, these models tend to depend more on LiDAR detection results, further diminishing the practicality of the proposed attack.\n\n- I found it hard to understand the positioning of this paper. There are many existing works studying the attacks against camera-LiDAR fusion models [1,2]. These methods can be used to attack all three types of sensor fusion models including data-level fusion, feature-level fusion, and decision-level fusion. However, the method proposed in this paper can only be used to attack the first two types of fusion models. So, what is the major advantage of this work compared to those existing works? The authors should compare their method with existing attacks to demonstrate superiority of the proposed attack. \n\n[1] Exploring adversarial robustness of multi-sensor perception systems in self driving.\n[2] Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks.\n\n- The practicability of the adopted adversarial patch is questionable. Table 7 shows that the minimum dimensions of the patch are 1 meter by 1 meter, and the patch is too large to be practical. How to place such a large patch on a pedestrian in the real world? In addition, it's impractical to place such a large patch on the back of a vehicle. The patch may hide the license plate of the vehicle, which is prohibited by the traffic law. \n\n- The real-world evaluation is weak. The authors propose to use a patch with a special color pattern to conduct the attack. Such a color pattern can be affected by many factors in the physical world such as light condition, the distance between the camera and the patch, as well as the view angle of the camera. However, the authors do not evaluate the impact of these factors in the real-world setting. \n\n- The impact of the proposed attack on the vehicle's motion remains unclear. Is the perception system of the vehicle consistently deceived by the attack? Is the vehicle's trajectory affected by the attack?"
                },
                "questions": {
                    "value": "- What is the threat model. What information is available to the attacker during the attack? How feasible is it for the attacker to access this information in a real-world setting? What are the attacker's capabilities?\n\n- What is the major advantage of this work compared to existing attacks against camera-LiDAR fusion models?\n\n- How to place the adversarial patch on a pedestrian in the real world (as shown in Figure 10)? How to place the patch on the back of a vehicle? Does the patch hide the license plate of the vehicle?\n\n- Does the proposed attack maintain its effectiveness under varying light conditions in the real world?\n\n- The impact of the proposed attack on the vehicle's motion remains unclear. Is the perception system of the vehicle consistently deceived by the attack? Is the vehicle's trajectory affected by the attack?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The experiment in this paper involves human subjects, but the authors do not report ethical approvals from an appropriate ethical review board."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806947109,
            "cdate": 1698806947109,
            "tmdate": 1699636045608,
            "mdate": 1699636045608,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ayINnwVnPq",
                "forum": "3VD4PNEt5q",
                "replyto": "rcMCkdVz9Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Part 1/3] Official Response to Reviewer qqbx (Q1, Q2 & Q3)"
                    },
                    "comment": {
                        "value": "We express our deepest gratitude for the reviewer\u2019s  time and  insightful comments. It is with great respect that we have endeavored to address the primary concerns as follows.\n\n**Q1: What is the threat model of the proposed attack?**\n\n**A1**: We sincerely appreciate your insightful question. Our attack assumes that the attacker has complete knowledge of the camera-LiDAR fusion model used by the target autonomous driving vehicle. Therefore, our attack model is considered to be in a white-box setting. This assumption is congruent with existing endeavors in the literature dedicated to adversarial attacks on autonomous driving systems [1-5]. To achieve this level of knowledge, the attacker may employ methods such as reverse engineering the perception system of victim vehicles [6], employing open-sourced systems, or exploiting information leaks from insiders. In addition, the attacker is clear about the target object and driving scenes he wants to attack. For example, he can record both video and LiDAR data using his own vehicle and devices while following a target object (for object-oriented attacks) or stopping at a target scene (for scene-oriented attacks). Leveraging this pre-recorded data, the attacker undertakes a one-time effort to generate an adversarial patch using our proposed approach. The attacker can then print and deploy the generated patch on the target object or in the target scene for real-time attacks against victim vehicles. The detailed elaboration of our threat model can be found in the appendix Q of our manuscript. \n\n**Q2: The method\u2019s ineffectiveness on decision-level fusion models.**\n\n**A2**: We express our gratitude for your thoughtful comments. As we elaborated in Related Works, Discussion of Other Fusion Strategies (Appendix B) and Limitations (Appendix P) sections, our research primarily focuses on data-level and feature-level fusion strategies. The decision-level fusion is less prevalent in recent fusion models. In such a setup, our camera-only attacks would only influence the output of the camera-based model. Therefore, the impact of such an attack is significantly dependent on the relative weights assigned to the two modalities during the fusion process. This dependency is a common limitation for single-modal attacks. Although prior LiDAR-only attacks have succeeded on Baidu Apollo [10], it can be attributed to the current higher weighting of LiDAR results in Apollo. Our focus on data-level and feature-level fusion is driven by the fact that advanced fusion models, which demonstrate superior detection performance, predominantly employ these fusion strategies [7], due to their enhanced feature extraction capabilities. The adoption of such early-fusion schemes is also a general trend with the increasing popularity of end-to-end autonomous driving [8-9]. The challenge of enhancing the success rate of camera-only attacks against decision-level fusion models remains an open question, and we duly acknowledge this as a promising area for future exploration and research. Based on your comments, we have highlighted the fusion strategies we focused on in the introduction of our revised version and discussed more about decision-level fusion in Appendix B. \n\n**Q3: The positioning of this paper. What is the major advantage of this work compared to existing attacks against camera-LiDAR fusion models?**\n\n**A3**: We are genuinely grateful for your thoughtful considerations. In the Related Work section of the original manuscript, we have discussed the interplay between our work and the works you referenced. In essence, the referenced works encompass multi-modal attacks that deceive  camera and LiDAR modalities either individually (with colors and shape) or concurrently (with shape). In contrast, our focus is on single-modal attacks directed exclusively at the camera modality. As stated in the introduction, prior approaches, as you indicated, present implementation challenges due to additional equipment requirements, such as photodiodes, laser diodes [10], or industrial-grade 3D printers [1], for manipulating LiDAR data. This increases deployment costs for attackers. In contrast, our camera-only attacks offer enhanced affordability and ease of deployment, enabling attackers to print the generated adversarial patch with home printers. This reduction in effort amplifies the threat to autonomous vehicle security."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274539256,
                "cdate": 1700274539256,
                "tmdate": 1700274539256,
                "mdate": 1700274539256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "utxBmXzVGA",
                "forum": "3VD4PNEt5q",
                "replyto": "rcMCkdVz9Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Part 2/3] Official Response to Reviewer qqbx (Q4 & Q5)"
                    },
                    "comment": {
                        "value": "**Q4: The practicability of the adopted adversarial patch is questionable. How to place the patch on the back of a vehicle? Does the patch hide the license plate of the vehicle? How to place the adversarial patch on a pedestrian in the real world (as shown in Figure 10)?**\n\n**A4**: We sincerely appreciate your inquiries. Allow us to clarify that the 1m * 1m is not the minimum patch size required for attacks. The minimum patch size necessary for a successful attack is contingent upon the distance between the target object and the victim vehicle. The critical factor is the pixel count influenced by the patch in the input images, and a smaller patch can exhibit efficacy when in close proximity to the victim vehicle.  Table 5 in our study demonstrates that the attack performance of adversarial patches diminishes with increasing distances. Our additional exploration of object-oriented attacks, considering patch size, shape and varying distances between target and victim vehicles, is detailed in Table R1. As shown, the 0.5m * 0.5m patch still performs well at a closer distance (e.g. less than 8m). It is crucial to underscore that failure to detect the target object at a close range is still hazardous, as the driver may lack sufficient reaction time, leading to a braking distance exceeding the actual distance.  \n\nAdditionally, the patch does not need to be in square shape. Attackers have the flexibility to define patches with rectangular or arbitrary shapes to circumvent covering the target vehicle's license plate. \n\nFigure 10 in Appendix G (Property Validation of Sensitivity Heatmap) aims to demonstrate the property of the sensitivity heatmap instead of real-world attacks against pedestrians. Attackers may use a smaller patch on T-shirts (like [11]) to attack the pedestrians at a closer distance (e.g. 5 m). \n\nThe additional experiments and explanations have been added to the revised manuscript in Appendix M. \n\nTable R1: Detection score of the target object under different distances and patch sizes. \n\n| **Distance** |  | 1m$\\times$1m | | 1m$\\times$0.5m | | 0.5m$\\times$0.5m | |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| | **Ben.** | **Adv.** | **Diff.$\\uparrow$** | **Adv.** | **Diff.$\\uparrow$** | **Adv.** | **Diff.$\\uparrow$** |\n| **5m** | 0.711 | 0.074 | **89.59%** | 0.081 | **88.61%** | 0.146 | **79.47%** |\n| **6m** | 0.726 | 0.112 | **84.57%** | 0.108 | **85.12%** | 0.144 | **80.17%** |\n| **8m** | 0.682 | 0.105 | **84.60%** | 0.137 | **79.91%** | 0.191 | **71.99%** |\n| **10m** | 0.643 | 0.127 | **80.25%** | 0.242 | **62.36%** | 0.402 | **37.48%** |\n| **15m** | 0.655 | 0.146 | **77.71%** | 0.48 | **26.72%** | 0.626 | **4.43%** |\n\n**Q5: The real-world evaluation is weak.**\n\n**A5**: We express our gratitude for your observation. In Section 4, our approach accounts for diverse lighting conditions, distances, and viewing angles during the patch optimization process. This is achieved by randomizing brightness, contrast, and projection parameters of the patch and incorporating Estimation of Transformations (EoT) in training.  The evaluation of attack performance under varied distances and viewing angles is meticulously presented in Appendix M. To assess the practicality of our attacks, we conducted experiments in both real-world and high-fidelity simulator environments following the practical attack procedures. Responding to your insightful suggestion, we augmented the practicality evaluation by conducting additional experiments at different times of the day to simulate lighting changes. The experimental settings remain consistent with those detailed in Appendix K, and the results are presented in Table R2.  The second and third columns indicate the benign and adversarial performance of BEVFusion-PKU respectively. The fourth column presents the percentage of model performance degradation, indicating our attack effectiveness. As shown, our patch remains a robust attack performance at different lighting conditions. \n\nTable R2: Attack performance at different times of the day with various lighting conditions.\n\n| Time of the day | Ben. AP | Adv. AP | Difference $\\uparrow$ |\n|:---------------:|:---------:|:--------------:|:----------:|\n|     **9:00 AM**     |   0.428   |      0.191     | **55.37%** |\n|     **12:00 PM**    |   0.457   |      0.184     | **59.74%** |\n|     **3:00 PM**     |   0.481   |      0.165     | **65.70%** |\n|     **6:00 PM**     |    0.43   |      0.204     | **52.56%** |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274669068,
                "cdate": 1700274669068,
                "tmdate": 1700277164168,
                "mdate": 1700277164168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oBpxDLxHus",
                "forum": "3VD4PNEt5q",
                "replyto": "utxBmXzVGA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_qqbx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_qqbx"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions"
                    },
                    "comment": {
                        "value": "Thanks for the authors' feedback. The following are some follow-up questions.\n\n- For an autonomous vehicle equipped with an advanced planning module, would it initiate actions such as changing lanes or braking upon detecting an object at a distance equal to or greater than 8 meters?\n\n- I think light conditions in the physical world are more complex. Does the proposed attack maintain its effectiveness under varying lighting conditions in a physical environment?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541467955,
                "cdate": 1700541467955,
                "tmdate": 1700541467955,
                "mdate": 1700541467955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]