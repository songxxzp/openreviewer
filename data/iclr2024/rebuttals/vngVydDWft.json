[
    {
        "title": "From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication"
    },
    {
        "review": {
            "id": "soeIhxPpej",
            "forum": "vngVydDWft",
            "replyto": "vngVydDWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about enhancing the relative representation. Relative representation is determined with dissimilarity measure between target data and anchor that is invariant to angle transformation. The former work of Moschella et al. (2022) uses cosine angle as this dissimilarity, but in this paper, it aggregates other dissmilarity to enhance latent communication. The results of this aggregation is assessed by accuracy of zero-shot classification using stiching models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper gives evidences that why the relative representation only using cosine angle can be inappropriate."
                },
                "weaknesses": {
                    "value": "1. The definition of the RR framework is strange. It is stated that RR is concatenation of $d(z, a_i)$, but $z$ and $a_i$ should be in different domain $(\\mathcal{Z},$ and $\\mathcal{X})$. I am assuming that the anchors are also encoded with $E_\\theta$ so that $a_i$'s in the latent space $\\mathcal{Z}$\n\n2. The experiments setting in the section 4 is unclear. I am having trouble figuring out what is a stiching model for this downstream task and how it is trained. I am assuming it is the same definition as the stiching model defined in Moschella et al. (2022), but I am having trouble how the decoder for this down-stream task is (pre-)tained.\n\n3. The enhancement of relative representation through aggregating is not convincing for me. In Table 2, the aggreagated accuracies closely matches with using $L_1$ encoder. Using MLP or SelfAttention in aggreagation does not seems to be fair in that it requires an addtional training to get the additional parameters for these layers (correct me if I am wrong.)"
                },
                "questions": {
                    "value": "1. In experiment in Section 4.3 ~ 4.4, is the MLP and self-attention aggregation trained (fine-tuned) in end-to-end fashion? \n\n2. Does the downstream task with relative representation presented also enhance the performance of other tasks? (e.g. generation)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599692625,
            "cdate": 1698599692625,
            "tmdate": 1700668627509,
            "mdate": 1700668627509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s7Ul9uAjXo",
                "forum": "vngVydDWft",
                "replyto": "soeIhxPpej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our appreciation to the reviewer for their valuable comments and inquiries. We would like to provide clarifications on the following points:\n\n&nbsp;\n\nWeakness 1 (**RR definition**): We sincerely thank the reviewer for highlighting that the way we formulated RR contains an abuse of notation. The corrected version has been incorporated into the PDF revision. \n\n&nbsp;\n\nWeakness 2 (**Model-stitching definition**): We expanded the description of the stitching procedure, dedicating a specific paragraph after the experimental setting (Section 4.2) and Figure 10 describing the stitching procedure. The revised version is accessible in the PDF revision.\n\n&nbsp;\n\nWeakness 3: \n- **Paper aim**: Our paper aims to explore and understand the transformations that relate different latent spaces, introducing a method to achieve latent communication without relying on prior knowledge of the optimal invariance to incorporate. As further clarified in the [general response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv), the purpose is not to enhance relative representation through an aggregating mechanism. \n- **Additional training**:  Our methodology does not require additional training in the stitching procedure. We have added a paragraph in Section 4.2 and Figure 10 to clarify this important aspect of our method. For further details, please refer to the response to Question 1 and the [General Response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv).\n\n&nbsp;\n\nWe would also like to reply to the following questions:\n\nQuestion 1 (**Fine-tuned vs end-to-end**): All the downstream experiments are performed in a zero-shot way (including Section 4.3), the learnable parameters are exclusively trained during the initial, end-to-end network training phase and are then employed in their trained frozen state in the stitched model (as part of the decoder model). The only exception to this setting is the one in Section 4.4, \u201cSubspace selection\u201d, where we want to assess the possible gains in integrating additional information into the subspace selection process. For further details, please refer to the [general response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv), Section 4.2, and Figure 10 of the revision.\n\n&nbsp;\n\nQuestion 2 (**Other downstream tasks**): To assess the applicability of our method to other downstream tasks, we conducted the **zero-shot generation task on CIFAR100**. Qualitative (Figure 14) and quantitative results (Table 24) demonstrate that our approach, incorporating all invariances (*Cos, Euc, $L_1$, $L_\\infty$*) with the MLP+Sum aggregation (same setting of all the other experiments), consistently equals or exceeds the performance of the best single-invariance model. We would like to remark that this achievement is accomplished without prior knowledge of the most effective invariance and without any training or fine-tuning involved after assembling the pre-trained models into the stitched model. Results can be found in the PDF revision.\n\n&nbsp;\n\n---\n\nWe hope to have clarified the reviewer's concerns and remain available to discuss any further concerns or questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407390013,
                "cdate": 1700407390013,
                "tmdate": 1700407390013,
                "mdate": 1700407390013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4CPaGTN34L",
                "forum": "vngVydDWft",
                "replyto": "soeIhxPpej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their efforts on the rebuttal and kind responds. My questions are all cleared up and I adjusted the score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668716413,
                "cdate": 1700668716413,
                "tmdate": 1700668716413,
                "mdate": 1700668716413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9rWxldKVkd",
            "forum": "vngVydDWft",
            "replyto": "vngVydDWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a product projection mechanism to generalize the framework of relative representation. In particular, the authors incorporate a set of invariances into the representation by constructing a production space of invariances. The findings are intuitive that multiple projections behave differently across different choices on initialization, model architecture, etc. Experimental results proved the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The motivation is well presented of infusing multiple invariances into relative representation.\n\n(2) The explanations and illustrations are mostly clear and intuitive of the manifold assumption and the product projection mechanism."
                },
                "weaknesses": {
                    "value": "(1) On Page 5, the result analysis presents the discovery challenges the assumption in Moschella et al. (2022). More explanations are required to make this point clear. Besides, wondering if the experimental results are just a normal fluctuation due to different runs.\n\n(2) On Page 6, the authors used 1280 randomly selected but fixed anchors. This is also a kind of randomness that is not explained away. In fact, for different choice of anchors, the sensitivity is different of the projection and measure function.\n\n(3) On the experiments, the employed datasets and models are in small-scale and probably prone to overfitting issues. Do the analysis conclusions hold for large-scale models such as Stable Diffusion and GPT? There should be large-scale results to support the findings."
                },
                "questions": {
                    "value": "No."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740438593,
            "cdate": 1698740438593,
            "tmdate": 1699637016448,
            "mdate": 1699637016448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "51s8GmOjur",
                "forum": "vngVydDWft",
                "replyto": "9rWxldKVkd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing valuable feedback, and we would like to offer further clarification on the following points:\n\n&nbsp;\n\nWeakness 1 (**Runs fluctuation**): To ensure the robustness of our experiments, we deliberately employed a minimum of three different random seeds. We selected this limit not only to validate the reliability of our experiments but also to assess the applicability and generalizability of our method across a broader range of experiments.\n\n&nbsp;\n\nWeakness 2 (**Anchor selection**): When referring to \"randomly but fixed anchors,\" we mean that the anchors are randomly sampled uniformly from the training set but with a fixed seed value. In this regard, we **conducted an ablation study on the number of randomly selected anchors** for the stitching task using CIFAR100 across three different anchor seeds. The results reveal that varying the number of anchors leads to different transformations, indicating that a single projection function cannot incorporate the desired invariance. In contrast, our method enables the attainment of the highest score regardless of the number of anchors or the seed value employed for the uniform random sampling of anchors. We have introduced a new section (A.7) in the PDF revision for additional details, including Table 10 and Figure 9.\n\n&nbsp;\n\nWeakness 3 (**Large scale datasets and models**): In our work, we intentionally aligned our choice of models with those used in Moschella et al. (2022), which are widely recognized in the field for their embedding capabilities [1,2,3,4,5,6,7,8,9,10,11]. While these models do not match the large-scale scope of systems like Stable Diffusion or GPT, they are nonetheless significant for their demonstrative value in capturing emerging behaviors. This decision is further substantiated by existing literature, which suggests that even super large-scale models exhibit similar phenomena of emergent similarities [12, 13, 14, 15]. \nAdditionally, in this revision, we measure the performance of the proposed model on the ImageNet1k dataset, which is larger than the other employed datasets. Results in Table 23 show that the proposed methodology utilizing the *Aggregation by sum* consistently matches or outperforms the others.\n\n&nbsp;\n\n---\n\nWe hope that our responses address all the concerns raised by the reviewer. Additionally, we have included supplementary experiments on ImageNet and reconstruction tasks, detailed in the [General Response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv). If further clarification is needed or if there are additional concerns, we are more than willing to engage in further discussion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407235567,
                "cdate": 1700407235567,
                "tmdate": 1700407235567,
                "mdate": 1700407235567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "msRxxU1eGX",
                "forum": "vngVydDWft",
                "replyto": "9rWxldKVkd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors's response and the concerns are addressed clearly, thus I will keep the score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706775898,
                "cdate": 1700706775898,
                "tmdate": 1700706775898,
                "mdate": 1700706775898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZyzP7Cvpb3",
            "forum": "vngVydDWft",
            "replyto": "vngVydDWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the work on Relative Representation by ensembling multiple relative representations obtained by different distances. The combination of four distances cos, Euc, L1, $L_\\infty$ and three ensemble methods concat, sum, and attention are explored in the text. Extensive experiments across text, graph, and vision domains demonstrated that the ensembled version can improve the performance of zero-shot stitching."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Ensembling multiple relative representations is a reasonable idea and it enhances the power of the original cosine relative representation.\n2. The experiments are extensive. There are 28 tables including the appendix.\n3. The writing style is formal."
                },
                "weaknesses": {
                    "value": "1. The selection of distances seems arbitrary.\n    - (a) While the Euclidean distance is invariant under the Euclidean isometries and is a reasonable candidate beyond the Cosine distance. What is the rationale of the rest of the distances? Any geometric intuitions?\n    - (b) The Euclidean isometry is a special case of conformal (angle-preserving) map. For experiments that show better performance on Euclidean distances than on Cosine distances, what can we say about the underlying symmetries of the neural representation? Does it mean that that latent space contain less invarinace? I am asking this question because I want to see what extra understanding on neural representations we can get from this new formulation.\n    - (c) Page 2 \"... which, combined, can capture arbitrary complex transformations of the latent space\". It seems an overstatement to claim that the four chosen distances can capture \"arbitrary complex transformations\".\n2. The Assumption in page 3 does not read smoothly. \n    - (a) The equivalence class of encoders is defined as the set of E such that $ \\pi_\\mathcal{M}TE=\\pi_M E, \\forall T\\in\\mathcal{T}$. This definition is confusing. I fail to see why it is an equivalence class. For example, say, $\\mathcal{T}_1$ is scalings, and $\\mathcal{T}_2$ is rotations, and $E$ is a constant mapping to the origin. Does $\\mathcal{T}_1$ and  $\\mathcal{T}_2$ induces two different equivalence classes of transformations? But clearly $E$ belongs to both classes of transformations.\n    - (b) Suppose $\\mathcal{M}$ is a single point. Then $\\pi_{\\mathcal{M}}TE=\\pi_{\\mathcal{M}}E$ for all $E$ and all $T$. This definition does not contain any useful info then.\n3. Page 5 \"it is not possible to connect latent spaces of models with different initializations ...\" It seems that the Pearson correlation for Cosine is higher than 0.94 in the left subfigure of Fig. 3 and higher than 0.8 in the right subfigure. What is the criterion for the statement of \"no connection\"? Any reference for the choice of criterion? I do not see this as \"challenges the assumption in Moschella et al.\".\n4. Please clarify the aggregation used in Tab. 1 to Tab. 3, since there are multiple possibilities.\n5. Sec. 4.4 leads confusion. What is the difference between SelfAttention and SelfAttention + MLP opt? Isn't the SelfAttention trained (finetuned)? If not, what is the exact computation formula for the SelfAttention aggregation? Where is the initial values of the attention weights come from? Also, the numbers in Tab. 5 does not match that of in Tab. 15."
                },
                "questions": {
                    "value": "See weakness section.\n\ntypo:\n\n1. page 4, invriances\n2. page 9, fourth row -> fifth row"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE",
                        "ICLR.cc/2024/Conference/Submission8194/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835798191,
            "cdate": 1698835798191,
            "tmdate": 1700652345904,
            "mdate": 1700652345904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FuTt3KR9iq",
                "forum": "vngVydDWft",
                "replyto": "ZyzP7Cvpb3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, highlighting the extensive experiments and the formal writing style. We corrected the suggested typos, and we welcome the opportunity to address the raised concerns and offer clarifications on specific points:\n\n&nbsp;\n\nWeakness 1 (**Arbitrary complex**): \n- (a) **Arbitrary distances and geometric intuition**: The distances function chosen enforces invariances to distinct fundamental groups of transformations (e.g., orthogonal, conformal, and isometry groups with respect to different metrics: see Table 6 in the appendix for a complete picture). The aim is to combine the different sets of invariances as building blocks to capture more complex unknown transformations acting on real data. Each distance function will have its own specific properties: for instance, the choice of $L_{\\infty}$ provides invariances w.r.t.  the Chebyshev transformations, resulting in robust to bounded perturbations of the points. In general, our approach could employ arbitrary similarity functions: characterizing from a theoretical perspective which classes of transformations can be captured using the employed invariances as building blocks is a promising future direction.\n- (b) **Conformal map**: The angle-preserving transformation we consider corresponds to the conformal orthogonal group:  its elements are compositions of global orthogonal transformations and local rescalings, which depend on each coordinate. The cosine distance is invariant to this group of transformations. The Euclidean isometry group is not properly a subset of this group, as it contains translations, while the former isn\u2019t (similarly, cosine distance is invariant to local scaling, while euclidean distance isn\u2019t). Therefore we attribute the difference in performance to the fact that these two distance functions retain different invariances. To avoid confusion, we incorporated a precise definition of orthogonal conformal transformations in the manuscript. \n- (c) **Statement**: We will re-modulate the statement in the paper and make it less bold. Still, our framework can capture transformations that the single latent spaces can\u2019t capture (see qualitative example reported in Figure 14) where the compositions of all spaces lead to the best qualitative reconstruction), hinting at a major expressiveness in combining single invariances. In general, our framework could easily handle additional invariance other than the one considered in the paper, and as highlighted in the previous answer, it\u2019s a promising direction to characterize theoretically which classes of transformations can be captured by combining different invariances as building blocks.\n\n&nbsp;\n\nWeakness 2 (**Assumption**): We will incorporate in the assumption specifics that avoids degenerated case, as the one presented by the Reviewer.\n- (a) The definition of the equivalence class of encoders depends entirely on the projection $\\pi_\\mathcal{M}$. In the example provided by the reviewer, if the encoder maps everything to the origin, still the projection will be the identity in this case, and the equivalence class will be with respect to the class of transformations $\\mathcal{T}_1 \\cup \\mathcal{T}_2$. This is a degenerate case since the encoder will map everything to a constant; however, our definition still applies.  To the best of our understanding, this should address the reviewer's concern. If we missed something, we remain available for further clarification.\n- (b) The case where $\\mathcal{M}$ corresponds to a single point is degenerate. Our definition still applies, but we agree with the reviewer that treating this specific case is not of great interest. $\\pi_\\mathcal{M}$ should ideally be a map invariant to all transformations $T \\in \\mathcal{T}$, but still retaining enough information in the latent spaces in order to solve the task at hand. \n\n&nbsp;\n\nWeakness 3 (**RR challenges**):  We have revised the sentence to clarify our statement. We recognize that invariances differ, and consistently depending on the cosine measure, might not be the optimal choice, even though it is sufficient.\n\n&nbsp;\n\nWeakness 4 (**Employed aggregation**): We have added a statement in the \"Aggregation functions\" section specifying the aggregation function utilized in the experiment, namely the SumAggregation (MLP+Sum).  Additionally, details have been included in the table's caption for clarity. The revised version is accessible in the PDF revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407170249,
                "cdate": 1700407170249,
                "tmdate": 1700410181013,
                "mdate": 1700410181013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eVK9Z3vlMY",
                "forum": "vngVydDWft",
                "replyto": "ZyzP7Cvpb3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response [2/2]"
                    },
                    "comment": {
                        "value": "Weakness 5 (**Subspace selection Sec. 4.4**): We have incorporated additional details into the section, and we hope the following clarification provides a better understanding of our experiments:\n- **Experimental details**: Section 4.4 focuses on utilizing the Self-attention aggregation described in the method section under \"Aggregation functions,\" which involves a single self-attention layer. The distinction lies in using \"SelfAttention\" to denote our classic method, where a self-attention layer aggregates different projection functions. On the other hand, when referring to \"SelfAttention+... opt\" we indicate that there is a fine-tuning procedure at stitching time, which is unique to this particular experiment. In particular, \"SelfAttention+MLP opt\" indicates the fine-tuning is performed on the whole classifier head; meanwhile, \"SelfAttention+QKV opt\" indicates only the Query, Key, and Value are fine-tuned. This is an analysis to understand if we can select an optimal subspace (i.e., perform subspace selection) at a reasonable cost at stitching time. Please refer to the [general response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv) for further details on the zero-shot stitching procedure.\n- **Table results**: The results do not match because we utilized a more expressive classifier in this experiment. We chose this approach to establish a less favorable scenario and illustrate that our method still performs better than using a single projection function. In our opinion, this is a less favorable scenario since we are fine-tuning more parameters in the \"SelfAttention+MLP opt\" setting.\nFollowing the reviewer's suggestion, we have moved the deep classifier results to Appendix A.6 for a more accurate comparison. In the main paper, we now showcase the results obtained with a simpler classifier, aligning with the data in the full appendix table and with the classifier adopted in all the other experiments. The updates are already present in the PDF revision.\n\n&nbsp;\n\n---\n\nWe hope that our responses have addressed the concerns raised by the reviewer and that the modifications made to the paper have resolved any doubts. Additionally, we have included supplementary experiments on ImageNet and reconstruction tasks, detailed in the [General Response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv). We are open to further discussions and available to address any additional concerns or questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407188857,
                "cdate": 1700407188857,
                "tmdate": 1700410436873,
                "mdate": 1700410436873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WVyqtGx0AQ",
                "forum": "vngVydDWft",
                "replyto": "eVK9Z3vlMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. It addresses most of the questions, except for the definition of \"equivalent class\". My understanding of the equivalent class is per the definition in https://en.wikipedia.org/wiki/Equivalence_class, that the equivalence classes form a partition of all the encoders. In the paper's definition, the equivalent class is parametrized by manifold M and transformation T. I do not think different choice of M or T would results in non-overlapping subsets for the set of all encoders."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635744001,
                "cdate": 1700635744001,
                "tmdate": 1700635744001,
                "mdate": 1700635744001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iQf37bK84f",
                "forum": "vngVydDWft",
                "replyto": "ZyzP7Cvpb3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for their insightful comments and are pleased to note that their concerns have been adequately addressed.\n\nRegarding the definition of equivalence class:\n\nWe concur with the reviewer's observation that varying $\\mathcal{M}$ and $\\mathcal{T}$ may identify overlapping sets of encoders (for instance, considering $\\mathcal{T}_1$ as a subset of $\\mathcal{T}_2$, such as $\\mathcal{T}_1$ being rotations and $\\mathcal{T}_2$ being orthogonal transformations).\n\nHowever, our intention was to define the equivalence class as identified by the projection $\\pi_\\mathcal{M}$, i.e., fixing $\\mathcal{M}$ and $\\mathcal{T}$. In this context, for a specific $\\pi_\\mathcal{M}$, the set of possible encoders are partitioned in those invariant to $\\mathcal{T}$ on $\\mathcal{M}$ and those not. These two sets are disjoint by construction. \n\n\nDifferent selections of $\\mathcal{M}$ and $\\mathcal{T}$ will lead to identifying distinct equivalence classes. We will clarify this point and refine the definition accordingly in our manuscript.\n\nWe remain available for further clarifications and kindly ask the reviewer to reconsider their evaluation in light of their belief that their concerns have been correctly addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641963909,
                "cdate": 1700641963909,
                "tmdate": 1700642098068,
                "mdate": 1700642098068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SE4YioDzjU",
                "forum": "vngVydDWft",
                "replyto": "iQf37bK84f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
                ],
                "content": {
                    "title": {
                        "value": "Follow up discussion"
                    },
                    "comment": {
                        "value": "Yeah makes sense."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652324736,
                "cdate": 1700652324736,
                "tmdate": 1700652324736,
                "mdate": 1700652324736,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W8uXzoSDcb",
            "forum": "vngVydDWft",
            "replyto": "vngVydDWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_ay1h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8194/Reviewer_ay1h"
            ],
            "content": {
                "summary": {
                    "value": "This paper expands on Relative Representations by allowing several distances to be combined, which allows incorporating additional invariances in the resulting representation.\n\nThey first present evidence that single distances aren\u2019t sufficient as they are data/model dependent (the original work used Cosine distance), they then explore adding 3 new distances (Euclidean, L1, L_\\infty) in Text, Image and Graph domains.\n\nThis is a rather simple yet very clear and well-executed paper, which brought back Relative Representations to my attention, a nice idea which got washed up in the recent wave of LLM excitement. It might have limited scope, but currently I lean favorably."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper has a clear focus, presents the problem well, and is overall extremely well executed. \n2. It was easy to follow and the extensions to the math were very well brought up.\n3. It explores appropriate choices of distances and aggregations. Good details and interpretations were provided for what one should expect from them (e.g. Table 6 in the Appendix was exactly what I was looking for)\n4. Results are clear and do improve in predictable fashion over baselines."
                },
                "weaknesses": {
                    "value": "1. I feel like too much of the paper is spent on presenting evidence for the sub-optimality of single distance relative representations. I did not really understand why that point was made so repeatedly (Figure 1, Figure 3, Figure 4, Appendix Figure 8 and 9), instead of spending more time presenting different *combinations* of distances and their benefits/implications. In effect Table 1 is the first time a clear combination of distances is shown, and it is clearly better than the rest, so I would have wanted more of that.\n2. Equally, as a result, less emphasis and space was spent explaining the results in 4.2, 4.3 and 4.4. I had to go back to the original paper to remember/understand what \u201czero-shot stitching\u201d meant and how it was implemented.\n3. Details were lacking in a few places, for example which aggregation function was used for most of the results. I assume MLP+sum given it performed the best in Section 4.3, but this isn\u2019t spelt out?\n4. Section 4.4 is also lacking in details and could benefit from some improvements, see below.\n5. It is potentially of limited scope, but I would defer to the majority vote to see if that is a blocker or not."
                },
                "questions": {
                    "value": "1. Do you really need to spend that much space and energy on presenting the failures of single distance Relative Representations? \n   1. Figure 1, 3 and 4 are all making a similar point, and Section 4.1 does not feel as crucial as its length suggests it.\n   2. I would probably recommend re-balancing this down and using the extra space to expand on the other Results sections.\n   3. I would recommend keeping either 3 or 4 in the main text but not both.\n   4. I am not sure that Figure 1 is the best framing figure to open the paper with, I might prefer to start with Figure 2.\n2. The aggregations functions are presented well in Section 3, but it would have been useful to present implications for the choices of Sum and SelfAttention, in a similar manner to Concat (\u201cgiving to M the structure of a cartesian product space\u201d).\n   1. The Sum aggregation is actually a DeepSet by implementation. I would have liked having this spelt out explicitly and discussed?\n3. The choice of Anchor points A_X and their implications on the invariances or properties of the relative representations are not discussed.\n   1. Section 4.2 mentions using 1280 randomly selected fixed anchors. Did you try changing it? Does it affect distances differently?\n4. I could not find which aggregation function was used for results in Table 1, 2 and 3. This should be specified clearly.\n5. It feels like showing other combinations of distances (instead of \u201csingle\u201d vs \u201call\u201d) would have been helpful, especially if different domains require different distances.\n   1. Section 4.4 tries to go in that direction, but the Transformer aggregation is not the best one and combined with my issue 4, I wasn\u2019t sure what you used, so it muddles the results.\n6. Section 4.4 would benefit from being extended, I do not think it contains enough details currently.\n   1. The experimental setup needs more details, there is no description of the transformer aggregation anywhere I could find.\n   2. Table 5 should contain the value for the best other aggregation (e.g. MLP+sum?), as currently it makes it harder to see if QKV opt is sufficiently accurate or not.\n   3. It is unfortunate that the Transformer aggregation performs poorly. It would be good to bring the MLP+Transformer one to the main text, or at least present more clearly what model is used. It is not my expectation that a DeepSet should outperform a Transformer if it has enough layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858607788,
            "cdate": 1698858607788,
            "tmdate": 1699637016185,
            "mdate": 1699637016185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2MDOa8oRgO",
                "forum": "vngVydDWft",
                "replyto": "W8uXzoSDcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8194/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, highlighting the clear focus, presentation, and extremely good execution. We welcome the opportunity to address the raised concerns and offer clarifications on specific points:\n\n&nbsp;\n\nWeaknesses 1, 5 & Question 1 (**Work scope and space for presenting RR failures**): It is crucial to emphasize that our paper's primary goal is to delve into and comprehend the transformations that connect different and independent latent spaces. Building upon the observations that emerged with the analysis, we aim to achieve latent communication without prior knowledge of the optimal invariance to incorporate. The extensive discussion in the paper was undertaken as a first step to study the emerging similarities between latent spaces empirically. Please refer to the [General Response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv) for further discussion on our work scope.\n\n&nbsp;\n\nWeakness 2 (**Model-stitching definition**):  We expanded the description of the stitching procedure, dedicating a specific paragraph after the experimental setting (Section 4.2) and Figure 13 describing the stitching procedure. The revised version is accessible in the PDF revision.\n\n&nbsp;\n\nWeakness 3 & Question 4 (**Employed aggregation**): We have incorporated a statement in the \"Aggregation functions\" section specifying the aggregation function used in the experiments. Additionally, details have been included in each caption for clarity. The revised version is accessible in the PDF revision.\n\n&nbsp;\n\nWeakness 4 & Question 6 (**Subspace selection details Sec. 4.4**): We updated the section with additional details:\n- **Self-attention aggregation**: Section 4.4 focuses on using the Self-attention aggregation described in the method section under \u201cAggregation functions\u201d, which is a single self-attention layer. We want to remark that this is the only case in which we perform stitching-time fine-tuning since we fine-tune the Q, K, and V parameters responsible for blending the subspaces. The stitching is done zero-shot in all the other experiments, as detailed in sec 4.2. All the details were included in the PDF revision.\n- **Self-attention aggregation**: We have included the results using the MLP+Sum (DeepSet) aggregation mechanism in Table 5. We believe this addition enhances the comprehensiveness of our study.\n- **Self-attention results**: We acknowledge your doubt about DeepSet outperforming transformer architectures. However, in our case, it is only a single attention layer rather than a full transformer, and we believe this distinction is a crucial factor contributing to the performance.\n\n&nbsp;\n\nQuestion 2 (**DeepSet**): We agree on the similarity with DeepSet and added the appropriate citation in the \"Aggregation functions\" section. The revised version is accessible in the PDF revision.\n\n&nbsp;\n\nQuestion 3 (**Anchors selection**): In response to the reviewer's insightful question, which enabled us to further enhance the strength of the proposed method, we conducted an ablation study on the number of randomly selected anchors for the stitching task using CIFAR100 across three different anchor seeds. **The results reveal that varying the number of anchors leads to different transformations, indicating that a single projection function cannot incorporate the desired invariance.** In contrast, our method enables the attainment of the highest score regardless of the number of anchors or the seed value employed for the uniform random sampling of anchors. We have introduced a new section (Section A.7) in the PDF revision for additional details, including Table 10 and Figure 9.  Given the importance of this finding, we also mentioned it in the main manuscript.\n\n&nbsp;\n\nQuestion 5 (**Aggregation ablation**): We sincerely hope the experiments presented in the appendix effectively address the question and provide valuable insights. Tables 13 and 14 showcase the performances using image and text data, conducting an ablation study across all the aggregation methodologies described in Section 3. Additionally, Table 15 presents a comprehensive ablation on various numbers of distances, combining only one, two, or all four projection functions. While acknowledging resource and time constraints prevented a combinatorial exploration of all possible settings, we trust that the results offer valuable insights to the discussion. Your understanding and consideration are greatly appreciated.\n\n&nbsp;\n\n---\n\nWe hope that our responses address the primary concerns raised by the reviewer, particularly the weakness regarding the perceived limited scope of the paper. Additionally, we have included supplementary experiments on ImageNet and reconstruction tasks, detailed in the [General Response](https://openreview.net/forum?id=vngVydDWft&noteId=44LjVL0qyv). We are more than willing to engage in further discussions if additional clarification is needed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407114639,
                "cdate": 1700407114639,
                "tmdate": 1700410209434,
                "mdate": 1700410209434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]