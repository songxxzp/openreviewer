[
    {
        "title": "PolyFormer: Scalable Graph Transformer via Polynomial Attention"
    },
    {
        "review": {
            "id": "RXJW2SVyVy",
            "forum": "vUgeBN7F9l",
            "replyto": "vUgeBN7F9l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_13YF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_13YF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a graph transformer architecture that uses intra-node attention from polynomial bases computed from said nodes. The method, PolyFormer, achieves good empirical performance on a variety of artifical and real-world tasks. The method is computationally cheaper than existing graph transformer approaches, which also aids scalability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method achieves good empirical performance on a wide variety of artifical and real-world graph datasets.\n- The method is computationally cheaper than calculating inter-node attention (O(L^2) instead of O(N^2)) but does not appear to suffer vs methods that do inter-node attention.\n- The polynomial basis decomposition is interesting. Specifically, the monomial basis can be interpreted as the number of hops away from a node and still gets reasonable performance"
                },
                "weaknesses": {
                    "value": "- My understanding is that this method uses intra-node attention, where attention is calculated for a set of tokens associated with *each node* and not between nodes. The only cross-node information is from the tokens when they are constructed with A and L. This does not seem like a very efficient way of passing information between nodes, as the cross-node information is hardcoded in the tokens.\n- The experiments do not show a large improvement over baseline methods. For example, in Table 2, the ChebNetII baseline performs very well and is not that far off PolyAttn (Cheb). In fact, it outperforms PolyAttn (Mono) on some tasks. This is a bit disappointing since PolyAttn (Mono) is touted as being interpretable, but PolyAttn (Cheb) is the method that outperforms baselines.\n- I may have missed this, but are there any experiments showing how PolyAttn scales to more than one PolyAttn layer?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8779/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8779/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8779/Reviewer_13YF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698085727752,
            "cdate": 1698085727752,
            "tmdate": 1699637102887,
            "mdate": 1699637102887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K5gHcivy8n",
                "forum": "vUgeBN7F9l",
                "replyto": "RXJW2SVyVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 13YF (1)"
                    },
                    "comment": {
                        "value": "We are delighted to receive your positive feedback! Here are our responses and perspectives regarding the weaknesses you mentioned.\n\n**W1:** My understanding is that this method uses intra-node attention, where attention is calculated for a set of tokens associated with each node and not between nodes. The only cross-node information is from the tokens when they are constructed with A and L. This does not seem like a very efficient way of passing information between nodes, as the cross-node information is hardcoded in the tokens.\n\n**A1:** Our method employs intra-node attention. This choice is based on the following considerations:\n\n**Scalability**: In computer vision image classification, Transformer-based models usually calculate attention between patches of an image, rather than between entire images. Similarly, in natural language processing sentence classification, these models typically focus on attention between nodes within a sentence. This approach facilitates training and inference on mini-batches, thus avoiding scalability issues when dealing with a large number of images or sentences. Inspired by this, we design polynomial tokens for each node and compute attention between these tokens within a node for node classification tasks, thereby enhancing the model's scalability. Conversely, cross-node attention entails considerable computational costs, resulting in a complexity of $O(n^2d)$, where $n$ is the number of nodes, and $d$ is the dimensionality of the node representations. Such complexity is impractical for large-scale graph node classification tasks. Even with efficient attention mechanisms like [1], the complexity remains at least $O(nd^2)$, which is prohibitive for large-scale graphs with, for example, 100 million nodes. In short, intra-node attention enables Graph Transformer models with great scalability.\n\n**Model Performance**: While cross-node information can provide flexibility in learning node-pair relationships for graph Transformers, this flexibility can sometimes detract from practical performance, as noted in [2]. To incorporate structural information and boost performance, NAGphormer and [3, 4] have introduced various positional encodings. However, these additions incur extra costs, limiting scalability to large graph datasets and potentially failing to achieve ideal model performance. Although cross-node attention seems more efficient in transmitting information between nodes, most real-world graph data resemble small-world networks. In such networks, any node can be reached from any other node within a limited number of steps. Therefore, even with a truncated order of polynomial $h_k(\\mathbf{A}, \\mathbf{L})$, the polynomial tokens contain sufficient information for intra-node attention. Besides, our intra-node attention mechanism, PolyAttn, is theoretically equivalent to a node-wise polynomial filter in the spectral domain, as demonstrated in Theorem 3.1:\n$$\n\\mathbf{Z}\\_{i,:} = \\sum\\_{k=0}^K \\mathbf{H}'^{(i)}\\_{k,:} = \\sum\\_{k=0}^K \\alpha\\_k^{(i)}(g\\_k(\\mathbf{P})\\mathbf{X})\\_{i,:} = \\left(\\mathbf{U}h^{(i)}(\\mathbf{\\Lambda})\\mathbf{U}^\\top \\mathbf{X}\\right)_{i,:}\n$$\nThis kind of filter [5, 6, 7, 8, 9], which hardcode node information via polynomial, perform well in node classification tasks and outperforms some Graph Transformer models which employ cross-node attention. In fact, our node classification experiments (Tables 4 and 5) demonstrate that our model using hardcore encoding delivers outstanding results in most cases as well.\n\nIn summary, our intra-node attention mechanism, termed \"PolyAttn\", is designed considering both scalability and model performance.\n\n**W2:** The experiments do not show a large improvement over baseline methods. For example, in Table 2, the ChebNetII baseline performs very well and is not that far off PolyAttn (Cheb). In fact, it outperforms PolyAttn (Mono) on some tasks. This is a bit disappointing since PolyAttn (Mono) is touted as being interpretable, but PolyAttn (Cheb) is the method that outperforms baselines.\n\n**A2:** (1) ''ChebNetII performs well and is not far off polyattn(cheb).''\n\nBelow are the $R^2$ scores / the sum of squared error (denoted as SE) results from Table 2. A higher $R^2$ score indicates better performance, while a lower SE indicates better performance as well. Both the $R^2$ score and SE of PolyAttn (Cheb) significantly outperform those of ChebNetII. Specifically, comparing ChebNetII, PolyAttn (Cheb) reduced SE by up to **3500 times** in the Low-high pass setting."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044587842,
                "cdate": 1700044587842,
                "tmdate": 1700044587842,
                "mdate": 1700044587842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pSIqaV5EZI",
                "forum": "vUgeBN7F9l",
                "replyto": "83SeINg7Rp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Reviewer_13YF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Reviewer_13YF"
                ],
                "content": {
                    "comment": {
                        "value": ">In Section 4.2, we conducted experiments on real datasets using the Polyformer, which includes 1-4 layers of PolyAttn. More detailed information about the layers can be found in Appendix D and the supplementary materials.\n\nAre the final hyperparameters chosen from the hyperparameter search listed in the paper?\n\nOtherwise thank you for your response, I am keeping my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503749108,
                "cdate": 1700503749108,
                "tmdate": 1700503749108,
                "mdate": 1700503749108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ucmguYdH73",
                "forum": "vUgeBN7F9l",
                "replyto": "RXJW2SVyVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 13YF (3)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your valuable feedback on our paper. The final hyperparameters were selected from the hyperparameter search space presented in our paper. In the revised draft, these final hyperparameters for our model are listed in Appendix D.3."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554203686,
                "cdate": 1700554203686,
                "tmdate": 1700554371217,
                "mdate": 1700554371217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TrVpMi0lMw",
            "forum": "vUgeBN7F9l",
            "replyto": "vUgeBN7F9l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_BLsH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_BLsH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new Graph Transformer called PolyFormer which contains two main steps. In the first step, PolyFormer constructs polynomial tokens for each node. In the second step, PolyFormer leverages a tailored polynomial attention mechanism to learn the final node representations from the constructed polynomial tokens. Empirical results seem to demonstrate the effectiveness of PolyFormer on the node classification task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper introduces different polynomial bases to construct the token sequence for each node.\n2.\tThis paper develops a new attention mechanism to learn node representations from the token sequences.\n3.\tExtensive experiments have been conducted to demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper seems to be limited.\n2. More challenging datasets need to be adopted in the experiment.\n3. Some experimental settings are not reasonable."
                },
                "questions": {
                    "value": "1. The proposed PolyFormer actually combines many existing techniques. The Polynomial Token could be regarded as an extension of Hop2Token[1] with different propagation strategies. Moreover, utilizing tanh() to compute the attention score and the node-shared attention bias also have been proposed and successfully implemented in previous works [2] and [3] respectively.\n2. More challenging datasets need to be added into the experiments to validate the effectiveness of the proposed PolyFormer, including Actor, Squirrel, Chameleon and ogb-products. The first three are challenging heterophilic datasets and the last one is the representative large-scale graph dataset. Note that, Squirrel and Chameleon should consider the filtered versions proposed in [4]. \n3. I notice that authors conduct the complexity comparison in Section 4.3. The authors keep the total number of parameters approximately same for each model. However, I think the settings of this comparison are not reasonable since it is not the true parameter setting for each model to achieve the best performance on the dataset. Compared to NAGphormer, the proposed PolyFormer introduces order-wise MLP to initialize the query and key matrix, which inevitably increase the training cost. I just wonder the truly training cost of each model on its optimal parameter setting. \n\n\n[1] Chen et al. NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs. ICLR 2023.\n\n[2] Bo et al. Beyond Low-frequency Information in Graph Convolutional Networks. AAAI 2021.\n\n[3] Chien et al. Adaptive universal generalized pagerank graph neural network. ICLR 2022.\n\n[4] Platonov et al. A critical look at the evaluation of GNNs under heterophily: are we really making progress? ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719209678,
            "cdate": 1698719209678,
            "tmdate": 1699637102767,
            "mdate": 1699637102767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q1DhY8CpEe",
                "forum": "vUgeBN7F9l",
                "replyto": "TrVpMi0lMw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer BLsH (1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your insightful feedback!\n\n**W1/Q1:** The novelty of this paper seems to be limited. The proposed PolyFormer actually combines many existing techniques. The Polynomial Token could be regarded as an extension of Hop2Token with different propagation strategies. Moreover, utilizing tanh() to compute the attention score and the node-shared attention bias also have been proposed and successfully implemented in previous works respectively.\n\n**A1:** Our work is **motivated** by polynomial filters, a method that conveys both theoretical guarantees and good empirical performance in graph neural networks, particularly performs well in tasks like node classification. Despite their strong theoretical and empirical performance, polynomial filters are usually node-unified [1, 2, 3, 4, 5, 6], limiting their expressive power. It is challenging to extend these to node-wise filters directly. Fortunately, with our specially designed model, we achieve an equivalence to more expressive node-wise filters in the spectral domain. On the other hand, most existing graph transformers, including NAGphormer, add spectral information via Laplacian eigendecomposition into the spatial domain, which is costly. However, as our model arises directly from the spectral domain, it eliminates the need for this high-cost addition of spectral information, while still achieving outstanding performance due to the powerful expressive power of spectral filters. \n\nMore specifically, our model differs from NAGphormer in various aspects:\n\n1. **Node Token Design Perspective.** \n   As highlighted in our Introduction, our polynomial tokens are conceptualized from a spectral perspective, designed to convey spectral information through various polynomial bases, such as the Monomial basis, Chebyshev basis (used in our paper), and even the Bernstein basis (in response to Reviewer tq9c). In contrast, NAGphormer's spatially-oriented Hop2Token generates node tokens by aggregating information from different hops in the spatial domain. Even with the Monomial basis, our PolyAttn enables the model to capture information directly in the spectral domain, avoiding the high-cost process of adding additional spectral information, a step necessary in NAGphormer and other popular Graph Transformers [7, 8, 9].\n2. **Elaborately Designed PolyAttn vs. Vanilla Self-Attention.**\n   NAGphormer employs vanilla self-attention for encoding tokens derived from Hop2Token. In contrast, we introduce specially designed PolyAttn for our proposed polynomial tokens. **While some design elements (e.g., tanh(), node-shared attention bias) have been used in prior work, it is important to note that our primary focus is not on innovating new activation functions for attention scores or creating novel attention biases.** Instead, our focus is on developing an attention mechanism specifically tailored for polynomial tokens, establishing a direct connection with polynomial filters in the spectral domain.\n\nWith the polynomial tokens in the spectral domain and the tailored PolyAttn, our model effectively addresses two significant challenges in previous works:\n\n1. **Graph Transformer models** that add spectral information (e.g., NAGphormer and  [7, 8, 9]) enhance expressive power but require extensive time and space. In contrast, our model, which originates from the spectral domain, already possesses powerful expressive capabilities. Without the need for additional spectral information, which incurs significant costs, our model demonstrates great scalability and performance, capable of handling graphs with up to **100 million nodes**, as illustrated in Experiment 4.2.\n2. **Polynomial filters** (e.g., [1, 2, 3, 4, 5, 6]) are typically node-unified, learning the same filter for all nodes and thereby limiting their expressive power. Our model operates as a **node-wise** filter, adaptively learning distinct filters for different nodes, as theoretically substantiated in Theorem 3.1 and Proposition 3.1, and empirically demonstrated in Experiment 4.1.\n\nIn summary, inspired by the concept of spectral filters, our approach uniquely crafts node tokens and attention mechanisms from a spectral viewpoint, contrasting with NAGphormer's spatial focus. Leveraging polynomial tokens and PolyAttn, our model not only theoretically aligns with but also empirically aligns the expressive node-wise filters. These properties help to address challenges in both Graph Transformers and polynomial filters. Based on this elaborate design, our model offers a desirable balance between scalability and expressiveness."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043600868,
                "cdate": 1700043600868,
                "tmdate": 1700043600868,
                "mdate": 1700043600868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ykQVXxJJNV",
                "forum": "vUgeBN7F9l",
                "replyto": "gCwrbj9nwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Reviewer_BLsH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Reviewer_BLsH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed responses. \nMy opinion of the paper remains unchanged and I will keep my score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740612688,
                "cdate": 1700740612688,
                "tmdate": 1700740612688,
                "mdate": 1700740612688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YcuBmUdn8D",
            "forum": "vUgeBN7F9l",
            "replyto": "vUgeBN7F9l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_tq9c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_tq9c"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Polyformer, a novel graph neural network architecture designed to balance scalability and expressiveness for node-level prediction tasks on graphs. It innovatively introduces node tokens based on polynomial bases to efficiently capture node neighborhood information, which not only allows for minibatch training but also enhances scalability. Furthermore, the paper proposes a Polynomial Attention (PolyAttn) mechanism specifically designed for these polynomial node tokens. PolyAttn serves as a node-wise spectral filter, offering more expressive power than the node-unified filters used in previous works. The Polyformer architecture is built using these polynomial node tokens and the PolyAttn mechanism. Experimental results demonstrate that Polyformer outperforms previous state-of-the-art models in node classification tasks, effectively handling graphs with up to 100 million nodes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novel node token formulation using polynomial bases, enabling scalability.\n- PolyAttn provides node-wise filtering and enhanced expressiveness.\n- Strong experimental results demonstrating scalability and performance."
                },
                "weaknesses": {
                    "value": "- The polynomial bases are somewhat limited to Monomial and Chebyshev. More advanced bases could be explored.\n- Comparisons to very recent graph neural network methods are missing."
                },
                "questions": {
                    "value": "- How does PolyAttn relate to self-attention in standard Transformers? Is it a specialized version?\n- For large graphs, is recomputing the polynomial tokens for each minibatch really efficient?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8779/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8779/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8779/Reviewer_tq9c"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853475885,
            "cdate": 1698853475885,
            "tmdate": 1699637102663,
            "mdate": 1699637102663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f64LrSAWyJ",
                "forum": "vUgeBN7F9l",
                "replyto": "YcuBmUdn8D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tq9c"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable feedback!\n\n**W1:** The polynomial bases are somewhat limited to Monomial and Chebyshev. More advanced bases could be explored.\n\n**A1:** Your suggestion is constructive. In fact, our model framework can be extended to other polynomial bases. Accordingly, we have included results for PolyFormer with the Bernstein basis [1] and the optimal basis [2]. The specific results are as follows:\n\n|              | GPRGNN     | ChebNetII  | NAGphormer | Specformer | PolyFormer (Bern) | PolyFormer (Opt) | PolyFormer (Mono) | PolyFormer (Cheb) |\n| ------------ | ---------- | ---------- | ---------- | ---------- | ----------------- | ---------------- | ----------------- | ----------------- |\n| Tolokers     | 77.25\u00b10.61 | 76.83\u00b10.53 | 81.57\u00b10.44 | 80.42\u00b10.55 | 84.32\u00b10.59        | 83.15\u00b10.49       | 84.00\u00b10.45        | 83.88\u00b10.33        |\n| Roman-empire | 74.08\u00b10.54 | 72.70\u00b10.30 | 74.45\u00b10.48 | 69.94\u00b10.34 | 77.64\u00b10.33        | 77.15\u00b10.33       | 78.89\u00b10.39        | 80.27\u00b10.39        |\n\nCompared to the baseline models, PolyFormer models based on different basis all perform well.\n\n**W2:** Comparisons to very recent graph neural network methods are missing.\n\n**A2:** We have supplemented with the latest Graph Transformer model GOAT [3] to enrich our baseline. The specific results are as follows:\n\n|                   | CS         | Physics    | Tolokers   | Roman-empire |\n| ----------------- | ---------- | ---------- | ---------- | ------------ |\n| GPRGNN            | 95.26\u00b10.15 | 97.84\u00b10.59 | 77.25\u00b10.61 | 74.08\u00b10.54   |\n| ChebNetII         | 96.33\u00b10.12 | 97.25\u00b10.78 | 79.23\u00b10.43 | 74.64\u00b10.39   |\n| Specformer        | 96.07\u00b10.10 | 97.70\u00b10.60 | 80.42\u00b10.55 | 69.94\u00b10.34   |\n| NAGphormer        | 94.41\u00b10.35 | 97.18\u00b10.36 | 81.57\u00b10.44 | 74.45\u00b10.48   |\n| GOAT              | 95.12\u00b10.21 | 97.09\u00b10.24 | 83.26\u00b10.52 | 72.30\u00b10.48   |\n| PolyFormer (Mono) | 96.30\u00b10.14 | 98.16\u00b10.28 | 83.66\u00b10.63 | 78.47\u00b10.38   |\n| PolyFormer (Cheb) | 96.47\u00b10.14 | 98.01\u00b10.37 | 83.41\u00b10.47 | 79.62\u00b10.22   |\n\n**Q1:** How does PolyAttn relate to self-attention in standard Transformers? Is it a specialized version?\n\n**A3:** PolyAttn can be considered as a specially designed version of self-attention tailored for polynomial tokens. Its primary objective is to establish a connection between attention mechanisms and polynomial filters, as illustrated in Theorem 3.1.\n\nSpecifically, the design of PolyAttn incorporates elements such as $\\tanh()$ activation function and order-wise MLPs. Detailed information about its design can be found in Algorithm 1 of our paper. Using vanilla self-attention in conjunction with polynomial tokens would be insufficient to achieve equivalence with polynomial filters or may restrict the filtering capabilities. For instance, vanilla attention with polynomial tokens possibly only act as low-pass filters. These were discussed in Proposition 3.2 of our paper. This limitation would result in a reduction of expressive power.\n\nTherefore, to enrich model expressiveness and ensure theoretical guarantee, we designed a specialized version of self-attention, PolyAttn, for polynomial tokens.\n\n**Q2:** For large graphs, is recomputing the polynomial tokens for each minibatch really efficient?\n\n**A4:** During the preprocessing stage before training and inference, our model precomputes and stores polynomial tokens for all nodes. \n\nOnce training and inference begin, these precomputed polynomial tokens are directly loaded for each minibatch, **eliminating the need for their recomputation\uff0e** \n\nFrom the preprocessing stage to the phases of training and inference, **polynomial tokens for all nodes are computed only once.** Consequently, this approach is efficient for large graphs in practical applications.\n\nWe hope our answers are helpful. We are looking forward to your positive feedback.\n\n\n\nReferences:\n\n[1] Mingguo He, Zhewei Wei, Zengfeng Huang, and Hongteng Xu. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. In NeurIPS, pp. 14239\u201314251, 2021.\n\n[2] Yuhe Guo, and Zhewei Wei. Graph Neural Networks with Learnable and Optimal Polynomial Bases. In ICML, 2023.\n\n[3] Kong, Kezhi and Chen, Jiuhai and Kirchenbauer, John and Ni, Renkun and Bruss, C Bayan and Goldstein, Tom. GOAT: A Global Transformer on Large-scale Graphs. In ICML, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043484801,
                "cdate": 1700043484801,
                "tmdate": 1700043484801,
                "mdate": 1700043484801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nEVOfHlOLO",
            "forum": "vUgeBN7F9l",
            "replyto": "vUgeBN7F9l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_Wuvt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8779/Reviewer_Wuvt"
            ],
            "content": {
                "summary": {
                    "value": "Overall, the research problem is interesting and the research topic of graph transformer is novel. Furthermore, the proposed method PolyFormer method is clearly presented with theoretical analysis, and the experimental results are promising. On the other side, some concerns are raised. Please refer to the following sections for details."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The research problem is interesting. Trying to reduce the computational complexity and maintaining the effectiveness is pragmatic.\n\n- The proposed method PolyFormer is easy to understand and complete with theoretical analysis.\n\n- The experimental setting is extensive and results are competitive."
                },
                "weaknesses": {
                    "value": "- The introduction of NAGphormer seems incorrect somehow. On the one hand, the authors mention that NAGphormer was designed based on spatial information and neglected spectral information. On the other hand, the authors mention that NAGphormer attempted to use spectral information but eigendecomposition is costly. It seems contradictory. Moreover, if PolyFormer uses the proposed Monomial Basis especially, it seems that PolyFormer and NAGphormer are under the same general framework. The reviewer would appreciate if the authors could address this concern during the rebuttal.\n\n- The novelty is incremental compared with NAGphormer, PolyFormer adds MLP on each hop aggregation of NAGphormer, to some extent.\n\n- Theorem 1 seems not informative somehow."
                },
                "questions": {
                    "value": "Extending the first bullet point in the weaknesses section, could the authors explain why the proposed \"polynomial token\" with PolyAttn is a spectral method, not a spatial method, based on the polynomial type listed in Table 1? \n\nHow to understand $h(\\lambda)$ in Figure 1 with $\\alpha$ in Eq. 8? Moreover, during the learning process, how to obtain and interpret the low-pass and high-pass?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699483453794,
            "cdate": 1699483453794,
            "tmdate": 1699637102549,
            "mdate": 1699637102549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1bF01dmqFZ",
                "forum": "vUgeBN7F9l",
                "replyto": "nEVOfHlOLO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Wuvt (1)"
                    },
                    "comment": {
                        "value": "We are very grateful for your valuable and constructive feedback!\n\n**W1:** The introduction of NAGphormer seems incorrect somehow. Moreover, if PolyFormer uses the proposed Monomial Basis especially, it seems that PolyFormer and NAGphormer are under the same general framework. \n\n**A1:**  **(1) The clarification about NAGphormer.** \n\nNAGphormer mainly comprises two parts: \n\n*a. Hop2Token with vanilla multi-head self-attention.* NAGphormer generates node tokens via aggregating neighbors' information across different hops (Hop2Token) and encodes these tokens via vanilla multi-head self-attention. **This process is designed on the spatial domain** and neglects the spectral information, as discussed in the first paragraph of the Introduction section. \n\n*b. Structural encoding.* To include spectral information, NAGphormer uses eigenvectors derived from Laplacian eigendecomposition in the model's implementation. NAGphormer calls this technique the structural encoding, which is costly. We mentioned this in the second paragraph of our Introduction section.\n\n**(2) Different framework between PolyFormer and NAGphormer.** \n\nFirstly, we would like to emphasize that our work is **motivated** by node-wise filters. Polynomial filters have been proven to be a successful spectral method in graph neural networks and perform well in tasks such as node classification. However, most of them are node-unified filters, limiting the enhancement of expressive power. It is challenging to expand polynomial filters to node-wise filters directly. Fortunately, with the aid of attention mechanisms, we establish an equivalence between our model and node-wise filters.\n\nBased on the motivation above, we clarify that our proposed model operates within a **different framework** compared to NAGphormer. As we have claimed, one of our aims is to establish a connection between our model and expressive node-wise filters in the spectral domain. We design node tokens with the assistance of polynomial bases, which contain spectral information, especially when combined with our proposed PolyAttn. However, Hop2Token derives node tokens by aggregating different hop information in the spatial domain. The Monomial basis is a special case that only superficially makes our model appear similar to NAGphormer. Nonetheless, it turns out that PolyFormer is distinct from NAGphormer in terms of other bases (such as the Chebyshev basis in our paper, or the Bernstein basis and the optimal basis in response to Reviewer tq9c). Even when considering the Monomial basis, PolyFormer differentiates from NAGphormer as its PolyAttn with Monomial tokens enables the model to act as node-wise spectral filters, while the vanilla attention used in NAGphormer with tokens from Hop2Token cannot. Due to the superior spectral property in PolyFormer, our model generates a more difference in framework compared to NAGphormer: NAGphormer requires the addition of spectral information as a supplement, which incurs high costs, while our work effectively and directly captures spectral information, **eliminating this need.**\n\nIn summary, inspired by the spectral node-wise filter, our model integrates spectral information through polynomial tokens and PolyAttn, operating under a distinct framework compared to NAGphormer, which is designed from the spatial perspective.\n\n**W2:** The novelty is incremental compared with NAGphormer, PolyFormer adds MLP on each hop aggregation of NAGphormer, to some extent.\n\n**A2:** We distinguish our work from NAGphormer based on motivation, model design, and theory and empirical performance.\n\n**(1) Motivation.** Inspired by the strong performance and theoretical guarantees of spectral filters, we aim to build a bridge between Graph Transformers and expressive spectral filters, a feat not achieved by NAGphormer. Our design of polynomial tokens and PolyAttn even establishes an equivalence to more expressive node-wise filters.\n\n**(2) Detailed Design Differences between PolyFormer and NAGphormer.**\n\n*a. Spectral-based Node Tokens.* NAGphormer's Hop2Token aggregates information from different hops in the spatial domain. In implementation, NAGphormer necessitates additional structural encoding based on Laplacian eigendecomposition to supplement spectral information, leading to high costs. Conversely, our polynomial tokens, designed in the spectral domain, inherently contain spectral information, obviating the need for additional positional encoding."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042937778,
                "cdate": 1700042937778,
                "tmdate": 1700042937778,
                "mdate": 1700042937778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s71t6xY1Dy",
                "forum": "vUgeBN7F9l",
                "replyto": "Y90xRzuara",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8779/Reviewer_Wuvt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8779/Reviewer_Wuvt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, I have no further questions. The paper will be discussed among reviewers and AC."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718771028,
                "cdate": 1700718771028,
                "tmdate": 1700718771028,
                "mdate": 1700718771028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]