[
    {
        "title": "Latent Lie Group Representations"
    },
    {
        "review": {
            "id": "mX0CfxnP17",
            "forum": "61hDIuox86",
            "replyto": "61hDIuox86",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9095/Reviewer_8A7f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9095/Reviewer_8A7f"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to learn symmetries in a latent representation of the data. The setting requires having pairs of inputs, one being the original and one the transformed data point. They propose a network to learn a latent space, the Lie algebra elements and the transformation parameters at the same time. \nThey show some limited experimental results on SyMNIST, where each image is paired with an affine transformed version of the image."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper discusses some good avenues for learning symmetries in data. \n1. For spatial data, the vector field approach taken for the Lie algebra elements $\\Gamma(x)$ (generators) is interesting. \n2. The overall architecture of their model and the loss have some good points, like the $\\mathcal{L}^Z$ term in eq. (8)   \n3. The learned generators in Figs 4 and 5 seem close to expected ground truth.  \n4. The theoretical sections of the paper are fairly polished, well-structured and presented well."
                },
                "weaknesses": {
                    "value": "This paper is definitely moving in a good direction, but it seems to be work in progress. \n1. __Lack of baselines and ground truth:__ The experiments are incomplete. They authors should compare against other symmetry discovery methods such as Augerino (Benton 2020), LieConv (Finzi 2020), L-conv (Dehmamy 2020) and recently LieGAN (Yang 2023). Similar to your eq (5), LieConv uses a subalgebra of $GL(3)$. The rotated MNIST experiment in L-Conv also dealt with pairs of input,   predicting $t$ and learning $\\mathbf{G}$ in the process. LieConv and LieGAN also both use the exponential map. \n2. __Novelty:__ Gabel 2023 seems to use a very similar formulation and architecture and even works with the same input data. How do your results contrast with Gabel 2023, and can their results serve as a baseline for yours?  \n3. __presentation of results:__ \n    1. Fig 3 is not clear or informative. What should the reader learn from it? What is the vertical axis? \n    2. Figs 4,5, qualitative and quantitative comparison with ground truth is missing (e.g. cosine similarity with GT and plotting GT)\n4. __Detials missing for reproducibility:__ I couldn't find any appendices or supplementary material. There is also not enough information in the paper (no latent dims or hyperpaparameters), which makes the paper harder to assess."
                },
                "questions": {
                    "value": "1. The affine transformations you are learning, for the most part, act linearly on the inputs. Thus, I don't think going to a latent space is required in this case. Can you elaborate why you use latent space here? \n2. what are the latent dims? List all hyperparameters. \n3. what are the structures of neural nets $T_\\theta$ and autoencoder layers? \n4. Have you done ablation studies for the terms in the loss function (9)? How much do $\\mathcal{L}_\\alpha$ and $\\mathcal{L}^Z_T$ matter?\n5. $\\mathcal{L}_\\alpha$ needs more explanation. How does one go to the prime basis $\\mathbf{D}'_i$? This may be doable if the $z$ space was also 2D, like the pixel space. But what is the exact procedure? \n5. Compared to Gabel 2023, what is new in your paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443748134,
            "cdate": 1698443748134,
            "tmdate": 1699637145310,
            "mdate": 1699637145310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "6CrSYN2V6i",
            "forum": "61hDIuox86",
            "replyto": "61hDIuox86",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9095/Reviewer_bdCU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9095/Reviewer_bdCU"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose to learn the symmetry of a dataset via neural networks. The symmetry is represented by a one-parameter Lie group and its generator is represented and learned in the network. The sample dependent $t$ parameter is also output by the network and it is used to estimate the paramter distribution. A SyMNIST dataset is built and is used to verify the method by discovering the applied affine transformations. Qualitative results are shown."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper tackles an interesting topic of symmetry detection. Although it feels like the task is still at an early stage and only toy problems are tested, the related methods may become more influential after more developments."
                },
                "weaknesses": {
                    "value": "1. The general writting is hard to follow for me. Many sentences are hard to decode, and mathematical notations are not explained well in texts. For example, \n    - (a) The definition of \"class-generic symmetries\" is unclear. What exactly are \"symmetries within each class, but also common among all classes\"? Any concrete examples?\n    - (b) The \"SyMNIST task\" may be better described as \"SyMNIST dataset\". A figure that visualizes sample images of the dataset is helpful to readers.\n    - (c) \"We wish to learn non-canonical symmetries\". What are the non-canonical symmetries refer to? Any examples? In section 4, only affine transformations are used in experiments. Does affine transformation belong to non-canonical symmetries?\n    - (d) \"A collection of multiple power series applied in succession is the neural network\". What is the role of non-linear activations in this description?\n    - (e) \"avoid making calculations in pixel-space directly\". What is the calculation in pixel-space refer to? Any examples?\n    - (f) \"pixel-level transformation will need to be extracted by special methods\". Any examples?\n    - (g) The network structures of encoder $f$, decoder $g$ and t-network $T$  are not discussed.\n    - (h) What is \"aliasing artifacts\" in the translation setting?\n\n2. This work focuses on the one-parameter groups. What if the underlying data contains a wider range of transformations, e.g., both rotation and translation, that cannot be described by a one-parameter group?\n\n3. There is a lot overlap with Gabel et al. (2023), including figures, equations and descriptions. It would be helpful if the authors could highlight the contribution and novelty compared to it.\n\n4. The t-test and $\\epsilon$-test are missing in experiments.\n\n5. There is no quantitative evaluations, and there is no comparision to related algorithms.\n\n6. Fig. 4 and Fig 5 are hard to parse and understand due to the lack of description."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9095/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9095/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9095/Reviewer_bdCU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596800021,
            "cdate": 1698596800021,
            "tmdate": 1699637145191,
            "mdate": 1699637145191,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "OGyHBnyZX7",
            "forum": "61hDIuox86",
            "replyto": "61hDIuox86",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9095/Reviewer_FZBr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9095/Reviewer_FZBr"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses a symmetry identification technique to improve learning in neural networks. The mathematics underpinning the technique is robust. A good implementation would lead to improvements in neural network learning. It will also make the behaviour of NN's more transparent, as--  at least symbolically -- we get to learn what symmetries exists in data beforehand."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The use of mathematical techniques to improve data-oriented machine learning."
                },
                "weaknesses": {
                    "value": "It is not clear if the implemented technique worked at all! A few figures are produced on page 7 (Figures 4 and 5), and we read that a z-test has been performed. No numerical results are however presented and I am not sure how did the model do!"
                },
                "questions": {
                    "value": "Does the new model learn the data better, faster, with less parameters etc etc? Can you please provide a comparison table of some sort?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9095/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9095/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9095/Reviewer_FZBr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698918571671,
            "cdate": 1698918571671,
            "tmdate": 1699637145078,
            "mdate": 1699637145078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]