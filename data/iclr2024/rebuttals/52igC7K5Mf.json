[
    {
        "title": "GC-Mixer: A Novel Architecture for Time-varying Granger Causality Inference"
    },
    {
        "review": {
            "id": "3Eq1eTnTGC",
            "forum": "52igC7K5Mf",
            "replyto": "52igC7K5Mf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_NC5z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_NC5z"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate the problem of the Granger causal structure discovery. Considering that the existing method can hardly address the time-varying Granger-Causality inference, the authors proposed the GC-Mixer, which contains a mixer Block and a causality inference block. The authors further devise the multi-level fine-tuning method. The authors evaluate the proposed method on the VAR and Lorenz-96 datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors address the problem of the time-varying Granger Causality inference."
                },
                "weaknesses": {
                    "value": "1.\tThere are several methods that are proposed to solve the Granger Causality, for example [1],[2],[3][4][5]. Moreover, [2] is similar to the proposed method. Hence, it is suggested that the authors should discuss the difference between the proposed method and these methods and consider them as baseline.   \n2.\tThe authors propose multi-level fine-tuning to address the time-varying causal structure, but the motivation and intuition are not clear. Moreover, it is suggested that the authors should provide the complexity analysis for the multi-level fine-tuning method.   \n3.\tI am also curious if the proposed method can address the causal structures that do not exist in the training set (OOD causal structure).   \n\n\n\n[1] CUTS: Neural Causal Discovery from Irregular Time-Series Data   \n[2] Interpretable Models for Granger Causality Using Self-explaining Neural Networks   \n[3] GRANGER CAUSAL INFERENCE ON DAGS IDENTIFIES GENOMIC LOCI REGULATING TRANSCRIPTION   \n[4] Causal Discovery from Temporal Data    \n[5] Neural Time-Invariant Causal Discovery from Time Series Data"
                },
                "questions": {
                    "value": "N.A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3298/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3298/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3298/Reviewer_NC5z"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764924343,
            "cdate": 1698764924343,
            "tmdate": 1699636278861,
            "mdate": 1699636278861,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bFoJBS265S",
                "forum": "52igC7K5Mf",
                "replyto": "3Eq1eTnTGC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. We address all the points as follows.\n\nQ1. There are several methods that are proposed to solve the Granger Causality, for example [1],[2],[3][4][5]. Moreover, [2] is similar to the proposed method. It is suggested that the authors should discuss the difference between the proposed method and these methods and consider them as baseline.\n\nWe have supplemented the comparison result of the study [2] in our paper, as shown in table 1, 2, 3. While the comparison results of other studies are still being tested, we are temporarily unable to provide test results.\n\n\nQ2. The authors propose multi-level fine-tuning to address the time-varying causal structure, but the motivation and intuition are not clear. Moreover, it is suggested that the authors should provide the complexity analysis for the multi-level fine-tuning method.\n\nWe initially manually divided the time series into multiple sequences and fed them into the model to infer Granger causality. However, we encountered issues with overfitting, and did not know how many segments would be best for splitting. Therefore, our motivation is to propose an algorithm that can automatically split the time series and make the model less prone to overfitting than before. The complexity analysis is updated and shown in Section 3.2.\n\nQ3: I am also curious if the proposed method can address the causal structures that do not exist in the training set (OOD causal structure).\n\nWe have not conducted tests on the training set with an out-of-distribution causal structure and plan to incorporate such a dataset into our future experiments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227354210,
                "cdate": 1700227354210,
                "tmdate": 1700227392332,
                "mdate": 1700227392332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nRdTg7pEMF",
            "forum": "52igC7K5Mf",
            "replyto": "52igC7K5Mf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a model for time-invariant Granger causality inference which exhibits consistent performance across various time series using the same hyperparameters. The model can also be extended to infer the time-varying Granger causality within a multi-level fine-tuning framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The literature review is comprehensive and clear.\n2. The authors conduct extensive experiments on simulated datasets with different configurations.\n3. The authors proposed an algorithm that has been claimed as the first algorithm to utilize an MLP Mixer-based architecture for inferring Granger causality in time series."
                },
                "weaknesses": {
                    "value": "1. Given that the proposed algorithm involves significantly more parameters than the baselines, it would be helpful to include a comparison of the number of parameters required in the experiment section, along with the corresponding training times.\n2. Some details about the loss function are not clear, which has been stated in the Questions section.\n3. Some details about the experiments are not clear, which has been stated in the Questions section."
                },
                "questions": {
                    "value": "1. In the equation 12 loss function, the right-hand side of the equation contains the time index $t$ and time series index $i$, and there is no summation regarding $t$ and $i$. Does this mean the loss function is computed for each time series $i$ at a specific time $t$? Personally, I do not think the loss function is linked to each $t$, but equation 12 seems to suggest otherwise.\n2. What is the value of the threshold $\\epsilon$ in equation 13 in the experiment, and how to determine it? Could you help me locate the place if you have stated it already in the paper?\n3. It is advisable to cite the related work in the first paragraph in section 2.3 regarding the existing approach?\n4. Can you clarify how you compute the True Positive Rate (TPR) and False Positive Rate (FPR)? Do these metrics calculate from the F-norm of $W_{j,k}^n$? In other words, do they not only assess whether time series $j$ Granger-causes time series $i$ but also consider whether the lag is accurate? For instance, if the true lags are $1,2,3$, would lag $4$ from time series $j$ to $i$ be regarded as a False Positive?\n5. Given that the loss function is computed for each time series $i$, does this mean the whole algorithm will run $p$ times for $p$-variate time series?\n6. Table 3 shows that the cMLP algorithm outperforms the proposed algorithm. Since no other nonlinear experiment has been provided, does this imply that the proposed algorithm performs worse than the baselines in the nonlinear cases? Personally, I recommend conducting further nonlinear experiments to thoroughly examine the performance of the proposed algorithm in nonlinear cases.\n7. Can additional nonlinear experiments be conducted in section 3.3 on automatic lag selection, given that the Lorenz-96 dataset does not involve time lag? Since the proposed method did not yield the best performance in the Lorenz-96 dataset, might this also be the case in the time lag selection stage? Additional results would provide valuable insights.\n8. Can you offer more details regarding the procedure for generating the time series in section 3.4 on time-varying Granger causality inference? If, in the four scenarios, two sets of time series with different configurations are merged together with equal lengths, would this potentially favor the proposed algorithm? Considering that the multilevel fine-tuning algorithm separates input time series into $2^{i-1}$ segments, such a configuration might be advantageous. Furthermore, could you provide the information about the value of $i$ when the algorithm stops in the time-varying experiment?\n9. Still in section 3.4, how to do manual splitting? Does it utilize additional information about the true time series?\n10. Could you briefly explain why the algorithms mentioned in section A.2 have not been applied as baselines, though they ask to select the time lag manually?\n11. It is advisable to enhance the visualization by incorporating axis titles and subtitles."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3298/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3298/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794566560,
            "cdate": 1698794566560,
            "tmdate": 1699636278792,
            "mdate": 1699636278792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t3d3qzytlH",
                "forum": "52igC7K5Mf",
                "replyto": "nRdTg7pEMF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and correcting our mistakes. We address all the points as follows.\n\n Q1. Does this mean the loss function is computed for each time series $i$ at a specific time $t$? \n\nThe loss function is independent of $t$, and we have corrected this mistake in the Equation 12.\n\nQ2. how to determine threshold in equation 13 in the experiment?\n\nFirstly, we generate the AUROC using one hyperparameter lambda and sweep threshold epsilon. Then, when the AUROC curve takes the optimal operating point (the point closest to the upper-left corner), the corresponding epsilon are taken to infer the Granger causality.\n\nQ3: It is advisable to cite the related work in the first paragraph in section 2.3 regarding the existing approach?\n\nWe have added references in the corresponding section.\n\nQ4: Can you clarify how you compute the True Positive Rate (TPR) and False Positive Rate (FPR)?  Do these metrics calculate from the F-norm of W?  Do they not only assess whether time series $j$ Granger-causes time series $i$ but also consider whether the lag is accurate?\n\nYes, the TPR and FPR are calculated from the F-norm of W and did not consider whether the lag is accurate. This is why we still need to compare the performance of cMLP and GC-Mixer on automatic lag selection in Section 4.3.\n\nQ5. Does this mean the whole algorithm will run $p$ times for $p$-variate time series?\n\nYes, the whole algorithm will run $p$ time for $p$-dimensional time series.\n\nQ6. Foes this imply that the proposed algorithm performs worse than the baselines in the nonlinear cases? Personally, I recommend conducting further nonlinear experiments to thoroughly examine the performance of the proposed algorithm in nonlinear cases.\n\nWe have begun to evaluate performance on more nonlinear datasets, including Dream3 and Lotka\u2013Volterra. Due to time limitations, we are unable to immediately provide specific results.\n\nQ7. Can additional nonlinear experiments be conducted in section 3.3 on automatic lag selection, given that the Lorenz-96 dataset does not involve time lag?\n\nWe considered adding more nonlinear datasets to test the performance of automatic time lag selection. Unfortunately, neither the datasets provided in the existing literatures, including Lorenz-96, Dream-3, FMRI Bold, and Lotka\u2013Volterra, have no time lag. We are actively looking for nonlinear datasets including time lags, but for now, we cannot test the performance of automatic lag selection on nonlinear datasets.\n\nQ8. Can you offer more details regarding the procedure for generating the time series in section 3.4 on time-varying Granger causality inference? Could you provide the information about the value of $i$ when the algorithm stops in the time-varying experiment?\n\nSuch a configuration will indeed be beneficial to our proposed algorithm. However, the comparison of algorithms is under the same configuration, so it can reflect the performance of the proposed algorithm. Moreover, four values of $i$ when the algorithm stops for four scenarios are 3.\n\nQ9. Still in section 3.4, how to do manual splitting? Does it utilize additional information about the true time series?\n\nWe manual split the sequence into two equal-length time series and use the same lambda for training. In addition, the time-varying data we currently test is still generated by simulation. Due to time constraints, our future work will conduct experiments on the real-time series.\n\nQ10. Could you briefly explain why the algorithms mentioned in section A.2 have not been applied as baselines, though they ask to select the time lag manually?\n\nOur research focuses on deep learning models, which differ from traditional time-varying Granger causality models. Therefore, we have chosen not to conduct a direct comparison with traditional models in this paper, but we have planned to conduct an exhaustive comparison of our proposed algorithm with the related algorithms mentioned in Section A.2 in future research.\n\nQ11: It is advisable to enhance the visualization by incorporating axis titles and subtitles.\n\nWe have updated the axis titles in Figure 5, 6, 7, 8."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227128215,
                "cdate": 1700227128215,
                "tmdate": 1700227128215,
                "mdate": 1700227128215,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TknBsoVKOl",
                "forum": "52igC7K5Mf",
                "replyto": "nRdTg7pEMF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for all the clarifications. While most of my inquiries have been resolved, I still have a couple of remaining questions:\n\n1. Can you briefly address the first point outlined in the Weakness section regarding the number of parameters needed and the training time?\n2. Regarding the answer for Q6, could you provide an approximate estimate of the time required to execute the algorithm on a dataset of a specific size?\n3. Concerning the statement \"The time complexity of the algorithm is O(log2n)\" included in the main paper, does the variable \"n\" denote the number of time series subsequences? If so, should it be represented as \"T \u2212 K + 1\" instead of \"n,\" considering that \"n\" denotes the subsequence index rather than the total number of subsequences? Additionally, is the complexity solely associated with \"n\" and not influenced by the dimension of the multivariate time series \"p\" or the level \"i\"?\n4. Based on the answer provided for Q9, does it suggest that in the manual splitting scenario, \"i\"= 2, while in the automatic splitting scenario, \"i\" is algorithmically determined and turns out to be 3 across all four scenarios? Table 4 shows that for the proposed method, the outcome of \"Multi-level fine-tuning (Automatic splitting)\" doesn't significantly outperform the proposed method employing manual splitting. Considering the computational cost and the parameter requirements associated with the multi-level approach, does this indicate that manual splitting might be preferable from a practical standpoint?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639752276,
                "cdate": 1700639752276,
                "tmdate": 1700639934772,
                "mdate": 1700639934772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BqHhHniAdu",
                "forum": "52igC7K5Mf",
                "replyto": "nRdTg7pEMF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and correcting our mistakes. We address all the points as follows.\n\nQ1: Does the \"first point outlined in the Weakness section\" refer to the sentence that \"GC-Mixer has more parameters than cMLP and cLSTM, leading to more prone to overfit\"? If so, when the number of the Mixer Block in GC-Mixer is 1, the total number of parameters of GC-Mixer is 485520. The total number of parameters of cMLP is 52010, and cLSTM is 449010. Our model contains more parameters than cMLP and cLSTM.\n\nQ2: Perhaps the time comparison of running GC-Mixer and cMLP on the same computer could answer this question. For VAR (3) (sparsity =0.2), the time consumption of GC-Mixer (optimizer: Adam) is about 150s; cMLP (optimizer: ISTA) is over 1400s, cMLP (optimizer: Adam) is only 5s. For Lorenz-96 (F=10), the time consumption of GC-Mixer (optimizer: Adam) is about 700s, and cMLP (optimizer: ISTA) is about 70s (early stop); cMLP (optimizer: Adam) is about 5s. (By using ISTA, not Adam optimizer, cMLP can achieve the best performance AUROC.)\n\n\nQ3: Yes, we have made a mistake. The time complexity is related to the number of subsequence indexes $(T-K+1)$, the dimension p, and the level i, that is, $O((T-K+1) \\times p \\times (2^i-1))$. In addition, in our code, $T-K+1$ subsequences are calculated in parallel so that the actual complexity will be less than $O((T-K+1) \\times p \\times (2^i-1))$.\n\nQ4: Yes, we split the time series into two sequences for manual splitting. For automatic splitting, the algorithm splits the time series into four sequences (i=3). \nFor the second question, in our experiments, we adopted the optimal splitting number for the manual splitting: to directly split the sequence into two for calculation. However, for a practical scenario, we may not know the optimal number of splitting, so using an algorithm to split automatically may be a better option."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672893504,
                "cdate": 1700672893504,
                "tmdate": 1700672933735,
                "mdate": 1700672933735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fDdCQAYb1S",
                "forum": "52igC7K5Mf",
                "replyto": "BqHhHniAdu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for answering those follow-up questions.\n\nThe fourth question revolves around the observation that although an optimal number of splits exists, Table 4 indicates no substantial difference between manual and automatic splitting. In such a scenario, why do people choose the latter option despite requiring more parameters and longer training time?\n\nFor a clearer demonstration of the importance of automatic splitting, would it be more effective to conduct experiments using an optimal number of splits greater than 2? This could highlight how automatically determined splits align closely with the optimal choice and yield significantly better results compared to manual splitting, especially when using a smaller or incorrect number of splits in the manual splitting, e.g,, 2."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676420933,
                "cdate": 1700676420933,
                "tmdate": 1700676420933,
                "mdate": 1700676420933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OD0vZvMJ7O",
                "forum": "52igC7K5Mf",
                "replyto": "77H9CpmpfM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_mLYv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for carrying out additional experiments. I don't have any more questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718119589,
                "cdate": 1700718119589,
                "tmdate": 1700718119589,
                "mdate": 1700718119589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0d3vy43riD",
            "forum": "52igC7K5Mf",
            "replyto": "52igC7K5Mf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_KhHm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_KhHm"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the topic of Granger causality discovery in multivariate time series and focuses on how to do so using deep learning. The main contribution is a new method called GC-Mixer that leverages an alternative network architecture, and which shows promising results in GC inference with synthetic datasets where the ground truth is known. In addition, the authors propose a method for automatically splitting a long time series to discover time-varying GC dynamics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Granger causality discovery is a difficult problem which is not fully solved by current methods, including those using deep learning. Capturing nonlinear dynamics during GC inference is challenging, and neural networks can help alleviate this problem. However, current approaches can be cumbersome to train and offer performance that is far from perfect, even on these relatively simple datasets. It is therefore worthwhile to pursue alternative approaches like this one, which leverage alternative network architectures. On top of that, it's important to explore solutions for detecting time-varying Granger causal relationships."
                },
                "weaknesses": {
                    "value": "Several questions and concerns:\n\n- It was difficult to follow the description of the mixer architecture, which is one of the main contributions of this work. If I understand correctly, it seems like the second projection in the mixer block, shown in eq. 8, actually mixes information between all the time series. If that's true, wouldn't it mean that every prediction depends on every time series? That should make it difficult to identify which predictors are important, because every forecast automatically depends on every input time series.\n\n- Again, it was hard to follow the description of the network architecture, but it seems like the predictions are ultimately based on an element-wise multiplication between the inputs and the output of the mixer block (this is then fed to a MLP). Would it be fair to interpret these as attention weights? It might be a helpful analogy, because similar notions of soft attention have been used in transparent deep learning.\n\n- I'll temporarily assume, following my question above, that the $W^{(n)}$ values can be viewed as attention weights. When we pass the attention-weighted inputs $M$ into the MLPs $g_i$, do we use separate attention weights for each $g_i$? Otherwise, it would seem that we're forced to make one set of selections for all predictions, whereas we should instead select the relevant inputs for the prediction corresponding to each output series. Either I'm missing something in the notation, or this seems like a restrictive choice.\n\n- It seems inconvenient that the weights $W^{(n)}$ determine the Granger causality relationships, but that they vary for every time point. Compared to the cMLP/cLSTM, it means that an input can be deemed non-causal only if it has small weights for all time points. Is that correct? \n\n- The authors claim that they cannot make the $W^{(n)}$ weights exactly equal to zero, even with the group lasso penalty. This seems correct, because the weights are the output of the mixer block. However, it is untrue that the cMLP/cLSTM share this issue: they regularize parameters of the network, and the parameters can reach zero exactly due to optimization with proximal updates. \n\n- The authors state that they applied the hierarchical penalty to all models tested here, including GC-Mixer, cMLP and cLSTM. However, the cLSTM only has an explicit dependence on one past timepoint, so it's not possible to apply the hierarchical penalty. Indeed, Tank et al discussed that penalty only in the context of the cMLP. Can the authors explain what they mean about using the hierarchical penalty with the cLSTM, because this sounds like a mistake.\n\n- The methods for splitting the time series to discover time-varying dynamics seems reasonable. However, the experiment that tests it seems quite simplistic, and I wonder if the authors could design an experiment that is either more challenging or more realistic. Also, I'm not certain about this, but it seems like the algorithm has no specific relationship with GC-Mixer: it is perhaps unfair to only use it with GC-Mixer in Table 4 and not apply it with cMLP or cLSTM?\n\n- The results with VAR data are encouraging, but this is not the type of data where GC-Mixer should be most valuable. Indeed, we would expect that traditional linear methods would perform far better with this data. On the Lorenz dataset that's actually nonlinear, GC-Mixer underperforms both cMLP and cLSTM.\n\n- The results in Figure 4 look like they did not involve tuning the penalty strength for cMLP. Can the authors describe what they did here and whether it's providing a fair comparison?\n\n- The final architecture is quite complicated, and I wonder if the authors performed any ablations to understand what aspects are important for it to work. For example, could they try with different numbers of blocks? Or removing batch normalization? Currently, it is hard to understand why this type of autoregressive model should enable better GC discovery than a MLP or LSTM - it's a different parameterization, but not obviously better. (And as mentioned above, the empirical results are not convincing on their own.)\n\n- Could the authors provide more details about how they generated AUROC curves? For example, did they keep epsilon fixed and train with different lambda values? Or did they train with one lambda value and sweep epsilon? It would be important to know whether there are any differences compared to previous methods.\n\n- For the title of Section 2.1.2, the \"non-autoregressive\" model looks like it actually is autoregressive, in that it predicts the future using the past. Perhaps what the authors meant to say is that it's nonlinear? This should be corrected.\n\n- Typo in Section 2.2.3: GULE -> GELU"
                },
                "questions": {
                    "value": "Several questions are mentioned in the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799698219,
            "cdate": 1698799698219,
            "tmdate": 1699636278714,
            "mdate": 1699636278714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rK8PhkT3DX",
                "forum": "52igC7K5Mf",
                "replyto": "0d3vy43riD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and correcting our mistakes. We address all the points as follows.\n\nQ1. Wouldn't it mean that every prediction depends on every time series? \n\nYes, every prediction depends on every time series. According to Tank et al., 2021, they proposed component-wise MLP and tackled the challenge that each $X_{ti}$ may depend on different past lags of the other series. Therefore, our mixer architecture will not make it difficult to identify which predictors are important.\n\nQ2. The predictions are ultimately based on an element-wise multiplication between the inputs and the output of the mixer block (this is then fed to a MLP). Would it be fair to interpret these as attention weights?\n\nFirst, we correct the description of W to the causal matrix in our paper, which can better describe W than the previous weight matrix (attention weights). Furthermore, this operation does have some similarities with the soft attention mechanism. Our model adjusts attention to the input time sequences by learning weights for better prediction. The learned weight is the impact of a time lag k of a time series j on the prediction of $X_{ti}$ (due to the matrix being element-wise multiplicated with input time series). Therefore, the matrix W can be used to infer Granger causality after the hierarchical group lasso penalty.\n\nQ3. Do we use separate attention weights for each $g_{i}$?\n\nYes, we use separate $W$ for each $g_{i}$. We have corrected the equation 10, 11, 12, 13 and give a more detailed the description about W in Section 3.1.3.\n\nQ4.  Compared to the cMLP/cLSTM, it means that an input can be deemed non-causal only if it has small weights for all time points. Is that correct?\n\nYes, according to Equation 13, time series $i$ not Granger-cause to series $j$ only if it has small weights for all time points.\n\nQ5. It is untrue that the cMLP/cLSTM share this issue: they regularize parameters of the network, and the parameters can reach zero exactly due to optimization with proximal updates.\n\nWhether the proximal update optimization can penalize the parameter to zero is relative to the lambda value. Here, we give two test cases (cMLP; lambda: 5; Lorenz-96; F:40; optimizer: ISTA), the AUROC we tested is 0.926. The proximal update optimization did not penalize any parameters of the weight matrix in cMLP to zero. However, for another test case (cMLP; lambda: 20; Lorenz-96; F:40; optimizer: ISTA), the AUROC we tested is 0.932. In this case, the proximal update optimization indeed penalizes most parameters of the weight matrix in cMLP to zero. Therefore, the proximal update optimization can regularize parameters to zero, but experimental results show that it does not work in all cases.\n\nQ6: Can the authors explain what they mean about using the hierarchical penalty with the cLSTM, because this sounds like a mistake.\n\nYes, it is a mistake, cLSTM doesn\u2019t use hierarchical penalty, we have corrected this mistake in Section 3.1.3. \n\nQ7.  It is perhaps unfair to only use it with GC-Mixer in Table 4 and not apply it with cMLP or cLSTM?\n\nFollowing your advice, we have applied the proposed multi-level fine-tuning algorithm on cMLP and cLSTM. The results are updated in Table 4.\n\nQ8. The results with VAR data are encouraging, but on the Lorenz dataset, GC-Mixer underperforms both cMLP and cLSTM.\n\nIn our initial tests, we found that cMLP and cLSTM performed excellent on the Lorenz-96 dataset, and their AUROC could be close to 1 in many cases. Unfortunately, these two models performed unsatisfactorily on the VAR dataset. Therefore, our motivation is to propose a model that performs well on both datasets, allowing our model to be adapted for both time-invariant and time-varying scenarios.\n\nQ9. The results in Figure 4 look like they did not involve tuning the penalty strength for cMLP. Can the authors describe what they did here and whether it's providing a fair comparison?\n\nWe selected the best-performing penalty strength for cMLP and GC-Mixer on sparsity = 0.2. Then, we kept the penalty strength unchanged for both cMLP and GC-Mixer and conduct the experiment on sparsity = 0.3. Therefore, it was a fair comparison.\n\nQ10.  Could they try with different numbers of blocks? Or removing batch normalization? \n\nCurrently, we are engaged in ablation experiments to assess the results when altering the number of blocks or removing batch normalization from the model and find what aspects are important for it to work.\n\nQ11. Could the authors provide more details about how they generated AUROC curves?\n\nThe AUROC is generated with one lambda value and sweep epsilon.\n\nQ12. Perhaps what the authors meant to say is that it's nonlinear?\n\nWe have corrected the title of Section 3.2 \u201cnon-autoregressive\u201d into \u201cnonlinear autoregressive\u201d.\n\nQ13. Typo in Section 2.2.3: GULE -> GELU\n\nWe have corrected this mistake in Section 3.1.3."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226621425,
                "cdate": 1700226621425,
                "tmdate": 1700227158782,
                "mdate": 1700227158782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d55V91xayb",
                "forum": "52igC7K5Mf",
                "replyto": "rK8PhkT3DX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_KhHm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Reviewer_KhHm"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their edits to the paper. Some thoughts on the revisions and clarifications:\n\nQ1. It's worth acknowledging that this is strange design choice. The point of sparsity in Tank et al is to fit networks that eliminate dependence on certain inputs. Here, dependence is eliminated via sparsity in $W$, but $W$ itself still depends on all the inputs. It seems possible for a certain input to be truly Granger causal, yet receive no weight via $W$ because its role is to determine which other inputs should be used in the forecast (i.e., it acts like a gating variable). This is kind of a hypothetical concern that may or may not show up with real datasets, so including more realistic ones would be helpful.\n\nQ5. The difference is still notable: choosing a high $\\lambda$ value can ensure that the cMLP/cLSTM eliminates certain input dependencies. For this approach, because we're attempting to sparsify predictions rather than weights, I'm not sure we can guarantee sparsity regardless of the $\\lambda$ value.\n\nQ8. It makes this method significantly less interesting that it underperforms on nonlinear data. Also, I wonder if it's especially well matched for VAR data because $W$ can basically be a constant prediction for all timepoints, $g$ can behave linearly, and we'll recover the exact VAR model. I didn't realize this before, but now that I do I think it's important to expand the experiments before publication.\n\nQ10. I think these ablation results would be important to include before publication.\n\nQ11. I'm pretty sure this differs from how Tank et al generated results: they effectively set $\\epsilon = 0$ and swept $\\lambda$. How do the authors set $\\lambda$ before sweeping $\\epsilon$? This seems like an important hyperparameter choice."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511376287,
                "cdate": 1700511376287,
                "tmdate": 1700511376287,
                "mdate": 1700511376287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X4WcLlelly",
            "forum": "52igC7K5Mf",
            "replyto": "52igC7K5Mf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_1qK9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3298/Reviewer_1qK9"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the Granger Causality inference with the proposed GC-Mixer model. The research topic is interesting, but the paper seems to lack unique motivation and contribution. Also, the theoretical contribution and empirical analysis of the paper are inadequate."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research topic is interesting, using deep learning tools for effectively capturing the non-linear Granger Causality.\n\n- The paper is somehow easy to follow."
                },
                "weaknesses": {
                    "value": "- The novelty and the motivation of the paper are not clear.\n\n- Many state-of-the-art Granger Causality studies like [1,2,3] are missed or not fully discussed and compared in the paper.\n\n- The theoretical contribution seems inadequate.\n\n- The experiments seem weak, to some extent.\n\n- The organization of the paper is busy and can be improved, the authors may want to split Section 2.\n\n[1] Saurabh Khanna, Vincent Y. F. Tan: Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality. ICLR 2020\n\n[2] Ricards Marcinkevics, Julia E. Vogt: Interpretable Models for Granger Causality Using Self-explaining Neural Networks. ICLR 2021\n\n[3] Wenbo Gong, Joel Jennings, Cheng Zhang, Nick Pawlowski: Rhino: Deep Causal Temporal Relationship Learning with History-dependent Noise. ICLR 2023"
                },
                "questions": {
                    "value": "What is the unique motivation and novel contribution of the paper that set it apart from previous studies?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699488028687,
            "cdate": 1699488028687,
            "tmdate": 1699636278644,
            "mdate": 1699636278644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3wHbM8HTsA",
                "forum": "52igC7K5Mf",
                "replyto": "X4WcLlelly",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3298/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. We address all the points as follows.\n\nQ1: The novelty and the motivation of the paper are not clear.\n\nIn our preliminary evaluations, cMLP and cLSTM demonstrated outstanding performance on the Lorenz-96 dataset, with AUROC values often approaching 1. Unfortunately, these two models performed unsatisfactorily on the VAR dataset. Therefore, our motivation is to propose a model that performs well on both datasets, which can adapt for both time-invariant and time-varying scenarios. The novelty of this paper: We introduce a new approach to extract time-invariance Granger causality from the output of Mixer Block in our model, which is different from other models. Furthermore, we propose a new algorithm that enables the model to infer time-varying Granger causality.\n\nQ2: Many state-of-the-art Granger Causality studies like [1,2,3] are missed or not fully discussed and compared in the paper.\n\nWe have supplemented the comparison results of the study [2], as shown in Tables 1, 2, and 3. While the comparison of studies [1] and [3] are still being tested, we are temporarily unable to provide test results.\n\n\nQ3: The experiments seem weak, to some extent.\n\nWe have begun to evaluate performance on more datasets, including Dream3 and Lotka\u2013Volterra. Due to time limitations, we are unable to immediately provide specific results.\n\nQ4: The organization of the paper is busy and can be improved, the authors may want to split Section 2.\n\nWe have split Section 2 into two sections following your suggestion.\n\n[1] Saurabh Khanna, Vincent Y. F. Tan: Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality. ICLR 2020\n\n[2] Ricards Marcinkevics, Julia E. Vogt: Interpretable Models for Granger Causality Using Self-explaining Neural Networks. ICLR 2021\n\n[3] Wenbo Gong, Joel Jennings, Cheng Zhang, Nick Pawlowski: Rhino: Deep Causal Temporal Relationship Learning with History-dependent Noise. ICLR 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227499699,
                "cdate": 1700227499699,
                "tmdate": 1700227499699,
                "mdate": 1700227499699,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]