[
    {
        "title": "Revisiting High-Resolution ODEs for Faster Convergence Rates"
    },
    {
        "review": {
            "id": "b79jFoByWs",
            "forum": "naEeJTlRsr",
            "replyto": "naEeJTlRsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission667/Reviewer_ut71"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission667/Reviewer_ut71"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a general high-resolution ordinary differential equations (ODE) model to investigate the dynamics of various momentum-based optimization methods. The high-resolution ODE proposed unifies many different ODE models and leads to improvement in the convergence guarantee of several existing algorithms, such as triple momentum method in continuous setting and quasi hyperbolic momentum algorithm in discrete setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work provides a high-resolution ODE framework ($\\text{GM}^2$-ODE) that unifies and extends different ODEs in literature. The theoretical analysis is solid and provides some improvement over existing results of accelerated methods. The presentation of the paper is also clear to me."
                },
                "weaknesses": {
                    "value": "- This work follows the line of research on understanding accelerated methods via (high-resolution) ODE. Given the vast literature on this topic, I am afraid the contribution of this work is not significant enough. Although generalization and unification are developed, the results derived here are expected and the techniques are quite standard.\n- The theoretical improvements are kind of minor to me, for example, improving the constant from $1/2$ to $2/3$. The theoretical understanding based on this new high-resolution ODE does not provide any new insight on acceleration. Neither does it lead to any novel algorithms with more attractive practical performance."
                },
                "questions": {
                    "value": "- As I mentioned in Weakness, I am afraid the contribution of this work is significant enough given many existing works on the same topic using almost the same analysis techniques. Could the authors justify this point? Is there any particular novelty in technical and algorithmic developments I'm missing?\n- The work is focused on the theoretical analysis of existing momentum-based algorithms. I'm wondering if the understanding can help develop some new approaches leading to stronger practical performance? For example, does the best possible rate improve empirical performances in practice?\n- In Figure 2, it is observed that NAG is the algorithm with fastest convergence against QHM. I'm curious if the viewpoint of the ODE developed here can provide some explanation to this phenomenon."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712353015,
            "cdate": 1698712353015,
            "tmdate": 1699635993834,
            "mdate": 1699635993834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sRaZmx9QuH",
                "forum": "naEeJTlRsr",
                "replyto": "b79jFoByWs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and comments. Below, we try to address your concerns:\n\n# Weaknesses\n\n- **There is a vast literature**: The *vast literature* on this subject justifies the relevance of our work. We believe that our contribution is both significant and timely, as it unifies various approaches within this extensive body of literature, explaining the connections between them, all while achieving improved convergence rates. \n\n- **Rates are expected**: We respectfully disagree. Please observe that proving these rates through a continuous-time analysis is a non-trivial task, and it was not clear whether it was possible. It is important to note that the rates presented in the earlier works [1,2] are known to be suboptimal in discrete and continuous-time settings, whereas in many cases we achieve the optimal rates. To further highlight this, we have enhanced the discussion about comparisons with prior work in Section 5 with Table 2 in our revision to compare our rates with existing results.\n\n- **Improvements are in the constants**: We agree that our rates are linear, just as in the prior work. However, please note that our improvements are in the *exponents* and not the *constants*. This forms a significant enhancement in comparison with the prior work. \nFor example, Theorem 4.1 proves a convergence rate of order $C_{GM}(1-\\sqrt{ 1/\\kappa})^k)$, and the best previous result in this setting was $5L||x_0-x^*||^2_2(1+\\sqrt{ 1/\\kappa}/9)^{-k}$. Suppose $L=10,\\mu=1,\\kappa=10$ and $k=50$, then $C_{GM}(1-\\sqrt{ 1/\\kappa})^k \\approx 2.35\\times 10^{-7}\\times ||x_0-x^*||_2^2$ while $5L||x_0-x^*||_2^2((1+\\sqrt{ 1/\\kappa}/9)^{-k}) \\approx 8.8939||x_0-x^*||_2^2$.\nSimilarly, for QHM, we prove a rate of order $O((1-\\sqrt{\\frac{1}{3\\kappa}})^k)$ while the previous best rate was $O((1+\\frac{1}{40\\sqrt{\\kappa}})^{-k})$. If $\\kappa=10$ and $k=50$, then $(1-\\sqrt{\\frac{1}{3\\kappa}})^k \\approx 4.19 \\cdot 10^{-5}$ while $(1+\\frac{1}{40\\sqrt{\\kappa}})^{-k} \\approx 0.6745$.\nFinally, we would like to draw your attention to the ability of (GM2-ODE) to recover other high-resolution ODEs. \n- **Leading to other ODEs** \nAs shown in Table 1, (GM2-ODE) and its SIE discretization recover the TM method and its ODE through different choices of coefficients. In addition, the current existing rates on (HR_TM) in the paper are not comparable to the algorithm's convergence rate $(\\mathcal O\\left((1-\\sqrt{\\tfrac{\\mu}{L}}\\right)^{2k}$ which should correspond to $\\mathcal{O}\\left(e^{-2\\sqrt{\\mu}t}\\right)$ in continuous time). This observation suggests a new high-resolution ODE for the TM method. Setting the same coefficients that recover the TM method in (GM2-ODE) reads\n    $$\\ddot{X}_t+\\sqrt{\\mu}\\left(\\frac{3-\\sqrt{\\tfrac{\\mu}{L}}}{1-\\sqrt{\\tfrac{\\mu}{L}}}\\right)\\dot X_t +\\frac{1}{\\sqrt{L}}\\nabla^2 f(X_t)\\dot X_t+\\left(\\frac{2}{1-\\sqrt{\\tfrac{\\mu}{L}}}+\\sqrt{\\frac{\\mu}{L}}\\right)\\nabla f(X_t)=0.$$\nSurprisingly, if the step-size $\\tfrac{1}{\\sqrt{L}}\\rightarrow 0$ the recent ODE reduces to$$ \\ddot{X}_t+3\\sqrt{\\mu}\\dot X_t+2\\nabla f(X_t)=0$$\nwhich is the low-resolution ODE corresponding to the TM method [3]. We will include this discussion in the revision of our paper. ."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506476317,
                "cdate": 1700506476317,
                "tmdate": 1700506476317,
                "mdate": 1700506476317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NI5BsdIiGt",
            "forum": "naEeJTlRsr",
            "replyto": "naEeJTlRsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission667/Reviewer_fWEa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission667/Reviewer_fWEa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a high resolution ODE (GM2-ODE) for analyzing accelerated gradient descent (AGD) for convex optimization. A Lyapunov function based on integral quadratic control is derived to analyze the stability and convergence rate of GM2-ODE. Semi-Implicit Euler discretization (SIE) of the ODE recovers the accelerated gradient algorithms and the known optimal convergence rates. Many previous ODEs for accelerated gradient descent can be formulated into the proposed GM2-ODE form and the convergence rates \n can be obtained using their results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written. The presentation of their results is clear and sound.\n\nsiginificance:\nThe proposed GM2-ODE enjoys intuitive form and design of Lyapunov function. The discrete time convergence rates based of the continuous time Lyapunov function recovers the optimal convergence rate of accelerated gradient method. The analysis framework applies to many previous ODEs for accelerated gradient methods and recover (even enhance) the discrete-time convergence rates."
                },
                "weaknesses": {
                    "value": "NA"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735755133,
            "cdate": 1698735755133,
            "tmdate": 1699635993746,
            "mdate": 1699635993746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5Vf5WnOfye",
                "forum": "naEeJTlRsr",
                "replyto": "NI5BsdIiGt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive evaluation and comments on our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505750765,
                "cdate": 1700505750765,
                "tmdate": 1700505750765,
                "mdate": 1700505750765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Js9PjL6UUQ",
            "forum": "naEeJTlRsr",
            "replyto": "naEeJTlRsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission667/Reviewer_oAou"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission667/Reviewer_oAou"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a unified framework to analyze the high-resolution ODEs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "No"
                },
                "weaknesses": {
                    "value": "The high-resolution ODE was originally proposed to find the mechanism behind Nesterov's acceleration.  I have never found any new in this paper."
                },
                "questions": {
                    "value": "Could you express your motion to do this paper? Could you show where are the new parts beyond the current research?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission667/Reviewer_oAou"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746615516,
            "cdate": 1698746615516,
            "tmdate": 1699635993664,
            "mdate": 1699635993664,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mZR1jieYMH",
            "forum": "naEeJTlRsr",
            "replyto": "naEeJTlRsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission667/Reviewer_6yy2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission667/Reviewer_6yy2"
            ],
            "content": {
                "summary": {
                    "value": "The approach of this paper is (1) to provide a unifying high-resolution ordinary differential equations (HR-ODEs) to several ones in the literature for momentum-based methods for minimization, and then (ii) to use a tool from control theory called integral quadratic constraints (IQC) to derive a Lyapunov function used for convergence analyses.\n\nFor strongly convex and smooth functions, it:\n- achieves a faster convergence rate for the triple momentum method,\n- achieves a faster rate for the Quasi Hyperbolic Momentum method (and for a larger step size range)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The overall idea of unifying ODE for the momentum-based HR-ODEs is interesting. \n- The paper improves some of the convergence rates in the literature for accelerated methods in the strongly convex regime."
                },
                "weaknesses": {
                    "value": "# Novelty \\& incremental results\n\nThe main concern is the incremental contributions. \n- The techniques used are well known (Lyapunov analysis, Gr\u00f6nwall lemma, etc), e.g. Thm. 3.2. is an instance of Gronwall lemma. \n- The improved factor for NAG is only 2. The triple momentum method is not widely used; there has been less focus on improving its rate.\n- *GM2-ODE.*  In terms of structuring, it is surprising that the proposed GM2-ODE is stated in the introduction. Moreover, this ODE is not derived but rather considers a set of some HR-ODEs that exist and aims to unify them in the sense that these can be seen as instances of the GM2-ODE. This is fairly straightforward to do given several ODEs as the terms that appear have already known interpretations; there's no discussion or further development if this ODE is general enough to lead to other useful methods. Also, it is very similar to the existing ODE in Zhang et al. (2021), see eq. GM-ODE in the main part. Considering all, this is a fairly limited contribution stated as central/main.\n- The only considered setting is (deterministic) smooth, strong convexity.\n\nAlthough these contributions are interesting, they are not developed sufficiently for acceptance.\n\n# Writing \n\nThe paper reads well, and I enjoyed reading it. However, content-wise, it is not on point regarding the actual focus of this paper / exact contributions / motivation for these contributions, etc. It often focuses on general optimization comments that are enjoyable to read but perhaps more suitable for a textbook, etc., and due to that, it is not concise in bringing the reader to the actual contributions and their motivation. For example:\n- Abstract. A large part focuses on general comments about HR-ODEs or the Lyapunov function, which is an intermediate step of proving convergence that many methods can be seen as a discretization of the ODE, which is often the case. \n   - Importantly, it leaves very unclearly what precisely the \"improved convergence guarantees compared to prior art\" are -- it would be helpful to state precisely if the constants or the order is improved and by what factor; what is the precise advantage of this unifying HR-ODE (is it more interpretable, etc), etc.\n   - It does not even mention the setting, e.g., that the results are for strongly convex functions\n   - That discrete methods can be viewed as discretizations of ODEs is well known. If keeping this sentence, it is worth mentioning the type of discretization.\n- Introduction. The first two paragraphs that refer to (discrete) optimization methods generally are very enjoyable to read. Still, the motivation for using continuous-time analyses is rushed, which is more relevant to this paper. The paper would benefit from reconsidering the content vs. the page limit and prioritizing better.\n\n\n\n# Missing smoothness assumption in Thm 3.1 and unclear notations\n\nThm. 3.1. states that $f$ is strongly convex, but the proof relies on Thm. 6.4 in (Fazlyab et al., 2018), which uses the assumption that $f$ is also $L$ smooth. This assumption should be stated. \nThe proof in App. B.2. also mentions $\\sigma$, which is not defined in the paper.\nThe curly F notation, used in the main part and Thm. A.1 was not introduced.\n\n# Other\n\n- The constant $C_{QHM}$ that appears in Cor. 4.1.1. is not defined\n\n# minor comments\n\n- missing full stop in eq. (1)\n- typo: instable\n- sec. 3 title should be continuous-time analysis"
                },
                "questions": {
                    "value": "1. The abstract highlights that different methods can be seen as discretizations of the GM2-ODE. There are many discretization methods, and many works highlight that discrete methods can be obtained of a general ODE under some discretization [1]. Could you elaborate on why your work concentrates on crafting ODE that yields the methods through the SIE discretization versus the others and why it is more validating the derived ODE versus the other? Or is it providing more consistency with the discrete analysis? I believe this is central to be discussed since it is highlighted.\n2. On Page 7, with \"the phase space representation [..] cannot exactly recover the NAG algorithm after discretization\", which discretization do you assume here?\n3. In the [2] follow-up work of Shi et al., 2021 (on which this work builds the idea of HR-ODEs), a more \"consistent\" way of deriving the HR-ODEs is proposed. App. A.3. of [2] points out that such derivation is more consistent because the Taylor expansion is done on all applicable terms instead of some. Does your HR-ODE unify the HR-ODEs of the NAG and HB methods derived that way? \n4. Do you know if this HR-ODE will lead to better convergence rates on other setups, e.g., convex? In other words, is the benefit of modifying the ODE of Zhang et al. 2021 specific to the strongly convex setup?\n5. Is the upper bound dictated by Thm 4.1. matching the known one for this setting for the constants?\n\n-----\n[1] *On dissipative symplectic integration with applications to gradient-based optimization*, Fran\u00e7a, Jordan, and Vidal, 2021.\n\n[2] *Last-Iterate Convergence of Saddle-Point Optimizers via High-Resolution Differential Equations*, Chavdarova, Jordan, and Zampetakis, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699127087784,
            "cdate": 1699127087784,
            "tmdate": 1699635993584,
            "mdate": 1699635993584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IumphTCYoZ",
                "forum": "naEeJTlRsr",
                "replyto": "mZR1jieYMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time, careful reading, and valuable comments. Our responses to the specific concerns and comments are below: \n\n# Novelty and Significance\n- **Use of IQCs**: It is true that we leveraged IQCs, but please note we use it only in one of the three Lyapunov functions we propose in this paper. We introduced two more Lyapunov functions which cannot be derived from the IQC theorem: One is in continuous time (Theorem 3.2), and the other one is in discrete time (equation (7)). The latter is the first Lyapunov function that uniformly captures the discrete time behavior of the algorithms for different choices of parameters.\n- **Use of well-known techniques**: It is true that we use well-established techniques like Lyapunov analysis and Gr\u00f6nwall lemma. Nevertheless, we leverage these tools to achieve a family of Lyapunov functions whose form is not seen before in the literature. Crucially, our analysis is NOT a trivial extension of another work, the form of the unified Lyapunov function requires different steps in the analysis. For example, we use a limit analysis after (55) in the appendix which is different from all of the prior works. This is just one example of the many analytical and algebraic complexities that arise from the generality of the Lyapunov function we addressed in our analysis. \n- **Improvements are in the constants**: We agree that the rates are linear similar to the state-of-the-art. However, it is important to note that the improvements are not in the constants but in the exponents of the linear rate, which significantly tightens the bounds. \nFor example, Theorem 4.1 proves a convergence rate of order $C_{GM}(1-\\sqrt{ 1/\\kappa})^k)$, and the best previous result in this setting was $5L||x_0-x^*||^2_2(1+\\sqrt{ 1/\\kappa}/9)^{-k})$. Suppose $L=10,\\mu=1,\\kappa=10$ and $k=50$, then $C_{GM}(1-\\sqrt{1/\\kappa})^k \\approx 2.35\\times 10^{-7}\\times ||x_0-x^*||^2_2$ while $5L\\|x_0-x^*\\|^2_2((1+\\sqrt{ 1/\\kappa}/9)^{-k}) \\approx 8.8939||x_0-x^*||_2^2$.\nSimilarly, for QHM, we prove a rate of order $O((1-\\sqrt{\\frac{1}{3\\kappa}})^k)$ while the previous best rate was $O((1+\\frac{1}{40\\sqrt{\\kappa}})^{-k})$. If $\\kappa=10$ and $k=50$, then $(1-\\sqrt{\\frac{1}{3\\kappa}})^k \\approx 4.19 \\cdot 10^{-5}$ while $(1+\\frac{1}{40\\sqrt{\\kappa}})^{-k} \\approx 0.6745$.\nTo highlight our improvements, we have now added Table 2 to showcase the faster convergence rates both in continuous and discrete-time.\n\n- **Comparison with GM-ODE**: We discussed the differences between GM-ODE and GM2-ODE in detail in pages 8-9 in our submission. The main drawback of GM-ODE is its inconsistency in terms of ODE and algorithm recovery. Moreover, GM2-ODE is more capable of recovering algorithms and leads to better convergence guarantees. Please note that the Lyapunov function corresponding to GM-ODE does not achieve the optimal rates. Also note that discretization of GM-ODE using SIE method does not exactly recover the NAG algorithm. Finally, we would like to draw your attention to the ability of GM2-ODE to recover other high-resolution ODEs. \nOur unified model (GM^2-ODE) has the following two attractive features: \n1. Not only does it unify the ODEs in continuous time, but also existing algorithms through SIE discretization. Surprisingly, all these algorithms are found to be well-known methods (NAG,HB,TMM, etc) and they are all recovered through the same routine. \n 2. The corresponding *unifying* Lyapunov function leads to better convergence rates than the previous results obtained by continuous time analysis. To further emphasize this, we added Table 2 to compare our rates with existing results, which shows precisely the improvements.    \n- **Less condition on the convergence of the QHM algorithm**: Please note that the previous rate on QHM requires $L/\\mu\\geq 9$ [6], while our result in corollary 4.1.1 is free from such condition.\n- **Does GM2-ODE lead to new methods?**: As shown in Table1, (GM2-ODE) and its SIE discretization recover the TM method and its ODE through different choices of coefficients. In addition, the current existing rates on (HR_TM) in the paper are not comparable to the algorithm's convergence rate ($\\mathcal O\\left((1-\\sqrt{\\tfrac{\\mu}{L}}\\right)^{2k}$ which should correspond to $\\mathcal{O}\\left(e^{-2\\sqrt{\\mu}t}\\right)$ in continuous time). This observation suggests a new high-resolution ODE for the TM method. Setting the same coefficients that recover the TM method in (GM2-ODE) reads\n\n$$\\ddot{X}_t+\\sqrt{\\mu}\\left(\\frac{3-\\sqrt{\\tfrac{\\mu}{L}}}{1-\\sqrt{\\tfrac{\\mu}{L}}}\\right)\\dot X_t +\\frac{1}{\\sqrt{L}}\\nabla^2 f(X_t)\\dot X_t+\\left(\\frac{2}{1-\\sqrt{\\tfrac{\\mu}{L}}}+\\sqrt{\\frac{\\mu}{L}}\\right)\\nabla f(X_t)=0.$$\n\nSurprisingly, if the step-size $\\tfrac{1}{\\sqrt{L}}\\rightarrow 0$ the above ODE reduces to $$ \\ddot{X}_t+3\\sqrt{\\mu}\\dot X_t+2\\nabla f(X_t)=0$$\nwhich is the low-resolution ODE corresponding to the TM method [3]. This discussion will be added to the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505636411,
                "cdate": 1700505636411,
                "tmdate": 1700505636411,
                "mdate": 1700505636411,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]