[
    {
        "title": "Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data"
    },
    {
        "review": {
            "id": "ZAcFkuirYH",
            "forum": "Tr3fZocrI6",
            "replyto": "Tr3fZocrI6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_St8o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_St8o"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of linear representation in a multi-task regression setting. As a starting point, they use an alternating minimization procedure (AMD) developed in prior works on the same problem. They showed empirically that this procedure can fail to learn the correct representation when there is noise in the observations or non-isotropic covariates, even when the different tasks are identical, and gave a theoretical explanation for the sources of error. Based on their analysis, they propose a modification to the alternating minimization procedure (dubbed DFW) which can handle noisy observations and non-isotropic covariates, and experiments confirm the efficacy of their modification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Clarity of exposition.** The paper is very well written and easy to follow. The authors give extensive interpretation of their results which greatly contributed to my understanding of the paper. The precise relationship to previous work is made explicit, so readers unfamiliar with this sub-field can still parse the paper and understand its contribution easily.\n\n**Intuitive and well-motivated algorithm.** The shortcomings of the base algorithm (AMD) are explained clearly, as are the modifications the authors proposed in DFW, making for an intuitive algorithm. The modifications are simple, easy to implement, and obtain near optimal sample complexity rates.\n\n**Technical contribution.** The authors remove strong technical assumptions found in previous work. They show both theoretically and empirically that these strong assumptions are necessary for AMD to succeed, and are not merely artifacts of previous proofs. Their results are strong both statistically (obtaining optimal sample complexity) and algorithmically (not requiring access to optimization oracles for non-convex problems, which were assumed in some previous works). Their algorithm is also constructed in such a way that data does not need to be shared in its explicit form across tasks, making it attractive when data privacy is a concern. (Remark: It is unclear if the representation _updates_ from each task will still leak private information, but anyway this is not the main focus of the paper.)"
                },
                "weaknesses": {
                    "value": "**Theory.** While the assumptions are much weaker than those in related works, some of the assumptions are still very strong. Two in particular stand out.\n1. The representation dimension $r$ is required to be at most $\\min(d_x, d_y)$, where $d_x$ is the dimension of the covariates and $d_y$ is the dimension of the observations (Section 2, just after equation (1)). In the linear regression setting, this would mean that there must be a one-dimensional representation. This is a very strong assumption. It seems like we should still be able to obtain some benefit if the representation only has a lower dimension than the _covariates_. This would more closely mirror practical settings such as e.g. computer vision, where the data are assumed to belong to a lower-dimensional manifold.\n2. Assumption 3.1: It is assumed that the $\\beta$-mixing coefficient follows an _exact_ geometric decay, i.e., $\\beta^{(t)}(k) = \\Gamma^{(t)} \\mu^{(t)k}$ for each task $t$. This should place strong restrictions on the possible types of covariate trajectory distributions. It seems like we should expect the results if the decay is _at least_ geometric in nature, i.e., $\\beta(k) \\leq \\Gamma \\mu^k$ for some $\\mu < 1$.\n\n**Experiments.** The empirical results would be more convincing at showing a fundamental limit on the accuracy for AMD if final accuracy vs. number of tasks was shown at a fixed sample size per task, and showing that this accuracy does not approach 0 as the number of tasks increases. At present, it is just shown for T=25. While DFW does converge in this scenario, in principle, it could just be that DFW has a better sample complexity, but AMD will still eventually converge given enough tasks, albeit at a slower rate. Adding this experiment would strengthen the paper.\n\nA minor point: the title of the OpenReview submission does not match the title on the paper. This should be fixed."
                },
                "questions": {
                    "value": "1. I am curious why the required number of samples $N$ grows (moderately) with the number of tasks $T$. I assume this is to enforce some sort of uniform bound on the random fluctuations across all of the tasks. Can the authors confirm if this intuition is correct?\n\n2. Is there some intuition for why the representation dimension $r$ must be smaller than both the covariate _and_ measurement dimensions? If this is a necessary assumption, can the authors comment on how they would justify this restriction, especially in the linear regression case when $d_y=1$?\n\n3. In Definition 3.1, is there an implicit assumption that the stationary distribution $\\nu_\\infty$ exists, or are there some conditions imposed on the covariate trajectory distributions which guarantee that a stationary distribution will exist as a consequence?\n\n4. Do the results still hold if the equality for $\\beta(k)$ in Assumption 3.1 is replaced with an inequality?\n\n5. It is very interesting that the use of an MLP allows the original AMD algorithm to overcome the fundamental lower bound on the error present when learning a linear representation (even if the sample complexity is much worse than DFW). Is this just because the quantity being measured (validation loss instead of subspace distance) is different, or would AMD with a linear representation fail to converge to 0 validation loss in this setting? If this is particular to the MLP representation, do the authors have any intuition for why this might be the case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Reviewer_St8o",
                        "ICLR.cc/2024/Conference/Submission4744/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4744/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697748423984,
            "cdate": 1697748423984,
            "tmdate": 1700495688876,
            "mdate": 1700495688876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UF8q71j10P",
                "forum": "Tr3fZocrI6",
                "replyto": "ZAcFkuirYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments. To address the remaining questions/concerns:\n\n- The most updated title of our paper is the one on OpenReview: \u201cSample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data\u201d. We apologize for the confusion.\n\n- The $r \\leq \\min\\{d_y, d_x\\}$ is a typo; as one should expect, we only require $r \\leq d_x$ for our analysis and can accommodate arbitrary $d_y$, e.g. $d_y = 1 \\ll d_x$ for scalar-output linear regression, or $d_y \\approx d_x$ in linear system identification (Appendix B.1). We thank the reviewer for identifying this important oversight.\n\n- The beta-mixing coefficient only needs to be upper bounded by geometric decay. This will be fixed in the revision.\n\n- **Regarding the requested experiment for alternating minimization-descent (AMD)**: we took the same experimental parameters for linear representations as described in Section 4.1 (corresponding to Fig 1a), except varying the number of total tasks $T \\in \\{1, 5, 10, 25, 50, 100\\}$. For each $T$, we run alternating minimization-descent for a single run of 5000 iterations ($\\gg 100$ iterations shown in Fig 1a and more than in the MLP experiment) and record the subspace distances. For comparison, we run DFW with a single task $T=1$. This generates the following figure ([anonymous google drive link](https://drive.google.com/file/d/1ztBpxcl0MTcoupD8L_DZ1gsRENrCQPGr/view?usp=sharing)). The plot demonstrates that: 1. increasing the number of tasks *cannot push AMD past a (large) threshold*, only serving to reduce the variance, precisely as our theory predicts, 2. *AMD does not encounter a feature learning phase* as does an MLP (Figure 1b) that drives the distance to $0$, even after many iterations. In comparison, DFW even for a single task quickly converges to $0$, confirming that the suboptimality of AMD is due to a fundamental bias, rather than the noise level of the problem.\n\n- **Regarding the moderate growth of $N$ with respect to tasks $T$**: this is a question with some subtlety. There are two sources to the $\\log(T)$ dependence\u2013one that arises in the analysis of the iid setting, and the other from $\\beta$-mixing.\n   - In the iid setting, as the reviewer suspects, the $\\log(T)$ dependence comes from an eventual union bound (a.k.a. uniform control) over the noise processes. In particular, to bound the largest singular value of the average of self-normalized martingale (SNM) noise across tasks $T$, we appeal to a Matrix Hoeffding inequality (Lemma A.4). However, like the traditional Hoeffding inequality, this requires boundedness in the psd order of each summand (i.e. each task\u2019s SNM). Since we have concentration inequalities for individual SNMs (Propositions A.1 and A.2), a simple way to simulate boundedness is to condition on the high-probability boundedness event of each task\u2019s SNM. However, conditioning simultaneously on each task\u2019s boundedness equates to a union bound, which leads to a $\\log(T)$ factor when inverted for a desired failure probability $\\delta$. The $\\log(T)$ factor cannot be avoided when going through the route of boundedness/truncation even though SNMs are independent between different tasks (see e.g. subgaussian maximal inequalities). *However*, we conjecture that this $\\log(T)$ factor is only technical, and *may be removable in the iid setting*. This requires going from a (matrix) Hoeffding to a Bernstein-type inequality, which in turn requires control of the higher-order moments of the SNM. As far as we can tell, this would require highly non-trivial, novel analysis of self-normalized martingales, and importantly, **the mild savings will be washed away (order-wise) in the $\\beta$-mixing setting**, where they arise for a fundamental reason. (continued in next comment)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279388994,
                "cdate": 1700279388994,
                "tmdate": 1700279388994,
                "mdate": 1700279388994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rhPXamfNQW",
                "forum": "Tr3fZocrI6",
                "replyto": "O14zkb6ReW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Reviewer_St8o"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Reviewer_St8o"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their very thorough response. All of my concerns have been fully addressed, and the technical discussions provided by the authors (specifically on the log(T) dependence for N) provided even more insight into the results. The other reviewers raised some interesting questions, and it seems that the authors have resolved these as well. I have increased my score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495664015,
                "cdate": 1700495664015,
                "tmdate": 1700495664015,
                "mdate": 1700495664015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HwvWf8SJtg",
            "forum": "Tr3fZocrI6",
            "replyto": "Tr3fZocrI6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_ZRaH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_ZRaH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an algorithm about learning the representation is a linear connection between feature and labels. The algorithm is based on gradient descent and QR decomposition on the iterates. The paper further proves a bound about the sample complexity and error of the algorithm, which is optimal in terms of problem parameters (degree of freedom). Numerical experiments validates the performance of the algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes A practical and simple algorithm, and the theories as well as the math proof of the sample complexity (per batch and in total) and error are solid in terms of the degree of the freedom. The logic and the writing is clear. \nEspecially, Remark 3.2 is great where we can see that the lower bound of $N$ makes sense. Some other papers, although claiming optimality with respect to total samples $NT$, there is a strong assumption on lower bound $N$ that makes them trival, e.g., Du et al."
                },
                "weaknesses": {
                    "value": "On the other hand, does Tripuraneni et al. work when $N = O(1)$? This paper assumes $N = \\Omega(r)$ so there is still a gap from the optimum. The result in this paper is still good because $r$ it's a small number in low rank setting which we are interested in, and it is already better than the papers listed in Remark 3.2. But it would be great to propose why this paper cannot achieve $N = O(1)$. \n\nSince this paper discusses general feature covariances, it would be great to talk more about the impact of the spectrum of the covariance matrix. There are a few papers about how the feature and operator covariances\u2019 spectrums show up in the bound, and how the \"aligned\" covariances help learning, for example,\n\nWu and Xu, On the Optimal Weighted $\\ell_2$ Regularization in Overparameterized Linear Regression\n\nAnd a few relevant ones. \n\nIt would be great to have a notation table, either in main text or appendix, because there are many different notations/definitions."
                },
                "questions": {
                    "value": "No more questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4744/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437561527,
            "cdate": 1698437561527,
            "tmdate": 1699636456438,
            "mdate": 1699636456438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hLbrOxSAj2",
                "forum": "Tr3fZocrI6",
                "replyto": "HwvWf8SJtg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their evaluation. To address the remaining questions:\n\n- We are introducing a notation table to our revised appendix.\n\n- **Regarding the burn-in requirement $\\Omega(r)$**: the requirement in Tripuraneni et al. (and similar papers) is indeed $\\Omega(1)$. However, we strongly emphasize that the proposed algorithm in that paper critically relies on the fact that the covariates across **all tasks** are iid and isotropic. From an algorithmic perspective, their proposed method-of-moments estimator is designed with the implicit foreknowledge that $\\mathbb E[x x^\\top] = I$, and is only guaranteed to be consistent then. On the other hand, their proposed lower bound on the subspace distance, by assuming all covariates are iid $\\mathcal N(0,I)$, in fact does not even require $N > 0$ for any given task, as long as the total samples $NT$ is large enough. However, this quirk is inextricably tied to the iid covariate assumption, which is reflected in the proof of the result (Lemma 22 of their appendix). Intuitively, we should not expect $N = \\Theta(1)$ to be possible, in the sense that allowing the covariances across each task to differ permits the design of weights $F^{(t)}$ and covariances $\\Sigma_x^{(t)}$ that are arbitrarily ill-conditioned such that distinguishing $\\Phi_\\star$ and a perturbed $\\Phi\u2019_\\star$ from sampled-data is provably hard unless a sufficient per-task burn-in on $N$ (e.g. proportional to the row-rank of $\\Phi$) is satisfied. Formalizing this lower bound, as well as extracting the task-diversity/task-coverage assumptions of the task-wise covariances $\\Sigma_x^{(t)}$ that do not manifest in the task-wise iid setting is ongoing work.\n\n- We agree that **random features in addition to random covariate design** is an interesting consideration. We note that when we set the weights $F^{(t)}$ to be random (e.g. Gaussian random matrix), keeping $\\Phi_\\star$ as deterministic (otherwise the low-rank representation learning problem changes), our analysis actually goes through *mutatis mutandis*: in particular, instead of deterministic task diversity parameters $\\lambda_{\\min}^{\\mathbf F}, \\lambda_{\\max}^{\\mathbf F}$ (Definition 3.2), we can replace them with high-probability variants, carrying out the rest of the analysis as written and simply accruing the additional failure probability in the final bound. **Regarding the alignment of covariances**, we note some subtlety in Theorem 3.1 that was washed out for simplicity: the dependence on task-specific quantities, such as the smallest covariance eigenvalue $\\lambda_{\\min}(\\Sigma^{(t)}_x)$, are actually averaged across tasks in a certain way, where they currently appear as a max over tasks $t$ for simplicity. After inverting for sample complexity bounds as in Corollary 3.1, these task \"coverage\" / \"overlap\" quantities naturally manifest in the bound, which may be seen as notions of how the alignment and/or coverage of source task covariances affect learning. Lastly, a **theory for an overparameterized setting** is quite interesting, and seems to be getting concurrent attention; however, the data / feature assumptions are necessarily quite distinct from ours, and thus we cannot claim to easily extrapolate the trends in our results there."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278696493,
                "cdate": 1700278696493,
                "tmdate": 1700280049704,
                "mdate": 1700280049704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4SVUIo0aCi",
            "forum": "Tr3fZocrI6",
            "replyto": "Tr3fZocrI6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_6Cex"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_6Cex"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out a failed example of traditional algorithms in handling non-isotropic data. To overcome this issue, it proposes an algorithm called De-bias & Feature-Whiten (DFW) for multi-task linear representation learning from non-iid and non-isotropic data. DFW provably recovers the optimal shared representation at a rate that scales favorably with the number of tasks and data samples per task. Numerical verification is also provided to validate the proposed algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Regarding the originality, few meta- federated- learning papers are working on non-iid settings. So this paper has its own novelty.  \n\nThe paper is also well-structured and clearly states the necessary backgrounds, though some technical details should be further extended.\n\nThe example on the non-IID non-isotropic data provides a clear motivation of proposing a new algorithm to overcome this issue. It indicates parts of significance of this work."
                },
                "weaknesses": {
                    "value": "The title \"META-LEARNING OPERATORS TO OPTIMALITY FROM MULTI-TASK NON-IID DATA\" is so vague. It is really hard to understand what this paper studies from the title. It should indicate that the goal is to learn the shared parameter $\\Phi$.\n\nThe failed example given in Section 3.1 serves as the main motivation of introducing new algorithms. However, these two crucial issues in this example are not really resolved. I am concerned if the de-bias and feature-whiten steps could really resolve these issues. I put more comments in the next section."
                },
                "questions": {
                    "value": "1. First about clarifying the key idea de-bias and feature-whiten methods. In Section 3.2, it says that $\\hat{F}^{(t)}$ is computed on independent data. It is not clear to me why $\\hat{F}^{(t)}$ is independent from $X^{(t)}$.  To my understanding, for example, the Partition trajectories step (Line 5, Algorithm 1) splits the dataset $N$ to $N_{1}=\\\\{x_1, x_2, \\dots, x_n \\\\}$ and $N_{2}=\\\\{x_{n+1}, x_{n+2}, \\dots, x_{n+N} \\\\}$. But they come from the same $\\beta$-mixing stochastic process, will they become independent?\n\n2. What is \"the aforementioned batching strategy\" mentioned in Section 3.2 right after Eq.(5)? It seems that there is no batching strategy mentioned before. \n\n3. The proof for the non-iid case simply says after taking the \"blocking technique on each trajectory\", everything is same as the iid case. First, what is the \"blocking technique on each trajectory\"? Has this technique been introduced before? \n\n4. Then regarding the proof for the non-iid case, I am mainly concerned if the iid case could be simply immigrated to the non-iid case. For example, on page 16 of the supplimentary material, it says \"We observe that since $\\hat{F}(t)$ is by construction independent of ... \" and obtains $$E[FWX\\Sigma^{-1}]=E[F]E[W]E[X\\Sigma^{-1}]$$\nThis equation won't hold for the non-iid case. It is because $F$ here is evaluated using a part of the process $\\{x\\}$ and $\\Sigma$ is estimated using another part of the same process."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Reviewer_6Cex"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4744/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698526324361,
            "cdate": 1698526324361,
            "tmdate": 1700332159346,
            "mdate": 1700332159346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D13G2TWceG",
                "forum": "Tr3fZocrI6",
                "replyto": "4SVUIo0aCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments. To address their questions:\n\n- The title in the submitted pdf is in error. The most updated version is the one seen on the OpenReview page \u201cSample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data\u201c, which clarifies the goal of learning a shared representation.\n \n- In general, **regarding the possible correlation due to sampling from the same $\\beta$-mixing process**, our main technical workhorse is Lemma A.3, which is a standard tool used in the analysis of mixing processes. In short, the expected value of a bounded measurable function of a given $\\beta$-mixing process and the same function on the iid version of the process (where each covariate $x_i$, $i=1,\\dots,$ is sampled from its population distribution), can be bounded in terms of the mixing function $\\beta(\\cdot)$. In particular, if said function is an indicator of an event of interest, e.g. the complement of the descent guarantee Eq (11) in Theorem 3.1, then this implies the probability of the event occurring on the mixing process versus the independent version is the same up to an additive factor that depends on the mixing function. Therefore, the iid results hold for the mixing case, albeit with a smaller effective sample size in order to achieve the same failure rate $\\delta$. For geometric mixing processes, this essentially amounts to reducing $N \\to N/\\log(N)$ samples. This addresses a couple of questions:\n   - **The discussion of the \u201cblocking\u201d technique** is located around Definition A.1 ($\\beta$-mixing) and Lemma A.3. We will make references to blocking clearer in our revision.\n   - **Regarding independence of $X^{(t)}$ and $\\hat F^{(t)}$: in the mixing case**, they are indeed possibly not independent. However, as discussed, their behavior can be bounded by the analysis of the iid case, paying the appropriate costs for dependency in the mixing coefficients. Similarly, the decomposition of $F, W, X\\Sigma^{-1}$ need only hold for the iid setting.\n\n- **The \u201caforementioned batching strategy\u201d mentioned after Eq (5)** refers to the discussion just preceding Eq (5): \u201ceach agent computes the least squares weights $\\hat F^{(t)}$ and the representation update on independent batches of data, e.g. disjoint subsets of trajectories\u201d. We will adjust the wording in the revision, as we indeed do not discuss a concrete strategy until the subsequent paragraphs.\n\nIn light of this discussion, we hope that this answers the reviewer\u2019s concerns about whether our proposed algorithm and analyses fully overcome the identified non-isotropy and non-iid issues, as far as the $\\beta$-mixing assumption holds."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278429121,
                "cdate": 1700278429121,
                "tmdate": 1700278429121,
                "mdate": 1700278429121,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nK1BDxsOEx",
                "forum": "Tr3fZocrI6",
                "replyto": "D13G2TWceG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Reviewer_6Cex"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Reviewer_6Cex"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification! Now I could understand the proof of the non-iid case. Since my concerns are addressed, I will increase my score accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332139267,
                "cdate": 1700332139267,
                "tmdate": 1700332139267,
                "mdate": 1700332139267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TXmhWmUB2G",
            "forum": "Tr3fZocrI6",
            "replyto": "Tr3fZocrI6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_4V27"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4744/Reviewer_4V27"
            ],
            "content": {
                "summary": {
                    "value": "In \"Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data\" proposes a scheme and statistical guarantees to problems stemming from multi-task learning. In this setting, prior works focused on i.i.d. and isotropic data while the proposed work allows for non-i.i.d. and non-isotropic data. In order to design the scheme and provide with statistical guarantees, in which learning all tasks jointly implies a statistical gain, the authors modify a proposed scheme for the i.i.d. and isotropic data by including mini-batches and whitening.\n\nThe obtained results are what is expected in terms of statistical precision, given the total number of samples, tasks and problem dimension."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is overall well written, and I think the result is good enough. While a criticism can be put forth in that it combines existing known techniques to establish the final result, it is not necessarily obvious that the combination yields the desired statistical result."
                },
                "weaknesses": {
                    "value": "I am overall happy with the paper. I think the authors did a good job at presenting their work. I mainly have two questions/weaknesess. The result provided by the authors requires a minimum number of samples under which contraction upto a ball of the alignment of the estimated and optimal subspace are. Is there any sense to how tight this bound is from an information theoretical sense, i.e. the scaling with gamma, mu, etc? \n\nSecond, in corollary 3.1. the authors establish the existence of a partition of independent batches that guarantees this result. Can it be guaranteed that such partition is found in practice?"
                },
                "questions": {
                    "value": "- While both are valid, the title in the pdf file and the title given within the open review system do not match."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4744/Reviewer_4V27"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4744/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634520196,
            "cdate": 1698634520196,
            "tmdate": 1700416302320,
            "mdate": 1700416302320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1xhOaUWcxw",
                "forum": "Tr3fZocrI6",
                "replyto": "TXmhWmUB2G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. To address the reviewer\u2019s questions:\n\n- The most updated title of our paper is the one on OpenReview: \u201cSample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data\u201d. We apologize for the confusion.\n\n- **Regarding Corollary 3.1**, the requisite partitions are actually very simple to construct in practice. In short, given the assumptions of Theorem 3.1, we are guaranteed the distance-to-optimality of the next iterate is decomposed into a contraction of the previous iterate\u2019s distance and an additive noise term. Therefore, the idea that yields Corollary 3.1 is to simply partition a given offline dataset into exponentially growing chunks, such that the noise term is sufficiently small with respect to the contraction. Namely, if $d_{t+1} \\leq \\rho \\cdot d_t + \\frac{\\sigma}{\\sqrt{n}}$, then any schedule for $n$ that ensures $d_{t+1} \\leq \\rho\u2019 \\cdot d_t$, $\\rho\u2019 < 1$, $t =0,1,\\dots$ suffices. For an offline dataset of size $N$, an exponential growing partition can be accommodated $K = O(\\log N)$ times, which yields $d_K \\lesssim C(\\rho\u2019) \\frac{\\sigma}{\\sqrt{N}}$, hence approximating \u201cERM-like\u201d rates of $\\approx \\frac{\\sigma}{\\sqrt{N}}$. We note that this is a straightforward adaptation of the standard \u201cdoubling trick\u201d in online learning, and discussed in further detail in Lemma A.11. Any exponential schedule can be used in principle, as long as contraction is enforced (visualized in practice, for example, via a validation loss), without hurting the rate of the final bound.\n\n- **Regarding tightness of dependence on various problem quantities**. Starting with our per-task sample requirement $N \\geq \\Omega(r + \\sigma_w^2 \\max\\{d_y, r\\})$, we do not provide a formal minimax lower bound in this paper, but we have reasons to believe it cannot be significantly improved, e.g. $N = \\mathcal O(1)$, in general. For $d_y = 1$, the lower bound of $N = \\mathcal O(1)$ provided by Tripuraneni et al. critically depends on the covariates being iid $\\mathcal N(0,I)$ across all tasks. Allowing covariances to change across tasks opens the door to arbitrarily ill-conditioned interactions between weights $F^{(t)}$ and covariances $\\Sigma_x^{(t)}$ which cannot happen when $\\Sigma_x^{(t)} = I$. Formalizing the resulting local-minimax lower bound is ongoing work. Whether the mixing coefficients $\\Gamma, \\mu$ in our final bounds is optimal is a challenging question. Indeed, recent work [1] establishes in the general case that a final error bound on least squares regression scaling as $\\frac{\\tau_{\\mathrm{mix}}}{N}$ (for $N$ total data points) is minimax-optimal, where $\\tau_{\\mathrm{mix}}$ is proportional to the dependence on $\\log(N), \\Gamma, \\mu$ in our bounds, at least in the Markov chain setting. However, in parallel, even more recent work [2] demonstrates that under a related notion of mixing, the dependence on the mixing time becomes lower order after sufficiently large burn-in on $N$, after which the iid regression rate $\\mathcal O(1/N)$ is recovered. Regardless, these works are specific to estimators such as the empirical risk minimizer which are distinct from ours, not to mention the additional bilinear structure on our multi-task operators. In short, fully determining whether the dependence on mixing coefficients in our bounds is optimal is non-trivial, though we hypothesize that it is not, thanks to the certain amenable features of our model, such as realizability, subgaussian covariates etc.\t\n\n\n[1] Guy Bresler, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli, Xian Wu. \u201cLeast Squares Regression with Markovian Data: Fundamental Limits and Algorithms\u201d\n\n[2] Ingvar Ziemann and Stephen Tu. \u201cLearning with little mixing\u201d"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278395815,
                "cdate": 1700278395815,
                "tmdate": 1700278395815,
                "mdate": 1700278395815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cur5UnVUAm",
                "forum": "Tr3fZocrI6",
                "replyto": "1xhOaUWcxw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4744/Reviewer_4V27"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4744/Reviewer_4V27"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! I am happy to increase my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4744/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416285270,
                "cdate": 1700416285270,
                "tmdate": 1700416285270,
                "mdate": 1700416285270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]