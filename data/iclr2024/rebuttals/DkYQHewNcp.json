[
    {
        "title": "Unsupervised Detection of Recurrent Patterns in Neural Recordings with Constrained Filters"
    },
    {
        "review": {
            "id": "brOFJzoXnm",
            "forum": "DkYQHewNcp",
            "replyto": "DkYQHewNcp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7388/Reviewer_C8pe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7388/Reviewer_C8pe"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a convolution dictionary learning method designed for neural data. They also propose a way to assess the statistical significance of their pattern detection and demonstrate speedup compared to other methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly written and there is an appreciable progression from experiments on synthetic data to real data. Moreover, the authors present a method to assess the statistical significance of their convolutional pattern detection.\n\nFigures 3.D and 3.E are reassuring in that they seem to show that maximizing the variance in the objective eq.1 (which was motivated intuitively) does indeed correlate with pattern detection. \n\nFinally, beyond the interpretability of their method, the authors exhibit a speedup compared to other methods."
                },
                "weaknesses": {
                    "value": "I am surprised in Figure 8 that there are few standard convolutional dictionary learning methods to compare against, given that convolutional dictionary learning is a field with a rich literature. Could the authors explain how their method differs from other convolutional dictionary learning methods used for neural data, e.g. [1]?\n\n[1]  Dupre La Tour et al. Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals. NeurIPS, 2018."
                },
                "questions": {
                    "value": "Can the authors specify in the main text the data modality used in section 4.3.: are these measurements from cell calcium imaging? \n\nCan the authors explain the main argument for the speed of their method compared to other methods in Figure 8?\n\nWhat is f, on line 113?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697792229766,
            "cdate": 1697792229766,
            "tmdate": 1699636884580,
            "mdate": 1699636884580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zJ1yDhMXSN",
                "forum": "DkYQHewNcp",
                "replyto": "brOFJzoXnm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***Could the authors explain how their method differs from other convolutional dictionary learning methods used for neural data, e.g. [1]?***\n\nThank you for this question. convNMF, SeqNMF, convolutional sparse coding ([1]) aim to minimize the error (typically MSE) between the original signal and its reconstruction (obtained with some sparse filters and their activity, both of which are learned). By contrast, our method learns the filters only (or, in the second formulation of our method, only the means of per-neuron Gaussians, which are used to parametrize the filters).\n\n***Can the authors specify in the main text the data modality used in section 4.3.: are these measurements from cell calcium imaging?***\n\nYes. These are binarized calcium imaging data. We now make this explicit in the footnote on p. 7 (in addition to providing a reference to the original paper and a link from which the data was downloaded).\n\n***Can the authors explain the main argument for the speed of their method compared to other methods in Figure 8?***\n\nWe did not specifically check of PP-Seq or seqNMF for algorithmic inefficiencies. However, it seems that PP-seq is mostly bottlenecked by the expensive sampling operations during optimization. By contrast, our algorithm does not rely on sampling, and the compute-intensive matrix multiplications (which are at the core of our method) are very efficiently parallelized in PyTorch. For seqNMF, a possible bottleneck is the need to learn both the filters and their activations at the same time.\n\n***What is $f$, on line 113?***\n\n$f(\\cdot)$ is a truncated Gaussian function. We now make it explicit on line 111."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054694005,
                "cdate": 1700054694005,
                "tmdate": 1700054694005,
                "mdate": 1700054694005,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Dm8uT4mxP",
                "forum": "DkYQHewNcp",
                "replyto": "zJ1yDhMXSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_C8pe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_C8pe"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging author response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response which addresses my concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576951424,
                "cdate": 1700576951424,
                "tmdate": 1700576951424,
                "mdate": 1700576951424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ydns0yCdKl",
            "forum": "DkYQHewNcp",
            "replyto": "DkYQHewNcp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a way to find repeating spike patterns in multi-channel neural data. The method uses a novel loss function to learn multiple kernels that respond strongly to different recurring patterns. It is tested and found to perform well on several synthetic datasets as well as a dataset of mouse place-cell responses for which the ground truth is known via the mouse\u2019s position on a track. The method is shown to run more quickly than related past methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses a substantial issue in analysis of large neural data. It seems to work well and efficiently. The method and the results are clearly presented. \n\nThe loss function seems elegant and well designed, and its explanation is clear. I didn\u2019t find the loss obvious at first but rather felt that reading this section broadened my mind a little."
                },
                "weaknesses": {
                    "value": "I take it the point of unsupervised detection of neural patterns is to find them even when ground truth isn\u2019t known. However, the method wasn\u2019t applied to such data. Such an application couldn\u2019t be used to test the accuracy of the method, but it would help to illustrate qualitatively what can be expected from it in a realistic scenario, and it might provide an example of downstream use of the results."
                },
                "questions": {
                    "value": "Line 74: Can \u201crepeating\u201d be defined more clearly? What kinds of variations aside from independent jitter are expected biologically, if any? \n\nWhat are the spike rates of the background activity? \n\nFigure 3E: Why does it appear that the network learns nothing for 150 epochs and then suddenly converges? This seems inconsistent with the choice of 100 steps in section 4.4, particularly the claim of faster convergence in lines 195-197. \n\nFigure 7: Could the red traces be overlaid on panels C and D as well? Also it appears that the slopes in these panels are smaller than the speed of the mouse. Is that expected? Why? The detections are clear in any case, which is the main point. \n\nLine 204: Is there really a 2D convolution operation? In the neuron dimension maybe you have a non-padded convolution with kernel size equal to input size, but I don\u2019t think it\u2019s standard to call that 2D. \n\nAppendix B.4 & B.5: The comparison with PP-Seq is hard to interpret because both the true and false positive rates of PP-Seq are higher. Can you change a threshold to match one of these measures and compare the other? \n\nThe dropout probabilities range from 0.2 to 0.4, and I was not sure how to relate that to spike statistics (e.g. Poisson or otherwise). Can this be clarified? \n\nFigure B.9: The sorted spike sequences look tighter here than in Figure 3. Are they? Why? Does it matter? \n\nFigure B.12: This looks qualitatively quite different than Figure 7 and perhaps more should be said about this in the main text. \n\nThe learned kernels seem to include all the neurons, whether or not they participate in the sequence. Is it desirable to ignore non-participating neurons? Figure B.16 seems to suggest one way to do this, i.e. by checking for a Gaussian-like kernel. Are there better ways?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810806682,
            "cdate": 1698810806682,
            "tmdate": 1699636884441,
            "mdate": 1699636884441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MdALR21aVV",
                "forum": "DkYQHewNcp",
                "replyto": "ydns0yCdKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***I take it the point of unsupervised detection of neural patterns is to find them even when ground truth is known. However, the method wasn\u2019t applied to such data. Such an application couldn\u2019t be used to test the accuracy of the method, but it would help to illustrate qualitatively what can be expected from it in a realistic scenario, and it might provide an example of downstream use of the results.***\n\nThat is correct. The point of unsupervised detection is to find patterns for which the ground truth is unknown. However, before we can trust any such method, it must be shown to produce results that make sense when checked against known ground truth. This is why we focused on cases with known ground truth. \n\n***Line 74: Can \u201crepeating\u201d be defined more clearly? What kinds of variations aside from independent jitter are expected biologically, if any?***\n\nA \u201crepeating\u201d (which perhaps should be changed to \u201crecurring\u201d) pattern is one that appears more than a couple of times in the data. The occurrences of a pattern are not expected to be exact copies of each other (that is with same relative interspike intervals and number of participating neurons, in which case their detection would be much easier). Rather, the occurrences of a sequence are expected to differ due to spike timing jitter, dropout (that is when some neurons, otherwise part of a sequence, fail to fire) and noise (the presence of spikes from other neurons that are not part of the sequence).\n\n***What are the spike rates of the background activity?***\n\nWe assumed an average rate of 0.04 Hz based on the dataset from Rubin et al. (2019), which the original authors obtained by the thresholding Ca2+ imaging data of area CA1 of the hippocampus.\n\n***Figure 3E: Why does it appear that the network learns nothing for 150 epochs and then suddenly converges?***\n\nThis is because to be able to start detecting the sequences, the main objective (variance term in the loss function (Fig 3D)) needs to reach a certain level. In other words, as the filter optimization progresses, the filter becomes better \u201ctuned\u201d to the sequence, producing higher peaks when convolved with the data. Initially none of these peaks reach the statistical significance threshold (hence no true positives). At around 150 epochs a few peaks exceed the threshold and finally, at about 200 epochs, all the peaks reach significance.\n\n***This seems inconsistent with the choice of 100 steps in section 4.4, particularly the claim of faster convergence in lines 195-197.***\n\nThis is because of the different datasets: the one used in Fig 3E had 452 neurons and those used in the grid of runs in Section 4.4 had only 76 and 152 neurons. For those datasets, we observed that 100 epochs was sufficient for the $Var(x^{(k)})$ term in the loss function to reach a plateau (as we say on lines 195-197).\n\n***Figure 7: Could the red traces be overlaid on panels C and D as well?***\n\nYes, the traces are added in the updated manuscript.\n\n***Also it appears that the slopes in these panels are smaller than the speed of the mouse. Is that expected? Why? The detections are clear in any case, which is the main point.***\n\nIndeed, the slopes do not perfectly match the slope of the sequence. This an expected artifact of rearranging the neurons based on optimized filters. Specifically, as explain in the Algorithms, in a given optimized filter, we find the positions of each row\u2019s maxima and then sort the rows so that the maxima become temporally ordered. This may cause the recovered sequence to be stretched, vertically flipped or shifted in time (by a maximum of M/2 timesteps) compared to the ground truth. This is to be expected."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054493641,
                "cdate": 1700054493641,
                "tmdate": 1700055395392,
                "mdate": 1700055395392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iYoaqWGpI3",
                "forum": "DkYQHewNcp",
                "replyto": "MdALR21aVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. My question about slopes was unclear. I take it a slope could stretch or contract due to non-uniform density of place fields of neurons in the dataset. However, shouldn't the whole sequence be contained within the movement time? I don't see why an ordered sequence of spikes would take longer than the associated movement. If this is expected, maybe an explanation could be added to the caption or methods."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059268830,
                "cdate": 1700059268830,
                "tmdate": 1700059268830,
                "mdate": 1700059268830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bjU5toMZN7",
                "forum": "DkYQHewNcp",
                "replyto": "rafCpu9ot5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for your responses. \n\nRe. the convolution dimension: In a standard 2D image convolution, a 2D feature map is created by convolving a HxWxC input with a KxKxC kernel (C being the number of channels). Your description sounds analogous except with one fewer dimension. I guess you could implement it as Conv2D with one channel or Conv1D with multiple channels. But you have multiple channels of 1D data rather than one channel of 2D data, so the 1D terminology seems more suitable to me. \n\nRe. comparisons in B.4 and B.5, I don't see another way to achieve a clear comparison. I appreciate that a lot of computation is needed, but you might use a numerical method to quickly find the thresholds. Also, a less comprehensive grid of clearer comparisons might be more informative than the current comprehensive grid of hard-to-interpret comparisons."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060236210,
                "cdate": 1700060236210,
                "tmdate": 1700060236210,
                "mdate": 1700060236210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WLLJjUVFDP",
                "forum": "DkYQHewNcp",
                "replyto": "ydns0yCdKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: slopes"
                    },
                    "comment": {
                        "value": "***Thank you for the clarifications. My question about slopes was unclear. I take it a slope could stretch or contract due to non-uniform density of place fields of neurons in the dataset. However, shouldn't the whole sequence be contained within the movement time? I don't see why an ordered sequence of spikes would take longer than the associated movement. If this is expected, maybe an explanation could be added to the caption or methods.***\n\nThank you for the chance to clarify this further. After rearranging the neurons with an optimized filter a sequence can get stretched up to the temporal width of the filter (which was 200 steps). To illustrate this in Fig S7 (Supplementary Note), we overlay a transparent box spanning 200 time steps over the same raster as in Fig. 7. We agree that the appearance of the recovered sequence (in Fig 7 C and D) suggests that the animal was moving closer to the ends of the track (while it was actually stationary). This is an artifact of sorting the data with imperfectly optimal filters."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700113521681,
                "cdate": 1700113521681,
                "tmdate": 1700113566026,
                "mdate": 1700113566026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zouZdV8uCS",
                "forum": "DkYQHewNcp",
                "replyto": "WLLJjUVFDP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry, I am still missing something and maybe I have misunderstood what is actually being plotted. If I understand correctly the plots are calcium-event rasters, and all you have done to them is re-order the rows. Of course the method finds repeating event patterns that are within the temporal width of the filter. However, the method doesn't shift the events themselves in time, does it? Does the plot not show an orderly progression of events that lasts longer than the movement?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191606098,
                "cdate": 1700191606098,
                "tmdate": 1700191606098,
                "mdate": 1700191606098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hXDFvE1wbQ",
                "forum": "DkYQHewNcp",
                "replyto": "ydns0yCdKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Your understanding is correct: we only rearrange the order of the rows and do nothing else. The individual spikes are never shifted in time. We admit that, perhaps due to some sort of visual illusion, sequences might appear longer than they actually are. To completely clarify the sorting procedure, we attach as Supplementary Material minimal code and necessary data to reproduce the entire process of sorting the rows of the raster to reveal the sequences. It is quite short an we hope this will make this matter clear."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197816534,
                "cdate": 1700197816534,
                "tmdate": 1700197842121,
                "mdate": 1700197842121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MB1qzmFXCA",
            "forum": "DkYQHewNcp",
            "replyto": "DkYQHewNcp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7388/Reviewer_qa1s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7388/Reviewer_qa1s"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an unsupervised method for identifying sequences in neural spiking data.   The method learns a set of K filters that summarize the spiking data subject to what seem like some pretty minimal constraints.  The method is applied to ground truth data and recordings from rodent hippocampus.  The method is faster and perhaps more reliable than other competing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Right now, the priors that working neuroscientists bring to analyzing their data has a huge effect on the results they are able to discover.  It would be really very useful to have an unsupervised method to automatically extract sequential information from spiking neurons.  Not only would it be outstanding for analyzing data from freely moving animals (as in Fig 7)  but also would be really impactful for understanding population burst events, theta sequences, etc etc.\n\nSuch tools will become increasingly important as recording techniques continue to advance."
                },
                "weaknesses": {
                    "value": "I am concerned about priors that may be ``baked in'' to the method (perhaps inadvertently).  At mimimum these priors should be made more explicit.  In particular, I'm concerned that the model seems to find ``straighter'' sequences than are present in the data (Fig 7).  The ground truth experiments all use linear sequences, exacerbating this concern.\n\nIt's not obvious that the method can generalize to sequences (such as PBEs, theta etc) that unfold over more than 1 continuous dimension."
                },
                "questions": {
                    "value": "If ground truth includes sequences that unfold at varying rates can this method identify them?  For instance, suppose that there are place fields along a linear track but they are overrepresented near the ends.   The animal runs at a constant velocity. Now the sequence, rather than appearing as a straight line in, say, Fig 3b, would appear as a hook.  Can this method find those sequences as well?  I think this is a very important question as it seems that these kinds of sequences are very general.  Is there a way to make the filters more or less sensitive to these kinds of sequences?\n\nSuppose we had a set of place cells that tile a 2-D enclosure.  Would this method work?  I'm concerned that the filters will have to cover a 2-D surface with piecewise 1-D filters and this will fail really badly.  \n\nTake the situation in Fig. 7.  Suppose the animal starts out on the linear track at a constant velocity, stops half way through, backtracks for 10 cm, then turns around and continues along its original trajectory to the end.  What filters does this method find?  What should it identify?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816254671,
            "cdate": 1698816254671,
            "tmdate": 1699636884324,
            "mdate": 1699636884324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4C1nzgRlrg",
                "forum": "DkYQHewNcp",
                "replyto": "MB1qzmFXCA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***I am concerned about priors that may be \u201cbaked in\u201d to the method (perhaps inadvertently). At mimimum these priors should be made more explicit. In particular, I'm concerned that the model seems to find straighter' sequences than \u00acare present in the data (Fig 7).***\n\nThe sequences in Fig. 7 (panels C and D) show the spike raster rearranged with one of the two optimized filters. The objective only maximizes the detection strength, but imposes no prior on how the rearranged sequences will look in the raster. The seeming straightness or the sequences is an artifact of this (imperfect) rearrangement, not the effect of priors. To illustrate this, compare Fig. B. 12 and Fig. 7 (in the main text). Even though the datasets were exactly the same, the sequences look somewhat different. Unless the same seed is used to initialize all the trainable parameters of the model, the look and shape of the recovered sequences will differ from run to run. Most importantly, the main purpose of rearranging the rows of the spike raster is to help the researcher to visually confirm the presence of a sequence detected by convolution peaks exceeding the significance threshold.\n\n***The ground truth experiments all use linear sequences, exacerbating this concern.***\n\nWe assume that Reviewer refers to a \"linear sequence\u201d as a pattern where neurons (or ensembles of neurons) A, B, C, and D are activated in a strict predictable order (e. g. A \u2192 B \u2192 C \u2192 D, thus following a linear path). In that case, a \"non-linear sequence\u201d would involve a more complex pattern (e. g. a branching sequence like A \u2192 B \u2192 C or D). In fact, our approach detects such \u201cnon-linear\u201d sequences without a problem if they fit entirely within the width of the filter ($M$). We illustrate this in a Supplementary Note, which we upload as Supplementary Material.\n\n***It's not obvious that the method can generalize to sequences that unfold over more than 1 continuous dimension.***\n\nThank you for the chance to clarify this. The method works for any pattern if it fits entirely within the filter\u2019s temporal width ($M$). To substantiate this, in the Supplementary Note (that we have uploaded as a separate document) we consider an artificial 2D case in which, place cells tile a 2D enclosure, and this enclosure is traversed by an animal along two trajectories resulting in two sequences that unfold over more than 1 continuous dimension. The place cell neighboring in 2D space often end up far apart in the 1D space of the raster. However, even though this \u201clocality\u201d is broken, the method still works fine.\n\n***If ground truth includes sequences that unfold at varying rates can this method identify them?***\n\nYes. In fact, the method is tolerant to moderate time-warping of sequences (we illustrate this in Appendix H).\n\n***\u2026 suppose that there are place fields along a linear track but they are overrepresented near the ends. The animal runs at a constant velocity. Now the sequence, rather than appearing as a straight line in, say, Fig 3b, would appear as a hook. Can this method find those sequences as well? I think this is a very important question as it seems that these kinds of sequences are very general.***\n\nYes. We demonstrate this in Section 4.3.  Fig. 7. shows how our method identifies the sequences of places cells of a mouse running on a linear track. If we look closely at the sorted raster (panels C and D), we can see that the straight line of the sequences bends at the ends slightly, forming what looks like a \u201chook\u201d (e.g. around 800 and 1000 timesteps). We can\u2019t be sure though if this is due to the overrepresentation of track ends by place fields or due to the animal making brief stops. Importantly, if one is looking for repeated patterns, what matters ultimately is the presence of multiple significant peaks, while visual inspection of the rearranged neural data can be used as additional evidence for the presence of patterns.\n\n***Is there a way to make the filters more or less sensitive to these kinds of sequences?***\n\nThank you for this important question. If the researcher has strong assumptions (or knowledge) about what kinds of sequences they are looking for, it is possible in principle to incorporate these assumptions (or inductive biases) through the use of additional learnable parameters and regularization terms in the loss function.\n\n***Suppose we had a set of place cells that tile a 2-D enclosure. Would this method work? I'm concerned that the filters will have to cover a 2-D surface with piecewise 1-D filters and this will fail really badly***\n\nOn the contrary, our model uses 2D, not 1D, filters (as we explain on L72). This allows the detection of any (sufficiently strong) pattern as long as it fits entirely within the temporal width of the filter ($M$). We illustrate a 2D case in the Supplementary Note uploaded with the revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054294351,
                "cdate": 1700054294351,
                "tmdate": 1700054294351,
                "mdate": 1700054294351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sXfqNufqh4",
                "forum": "DkYQHewNcp",
                "replyto": "Os9ZBrN74y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_qa1s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Reviewer_qa1s"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their careful reply.\n\nI remain concerned that there is a scale built in to the method.  Choosing the width M may impose a choice of the experimenter on the results that are not present in the data.\n\nHippocampal neurons (and neurons in other parts of the brain) appear to show reliable sequential firing over many different time scales.  For instance, the ``sequence'' of time cells triggered by an event slows continuously and may extend out to minutes (Shikano, et al., 2021, Current Biol).  The time between the peak of time cell n and the peak of time cell n+1 changes systematically with the location in the sequence.  (There is reason to suspect that place cells show a similar phenomenon.) If Cao et al., (2022, eLife) are to be believed, the time between cells in the sequence goes up linearly with n.   If the sensitivity of the method depends on the choice M, then any specific window size means that the method would be blind to parts of the sequence slower than that (and perhaps has different resolution for parts of the sequence that are much faster).  \n\nI'm supportive of the general approach.  I encourage more thought on sensitivity to M, especially in the case where the data has sequences that unfold over many different characteristic scales."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668368764,
                "cdate": 1700668368764,
                "tmdate": 1700668368764,
                "mdate": 1700668368764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tsfAGIUwww",
                "forum": "DkYQHewNcp",
                "replyto": "MB1qzmFXCA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***I remain concerned that there is a scale built in to the method. Choosing the width M may impose a choice of the experimenter on the results that are not present in the data.***\n\nThank you very much for the opportunity to clarify this point. We would like to stress that if no pattern is present in the data, then our method will not detect anything (spurious regularities in the background neural activity should not produce peaks exceeding statistical significance). Regarding the choice of $M$, indeed, as we admit in the Limitations section, one must make some sensible assumption about the length of patterns that they are looking for in the dataset. This not problematic at all, because the speed of our method allows one to test multiple different values of $M$ in a reasonable amount of time.\n\n***Hippocampal neurons (and neurons in other parts of the brain) appear to show reliable sequential firing over many different time scales <...> if the sensitivity of the method depends on the choice M, then any specific window size means that the method would be blind to parts of the sequence slower than that (and perhaps has different resolution for parts of the sequence that are much faster).***\n\nIt is correct that if a pattern extends far beyond the filter\u2019s temporal width, it will not be detected. However, it is not difficult for one to look for patterns at more than one time scale. For example, if a researcher hypothesizes that a certain behavioral task (or experimental treatment) causes an elongation of pre-existing patterns of neural activity, they can test this hypothesis by running the method first with a narrow window (e.g. $M$) on a dataset recorded before the animal performs the task and once again with a wider window (e.g. $2M$) on a dataset recorded after the task."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717386117,
                "cdate": 1700717386117,
                "tmdate": 1700740211324,
                "mdate": 1700740211324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mh4LgotBME",
                "forum": "DkYQHewNcp",
                "replyto": "MB1qzmFXCA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "To further illustrate our point we have added a case study to the most recent revision of the Supplementary Note (separate PDF file). Specifically, we consider a synthetic dataset ($N = 452$) with one sequence of 160 neurons that unfolds at different speeds (the second is 3 times slower than the first). We begin by setting $M = 200$ and optimize the first filter. As expected, the optimized filter only \u201cresponds\u201d to the short version of the sequence (Fig. S8), because the longer sequence is not fully contained within the filter\u2019s temporal length. Next, we set $M = 600$ and optimize the second filter. After optimization, this wider filter produces higher peaks in response to the longer sequence (Fig. S9). In general, the patterns do not need to be straight (or linear) -- the method works with any pattern configuration as long as it fits within $M$."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739734551,
                "cdate": 1700739734551,
                "tmdate": 1700740089674,
                "mdate": 1700740089674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]