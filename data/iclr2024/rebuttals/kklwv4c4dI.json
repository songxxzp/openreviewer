[
    {
        "title": "Local Composite Saddle Point Optimization"
    },
    {
        "review": {
            "id": "SFM4lBhT4b",
            "forum": "kklwv4c4dI",
            "replyto": "kklwv4c4dI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes FeDualEx a federated primal-dual algorithm for solving distributed composite saddle point problems. The authors consider a homogeneous setting and provide convergence guarantees achieved by FeDualEx when the objective function is convex-concave. The authors also evaluate the proposed algorithm experimentally on synthetic and real datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall. the paper is well written with the ideas clearly explained. The proposed algorithm is well-motivated and backed by strong theoretical guarantees. The experiments show the effectiveness of the proposed approach."
                },
                "weaknesses": {
                    "value": "- The authors have missed an important reference [R1] which considers a nonconvex composite optimization and develops Douglas-Rachford Splitting Algorithms for solving the problem. \n\n- The authors should also discuss [R2] and [R3] which consider a non-convex composite problem but in a decentralized setting and without local updates. Also, in contrast to the duality-based approach taken by the authors, the works [R2] and [R3] propose primal algorithms that directly update the parameters using proximal stochastic gradient descent. The dual approach proposed by the authors is justified by the \"curse of primal averaging\". A question I have is why the algorithms [R2] and [R3] seem to work even though they are primal algorithms.\n\n- Why are the guarantees presented in the paper independent of the number of clients? The effect of the number of clients should be discussed after the main results. Importantly, does the proposed algorithm achieve linear speed-up with the number of clients in the network?\n\n- In the initial part of the paper the authors refer to the distance-generating function to be strictly convex but later it is assumed to be strongly convex. It is advisable to call it strongly convex from the beginning. \n\n- Define $h_1$, $h_2$ in Definition 3. \n\n- After Definition 3, the authors mention that the previous approaches that add the composite term to the Bregman\ndivergence may not work for dual extrapolation as certain parts of the analysis break down. Can the authors be more specific about what they mean here?\n\n[R1] Dinh et al., FedDR \u2013 Randomized Douglas-Rachford Splitting Algorithms for Nonconvex Federated Composite Optimization, 2021(https://arxiv.org/pdf/2103.03452.pdf)\n\n[R2] Yan et al., Compressed Decentralized Proximal Stochastic Gradient Method for Nonconvex Composite Problems with Heterogeneous Data, 2023 (https://arxiv.org/pdf/2302.14252.pdf)\n\n[R3] Xiao et al., A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization, 2023 (https://arxiv.org/pdf/2302.09766.pdf)"
                },
                "questions": {
                    "value": "See the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2585/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2585/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818436021,
            "cdate": 1698818436021,
            "tmdate": 1699636195724,
            "mdate": 1699636195724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "awcWf1rNNk",
                "forum": "kklwv4c4dI",
                "replyto": "SFM4lBhT4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[1/2] Response to Reviewer geTM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging our contributions and providing insightful comments. We are more than happy to address these comments and sincerely hope that the reviewer can consider raising the score if the following response helps resolve some concerns. \n\n> The authors have missed an important reference [R1] \n\nWe thank the reviewer for bringing this work to our attention and have included it in the updated Appendix B.3. We would note that [R1] solves minimization problems whereas we work on min-max saddle point problems.\n\n> The authors should also discuss [R2] and [R3] which consider a non-convex composite problem but in a decentralized setting and without local updates. \n\nWe again thank the reviewer for bringing these results to our attention and have included them in the updated Appendix B.4. We would again note that [R2] and [R3] deal with minimization problems whereas we work on min-max saddle point problems. And, as the reviewer pointed out, [R2] and [R3] are for decentralized optimization without local updates, whereas we focus on the server-client type of distributed optimization with local updates.\n\n> the works [R2] and [R3] propose primal algorithms that directly update the parameters using proximal stochastic gradient descent. ... A question I have is why the algorithms [R2] and [R3] seem to work even though they are primal algorithms.\n\n[R2] and [R3] measure the convergence only in terms of function value, not the structure of the solution. E.g. in Section 5 of [R2] and [R3], $l\\_1$ regularization is considered for inducing sparsity, but no explicit measure of sparsity is provided. If observing only the function value, the solution can be dense but still have a small $\\ell\\_1$ norm, because it's averaged by the number of machines. \n\n\n* For example, assume the solutions on each machine is $x\\_1 = [1, 0, ..., 0], x\\_2 = [0, 1, ..., 0], ..., x\\_{100} = [0, 0, ..., 1]$ for 100 machines, the final averaged solution is $\\bar{x} = [0.01, 0.01, ..., 0.01]$. In terms of the function value, the contribution from the regularization, $\\Vert\\bar{x}\\Vert\\_1 = 1$, is the same as the solution on each machine, but $\\bar{x}$ is apparently not a sparse solution.\n\nIn terms of function value convergence, distributed primal-averaging algorithms may indeed converge. This is also observed for the FedMiP algorithm we proposed as a baseline. It shares a similar convergence rate as FeDualEx and is observed to converge in terms of the duality gap as shown in Figure 4. However, as Figure 4 also shows, the sparsity / low rankness of the FedMiP solution differs greatly from that of FeDualEx."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085829608,
                "cdate": 1700085829608,
                "tmdate": 1700085829608,
                "mdate": 1700085829608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GWAl3bcZZV",
                "forum": "kklwv4c4dI",
                "replyto": "SFM4lBhT4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[2/2] Response to Reviewer geTM"
                    },
                    "comment": {
                        "value": "> Why are the guarantees presented in the paper independent of the number of clients? The effect of the number of clients should be discussed after the main results. \n\nAs shown in Theorem 1, the final rate is also dependent on $M$, i.e., the number of clients. In particular, the noise term $\\frac{5^\\frac{1}{2}\\sigma B^\\frac{1}{2}}{M^\\frac{1}{2}R^\\frac{1}{2}K^\\frac{1}{2}}$ decays with $M$. Since we mainly focus on communication complexity in the context of distributed learning with local updates, the previous rate in Table 1 took the dominating term with respect to the communication rounds $R$, assuming $M$ is large enough, as previously noted in the caption. We have updated Table 1, as well as the discussion after Theorem 1, to clarify this dependence on $M$. \n\n> Importantly, does the proposed algorithm achieve linear speed-up with the number of clients in the network?\n\nIn the case of distributed composite convex optimization, we achieve linear speed-up with respect to $M$, because when we assume $R$ to be large, the $O(\\frac{1}{\\sqrt{MKR}})$ term takes dominance over the $O(\\frac{1}{R^\\frac{2}{3}})$ term, leading to a linear speed up.\n\nAs for distributed composite saddle point optimization, linear speed-up is so far not guaranteed, as there is instead a $O(\\frac{1}{R^\\frac{1}{2}})$ dependence, which makes $O(\\frac{1}{\\sqrt{MKR}})$ not able to dominate.\n\nIt is important to note, however, we provide the first rate for composite saddle point optimization in the distributed setting. Thus, it remains to be seen whether in this setting linear speedup w.r.t. the number of clients is achievable, and we would consider this study for future work.\n\n> In the initial part of the paper the authors refer to the distance-generating function to be strictly convex but later it is assumed to be strongly convex. It is advisable to call it strongly convex from the beginning.\n\nWhile our intent was to provide a general definition of Bregman divergence (which only requires strict convexity), we agree it would be clearer in this context to assume strong convexity from the beginning, and so we have updated this accordingly.\n\n> Define $h\\_1$, $h\\_2$ in Definition 3.\n\n$h\\_1$ and $h\\_2$ are just any distance-generating functions chosen respectively for $x$ and $y$ in the saddle point function. We have included this in the updated Definition 3.\n\n> After Definition 3, the authors mention that the previous approaches that add the composite term to the Bregman divergence may not work for dual extrapolation as certain parts of the analysis break down. Can the authors be more specific about what they mean here?\n\nThough the exact reason is somewhat technically involved, we provide here a more detailed summary. In short, by ``certain parts of the analysis break down'',  we mean that by using previous definitions other than our Definition 4 for composite dual extrapolation, there will be extra composite terms in the analysis that neither cancel out nor form telescoping terms, but only accumulate and hinder the $O(1/T)$ result (even for the simplest sequential deterministic case as in Section 5 Theorem 4) expected for composite dual extrapolation. If the reviewer would like a more precise mathematical explanation, we would be happy to provide the derivations in full detail."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086042444,
                "cdate": 1700086042444,
                "tmdate": 1700086042444,
                "mdate": 1700086042444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zkU4GavOCD",
                "forum": "kklwv4c4dI",
                "replyto": "GWAl3bcZZV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I thank the authors for the responses. Overall, I am satisfied with the responses."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631313861,
                "cdate": 1700631313861,
                "tmdate": 1700631313861,
                "mdate": 1700631313861,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X5gtResnBe",
            "forum": "kklwv4c4dI",
            "replyto": "kklwv4c4dI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2585/Reviewer_rzX1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2585/Reviewer_rzX1"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study composite saddle-point problems in a Federated learning setup. They propose distributed gradient methods with local updates, which they call Federated Dual Extrapolation. They provide convergence analysis and communication complexity in the homogeneous case."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors propose a new method, for which they provide convergence analysis. This method has their own interest."
                },
                "weaknesses": {
                    "value": "Table 1 presents the previous and current results strangely:\n1) First of all, to compare the obtained complexity for the proposed method with the previous result in a strongly-convex concave case, it should be used standard regularization trick.\n2) From my point of you, when complexity contains several terms, each of them should be added. \n\nAbout Table 2, The authors claim that \"The sequential version of FeDualEx leads to the stochastic dual extrapolation for CO and yields, to our knowledge, the first convergence rate for the stochastic optimization of composite SPP in non-Euclidean settings .\" It is not true, there is a wide field related to operator splitting in deterministic and stochastic cases. Look at this paper please https://epubs.siam.org/doi/epdf/10.1137/20M1381678. \n\nAlso, compared to the previous works, the authors use bounded stochastic gradient assumption and homogeneity of data. In many federated learning papers, those assumptions are avoided. Despite that the authors write \"Assumption e is a standard assumption\", it would be better to provide analysis without it to have more generality. \n\nIn Theorem 1, and Theorem 2, the final result contains mistakes in complexity, because some of them were done in the proof. \nThe first mistake is made in theorem 3 and repeats in the main theorem. Please look at the last inequality on page 40:\nTo make $3\\eta^2\\beta^2 -1 \\leq 0$, the stepsize should be chosen in the following way: $\\eta \\leq \\frac{1}{\\sqrt{3}\\beta}$. This will change the complexity of the methods. The same was done in the proof of Theorems 1, and 2. Please see Lemma 3, 17.\n\nThe second mistake is made in the proof of Lemma 13, in the last two inequalities, where should be $\\dots\\sqrt{2V^l_z(\\cdot)} \\leq \\dots \\sqrt{B}$. This thing also will change the final complexity.\n\nThe appendix is hard to read in terms of the order of Lemmas. I think it would be better if the numeration of Lemmas had a strict order (for example, after Lemma 5 lemma 6 follows.)\n\nOther things dealing with weaknesses, please, see in questions."
                },
                "questions": {
                    "value": "1. In section 4, it is unclear how you define $\\ell_{r,k}$. Could you add an exact expression for it from the appendix to the main part? \n\nSmall typos:\n1. on the bottom of page 5 in the second argmin the bracket is missed. \n2. In definition 4, $t\\eta$ is missed in the formula for subgradient."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827526904,
            "cdate": 1698827526904,
            "tmdate": 1699636195649,
            "mdate": 1699636195649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HsKBuvCnaA",
                "forum": "kklwv4c4dI",
                "replyto": "X5gtResnBe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[1/2] Response to Reviewer rzX1 on main results"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. In the meantime,  we sincerely hope that the reviewer can consider raising the score if the following response helps highlight our contributions and clarify some of the technical details. \n\n---\n\nWe begin by addressing the reviewer's concerns regarding our main results and assumptions.\n\n> Table 1 presents the previous and current results strangely ... 1. to compare the obtained complexity for the proposed method with the previous result in a strongly-convex concave case ... 2. when complexity contains several terms, each of them should be added ...\n\nWe would kindly bring to the reviewer's attention that we are presenting a result of a new problem, i.e., distributed **composite** saddle point optimization, instead of comparing with or outperforming existing convergence rates. Table 1 is to show that previous methods either focus on composite convex optimization (instead of saddle point optimization) or non-composite problems in the Euclidean setting, as we highlighted in the second column. As we have noted in the caption, they are listed only for completeness, not for comparison, because they are simply not applicable to composite SPPs (i.e. SPPs with non-smooth regularization).\n\nAs for the complete convergence rates, they were omitted simply due to the space limit of the page. We have updated Table 1 to include the full rates.\n\nWe sincerely hope that the flaws in the presentation do not overshadow the core contribution of this paper, that is, **we present the first convergence rate for composite SPP with non-smooth regularization under the distributed paradigm.**\n\n> About Table 2 ... It is not true, there is a wide field related to operator splitting in deterministic and stochastic cases. Look at this paper please https://epubs.siam.org/doi/epdf/10.1137/20M1381678.\n\nWe would again kindly bring to the reviewer's attention that one of the core components of this paper is **composite optimization**, that is, we study SPPs with composite non-smooth regularization. Based on our understanding of the paper in the link provided, their result does not handle the composite setting. As a result, we would greatly appreciate it if the reviewer could specify which aspects of the paper they believe make our claim untrue.\n\nAs for their (smooth non-composite) setting, we have cited earlier work in the bottom right block of Table 2, i.e., stochastic Mirror Prox by Juditsky et al., 2011, though we have also included a citation for the work mentioned by the reviewer in Appendix B.2. \n\nWe further emphasize that it is nontrivial to take composite terms into consideration for saddle point problems. Even in the deterministic case, for example, the work composite Mirror Prox (CoMP) by He et al. (2015) shows the degree of technical involvement for extending the original Mirror Prox of Nemirovski (2004) to the composite setting.\n\n\n\n> In many federated learning papers, those assumptions are avoided. Despite that the authors write \"Assumption e is a standard assumption\", it would be better to provide analysis without it to have more generality.\n\nWe wish to note that we are not saying the bounded gradient assumption is a standard assumption in general federated learning, but a standard assumption in distributed composite optimization, even in the recent Federated Composite Optimization by Yuan et al., 2021. (e.g. in their Theorem 4.2 and Assumption 3 in their Appendix D). While we agree with the reviewer that the result would be more general without these assumptions, we wish to retain our focus on dealing with composite terms and saddle point optimization, and hope the reviewer can also acknowledge the technical difficulties we discussed in the ``Remark On Heterogeneity'' at the end of Section 4.2 on page 7.  \n\n---\n\nReferences:\n\nHe et al. \"Mirror prox algorithm for multi-term composite minimization and semi-separable problems.\" Computational Optimization and Applications, 2015.\n\nNemirovski, Arkadi. \"Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems.\" SIAM Journal on Optimization, 2004.\n\nYuan et al. \"Federated composite optimization.\" International Conference on Machine Learning, 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085176287,
                "cdate": 1700085176287,
                "tmdate": 1700085176287,
                "mdate": 1700085176287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hN52aOTTyM",
                "forum": "kklwv4c4dI",
                "replyto": "X5gtResnBe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[2/2] Response to Reviewer rzX1 on technical details"
                    },
                    "comment": {
                        "value": "We would sincerely thank the reviewer for their meticulous review and would like to clarify the typos and technical details.\n\n> In Theorem 1, and Theorem 2, the final result contains mistakes in complexity ... The second mistake is made in the proof of Lemma 13 ...\n\n\nThese are indeed typos, but only the constants were affected, and **the rate is in fact better after fixing these typos**. Because we would note that the first one only affects the power on $\\beta$. After fixing the typo, Theorem 3 becomes\n$$\\mathbb{E}[Gap] \\leq \\frac{{\\color{red}\\sqrt{3}\\beta} B}{T} + \\frac{3^\\frac{1}{2}}{T^\\frac{1}{2}}.$$\nOnly the part in red is changed from $3 \\beta^2$ to $\\sqrt{3} \\beta$. Similarly, for Theorem 1 and 2, choosing $\\eta^c \\leq \\frac{1}{\\sqrt{5}\\beta}$ instead of $\\frac{1}{5\\beta^2}$ only changes the first term in Theorem 1 and 2 from $\\frac{5\\beta^2 B}{RK}$ to $\\frac{\\sqrt{5}\\beta B}{RK}$.\n\nThe second one only affects the power on $B$ in the last term of Theorem 1. Letting $\\eta^c \\leq \\frac{B^\\frac{1}{4}}{2^\\frac{3}{4}\\beta^\\frac{1}{2}G^\\frac{1}{2}K R^\\frac{1}{2}}$ instead makes the last term of Theorem 1 become $\\frac{2^\\frac{3}{4}\\beta^\\frac{1}{2}G^\\frac{1}{2}{\\color{red}{B}^\\frac{3}{4}}}{R^\\frac{1}{2}}$. Only the part in red is changed from $B$ to ${B}^\\frac{3}{4}$.\n\n**We have fixed these typos throughout the paper in the updated pdf.**\n\n> The appendix is hard to read in terms of the order of Lemmas.\n\nWe apologize for the confusion caused by the ordering of the lemmas. The intention was to number the main lemmas used for Theorem 1 as Lemma 1,2,3 (i.e., the Lemma 11 and 12 on pages 30 and 31 should in fact be Lemma 1 and 2, and this is now fixed). We will certainly re-work the numbering but leave it unchanged for the moment for the convenience of the on-going discussion period.\n\n> Questions: In section 4, it is unclear how you define $\\ell\\_{r,k}$\n\n$\\ell\\_{r,k}$ is formally given on the 6th line of the ``Projection Reformulation'' paragraph under Section 4.2. It simply breaks $t$ into the number of communication rounds $r$ and the number of local updates $k$. Since $\\ell\\_t(z) = l(z) + t \\eta\\psi(z)$, $\\ell\\_{r,k}(z) = l(z) + (rK\\eta^s + k)\\eta^c\\psi(z)$ for $t = rK + k$, in which $r$ is the round of communications till now, $K$ is the number of local updates between each round, and $k$ is the number of local updates since last communication."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085286103,
                "cdate": 1700085286103,
                "tmdate": 1700085286103,
                "mdate": 1700085286103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FN9YZRLen0",
                "forum": "kklwv4c4dI",
                "replyto": "X5gtResnBe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing further concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer rzX1,\n\nWe appreciate your helpful comments, and we believe we have addressed your concerns in our rebuttal. As the deadline for the discussion period is approaching, please let us know if there are any further concerns to address, and thank you once again for your time and effort in this matter.\n\nSincerely,\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594258787,
                "cdate": 1700594258787,
                "tmdate": 1700594258787,
                "mdate": 1700594258787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JTuOBkaRpw",
                "forum": "kklwv4c4dI",
                "replyto": "hN52aOTTyM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2585/Reviewer_rzX1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2585/Reviewer_rzX1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response to addressing issues! Now complexity looks like as expected."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680992332,
                "cdate": 1700680992332,
                "tmdate": 1700680992332,
                "mdate": 1700680992332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HkI88EUg2U",
            "forum": "kklwv4c4dI",
            "replyto": "kklwv4c4dI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2585/Reviewer_jzbD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2585/Reviewer_jzbD"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose an Algorithm FeDualEx for solving composite saddle point problems under distributed settings. The proposed algorithm is inspired from the dual extrapolation algorithm while using a proximal operator which they define using the generalized Bregman divergence defined for saddle functions. They analyze this algorithm under homogeneous settings and derive its convergence rate for the duality gap. They also study the special cases when the number of clients equals 1, where the convergence rate of FeDualEx matches the existing rates known in the literature. The study also demonstrates that solving using the dual extrapolation has advantages of learning better sparse solutions than solving the primal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper studies federated learning of composite saddle point problems, for which there does not seem to be much existing work. The proposed convergence rates. \n\n(Novelty) The paper proposes a new bregman divergence for saddle functions and its associated proximal operator, which are used in the dual extrapolation steps. \n\n(Clarity) The main results are presented well and contrasted to the related ones. The experimental results illustrate the benefit of solving using the federated dual extrapolation over methods such as Federated Mirror Prox. The comparison to the sequential algorithms also help to position the contributions in relation to the existing work."
                },
                "weaknesses": {
                    "value": "The algorithm is similar that of Federated Dual Averaging (Yuan et.al.) while incorporating the dual extrapolation strategy over the newly defined Bregmen divergence and the proximal operators. The challenges associated with adapting the above strategy over FeDualAvg doesn't seem to be conveyed well in the paper. \n\nFrom the motivations perspective, some examples of practical setups which required distributed learning of saddle point formulations would be useful in appreciating the contributions better."
                },
                "questions": {
                    "value": "Questions / Comments\nIs it possible to have a similar algorithm only using the generalized bregman divergence on x ? \nSome discussion on the relation between the variables z=(x,y) and \u03c2 would help since the former already includes a primal and dual pair. \n\nTo improve the clarity, one may include convexity assumptions of the functions involved while the main problem is defined in (1)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2585/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2585/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2585/Reviewer_jzbD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2585/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700986110530,
            "cdate": 1700986110530,
            "tmdate": 1700986110530,
            "mdate": 1700986110530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]