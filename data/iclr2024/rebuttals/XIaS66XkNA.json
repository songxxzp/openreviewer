[
    {
        "title": "Idempotent Generative Network"
    },
    {
        "review": {
            "id": "ZYCZhq4Aer",
            "forum": "XIaS66XkNA",
            "replyto": "XIaS66XkNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel approach to generative modeling based on the idea of idempotence. An idempotent function $f: X \\rightarrow X$, is one for which $f(f(x)) = f(x)$ for any $x \\in X$. The aim of the model proposed in the paper, Idempotent Generative Network (IGN), is to achieve idempotence so that for any $x$ drawn from the source distribution, $f(f(x)) = f(x)$. The paper provides several justifications for why this property is desirable in the context of generative modeling that essentially amount to IGN having many of the nice properties held individually by either GANs, diffusion models, etc. The paper has an interesting discussion of the loss function needed to train such a model and provides one theoretical result justifying the optimization approach. Several experiments are run, including use of the idempotent property of IGN to project noisy or corrupted examples back to the target distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Underlying idea:** The use of idempotence in deep learning is appealing given its deep connection to mathematics and theoretical computer science. Because deep learning is essentially a science of function composition, connecting it to category theory and related domains focused on composition in pure mathematics is a rich area for exploration. This is a great example of what first steps in that direction look like. Since idempotents have been studied in great depth, there is likely many additional constructions that could be explored using this work as a starting point.\n- **Simplicity of construction and clarity of writing:** IGN is described very clearly so that readers from a range of backgrounds can understand the constructions. The different parts of the model (particularly the loss function terms) are well-justified both mathematically and informally in the text. As might be expected when researchers utilize a truly foundational idea, constructions are surprisingly simple (as was the case in this paper). \n- **Quality of the outputs:** Despite the fact that this is a novel generative framework, this reviewer felt that the outputs looked quite good. It was especially exciting to see that that $f(f(f(x)))$ looked mostly the same as $f(x)$. As described below, the reviewer would be interested to see $f^k(x)$ as $x \\rightarrow \\infty$. \n- **Opening paragraph:** Though it is perhaps a small thing, this reviewer really appreciated the opening quote. It captures the underlying idea of the paper and made me laugh."
                },
                "weaknesses": {
                    "value": "- **Motivation:** While the idea is interesting and utilizes foundational structures from mathematics, the motivation for idempotence in generative models is still a little weak. Several different consequences are described in the introduction (IGN provides outputs in a single step, yet allows additional refinements, etc.) Each of these properties is attractive, but already possessed by an existing mature method (as is pointed out in the paper). It was unclear to this reviewer whether starting from scratch with a completely new paradigm is the right way to obtain the desirable properties of GANs, diffusion models, etc. On the other hand, idempotence is a deep idea whose consequences have been explored in a wide range of settings and directions. It feels likely to this reviewer that there are good reasons for wanting idempotence in a model that go beyond properties we already have in alternative methods. However, these may yet need to be uncovered.\n- **Comparison to other methods:** While the reviewer agrees that it is not fair to hold this novel approach to the standards of more mature methods, it would still be useful to be able to compare the performance of IGN with known Approach. As it is, this reviewer was unable to tell how far IGN is from the performance of a comparably sized GAN, VAE, or diffusion model. \n- **Quantitative results:** It appears that the only way the reader can evaluate the experiments is through a handful of example generations. While these are helpful for getting a qualitative sense of the model (and look good, as described in the Strengths section), it would have made the analysis stronger if quantitative results were also presented. It would be interesting, for instance, to understand network performance for different types of initial $x$, especially since the domain of the network includes both $\\mathcal{P}_z$ and $\\mathcal{P}_x$. It would have also been interesting to quantitatively measure differences in $f^{(k)}(x)$ as $k \\rightarrow \\infty$.\n\n### Nitpicks\n\n- Idempotence is spelled wrong in the sentence below equation (7).\n- The tightness objective is a little subtle. It might make sense to utilize some of the notation already introduced (e.g., $\\mathcal{S}$) to help communicate why this loss term is necessary. \n- It may be a matter of personal taste, but this reviewer felt that while the PyTorch code was appreciated, it might be better to include in the Appendix and instead include additional analysis of the experiments."
                },
                "questions": {
                    "value": "- One of the claimed strengths of IGN is that through the idempotence property, it can be used to project noisy or corrupted images back to the data manifold. Based on the reviewer\u2019s understanding of the capability, there are already a range of methods for doing this (e.g., via image denoising with a diffusion model). Is there some additional nuance that the reviewer is missing? It is certainly the case that existing methods will not map an image plus noise $x + \\epsilon$ by to $x$ exactly, but it was our sense from reading the paper that neither would IGN.\n- It would be interesting to see the behavior of $f^{k}(x)$ as $k \\rightarrow \\infty$, does this converge or diverge? If it does converge, what does it converge to?\n- This reviewer did not understand the modified GAN network architecture described in Section 2.3.\n- As the authors note, the development of high-performing generative models has come about through numerous iterative improvements. Would it be possible to build an IGN model using the framework of a known generative approach?\n- What are the current drawbacks to scaling this method up?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3420/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698385504196,
            "cdate": 1698385504196,
            "tmdate": 1700597898401,
            "mdate": 1700597898401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "agyURlZg6a",
                "forum": "XIaS66XkNA",
                "replyto": "ZYCZhq4Aer",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the thoughtful consideration of the paper and constructive feedback."
                    },
                    "comment": {
                        "value": "**Motivation: \u201cSeveral different consequences are described in the introduction (IGN provides outputs in a single step, yet allows additional refinements, etc.) Each of these properties is attractive, but already possessed by an existing mature method (as is pointed out in the paper). \u201c**  \nIGN has some unique properties that do not exist in other models. In addition, IGN has some properties that exist in other models but do not co-exist together. For instance, let us consider the single step + refinement property the reviewer mentions. There are Diffusion based methods with fewer steps, such as consistency models. However IGN always does one step full generation after which it is possible to apply it again for refinement. \nAnother example is a consistent latent space. Surely, GANs have that, but IGN allows smooth interpolation across all iterations ($f(z), f(f(z)), f(f(f(z)))$...). (See video in supplementary).\nAlso, Unlike GANs and Diffusion models, editing of real images in IGN does not require any type of inversion. Simply inputting a real image in a feed-forward fashion into IGN allows manipulation on it (see new figures 9,10 in revised manuscript).\nLastly there is the projection property addressed below in response to your specific question.\n\n**\u201cIGN can be used to project corrupted images \u2026 already a range of methods for doing this (e.g., via image denoising with a diffusion model).\u201d**  \nWhile many methods handle certain degradations or modifications of images, they will typically be trained for it specifically (e.g., Diffusion-Denoising, Cold Diffusion, image enhancement methods, image-to-image methods), or they will require some manipulation at test time (adding noise, guidance in diffusion and some tricks in GANs). IGN is not trained for specifically on modified images. IGN only sees pure noise and real images during training. No different type of inference is needed. For example, no noise is added to bring the input into distribution, as would be in, for instance, SDEdit. Note that it is not only for corrupted images but also image-to-image translation such as sketches. It can be thought of as a generic  ``make-it-real\" button.\n\n**\u201c $\\mathbf{f^k(z) \\quad k\\rightarrow\\infty}$. Does this converge or diverge?\u201c:**   \nWe added fig.16 in the appendix of the revised manuscript which demonstrates applications of MNIST trained IGN up to $300000$ iterations. For the first few tens of iterations, the system seems to be stable but around 100 it diverges from the result of the first application. We hypothesize that the size of the model which influences the ability to get low reconstruction loss determines the stability.\n\n**\u201cThis reviewer did not understand the modified GAN network architecture described in Section 2.3.\u201d**  \nIn the revised manuscript, we added Fig.15 to illustrate the modification of DCGAN architecture to be used for IGN on CelebA. The exact architecture is in Table.1 in the appendix. \nDCGAN has a generator $G$ and a discriminator $D$. In training $D(G(z))$ is applied. $D$ produces a $1\\times1\\times1$ result per image. we modify $D$ to end with a $512\\times1\\times1$ tensor instead, denoted as $\\tilde{D}$. Our model in terms of DCGAN is $G(\\tilde{D}(z))$. \n\n**Tightness loss clarity:**  \nIndeed it is subtle, and challenging to convey. We added the notion of $\\mathcal{S}$ to the text around fig.2 and its caption as you suggested. We updated the text about $\\mathcal{L}\\_{tight}$ and also added Fig.17 that offers further clarification on the two paths to enforce idempotence. We welcome any suggestions to enhance the explanation in the paper.\nWe have provided some informal perspectives to help understand $\\mathcal{L}\\_{tight}$ in responses to other reviewers. please check them (not pasting here due to char limit, sorry.). \n\n**\u201cWould it be possible to build an IGN model using the framework of a known generative approach?\u201d, \u201cWhat are the current drawbacks to scaling this method up?\n\u201c**  \nScaling up is a current effort. Indeed, the ways explored are based also on existing methods. Among others, working in latent space as in Stable Diffusion and using transformer architectures are being tried. One challenge we have seen is the need of a bottleneck which makes the popular Unet challenging to work with due to its skip connections.\n\n**Quantitative evaluation and comparison to other methods:**  \nWe report FID=39 for IGN on CelebA. Naturally, this is not competitive with modern models, but is comparable to early models such as DCGAN (FID=34), while also capable of a wide range of conditional tasks. We updated the draft paper to include this info.\n\n**Idempotence spelled wrong:** Fixed, thanks!\n\n**Code better in appendix:**  \nThe provided PyTorch code is a substitute for an algorithm pseudocode. We felt that it is the cleanest way of presenting the algorithm. That said, we can consider moving it to the appendix and replace it with the regular pseudocode notation. We will also release full code."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870506804,
                "cdate": 1699870506804,
                "tmdate": 1699870506804,
                "mdate": 1699870506804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9doAh6Cz3u",
                "forum": "XIaS66XkNA",
                "replyto": "ZYCZhq4Aer",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup"
                    },
                    "comment": {
                        "value": "Dear reviewer, towards the end of the discussion phase, we trust that our response has successfully addressed your inquiries. We look forward to receiving your feedback regarding whether our reply sufficiently resolves any concerns you may have, or if further clarification is needed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512552714,
                "cdate": 1700512552714,
                "tmdate": 1700512552714,
                "mdate": 1700512552714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tzT8smdzrO",
                "forum": "XIaS66XkNA",
                "replyto": "9doAh6Cz3u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for clarification and updates"
                    },
                    "comment": {
                        "value": "The reviewer would like to thank the authors for their clarifications and updates. \n- While the reviewer now has a better sense of the strengths of the IGN approach, they still feel that the motivation for a new approach needs to be sufficiently compelling to overcome the advantages of existing finely polished methods. In this light, the reviewer feels that the motivation is still somewhat underwhelming. That being said, the field would be unhealthy if new (or perhaps not so new if one believes the public comment above) methods were not routinely explored.\n- The reviewer enjoyed the experiments investigating what happens to $f^k(x)$ as $k \\rightarrow \\infty$. It is not clear to the reviewer in what ways the size of the model would impact stability, but it is a direction that would be interesting to explore in the future.\n\nOverall, the reviewer feels that the work is valuable because of the way that it frames the generative task around the idempotence property, the interesting analysis of the task, and the fact that the model performs at around the same level as other (now successful) approaches at similar points in their development. The reviewer would be happy to raise their score to an 8."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597879651,
                "cdate": 1700597879651,
                "tmdate": 1700597879651,
                "mdate": 1700597879651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "diPJnAIYvD",
            "forum": "XIaS66XkNA",
            "replyto": "XIaS66XkNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_5afM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_5afM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new class of generative model that is based on idempotent, i.e., $f(f(x)) = f(x)$. It acts like a projection function that project anything to the target data manifold, and it preserves identity for data already in the target manifold.To train such a model, first a reconstruction loss is used to let the model map latent z to the data manifold, and an idempotent loss to ensure the property of idempotent. An additional tightness loss is used for regularization. Some preliminary results of generation is shown."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The biggest strength is that the idea is novel and refreshing. It introduces a new class of generative models with an interesting formulation.\n\n2. Overall the presentation is good. The idea is clearly stated in the pseudo code. The introduction of idempotent is clear and interesting in the first section.\n\n3. Although the results are preliminary, there are some interesting observations. For example, the method can naturally take things other than noise as input (e.g., Figure 5 c)"
                },
                "weaknesses": {
                    "value": "1. The biggest issue is that I don't get the motivation for designing such a model. When one designs a new class of generative model that is different from any previous approaches, it is necessary to clarify that it has the potential to address some limitations of previous approach. As a preliminary study, it does not need to have great results, but conceptually the advantage of the model has to be stated. Otherwise, it is just \"another class of generative models\". Since it suffers the same mode collapse and instability issue of GAN, the motivation becomes more important. Why do we need such a model? What are the potential advantages?\n\n2. I don't quite get the point of the tightness loss. Why maximize it is good for regularization? More explanation is needed."
                },
                "questions": {
                    "value": "My questions are stated in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784210553,
            "cdate": 1698784210553,
            "tmdate": 1699636293398,
            "mdate": 1699636293398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rba6S0oLF5",
                "forum": "XIaS66XkNA",
                "replyto": "diPJnAIYvD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful consideration of the paper and constructive feedback. \n\n**Motivation: \u201cWhen one designs a new class of generative model that is different from any previous approaches, it is necessary to clarify that it has the potential to address some limitations of previous approach.\u201d**  \nIGN has several potential advantages over the current methods:  \n1. Unlike DIffusion models, IGN generates in a single step, and then, unlike GANs,  allows for sequential refinement. There are Diffusion based methods with fewer steps, but for IGN the number of reprojections can be determined adaptively (apply again if desired). A single step will always yield good results. \n2. Unlike Diffusion models, IGN maintains a consistent latent space, allowing smooth interpolation across all iterations ($f(z), f(f(z)), f(f(f(z)))$...). (Please see videos in supplementary).\n3. IGN can be applied to types of data that it has never \u2018seen\u2019 and project them onto the data manifold. While Diffusion models can be guided to get condition signals, IGN does that when regularly applied to degraded/modified input.\n4. Unlike GANs and Diffusion models, editing of real images does not require any type of inversion. Simply inputting a real image in a feed-forward fashion into IGN allows manipulation on it (see new figures 9,10 in revised manuscript).\n\n\n**\u201dI don't quite get the point of the tightness loss. Why maximize it is good for regularization? More explanation is needed.\u201d**  \nWe appreciate the feedback and acknowledge the challenge in conveying the concept of $\\mathcal{L}\\_{tight}$. In the new draft, we added the notion of $\\mathcal{S}$ to the caption of fig.2. We updated the text about $\\mathcal{L}\\_{tight}$ and also added Fig.17 to the appendix of the revised manuscript. Fig.17 offers further clarification on the two paths to enforce idempotence. We offer additional informal perspectives here for better clarity and welcome any suggestions to enhance the explanation in the paper.\n\nConsider the metaphor of High Jump. The objective is to clear the bar. This can be achieved in two ways: jumping higher, or lowering the bar. If the athlete, $f$ trained directly for this objective, it would become an expert on lowering the bar (tends to become identity). Instead, the athlete tries to get **bigger** gap from the bar when jumping, but trying to get **smaller** gap from the bar (opposite from the objective), when setting it according to its recent jumps. We emphasize that this is the same athlete that both jumps and sets the bar.\n\nExamining the objective $\\delta\\_\\theta = D(f\\_\\theta(f\\_\\theta(z)),f\\_\\theta(z))$, we can split it into two phases:  \n(1) $y=f\\_\\theta(z)$ (jumping)  \n(2) $\\delta\\_\\theta = D(f\\_\\theta(y),y)$ (evaluating the gap from the bar)  \nBoth are done using the same set of parameters $\\theta$. See Fig.17 in the appendix. The instance of $f$ that is in phase (1) sends gradients to $\\theta$ that improve the generation (higher jump, lower drift $\\delta\\_\\theta(y) = D(f\\_\\theta(y), y)$). The Gradients to $\\theta$ through the instance of $f$ that is in phase (2) judge generated images and train to exclude them from the manifold (higher drift), just like setting the bar higher according to the recent jumps. \nThis aspect can also be thought of as a GAN where the generator and discriminator are the same network."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870203000,
                "cdate": 1699870203000,
                "tmdate": 1699892154403,
                "mdate": 1699892154403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cPSWtePIdF",
                "forum": "XIaS66XkNA",
                "replyto": "diPJnAIYvD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup"
                    },
                    "comment": {
                        "value": "Dear reviewer, towards the end of the discussion phase, we trust that our response has successfully addressed your inquiries. We look forward to receiving your feedback regarding whether our reply sufficiently resolves any concerns you may have, or if further clarification is needed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512531093,
                "cdate": 1700512531093,
                "tmdate": 1700512531093,
                "mdate": 1700512531093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FsdqI3Cbdu",
                "forum": "XIaS66XkNA",
                "replyto": "cPSWtePIdF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_5afM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_5afM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the response. My confusion on the tightness loss is addressed. However, I don't quite buy the motivation part. Yes, it is a single step method compared to diffusion, but so does GAN. If you claim advantages like sequential refinement, it becomes multi-step again. In addition, there isn't enough evidence showing that the refinement indeed significantly improve the sample quality. Overall, the minimax loss objective makes it look like a GAN variant, which is also pointed out by someone else. It suffers from the same issue of mode collapse of GAN, and the fact that it has only a single network does not distinguish itself enough. EBGAN is one such example, and the well-known connection between maximum likelihood training of EBM and adversarial training is another, where there's only one network for discrimination, and the generator is instantiate by the iterative sampling. \n\nAs a result, given the connection to earlier models and the behaviors exhibits, I think the proposed model does not look very promising. It's a good idea to shed new lights and find new connections to existing frameworks, but that would require a complete rewrite. Therefore, I maintain my judgement on the submission."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715909514,
                "cdate": 1700715909514,
                "tmdate": 1700715909514,
                "mdate": 1700715909514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wuR76FeQvW",
            "forum": "XIaS66XkNA",
            "replyto": "XIaS66XkNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_Cz9y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_Cz9y"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new kind of generative model for image synthesis called the Idempotent Generative Network (IGN). The key idea is to learn a function $f$ such that $f(x) = x$ if $x$ is on the data manifold, and $f(z)$ is on the data manifold if $z \\sim P_0$ for some prior $P_0$ (in which case $f(f(z)) = f(z)$). The network $f$ is learned with three loss terms: a reconstruction loss to encourage data samples to be mapped to themselves, an idempotent loss to encourage two applications of $f$ to a state $z$ from the prior to match a single application of $f$, and a tightness objective encouraging $f$ to map any state $z'$ not on the manifold as far away from the manifold as possible. A theoretical analysis is presented which claims that the proposed method will generated samples from the data distribution if $f$ is perfectly trained and has enough capacity. Experimental results show that the proposed method can generate MNIST and CelebA images from noise."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The work is an ambitious and refreshing approach to generative modeling. Idempotence is an interesting foundation for building a generative model. Overall the paper was enjoyable to read.\n* The central ideas are clearly presented and I could easily follow the training algorithm and motivation of the method.\n* Empirical results show that the method is capable of image generation and editing the features of generated faces."
                },
                "weaknesses": {
                    "value": "* I am not fully convinced of the theoretical validity of the method. In particular, Equation (19) does not appear fully rigorous. From my understanding, the value $M$ is the maximum distance between points in the state space. \n  * What if the state space is unbounded? Should we expect states to be restricted to an image hypercube?\n  * Why is it the case that $max_{\\theta*} \\delta_{\\theta*}(y) = M$ for any $y$ when $P_x (y) < \\lambda_t P_{\\theta*} (y)$? Wouldn't that mean the distance between $y$ and the furthest point from $y$ in the state space is $M$ for each $y$? I am not sure that makes intuitive sense to me. It seems that $M$ should not be a constant but rather a function $M(y)$ measuring the distance between $y$ and the furthest point in the state space, which greatly complicates the analysis that follows.\n\n  The proof heavily relies on the simple form of (19), but I am not sure this simple form is valid. Furthermore, in practice the distance $M$ is heavily restricted which leaves a gap between the theoretical development of the method and practical implementation. Since this method departs heavily from the established probabilistic principles used to justify generative models, clearly demonstrating the validity of the method is essential. I am willing to raise my score if these points can be addressed.\n* Although I understand that the current results are meant to be a proof of concept, it would still be good to provide a more formal comparison with prior generative models e.g. by calculating FID scores on CelebA."
                },
                "questions": {
                    "value": "Please see my questions about the theoretical justification of the method in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830669363,
            "cdate": 1698830669363,
            "tmdate": 1699636293309,
            "mdate": 1699636293309,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UGAHxSBlOy",
                "forum": "XIaS66XkNA",
                "replyto": "wuR76FeQvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful consideration of the paper and constructive feedback. \n\n**The value $M$:**  \n$M$ is the supremum of the distance metric for any pair $(y\\_1, y\\_2)$. There is no bounded space assumption -- M can also be infinity (which is why we did not define it as Maximum. We added a clarification in the paper). We examine both the scenario of unbounded $M$ (which is the case of MSE as used for MNIST and shown in the provided code), and the scenario of bounded $M$ which exists when using the clamp technique as introduced in eq.14. \nUnbounded $M$ is consistent with all the steps of the proof, under the practical assumption that $M \\cdot 0 < M$.\n\n**The role of $M$ in eq.21 (formerly eq.19):**  \nThank you for pointing out the lack of clarity. We have revised the text before the equation (that is eq.21 now). \nEq.21 describes what happens when $\\mathcal{L}\\_{rt}$ is minimized, according to how it is expressed in eq.20 (eq.18 second row in pre-revised version):\n$$ \\mathcal{L}\\_{rt}(\\theta;\\theta^*)=\\int \\delta\\_{\\theta}(y) \\Bigl(\n\\mathcal{P}\\_x(y)\n\\- \\lambda\\_t \n\\mathcal{P}\\_{\\theta^*}(y)\n\\Bigr)dy $$\n$\\delta\\_\\theta$ is multiplied by a given expression in the brackets that is not being optimized in this phase.\nTo obtain a minimum, $\\delta\\_\\theta\\geq0$ needs to be as big as possible when the expression in the brackets is negative and as small as possible when positive. Thus, for any $M>0$ eq.21 holds when  $\\mathcal{L}\\_{rt}$ obtains its minimum:\n$$ \\delta\\_{\\theta^*}(y) = M \\cdot 1\\_{\\{\\mathcal{P}\\_{x}(y) < \\lambda\\_t  \\mathcal{P}\\_{\\theta^*}(y) \\}}$$\n\n**Should it be $M(y)$?:**  \nThank you for identifying this subtle issue. Indeed, for a general metric in a general space, $M$ should be defined using a reference input $M(y\\_1) = \\sup\\_{y\\_2} D(y\\_1, y\\_2)$. An example could be MSE in some bounded space. We should point out that in the metrics used in the paper (MSE, L1, and clamped L1), $M$ would be constant ($\\infty$ for unbounded metrics and the clamp value for the clamped metric). Regardless, using $M(y)$ instead of $M$ would not have influence on the subsequent steps of the proof as long as $M(y)>0\\quad\\forall y$. \nTo see this we can examine minimizing $\\mathcal{L}\\_{idem}$ as in eq.24 modified with $M(y)$:\n$$\n    \\theta^* = argmin\\_\\theta  \\mathbb{E}\\_{z}\\bigl[ M(y) \\cdot  1\\_{\\{\n    \\mathcal{P}\\_{x}(y) < \\lambda\\_t  \n    \\mathcal{P}\\_{\\theta}(y) \n    \\}} \\bigr] \n$$\n$M$ being positive implies that for every value of $y$, $M(y)\\cdot 0 < M(y) \\cdot 1$. This leads to the same $\\theta^*$ for constant $M$ or $M(y)$.  \nWe are happy to revise and redefine $M(y)$. Another option is to add in the text assumptions on the metrics used.\n  \n**$M$ imposes gap between theory and practice?:**   \nHopefully, the above explanation was able to show that $M$ does not impose restrictions and that, while the theoretical results make very strong idealistic assumptions, the practical implementation does not diverge more than in any other generative model's theoretical results.  But please let us know if something is still unclear and we will do our best to respond quickly.  \n\n\n**FID on CelebA:**  \nWe report FID=39 for IGN on CelebA with the architecture in Table.1. Naturally, this is not competitive with modern models, but is comparable to early models such as DCGAN (FID=34), while also capable of a wide range of conditional tasks. We updated the draft paper to include this info."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869981377,
                "cdate": 1699869981377,
                "tmdate": 1700422271682,
                "mdate": 1700422271682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OaunfOUbOV",
                "forum": "XIaS66XkNA",
                "replyto": "wuR76FeQvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup"
                    },
                    "comment": {
                        "value": "Dear reviewer, towards the end of the discussion phase, we trust that our response has successfully addressed your inquiries. We look forward to receiving your feedback regarding whether our reply sufficiently resolves any concerns you may have, or if further clarification is needed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512480859,
                "cdate": 1700512480859,
                "tmdate": 1700512480859,
                "mdate": 1700512480859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uY0TscHjiQ",
                "forum": "XIaS66XkNA",
                "replyto": "wuR76FeQvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_Cz9y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_Cz9y"
                ],
                "content": {
                    "title": {
                        "value": "Revision including EBGAN would be helpful"
                    },
                    "comment": {
                        "value": "I read the response of the author, other reviewers, and the public commenter. I agree with the public commenter that this method is a special case of EBGAN where the generator and discriminator share the same structure. As noted by the author response, there are interesting properties of an idempotent network that EBGAN does not have and that the connection between idempotence and EBGAN is an interesting novel view that is perhaps more streamlined than the original EBGAN framework. I suggest that the authors simply use the EBGAN proof of convergence to theoretically justify their method. Overall, I believe this paper needs a major revision to incorporate EBGAN as a prior work and reframe its contributions. I will keep my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633584764,
                "cdate": 1700633584764,
                "tmdate": 1700633796761,
                "mdate": 1700633796761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LNE9jfvBw2",
            "forum": "XIaS66XkNA",
            "replyto": "XIaS66XkNA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_Sz9m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3420/Reviewer_Sz9m"
            ],
            "content": {
                "summary": {
                    "value": "This work presents what appears to me to be a novel way to model the generative process from noise (or corrupted images) using idempotence as the underlying principle for the objective(s). Notably, they train a model which at optimum should maintain given datapoints from the target distribution, yet generate samples whose manifold is maximally \"tight\". These objectives together yield a novel generative model algorithm, and there is theoretical and empirical support."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall I found the approach to be appealing and interesting. Though there isn't necessarily a strong compelling reason here why to use idempotence to train a generative model, I don't find this as a weakness (prefacing here), as there is scientific curiosity here, the intuition is compelling, the theory seems correct wrt the optima fitting the target distribution, and the empirical results support everything neatly. I'd also mention that the results are stellar / SOTA, but as noted in the paper: this is fine. Engineering might improve things or it might not, just generally it looks like good science with a solid story and results to support."
                },
                "weaknesses": {
                    "value": "Some points could be clarified, notably I found the discussion around Figure 2 to be less clear than I would have liked. I think I understand the general gist, but I honestly didn't think I understood how the different objectives work together until I got through the theory section. Could this be cleaned up a bit / made a bit more clear? I'd enjoy a bit of some discussion here to see if we can arrive at a better way to present this."
                },
                "questions": {
                    "value": "Why does M:= sup(D(y1, y2)) show up in eq 19? Are we saying if y is outside the target data manifold + the margin that the repeated applications of f should maximize the distance? How is this guaranteed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698873070735,
            "cdate": 1698873070735,
            "tmdate": 1699636293237,
            "mdate": 1699636293237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8jFGxh9FL5",
                "forum": "XIaS66XkNA",
                "replyto": "LNE9jfvBw2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful consideration of the paper and constructive feedback.\n\n**\"I found the discussion around Figure 2 to be less clear than I would have liked.\"**  \nWe appreciate the feedback and acknowledge the challenge in conveying the concept of $\\mathcal{L}\\_{tight}$. We added the notion of $\\mathcal{S}$ to the text around fig.2 and its caption. We updated the text about $\\mathcal{L}\\_{tight}$ and also added Fig.17 to the appendix of the revised manuscript. Fig.17 offers further clarification on the two paths to enforce idempotence. We offer additional informal perspectives here for better clarity and welcome any suggestions to enhance the explanation in the paper.\n\nConsider the metaphor of High Jump. The objective is to clear the bar. This can be achieved in two ways: jumping higher, or lowering the bar. If the athlete, $f$ trained directly for this objective, it would become an expert on lowering the bar (tends to become identity). Instead, the athlete tries to get **bigger** gap from the bar when jumping, but trying to get **smaller** gap from the bar (opposite from the objective), when setting it according to its recent jumps. We emphasize that this is the same athlete that both jumps and sets the bar.\n\nExamining the objective $\\delta\\_\\theta = D(f\\_\\theta(f\\_\\theta(z)),f\\_\\theta(z))$, we can split it into two phases:  \n(1) $y=f\\_\\theta(z)$ (jumping)  \n(2) $\\delta\\_\\theta = D(f\\_\\theta(y),y)$ (evaluating the gap from the bar)  \nBoth are done using the same set of parameters $\\theta$. See Fig.17 in the appendix. The instance of $f$ that is in phase (1) sends gradients to $\\theta$ that improve the generation (higher jump, lower drift $\\delta\\_\\theta(y) = D(f\\_\\theta(y), y)$). The Gradients to $\\theta$ through the instance of $f$ that is in phase (2) judge generated images and train to exclude them from the manifold (higher drift), just like setting the bar higher according to the recent jumps. \nThis aspect can also be thought of as a GAN where the generator and discriminator are the same network.\n\n**\"Why does M:= sup(D(y1, y2)) show up in eq 19?\"**  \nThank you for pointing out the lack of clarity. We have revised the text before the equation (that is eq.21 now). \nEq.21 describes what happens when $\\mathcal{L}\\_{rt}$ is minimized, according to how it is expressed in eq.20 (eq.18 second row in pre-revised version):\n$$\n\\mathcal{L}\\_{rt}(\\theta;\\theta^*)=\\int \\delta\\_{\\theta}(y) \\Bigl(\n\\mathcal{P}\\_x(y)\n\\- \\lambda\\_t \n\\mathcal{P}\\_{\\theta^*}(y)\n\\Bigr)dy\n$$\n\n$\\delta\\_\\theta$ is multiplied by a given expression in the brackets that is not being optimized in this phase.\nTo obtain a minimum, $\\delta\\_\\theta\\geq0$ needs to be as big as possible when the expression in the brackets is negative and as small as possible when positive. Thus, for any $M>0$ eq.21 holds when  $\\mathcal{L}\\_{rt}$ obtains its minimum:\n$$ \\delta\\_{\\theta^*}(y) = M \\cdot 1\\_{\\{\\mathcal{P}\\_{x}(y) < \\lambda\\_t  \\mathcal{P}\\_{\\theta^*}(y) \\}}$$"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869673413,
                "cdate": 1699869673413,
                "tmdate": 1699892025784,
                "mdate": 1699892025784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5LSIky8Il3",
                "forum": "XIaS66XkNA",
                "replyto": "LNE9jfvBw2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, towards the end of the discussion phase, we trust that our response has successfully addressed your inquiries. We look forward to receiving your feedback regarding whether our reply sufficiently resolves any concerns you may have, or if further clarification is needed."
                    },
                    "title": {
                        "value": "Followup"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512414334,
                "cdate": 1700512414334,
                "tmdate": 1700512498000,
                "mdate": 1700512498000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5KCpdj7XCB",
                "forum": "XIaS66XkNA",
                "replyto": "Q61U9IJgqP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_Sz9m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_Sz9m"
                ],
                "content": {
                    "title": {
                        "value": "Comparisons to EBGAN"
                    },
                    "comment": {
                        "value": "I would like to thank the Michael for their insights on the connections between EBGAN and IGN. However, I would also like to alert them, as they are a more senior researcher, that their tone has begun to become somewhat hostile, and I am beginning to become concerned that this might bring undo influence over the review process of this paper (particularly when received by more junior researchers). \n\nI intend to go over the connections between EBGAN and IGN (it's been a long time since I've read that paper) in detail, and I acknowledge the connections you have laid out. However, I may still consider the work to be publishable, as the insights are still interesting and novel.\n\nMore specifically, I believe there are many ways to arrive at the same solution. And that's OK: these works should still be permitted to exist, despite they come around to a similar (or nearly the same) answer. If we are to completely discard such works, then we may unfortunately be stifling good and well-intentioned research, which *will reduce the quality of this conference*. Please keep this in mind: the purpose here should be to improve science, not gate-keep it."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706968167,
                "cdate": 1700706968167,
                "tmdate": 1700707363649,
                "mdate": 1700707363649,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]