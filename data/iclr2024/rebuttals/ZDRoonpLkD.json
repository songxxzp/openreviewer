[
    {
        "title": "Revisiting GNNs for Boolean Satisfiability"
    },
    {
        "review": {
            "id": "3vNqOd7urI",
            "forum": "ZDRoonpLkD",
            "replyto": "ZDRoonpLkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
            ],
            "content": {
                "summary": {
                    "value": "The paper primarily builds upon NeuroSAT and proposes several improvements to the original model, including the use of curriculum learning to speed up model training multiple initial assignments to embeddings, and decimation to enhance model accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n1. The authors significantly accelerate training time by employing curriculum learning.\n2. The model exhibits substantial improvements in accuracy compared to NeuroSAT.\n3. The decimation measure, inspired by Belief Propagation (BP), is quite convincing."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n1. Neither the samples nor the decimation techniques are subjected to ablation experiments.\n2. The last sentence of the first paragraph in the introduction is not particularly convincing.\n3. The sampling technique seems to enhance model accuracy solely by initializing values across multiple embeddings, and its relationship with SDP appears weak."
                },
                "questions": {
                    "value": "Could you please clarify the nature of the initial embeddings? Are they generated randomly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698485948027,
            "cdate": 1698485948027,
            "tmdate": 1699636363153,
            "mdate": 1699636363153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FdbDGckkJI",
                "forum": "ZDRoonpLkD",
                "replyto": "3vNqOd7urI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We answer your comments and questions below.\n\nComment 1:\nNeither the samples nor the decimation techniques are subjected to ablation experiments.\n\nAnswer:\nThank you for pointing this out, we extended the table with experiments (Table 1) so that the impact of decimation and sampling can be separated. Concretely, there is now a row for runs with 32 random initialization samples and no decimation. These runs will have the same number of samples as the runs in the first row with 16 samples in the first pass and 1 sample for each decimated problem in the second pass. One can, therefore, see the effect of decimation which is simply the difference between the number of solved problems in total between these two rows. The effect of sampling can be seen by comparing to the second row for runs with just 1 sample and no decimation.\n\n-----------------------------------------------\n\nComment 2:\n`The first paragraph in the introduction is not particularly convincing.\n\nAnswer:\nYou probably meant this sentence: \u201cThe presented ideas should be easily generalizable to other combinatorial domains where these approximation algorithms could be applied.\u201d We believe this should be the case, because SDP relaxation is a very general technique, and relaxations for many other combinatorial problems can be found in the literature. If the GNN is able to discover this relaxation, it should be, together with the ideas we presented, applicable to these other combinatorial domains. Nevertheless, we removed this sentence, as it is not essential and we did not have a space to expand on it.\n\n-----------------------------------------------\n\nComment 3 and question 1:\nThe sampling technique seems to enhance model accuracy solely by initializing values across multiple embeddings, and its relationship with SDP appears weak. Could you please clarify the nature of the initial embeddings? Are they generated randomly?\n\nAnswer:\nThe initial embeddings are random unit vectors. In Appendix A.5 we included results from other experiments which show that the iterations of the GNN can be viewed as optimization steps of the embeddings, optimizing the SDP objective value. Therefore, the intuition is that if these vectors are initialized in different positions, they can converge to a different local optimum. This is how the SDP solver behaves. Some initializations may be more lucky."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548567244,
                "cdate": 1700548567244,
                "tmdate": 1700548607069,
                "mdate": 1700548607069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VHpzAq9EDj",
                "forum": "ZDRoonpLkD",
                "replyto": "3vNqOd7urI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\nRegarding the ablation study you have presented: including a scenario where only decimation is adopted could provide additional insights. Nevertheless, it is fine to see the results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622789038,
                "cdate": 1700622789038,
                "tmdate": 1700625268528,
                "mdate": 1700625268528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4K2IIxIiHE",
                "forum": "ZDRoonpLkD",
                "replyto": "nf4939bTQ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the misunderstanding. I would ask about multiple rounds of decimation, but I found that it has been discussed in the Appendix. So I delete the paragraph."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625360676,
                "cdate": 1700625360676,
                "tmdate": 1700625360676,
                "mdate": 1700625360676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wEmseQrEsW",
            "forum": "ZDRoonpLkD",
            "replyto": "ZDRoonpLkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_wK6B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_wK6B"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the Graph neural networks for combinatorial problems. In this work, the authors applied a curriculum training procedure, a decimation procedure and initial-value sampling. The authors claim that their proposed curriculum and optimization methods reduce training time by more than an order of magnitude and significantly increase the percentage of solved problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written, with clear explanations. It clearly introduces about the motivation of the optimizations and the methods applied.\n\n2. Publicly available source code is provided to reproduce the results."
                },
                "weaknesses": {
                    "value": "I have a number of comments regarding the experimental setup.\n\n1. According to Section 5, the training instances are of very small scale. For the generated, random SAT instances, the number of variables are up to 40 variables. In fact, existing local search SAT algorithms are able to solve random satisfiable instances around phase-transition threshold with thousands of variables very efficiently. Hence, solving random instances with up to 40 variables are quite trivial.\n\n2. After reading Appendix A.3.2, it seems that the authors also do not introduce the number of variables for those generated, structured instances (i.e., those instances generated from the domains of Latin squares, Sudoku, and logical circuits). Actually, it is widely recognized that modern CDCL SAT solvers can solve structured SAT instances with tens of thousands of variables. Could you please claim the numbers of variables for those generated, structured instances?\n\n3. It seems that your proposed method can only handle satisfiable instances. Could you discuss the behavior of your proposed method when dealing with unsatisfiable instances?\n\n4. The authors only compare their proposed method with NeuroSAT. However, CDCL solvers stand for the current state of the art in SAT solving. As a submission to a top-tier conference, lack a comparison against the real state of the art is unacceptable."
                },
                "questions": {
                    "value": "Please see my comments that are listed in the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not find ethics concern."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4009/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4009/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4009/Reviewer_wK6B"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570742925,
            "cdate": 1698570742925,
            "tmdate": 1699636363067,
            "mdate": 1699636363067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KBPIfMhaeX",
                "forum": "ZDRoonpLkD",
                "replyto": "wEmseQrEsW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. We believe that there was a misunderstanding of the purpose of our paper. We were not proposing a method that would be able to compete with mainstream SAT solvers. Our aim was to shed light on the process by which NeuroSAT learns to solve SAT instances. We clearly state that in the introduction: \u201cAlthough these learned solutions lag far behind the solvers used in practice\nin terms of the size of problems they are able to solve, it is desirable to understand the process by which they arrive at their outputs.\u201d\n\nWe demonstrated the similarities of the trained NeuroSAT and two well-known approximation algorithms. Using these connections we proposed several improvements, one of which enables us to train NeuroSAT in order of magnitude shorter time, which can enable quicker experimentation with NeuroSAT.\n\nRegarding the comment: \u201cAs a submission to a top-tier conference, lack of a comparison against the real state of the art is unacceptable.\u201d Please note that Selsam et al. published their paper about NeuroSAT at this conference and it was already cited more than 300 times. We believe that it is paramount to understand what kind of algorithms are GNNs able to learn, irrespective of how practically scalable these learned algorithms are. \n\nMany papers presented at this conference have similar flavors and show the behavior of the trained GNN only on toy examples (e.g. https://openreview.net/forum?id=rJxbJeHFPS , https://openreview.net/forum?id=hhvkdRdWt1F ).  These papers show how GNN are able to learn simple algorithms on small instances. It is clear that GNNs trained end-to-end are currently not able to compete with manually designed solvers. This kind of work fits well with ICLR as the conference is focused mainly on learning representations. \n\nRegarding unsatisfiable instances, we hoped it was clear from the text that our method cannot produce any guarantee of unsatisfiability. This is also the case for local search but it is still used in practice for instance within Cadical or Kissat. We also mentioned in the text that the solvers obtained from these approximation algorithms are incomplete. To make it clear, we now state it explicitly at the end of section 5.\n\nRegarding the number of variables of the structured formulas, they were included in Table 1, in the third column."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548420071,
                "cdate": 1700548420071,
                "tmdate": 1700548420071,
                "mdate": 1700548420071,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "URmAnNbXK8",
            "forum": "ZDRoonpLkD",
            "replyto": "ZDRoonpLkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_rKtv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_rKtv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two improvements over NeuroSAT, a very popular\nmessage-passing NN for Boolean satisfiability (SAT).  These\nimprovements are inspired by two other approaches to (Max-)SAT:\nSemidefinite Programming (SDP) relaxations and Belief Propagation\n(BP). The first improvement is a form of curriculum learning, in which\nthe size of formulas and number of message-passing iterations is\nincreased throughout the training. This first improvement results in a\nsignificantly faster training convergence. The second improvement is\ntwofold: 1) running in parallel NeuroSAT with multiple initializations\nof the embedding vectors. 2) Once the model is trained, it is possible\nto recover the notions of true/false values in the latent space. This\nenables a decimation procedure during message passing, that is, early\nfixing of the truth values if these get too close to true/false.\nThese two modifications result in more robust predictive performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presentation is good overall\n- Well-motivated improvements over the existing work"
                },
                "weaknesses": {
                    "value": "- I have a few minor points on the presentation\n- The experimental section does not address some important questions\n\n---\nDetailed comments:\n\n  \"Moreover, neural networks can potentially find solutions, which\n  could lead to unexpected insight (Pickering et al., 2023; Davies et\n  al., 2021).\"\n\nI don't understand this sentence. Is \"solutions\" in this context\nreferring to approximate algorithms for a target class of problems,\ni.e. a trained model?  What unexpected insights are you hinting at?\n\n  \"Recently, Kyrillidis et al. (2020) demonstrates scenarios where\n  solving a continuous relaxation formulation may provide benefits\n  over solving the formula using standard solvers.\"\n\nI would summarize these benefits.\n\n  \"[..] which has demonstrated the ability to exhibit nontrivial\n  behaviors resembling a search in a continuous space, rather than\n  mere classification based on superficial statistics.\"\n\nWhat nontrivial behaviour are the authors referring to?\n\nIn Section 4: \"Selsam et al. (2019) observes that for formulas that\nthe model correctly classified as satisfiable, the embeddings of\nliterals form two well-separated clusters.\"\n\nThis is already mentioned earlier in the text.\n\n  \"In Figure 4 in the Appendix, we recapitulate their visualization of\n  embeddings with UMAP instead of PCA.\"\n\nI was not able to connect the figure with the following text. Either\nFig. 4 is instrumental in understanding the following paragraphs and\nshould be moved to the main text, or it isn't and this sentence should\nbe removed. It is also unclear to me why you used UMAP instead of PCA.\n\nIt is not clear to me whether sampling multiple initializations and\ndecimation are two orthogonal improvements. If so, I would expect a\nseparate empirical evaluation for the two.\n\nGiven the nice performance improvements, I am left wondering if the\n(augmented) NeuroSAT architecture is competitive in some settings with\nother end-to-end approaches. This is not addressed in the experimental\nsection. Can we leverage curriculum learning to push the predictive\naccuracy over 85% using a more expressive model? Can it also\nbetter generalize to larger problems wrt the original NeuroSAT?\n\nReporting the inference time of the standard vs. decimation approaches\nis necessary to the evaluation. I also wonder why only 2 passes of\ndecimation were evaluated. What happens if we do more?"
                },
                "questions": {
                    "value": "1) If multiple initializations and decimation are orthogonal improvements, can you provide ablation results for the two?\n2) Can you provide a more throughout evaluation of decimation (i.e. with more than 2 passes)?\n3) What is the runtime cost of your approach wrt standard NeuroSAT?\n4) Does the augmented NeuroSAT method better generalize to larger instance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766170287,
            "cdate": 1698766170287,
            "tmdate": 1699636362968,
            "mdate": 1699636362968,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uDenuqhaDZ",
                "forum": "ZDRoonpLkD",
                "replyto": "URmAnNbXK8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback. Bellow are individual answers for your questions and some of your comments. For the comments we don\u2019t reply to, changes were made in the text to make these points clearer):\n\nComment:\n\"In Figure 4 in the Appendix, we recapitulate their visualization of embeddings with UMAP instead of PCA.\"\nI was not able to connect the figure with the following text. Either Fig. 4 is instrumental in understanding the following paragraphs and should be moved to the main text, or it isn't and this sentence should be removed. It is also unclear to me why you used UMAP instead of PCA.\n\nAnswer:\nThank you for pointing this out. We included a simplified version of this figure into the main text. The figure shows how the embeddings evolve during the message-passing process. For satisfiable formulas, these embeddings create two well-separated clusters at the end. The text explains that it is possible to extract the solution of the formula by running a clustering algorithm on the embeddings produced by the last step of the MP process (for SAT formulas there should be 2 distinct clusters). One cluster will correspond to literals which will take the value TRUE and the other to literals which will take the value FALSE. We used UMAP as the clusters were more distinct in comparison to the visualization obtained by PCA.\n\n\u2014-----------------------------------------------\n\nComment and question 1:\nIt is not clear to me whether sampling multiple initializations and decimation are two orthogonal improvements. If multiple initializations and decimation are orthogonal improvements, can you provide ablation results for the two?\n\nAnswer:\nThank you for pointing this out, we extended the table with experiments (Table 1) so that the impact of decimation and sampling can be separated. Concretely, there is now a row for runs with 32 random initialization samples and no decimation. These runs will have the same number of samples as the runs in the first row with 16 samples in the first pass and 1 sample for each decimated problem in the second pass. One can, therefore, see the effect of decimation which is simply the difference between the number of solved problems in total between these two rows. The effect of sampling only can be seen by comparing to the second row for runs with just 1 sample and no decimation.\n\n\u2014-----------------------------------------------\n\nComment:\nGiven the nice performance improvements, I am left wondering if the (augmented) NeuroSAT architecture is competitive in some settings with other end-to-end approaches. This is not addressed in the experimental section. \n\nAnswer:\nOur main goal in this paper was to shed light on the inner workings of trained NeuroSAT. Even though we show practical improvements, their main purpose was to support our claim that the trained GNN behaves as an SDP solver (which can be initialized with different values). Decimation on the other hand support the connection to the BP. The practical benefit of the curriculum is that one can iterate experiments with NeuroSAT faster. \nWe are also interested in understanding how other end-to-end approaches are able to predict satisfiability (i.e. whether they are also doing something similar to an SDP solver) but we leave this direction for future work.\n\n\u2014-----------------------------------------------\n\nComment and question 4:\nCan we leverage curriculum learning to push the predictive accuracy over 85% using a more expressive model? Can it also better generalize to larger problems wrt the original NeuroSAT?\n\nAnswer:\nThe curriculum learning by itself does not seem to push the predictive accuracy. We saw that NeuraSAT with the curriculum can sometimes reach an accuracy of 87% (on the original test set) but we guess that NeuroSAT would also be able to reach this accuracy if trained long enough and with proper hyperparameters. We saw no difference in generalization to larger problems between the original NeuroSAT and the one trained with the curriculum. \n\n\u2014-----------------------------------------------\n\nQuestion 2 and question 3:\nCan you provide a more throughout evaluation of decimation (i.e. with more than 2 passes)? What is the runtime cost of your approach wrt standard NeuroSAT?\n\nAnswer:\nIn Appendix A.6, we included a table with one more decimation step. The fourth pass did not solve any other problem in our test datasets. We will study this more closely in future work.  \nWe did not optimize the runtime of the decimation procedure. We decimate and obtain a new prediction for each sample separately. If we have 16 samples for which we obtain predictions in parallel and for each of the 16 decimated formulas we produce 1 sample, then this should take approximately 16x more time than without the decimation. Decimation could probably be parallelized but as these end-to-end approaches are currently not used in practice we did not explore such optimizations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549078029,
                "cdate": 1700549078029,
                "tmdate": 1700549078029,
                "mdate": 1700549078029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zpruTrPHsD",
                "forum": "ZDRoonpLkD",
                "replyto": "uDenuqhaDZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_rKtv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_rKtv"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response, I think I have a better grasp of the purpose and significance of your work now.\nThe changes to the manuscript are also welcome."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667657199,
                "cdate": 1700667657199,
                "tmdate": 1700667657199,
                "mdate": 1700667657199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rXuBFcBg3m",
            "forum": "ZDRoonpLkD",
            "replyto": "ZDRoonpLkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes enhancements for the training and inference procedure of Graph Neural Networks (GNNs) that are trained to predict solutions of combinatorial problems, with a focus on Boolean Satisfiability (SAT). The proposed optimizations include a curriculum training procedure, a novel loss function, and a dynamic batching strategy. The idea is inspired by the possible connection of the behavior of GNN and two algorithms: Belief Propagation and Semidefinite Programming Relaxations. These enhancements significantly reduce training time and increase the percentage of solved problems. The paper also provides a comprehensive review of related work in the context of GNNs and Boolean Satisfiability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem studied in this paper is fundamental: what does a GNN learn? Does it devise a new algorithm? Combinatorial problems are perfect objections for conducting those studies as they are well-studied and we already know a bunch of algorithms. NeuroSAT is a well-known work in the application of GNN on combinatorial problems. What algorithm NeuroSAT really learns has not been fully understood. Therefore, the behavior of NeuroSAT is of great interest.\n\nI like the algorithmic part of this work, which greatly improves the training efficiency. This paper also simplifies the structure of NeuroSAT, which may make it easier for future work to investigate its behavior.\n\nThe paper is well-written and easy to follow. The introduction and preliminary sections give comprehensive context and background. \n\nDespite the over-claimed connection between NeuroSAT and SDP/MP, I still like the direction of this work. I am happy to change my evaluation if my concerns can be addressed."
                },
                "weaknesses": {
                    "value": "The paper claims to reveal the similarity of GNN and Belief propagation. However, there is little convincing evidence of those similarities, in my opinion. It is mentioned in the paper that: \n\n\"For satisfiable formulas, this happens when the vectors form two well-separated clusters, which makes the whole\nprocess qualitatively similar to the optimization of the SDP relaxation described in Section 2.3.\"\n\nThe vectors forming two well-separated clusters, while interesting, is not strong evidence that NeuroSAT is similar to SDP. There may be other algorithms for SAT based on lifting to high-dimension vectors that also obey this behavior. Either stronger evidence (e.g. NeuroSAT is optimizing some quadratic objective) should be revealed or the statement of NeuroSAT & SDP should be removed. It would be great to see more experiments for the behavior of NeuroSAT, besides showing the efficiency of the new network structure with the curriculum."
                },
                "questions": {
                    "value": "Can more evidence be discovered for the similarity of NeuroSAT and SDP/MP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4009/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf",
                        "ICLR.cc/2024/Conference/Submission4009/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823038371,
            "cdate": 1698823038371,
            "tmdate": 1700693576985,
            "mdate": 1700693576985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g0O7zkidGj",
                "forum": "ZDRoonpLkD",
                "replyto": "rXuBFcBg3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, we appreciate your constructive feedback. Based on your comments we ran several experiments to explore the connection to SDP further and included the results in the appendix A.5. Here we summarize our findings for 2-SAT formulas:\n\nFirst, we visualized what happens with the SDP objective if we compute it from the embeddings of literals obtained from NeuroSAT after each MP iteration. Our intuition was that each message-passing iteration would increase the SDP objective. \n\nThe objective is a linear function of the Gram matrix (matrix with all pairs of inner products) of the literal embeddings together with the embedding for the additional variable representing the value TRUE. As this additional variable is not explicitly represented in NeuroSAT, we obtain it by averaging all embeddings of literals that will be assigned to value TRUE in the final solution. Before computing the Gram matrix and the SDP objective value, we also center all embeddings to 0 and normalize them to unit vectors. As can be seen in Figure 6 in the appendix, the SDP objective value is mostly increasing after each MP iteration. \n\nThere is a visible gap between the solution from NeuroSAT and the solution found by the SDP solver. In the text, we explain that this gap is largely caused by suboptimally choosing the vector representing the value TRUE. Therefore, the evolution of the literal embeddings in NeuroSAT can be seen as an optimization process of the SDP objective (modulo the centering and normalization).\n\nWe believe that it is valuable to point to these connections as the SDP relaxation for MaxSAT is well-known and can provide ideas for experiments. To the best of our knowledge, it is the only theoretically understood algorithm for MaxSAT that lifts the literals to high dimensional vectors. \n\nTo not over-claim the connections, we also added the following two sentences at the end of the introduction.\n\u201cWe also emphasize that we do not derive any formal statements regarding these connections. We demonstrate that the behavior of the trained GNN is qualitatively similar to these approximation algorithms and that using insights from these approximation algorithms can lead to practical improvements.\u201d"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547928145,
                "cdate": 1700547928145,
                "tmdate": 1700547928145,
                "mdate": 1700547928145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9IN8IzfT7B",
                "forum": "ZDRoonpLkD",
                "replyto": "g0O7zkidGj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and the results! With the changes and clarifications, I am willing to increase my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693567474,
                "cdate": 1700693567474,
                "tmdate": 1700693567474,
                "mdate": 1700693567474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]