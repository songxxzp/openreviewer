[
    {
        "title": "JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention"
    },
    {
        "review": {
            "id": "rcAGDvJMNL",
            "forum": "LbJqRGNYCf",
            "replyto": "LbJqRGNYCf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_5uGr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_5uGr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Joint MLP/Attention (JoMA) dynamics, a mathematical framework designed to understand the training process of multilayer Transformer architectures. This is accomplished by integrating out the self-attention layer in Transformers, resulting in a modified dynamics that focuses solely on the MLP layers. JoMA addresses limitations present in previous analyses by removing unrealistic assumptions, such as the lack of residual connections.\n\nThe authors predict that attention in JoMA initially becomes sparse to learn salient tokens and then transitions to dense attention to capture less salient tokens when nonlinear activations are incorporated. In the case of linear activations, the predictions align with existing literature. Additionally, JoMA is leveraged to provide qualitative explanations of how tokens combine to form hierarchies in multilayer Transformers, specifically in scenarios where input tokens are generated by a latent hierarchical generative model.\n\nTo validate their theoretical findings, the authors conduct experiments using real-world datasets (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia). The experimental results support their claims and contribute to the credibility of their research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper introduces the JoMA dynamics, which is a new mathematical framework specifically designed to understand the training procedure of multilayer Transformer architectures.\n\n+ The authors highlight that previous analyses of similar models often make unrealistic assumptions, such as the lack of residual connections. In contrast, the JoMA framework removes these assumptions, making it more accurate and realistic in capturing the training dynamics of multilayer Transformers."
                },
                "weaknesses": {
                    "value": "- Although the paper mentions experimental validation using real-world datasets and pre-trained models, it does not provide detailed quantitative evaluation metrics or comparisons against existing approaches. Including quantitative analysis would enhance the robustness and comprehensiveness of the findings.\n\n- The paper does not delve into specific implementation details of the JoMA framework, such as hyperparameter choices or training procedures. Providing more information on these aspects would enable researchers to replicate the experiments and further validate the proposed approach."
                },
                "questions": {
                    "value": "1. Why is Figure 1(b) labeled as the \"problem setting\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698041771244,
            "cdate": 1698041771244,
            "tmdate": 1699636378050,
            "mdate": 1699636378050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RRermDT0mv",
                "forum": "LbJqRGNYCf",
                "replyto": "rcAGDvJMNL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and encouraging comments! \n\nNote that the reviewer may misunderstand our main contribution. We do not propose a novel approach to push SoTA, but instead we analyze existing Transformer architectures via a novel angle, by finding the joint dynamics of the self-attention and the MLP layer and using these new mathematical finding to analyze the training dynamics of multi-layer Transformer architectures. Therefore, there is no comparison against existing approaches. \n\nWe indeed provide detailed quantitative analysis to show our theoretical findings are consistent with the real world scenarios. This includes the following:\n+ We plot the change of entropy of attention patterns over time at different layers (Fig.6-8), as well as changes of stable rank in MLP layers (Fig. 7), verifying our theoretical findings (Sec. 4 and Theorem 4). \n+ We show that the hidden neurons in the Transformer MLP layer indeed learn the latent concepts in the hierarchy (Tbl. 1). \n+ We have multiple experiments that verify our intermediate theoretical findings. For example, Fig. 2 shows that Theorem 1 largely holds for softmax attention, even if the assumption for the theorem to hold for softmax may not be that realistic. Fig. 3 verifies Theorem 2 (growth in weights in the case of linear activations). \n\nSince JoMA is not a novel approach but a way to analyze existing Transformer architectures with more realistic assumptions, we do not have specific implementation details of the framework like hyper-parameter choices. We follow the standard practice in training and mention our choices of hyper-parameters in Sec. 6.\n\nFig. 1(b) is labeled as \"problem setting\" since this is the specific, mathematically well-defined Transformer architecture we analyze in JoMA."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201509871,
                "cdate": 1700201509871,
                "tmdate": 1700201593850,
                "mdate": 1700201593850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZYPLhkltry",
            "forum": "LbJqRGNYCf",
            "replyto": "LbJqRGNYCf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_cMvf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_cMvf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel mathematical framework called JoMA to understand the training procedure of multilayer Transformer architectures. By integrating out the self-attention layer in Transformers, the authors focus on the dynamics of MLP layers. The framework removes unrealistic assumptions and provides insights into the behavior of attention mechanisms. The authors leverage JoMA to explain how tokens form hierarchies in multilayer Transformers using a latent hierarchical generative model. Experimental results on real-world datasets and pre-trained models validate the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed JoMA framework offers a new perspective on understanding the training dynamics of multilayer Transformers. By integrating out the self-attention layer, the authors provide valuable insights into the behavior of MLP layers.\n\n2. This paper presents theoretical findings that explain how tokens are combined to form hierarchies in multilayer Transformers. These findings are supported by experiments on real-world datasets and pre-trained models, enhancing the credibility of the results.\n\n3. This paper is well-written and structured, making it easy to follow the proposed framework and understand the theoretical explanations. The inclusion of references to related works in the field strengthens the paper's contribution."
                },
                "weaknesses": {
                    "value": "1. Limited practical implications. This paper focuses on theoretical analysis and understanding the training dynamics of multilayer Transformers. It would be valuable to discuss potential practical applications or implications of the proposed framework in real-world scenarios.  \n\n2 Several lines of work around the learning preference of DNNs need to discussed [cite1-3].\n\n[cite1] Devansh Arpit, Stanis\u0142aw Jastrz\u02dbebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,\nMaxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A\ncloser look at memorization in deep networks. In International conference on machine learning,\npages 233\u2013242. PMLR, 2017  \n[cite2] Karttikeya Mangalam and Vinay Uday Prabhu. Do deep neural networks learn shallow learnable\nexamples first? 2019.  \n[cite3] Huh M, Mobahi H, Zhang R, et al. The low-rank simplicity bias in deep networks[J]. arXiv:2103.10427, 2021.\n\n3  I am not sure how the insights from this paper could help us train/design better DNNs."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722604893,
            "cdate": 1698722604893,
            "tmdate": 1699636377963,
            "mdate": 1699636377963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PiImTzyswY",
                "forum": "LbJqRGNYCf",
                "replyto": "ZYPLhkltry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank he reviewer for the insightful feedbacks!\n\n# Comparison with existing works regarding learning preference of DNN\n\nWe thank the reviewer for providing these related work and will include the following comparison in our next revision. \n\nBoth [cite1] and [cite2] focus on empirical study of the training dynamics of DNNs (not transformers) on real data vs random data, following the seminar paper \u201cUnderstanding deep learning requires rethinking generalization\u201d in ICLR\u201917. \n\n[cite1] shows that DNN will learn simple data points/patterns first, when training with real data, while for random data (either random input or random label), each data sample is equally difficult. It also studies the effects of different regularization on training curves on real/random data. [cite2] further shows that the concept of \u201csimple samples/patterns\u201d generalizes across multiple architectures and learning methods (e.g., SVM versus DNN). \n\nNote that both JoMA and [cite1][cite2] conclude that simple patterns are learned first, followed by learning of more complicated patterns. Both [cite1] and [cite2] treat DNNs as a black box and provide thorough empirical studies. They also do not analyze Transformers. In contrast, focusing on Transformer architecture that shows impressive performance across multiple domains, JoMA opens the architecture blackbox and gives more quantitative definition of patterns in terms of co-occurrence, and explains how the specific architecture of Transformer learns these patterns by implicitly learning a latent hierarchy. \n\nFurthermore, we could also relate our analysis to [cite1][cite2], by re-defining the somehow vague term \u201csimple/difficult patterns\u201d with more rigorous quantity: \u201csimple patterns\u201d can be co-occurred tokens in the lowest layer of hierarchy, while \u201cdifficult patterns\u201d may come across multiple layers of hierarchy. In this case, our JoMA framework naturally explains why simple patterns are learned first, followed by difficult patterns. It also explains why the \u201ceasiness of pattern\u201d translates from SVM to more complicated architectures: since co-occurred tokens in the lowest hierarchy layer can be easily learned by linear models. \n\n[cite3] focuses on empirical study of low (effective) rank bias of DNNs after training. It shows that low-rank bias happens in different optimizers and is insensitive to initializations. Compared to [cite1] and [cite2], [cite3] focuses less on the training dynamics but more on the property of the final trained model. In comparison, from the training dynamics point of view, JoMA not only provides a theoretical justification why such phenomena (i.e., emergence of low-rank structure) could happen during training, but also characterizes the change of ranks in a more refined way (i.e., high rank -> low rank -> high rank again) and discusses the underlying reason why the learning is done in this way (i.e., to learn salient feature first at lower layer, leaving non-salient features at the top layer of the hierarchy).   \n\n# How the insights from JoMA help with practical applications for real-world scenarios\nJoMA focuses on specific architectures and can provide meaningful suggestions on how the architectures can be changed. Here are a few examples:\n+ Theorem 1 suggests that certain designs of soft-attention activation function lead to the invariance and future attention designs may be inspired by that. \n+ The attention entropy (and stable rank) re-bouncing curves suggest that low-rank structure may not be present over the entire training procedure. This suggests that we may want to use high-rank weight matrices at the beginning and the end of the training era. \n+ Fig. 8 shows that the re-bouncing of attention entropy over time is highly correlated with low validation error of the model after training. This suggests that we could check whether a run is healthy by checking the attention curve.  \n+ From Sec. 5, we know that the learning of non-salient co-occurrence is slow, because the model is unsure whether they should be learned under the current hierarchy, or in the higher hierarchy. Knowing this, if we have prior knowledge on the structure of the feature hierarchy, we may develop novel algorithms to accelerate the training.\n\nReviewer **h5HX** also appreciates our work and thinks our understanding of hierarchical learning via self-attention can guide the design of follow-up transformer-based networks. \n\nNote that overall, the main focus of JoMA is to understand how multi-layer Transformer works and provide theoretical insights for the community, rather than improving practical applications for real-world scenarios."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201248849,
                "cdate": 1700201248849,
                "tmdate": 1700201248849,
                "mdate": 1700201248849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tApfMX3zPi",
                "forum": "LbJqRGNYCf",
                "replyto": "PiImTzyswY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Reviewer_cMvf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Reviewer_cMvf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. The revision can improve this paper. I will keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660305991,
                "cdate": 1700660305991,
                "tmdate": 1700660305991,
                "mdate": 1700660305991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ULCa8Zdr0v",
            "forum": "LbJqRGNYCf",
            "replyto": "LbJqRGNYCf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_h5HX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_h5HX"
            ],
            "content": {
                "summary": {
                    "value": "This paper named JoMA, explores to explain the training procedure of multi-layer transformers. JoMA delivers a conclusion that transformers learn salient tokens at low layers while learning less salient tokens at high layers when the nonlinear activations involve training. To build a mathematical framework to show the learning mechanism of multi-layer transformer, JoMA proposes joint dynamics of self-attention and MLP.\n\nCompared with former works that focus on shallow transformer networks, linear activation or local gradient steps, JoMA builds a unified mathematical framework to characterize the learning mechanism of multi-layer transformers. Analyzing deep transformer networks is more challenging."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "JoMA is able to analyze deep and sophisticated transformer networks.\n\nIt's interesting to use dynamics to unify the MLP, self-attention, and non-linear activation.\n\nThe derived conclusion of how self-attention learns hierarchical data distribution is meaningful and can guide the design of follow-up transformer-based network."
                },
                "weaknesses": {
                    "value": "It would be better to discuss the limitation of JoMA.\n\nIt's not clear why chose OPT and Pythis for verification. Can the findings in this work coincide with BERT-style models?"
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4127/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4127/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4127/Reviewer_h5HX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735394346,
            "cdate": 1698735394346,
            "tmdate": 1699636377872,
            "mdate": 1699636377872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1E2RFtSCXW",
                "forum": "LbJqRGNYCf",
                "replyto": "ULCa8Zdr0v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful feedbacks!\n\n# Can the findings in this work coincide with BERT-style models?\nWe choose OPT and Pythia because they provide public intermediate checkpoints so that we can check the training dynamics. \n\nTheoretically our analysis can be extended to BERT like models (or encoder-decoder models). Empirically, We have done experiments on BERT and the conclusion is similar. The attention entropy, as well as the stable rank, has the bounce-back behavior over time (see Appendix B.2 in our revised paper).  \n\n# Limitation of JoMA\nJoMA has the following limitations:\n+ We assume the back-propagated gradients are stationary (or slowly changing over time). \n+ We assume the embedding vectors are orthogonal and fixed during training. \n+ We provide qualitative analysis for hierarchical latent generative models, but not quantitative. \n+ We also do not analyze how the model size affects the learning procedure. \n\nNote that these limitations are necessary assumptions to make the main message of this paper concise. Some substantial amount of work is needed to remove these assumptions, which is beyond the scope of this work. We will address them in our future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201079042,
                "cdate": 1700201079042,
                "tmdate": 1700201079042,
                "mdate": 1700201079042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wv0OCINRZA",
                "forum": "LbJqRGNYCf",
                "replyto": "1E2RFtSCXW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Reviewer_h5HX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Reviewer_h5HX"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. \n\nThe authors have addressed my concerns and I will maintain my positive score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441524864,
                "cdate": 1700441524864,
                "tmdate": 1700441524864,
                "mdate": 1700441524864,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KBO7SLYRwy",
            "forum": "LbJqRGNYCf",
            "replyto": "LbJqRGNYCf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_P4nM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4127/Reviewer_P4nM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a mathematical framework called JoMA to understand the training dynamics of multilayer Transformers. JoMA integrates out the self-attention layer and produces a modified dynamics of MLP layers only. JoMA reveals the training dynamics under linear or non-linear attention, as well as how the attention sparsity change over time, and how they depend on the input distribution.. The paper also verifies its theoretical findings with experiments on real-world data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "By integrating out the self-attention layer and analyzing only the modified dynamics of MLP layers, JoMA provides a fresh perspective on how these models learn and adapt during training. The paper demonstrates rigorous theoretical analysis on training dynamics, especially on how attention evolves over time. An interesting observation from the paper is that MLP and attention shares the same dynamics, which leads to several intriguing conclusions, for example, the rank of lower mlp layers first goes down and then bounces up. The mathematical derivations are presented clearly, making it accessible to researchers with different backgrounds."
                },
                "weaknesses": {
                    "value": "1. One assumption in the paper is that the embedding vectors of all tokens are orthogonal to each other. Does this need to hold for token embedding in every layer? Have the authors verified this assumption is valid in real cases? How much does this assumption affect the following conclusions?"
                },
                "questions": {
                    "value": "1. This paper analyze the training dynamics of multi-layer transformer and reveals how the attention is learned through time (first learning attention on tokens with most co-occurrence, and then expanding attention on other tokens). One question is, how is this dynamics dependent on model size? As we see from empirical results in the literature, larger model tends to learn attention that involves more complicated reasoning instead of simple co-occurrence. Can this factor be reflected by the analysis in the paper? How does the training dynamics (and entropy of attention) change when model size changes?\n\n2. Is the proposed JoMA also applicable to vision transformers? Since the input to vision transformer is different from text transformer in that the tokens are continuous instead of discrete and several assumptions in the paper may not hold. For example, the paper assumes tokens are orthogonal to each other, but for vision input lots of input tokens are very similar."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792455809,
            "cdate": 1698792455809,
            "tmdate": 1699636377789,
            "mdate": 1699636377789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EUNh5zN5zd",
                "forum": "LbJqRGNYCf",
                "replyto": "KBO7SLYRwy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the insightful feedback!\n\n# Orthogonality assumption of embedding vectors\nIn order to reach concise conclusions, we made this assumption. The assumption needs to be true for all embedding layers (i.e., the upper out-projection matrix of MLP at every layer), otherwise there will be many additional terms in our analysis, and we won\u2019t be able to give a clear overall picture of the training. \n\nIn practice, we show that the embedding vectors are almost orthogonal to each other in many models. For Pythia models of different sizes (from 70M to 6.9B), the averaged absolute cosine similarity over all embedding vector pairs is bounded by 0.1 throughout the training, much smaller than the maximal value 1. For other pre-trained models like BERT, OPT-6.7B and ViT-Huge, it is also small (<0.05), for LLaMA-2-7B, it is <0.17. Please check Appendix B.1 for these additional experiments in our revised submission. \n\nTherefore, our analysis largely holds, but may contain many small terms. A detailed analysis with almost orthogonal vectors will be left for future work.\n\nNote that for VIT, we measure the embedding layer (i.e., the upper out-projection matrix of MLP at every layer), but not the first input layer, which may have very strong correlations between nearby embeddings. \n\n# How larger models tend to learn attention that involves more complicated reasoning instead of simple co-occurrence\nFor this, we break the questions into two parts:\n+ How do Transformers learn more complicated reasoning than simple co-occurrence?\n+ How do large models help?\n\n[**Learning more complicated reasoning**] In this work, we indeed focus on feature composition based on co-occurrence, which is a simple form (maybe the simplest form) of feature hierarchy. For reasoning, an intuition is that the model still captures co-occurring patterns. The difference is that thanks to embedding representation, these patterns are combinations of certain learnable \u201cattributes\u201d of input data, rather than input data themselves. \n\nFor example, from the dataset \u201ca x 3 = a a a\u201d, \u201cb x 3 = b b b\u201d, the model will predict \u201cc c c\u201d from \u201cc x 3 =\u201d. Why can this be learned? From our point of view, there could be two critical components that contribute to this process:\n+ The embedding of \u201ca\u201d, $u_a$, can contain multiple aspects of the letter \u201ca\u201d, e.g., $u_a = u_{[alphabet]} + u_{[first]}$, which means that a is \u201cthe first letter in the alphabet\u201d. In this case, The pattern \u201c[alphabet] x 3\u201d is a common co-occurred pattern that can be extracted from \u201ca x 3\u201d and \u201cb x 3\u201d.  \n+ As JoMA suggests, some neuron in the MLP hidden layer may represent the pattern p = \u201c[alphabet] x 3\u201d, and use an embedding to represent it for the next layer. Due to the residual connection, the resulting representation of \u201ca x 3\u201d can be $u_{p} + u_{[alphabet]} + u_{[first]}$. \n+ The co-occurred pattern $u_p + u_{[first]}$ will have a large inner product with the embedding \u201ca\u201d in the decoder layer, which also has a $u_{[first]}$ component. As a result, the letter \u201ca\u201d is decoded with high probability. Note that this procedure can generalize to any letter (e.g., c or d), as long as their embedding takes the form of $u_{[alphabet]} + \u2026$.\n\nIn summary, for reasoning tasks:\n+ Learning co-occurred patterns of the attributes that summarize different cases, by leveraging the power of embeddings. \n+ The matched pattern is also represented as embedding, superimposed to the original embeddings, thanks to residual connections. \nOverall, this gives very efficient representations and avoids enumerating all possible instantiation of patterns. Note that this is beyond JoMA's scope and we will leave a more mathematically rigorous study as the future work.\n\n[**How do large models help?**] Large model helps in the following two ways:\n+ Larger models typically have more hidden neurons in the MLP layers. According to lottery ticket hypothesis[1], it is more likely that there exists at least one hidden neuron that happens to initialize with a weight vector and an embedding that is useful for modeling a compositional concept. Such a good initialization will facilitate training. \n+ Larger models typically have larger feature dimension d, as suggested in Sec. 7 (Johnson\u2013Lindenstrauss lemma), substantially more embedding vectors can be placed into a space with larger d, while maintaining almost orthogonality. \n\nNote that JoMA is just a starting point that captures the simplest mechanism of feature compositions. To make our analysis more straightforward and easier to follow, we also assume all embeddings are fixed/orthogonal during training. A thorough and more rigorous theoretical study of how Transformer performs reasoning, and how the embedding vectors are learned during training, is left for future work."
                    },
                    "title": {
                        "value": "Rebuttal"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200716595,
                "cdate": 1700200716595,
                "tmdate": 1700200939095,
                "mdate": 1700200939095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B2S6TLgTB7",
                "forum": "LbJqRGNYCf",
                "replyto": "EUNh5zN5zd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4127/Reviewer_P4nM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4127/Reviewer_P4nM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I appreciate the detailed response from the authors. I will remain my positive score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550704582,
                "cdate": 1700550704582,
                "tmdate": 1700550704582,
                "mdate": 1700550704582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]