[
    {
        "title": "BiXT: Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers"
    },
    {
        "review": {
            "id": "OGCjbbrCnq",
            "forum": "xvhjRjoFCN",
            "replyto": "xvhjRjoFCN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7336/Reviewer_bDbJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7336/Reviewer_bDbJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Bi-Directional cross-attention to model the interactions of the visual tokens. Experiments on ImageNet1K and ShapeNetPart are performed to evaluate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. Cross-attention has been widely used for years and the proposed method is simply some combination of cross-attention operation.\n2. The experimental results are not very impressive. The accuracy on ImageNet is only 82.0, which is not competitive."
                },
                "questions": {
                    "value": "see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7336/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7336/Reviewer_bDbJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698567200893,
            "cdate": 1698567200893,
            "tmdate": 1699636877564,
            "mdate": 1699636877564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HaphiKXuLI",
                "forum": "xvhjRjoFCN",
                "replyto": "OGCjbbrCnq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bDbJ"
                    },
                    "comment": {
                        "value": "We thank you for your time put into reviewing our work. However, we have to respectfully disagree with the two stated weaknesses:  \n\n**[W1]: Limited novelty, Cross-Attention widely used**\n> *The novelty is limited. Cross-attention has been widely used for years and the proposed method is simply some combination of cross-attention operation.*\n\nWe'd like to clarify that we do not claim to introduce cross-attention, nor do we simply combine CAs. While we do analyze a na\u00efve version of sequential CA in our paper as comparison, one of the core contributions of our work is the introduction of a *novel efficient bi-directional cross-attention method* which is built upon the *observation of symmetric tendencies* that emerge in the attention patterns. This combined with our BiXT architecture scales linearly with the input sequence length, is generally applicable to a variety of input modalities, and additionally saves compute, memory and parameters. \n\n---\n**[W2]: Experimental results / not beating SOTA on ImageNet**. \n> *The experimental results are not very impressive. The accuracy on ImageNet is only 82.0, which is not competitive.*\n\nNote that we achieve the 82% with our **general** architecture that is **applicable across different input modalities** (not only images) and scales linearly w.r.t. the input sequence length. Our results therefore need to be *interpreted in the context of other such methods* like the recent Perceiver~IO (2022) models, which however perform either worse and/or require significantly more compute as we demonstrate in Table 2:  \n| Method | Top-1 Acc | FLOPs | \n|:----|:----:|:-----:|\nPerceiver-v2 | 77.4% - 78.6% | 367-404G\nPerceiver-IO |  79.0% - 82.1% | 369-407G\nBiXT (ours) |  79.1% - 82.0% | **1.7- 48G**\n\n---\nWe hope this sheds some new light on your interpretation of our work. Please do let us know if we can provide any additional details or clarify potentially misleading points, and we encourage you to further elaborate in context of this new information."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272313485,
                "cdate": 1700272313485,
                "tmdate": 1700272351371,
                "mdate": 1700272351371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QKkYfXYo4t",
                "forum": "xvhjRjoFCN",
                "replyto": "OGCjbbrCnq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer bDbJ,\n\nWe hope this message finds you well. As the deadline for the discussion phase is approaching, we wanted to check if you have any remaining questions that we could help clarify.\n\nWarm regards,  \nThe Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703165921,
                "cdate": 1700703165921,
                "tmdate": 1700703165921,
                "mdate": 1700703165921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nucesrF9Ll",
            "forum": "xvhjRjoFCN",
            "replyto": "xvhjRjoFCN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7336/Reviewer_qx3u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7336/Reviewer_qx3u"
            ],
            "content": {
                "summary": {
                    "value": "This research paper provides an enhancement to the Perceiver architecture that employs latent queries for the distillation of input tokens. The main novelty introduced is a bidirectional cross-attention module aimed at reducing computational demands. The authors analyze an architecture that iteratively stacks query-to-token and token-to-query cross-attention modules and find symmetry between these two attention values, suggesting that these two iteratively stacked modules can be merged into one. Therefore, a new bidirectional transformer architecture that only scales linearly with the input tokens as a means of dealing with general modal input data. This replacement results in a reduction of computational cost by approximately one-third, compared to iterative stacking cross-attentions, while also achieving higher accuracies.\n\nThe improved method demonstrates an impressive 82.0% accuracy for classification tasks on ImageNet-1K using compact models. These models require only a small fraction of the FLOPS compared to the original Perceiver. The paper also provides verification tests on more generalized input modalities, reinforcing the versatility and effectiveness of the proposed enhancements to the Perceiver architecture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of introducing bidirectional attention to replace the iterative stacking cross attention is both innovative and simple. \n2. The way that the authors present their idea is also appreciated. An analog between the latent queries and \"what\" queries, and that between the input tokens and the \"where\" information, is first presented. Then, the symmetry between the \"what\" and \"where\" tokens are exemplified to suggest the improvement of the bidirectional attention. Overall, I enjoy reading this paper, and it is easy to follow.\n3. The bi-directional cross-attention is effective in reducing the computational cost of the Perceiver architecture and increasing its performance. It achieves the same performance with only a fraction of FLOPS."
                },
                "weaknesses": {
                    "value": "- Despite its effectiveness, the motivation is more from an intuitive analogy of \"what\" and \"where\" tokens than a comprehensive theoretical or experimental conclusion. Only an image classification task is presented when analyzing the symmetry between iterative cross attentions between \"what\" and \"where\" tokens. There could also exist many others scenarios, where these cross attention value may violate the symmetry property. For example, the \u201cwhat\u201d tokens would attend to the context background tokens when detecting small object in the image, whereas these attended \u201cwhere\u201d tokens would more likely to attend to other \u201cwhat\u201d tokens in the next cross attention. Therefore, it is less convincing to conclude the \u201csymmetry\u201d behavior of the \u201cwhat\u201d and \u201cwhere\u201d tokens from a single illustration.\n    \n\n- It is surprising and strange in Table 1(a) that the most performance gain is brought by the naive approach that sequentially stacking two cross attentions with reverse orders by exchanging the query and key positions (+ 11 acc); whereas bidirectional only brings in an additional 0.8 acc. This result seems a bit contradictory with the emphasis of the paper on the bi-directional attention. In this regard, a more important part about the reason of the largely improved performance of the sequential cross attention deserves more detailed analysis. Specifically, this phenomenon would highlight the importance of refining the image tokens instead of fixing them as in Perceiver.\n    \n- The FLOPSs and Params reported in Table 1(a) is confusing. The FLOPSs and Params of bi-directional cross attention are even larger than that of sequential cross attention. However, \u00a0it is described that the implementation of bidirectional cross attention saves 1/3 parameters compared to naive sequential cross attention. Results in Table 1(a) contradicts this statement."
                },
                "questions": {
                    "value": "See the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672820536,
            "cdate": 1698672820536,
            "tmdate": 1699636877405,
            "mdate": 1699636877405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5BSW1chr0T",
                "forum": "xvhjRjoFCN",
                "replyto": "nucesrF9Ll",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qx3u (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive review and the constructive comments. We address each of them individually in the following.\n\n**[W1]: Symmetry constraint**\n> *Despite its effectiveness, the motivation is more from an intuitive analogy of \"what\" and \"where\" tokens than a comprehensive theoretical or experimental conclusion. Only an image classification task is presented when analyzing the symmetry between iterative cross attentions between \"what\" and \"where\" tokens. There could also exist many others scenarios, where these cross attention value may violate the symmetry property. For example, the \u201cwhat\u201d tokens would attend to the context background tokens when detecting small object in the image, whereas these attended \u201cwhere\u201d tokens would more likely to attend to other \u201cwhat\u201d tokens in the next cross attention. Therefore, it is less convincing to conclude the \u201csymmetry\u201d behavior of the \u201cwhat\u201d and \u201cwhere\u201d tokens from a single illustration.*\n\nWe agree with you that there might be cases where complete flexibility could lead to attention maps that differ from a symmetric structure. However, having frequently observed the symmetric tendencies in the sequential cross-attention maps has motivated us to try to leverage this phenomenon to reduce FLOPS, memory and parameters. While this indeed imposes a constraint, we found that the performance was almost unaffected throughout our experiments. When we re-spent the saved compute/memory on additional layers, we were able to consistently obtain a net benefit.\n\nWe suspect that the slightly deeper architectures provide our method with enough flexibility to learn any additionally required operations to compensate and realize a more complex information exchange in cases where this might be required (potentially across multiple layers, and/or by using a subset of the latent vectors to facilitate this). \nThe exact internal behavior that such shared attention maps yield in terms of information exchange and its structure is a very interesting problem that we consider a promising area for future work.\n\n$\\rightarrow$ To provide some further insights for the community, we have added additional visualizations to the appendix of our paper showing the attention maps of all latent vectors (complete set of 64) for the last two layers, as well as the maps of the four latent vectors presented in the paper across all architectural layers (Figures A1-A3, Section D).   \n$\\rightarrow$ We will also make our code publicly available including our trained models to facilitate future investigations. \n\n*(please also see next response - part 2)*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270951990,
                "cdate": 1700270951990,
                "tmdate": 1700272391098,
                "mdate": 1700272391098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JwRX9QUa2z",
                "forum": "xvhjRjoFCN",
                "replyto": "nucesrF9Ll",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qx3u (2/2)"
                    },
                    "comment": {
                        "value": "**[W2]: Uni-directional & Sequential vs. Bi-directional Attention**  \n> *It is surprising and strange in Table 1(a) that the most performance gain is brought by the naive approach that sequentially stacking two cross attentions with reverse orders by exchanging the query and key positions (+ 11 acc); whereas bidirectional only brings in an additional 0.8 acc. This result seems a bit contradictory with the emphasis of the paper on the bi-directional attention. In this regard, a more important part about the reason of the largely improved performance of the sequential cross attention deserves more detailed analysis. Specifically, this phenomenon would highlight the importance of refining the image tokens instead of fixing them as in Perceiver.*\n\nWe apologize if this aspect of our paper has been slightly unclear. The main intention of Section 3.2 on \"Iterative, sequential or bi-directional\" attention is to demonstrate that extending the working memory over Perceiver-like architectures by using both-ways cross-attention can already help to unlock the information bottleneck and significantly improve the results. Using our bi-directional cross-attention built on the observation of emerging symmetric tendencies in the attention maps further boosts the results by improving both efficiency and robustness (w.r.t. hyperparameter changes and across different initializations).  \nIn other words, saving FLOPs (as well as parameters and memory) via the use of our bi-directional CA means they can be spent on additional layers, further improving results and therefore creating a net benefit in terms of accuracy-to-FLOPs (as well as parameters and memory). \n\nTo validate that such behaviour cannot easily be obtained in a Perceiver-style uni-directional manner, we have created and investigated 10 different architectural variants of which the best 5 are presented in Table 1(a) (with the full details reported in the appendix), and contrasted these to a sequential and bi-directional variant of similar FLOP count.  \n\nWe have now conducted additional experiments for two more architectures to further validate this point: bi-dir with 13 layers and seq-attn with 12 layers (3 seeds each). The results reflect the same trend as the architectures in Table 1 (a), with the bi-directional attention yielding a relative improvement of $\\sim 1.22$%  over its sequential counterpart while exhibiting a lower standard deviation across the three randomly seeded training runs:\n| Method | Top-1 Acc | Top-5 Acc | FLOPs | Mem. | #Param |\n|:----|:----:|:-----:|----:|------:|----:|\n| Seq.   (d12) | 72.72\u00b10.76 | 90.95\u00b10.44 | 1.81G | 8.19M | 15.92M |\n| Bi-Dir (d13) | 73.61\u00b10.34 | 91.42\u00b10.19 | 1.82G | 7.89M | 16.38M |\n\n$\\rightarrow$ We have included these results into the more detailed overview presented in the appendix (Table~A1). \nIf the reviewers consider it helpful to further strengthen our paper, we are happy to include additional results over other variants (e.g. 10, 8 layers) into the final version of our manuscript to further validate this aspect. \n\n---\n**[W3]: Confusing presentation in Table 1(a)**\n> *The FLOPSs and Params reported in Table 1(a) is confusing. The FLOPSs and Params of bi-directional cross attention are even larger than that of sequential cross attention. However, it is described that the implementation of bidirectional cross attention saves 1/3 parameters compared to naive sequential cross attention. Results in Table 1(a) contradicts this statement.*\n\nThe presentation in Table 1(a) has indeed been lacking some clarity. We report architectures that have been matched to a similar FLOP count: between 1.58 - 1.99 for uni-directional attention, and we compare these to the two most similar variants of sequential and bi-directional attention in terms of FLOP count. However, due to the increased efficiency of bi-directional CA, the reported bi-directional model consists of *12 layers*, whereas the sequential model *only has 11 layers*. \n\n$\\rightarrow$ We have adapted the methods' names in Table 1(a) and explicitly added the model depth to improve clarity, as well as expanded the details and explanations in the appendix (Section A.3).\n\n---\nWe hope our answers and additional insights have helped to address your questions. Please do let us know if there is any further concern or whether we can provide any further information that is helpful to you."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271396210,
                "cdate": 1700271396210,
                "tmdate": 1700272377181,
                "mdate": 1700272377181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PuwBRsIJ3o",
                "forum": "xvhjRjoFCN",
                "replyto": "JwRX9QUa2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Reviewer_qx3u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Reviewer_qx3u"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for addressing my concerns"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for clarifying the novelty of this study and the originally missing details in the manuscript. I find my concerns mostly addressed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643531508,
                "cdate": 1700643531508,
                "tmdate": 1700643531508,
                "mdate": 1700643531508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sx6gfpIAds",
            "forum": "xvhjRjoFCN",
            "replyto": "xvhjRjoFCN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a bi-directional cross-attention Transformer (BiXT) that can process long sequences efficiently and effectively by using a small set of latent vectors to represent the \u2018what\u2019 and input tokens to represent the \u2018where\u2019 of the data. At the core of BiXT is the bi-directional cross-attention module that simultaneously refines latent vectors and input tokens. Compared to sequential cross-attention, the bi-directional cross-attention module leverages the symmetry of attention patterns between latent vectors and input tokens to reduce computational cost and memory consumption.  The authors evaluate BiXT on image classification, semantic image segmentation, and point cloud part segmentation.  They show that BiXT outperforms comparable methods in the low-FLOP regime and can easily integrate modality-specific components to improve performance further."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed bi-directional cross-attention has a simple and neat design\n2. Evaluations are conducted on two modalities, i.e., images and point clouds.\n3. The paper is well-written, hence easy to follow"
                },
                "weaknesses": {
                    "value": "1. Despite the simple and neat design, the strength of the proposed method, bi-directional cross-attention, is unclear. Compared to using two uni-directional cross-attention modules sequentially, the system-level accuracy, FLOPs, and memory requirements are all similar (Table 1 on page 6). \n\n2. Insufficient comparison with some of the latest vision backbones. The methods in image classification (Table 2) and semantic segmentation (Table 3) are somewhat outdated. Many works were proposed to overcome the quadratic complexity of multi-head self-attention, such as MaxViT [1], BiFormer [2], and especially DualViT [3], which has a very similar design to BiXT. The performances of BiXT are not attractive if these approaches are included in comparison. Why are these methods not comparable with BiXT?\n\n3. Lack of experiments with larger models. It is unclear why the comparisons are positioned in a low-FLOP regime (Table 2). BiXT seems not to be specially designed for lightweight models, and the budgets of BiXT-Ti/8 and BiXT-Ti/4 in the final section of Table 2 are sufficient to cover training larger models with more parameters. It may be better to demonstrate the effect of model scaling.\n\n[1] Maxvit: Multi-axis vision transformer, ECCV 2022.   \n[2] BiFormer: Vision Transformer with Bi-Level Routing Attention, CVPR 2023.    \n[3] Dual Vision Transformer, TPAMI 2023."
                },
                "questions": {
                    "value": "See the weaknesses part. Overall, I appreciate the simple and neat design of the bi-directional cross-attention. Still, I would like more clarification on its strengths and the experimental settings in the rebuttal. I will raise my rating if these concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx",
                        "ICLR.cc/2024/Conference/Submission7336/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827869954,
            "cdate": 1698827869954,
            "tmdate": 1700634642019,
            "mdate": 1700634642019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "214HhMgLUn",
                "forum": "xvhjRjoFCN",
                "replyto": "sx6gfpIAds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mPnx (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your detailed review and the constructive feedback! We address your comments individually in the following:\n\n**[W1] Sequential vs. bi-directional attention**:\n> *Despite the simple and neat design, the strength of the proposed method, bi-directional cross-attention, is unclear. Compared to using two uni-directional cross-attention modules sequentially, the system-level accuracy, FLOPs, and memory requirements are all similar (Table 1 on page 6).*\n\nThis aspect has indeed been lacking some clarity. The architectures reported in Table 1 (a) have been matched to similar FLOP counts: between 1.58 - 1.99 for uni-directional attention, and we contrast them to the two variants of sequential and bi-directional attention that fall into this range (hence the similar FLOPs and memory). The bi-directional architecture using our more efficient attention mechanism has *12 layers* whereas the sequential one has *only 11 layers*.\n\nOur bi-directional CA only requires 4 instead of 6 projection matrices (2x[R,V] vs. 2x[Q,K,V]) and only computes the attention matrix once (instead of twice). The hereby saved FLOPs (as well as parameters and memory) can then be spent on additional layers, further improving results by another $\\sim 1.2$% in this configuration.   \nIn other words, by holding FLOP and/or memory requirements constant, we consistently observed a net benefit with our bi-directional attention in terms of accuracy throughout our experiments. We empirically found that it additionally improved robustness across different parameter initializations (seeds).\n\nTo further validate this point, we have conducted additional experiments for two more architectural variants: bi-dir with 13 layers and seq-attn with 12 layers (3 seeds each). These results show the same trend as the two architectures already included in Table 1 (a), with the bi-directional attention outperforming its sequential counterpart while exhibiting a lower standard deviation across the three randomly seeded training runs: \n| Method | Top-1 Acc | Top-5 Acc | FLOPs | Mem. | #Param |\n|:----|:----:|:-----:|----:|------:|----:|\n| Seq.   (d12) | 72.72\u00b10.76 | 90.95\u00b10.44 | 1.81G | 8.19M | 15.92M |\n| Bi-Dir (d13) | 73.61\u00b10.34 | 91.42\u00b10.19 | 1.82G | 7.89M | 16.38M |\n\n\n$\\rightarrow$ We have included the results and additional discussion into the detailed overview presented in the appendix of our revised version (Section A.3).  \n$\\rightarrow$ We have additionally updated the methods' names in Table 1(a) to clearly represent the difference in model depth (and explain the similar FLOP and memory counts).\n\n---\n\n**[W2-1]: Comparison to recent vision-specific works -- Rel. works**\n> *Insufficient comparison with some of the latest vision backbones. The methods in image classification (Table 2) and semantic segmentation (Table 3) are somewhat outdated. Many works were proposed to overcome the quadratic complexity of multi-head self-attention, such as MaxViT [1], BiFormer [2], and especially DualViT [3], which has a very similar design to BiXT.*  \n\nWe thank you for drawing our attention to these very interesting works, especially [2] and [3] which we will add to our manuscript ([1] is already included in related work). \n\nBrief discussion of differences to [2] & [3]:  \nDualViT's [3] *dual block* used in the early layers of their architecture does indeed show similarity to the na\\\"ive solution of sequential cross-attention, but is distinctly different from our bi-directional approach as it does not leverage any symmetry. Importantly, their multi-stage pyramidal vision-only architecture uses a large number of `merging blocks/layers' (between 9 - 24) which cast full self-attention over the concatenated sequence of latents and tokens. This prevents linear scaling and also introduces a shared embedding space of latent vectors and tokens through the use of the same key-query-value projection matrices -- whereas our architecture keeps those separate (aligned with the presented 'what' and 'where' analogy and the level of information they represent) and scales linearly with respect to the input length.\n\nBiformer [2] similarly is a pyramidal vision-only approach that reduces the computational complexity through routing information between selected tokens via a directed graph, thus achieving sparsity to skip computation of certain regions that are deemed irrelevant. While this is a very neat way of dynamically reducing complexity, it is distinctly different from our approach and does not achieve true linear scaling. \n\n$\\rightarrow$ We will include both works into the related works section of our revised paper, with additional more in-depth discussion in the appendix. (please also see the following response - part2)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700269018497,
                "cdate": 1700269018497,
                "tmdate": 1700272438888,
                "mdate": 1700272438888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6GSjvZojGY",
                "forum": "xvhjRjoFCN",
                "replyto": "sx6gfpIAds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mPnx (2/2)"
                    },
                    "comment": {
                        "value": "**[W2-2]: Comparison to recent vision-specific works -- Performance**\n> *The performances of BiXT are not attractive if these approaches are included in comparison. Why are these methods not comparable with BiXT?*\n\nWe have included the recent BiFormer-T [2] with its 2.2 GFLOPs and 81.4% in our revised manuscript as it does indeed fall into (or close to) our intended FLOP regime. In the comparison of low-FLOP Transformer methods in Table 2, we had decided on a cut-off at 2.0 GFLOPS. The only two included methods slightly above this, i.e. XCiT-T24 with 2.3 GFLOPs and PVTv2-B1 with 2.1 GFLOPs, had been included due to them being improvements/modifications of previous versions of these same architectures (XCiT-T12 and PVTv1).   [1]'s and [3]'s smallest models require however more than double and do thus not fall into this range, neither does any of these for semantic segmentation.\n\nHowever, we would like to clarify that our intent of this comparison in Table 2 is to demonstrate that the core of our method is mostly orthogonal to domain-specific approaches and that many of their techniques can be used to easily extend our architecture to further improve results while trading off generality. The important comparison is hence *'general BiXT'* vs. *'BiXT + LPI'* (and potentially vs. XCiT), where LPI was simply chosen to provide *one example* of such a possible extension (of which there are many, including pyramidal structures). We see a more dense exploration of such domain-specific variants and interactions as an interesting future research direction.\n\n$\\rightarrow$ We have updated Table 2 to represent [2] in our revised version and will modify the wording of the describing paragraph accordingly to better outline our intent. \n\n---\n**[W3]: Larger models & reasons for conducted experiments**\n> *Lack of experiments with larger models. It is unclear why the comparisons are positioned in a low-FLOP regime (Table 2). BiXT seems not to be specially designed for lightweight models, and the budgets of BiXT-Ti/8 and BiXT-Ti/4 in the final section of Table 2 are sufficient to cover training larger models with more parameters. It may be better to demonstrate the effect of model scaling.*\n\nThere are two reasons for our focus on a low-FLOP regime: \n 1) The limited computational budget available to us and many other researchers, which only allows us to run a few larger experiments, and \n 2) The aspect that our bi-directional cross-attention is focused on scaling linearly as well as requiring fewer FLOPs, memory and parameters than a na\u00efve sequential realization.  \n\nSince one of the core attributes of BiXT is its linear scaling w.r.t. the input sequence length, we chose to spend our available budget to showcase the effect & benefit which BiXT's ability to process longer sequences yields. Note that since the investigated architecture stays the same and only more tokens are passed as input, we were able to directly use the exact same training hyperparameters as we used in the other experiments. \n\nWhile it would indeed be interesting to additionally analyze large models, we'd like to note that this would require a substantial number of additional large experiments. Even though such models might at first appear to require similar compute, the actually required computational budget not only encompasses the training runs but also the hyperparameter search.  \nThe importance of well-chosen hyperparameters and augmentation strategies grows significantly with model size, as can be seen in the literature (e.g. in the transition from ViT$\\rightarrow$DeiT$\\rightarrow$DeiT3). This makes an appropriate exploration of this vast search space essential but computationally very expensive, and we (have to) leave this as an opportunity for future work.\n\n---\nWe hope that our provided answers are able to address all your questions. Please do let us know if there are any remaining concerns, and we will do our best to promptly address these."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270397620,
                "cdate": 1700270397620,
                "tmdate": 1700272415561,
                "mdate": 1700272415561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ndlOW5zupK",
                "forum": "xvhjRjoFCN",
                "replyto": "6GSjvZojGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx"
                ],
                "content": {
                    "comment": {
                        "value": "My primary concerns are resolved. Although the BiXT currently does not achieve a compelling performance, it brings new insights for transformers, e.g., disentangling \"where\" and \"what\" in the internal representations. It is also understandable that academic researchers have a limited budget. I agree that prioritizing the budget to show the capability for coping with long sequences instead of building larger models is a reasonable choice.\n\nTherefore, I would like to raise my rating from 5 (marginally below the acceptance threshold) to 6 (marginally above the acceptance threshold)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634595305,
                "cdate": 1700634595305,
                "tmdate": 1700634595305,
                "mdate": 1700634595305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]