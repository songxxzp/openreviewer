[
    {
        "title": "Federated Binary Matrix Factorization using Proximal Optimization"
    },
    {
        "review": {
            "id": "J7kg2Wl7EZ",
            "forum": "IpJIq3iwMH",
            "replyto": "IpJIq3iwMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8420/Reviewer_d72F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8420/Reviewer_d72F"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the Boolean matrix factorization problem, where you are given a Boolean matrix $A$, and you need to find two low-rank matrices $U$ and $V$ such that $A\\approx U\\circ V$, where the operations are over the Boolean semiring. This problem has many practical applications. However, it is NP-hard to solve it exactly. This paper considers a heuristic approach based on the proximal gradient descent. The main contribution of this paper is to propose a federated proximal-gradient-descent algorithm for BMF (FELB) and its adaptive version (FALB). In theory, it proves the convergence of these algorithms and shows some differential privacy guarantees. In experiments, it tests the new algorithms in several different datasets and demonstrates advantages in accuracy and efficiency over prior approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The algorithm is the first of its kind in the field of federated BMF. It establishes a new benchmark for future studies in this area. Additionally, the algorithm works well empirically, which could be useful in many real-world applications. The experimental results are comprehensive and clearly stated in this paper."
                },
                "weaknesses": {
                    "value": "The technical novelty of this paper is limited. The idea of using proximal gradient descent has been previously used by Dalleiger and Vreeken (2022). This paper merely transforms the previous algorithm into a federated version. The non-trivial part is the aggregation procedure. However, it is a heuristic and also increases the time complexity for the server. The paper does not theoretically show the convergence rate of the algorithm. Furthermore, the analysis relies on a strong assumption of the loss function. The writing of this paper could be improved. For example, more intuition for the proximal operator should be provided. The current manuscript is not self-contained, and it could be difficult for people outside of this field to understand."
                },
                "questions": {
                    "value": "* Eq. (2): why $A_{ij}$ or $[U\\circ V]_{ij}$? It seems to be xor. \n\n* After Eq. (3): the definition of $R$ is confusing. It maps $\\mathbb{R}^{n\u2019\\times m\u2019}$ to $\\mathbb{R}$. However, Eq. (3) applies it to $U$ and $V$, which are matrices with different dimensions.\n\n* Eq. (4): it is unclear what it means for $x\\in X$. Is $X$ a set of points, and $x$ is a vector? Or is $x$ a number? And what is $x-1$?\n\n* Page 4: \u201cWithout the knowledge of $U_i$, the server cannot estimate specific attributes of individual users (assuming sufficiently large client datasets).\u201d Can the sever learn $U$ from $A$ and $\\hat{V}$? Is it proved in this paper to rule out such an attack? If so, a reference should be added here.\n\n* Proposition 1: the symbol R should be replaced by some other symbol, as it has been used to denote the regularizer.\n\n* Page 5, line 4: is it a typo for $\\nabla_U\\nabla_U$?\n\n* Proposition 1: even if the gradient tends to zero, the loss function may not tend to zero without further assumption on the loss function. Is the loss function considered here convex? And how do we justify the assumption on the bounded gradient?\n\n* Section 4.2: the differential privacy claims should have a formal statement with proof.\n\n* Page 7: the index of the summation should be stated explicitly.\n\n* (Chen et al. 2022) considers the symmetric and sparse version of BMF and shows a combinatorial algorithm for the average-case instances. It is interesting to see whether the method in this paper can be generalized to this regime.\n\nChen, Sitan, et al. \"Symmetric Sparse Boolean Matrix Factorization and Applications.\" 13th Innovations in Theoretical Computer Science Conference (ITCS 2022)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697520928757,
            "cdate": 1697520928757,
            "tmdate": 1699637048887,
            "mdate": 1699637048887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tFmNrgykF5",
                "forum": "IpJIq3iwMH",
                "replyto": "J7kg2Wl7EZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8420/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer d72F (part 1)"
                    },
                    "comment": {
                        "value": "Dear reviewer d72F,\n\nThank you for the kind words, for pointing-out the strength of our work, and for the constructive feedback and extensive list of interesting questions. We address the remaining open points in the following. \n\n**W1) Significant Contribution**\n\nWhile we draw upon the work of Dalleiger et al. in Boolean matrix factorization, our enhancements are substantial. Firstly, we introduce a novel regularization scheme that surpasses the effectiveness of Dalleiger et al.'s methodology. Secondly, our methodologies represent the inaugural and sole truly federated Boolean matrix factorization algorithm, bridging a crucial research gap and meeting practical demands. Thirdly, we furnish comprehensive guarantees on convergence.\n\n**W2) Aggregation complexity**\n\nWe acknowledge that our proximal aggregation increases the work required from our server. However, the proximal operator is not inherently costly to compute. In fact it can be implemented as a single element-wise matrix operation as a GPU kernel. Computational complexity speaking, it is therefore cheaper than matrix multiplication, and will not raise the computational complexity class neither of our server nor of our clients .\n\nAdditionally, aggregation does not occur after every update. Given that the proximal operator is applied to the aggregate, the associated increase in complexity remains independent of the number of clients. Consequently, our approach demonstrates scalability with the number of clients, as illustrated in Figures 3 and 4 of the manuscript.\n\n**W3) convergence rate**\n\nThe convergence rate trivially follows from our proof of Proposition 1, which will yield a rate of $\\mathcal{O}\\left(\\frac1{\\lambda_T}\\right)$. That is, setting $\\lambda_t = t$ yields a linear convergence rate of $\\mathcal{O}\\left(T^{-1}\\right)$. Note that, by choosing a large lambda (such as $t^2$) we can yield faster convergence, but in practice might lead to a reduction in the relative reconstruction loss.\n\nWe thank the reviewer for the interest in this aspect of our algorithm. We will clarify the implications in the final version of our manuscript.\n\n**W4) assumptions regarding loss**\n\nWe make use of the standard squared error as loss function and furthermore only make the standard assumption that gradients are bounded. Could you please point out which assumption you deem strong?\n\n**W5) writing, proximal operator intuition**\n\nTo improve our presentation and raise the accessibility of our work, we will include more intuitive descriptions in the main body of our paper, regarding\u2014for example\u2014the interplay of gradient descent, proximal operator, regularization, and aggregation.\n\n**W6) self-contained**\n\nTo make our paper self-contained, we will expand the details and a complete derivation of our proximal algorithms in the appendix.\n\n**Q1) Eq. (2)**\n\nWe deemed this to be more readable, but will revisit the notation.\n\n**Q2) After Eq. (3)**\n\n$m$ and $n$ are meant to be the parameter-dependent row- or column-counts of the input matrix of $R$. To address confusion, we will adapt our notation.\n\n**Q3) Eq. (4): $x$**\n\n$x$ is a single real-valued matrix cell from one of the factor matrices. For example $x = U_{ij}$. \n\n**Q4) Can the server learn I from A and V=?**\n\nOur point is that the server cannot trivially estimate specific attributes about users (i.e., learn $U$ from $A$ and $V$, since factorizations are not unique (nonnegative factorization are not unique, even in case of infinitesimal rigidity [1], and Boolean factorizations can be shown to be not unique by a simple application of the pigeonhole principle, see [2] for further details). We apologize for the imprecision. As stated in Sec. 4.2, we are well aware that a curious server could in principle obtain information about local data from V - one could imagine approaches similar to gradient leakage or membership inference attacks. This is the whole motivation for the differential privacy mechanism in Sec. 4.2 and our empirical evaluation of it in Sec. 5.1.3. \n\n\n**Q5) Proposition 1: the symbol R should be replaced by some other symbol, as it has been used to denote the regularizer.**\n\nIndeed, this is a mistake. We will replace it with another symbol. Thank you!\n\n**Q6) typo**\n\nGood catch!\n\n**Q7) Loss function and gradient**\n\nIt is correct that the loss function does not necessarily converge to $0$. Indeed, it is rather unlikely that it does, since it would imply a perfect factorization. Proposition 1 does not assume this. The assumption of a bounded gradient is ubiquitous in the optimization literature and holds, e.g., for Lipschitz continuous loss functions and data from a bounded domain."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564709728,
                "cdate": 1700564709728,
                "tmdate": 1700564709728,
                "mdate": 1700564709728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "litA83cg8d",
            "forum": "IpJIq3iwMH",
            "replyto": "IpJIq3iwMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8420/Reviewer_iqWp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8420/Reviewer_iqWp"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for federated binary matrix factorization.\nIt also considers differentially private variants of the proposed method, by adding noise to the local updates (offering local DP guarantees)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Presentation and clarity: The paper is overall well-motivated, the ideas are simple and easy to follow, and adequate background is given about binary matrix factorization, federated learning and differential privacy.\nThe method seems like a reasonable extension of existing methods to the federated setting."
                },
                "weaknesses": {
                    "value": "The contribution of the paper seems limited. This is a straightforward extension of the method of Dalleiger et al. to the federated setting. Each client applies the same method, then the $V_i$ updates are aggregated on the server. The main algorithmic novelty seems to be to how aggregation is done, and the idea was to apply the proximal operator (the *same one used by Dalleiger et al.*) to the mean of the updates.\n\nThe theoretical result (convergence of the iterates in Proposition 4.1) has several issues.\nFirst, the proof is wrong. It is argued that the gradients tend to 0, thus the iterates converge. This is not true (think of the sequence $x_k = 1/k$).\nEven if the proof were to be corrected, the result is still very weak. It only claims that the process converges (to a binary solution) without saying anything about the limit's utility (for example whether this limit is close to the solution). Such a result can be obtained by a trivial algorithm (for example one that uses a step size of 0 after a finite number of steps).\n\nThe paper also seems completely unaware of the extensive literature on private matrix completion (it is claimed in the related work that \"algorithms for matrix factorization seek computational efficiency while disregarding privacy aspects\"). The problem has been studied (without the binary constraints) in a number of papers both in the central and federated settings [1-4]. Even though the exact setting is different (due to the binary constraints) the methods are closely related, and several ideas used in this paper were already proposed in prior work. For example, this paper proposes \"partial sharing\" (sharing the V factor but keeping the U_i factors local to each client), but this exact scheme was used in [1-5] (in the privacy literature this is referred to as joint-DP or sometimes as the billboard model of DP).\n\nOther comments:\n- What is the motivation to add binary constraints in recommender systems? This does not seem to be a common practice. (Related to this point: some of the recommendation data sets used in experiments were used in prior work on private matrix completion, without binary constraints. It would be good to compare.)\n- In section 4, it is claimed that \"Without the knowledge of $U_i$, the server cannot estimate specific attributes of individual users\". This is not true. For example access to gradients can leak information about the client's data. This has been shown via reconstruction attacks (for example in [6] which looked specifically at matrix completion).\n- Privacy-related experiments: it seems that in all cases, for epsilon up to roughly 10, the loss exceeds 1 (which if I understand the loss definition correctly, means the learned solution is worse than even a trivial solution of all 0). This seems surprising, it basically means that learning is impossible except for very large epsilon. This might indicate that the method has not been properly tuned. Perhaps the authors should carefully revisit these experiments.\n- It is mentioned in the appendix that sensitivity was estimated by the maximum number of 1s in the input data. But note that this is a private quantity, and it should be estimated privately (not computed exactly), or you can perhaps set a maximum number then subsample each client to this number.\n- [a comment that does not affect my evaluation] the authors included their code in the supplementary material, and this is commendable. However, including code without comments or instructions on running experiments makes it virtually impossible to reproduce the results. Ideally, one should include at least a readme file with instructions on how to run the methods (and how to get the data).\n\n1. Chien et al. Private alternating least squares: Practical private matrix completion with tighter rates. ICML 2021\n2. Jain et al. Differentially private model personalization. NeurIPS 2021\n3. Wang et al. Differentially Private Matrix Completion through Low-rank Matrix Factorization. AISTATS 2023\n4. Shen et al. Share your representation only: Guaranteed improvement of the privacy-utility tradeoff in federated learning. ICLR 2023\n5. Singhal et al. Federated reconstruction: Partially local federated learning. NeurIPS 2021\n6. Chai et al. Secure Federated Matrix Factorization. IEEE Intelligent Systems, 2021\n\n========= Post rebuttal =========\nI thank the authors for their response regarding related work, sensitivity, and reconstruction attacks.\n\nRegarding the convergence proof: unfortunately the response indicates a fundamental misunderstanding of convergence. It is claimed in the paper that \"FELB converges to a stable binary solution\". Convergence was never proved. What was shown instead is that $\\|\\eta_t\\nabla F\\|$ converges to 0. Notice that what converges to 0 is the gradient *scaled by the learning rate* (indeed proving this is trivial, as the gradient is assumed to be bounded, and the learning rate is assumed to tend to 0).\nThis does not prove anything about convergence of the gradient, nor the trajectory.\n\nRegarding the relative loss: I understand that it can exceed 1. My point is that a relative loss exceeding 1 indicates a solution that's essentially useless, as even the trivial 0 solution would achieve a loss of 1. This indicates that the private method is not learning much except for very large epsilon. I encourage the authors again to revisit these experiments more carefully and investigate this poor performance.\n\nI maintain my initial assessment."
                },
                "questions": {
                    "value": "- I invite the authors to reflect/comment on connections to prior work mentioned above.\n- Do the authors think they can strength the theoretical analysis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8420/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8420/Reviewer_iqWp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451365181,
            "cdate": 1698451365181,
            "tmdate": 1700809229553,
            "mdate": 1700809229553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q1vBQCVpfu",
                "forum": "IpJIq3iwMH",
                "replyto": "litA83cg8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer iqWp,\n\nThank you for recognizing and highlighting the quality and clarity of our presentation, as well as the relevance of our work. Next, we address your concerns in detail.\n\n**W1) straightforward extension**\n\nAlthough we make use of work from Dalleiger et al. on Boolean matrix factorization, our improvements are significant. First, we contribute a novel regularization scheme that clearly outperforms Dalleiger et al.\u2019s approach. Second, our approaches are the first and only truly federated Boolean matrix factorization algorithm in existence, filling an important research gap and addressing real-world needs. Third, we provide rigorous guarantees in terms of convergence.\n\n**W2) Proof**\n\nWe respectfully disagree with the comment that the proof is incorrect. The proof is, in fact, correct. There seems to be a misunderstanding in what approaches zero. It is not the gradient itself, as the reviewer seems to have understood, but rather, we assume that the gradient is limited. It is the gradient component, as opposed to the regularizer, that diminishes towards zero. The proof demonstrates that the gradient is bounded by $R$, and since $\\eta_t\\propto \\lambda_t^-1\\rightarrow 0$ for $t\\rightarrow \\infty$, the limited gradient multiplied by a decreasing learning rate tends to zero. We apologize if the wording caused any confusion, and will rewrite this part to avoid this for any future reader.\n\nWe also respectfully disagree that a convergence result is without value if the limit\u2019s utility is not demonstrated. In non-convex optimization, such as for deep learning or federated deep learning, it is common practice to illustrate that $\\mathbb{E}[\\nabla_w F(w)]$ approaches zero, signifying the attainment of a local minimum or saddle point. These convergence results are highly valuable, although they do not imply the usefulness of that limit.\n\n**W3) Matrix Completion**\n\nWe thank the reviewer for making us aware of related work.\nWe acknowledge that there is related prior art on partially sharing matrices (joint-DP) for federated matrix completion and will reflect this in our related work section.\nHowever, federated matrix completion methods are only complementary to our problem.\nAs you also already point out yourself, Algorithms [1-5] focus on non-Boolean data.\nImposing Boolean constraints on the resulting factorization is central to our problem setting.\n\n**Q1) Datasets**\n\nBoolean data is ubiquitous in real-world settings, such as gene mutation data.\nIn many recommender applications, the goal is to distinguish between good or bad recommendations. Factorizing a binarized data matrix with two Boolean factor matrices,  will result in sparse (Miettinen et al. 2006), informative, and easily interpretable factorizations by default.\n\nOur datasets are derived from commonly-used and well-known benchmark datasets.\nHowever, as we require Boolean inputs, we binarized our datasets first.\nComparing aforementioned matrix completion algorithms with our method, would not only compare the performance of different tasks, but would also introduce a bias in favor of our algorithm, as\u2014unlike our approaches\u2014the completion algorithms are not specialized towards Boolean data. \n\n**Q2)**\n\nThe claim is that the server cannot trivially estimate specific attributes about users, since factorizations are not unique (nonnegative factorization are not unique, even in case of infinitesimal rigidity [1], and Boolean factorizations can be shown to be not unique by a simple application of the pigeonhole principle, see [2] for further details). We apologize for the imprecision. As stated in Sec. 4.2, we are well aware that a curious server could in principle obtain information about local data from V - one could imagine approaches similar to gradient leakage or membership inference attacks. This is the whole motivation for the differential privacy mechanism in Sec. 4.2 and our empirical evaluation of it in Sec. 5.1.3.\n\n**Q3) Relative Loss**\n\nThe relative loss can exceed $1$, if the reconstruction consists of more $1$s than the input.\n\n**Q4) Sensitivity**\n\nThank you for this great observation. Indeed, we have used a very straight-forward estimation of sensitivity in our experiments, that reveals the maximum number of $1$s in the dataset. Using a differential privacy mechanism to share local maxima is a great idea. Setting a global maximum is a neat practical trick, but requires some domain knowledge to set it right - too low and we have to distort data substantially by subsampling, too large and we decrease the quality due to excessive noise. \n\n**Q5) Code**\n\nOf course, we will submit a complete, fully documented, and easily usable reproducibility package, that comes as an archive with DOI and a publicly available and maintained GitHub repository.\n\nWe thank the reviewer for the constructive feedback that clearly aims to improve our work. In case we answered your questions in a satisfactory way, please do consider raising your score accordingly."
                    },
                    "title": {
                        "value": "Reply to Reviewer iqWp"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564092786,
                "cdate": 1700564092786,
                "tmdate": 1700564108520,
                "mdate": 1700564108520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wKlsHNbjE5",
            "forum": "IpJIq3iwMH",
            "replyto": "IpJIq3iwMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8420/Reviewer_nPms"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8420/Reviewer_nPms"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a novel method called Felb, which is a federated binary factorization algorithm based on proximal gradients along with the adaptive alternative Falb."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the authors bring a novel algorithm designed for binary matrix factorization. The previous federated factorization algorithms were not for the binary/boolean matrices.\n- the paper is well-written, easy to follow."
                },
                "weaknesses": {
                    "value": "- The contribution is limited to the binary matrices, unless a further performance analysis on the non-binary matrices are provided. (the most of the datasets selected are by default not boolean matrices but converted to boolean)\n\n- The potential disadvantages of having a proximal operator is not being discussed. \n\n- The methods compared could be discussed further, for instance in Figure 6. the loss values for most of the models seem to be stuck at 1. What is the reason for that? What are the parameters used for these methods? How are they implemented? It might be possible that this is a bug. Similarly the recall values are zero. This is skeptical.\n\nPlease refer to the Questions section for the minor questions."
                },
                "questions": {
                    "value": "- We demonstrate that our approach converges for a strictly monotonically increasing regularization rate.\n\nCould authors elaborate more on this? How is this useful? What is the rate? Is there an ablation study on different rates? \n\n- Most of the recent research shows that preserving privacy inevitably brings a reduction in accuracy. The authors claim that the accuracy of the method is even higher than the methods that do not have a privacy component. What is the trade-off? computational complexity? scalability? time complexity? difficulty in implementation?\n\n- What are the advantages that the proximal gradients bring to the problem? \n\n- Did the authors do any comparison with the new federated matrix factorization (Du et al., 2021) and federated non-negative matrix factorization (Li et al., 2021).\n\n- Figure 6, is there a logical order of the datasets?\n\n- Where is the differential privacy stated in the formulation?\n\n- Is the lambda different for u and v? (Algorithm 1, line 4 and 5)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper does not need ethics review."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823599425,
            "cdate": 1698823599425,
            "tmdate": 1699637048630,
            "mdate": 1699637048630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2pObcQ5Ixi",
                "forum": "IpJIq3iwMH",
                "replyto": "wKlsHNbjE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8420/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer nPms (part 1)"
                    },
                    "comment": {
                        "value": "Dear reviewer nPms,\n\nThank you for your concise summary, for highlighting the quality of our writing, and for recognizing the novelty of our algorithms. We appreciate your valuable feedback and questions that will help us to further improve our work. Next, we address the remaining points.\n\n**W1)  Limited to binary data**\n\nWe do not agree that our scope is limited. Our scope is merely different from the scope of non-Boolean matrix factorization. Boolean data is ubiquitous in real-world settings, such as gene mutation data. In many recommender applications, the goal is to distinguish between good or bad recommendations. Factorizing a binarized data matrix with two Boolean factor matrices,  will result in sparse, informative, and easily interpretable factorizations by default.\n\nThat being said, it would be unfair to experimentally compare binary matrix factorizations of discretized inputs to non-binary matrix factorizations of non-discretized inputs.\nApplying non-Boolean matrix factorization, e.g. NMF, neither gives such guarantees and often simply does not make sense (e.g. gene activations). \n\nIn our experiments, we have selected a mix of easily-recognizable datasets for the MF community and a mix of datasets where the discretization yields results that are sparser and easier to interpret. That is, in those applications it is important to gain insight into which features tend to be jointly present (e.g. yes/no recommendations) resp. for which it primarily matters whether the value is high or low, and the actual values matter much less (e.g. gene expression data).\n\n**W2) advantages of a proximal operator**\n\nApproaching binary matrix factorization in terms of a proximal algorithm has similar benefits as approaching NMF with proximal gradient descent. That is, by separating loss and binary regularization, we can take a much longer \u201cunregularized\u201d gradient step than the steps taken by e.g., multiplicative update rules (MUR). This will usually result in significantly faster convergence rates in comparison to MUR.\n\nThe downside is that we need to compute the proximal step. This, however, is done by a single element-wise matrix operation via an efficient GPU kernel in O(n*k). We will clarify the advantages of our proximal approach in the updated version of the manuscript.\n\n**W3) competitors, parameters**\n\nWe share the desire for having a rigorous and extensive evaluation. We do believe that our evaluation fits this standard well.\n\n**W3.1) Regarding reproducibility**\n- The parameters can be found and are discussed in the \u201cReproducibility\u201d section in the appendix.\n- The implementations for the competing BMF algorithms are publicly available from their respective authors.\n\n**W3.2) Regarding results**\n\n- We consider a _relative_ loss, normalized by the Frobenius norm of the input matrix, rather than an absolute loss, as this allows for more insightful comparison across datasets.\n- During the empirical evaluation, we extensively studied the performance, looked into reconstructions, and validated the correctness of all methods, including the \u201ccompeting\u201d algorithms. On synthetic data, we observe a different trend in the performance of our \u201ccompetitors\u201d on synthetic data, which is evidence for this. On real-world data, those algorithms do not allow for an accurate reconstruction in terms of shared coefficient (component) matrices. Because the algorithms perform as expected locally, we discovered that widely-used aggregation strategies fail to address the problem well. \nWe therefore proposed our two methods to address this problem.\n\n**Q1)  Usefulness of a Regularization Rate**\n\nThe regularization rate allows us to converge to a Boolean factorization in the limit.\nThis is a crucial aspect of the convergence proof of our algorithm.\nThe exact rates are given in the \u201cReproducibility\u201d section of the Appendix.\n\nBecause FELB and FALB exhibited robust and stable performance behavior, under different yet sensible regularization rates, we skipped a full-blown ablation study for brevity. \nTherefore, we chose a single exponentially-growing regularization rate, which is $\\lambda_t = \\lambda \u00b7 1.005^t$. As is the case for any method, adversarially chosen parameters will of course yield bad results.\n\n**Q2) privacy**\n\nThe enhanced privacy of occluding observation-specific factors not only reduces the computational complexity on the server-side but also the amount of data that needs to be transmitted. To guarantee differential privacy, we use an additive noise mechanism, which _reduces_ the accuracy\u2014as expected\u2014with regards to having no additive noise."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563859259,
                "cdate": 1700563859259,
                "tmdate": 1700563859259,
                "mdate": 1700563859259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]