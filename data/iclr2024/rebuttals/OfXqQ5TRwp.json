[
    {
        "title": "ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models"
    },
    {
        "review": {
            "id": "Yv1Fiu3iKq",
            "forum": "OfXqQ5TRwp",
            "replyto": "OfXqQ5TRwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_7RJR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_7RJR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel ACT framework--ALAM to quantize activations for memory-efficient training of transformer models. ALAM utilizes average quantization and a lightweight sensitivity calculation scheme. The experiments show that ALAM significantly reduces activation memory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe proposed ALAM provides high-quality deeply compressed activations with precision of less than 1 bit, which enables further compression without sacrificing accuracy.\n2.\tThe proposed ALAM is demonstrated to successfully compress activations in various transformer models."
                },
                "weaknesses": {
                    "value": "The complexity of ALAM is not analyzed. And the paper only compares training time for LLaMA-3B."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717586026,
            "cdate": 1698717586026,
            "tmdate": 1699636322353,
            "mdate": 1699636322353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uJuYWqFAMY",
                "forum": "OfXqQ5TRwp",
                "replyto": "Yv1Fiu3iKq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7RJR"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for carefully reviewing our submission and providing valuable feedback. Please see below for our response to the questions and comments.**\n\n**Q1)** The complexity of ALAM is not analyzed. And the paper only compares training time for LLaMA-3B.\n\n**A1)** We thank the reviewer for this valuable feedback. To address the reviewer\u2019s concern, here we first analyze the time complexity of ALAM in more detail. Please note that the sensitivity calculation is conducted only twice (before training and at 10% of the training process), and hence its overhead is excluded from the analysis. Our ALAM utilizes average quantization, which involves average calculation and quantization including searching for min and max values, multiplication with a scaling factor, and stochastic rounding. These operations have a time complexity of $\\mathcal{O(n)}$, where n represents the number of activations. The authors in [1] calculated the FLOPs and the number of activations of a transformer block with flash-attention as $24bsh^{2} + 4bs^{2}h$ and $34bsh$, respectively, where $b$, $s$, and $h$ denote the batch size, sequence length, and hidden size, respectively. Based on this and the fact that the size of the hidden dimension is generally larger than the sequence length in LLMs, the time complexity of training an LLM and the additional time complexity due to ALAM are calculated as shown in Table E.1 below.\n\n**Table E.1. Time complexity of training and activation compression required by ALAM.**\n\n|                 | Training   | Activation compression |\n|-----------------|------------|------------------------|\n| Time complexity | $\\mathcal{O}(bsh^{2})$ | $\\mathcal{O}(bsh)$                 |\n\nTherefore, as the model size $h$ increases, the training time overhead due to ALAM becomes relatively smaller. To verify this, we trained models larger than 7B on the LIMA (1K) dataset for 5 epochs, and the results are displayed in Table E.2 below.\n\n**Table E.2. Time for fine-tuning LLaMA2-7B for 5 epochs on the LIMA datasets using QLoRA with different ACT frameworks with a single NVIDIA A6000 GPU. The micro batch size is set to 2.**\n\n| Model | Baseline | GACT (4-bit) | ALAM (1-bit) |\n|---|---|---|---|\n| LLaMA2-7B | 30 min | 36 min | 37 min |\n| LLaMA2-13B | 51 min | 58 min | 60 min |\n| LLaMA-30B | 115 min | 127 min | 128 min |\n\nTable E.2 shows that while there is a 23% time overhead for the LLaMA2-7B model, the overhead is reduced to 18% and 12% for the LLaMA2-13B and LLaMA-30B models, respectively. We once again thank the reviewer for this great suggestion, and we will include this analysis in the camera-ready version if the paper is accepted.\n\n[1] Korthikanti, V. A., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., & Catanzaro, B. Reducing activation recomputation in large transformer models.\u00a0*Proceedings of Machine Learning and Systems*,\u00a0*5, 2023*.\n\n[ref1]: Aspose.Words.e37027c5-5038-4eae-ad68-a6473d90ddd2.002.png\n[ref2]: Aspose.Words.e37027c5-5038-4eae-ad68-a6473d90ddd2.006.png"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275914963,
                "cdate": 1700275914963,
                "tmdate": 1700461698939,
                "mdate": 1700461698939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rRAnuAGOFD",
                "forum": "OfXqQ5TRwp",
                "replyto": "Yv1Fiu3iKq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for carefully reviewing our submission. As the reviewer-author discussion period ends in less than 12 hours, we would be very grateful if we could hear back for any thoughts. If possible, please take a look at our responses and see if they adequately address your concerns. We are also open to any further feedback or queries you may have.\n\nThank you,\n\nAuthors of Paper #3660"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705152624,
                "cdate": 1700705152624,
                "tmdate": 1700705152624,
                "mdate": 1700705152624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jle5RDYQFg",
            "forum": "OfXqQ5TRwp",
            "replyto": "OfXqQ5TRwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_UZqy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_UZqy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for reducing the memory usage of activations during the training of large-scale models. The techniques extend well-established methods like GACT, further improving the activation compression rate by introducing group-wise quantization. The methods are evaluated on popular transformer models such as BERT and LLaMa, demonstrating enhanced compression rate while preserving baseline accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1). The paper is well-written. \n\n2). The compression rates have been improved compared with previous methods."
                },
                "weaknesses": {
                    "value": "The primary concern is the lack of novelty. The method relies heavily on existing frameworks, such as GACT and ActNN, using the same concept of quantization for activation compression. Additionally, the proposed quantization method is also a well-known clustering approach. The theoretical analysis, suggesting the minimization of quantization MSE (as shown in quation 6) appears rather straightforward. While the authors did introduce some tricks, such as gradient normalization variance, to improve the existing framework, the overall technical contribution seems to be relatively modest."
                },
                "questions": {
                    "value": "Could the authors explain the overhead involved in doing the group-wise quantization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3660/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3660/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3660/Reviewer_UZqy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718344608,
            "cdate": 1698718344608,
            "tmdate": 1700713500681,
            "mdate": 1700713500681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uf5YQY86AT",
                "forum": "OfXqQ5TRwp",
                "replyto": "Jle5RDYQFg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UZqy"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for carefully reviewing our submission and providing valuable feedback. Please see below for our response to the questions and comments.**\n\n**Q1)** The primary concern is the lack of novelty. The method relies heavily on existing frameworks, such as GACT and ActNN, using the same concept of quantization for activation compression. Additionally, the proposed quantization method is also a well-known clustering approach. The theoretical analysis, suggesting the minimization of quantization MSE (as shown in equation 6) appears rather straightforward. While the authors did introduce some tricks, such as gradient normalization variance, to improve the existing framework, the overall technical contribution seems to be relatively modest.\n\n**A1)** We thank the reviewer for this valuable feedback. We agree with the reviewers that our AQ algorithm has been developed and proven to work in a similar way to ActNN [1] and GACT [2]. However, we would like to point out that **the core idea of our algorithm, simply compressing a group of adjacent activations using a group average value without the need for extensive iterations to categorize similar values, is entirely new and has not been reported in the literature**. In other words, to the best of our knowledge, our work is the first attempt to compress activations through simple averaging.\n\nUnlike a previous method [3] that employs K-means clustering to group weights, which requires a time-consuming process to find weights with similar values, our novel activation compression method relies solely on simple averaging of adjacent activations without the need to identify similar values, which can be very efficiently implemented. In addition, this approach is validated mathematically, proving that simple averaging can minimize gradient variances.\n\nWe also agree that the minimization of quantization MSE in Eq. 6 looks straightforward, but actually deriving it was not very straightforward, as shown in Appendix A.2. More specifically, we mathematically proved that the gradient variance of batch normalization and layer normalization has the form of Eq. 6, which was not proven in prior work [1, 2, 4]. This was a key contribution that allowed us to prove that representing activation as a group average can minimize gradient variance for the first time.\n\nLastly, to the best of our knowledge, **we are the first to demonstrate an effective activation compression algorithm that supports LLMs with more than 7B parameters**. Unlike model compression algorithms like LoRA and QLoRA, previous activation compression algorithms such as ActNN, MESA, and GACT were not proven to work with LLMs. Our experimental results confirm that ALAM can train large language models such as LLaMA2-7B, LLaMA2-13B, and LLaMA-30B even at 1-bit, whereas conventional methods such as GACT fail to train the models.\n\n**Q2)** Could the authors explain the overhead involved in doing the group-wise quantization?\n\n**A2)** This is an important point, and we thank the reviewer for bringing up this issue. Average Quantization generates sub-1b values by first compressing adjacent activations to a group average value and then performing conventional quantization to the group average value. We suspect that the reviewer is wondering if there is any additional memory or time overhead involved in this process. First, there is no additional memory overhead for compressing activation. As explained in Sections 3.1 and 3.2, we group adjacent activations in the flattened activation vector during the forward pass. During the backward pass, they are restored simply by repeating the group average values. For example, when the group size is 2,\n\n- Averaging in the forward pass: [4, 6, 7, 1, 6, 8] -> [5, 4, 7]\n- Restoring in the backward pass: [5, 4, 7] -> [5, 5, 4, 4, 7, 7]\n\nTherefore, there is no need to indicate specific locations, and hence our algorithm does not save any masks. Second, our Average Quantization requires additional time to compute the group average value. However, its time overhead is similar to that of the activation quantization process in conventional activation compression schemes, which is in line with experimental results shown in Table D.1 below.\n\n**Table D.1. Time for fine-tuning LLaMA2-7B over 5 epochs on the LIMA datasets using QLoRA with different ACT frameworks with a single NVIDIA A6000 GPU. The micro batch size is set to 2.**\n\n| Model | Baseline | GACT (4-bit) | ALAM (1-bit) |\n|---|---|---|---|\n| LLaMA2-7B | 30 min | 36 min | 37 min |\n| LLaMA2-13B | 51 min | 58 min | 60 min |\n| LLaMA-30B | 115 min | 127 min | 128 min |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275665081,
                "cdate": 1700275665081,
                "tmdate": 1700646981561,
                "mdate": 1700646981561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rlxCpqo9pf",
                "forum": "OfXqQ5TRwp",
                "replyto": "Jle5RDYQFg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for carefully reviewing our submission. As the reviewer-author discussion period ends in less than 12 hours, we would be very grateful if we could hear back for any thoughts. If possible, please take a look at our responses and see if they adequately address your concerns. We are also open to any further feedback or queries you may have.\n\nThank you,\n\nAuthors of Paper #3660"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705144923,
                "cdate": 1700705144923,
                "tmdate": 1700705144923,
                "mdate": 1700705144923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JHbeHiuVEY",
                "forum": "OfXqQ5TRwp",
                "replyto": "rlxCpqo9pf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Reviewer_UZqy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Reviewer_UZqy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks you for the detailed response. Some of my concerns are addressed. I will raise my rating to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713469642,
                "cdate": 1700713469642,
                "tmdate": 1700713469642,
                "mdate": 1700713469642,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3a8XwFq4b8",
            "forum": "OfXqQ5TRwp",
            "replyto": "OfXqQ5TRwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_Eyz9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_Eyz9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ALAM, an ACT framework that applies average quantization and a novel sensitivity calculation method to reduce memory usage while preserving the performance of LLMs. By compressing activations to their group averages, ALAM minimizes the impact on gradient variance, enabling deep compression with an effective precision of less than 1 bit. ALAM\u2019s sensitivity calculation is based on the L2 norm of parameter gradients, which is a memory-efficient approach. The paper's experiments show that ALAM can achieve up to 12.5\u00d7 activation memory compression in LLMs without sacrificing accuracy. This method represents a significant advancement in making the training of large neural networks more memory-efficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The quantization method based on group average value is very easy to understand and very practical, given the experimental demonstration and equation proof in the paper.\n2. The training overhead is acceptable, since the efficient sensitivity calculation method is proposed, and the training time is comparable with the baseline.\n3. The evaluation result is very promising and the improvement compared with GACT (SOTA work based on activation compression) is very significant."
                },
                "weaknesses": {
                    "value": "1. The evaluation part is not sufficient. This paper only compares the result with GACT, which is based on activation compression. However, for model compression, other techniques like pruning and low-rank compression are widely used. \n2. How to choose the parameter group is unclear. Is this parameter related to the sensitivity? What if the adjacent activations have little similarity?"
                },
                "questions": {
                    "value": "1. It would be better to provide the evaluation result with other compression techniques.\n2. In Table 1, there is an accuracy drop from 98.72 to 96.09 in a small model. However, in the evaluation result, the compressed model retains the accuracy performance. Can you identify the reason for this? This is because the large model has more redundancy or other reasons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3660/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3660/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3660/Reviewer_Eyz9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773958635,
            "cdate": 1698773958635,
            "tmdate": 1699636322166,
            "mdate": 1699636322166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CacHVPwaPT",
                "forum": "OfXqQ5TRwp",
                "replyto": "3a8XwFq4b8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Eyz9"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for carefully reviewing our submission and providing valuable feedback. Please see below for our response to the questions and comments.**\n\n**Q1)** The evaluation part is not sufficient. This paper only compares the result with GACT, which is based on activation compression. However, for model compression, other techniques like pruning and low-rank compression are widely used. \n\n**A1)** We would like to thank the reviewer for bringing up an important point. Following the reviewer\u2019s advice, we conducted detailed comparisons between ALAM and other model compression methods. For comparisons, we selected LoRA [1], which is the state-of-the-art low-rank compression method. We also implemented QLoRA [2], which performs 4-bit quantization for weights and only computes weight gradient for LoRA parameters. QLoRA allows for a 4\u00d7 reduction in the weights with comparable accuracy, making it widely used for fine-tuning LLMs in constrained environments. The results are presented in Table C.1. Please note that \u2018average accuracy\u2019 represents the average accuracy over all tasks in Tables A.1 and A.2 in the global response. Also, \u2018param mem\u2019 represents the parameter memory occupied by weights, gradients, and optimizer states during training.\n\n**Table C.1. Comparisons with model compression techniques in fine-tuning LLaMA2-7B and LLaMA2-13B on the LIMA dataset. The micro batch size is set to 8.**\n\n| Model      | Strategy       | Average accuracy | Param mem | Act mem | Total mem |\n|------------|----------------|------------------|-----------|---------|-----------|\n| LLaMA2-7B  | Baseline       | OOM              |   52 GB   |  21 GB  |   73 GB   |\n|            | + LoRA         | 62.5             |   13 GB   |  21 GB  |   34 GB   |\n|            | + LoRA + ALAM | 62.2             |   13 GB   |  2.0 GB | **15 GB** |\n| LLaMA2-13B | Baseline       | OOM              |   104 GB  |  33 GB  |   137 GB  |\n|            | + QLoRA        | 65.1             |   6.6 GB  |  33 GB  |   40 GB   |\n|            | + QLoRA + ALAM | 65.0             |   6.6 GB  |  3.2 GB | **10 GB** |\n\nIn experiments, LoRA noticeably reduces the memory allocated for gradients and optimizer states, which results in a 4\u00d7 compression of parameter memory during training. Additionally, QLoRA exhibits a large compression rate of 16\u00d7 by utilizing 4-bit weights. However, neither LoRA nor QLoRA compresses activation memory. Our ALAM, on the other hand, focuses on compressing activation memory, and hence achieves 10.6\u00d7 and 10.3\u00d7 compression rates on LLaMA2-7B and LLaMA2-13B, respectively. As a result, compared to when only LoRA and QLoRA were applied, additionally employing ALAM further saves 56% and 75% of the total memory with comparable accuracy for LLaMA2-7B and LLaMA2-13B, respectively. In summary, our ALAM is orthogonal to other model compression techniques, and they can be applied simultaneously for maximum memory savings.\n\nPlease note that only a handful of pruning schemes supporting very large models have been reported in the literature, with limited memory savings. For instance, LLM-Pruner [3] exhibits a significant drop in accuracy even when pruning just 20% of the parameters in LLaMA-7B, translating to similar saving only in the parameter memory."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275160059,
                "cdate": 1700275160059,
                "tmdate": 1700275160059,
                "mdate": 1700275160059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "efKioUIUAQ",
                "forum": "OfXqQ5TRwp",
                "replyto": "3a8XwFq4b8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Eyz9"
                    },
                    "comment": {
                        "value": "**Q2)** How to choose the parameter group is unclear. Is this parameter \u2018group\u2019 related to the sensitivity? What if the adjacent activations have little similarity? \n\n**A2)** We thank the reviewer for raising an important issue, and we apologize for the confusion. As described in Sections 3.1 and 3.2, we group adjacent activations together after simply flattening the activations, without any other complicated processes. As the reviewer insightfully pointed out, varying the group size (g) and the precision of the group average (n) could produce different bit precisions (AQ-n/g bit). However, in all experiments using Average Quantization, we fixed the group size to 4 and performed 2-bit quantization on the group average value to obtain AQ 0.5-bit, which is an optimal configuration identified in experiments, as shown in Table C.2. \n\n**Table C.2. Performan comparisons of different AQ 0.5-bit configurations when training VGG-11 on CIFAR-100 (n: precision of group average, g: group size).**\n\n| Precision        | FP32     | 1-bit |  |          |    AQ 0.5-bit    |         |          |\n|------------------|----------|-------|------------|----------|--------|---------|----------|\n| AQ scheme [n,g]  | -        | -     | (1, 2)     | (2, 4)   | (4, 8) | (8, 16) | (32, 64) |\n| Accuracy (%)     | **67.7** | 1.0   | 1.0        | **52.3** | 1.87   | 8.07    | 3.08     |\n\nThe group size is not related to sensitivity. More specifically, the sensitivity of each layer is first calculated using the GradNormVar algorithm, which determines the layer\u2019s bit precision. In the layers with low sensitivity, we quantize activations into AQ 0.5-bit, where average quantization is performed with a fixed group size of 4 as discussed above.\n\nThe reviewer is certainly correct that adjacent activations may show little similarity. This could be mitigated by only grouping activations with similar distributions, but we did not implement this in our algorithm since this process could be time-consuming and require additional memory to save the mask for recovery. Our simple grouping scheme may lead to some loss of information in activations, but the impact on actual accuracy is minimal because ALAM assigns AQ 0.5-bit only to layers with very low sensitivity. Furthermore, the quality of AQ 0.5-bit activations is still higher than that of 1-bit quantized activations, as shown in Table C.2."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275239546,
                "cdate": 1700275239546,
                "tmdate": 1700276224500,
                "mdate": 1700276224500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5Gcndkk6RV",
                "forum": "OfXqQ5TRwp",
                "replyto": "3a8XwFq4b8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for carefully reviewing our submission. As the reviewer-author discussion period ends in less than 12 hours, we would be very grateful if we could hear back for any thoughts. If possible, please take a look at our responses and see if they adequately address your concerns. We are also open to any further feedback or queries you may have. \n\nThank you,\n\nAuthors of Paper #3660"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705132095,
                "cdate": 1700705132095,
                "tmdate": 1700705132095,
                "mdate": 1700705132095,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uEFqy0Mfnv",
            "forum": "OfXqQ5TRwp",
            "replyto": "OfXqQ5TRwp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_HytY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3660/Reviewer_HytY"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on the challenge of high GPU memory consumption for storing activation for deep neural network training, especially for large language models (LLMs). Existing activation-compressed training (ACT) approaches introduce significant performance drops when applied to LLMs. This paper proposes a new ACT framework, ALAM, that applies average quantization and a lightweight sensitivity calculation scheme to enable substantial memory savings for LLM training while maintaining training performance. The experiments show that the ALAM framework can support up to a 12.5x compression rate with 1-bit quantization without compromising accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well-written and easy to follow.\n+ The proposed average quantization in ACT is simple but effective.\n+ The proposed GradNormVar algorithm significantly reduces the memory requirement for storing the gradients during the sensitivity evaluation.\n+ The evaluation results cover both the classical model BERT and the latest model LLaMa in terms of large language model training/finetuning. The results of fine-tuning LLaMa-2 with 2-bit and even 1-bit ALAM are promising."
                },
                "weaknesses": {
                    "value": "- It would be better to add QLoRA as a baseline for evaluation.\n- It is unclear how the proposed ALAM performs on larger models, such as LLaMa-13B, LLaMa-30B, etc. It would be better to show the results on these larger models."
                },
                "questions": {
                    "value": "Please answer the questions in weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699323697298,
            "cdate": 1699323697298,
            "tmdate": 1699636322075,
            "mdate": 1699636322075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ydV7VQFjKQ",
                "forum": "OfXqQ5TRwp",
                "replyto": "uEFqy0Mfnv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HytY"
                    },
                    "comment": {
                        "value": "**We thank the reviewer for carefully reviewing our submission and providing valuable feedback. Please see below for our response to the questions and comments.**\n\n**Q1)** It would be better to add QLoRA as a baseline for evaluation. Also, it is unclear how the proposed ALAM performs on larger models, such as LLaMa-13B, LLaMa-30B, etc. \n\n**A1)** We thank the reviewer for this great suggestion. Following the reviewer\u2019s comment, we applied QLoRA [1] and evaluated our algorithm on larger models: LLaMA2-13B and LLaMA-30B. Please note that we used the recently released LLaMA2 for a 13B model, which has the same structure as LLaMA but is pre-trained on a broader range of datasets, leading to improved accuracy. On the other hand, LLaMA2-30B is not publicly available, and hence LLaMA-30B was employed instead. In this experiment, we fine-tuned the model for 5 epochs on the LIMA dataset. The results are displayed in Table B.1. In the table, 'precision' refers to the target average precision of activation during training as detailed in the manuscript, not the precision of the weight.\n\n**Table B.1. Test accuracy, activation memory (ACT mem) with compression, and memory for calculating sensitivity (sens mem) in fine-tuning LLaMA2-13B and LLaMA-30B on the LIMA dataset. The micro batch size is set to 2.**\n\n| Model       | PEFT  | Scheme   | Precision | Act mem            | MMLU     | Arc-c    | PIQA     | Hellaswag | WinoGrande | BoolQ    | TruthfulQA |\n|-------------|-------|----------|-----------|--------------------|----------|----------|----------|-----------|------------|----------|------------|\n| LLaMA2-13B  | QLoRA | Baesline | 16-bit    | 8.2 GB             | **55.0** | **50.3** |   80.0   |  **79.2** |    71.5    |   80.9   |  **39.1**  |\n|             |       | ALAM     | 1-bit     | **0.8 GB (10.3x)** | 54.8     |   50.0   | **80.2** |    78.8   |  **71.6**  |  **81**  |    38.9    |\n| LLaMA-30B   | QLoRA | Baesline | 16-bit    | 15.8 GB            | 56.6     |   52.4   | **81.4** |  **82.1** |    75.0    | **82.7** |  **42.8**  |\n|             |       | ALAM     | 1-bit     | **1.6 GB (9.9x)**  | **56.7** | **52.7** |   80.8   |    81.8   |  **75.3**  |   82.5   |    42.5    |\n\nExperimental results demonstrate that our ALAM (1-bit) compresses activation by 10.3\u00d7 in LLaMA2-13B and 9.9\u00d7 in LLaMA-30B while closely matching the baseline in accuracy for all tasks. We will include these updated experimental results in the camera-ready version if the paper is accepted.\n\n[1] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, & Luke Zettlemoyer (2023). QLoRA: Efficient Finetuning of Quantized LLMs. CoRR,\u00a0abs/2305.14314."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275046378,
                "cdate": 1700275046378,
                "tmdate": 1700533089072,
                "mdate": 1700533089072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MiOUSJ9VR6",
                "forum": "OfXqQ5TRwp",
                "replyto": "uEFqy0Mfnv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3660/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you once again for carefully reviewing our submission. As the reviewer-author discussion period ends in less than 12 hours, we would be very grateful if we could hear back for any thoughts. If possible, please take a look at our responses and see if they adequately address your concerns. We are also open to any further feedback or queries you may have. \n\nThank you,\n\nAuthors of Paper #3660"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705111663,
                "cdate": 1700705111663,
                "tmdate": 1700705111663,
                "mdate": 1700705111663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]