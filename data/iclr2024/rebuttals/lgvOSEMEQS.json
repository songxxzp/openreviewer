[
    {
        "title": "Lightweight Unsupervised Federated Learning with Pretrained Vision Language Model"
    },
    {
        "review": {
            "id": "WsGJOO1mQR",
            "forum": "lgvOSEMEQS",
            "replyto": "lgvOSEMEQS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6655/Reviewer_91xa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6655/Reviewer_91xa"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses the CLIP image and text encoder for unsupervised learning in FL. Specifically, it uses the image encoder and text features extracted in the server in the client for training. Besides, it also generates data samples on clients to mitigate data imbalance problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is interesting and the Figure 1 illustrates the idea well.\n- The paper the generally well-written and easy to follow.\n- The proposed method achieves significantly better performance than compared counterparts, especially under heterogeneous data distribution.\n- The proposed method only requires a few rounds of communication."
                },
                "weaknesses": {
                    "value": "- Some important experimental details are missing. For example, the model architecture used for training the other methods.\n- The paper mentions computation efficiency in Section 4.2.3, but it seems that the computation saving is mainly from the reduction of the communication round. The paper seems to focus on cross-device FL. It would be useful to further investigate whether the device could fit in the model size of CLIP and has enough memory to use it for inference.\n- The compared methods are somewhat weak baselines. Some important unsupervised FL baselines and methods are not discussed or compared in the paper, e.g., [1][2][3][4]\n    - [1] Collaborative Unsupervised Visual Representation Learning from Decentralized Data. ICCV\u201921\n    - [2] Divergence-aware Federated Self-Supervised Learning. ICLR\u201922.\n    - [3] Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering. ICML\u201922\n    - [4] MocoSFL: Enabling Cross-client Collaborative Self-supervised Learning. ICLR\u201923\n- Some papers mentioning adopting CLIP to FL are not discussed. e.g. [5][6]\n    - [5] Fedclip: Fast generalization and personalization for clip in federated learning.\n    - [6] When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions."
                },
                "questions": {
                    "value": "- What is the impact of using different types of backbone?\n- What is the impact of training for more local epochs in each round?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6655/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6655/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6655/Reviewer_91xa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552222004,
            "cdate": 1698552222004,
            "tmdate": 1699636760972,
            "mdate": 1699636760972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x2zZMnozYs",
                "forum": "lgvOSEMEQS",
                "replyto": "WsGJOO1mQR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply from Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for dedicating time and effort to review our paper.\n\n**1. Experimental details**\n\nAs outlined in subsection 4.1 of the experimental settings, it is essential to note that all the baselines adopt the same model architecture as the proposed method to ensure a fair comparison. Each baseline utilizes the pretrained CLIP image encoder (RN-50 variant) as the fixed encoder and subsequently trains a linear layer classifier. To enhance clarity, we have revised the descriptions in the baseline section accordingly.\n\n**2. Computation efficiency**\n\nWe kindly direct the reviewer to the authors' response to the second question posed by reviewer b2QK for further information and clarification.\n\n**3. Fitting CLIP for inference in local clients**\n\nGiven the constrained computation resources in local clients, our proposed lightweight framework adopts a strategy where textual embeddings are generated on the server side and then distributed to each client. This approach eliminates the need for repeated inference from the CLIP text encoder on each client. To further alleviate the local computation requirements, our method utilizes the smallest variant of the CLIP image encoder, namely RN50. Devices capable of accommodating ResNet-50 for inference can similarly support the model in our proposed method.\n\n**4. Other works on self-supervised federated representation learning**\n\nThis paper tackles the novel challenge of unsupervised federated learning and presents a lightweight framework designed to address it. The framework incorporates the pretrained CLIP image encoder, fixing it to minimize the computation and communication demands on local clients. The listed works, however, focus on self-supervised federated representation learning. In contrast to our approach, these methods propose techniques to train the image encoder in a federated manner. Consequently, they are not directly relevant to the problem addressed by this paper.\n\n**5. Other papers adopting CLIP to FL**\n\nWe thank the reviewers for providing the related works and have included them in the related works part. \nIt's essential to highlight that FedCLIP similarly incorporates the pretrained CLIP model into federated learning. However, it is designed to address the traditional supervised federated learning setting where labeled data are available in the clients. In contrast, our method focuses on the unsupervised federated learning setting, where only unlabeled data is available in the clients. This distinction underscores the unique contribution and applicability of our approach in scenarios with exclusively unlabeled client data.\n\n**6. Impact of difference encoder backbones**\n\nThe CLIP model offers a range of image encoder variants, each striking a balance between zero-shot prediction ability and model size. In the context of the lightweight unsupervised federated learning setting and with the aim of minimizing computational requirements on local clients, we opted for the smallest variant, RN-50, as the fixed image encoder. It's noteworthy that while the proposed method's performance benefits from the CLIP zero-shot prediction model, incorporating larger variants of the CLIP image encoder can further enhance performance, starting from the corresponding zero-shot performance of the selected image encoder. However, this improvement comes at the cost of increased computational demands on the local clients.\n\n**7. Impact of more local update epochs**\n\nWe conducted an additional ablation study to explore the influence of varying the number of local update epochs. The results for the two datasets, CIFAR-10 and CIFAR-100, under both homogeneous and heterogeneous settings are presented in the following table. Increasing the number of local update epochs did not lead to a notable improvement in the performance of the proposed method. The overall best performance was achieved with only 1 local update epoch. This observation underscores the lightweight property of the proposed method. Moreover, it eliminates the necessity for tuning this hyper-parameter, simplifying the implementation. This phenomenon can be attributed to the high entropy property of CLIP zero-shot prediction, emphasizing the need for stable local updating.\n\n| # epochs | CIFAR-10 |  | CIFAR-100 |  |\n|:---:|---|---|---|---|\n|  | s=2 | i.i.d. | s=10 | i.i.d. |\n| 1 | 72.0 | 74.0 | 43.3 | 43.2 |\n| 2 | 71.7 | 73.6 | 41.4 | 41.2 |\n| 3 | 72.0 | 73.1 | 37.3 | 37.4 |\n| 4 | 72.4 | 73.2 | 33.0 | 31.7 |\n| 5 | 73.0 | 73.2 | 28.6 | 26.6 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533576452,
                "cdate": 1700533576452,
                "tmdate": 1700533576452,
                "mdate": 1700533576452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FViNP8lNz7",
            "forum": "lgvOSEMEQS",
            "replyto": "lgvOSEMEQS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6655/Reviewer_JRVk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6655/Reviewer_JRVk"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a novel lightweight unsupervised federated learning approach, FSTCBDG, to alleviate the computational and communication costs as well as the human labor of data annotations. The evaluation results illustrated the proposed FSTCBDG significantly outperforms the baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Developed a lightweight unsupervised federated learning approach based on a single linear layer.\n\n2. Designed a self-training objective for the linear classifier\n\n3. Conducted extensive experiments to show the good performance of the proposed method over the baselines."
                },
                "weaknesses": {
                    "value": "1. The class prototype augmentation based on Gaussian noise is not novel, since this idea has been used in prior work called PASS [Zhu CVPR 2021]. In addition, some follow-up works like [Zhu CVPR 2022] have pointed out that synthetic data from Gaussian-based augmentation would make some similar classes overlap with each other. Thus, the proposed method in this paper may not work well.\n\n2. It may need to explain why the testing accuracy of baselines drops as the number of communication rounds increases.\n\n3. Why did not compare with the FedUL baseline?\n\n**References:**\n\n[Zhu CVPR 2021] Prototype Augmentation and Self-Supervision for Incremental Learning, CVPR,2021\n\n[Zhu CVPR 2022]. \"Self-sustaining representation expansion for non-exemplar class-incremental learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "Please see the comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718747063,
            "cdate": 1698718747063,
            "tmdate": 1699636760828,
            "mdate": 1699636760828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e6JENWqTft",
                "forum": "lgvOSEMEQS",
                "replyto": "FViNP8lNz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply from Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for dedicating time and effort to review our paper.\n\n**1. Novelty**\n\n- This paper introduces a novel setting of unsupervised federated learning and presents a lightweight framework designed to address this challenge effectively.\n- Notably, our work is the first to tackle federated learning with completely unlabeled data. While prior work, such as FedUL [1], claims to handle federated learning from unlabeled data, it necessitates knowledge of precise label frequencies for each client, which is often impractical in real-world applications.\n- Additionally, we propose a lightweight framework that incorporates the CLIP image encoder and text embeddings to enhance unsupervised federated learning. This framework trains a cleverly-initialized linear classifier on each client, significantly reducing computation and communication requirements.\n- As an illustrative solution within this framework, we introduce a resilient self-training approach with evolving pseudo-labels to address the unsupervised learning problem. To handle heterogeneous data distribution in federated learning, we propose a class-balanced data generation approach that generates synthetic feature instances from textual embeddings.\n- Experimental results demonstrate that our proposed method enhances the performance of CLIP's zero-shot prediction. Moreover, we show that our approach converges rapidly, substantially reducing the computation and communication demands on clients.\n\n**2. Class prototype augmentation**\n\nDifferent from the work [Zhu CVPR 2021] that generate Gaussian-based augmentation from prototypes during the model training, our method proposed to generate synthetic instances from the textual embeddings produced from the pre-trained CLIP text encoder. The textual feature vectors are trained to be discriminative such that the zero-shot prediction works. On the other hand, controlling the variance of the Gaussian distribution effectively avoids the feature distribution overlap across classes. We tried different $\\sigma$ values and found that the simplest unit variance produces the best performance.\n\n**3. Reason that testing accuracy of baselines drops**\n\nAs depicted in Figure 2 of our paper, a crucial observation regarding CLIP zero-shot prediction is that the predicted probability vectors exhibit high entropy, resembling a uniform distribution. When the CLIP image encoder is fixed, and the linear classifier is trained with strong supervision from ground-truth labels, it optimizes the classifier weights to diverge significantly from the textual embeddings. The divergence leads to biased and highly dynamic local classifiers, ultimately resulting in degraded performance. In contrast, our approach employs resilient self-training with evolving pseudo-labels to stabilize the local model updating process. Additionally, we incorporate class-balanced synthetic data generation to address the heterogeneous distribution of local data.\n\n**4. Comparing with FedUL baseline**\n\nAs discussed in the related works section, it's crucial to highlight that FedUL necessitates precise knowledge of label frequencies for each client, making it an unfair comparison method for the proposed approach. Additionally, FedUL involves the transformation and recovery of model parameters, while the proposed lightweight framework maintains a fixed state for the image encoder. Consequently, FedUL cannot be seamlessly adapted to the proposed lightweight framework."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532630052,
                "cdate": 1700532630052,
                "tmdate": 1700533683813,
                "mdate": 1700533683813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dtRASTPdSz",
                "forum": "lgvOSEMEQS",
                "replyto": "e6JENWqTft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6655/Reviewer_JRVk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6655/Reviewer_JRVk"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response! Regarding augmentation, I think you use the same idea as the PASS paper. PASS adds noise to the embeddings of images while you add noise to the embeddings of texts. In essence, the idea is quite similar. I think may you need to cite this work to respect the other authors. Thanks a lot!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673982209,
                "cdate": 1700673982209,
                "tmdate": 1700673982209,
                "mdate": 1700673982209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HyIfR7afzv",
            "forum": "lgvOSEMEQS",
            "replyto": "lgvOSEMEQS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6655/Reviewer_b2QK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6655/Reviewer_b2QK"
            ],
            "content": {
                "summary": {
                    "value": "This study leverages a pre-trained vision-language model for unsupervised image classification within a federated learning framework. The authors introduce two strategies to enhance the zero-shot prediction capabilities of CLIP. Experiments show that the proposed framework achieves better results compared to conventional supervised federated learning approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tCompared to other federated learning scenarios, such as supervised and semi-supervised methods, unsupervised federated learning remains a relatively unexplored domain.\n2.\tFrom the ablation study, the proposed two approaches can effectively improve the zero-shot prediction accuracy of CLIP and mitigate the class imbalance problem to some extent."
                },
                "weaknesses": {
                    "value": "1.\tThe contributions of this study did not meet the anticipated expectations. The two methods introduced lack novelty. The idea of refining pseudo-labels has been previously explored within the context of self-supervised learning. Additionally, the class-balanced data generation draws parallels with the Synthetic Minority Over-sampling Technique and can be categorized as an oversampling strategy.\n2.\tWhile the authors highlight the lightweight nature of their proposed framework, evidence throughout the paper seems insufficient. The sole indication of its lightweight character is the use of a linear classifier during training. However, this isn't a unique aspect as the baseline methods employ the same classifier. The purported lightweight advantage of the proposed framework isn't adequately substantiated. Furthermore, a comprehensive analysis of both computation and communication costs is essential to truly label the framework as lightweight and communication efficient.\n3.\tMore experiments are required, especially for large scale datasets like ImageNet.\n4.\tThere are some typos, like FedBR (Guo et al., 2023b) FedBR (Guo et al., 2023b), tranfer, etc."
                },
                "questions": {
                    "value": "please respond to the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834938831,
            "cdate": 1698834938831,
            "tmdate": 1699636760695,
            "mdate": 1699636760695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wZ98ju4gze",
                "forum": "lgvOSEMEQS",
                "replyto": "HyIfR7afzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply from Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for dedicating time and effort to review our paper.\n\n**1. Novelty of this paper**\n- This paper introduces a novel setting of unsupervised federated learning and presents a lightweight framework designed to address this challenge effectively.\n- Notably, our work is the first to tackle federated learning with completely unlabeled data. While prior work, such as FedUL (Lu et al, 2022), claims to handle federated learning from unlabeled data, it necessitates knowledge of precise label frequencies for each client, which is often impractical in real-world applications.\n- Additionally, we propose a lightweight framework that incorporates the CLIP image encoder and text embeddings to enhance unsupervised federated learning. This framework trains a cleverly-initialized linear classifier on each client, significantly reducing computation and communication requirements.\n- As an illustrative solution within this framework, we introduce a resilient self-training approach with evolving pseudo-labels to address the unsupervised learning problem. To handle heterogeneous data distribution in federated learning, we propose a class-balanced data generation approach that generates synthetic feature instances from textual embeddings.\n- Experimental results demonstrate that our proposed method enhances the performance of CLIP's zero-shot prediction. Moreover, we show that our approach converges rapidly, substantially reducing the computation and communication demands on clients.\n\n**2. Evidence of lightweight**\n\nThis paper addresses the novel unsupervised federated learning problem and introduces a lightweight framework to address it. The lightweight nature of our approach is evident in the following aspects:\n- To minimize the number of parameters optimized in each client and transmitted between clients and the server, we introduce the CLIP image encoder, fixing it during local training. Only the weight parameters of the linear layer classifier are updated and transmitted.\n- To further reduce the number of communication rounds, we initialize the weight parameters of the linear layer classifier with text embeddings associated with class names. This results in a minimal number of local update epochs and communication rounds required for model convergence.\n\nA comprehensive analysis of both computation and communication costs is presented below. Comparing the training of the full model, including the image encoder and the classifier, with the ResNet-50 based CLIP image encoder and a linear layer classifier with 10 classes (e.g., CIFAR-10), the number of parameters in the former is approximately 38 million (M), while the latter is about 10 thousand (K), making a difference of almost 3800 times.\n- Computation Costs: Training the full model typically takes over 100 rounds with 5 local update epochs for each round according to FedAvg and FedNTD. This requires updating a total of $38M \\times 100 \\times 5$ parameters for each client. In contrast, our proposed lightweight method usually takes 10 rounds with 1 local update epoch for each round, involving the updating of only $10K \\times 10 \\times 1$ parameters for each client. The difference is approximately 190K times.\n- Communication Costs: Transmitting the full model incurs a cost of $38M \\times 100 \\times 32$ bits of bandwidth, while transmitting only the linear model costs $10K \\times 10 \\times 32$ bits of bandwidth. The difference is about 38K times.\n\n**3. Results on more datasets**\n\nWe evaluate the proposed methods on three standard datasets: CIFAR-10, CIFAR-100, and CINIC-10, following established methods like FedAvg and FedNTD. These datasets cover various numbers of classes and are widely used for evaluating federated learning methods. While we acknowledge that including larger datasets would provide a more comprehensive evaluation, considering time limitations, we plan to explore this in future works.\n\n**4. Typos**\n\nWe appreciate the reviewer for highlighting these typos, and they have been corrected in the latest revision."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532408354,
                "cdate": 1700532408354,
                "tmdate": 1700533697558,
                "mdate": 1700533697558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]