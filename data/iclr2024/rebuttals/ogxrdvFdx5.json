[
    {
        "title": "ReLU soothes NTK conditioning and accelerates optimization for wide neural networks"
    },
    {
        "review": {
            "id": "vTysvxzalT",
            "forum": "ogxrdvFdx5",
            "replyto": "ogxrdvFdx5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_XQe2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_XQe2"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effect of the ReLU activation function in terms of (i) the data separation in the feature space of the model gradient, and (ii) the conditioning of the NTK. As for (i), Theorem 4.4 proves that, if two network inputs have small angle, then the model gradients become less and less aligned as the depth of the network increases. As for (ii), Theorems 5.2 and 5.3 show that the condition number of the NTK is smaller than that of the Gram matrix containing the inputs (meaning that the NTK is better conditioned). Specifically, Theorem 5.2 looks at the case in which we have only 2 data samples and shows that the condition number decreases with depth; Theorem 5.3 looks at an arbitrary non-degenerate dataset (i.e., inputs not aligned), considers the NTK of a two-layer network where the outer layer is not trained, and proves that the condition number of such NTK is smaller than the condition number of the Gram matrix of the input. All NTK results refer to the infinite width limit. For comparison, in a linear network, data separation and condition number of the NTK do not change with depth. The fact that the NTK is better conditioned has implications on optimization which are shown via numerical results on MNIST, f-MNIST and Librispeech (see Section 6). Numerical experiments also demonstrate the better separation and conditioning of ReLU networks (compared to linear networks)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* While there has been some work on the impact of the activation function on the NTK (mentioned below), the results presented here are new. Specifically, the authors focus on the regime in which the angle between two data points is small and establish what's the effect of the ReLU nonlinearity on the corresponding gradients in the infinite-width limit. \n\n* The numerical results show a similar phenomenology also at finite widths, which is a nice addition.\n\n* The results appear correct (after also looking at the appendix)."
                },
                "weaknesses": {
                    "value": "Overall, although the results are correct and the regime being investigated is new, the findings are a bit underwhelming, due to the restrictive regime in which they hold.\n\nSpecifically, Theorem 4.4 only tracks the input angle which is assumed to be $o(1/L)$. Other relevant parameters such as the input dimension $d$ and the number of samples $n$ are assumed to be constant (which is rather unrealistic in typical datasets). \n\nThe regime being restrictive is even more evident in the NTK results. Theorem 5.2 holds only for two data points. Theorem 5.3 holds for a general two layer network with the outer layer being fixed. However, it implicitly requires that the input dimension $d$ is bigger than the number of samples $n$. In fact, if that's not the case, $G$ is not full rank, and the statement becomes trivial (as the smallest eigenvalue of $G$ is $0$, and it is well known that the smallest eigenvalue of the NTK is bounded away from $0$). Note that having $d>n$ is violated in all the experiments of Section 5. Actually, the numbers reported in Figure 2(b) when $L=0$ are a bit suspicious. I would expect the condition number to be $\\infty$ since $G$ has at most rank $d$. Or am I missing something here?"
                },
                "questions": {
                    "value": "(1) Can the authors comment on the points reported in Figure 2(b) when $L=0$?\n\n(2) A clear way in which the results can be made stronger is to track the quantities $d, n$ in the various results. Having some assumption on the data (e.g., sub-Gaussian tails) may be necessary in order to provide non-trivial statements. Also being able to track the number of neurons $m$ (and therefore consider the empirical NTK) would add value to the narrative of the paper.\n\n(3) There are some works that study the impact of the activation function on the NTK, see [R1], [R2]. How do the results of the manuscript compare to such existing works?\n\n[R1] Panighahi et al., \"Effect of Activation Functions on the Training of Overparametrized Neural Nets\", ICLR 2020.\n\n[R2] Zhu et al., \"Generalization Properties of NAS under Activation and Skip Connection Search\", NeurIPS 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682785439,
            "cdate": 1698682785439,
            "tmdate": 1699636371742,
            "mdate": 1699636371742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ojfIgJn2Q7",
                "forum": "ogxrdvFdx5",
                "replyto": "vTysvxzalT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XQe2: part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. We are grateful that the reviewer recognized the novelty of the work. We also understand your concerns. We would like to address your concerns and questions one by one below.\n\n**W1:** *\u201c... However, it implicitly requires that the input dimension $d$ is bigger than the number of samples $n$. In fact, if that is not the case, $G$ is not full rank, and the statement becomes trivial \u2026\u201d*\n\n**A:** We apologize for not making this point clear in the submission. **In short, we do not require $d>n$.** Technically speaking, it is the \u201c*effective*\u201d NTK condition number (largest eigenvalue divided by the smallest *non-zero* eigenvalue) that controls the convergence rate. When $G$ is not full rank, this relates to the smallest *non-zero* eigenvalue of $G$. We would like to explain this in detail below:\n\nLet\u2019s consider the case $d<n$, where the concern arises. *Why it is the smallest *non-zero* eigenvalue?*\n\n\u2013 In this case, the Gram matrix $G$ is just the NTK of the linear model, and is not full rank. Note that this linear model is in the *under-parameterized regime*. As we know, for the linear model, the NTK $K=XX^T$ has the same spectrum as the Hessian of least square loss $H=X^TX$, except the zero eigenvalues. This Hessian is expected to have full rank, and the least square loss is convex. As is well-known, the condition number is defined as $\\lambda_{max}(H)/\\lambda_{min}(H)$, which is equivalent to $\\lambda_{max}(K)/\\lambda_{min}^*(K)$, with $\\lambda_{min}^*(K)$ as the smallest **non-zero** eigenvalue of NTK. \n\n\u2013 How about the linear network and ReLU network? In the infinite width limit, they are essentially linear models, with the model gradient $\\nabla f(x)$ (instead of original input $x$) as the feature. These linear models are *over-parameterized*, and have a hyper-plane as the solution set $S$. Intuitively, the optimization (by gradient descent) occurs in sub-spaces that are perpendicular to $S$. In addition, the zero eigenspace of NTK are also perpendicular to $S$. Hence, the zero eigenvalues of NTK contribute nothing to the optimization. Therefore, the condition number of optimizational interest is the \u201ceffective\u201d condition number (i.e., ignoring the zero eigenvalues).\n\nIn summary, our results do *not* require $d>n$. Our results essentially rely on the \u201c*effective*\u201d condition number of NTK. We will clarify this point in the revision.\n\n**W2:** *\u201cTheorem 5.3 holds for a general two layer network with the outer layer being fixed\u201d.*\n\n**A:** This setting is commonly used in literature, including very good papers, for example Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. \u201cGradient descent provably optimizes over-parameterized neural networks\u201d. We also observe that many novel findings originally come out in simple settings, which does not affect its completion by later works. We believe that this simple setting is a good starting point. We are aware that for deeper networks it requires much complicated analysis. It is believable the same results hold for deep networks, as partially evident by Figure 2 and Theorem 5.2. \n\n\n**W3:** *\u201cthe numbers reported in Figure 2(b) when $L=0$ are a bit suspicious\u201d & Q1: \u201cCan the authors comment on the points reported in Figure 2(b) when $L=0$?\u201d*\n\n**A:** In the experiment to compute the numbers in Figure 2(b), we actually evaluated NTK based on batches of size $512$. This is due to the limitation by our computational resource, which could not compute and store larger Jacobian matrices. The reported numbers are averaged over the batches. In this case, the Gram matrix $G$ is always full rank, hence it is not surprising that $\\kappa$ is finite. One can anticipate that the condition numbers $\\kappa$ at $L=0$ may increase with larger batch size, and perhaps be infinite if it exceeds $d$. We will clarify this setting in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183558297,
                "cdate": 1700183558297,
                "tmdate": 1700540405433,
                "mdate": 1700540405433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4qG9KvGMAD",
                "forum": "ogxrdvFdx5",
                "replyto": "cLnEHfVHB3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_XQe2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_XQe2"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the response, but some issues remain"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the thoughtful response. However, I disagree with the following statements:\n\n\"Intuitively, the optimization (by gradient descent) occurs in sub-spaces that are perpendicular to $S$. In addition, the zero eigenspace of NTK are also perpendicular to $S$. Hence, the zero eigenvalues of NTK contribute nothing to the optimization. Therefore, the condition number of optimizational interest is the \u201ceffective\u201d condition number (i.e., ignoring the zero eigenvalues).\"\n\nIf the NTK is low rank, then gradient descent can be stuck precisely in the span of the eigenvectors corresponding to the 0 eigenvalues. In fact, all the works proving convergence of gradient descent in the NTK regime either assume or prove that the smallest NTK eigenvalue is bounded away from 0 (including the paper by Du et al. cited by the authors, see their Theorem 3.1). Note also that, if we were to consider the \"effective\" NTK condition number as suggested by the authors, then one could just set to 0 the value of the smallest non-zero eigenvalue. The resulting matrix is a lower bound on the NTK (in the PSD sense), but the condition number has improved. \n\nFor this reason, I don't quite understand the motivation (from an optimization viewpoint) of removing the 0 eigenvalues from the NTK."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484959082,
                "cdate": 1700484959082,
                "tmdate": 1700484959082,
                "mdate": 1700484959082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7M2UnkNZM7",
                "forum": "ogxrdvFdx5",
                "replyto": "vTysvxzalT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the follow up question"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the follow up question. Sorry for the confusion. \n\nPlease note that the NTK (a $n\\times n$ matrix) and loss Hessian (a $p\\times p$ matrix) have different spaces, where $p$ is the number of parameters which is much larger than $n$ in the over-parameterized networks. Also note that it is this $p$-dimensional parameter space that the weights are updated and gradient descent resides in. Hence, we don't quite see how \"If the NTK is low rank, then gradient descent can be stuck precisely in the span of the eigenvectors corresponding to the 0 eigenvalues\", as gradient descent is not in the same space as the eigenvectors.\n\nFor ReLU network, the NTK is indeed of full rank, as shown by Du et al. However, no matter NTK is of full rank or not. As $p\\gg n$ for over-parameterized models, the loss Hessian, $p\\times p$, is low rank and have a lot of zero eigenvalues. Our statement that you quoted is actually talking about the gradient descent in this $p$-dimensional parameter space. \n\nWe hope this can address your concern. We are also happy to discuss if you have further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540329587,
                "cdate": 1700540329587,
                "tmdate": 1700541316738,
                "mdate": 1700541316738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0pT5AVpRzJ",
                "forum": "ogxrdvFdx5",
                "replyto": "7M2UnkNZM7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_XQe2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_XQe2"
                ],
                "content": {
                    "title": {
                        "value": "discussion"
                    },
                    "comment": {
                        "value": "I perfectly agree with the authors in that the NTK is an $n$ by $n$ matrix, while gradient descent acts on a $p$-dimensional space. However, the smallest eigenvalue of this $n$ by $n$ NTK is still what matters for the analysis of gradient descent in the NTK regime. The requirement that this eigenvalue is bounded away from 0 is present in pretty much any paper that does a convergence proof in the NTK regime. I can quote as a representative example Theorem 5.1 in the review paper https://arxiv.org/pdf/2103.09177.pdf\n\nIn short, I still fail to see the importance of the 'effective' condition number. As mentioned above, I can always push to 0 the smallest non-zero eigenvalue, get a lower bound on the NTK in the PSD sense and improve the effective condition number."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645861066,
                "cdate": 1700645861066,
                "tmdate": 1700645861066,
                "mdate": 1700645861066,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hJdl8ABUAv",
            "forum": "ogxrdvFdx5",
            "replyto": "ogxrdvFdx5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_4jiX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_4jiX"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effects of ReLU in the neural tangent kernel regime. Specifically, the authors compare ReLU network with linear network and show that (1) ReLU is able to produce better data separation in the feature space of model gradient and (2) ReLU improves the NTK conditioning. The authors further show that depth is able to further amplify those effects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proof of this work is clean and solid and the presentation of this work is very clear. The idea of analyzing the model gradient feature makes sense and is an interesting subject in kernel learning. I do appreciate the authors' result on providing the exact formula of model gradient angle in Lemma 4.3."
                },
                "weaknesses": {
                    "value": "This work overall gives the reviewer a feeling that it is more or less a direct consequences of [1] as [1] also shows the formula for the angle between post-activations. I applaud the authors for studying Equation (7) and (8) as they are challenging objects. However, the current results (Theorem 4.2 and Theorem 4.4) are only considering the points that are very close to each other $\\Theta(x,z) = o(1/L)$ and if $L$ is big, this quantity is very small. As the authors mentioned, for small $z$, $g(z)$ behaves like identity. Thus, although Theorem 4.2 and Theorem 4.4 is able to show that ReLU improves the data separability, the improvement is also very small in the regime the authors are considering in this paper. Thus, the model gradient angles are nearly non-changing from the input angles. This is also why the improvement for the condition number in Theorem 5.2 can also be very small. It would be more interesting to see ReLU can improve data separability for input pair with small angle (but larger than the regime this paper presents). Further, although Proposition 5.1 is able to connect the model gradient angle with the upper bound of the smallest eigenvalue and the lower bound of condition number, it is also not clear how tight the upper bound is. \n\n[1] Arora, Sanjeev, et al. \"On exact computation with an infinitely wide neural net.\" Advances in neural information processing systems 32 (2019)."
                },
                "questions": {
                    "value": "Another question that can be explored is whether other non-linear activation has the same properties (better data separation and conditioning) as ReLU. Although for activations other than ReLU, it is significantly more challenging to get a close-form formula."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Reviewer_4jiX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713277455,
            "cdate": 1698713277455,
            "tmdate": 1699636371665,
            "mdate": 1699636371665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r760bLkk4W",
                "forum": "ogxrdvFdx5",
                "replyto": "hJdl8ABUAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4jiX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. We would like to address your concerns one by one below.\n\n**W1:** *\u201c... it is more or less a direct consequence of [1] as [1] also shows the formula for the angle between post-activation.\u201d*\n\n**A:** The formula for the angle between post-activation is merely the starting point of our analysis. Note that most of our analysis after this formula is very different from that of [1]. We will cite [1] around this formula in the revision.\n\nWe would like to draw the reviewer\u2019s attention to our main contribution. The paper showed a new advantage of certain non-linear activations \u2013 decreasing the condition number of NTK. Before this, non-linear activations is just known to increase the expressivity. We view this result as an addition to our understanding of the effect of non-linear activation functions. In addition, this new advantage has important implications on the convergence rate.\n\n**W2:** *\u201cthe current results (Thm 4.2 and 4.4) are only considering the points that are very close \u2026 , if L is big, this quantity is very small.\u201d*\n\n**A:** The small data input angle regime is the key and most interesting part. This is because the NTK condition number is mainly affected by its smallest eigenvalue, which in turn is closely related to the smallest angle between points. A tiny change in the smallest angle between points may lead to a non-negligible change in the NTK smallest eigenvalue $\\lambda_{min}$, which may further lead to a significant change in NTK condition number $\\kappa$ (as $\\kappa \\sim 1/\\lambda_{min}$, and $\\lambda_{min}$ is very small).\n\n**W3:** *\u201cAs the authors mentioned, for small $z$, $g(z)$ behaves like identity. \u2026 Thus, the model gradient angles are nearly non-changing from the input angles. This is also why the improvement for the condition number in Theorem 5.2 can also be very small. It would be more interesting to see ReLU can improve data separability for input pair with small angle (but larger than the regime this paper presents).\u201d *\n\n**A:** In the small but positive angle regime, a tiny difference between $g(z)$ and $z$ could make a large difference in the NTK condition number. As discussed above, the NTK condition number scales roughly as $1/z$ for very small $z$. Please take a look at the Figure 2(a), for a numerical verification of this better separation property on real-world datasets. It shows that the ReLU non-linearity can significantly increase the smallest angle between points. Moreover, please take a look at Figure 2(b), the condition number improved significantly, often by about $10^2 \\sim 10^4$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183369376,
                "cdate": 1700183369376,
                "tmdate": 1700183809305,
                "mdate": 1700183809305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xZ92lto2Da",
                "forum": "ogxrdvFdx5",
                "replyto": "r760bLkk4W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_4jiX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_4jiX"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the clarification.\n\nI would like to point out that when I was saying \"the improvement on the condition number can be quite small\" in my original review, I was referring to theorem 5.2, which shows that depth can improve condition number. Notice that theorem 5.2 only shows $\\kappa < \\kappa_{0}$ and it is not clear how big the improvement $\\kappa_{0} - \\kappa$ is. Currently, it seems the improvement is very small. The reason is that the current proof of theorem 5.2 is using theorem 4.4 and theorem 4.4 is only able to show a $o(1)$ improvement in the cosine value of the angles: if we take $\\theta_{\\text{in}} = o(1/L)$, then equation 9 simplifies to $\\cos \\phi = (1-o(1)) \\cos \\theta_{\\text{in}}$. If the data set is normalized, then in the context of the proof of theorem 5.2 on page 19, we have $\\lambda_2(K), \\lambda_2(G) \\geq \\Omega(1)$. However, $|\\lambda_2(K)- \\lambda_2(G)| = o(1)$ which only implies a very small improvement on the condition number. Further, the experiment results show that both the angle and the condition number can improve by quite a bit. Thus, if the authors are able to show a result like $\\kappa_{0} - \\kappa \\geq \\text{some substantial quantity}$ then I am willing to reconsider my evaluation. However, currently, I remain my previous judgement."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693905843,
                "cdate": 1700693905843,
                "tmdate": 1700693905843,
                "mdate": 1700693905843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GkDVWuCYG6",
            "forum": "ogxrdvFdx5",
            "replyto": "ogxrdvFdx5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_hbD5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_hbD5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effect of the ReLU activation on the conditioning of the NTK and the separation of data points when passed through a deep neural network. The authors show that in contrast to linear activations, the ReLU activation causes an increase in angle separation of data points as well as an improvement in the condition number of the NTK. Additionally, this effect scales with the depth of the network. They corroborate their theoretical results with numerical experiments on a variety of datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The main result appears to be fairly novel and interesting. \n2. Numerical experiments corroborate the theory well.\n3. The experiments are thorough and clear."
                },
                "weaknesses": {
                    "value": "1. The writing style and general clarity in some parts of the paper is lacking.\n2. The main theoretical results compare with the baseline of a fully linear network, which is not a very interesting comparison. It does not seem that surprising that a linear model cannot increase angle separation while the ReLU model can. A more interesting comparison would be with other non-linear models like kernel machines for example.\n3. The theory of the improved convergence rates of gradient descent is restricted to the infinite or large width regime where the NTK is constant. However, it is observed in practice that the NTK changes significantly in most interesting cases [1], and this change corresponds to important feature learning. Per my understanding, the theory in this work fails to extend to this changing NTK regime.\n\n[1] - Fort, Stanislav, et al. \"Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel.\" Advances in Neural Information Processing Systems 33 (2020): 5850-5861."
                },
                "questions": {
                    "value": "1. Can the authors comment on possible ways to extend this result to other non-linear models like kernel machines?\n2. Does the conditioning result extend to the finite-width empirical NTK?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Reviewer_hbD5",
                        "ICLR.cc/2024/Conference/Submission4073/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825895291,
            "cdate": 1698825895291,
            "tmdate": 1700697243927,
            "mdate": 1700697243927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g1X5TqKkbN",
                "forum": "ogxrdvFdx5",
                "replyto": "GkDVWuCYG6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hbD5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. We would like to address your concerns one by one below.\n\n**W1:** *\u201cThe writing style and general clarity in some parts of the paper is lacking\u201d*\n\n**A:** Could you provide more details? For example, which part of the paper is not clear to you.  We are happy to revise wherever was not clear. \n\n**W2:** *\u201cThe main theoretical results compare with \u2026 a fully linear network \u2026 is not a very interesting comparison.\u201d*\n\n**A:** First of all, we would like to restate our main contributions here: the paper showed a new advantage of certain non-linear activations \u2013 decreasing the condition number of NTK. (in parallel with the well-known advantage of increasing the expressivity of network functions).\n\nTo show this new advantage in decreasing condition number, it is necessary to compare with the *linear network* that has exactly the same architecture except the non-linear activation. It was this comparison that makes this new advantage evident. A comparison with other non-linear models (e.g., kernel machines) does not support such a claim (although we agree that this kind of comparison is an interesting future direction).\n\nAnalogously, think about the other well-known advantage of non-linear activation \u2013 increasing the expressivity of network function. This better expressivity was made clear by the comparison between non-linear network and linear network, but not between different non-linear activations. \n\nWe would like to point out that this paper is never meant to say the ReLU model is superior to any other models. Instead, we aim to deliver a message on understanding the role of non-linear activations. \n\n**W3:** *\u201cIt does not seem that surprising that a linear model cannot increase angle separation while the ReLU model can.\u201d*\n\n**A:** We are not aware of any similar results in existing works. \n\n**W4:** *\u201cThe theory \u2026 is restricted to the infinite or large width regime\u2026 in practice the NTK changes significantly \u2026\u201d*\n\n**A:** We would like to point out that many recent theoretical works are conducted in this large width regime, for example [1,2,3,4]. This regime, although many people believe impractical, provides a lot of new understanding of modern deep learning. We believe this is a very good starting point. \n\n\n[1]: Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. \u201cReconciling modern machine learning practice and the classical bias\u2013variance trade-off\u201d. PNAS 2019.\n\n[2]: Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013 115, 2021.\n\n[3]: Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. \u201cGradient Descent Finds Global Minima of Deep Neural Networks\u201d. In: International Conference on Machine Learning. 2019.\n\n[4]: Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. \u201cStochastic gradient descent optimizes over-parameterized deep relu networks\u201d. In: arXiv preprint arXiv:1811.08888 (2018).\n\n[5]: Chaoyue Liu, Libin Zhu, and Mikhail Belkin. \u201cLoss landscapes and optimization in over-parameterized non-linear systems and neural networks\u201d. In: Applied and Computational Harmonic Analysis 59 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183315762,
                "cdate": 1700183315762,
                "tmdate": 1700183315762,
                "mdate": 1700183315762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TkUyCKeZ8l",
                "forum": "ogxrdvFdx5",
                "replyto": "g1X5TqKkbN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_hbD5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Reviewer_hbD5"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed clarifications, they are helpful. \n\nIndeed, this is a novel viewpoint in understanding the benefits of non-linear activation functions. However, I still believe that the fact that this theory only applies in the infinite width regime is a significant limitation. Can the authors comment on theoretical or experimental evidence that measures the angle-separation property and conditioning of the changing finite-width NTK as training progresses? This would be an interesting observation to further the impact of the results in this paper. \n\nGiven these points, I am willing to increase my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697227081,
                "cdate": 1700697227081,
                "tmdate": 1700697227081,
                "mdate": 1700697227081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wel8x2cA5C",
            "forum": "ogxrdvFdx5",
            "replyto": "ogxrdvFdx5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_WJEr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4073/Reviewer_WJEr"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies several properties of wide neural networks in the neural tangent kernel (NTK) regime. By comparing the cases with and without the ReLU activation function, it is shown that ReLU has the effects of (i) better data separation, and (ii) better NTK conditioning. These results also indicate that deeper ReLU networks have better effects, and that ReLU activations can accelerate the optimization procedure."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces an interesting perspective in the study of deep neural networks focusing on the angles between data in the feature space.\n- The presentation of the paper is clear.\n- The experiments match theory results well."
                },
                "weaknesses": {
                    "value": "- The major conclusions of this paper (about the advantages of ReLU) are only demonstrated in comparison with linear networks. This makes the results not very strong, as the advantages of non-linear activations over linear networks are fairly clear. \n- Since the comparisons are only made between ReLU networks and linear networks, the results of the paper may not be very comprehensive. For example, in the title, \u201cReLU soothes NTK conditioning\u201d may give readers the impression that ReLU activation has a unique property when compared with other activation functions, which is not really the case. The results would be more comprehensive if the authors can extend the comparison between ReLU and linear activation functions to a comparison between a general class of non-linear activations and the linear activation. \n- The results of this paper may not be very surprising. As the authors mentioned, the major known advantage of non-linear activation is to improve the expressivity of neural networks. It seems that the conclusions of this paper, to a large extent, are still saying the same thing. Better data separation, better NTK conditioning, and faster convergence to zero training loss, all seem to be more detailed descriptions of better expressivity. \n- The impact of the results are not sufficiently demonstrated. For example, it is not very clear what is the benefit to achieve better data separation in the feature space."
                },
                "questions": {
                    "value": "I suggest that the authors should consider address the comments in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4073/Reviewer_WJEr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4073/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699766550885,
            "cdate": 1699766550885,
            "tmdate": 1699766550885,
            "mdate": 1699766550885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hLdjrxuD8H",
                "forum": "ogxrdvFdx5",
                "replyto": "Wel8x2cA5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WJEr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. After carefully reading the review, we think the reviewer\u2019s concerns are largely based on a misunderstanding of the paper. We apologize for not making it crystal clear in the submission. We hope the following explanation can address all the concerns. \n\n**W1:** *\u201cThe major conclusions of this paper \u2026\u2026,as the advantages of non-linear activations over linear networks are fairly clear\u201d.*\n\n**A:** We politely could not fully agree with this point. The advantage of non-linear activations was only **partially** clear. It was only known that non-linear activations increase the expressivity of the network function (i.e, can approximate more complicated functions). However, our paper showcases a new advantage: decreasing the NTK condition number (which in turn induces a faster worst-case convergence rate). **This advantage was not known before.**\n\n**W2:** *\u201cThe comparisons are only made between ReLU and linear networks \u2026\u201d*\n\n**A:** As we clarified above, our major contribution is to showcase an never-noticed advantage of certain non-linear activations.\n\nLet\u2019s make an analogy. Think about the advantage of non-linear activation in increasing expressivity. When talking about this advantage, it was the comparison between non-linear network and linear network that makes this advantage clear. Comparing between different activations does not support this known advantage of increasing expressivity.\n\nSame thing here. To show this new advantage of decreasing condition number, it is necessary to compare with the *linear network* that has exactly the same architecture except the non-linear activation. Comparing different activations only implies which activation might be relatively better or worse (we agree this is an interesting point, but not the main scope of this paper).\n\n**W3:**  *\u201cThe results of this paper may not be very surprising. \u2026 the conclusions of this paper are still saying the same thing. .. all seem to be more detailed descriptions of better expressivity\u201d.*\n\n**A:** We could not agree with this point, either. This new advantage of certain non-linear activation (i.e., better separation, decreasing NTK etc) is absolutely different/independent from the better expressivity. We don\u2019t see any connections between the two, and are not aware of any existing result making this connection. If you think the two advantages are the same, please provide details/evidence. We are happy to discuss this point.\n\n**W4:**  *\u201cIt is not very clear what is the benefit to achieve better data separation in the feature space.\u201d*\n\n**A:** As we elaborated in the paper, the better separation is deeply related to the better NTK conditioning, as well as the faster worst-case convergence rate. Loosely speaking, it is the better separation that leads to the better NTK conditioning. Intuitively, a better data separation in the feature space helps the learning of the model. Think about two similar data points with different labels, which is often hard to be distinguished by a model. With a better data separation in the feature space, it becomes easier for the model to distinguish. We will add a discussion of the intuition in the revision. \n\n\nOverall, we think the newly discovered advantage of certain non-linear activation (i.e., better separation, better NTK conditioning, etc) is important for theoretical understanding of the neural network, and should be inspiring for further research. We hope the reviewer can recognize the novelty and importance of this discovery."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183207035,
                "cdate": 1700183207035,
                "tmdate": 1700183207035,
                "mdate": 1700183207035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]