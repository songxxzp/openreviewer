[
    {
        "title": "PAGER: A Framework for Failure Analysis of Deep Regression Models"
    },
    {
        "review": {
            "id": "r52NsbTvuk",
            "forum": "DTwpuoaea4",
            "replyto": "DTwpuoaea4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4047/Reviewer_Kt6W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4047/Reviewer_Kt6W"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a risk estimation methods that can categorize regression prediction failure into four groups, including ID, Low Risk, Moderate Risk, and High Risk. The paper starts with demonstrating the weakness of epistemic uncertainty based method and show that it is insufficient to evaluate predictive risk when there is Out-of-Support in the modelling. With such motivation, the paper presents its method based on anchoring  predictive model. The transformation of inputs is able to construct a connection between training data points with test data point such that OOS / OOD data could be flagged out through carefully defined functions. In experiments, the paper includes synthetic benchmark function as well as real regression datasets to show the effect of the proposed method. Baselines are those epistemic uncertainty based solutions. Evaluation metrics are somewhat not standard, which is probably introduced by this paper itself. Thus, I cannot tell if the experimental result is indeed trustworthy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper focus on the risk estimation of regression tasks, which is less explored compared to classification tasks. While uncertainty estimation based solution has been there for a long time, there were less contribution made in this field that is not based on uncertainty estimation. This makes the paper stand out as a novel contribution.\n2. The paper is fairly well written with sufficient evidence to support its claim. While some concepts may be a bit hypothetical (I mean not really possible to distinguish between OOD OOS in practice), the motivation and solution are well connected."
                },
                "weaknesses": {
                    "value": "1. The solution is based on anchoring predictive model. I realized the novelty majorly comes from the nature of the anchoring predictive model, which makes me concerning the contribution this particular paper (as it is more incremental now). In addition, how does people use this method to measure regression risk if they don't use anchoring model (which more likely happen in real world)?\n2. Multiple concepts introduced in this paper are hypothetical and may not be possible to know in practice. E.g. OOS / OOD are hard to distinguish.\n3. The evaluation metric introduced in this paper is hard to be convincing in terms of the setting with 20 percentile or 90 percentile in the False Positive and $C_{low}$. Why? What makes the authors to choose those numbers. Is that the real reason of better performance compared with existing solutions?\n \nMinors: \nType: Section 3.3, False Negative (FP) should be False Positive."
                },
                "questions": {
                    "value": "My questions are included in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760601300,
            "cdate": 1698760601300,
            "tmdate": 1699636368207,
            "mdate": 1699636368207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g5j2YjMZZY",
                "forum": "DTwpuoaea4",
                "replyto": "r52NsbTvuk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Kt6W"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback and for raising important questions. Here is our detailed response. If our responses have adequately addressed the concerns, we request the reviewer to increase their rating to help champion our paper.\n\n> **Novelty and Implications of using Anchoring**\n\nWhile our solution is rooted in the established anchoring framework, this paper introduces several noteworthy contributions. \n+ We uncover a crucial insight that **uncertainties alone are insufficient** for a comprehensive characterization of failures in regression models. \n+ We advocate for incorporating **manifold non-conformity**, emphasizing adherence to the joint data distribution, as a valuable complement to uncertainty estimates. \n+ In an extension beyond the existing anchoring framework, we present a surprising finding that **non-conformity can be achieved through reverse anchoring** without the need for auxiliary models. \n+ Additionally, we propose a flexible analysis of model errors by introducing the concept of **risk regimes**, avoiding the necessity for a rigid definition of failure (such as an incorrect label) and the need for additional calibration data commonly seen in classification problems. \n\nConcerning the utilization of anchored models as the foundational framework, we believe that it is neither a weakness nor a limitation of PAGER. In this regard, we would like to offer two practical observations:\n\n+ **Anchoring does not necessitate any adjustments to the optimizer, loss function, or training protocols**. The sole modification lies in the input layer, requiring additional dimensions for vector-valued data or channels for images, and a modest addition of parameters to the first layer. This approach is adaptable to any architecture, be it a Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), or Vision Transformer (ViT). Recent research indicates that **incorporating anchoring not only aligns with test performance but can even enhance generalization to out-of-distribution (OOD) regimes** [1,2].\n+ Even in situations involving pre-trained models, it is feasible to **train solely an anchoring-based regression head attached to a conventionally trained feature extractor backbone**, effectively implementing PAGER. Optionally, one may choose to fine-tune the feature extractor concurrently with the anchored regression head in an end-to-end manner, adhering to standard practices.\n\nAs an illustration, in the experiment on CIFAR-10 rotation angle prediction under the Gaps setting with $\\mathtt{Score1}$, we considered a variant, where we exclusively trained an anchored regression head while keeping the feature extractor frozen. Note, the feature extractor was obtained through standard training on the same dataset. Our findings reveal that even with this approach, the **performance is comparable to that of a fully anchored ResNet-34 model**. We have updated the paper with this result (Appendix A.6)\n\n| Method                          | FN       | FP       | $C_{\\text{low}}$ | $C_{\\text{high}}$ |\n|---------------------------------|----------|----------|------------------|-------------------|\n| DEUP                            |   14.9   |   15.22  |       18.81      |       27.50       |\n| Full Anchored Training          | **3.34** | **7.86** |     **3.28**     |      **5.34**     |\n| Anchored Regression Head (only) |  _4.78_  |  _8.33_  |      _4.96_      |       _6.05_      |\n\n\n> **Definitions for OOD and OOS**\n\nThank you for your feedback. Our definitions of OOD (Out of Distribution) and OOS (Out of Support) are directly derived from existing literature [2], where the characterization is rooted in various regimes from which the test samples originate. Although an alternative variation, Out-of-Combination (OOC), has also been considered, in our framework, we categorize it within the broader umbrella of Out-of-Support data. Alongside these definitions, we have incorporated Figure 2 to offer readers a visual representation for enhanced clarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699725405827,
                "cdate": 1699725405827,
                "tmdate": 1699733288666,
                "mdate": 1699733288666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AVZGSMfnbk",
                "forum": "DTwpuoaea4",
                "replyto": "pisfkiSxat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4047/Reviewer_Kt6W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4047/Reviewer_Kt6W"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I will keep my current score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659438928,
                "cdate": 1700659438928,
                "tmdate": 1700659438928,
                "mdate": 1700659438928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r7sZFaQ30T",
            "forum": "DTwpuoaea4",
            "replyto": "DTwpuoaea4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4047/Reviewer_HtuK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4047/Reviewer_HtuK"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author provide a new framework for failure mode analysis in regression problem where they can identify different regions in the input space based on the model's generalization. They use the idea of anchoring and provide uncertainty and non-conformity scores to test samples. Based on these two scores, they categorize samples. They further evaluate their method and compare it to existing methods based on FN in detecting high-risk samples, FP in detecting low-risk samples, and confusion error in both low and high-risk regions. Their method brings improvement over the existing results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The observation that an uncertainty score is insufficient and a complementary non-conformity score can better organize different regions is interesting.\n+ The idea of using anchors to obtain uncertainty and non-conformity is interesting.\n+ Evaluation is comprehensive and includes various methods on different datasets.\n+ Propose method ($\\text{score}_1$) is efficient and is faster than existing work in inference time."
                },
                "weaknesses": {
                    "value": "+ $\\text{score}_2$ is slower than $\\text{score}_1$ in inference time, but it is not significantly better than $\\text{score}_1$ in all settings; I'd expect to get more information about why this method should be used and what settings it performs better."
                },
                "questions": {
                    "value": "i'd like to get more insight about the usefulness of $\\text{score}_2$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767770145,
            "cdate": 1698767770145,
            "tmdate": 1699636368121,
            "mdate": 1699636368121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r5WKShVqHh",
                "forum": "DTwpuoaea4",
                "replyto": "r7sZFaQ30T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer HtuK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work.  If our responses have adequately addressed the concerns, we request the reviewer to increase their rating to help champion our paper.\n\n> **Utility of $\\mathtt{Score2}$ and Comparison to $\\mathtt{Score1}$**\n\nConceptually, $\\mathtt{Score2}$ employs a more rigorous optimization approach to **effectively address test scenarios in which both a test sample and its residual are individually out-of-distribution (OOD), not just their combination, with respect to $P(X)$ and $P(\\Delta)$**. While the computationally efficient $\\mathtt{Score1}$ tends to perform comparably (or even better) to $\\mathtt{Score2}$ under challenging extrapolation conditions (such as gaps and tails in the target variable), $\\mathtt{Score2}$ often demonstrates noteworthy enhancements in confusion scores (Clow and Chigh). This becomes particularly valuable in practical applications **where users cautiously identify failures (Top K%) and it is essential to prevent the misclassification of some high-risk samples as moderate risk**.\n\nAnother scenario where $\\mathtt{Score2}$ can be very **useful is when the test samples are drawn from a different distribution compared to training (referred to as covariate shifts)**. To demonstrate this hypothesis, we repeated the CIFAR-10 rotation angle prediction experiment by applying *natural image corruptions (defocus blur and frost) at different severity levels*. Interestingly, we observed significant improvements in both false positive (FP) and false negative (FN) scores with $\\mathtt{Score2}$ as the severity of corruption increased. In summary, while $\\mathtt{Score1}$ excels in scalability and is well-suited for online evaluation, $\\mathtt{Score2}$ proves advantageous in addressing challenging testing scenarios. We have updated the paper with these results (Appendix A.5).\n\n|            \t|                   \t| **Defocus Blur** \t| **Defocus Blur** \t| **Defocus Blur** \t|  **Frost** \t|  **Frost** \t|  **Frost** \t|\n|---------------|-----------------------|-------------------|-------------------|-------------------|---------------|---------------|---------------|\n| **Metric** \t|     **Score**     \t|    **Sev. 1**    \t|    **Sev. 3**    \t|    **Sev. 5**    \t| **Sev. 1** \t| **Sev. 3** \t| **Sev. 5** \t|\n|     FP     \t| $\\mathtt{Score1}$ \t|     **8.05**     \t|       10.29      \t|       18.54      \t|  **7.81**  \t|  **8.80**  \t|    16.52   \t|\n|     FP     \t| $\\mathtt{Score2}$ \t|       9.28       \t|     **9.95**     \t|     **14.08**    \t|    9.06    \t|    9.58    \t|  **11.76** \t|\n|     FN     \t| $\\mathtt{Score1}$ \t|     **3.65**     \t|       5.85       \t|       19.81      \t|    3.77    \t|    6.12    \t|    15.63   \t|\n|     FN     \t| $\\mathtt{Score2}$ \t|       3.92       \t|     **5.05**     \t|     **14.99**    \t|    4.01    \t|  **5.68**  \t|  **11.14** \t|"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699724964129,
                "cdate": 1699724964129,
                "tmdate": 1699729669757,
                "mdate": 1699729669757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PvpA1lvMtz",
                "forum": "DTwpuoaea4",
                "replyto": "r5WKShVqHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4047/Reviewer_HtuK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4047/Reviewer_HtuK"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for providing explanations and experiments regarding $\\\\text{Score2}$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581298544,
                "cdate": 1700581298544,
                "tmdate": 1700581298544,
                "mdate": 1700581298544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fKUGe3oO3y",
            "forum": "DTwpuoaea4",
            "replyto": "DTwpuoaea4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4047/Reviewer_LqcK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4047/Reviewer_LqcK"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the need for proactively detecting potential prediction failures in AI regression models. The author introduces a framework called PAGER that combines epistemic uncertainties and non-conformity scores to systematically detect and characterize failures in deep regression models. PAGER is shown to effectively identify areas of accurate generalization and detect failure cases in various scenarios."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The presentation of the paper is clear, and it is insightful to characterize different failures of models in a unified framework.\n\n2. The idea of unifying epistemic uncertainties and complementary non-conformity scores is reasonable.\n\n3. The experimental results verify that PAGER achieves great improvement in failure analysis of deep regression models on synthetic and real-world benchmarks."
                },
                "weaknesses": {
                    "value": "Can the framework proposed in this paper for analyzing model failures be applied to multi-class classification tasks, and what are the potential differences between it and the regression task studied in the paper?"
                },
                "questions": {
                    "value": "Please refer to [Weaknesses]."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4047/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4047/Reviewer_LqcK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833382383,
            "cdate": 1698833382383,
            "tmdate": 1699636368057,
            "mdate": 1699636368057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H1lSOIIbja",
                "forum": "DTwpuoaea4",
                "replyto": "fKUGe3oO3y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer LqcK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback about the paper.  If our responses have adequately addressed the concerns, we request the reviewer to increase their rating to help champion our paper\n\n> **Utility of PAGER on Multi-class Classifier Models**\n\nThanks for raising this important question. \n\nIn classification tasks, incorrectly assigned labels are considered failures. Therefore, detecting failure is framed as **assessing the probability of an inaccurate label prediction**. State-of-the-art approaches typically employ auxiliary predictors, trained retrospectively, to correlate various sample-level scores (such as uncertainties, smoothness, and disagreement across multiple hypotheses) with the likelihood of an incorrect prediction. This approach inherently depends on extra labeled calibration data and has implications for the generalization of the learned failure detector in the face of shifts.\n\nA key difficulty in characterizing failures in regression tasks stems from the **variability in the permissible tolerance levels on error estimates**, which can differ across use-cases. This is in contrast to the clear-cut definition of failure in classification tasks. Addressing this challenge, PAGER proposes to organize samples into various risk regimes rather than relying on a binary pass-fail (0-1) categorization. This approach not only allows for a nuanced assessment without the need for a predefined, rigid definition of failure, but also eliminates the necessity for labeled calibration data. It\u2019s worth noting, however, that state-of-the-art methods like DEUP, which seek to emulate failure detectors from the classification literature by fitting an auxiliary predictor to estimate the loss, exhibit poor performance in practice.\n\nUltimately, while the risk regime characterization approach introduced by PAGER may not find direct applicability in the existing frameworks for failure detection in deep classifiers, it\u2019s crucial to underscore that the proposed uncertainty and non-conformity scores can serve as alternatives to widely used scoring functions for training the auxiliary predictor. Nevertheless, unlike the implementation in PAGER, adopting these scores in current classifier failure detectors would necessitate access to additional calibration data. **We have added a discussion to the paper (Appendix A.7), emphasizing the importance of developing calibration-free failure detectors, even in the context of multi-class classification**."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699724712387,
                "cdate": 1699724712387,
                "tmdate": 1699729689341,
                "mdate": 1699729689341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]