[
    {
        "title": "MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions"
    },
    {
        "review": {
            "id": "zJWiXVujvB",
            "forum": "bM6LUC2lec",
            "replyto": "bM6LUC2lec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3959/Reviewer_aF1D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3959/Reviewer_aF1D"
            ],
            "content": {
                "summary": {
                    "value": "For proteins that do not possess abundant homologous families, the extent of multiple sequence alignment (MSA) continues to be a limiting factor for protein structure prediction. This paper proposes the MSA-Generator, a protein language model that is trained in a self-supervised manner. The MSA-Generator utilizes attention mechanisms that are specific to proteins, enabling it to leverage extensive protein databases for the purpose of generating virtual MSAs that are informative.  The results on CASP14 and CASP15 benchmarks demonstrate significant enhancements in the Local Distance Difference Test (LDDT) scores, particularly for difficult sequences. The generated virtual MSAs re valuable for the enhancement of performance for both AlphaFold2 and RoseTTAFold."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper formulates the MSA generation problem to be an unsupervised sequences-to-sequences (seqs2seqs) task, and proposes the MSA-Generator. Nearly half of the generated virtual MSAs even outperform the real searched MSAs. MSA-Generator can enhance the performance of RoseTTAFold and AlphaFold2 even compared with other MSA generation methods on CASP14 and CASP15."
                },
                "weaknesses": {
                    "value": "Overall, I really appreciate the results of your experiments. The weakness/questions are listed as:\n1. It seems that the method of seqs2seqs pretraining is important in MSA generation. Can it be applied in protein design?\n2. From Table 1, it seems the proposed method gets marked improvements in both metrics across both RoseTTAFold and AlphaFold2 on CASP14 and CASP15. Do you have any ideas and insights for CASP16? \n3. The proposed MSA-Generator includes Tied-Row Attention, Cross-Row Attention, and Self/Cross-Column Attention, which may have appeared in previous models like AlphaFold2. However, it is promising that the proposed model can obtain remarkable improvements on CASP14 and 15. For more ablation studies, could you provide results to illustrate which part is crucial and explain why it works for the proposed Tied-Row Attention, Cross-Row Attention, and Self/Cross-Column Attention?\n4. In fig. 4 (b), what is the meaning of the dashed lines? Why do they seem thin in the middle and thick on the sides? In fig.4 (c), why are there more improvements if intervals are less than 10 for virtual over baseline and virtual over real?\n\n5.Where are the codes, do you have any plans to public code?\n\nTypos: The challenge of protein structure prediction (PSP), a pivotal issue in structural biology, has has\nexperienced transformative progress due to the deep learning revolution: two has."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Reviewer_aF1D"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504319677,
            "cdate": 1698504319677,
            "tmdate": 1700472974973,
            "mdate": 1700472974973,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1ifJx0kVmI",
                "forum": "bM6LUC2lec",
                "replyto": "zJWiXVujvB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3959/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Answer 1\n\nCertainly, this is an insightful query. Indeed, the seqs2seqs pretraining method can be applied in protein design. A following study, titled \"Protein generation with evolutionary diffusion: sequence is all you need,\" showcases this application. This research involves a diffusion framework that has been pretrained on extensive MSA datasets. It focuses on a sequence-driven design strategy, moving away from the conventional structure-function approach in protein engineering.\n\n## Answer 2\n\nIn our study, we strategically chose CASP14 and CASP15 as they represent the latest and most renowned benchmarks in protein structure prediction. Our decision was guided by the prominence of these benchmarks in the field. To the best of our knowledge, CASP16 has not been released at the time of our research. However, we are confident in the applicability of our method to future benchmarks, as evidenced by our encouraging results on CASP14 and CASP15. It's important to note that our current model is relatively modest in scale, with limitations in both parameter size and the extent of the pretraining dataset. We anticipate significant enhancements in performance with the expansion of our model, both in terms of its complexity and the breadth of training data.\n\n## Answer 3\n\nAlphaFold2 indeed utilizes an attention mechanism; however, its application differs from ours as it is not designed for generative purposes and thus does not incorporate a cross-attention mechanism. Our proposed MSA-Generator, in contrast, is specifically tailored to leverage various forms of attention to enhance generative capability.\n\n1. **Tied-Row Attention:** This component, whose efficacy is supported by research in the MSA Transformer field, is critical in our framework. It helps in accurately capturing the evolutionary information present in MSAs, a key aspect that drives our model's performance.\n2. **Cross-Row Attention:** This feature plays a vital role in reducing computational complexity. It is essential for the decoder to efficiently process the entire MSA input, enabling our model to handle large-scale data more effectively.\n3. **Self-Column Attention:** Consistent with prevailing insights from MSA-related studies, we recognize self-column attention as crucial for accurately modeling MSAs. Our framework incorporates this to better understand intra-column relationships, thereby enhancing prediction accuracy.\n4. **Cross-Column Attention:** We have specifically designed this to uphold the inductive bias related to the conservation property of MSAs. It adds a layer of sophistication to our model, allowing it to capture subtle but crucial evolutionary patterns.\n\nRegarding the possibility of conducting ablation studies, it is important to note that our framework's components are deeply integrated. Removing any one of them would not only diminish the model's effectiveness but also require extensive pretraining on large datasets. Hence, conducting such studies is not feasible within our limited computation resource in this period. We do, however, acknowledge the importance of these studies and plan to undertake them in our future work. This will allow us to further validate the individual contributions of each component to the overall effectiveness of our model.\n\n## Answer 4\n\nIn Fig. 4(b), created using *GraphPad Prism 9*, the solid line represents the linear regression equation, while the dashed lines indicate the 95% confidence intervals, automatically plotted by the software.\n\nRegarding your observant question about the performance in certain cases, it's true that sequences with a baseline depth of 5 occasionally outperform those with a real depth of 15, as seen in examples like T1035-D1, T1074s1-D1, T1048-D1 (refer to Tables 5 and 6 in the Appendix). However, generally speaking, there is a proportional relationship between depth and lddt on average.\n\n## Answer 5\n\nCertainly, we release our codebase including inference and pretraining for future research. Here is the link to the anonymous codebase: https://anonymous.4open.science/r/MSA-Augmentor-E2D7/README.md"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059306690,
                "cdate": 1700059306690,
                "tmdate": 1700059306690,
                "mdate": 1700059306690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cej4w66Mgl",
                "forum": "bM6LUC2lec",
                "replyto": "1ifJx0kVmI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3959/Reviewer_aF1D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3959/Reviewer_aF1D"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. Although there are still some areas that I may not fully understand, it does not significantly impact my overall impression. The thorough results, analysis, and the inclusion of code examples have instilled confidence in me regarding the quality of this work. In particular, I appreciate the significant improvements made in the presentation and experimental aspects compared to the initial version. Given the promising results and your confidence in the applicability of this method to future benchmarks such as CASP16, I am inclined to increase my score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472927148,
                "cdate": 1700472927148,
                "tmdate": 1700472927148,
                "mdate": 1700472927148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mXZhTSh5cL",
            "forum": "bM6LUC2lec",
            "replyto": "bM6LUC2lec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3959/Reviewer_b6rT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3959/Reviewer_b6rT"
            ],
            "content": {
                "summary": {
                    "value": "The authors emphasize the significance of Multiple Sequence Alignments (MSAs) in the prediction of protein structures. Their experiments demonstrate that a decrease in the quantity of MSAs leads to a decline in prediction accuracy. Consequently, they introduce a transformer-based model designed to generate protein sequences from a specific MSA dataset. The experimental results indicate that incorporating augmented MSAs can enhance the prediction performance of AlphaFold2."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The topic holds significant relevance in today's scientific landscape, as protein engineering continues to gain prominence as a cutting-edge field of research. \n- This method is straightforward to implement, making it accessible and efficient for integration."
                },
                "weaknesses": {
                    "value": "The authors' proposed task shares a fundamental similarity with the task of modeling the distribution of a given MSA. While their approach shows promise, it is crucial to provide a more comprehensive perspective and clarify the novelty of their contribution. Unfortunately, the manuscript overlooks several pertinent references, including:\n1. McGee, F., Hauri, S., Novinger, Q. et al. The generative capacity of probabilistic protein sequence models. Nat Commun 12, 6302 (2021).\n2. Riesselman, A.J., Ingraham, J.B. & Marks, D.S. Deep generative models of genetic variation capture the effects of mutations. Nat Methods 15, 816\u2013822 (2018).\n3. Sinai, S., Kelsic, E., Church, G. M. & Nowak, M. A. Variational auto-encoding of protein sequences. NeurIPS 2017 MLCB Workshop\n\nOnce a distribution for a specific MSA is acquired through learning, sampling from this distribution becomes an efficient method for generating multiple protein sequences that share similarities. \n\n---\n\nExamining Figure 4a, it's not immediately clear whether the depth or quality of the MSA holds more significance. For example, if we were to introduce random protein sequences into the MSA, we are uncertain how this would impact performance. This calls for a more in-depth analysis to understand the relationship between MSA depth and quality and their potential trade-offs in achieving optimal results. As an example, if we were to introduce random protein sequences by masking certain sequences within the original MSA, can we also achieve good performance? \n\n---\n\nMy primary concerns arise when I examine Figure 7. I'm curious about the methodology behind calculating the conservation and quality metric. It seems that these scores are derived solely from the provided sequences, such as the six sequences for T1093-D1. Consequently, I question the significance of these metrics. \n\nOne could argue that it's possible to achieve high scores for conservation, quality, or other metrics by simply inspecting the original MSA, identifying the region with the highest conservation metric, and selecting it as a segment. This segment could then be augmented by inserting arbitrary amino acids at random positions occasionally. Such a process could potentially yield perfect conservation scores, quality scores, and so on.\n\nThis concern also ties into my second question: What would the pLDDT score be if I were to employ the aforementioned approach to augment the original MSA?\n\n---\n\n\nIn Section 4.5, the authors make a comparison with Iterative Unmask; however, they do not provide a comparison of the pLDDT scores using this baseline method. Additionally, there is some ambiguity regarding the dataset used for the analysis. It remains unclear which sets of sequences were used by the authors to construct the Position-Specific Scoring Matrix (PSSM) and what the correlation coefficients are when employing Iterative Unmask."
                },
                "questions": {
                    "value": "Please refer to the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Reviewer_b6rT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734566968,
            "cdate": 1698734566968,
            "tmdate": 1700796981637,
            "mdate": 1700796981637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "97Ph8jcSVr",
                "forum": "bM6LUC2lec",
                "replyto": "mXZhTSh5cL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3959/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Answer 1\n\nWe appreciate the reviewer's suggestions and will include them in our revision. Our work differs from the VAE-based methods mentioned in key ways:\n\n1. Our method is tailored for MSA generation, emphasizing unsupervised generative pretraining for zero-shot downstream tasks. This is a capability not present in the cited VAE-based methods, which demonstrate a limited capacity in producing effective sequences **across diverse domains**.\n2. Unlike VAE works focused on evaluating and simulate protein mutations and their functional implication, our focus is on novel MSA generation for improved protein structure prediction. We have included Potts model which can generate MSAs in our comparison.\n3. Our main contribution is innovative pretraining tasks and a large-scale pretrained model ready to use for enhancing AlphaFold2 and others, distinct from prior research.\n4. Learning and sampling from MSA distributions is more complex than it appears within VAE framework, especially with the diversity of protein families and long protein sequences. We did not see improvement on structure prediction task in relevant works. Inspired by NLP advancements, we provide an effective solution for MSA generation, directly confronting these complexities.\n\n## Answer 2\n\nThe link between MSA depth, quality, and downstream performance is clear. For details, see AlphaFold2's paper \"Highly accurate protein structure prediction with AlphaFold\", specifically Fig 5 (a), discussing how shallower MSA depth reduces quality and impairs downstream tasks. This was our initial motivation.\n\nOur Fig 4(a), a violin plot, demonstrates that a baseline depth of 5 generally underperforms against real and virtual depths of 15. This is evident from the blue distribution (Baseline depth=5) being more concentrated in lower scoring areas compared to the yellow (Real depth=15) and red (Virtual depth=15).\n\nRegarding random protein sequences, our 'Iterative Unmasking' baseline already employed a similar concept, unmasking random tokens with the MSA Transformer. Yet, as Table 1 shows, this didn't significantly improve results. Addressing your concern, we tested introducing random sequences by replacing 5%, 10%, and 15% of tokens, increasing MSA depth. This worsened downstream performance, confirming our approach's effectiveness.\n|  | CSAP14-plddt | CSAP14-lddt | CSAP15-plddt | CSAP15-lddt |\n| --- | --- | --- | --- | --- |\n| AlphaFold2 | 65.1 | 53.2 | 65.1 | 55.6 |\n| AlphaFold2+5%random | 64.3 | 50.6 | 62.3 | 50.9 |\n| AlphaFold2+10%random | 60.5 | 49.5 | 59.6 | 43.2 |\n| AlphaFold2+15%random | 55.6 | 44.3 | 53.8 | 39.6 |\n| AlphaFold2+Ours | 71.6 | 57.5 | 73.7 | 63.7 |\n\n## Answer 3\n\nWe would like to clarify that Figure 7 in the Appendix serves **purely for visualization purposes** of the MSAs generated and is **not a basis for analytical evaluation in our study**. For visualization, we use Jalview, a standard tool that automatically calculates metrics like quality and consensus. However, **these metrics are not the focus of our experiments in the main content of the paper**. They are relevant only for single MSA analysis and **do not provide an accurate assessment of our methods**. We have **not employed these metrics in any part of our research presented** in the paper. The Diversity and Conservation metrics we use are derived from PSSM and Shannon Entropy, as detailed in section 4.5, and Figure 7 is not connected to this analysis.\n\nThese metrics are calculated per column and don't indicate overall MSA quality and conservation. Our model doesn't enforce conservation but learns it via cross-column attention. Fig 7's metrics don't show overall MSA quality, and we didn't target them specifically. We've removed this unclear part in our revision.\n\nIn our prior response, we detailed outcomes for sequences generated via random replacement; see those for details.\n\n## Answer 4\n\nSection 4.5 showcases the diversity of generated MSAs, **focusing on analyzing these MSAs directly** instead of downstream tasks like protein structure prediction. Thus, we omitted a comparison of pLDDT scores here, but it's available in section 4.3 and table 1.\n\nTo better clarify:\n\n- We describe that '*The results, derived from 10 runs following section 4.3*' showing dataset and strategies are consistent with Section 4.3.\n- The construction of the Position-Specific Scoring Matrix (PSSM) is detailed in the middle part of Section 4.5 as \u201c*we examined the PSSM of both **original** and **MSA-Generator generated MSAs***\u201d\n- To measure the similarity between generated and original MSAs' PSSMs, we used the **Pearson correlation coefficient**. Below is the pseudocode for this computation:\n\n```python\ngen_pssm = calculate_pssm(gen_msas_seqs[i], bg_freqs)\nsource_pssm = calculate_pssm(source_msas_seqs[i], bg_freqs)\ncorrelation = np.corrcoef(source_pssm.ravel(), gen_pssm.ravel())[0, 1]\n```\n\nwe have revised our draft for clear presentation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059172532,
                "cdate": 1700059172532,
                "tmdate": 1700059172532,
                "mdate": 1700059172532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MH5yxOrpSL",
            "forum": "bM6LUC2lec",
            "replyto": "bM6LUC2lec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3959/Reviewer_ix3n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3959/Reviewer_ix3n"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces MSAGenerator, a novel self-supervised generative protein language model aimed at advancing protein structure predictions. Drawing inspiration from the groundbreaking achievements of models such as AlphaFold2 in protein structure prediction, the paper addresses the key challenge of the paucity of deep multiple sequence alignments (MSA) for proteins that have limited homologous families. The MSAGenerator, pre-trained on a sequences-to-sequences task, employs an automatically constructed dataset and leverages protein-specific attention mechanisms. This design enables it to generate informative, virtual MSAs that enrich insufficient MSAs, thereby elevating the accuracy of predictions. The paper validates the efficacy of the MSAGenerator using the CASP14 and CASP15 benchmarks, indicating notable improvements in LDDT scores. These improvements are especially significant for sequences that are inherently challenging, and the results further emphasize the enhancement of performance for both AlphaFold2 and RoseTTAFold."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The study delivers promising results, indicating the potential of MSAGenerator to advance the field.\n2. The problem of limited or poor-quality MSAs for certain proteins is well-recognized, and the paper's focus on this issue is timely and relevant."
                },
                "weaknesses": {
                    "value": "The paper does not provide clear details on the benchmarking process, especially regarding the selection of search databases and the parameters used. I would consider raising my score if the authors address these crucial issues."
                },
                "questions": {
                    "value": "- The underlying rationale for the superiority of the MSAGenerator over traditional MSA search methods remains ambiguous, especially given that the pretraining of the model is based on the outputs from MSA searches. This warrants further elucidation.\n\n- There is a disparity in the choice of databases: the paper mentions using the UniRef50 database for pretraining and then switches to UniRef90 for testing. The reasons for this choice are not explained, and it raises questions about the potential variation in results had UniRef50 been employed for both stages.\n\n- The introduction of an \"artificial challenging MSA\" requires a more comprehensive explanation. Its significance and the rationale behind its inclusion in the study are unclear. Figure 4 a seems to depict an imbalanced comparison. If homologous sequences were present during the pretraining phase but not considered for the baseline, it would be an unfair representation. This suggests that the MSAGenerator's performance may not surpass real MSAs.\n- For Figure 4c, it is crucial to also represent the \"Real over Virtual\" comparison to provide a comprehensive view(negative interval)\n- Fig4 Queries arise regarding the consistency of the reported improvements when other protein structure prediction methods, like RoseTTAFold, are used.\n- In Table 1, the specific MSA methodology employed in the benchmarking process, as well as the precise parameters and databases used for MSA searches, are not detailed. It remains unclear if these are consistent with the pretraining stage."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3959/Reviewer_ix3n"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871766579,
            "cdate": 1698871766579,
            "tmdate": 1700802601834,
            "mdate": 1700802601834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QCQrTJnxZY",
                "forum": "bM6LUC2lec",
                "replyto": "MH5yxOrpSL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3959/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3959/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Answer 1\n\nTo clarify, our approach is **not intended to replace actual multiple sequence alignments (MSAs) with synthetic alternatives**. As we highlighted in the introduction of our manuscript, our main focus is on proteins characterized by a limited number of homologous sequences. In these instances, traditional MSA search algorithms often fail to construct comprehensive alignments. Our methodology, therefore, arises out of necessity: **we generate the required MSAs when existing data is insufficient**. An illustrative example is the protein T1122 from CASP15, identified in the Tipula oleracea nudivirus. The scarcity of homologous sequences for this protein in existing databases presents a significant obstacle in constructing MSAs, which are vital for the analysis of protein structure and function.\n\nOur MSAGenerator is designed to specifically address this gap, generating virtual sequences to effectively supplement real sequences, especially in scenarios where actual sequence acquisition is challenging or not feasible. Notably, our model, pretrained on a diverse array of searched sequences, is adept at learning complex co-evolutionary patterns within MSAs. This learning approach enables it to generalize to a broader domain, similar to the capabilities of a Large Language Model. This functionality significantly enhances downstream structural predictions, a claim substantiated by the experimental results we present.\n\n## Answer 2\n\nThank you for pointing out this typo. We apologize for any confusion caused by the apparent inconsistency in our manuscript. To clarify, we consistently used the UniRef90 database for both the pretraining and testing phases of our study, UniRef50 was not employed throughout our study\n\n## Answer 3\n\nThe concept of the *artificial challenging MSA* is introduced in our study to critically assess the efficacy of our generated MSAs in comparison with real MSAs. We wish to clarify that our intention is not to **suggest the exclusive use of virtual MSAs over real ones, nor do we claim that virtual MSAs are inherently superior** (please refer to response 1)**.** Our primary objective is to demonstrate that virtual MSAs can closely approximate real ones in downstream tasks.\n\nThe inclusion of this comparison is vital to establish the utility of our MSAGenerator in situations where real MSAs cannot be obtained, as discussed in section 4.3. Before proposing the use of virtual MSAs in such contexts, it is essential to validate their effectiveness in comparison to real MSAs derived from traditional search methods. We have renamed this section into \u201cAre Virtual MSAs as good as Real MSAs?\u201d for better presentation.\n\nWe also want to emphasize that none of the testing sequences were included in any part of our pretraining data. This ensures a fair and unbiased assessment of the MSAGenerator\u2019s performance\n\n## Answer 4\n\nThank you for your advice. We have incorporated the suggestion of providing a negative interval, and this has been included in the revised draft of our manuscript\n\n## Answer 5\n\nWe appreciate your inquiry regarding the consistency of our reported improvements when using different protein structure prediction methods, such as RoseTTAFold. In Section 4.2 of our paper, our primary goal is to demonstrate the quality of the MSAs generated by our method. This section is not intended to suggest the replacement of real MSAs with virtual MSAs when real ones are available. We chose to focus on AlphaFold2 (AF2) for these experiments because it generally outperforms RoseTTAFold.\n\nHowever, recognizing the validity of your concern, we have expanded our analysis in Section 4.3, the core results section of our paper. Here, we conduct experiments using both RoseTTAFold and AF2. This section specifically addresses scenarios where no homologues are identifiable using traditional search algorithms. By including both methods in our analysis, we aim to provide a more comprehensive validation of the effectiveness of virtual MSAs in these challenging situations\n\n## Answer 6\n\nThank you for your suggestion to clarify the experimental setup detailed in Table 1. In these experiments, we employed *real-world challenging MSAs* as introduced in Section 4.1. For MSA construction, we utilized JackHMMER with its default search parameters, specifically targeting the UniRef90 database. Each MSA created in this process contained fewer than 20 homologues, and these MSAs were consistently used across all experiments in this section\n\nTo elaborate further:\n\n- For single-sequence-based algorithms like ESMFold and OmegaFold, we directly input the query sequence, bypassing the need for an MSA.\n- For MSA-based algorithms, we employed various methods (Potts model, Iterative Unmasking and Ours) to generate MSAs, which were then fed into the folding algorithms.\n\nWe ensured a consistent experimental setup across all results reported in Table 1, aligning the benchmarking process closely with the conditions of our model's pretraining."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3959/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057709164,
                "cdate": 1700057709164,
                "tmdate": 1700057709164,
                "mdate": 1700057709164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]