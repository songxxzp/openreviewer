[
    {
        "title": "Learning Guarantees for Non-convex Pairwise SGD with Heavy Tails"
    },
    {
        "review": {
            "id": "c6DFMWPi0L",
            "forum": "7jWiBAWG0b",
            "replyto": "7jWiBAWG0b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4323/Reviewer_dGQ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4323/Reviewer_dGQ5"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the algorithmic stability type results for non-convex, heavy-tailed pairwise SGD by investigating the generalization performance and optimization jointly. Many theoretical results are obtained under various assumptions, including the general non-convex, non-convex without Lipschitz condition, non-convex with PL condition settings for pairwise SGD, and non-convex minibatch pairwise SGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A sequence of theoretical results is achieved, and the proofs seem to be correct and the logic is reasonable. The authors also introduced some new definition for pairwise learning algorithm to be $\\ell_{1}$ on-average model stable. The paper is well written."
                },
                "weaknesses": {
                    "value": "First, the model setup is a bit misleading. In the title and abstract, the authors call the setting heavy tails and heavy-tailed. However, what the authors really study is the sub-Weibull tails (Definition 3.6) that excludes polynomial decays as contrast to many papers the authors cited in the paper. Although there is no unique definition of heavy-tailedness, many readers would assume that you are talking about polynomial decay noise while you actually did not. I suggest you at least mention in the abstract that you are working with sub-Weibull tails. \n\nSecond, the technical novelty and the necessity of working with sub-Weibull distributed gradient noise is not very convincing to me. The reason is that I understand that sub-Weibull type distribution can appear in the concentration type inequalities if you want to obtain some high probability guarantees. But this is not what the authors are doing in this paper. The authors simply use very standard $L^2$ type arguments to study the SGD. What I meant by that is that if you look carefully at the proofs in the appendix, the authors only need some assumption to bound the 2nd moment, instead of relying on the full definition of the sub-Weibull distribution as is described in Definition 3.6. That means all the existing proof techniques in SGD for finite variance setups can all be directly used in the paper. If all you need is an application of Lemma C.3. with $p=2$, i.e. a second-moment estimate, why don\u2019t you simply assume that instead of your Definition 3.6. in your paper? Will all the results still go through? Essentially, Lemma C.3. with $p=2$ says that if the second-moment depends on the parameter $\\theta$ in a certain way (where $\\theta$ measures the heaviness of the tail), then it will also appear in the final results. In my view, the authors are not using the full information of sub-Weibull distribution, and there are numerous papers in the literature about SGD with finite variance, and hence to include heavy tails in the title and abstract and use that as a selling point is a bit misleading. \n\nThird, maybe I didn\u2019t read the paper carefully enough, but it is not clear to me whether the same setting has been studied for pointwise SGD. If the answer is yes, then the authors should highlight the technical novelty and difficulty to extend the results to pairwise setting, which to me does not seem to be very difficult. Moreover, the authors should compare the results with the pointwise setting. On the other hand, if the answer is no, then I am wondering why the authors do not study the pointwise setting, which is much more common and popular in the literature, and the authors should comment on whether similar results can hold for pointwise SGD."
                },
                "questions": {
                    "value": "For the main results, it would be better if the authors can provide some discussions on the monotonic (or not) dependence of the bound on $\\theta$, which measures the heaviness of the tail, and provide some insights.  \n\nOn page 2, before you talk about developing previous analysis techniques to the heavy-tailed pairwise cases, you should also cite some works about algorithmic stability and generalization bounds for pointwise SGD with heavy tails from the literature. \n\nOn page 3, you wrote that it is demonstrated in Nguyen et al. (2019); Hodgkinson and Mahoney (2021) that the generalization ability of SGD may suffer from the heavy-tailed gradient noise. However, I recently came across two more recent papers Raj et al. (2023) \u201cAlgorithmic stability of heavy-tailed stochastic gradient descent on least squares\u201d and Raj et al. (2023) \u201cAlgorithmic stability of heavy-tailed SGD with general loss functions\u201d that seem to argue heavy tails of gradient noise can help with generalization. I suggest you add more citations and discussions.\n\nAssuming the gradient of loss function being Lipschitz is very reasonable. But assuming the gradient and the loss function itself are both Lipschitz seems to be quite strong. It would be nice if you can add some examples and discussions about your Assumption 3.7.\n\nIn Assumption 3.8. and Assumption 3.9., it would be better for you to add a line or two to explain what the expectations are taken with respect to."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698708763967,
            "cdate": 1698708763967,
            "tmdate": 1699636401285,
            "mdate": 1699636401285,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BmTDW7OkVd",
                "forum": "7jWiBAWG0b",
                "replyto": "c6DFMWPi0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to\u00a0you for\u00a0your\u00a0valuable comments and constructive suggestions. The modifications mentioned in our response are all displayed in red font in our new manuscript.\nConsidering the strict upper limit of 9 pages for the main text of the submission, most modifications are shown in Appendix and we will demonstrate their specific locations. \n\n**Q1:** ... In the title and abstract, the authors call the setting heavy tails and heavy-tailed. However, what the authors really study is the sub-Weibull tails (Definition 3.6) ... I suggest you at least mention in the abstract that you are working with sub-Weibull tails. \n\n**A1:** Thanks for your constructive comments. In this work, we indeed assume the gradient noises in SGD have sub-Weibull tails. We have modified the related statements in the abstract of our new manuscript to avoid misunderstanding. Other heavy-tailed distributions (such as $\\alpha$-stable distributions[1]) will be considered in our future work to further explore the relationship between heavy tails and generalization performance.\n\n[1]T. Nguyen, et al. First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise. NeurIPS, 2019.\n\n***\n**Q2:** ... I understand that sub-Weibull type distribution can appear in the concentration type inequalities if you want to obtain some high probability guarantees. But this is not what the authors are doing in this paper. ... In my view, the authors are not using the full information of sub-Weibull distribution. ...\n\n**A2:** Thanks. For our bounds in expectation, we use the two bounds of second-moment in Lemma C.3, which can be regarded as the dependence of the variance of loss function on the parameter $\\theta$ of heavy tails. It should be noted that our current proof can be developed to establish high probability bounds with concentration inequalities, which has been demonstrated at the beginning of \u201cMAIN RESULTS\u201d section. Limited by the response time, we have only provided a relationship between generalization error and on-average model stability, which removes the expectation of Theorem 4.1 (b), and the corresponding high probability generalization bound for Theorem 4.4 in Appendix C.8 of our new manuscript. The orders of these high probability bounds are similar to our previous bounds in expectation. After the end of Rebuttal, we will provide the high probability bounds for all cases. \n***"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292678334,
                "cdate": 1700292678334,
                "tmdate": 1700292678334,
                "mdate": 1700292678334,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RFAxvyqkZk",
                "forum": "7jWiBAWG0b",
                "replyto": "c6DFMWPi0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***\n**Q3:** ... it is not clear to me whether the same setting has been studied for pointwise SGD. If the answer is yes, then the authors should highlight the technical novelty and difficulty to extend the results to pairwise setting... if the answer is no, then I am wondering why the authors do not study the pointwise setting, ... and the authors should comment on whether similar results can hold for pointwise SGD. \n\n**A3:** As far as we know, there is a gap for the non-convex stability-based generalization work under the sub-Weibull gradient noise setting in pointwise learning. [2] utilized uniform convergence tools to study high probability guarantees of non-convex pointwise SGD under the sub-Weibull gradient noise setting. Therefore, we try to analyze non-convex pairwise SGD with algorithmic stability tools. Our analysis can be directly extended to the corresponding pointwise case. In the following, we make comparisons with some stability bounds of non-convex pointwise learning (such as [3,4]) as the following **Vs. Theorem 4.2** and a discussion about the non-convex stability-based generalization work with heavy tails [5,6] in **Vs. Theorems 4.4, 4.6, 4.9**. \n\n**Vs. Theorem 4.2** [3] developed the uniform stability bound $\\mathcal{O}\\left((\\beta n)^{-1} L^{\\frac{2}{\\beta c + 1}} T^{\\frac{\\beta c}{\\beta c + 1}}\\right)$ under similar conditions, where the order depends on the smoothness parameter $\\beta$ and a constant $c$ related to step size $\\eta_t$. If $\\log T \\leq L^{\\frac{2}{\\beta c + 1} - 1} T^{\\frac{\\beta c}{\\beta c + 1} - \\frac{1}{2}}$, the bound of Theorem 4.2 is tighter than theirs. A stability bound $\\mathcal{O}\\left((nL)^{-1}\\sqrt{L+\\mathbb{E}_S[v_S^2]}\\log T \\right)$ [4] was established in the pointwise setting. Although it is $T^{1/2}$-times larger than the bound of Theorem 4.2, [4] made a more stringent limitation to the step size $\\eta_t$, i.e., $\\eta_t=\\frac{c}{(t+2)\\log(t+2)}$ with $0<c<1/L$. If we make the same setting, we will get a similar bound with [4].\n\n**Vs. Theorems 4.4, 4.6, 4.9** For the heavy-tailed distributions except for sub-Weibull distributions, e.g., $\\alpha$-stable distributions, there are a few papers [5,6] studied the stability-based generalization bounds and made the conclusion that the dependence of generalization bound on the heavy-tailed parameter is not monotonic. Especially, [5] analyzed the dependencies of several constants on heavy-tailed parameter. Our monotonic dependence on heavy-tailed parameter $\\theta$ essentially belongs to the dependence of the variance of the gradient for the loss function on heavy tails. Inspired by [5], we will further study the dependencies of other parameters (e.g., smoothness parameter $\\beta$) on $\\theta$, except for the monotonic dependence in our bounds.\n\nThe above statements have been discussed in Appendix C.7.3 of our modified manuscript.\n\n[2]S. Li and Y. Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. ICML, 2022.\n\n[3]M. Hardt, et al. Train faster, generalize better: Stability of stochastic gradient descent. ICML, 2016.\n\n[4]Y. Zhou, et al. Understanding generalization error of SGD in non-convex optimization. Machine Learning, 2022.\n\n[5]A. Raj, et al. Algorithmic stability of heavy-tailed SGD with general loss functions. ICML, 2023b.\n\n[6]A. Raj, et al. Algorithmic stability of heavy-tailed stochastic gradient descent on least squares. ALT, 2023a.\n\n***\n**Q4:** ... it would be better if the authors can provide some discussions on the monotonic (or not) dependence of the bound on $\\theta$ ...\n\n**A4:** Thanks. In our work, the sub-Weibull gradient noise assumption is introduced to derive the monotonic dependence of the bound on $\\theta$, which is consistent with the papers we mentioned [7]. However, inspired by [8], our dependence on $\\theta$ essentially belongs to the dependence of the variance of loss function on $\\theta$. Except for the variance, there are also some dependencies of other parameters on $\\theta$, such as the smoothness parameter, which need to be developed. These dependencies may be not-monotonic as [8,9]. We have added some related discussions in Appendix C.7.1 of our modified manuscript.\n\n[7]T. Nguyen, et al. First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise. NeurIPS, 2019.\n\n[8]A. Raj, et al. Algorithmic stability of heavy-tailed SGD with general loss functions. ICML, 2023b.\n\n[9]A. Raj, et al. Algorithmic stability of heavy-tailed stochastic gradient descent on least squares. ALT, 2023a.\n***"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292710572,
                "cdate": 1700292710572,
                "tmdate": 1700292781449,
                "mdate": 1700292781449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TfwEXwjTA3",
                "forum": "7jWiBAWG0b",
                "replyto": "c6DFMWPi0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***\n**Q5:** On page 2, before you talk about developing previous analysis techniques to the heavy-tailed pairwise cases, you should also cite some works about algorithmic stability and generalization bounds for pointwise SGD with heavy tails from the literature.\n\n**A5:** As mentioned in **A3**, there is a gap in the stability-based generalization work for pointwise SGD with sub-Weibull gradient noise. The most related works are [10,11,12]. [10] used uniform convergence tools to derive some high probability generalization bounds of non-convex pointwise SGD with sub-Weibull tails. [11] and [12] analyzed the stability-based generalization performance of pointwise SGD under $\\alpha$-stable Levy process. The latter studied the case of non-convex loss function. We have cited these works before talking about developing previous analysis techniques to the heavy-tailed pairwise cases on page 2 in our modified main text.\n\n[10]S. Li and Y. Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. ICML, 2022.\n\n[11]A. Raj, et al. Algorithmic stability of heavy-tailed stochastic gradient descent on least squares. ALT, 2023a.\n\n[12]A. Raj, et al. Algorithmic stability of heavy-tailed SGD with general loss functions. ICML, 2023b.\n\n***\n**Q6:** ... I recently came across two more recent papers ... that seem to argue heavy tails of gradient noise can help with generalization. ...\n\n**A6:** Thanks. These two papers [13,14] motivate us to further develop our work. In our work, the sub-Weibull gradient noise assumption is introduced to derive a monotonic relationship between the generalization error and heavy tails, which is consistent with the papers we mentioned. However, inspired by [13], our dependence on heavy tails essentially belongs to the dependence of the variance of the gradient for the loss function on heavy tails. Except for the variance, there are also some dependencies of other parameters, such as the smoothness parameter, on heavy tails. We have cited these two papers and added some related discussions in Appendix C.7.1 of our modified manuscript. In the future, we will further analyze these dependencies which may be non-monotonic.\n\n[13]A. Raj, et al. Algorithmic stability of heavy-tailed stochastic gradient descent on least squares. ALT, 2023a.\n\n[14]A. Raj, et al. Algorithmic stability of heavy-tailed SGD with general loss functions. ICML, 2023b.\n\n***\n**Q7:** ... It would be nice if you can add some examples and discussions about your Assumption 3.7.\n\n**A7:** Some previous work assumed the gradient and the loss function itself are both Lipschitz [15,16]. The Lipschitz w.r.t. the loss function itself is quite strong. Therefore, we attempt to remove this assumption with the sub-Weibull gradient noise assumption, which is one of our main contributions. We have cited some examples and modified the related statements to emphasize this contribution behind Assumption 3.7 in our new manuscript.\n\n[15]M. Hardt, et al. Train faster, generalize better: Stability of stochastic gradient descent. ICML, 2016.\n\n[16]Y. Lei and Y. Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. ICML, 2020.\n\n***\n**Q8:** In Assumption 3.8. and Assumption 3.9., it would be better for you to add a line or two to explain what the expectations are taken with respect to.\n\n**A8:** Thanks. For Assumption 3.8, $\\mathbb{E}$ is the expectation w.r.t. the samples indexes $i_t, j_t$. For Assumption 3.9, the definition of PL condition should be $\\|\\nabla F_S(w)\\|^2 \\geq  2\\mu \\left(F_S(w)- F_S(w(S))\\right)$ without expectation. We have made the related modifications in Assumptions 3.8 and 3.9."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292743787,
                "cdate": 1700292743787,
                "tmdate": 1700292743787,
                "mdate": 1700292743787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UtkUNKSjK3",
            "forum": "7jWiBAWG0b",
            "replyto": "7jWiBAWG0b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4323/Reviewer_dBNi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4323/Reviewer_dBNi"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the generalization performance of pairwise SGD in the non-convex setting, and in the presence of heavy tailed gradient noise. The generalization error for any learning algorithm is first bounded in terms of the $\\ell_1$ on-average stability under the bounded gradient assumption.  A similar relationship is derived for the SGD under the assumption of heavy tailed gradient noise (without bounded gradient assumption). Next, bounds on the  $\\ell_1$ on- average stability are derived for pairwise SGD under the aforementioned assumptions, which lead to explicit bounds on the generalization error.  Furthermore, bounds on generalization error and excess risk are derived for pairwise SGD under the PL condition, and assuming heavy-tailed gradient noise. These bounds are also extended to pairwise minibatch SGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is written well overall with a clear problem setup, notation and motivation. Moreover, the related work section is very thorough and puts into perspective the results of the paper.\n\n2. In terms of novelty, I believe there are no existing stability based guarantees for pairwise SGD with heavy tailed gradient noise in the nonconvex setting. Moreover, the relationship between generalization error and $\\ell_1$ on average stability (Theorem 4.1) seems new to my knowledge."
                },
                "weaknesses": {
                    "value": "1. Currently, no proof outline (or sketch) is provided in the main text. This makes it difficult to understand the extent of the novelty in the ideas underlying the proof. From my understanding, the proof steps seem to build heavily upon existing ideas from the literature on nonconvex pairwise SGD based learning (Lei et al, 2021b).\n\n\n2. The results are stated in expectation throughout, which is weaker than the \"high probability\" results that exist in the literature for similar learning problems that use stability based analysis."
                },
                "questions": {
                    "value": "1. In Assumption 3.8, is the expectation over all sources of randomness, including $w_t$?\n\n2. In Theorem 4.1 (b), it is not clear to me whether this applies to any learning algorithm A, or is specific to SGD? This is because of the gradient noise assumption made therein which suggests that it is for SGD."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746782476,
            "cdate": 1698746782476,
            "tmdate": 1699636401120,
            "mdate": 1699636401120,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HmP5UAPPbb",
                "forum": "7jWiBAWG0b",
                "replyto": "UtkUNKSjK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to\u00a0you for\u00a0your\u00a0valuable comments and constructive suggestions. The modifications mentioned in our response are all displayed in red font in our new manuscript.\nConsidering the strict upper limit of 9 pages for the main text of the submission, most modifications are shown in Appendix and we will demonstrate their specific locations. \n\n**Q1:** Currently, no proof outline (or sketch) is provided in the main text. ... the proof steps seem to build heavily upon existing ideas from the literature on nonconvex pairwise SGD based learning (Lei et al, 2021b).\n\n**A1:** Thanks for your constructive comments. Firstly, for the convenience of understanding the extent of the novelty in the ideas underlying the proof, we have provided a proof sketch figure at the beginning of Appendix C.\n\nNext, the main differences between our results and the non-convex stability bound of [1] are listed as follows.\n\n**1)Different stability tools:** We use $\\ell_1$ on-average model stability instead of uniform stability of [1].\n\n**2)Different assumption:** We make a sub-Weibull gradient noise assumption to remove Lipschitz condition, which is one of our main contributions. While [1] doesn\u2019t consider it. Besides, we compare the bound with PL condition and the one without PL condition to theoretically analyze the effect of PL condition. \n\n**3)Minibatch SGD:** Our analysis for SGD is extended to the minibatch case, while [1] doesn\u2019t consider it.\n\n**4)Better expectation bounds:** [1] provided a uniform stability bound $\\mathcal{O}\\left((\\beta n)^{-1}L^2T^{\\frac{\\beta c}{\\beta c+1}}\\right)$, where the constant $c=\\frac{1}{\\mu}$ ($\\mu$ is the parameter of PL condition). In general, $\\mu$ is typically a very small value (Examples 1 and 2 in [2]) which leads to a large value of $c$. Thus, $T^{\\frac{\\beta c}{\\beta c+1}}$ is closer to $T$ than the dependencies of our bounds on $T$. In other words, our bounds are tighter than [1].\n\n**5)High probability bounds:** Our proof can be developed to establish high probability bounds which have been added in Appendix C.8 of our modified manuscript. The orders of these high probability bounds are similar to our previous bounds in expectation. \n\nWe have emphasized these differences in Appendix C.7.2 of our modified manuscript.\n\n[1]Y. Lei, et al. Generalization guarantee of SGD for pairwise learning. NeurIPS, 2021b.\n\n[2]Y. Lei and Y. Ying. Sharper generalization bounds for learning with gradient-dominated objective functions. ICLR, 2021.\n\n***\n**Q2:** The results are stated in expectation throughout, which is weaker than the \"high probability\"...\n\n**A2:** Thanks for your constructive comments. Our current proofs are not only used to derive expectation bounds but also establish high probability bounds, which has been demonstrated at the beginning of \u201cMAIN RESULTS\u201d section. Limited by the response time, we have only provided a relationship between generalization error and on-average model stability, which removes the expectation of Theorem 4.1 (b), and the corresponding high probability generalization bound for Theorem 4.4 in Appendix C.8 of our new manuscript.  The orders of these high probability bounds are similar to our previous bounds in expectation. After the end of Rebuttal, we will give the high probability bounds for all cases. \n\n***\n**Q3:** In Assumption 3.8, is the expectation over all sources of randomness, including $w_t$?\n\n**A3:** In Assumption 3.8, $\\mathbb{E}$ is the expectation w.r.t. the samples indexes $i_t, j_t$. $w_t$ isn\u2019t included in the expectation. We have made the related modification in Assumption 3.8.\n\n***\n**Q4:** In Theorem 4.1 (b), it is not clear to me whether this applies to any learning algorithm A, or is specific to SGD?...\n\n**A4:** Thanks. Due to Assumption 3.8, Theorem 4.1 (b) is specific to SGD. We have modified the related statements in our main text.\n***"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292631536,
                "cdate": 1700292631536,
                "tmdate": 1700292631536,
                "mdate": 1700292631536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2DqZ2lh1Ud",
                "forum": "7jWiBAWG0b",
                "replyto": "UtkUNKSjK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer dBNi,\n\nThank you for dedicating your time and effort to offer insightful comments. We have covered all your concerns in our responses. We are looking forward to your reply.\n\nregards,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699809237,
                "cdate": 1700699809237,
                "tmdate": 1700699809237,
                "mdate": 1700699809237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EiqnAvLJNY",
                "forum": "7jWiBAWG0b",
                "replyto": "2DqZ2lh1Ud",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Reviewer_dBNi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Reviewer_dBNi"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the delayed response. I have read the authors response to my queries and I am satisfied with their response. I will keep my original score for this paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739401997,
                "cdate": 1700739401997,
                "tmdate": 1700739401997,
                "mdate": 1700739401997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e5tiMlzyMS",
            "forum": "7jWiBAWG0b",
            "replyto": "7jWiBAWG0b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4323/Reviewer_6TFb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4323/Reviewer_6TFb"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies generalization and excess risk bounds for pairwise learning with SGD under non-convex loss functions. The main tool for proving generalization is a notion of $\\ell_1$ on-average stability, which is adapted to the pairwise setting. It is shown that the bounded gradient condition for the loss function can be relaxed when having sub-Weibull noise in SGD iterates, which captures potentially heavy-tailed noise. Further assuming a PL condition leads to an improved generalization bound along with an optimization guarantee which overall leads to an excess risk bound for pairwise learning with SGD."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Relaxing the Lipschitzness requirement for the loss function and covering heavy-tailed noise distributions can be a significant forward step for stability-based SGD generalization bounds. Furthermore, the improvement from $T^{1/2}$ to $T^{1/4}$ in the stability bounds is a major improvement."
                },
                "weaknesses": {
                    "value": "* My main concern is with the readability and precision of the current submission. The main text seems to lack sufficient intuition on the techniques behind the improvements and how prior analyses are modified to achieve the refined rates. Some examples for improving readability:\n    * What is the valid range of values for $c$ in Table 1? Is it allowed to simply let $c \\to 0$? This is important in order to compare the $T$-dependence of the current stability bounds with the literature.\n    * Have the stability bounds of this paper, i.e. $\\tilde{O}(T^{1/2}/n)$ and $\\tilde{O}(T^{1/4}/n)$ under an additional PL condition, been established in the pointwise setting under the same assumptions or are they completely new?\n    * What is the intuition behind improving prior dependencies on $T$ to $T^{1/2}$ under smoothness and $T^{1/4}$ under PL and smoothness? Specifically, how does the PL allow a transition from $T^{1/2}$ to $T^{1/4}$?\n    * What is the dependence of the bounds in Theorems and Corollaries 4.6 to 4.11 on $\\mu$? Without any hidden dependencies, it seems that one can let $\\mu \\to 0$ to prove the same results without the PL condition. Similarly, it seems like the dependence on $\\beta$ is hidden in most statements which might be useful to highlight.\n\n* The bounds of this paper are in expectation, while many similar bounds in the literature are stated with high probability. It might be useful to add a discussion on the possibility of establishing high probability bounds, especially how such bounds would interact with the heavy-tailed noise of SGD."
                },
                "questions": {
                    "value": "* It seems that a term $|\\mathbb{E}[F_S(w(S))] - F(w^*)|$ is missing from the RHS of Equation (4).\n\n* Why is SGD initialized at zero in Definition 3.1? Is this a fundamental limitation or is it possible to handle arbitrary initialization?\n\n* Perhaps there could be a discussion on why Definition 3.5 is called $\\ell_1$ on-average stability even though the $\\ell_2$ norm is used in the definition."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4323/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4323/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4323/Reviewer_6TFb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699584016252,
            "cdate": 1699584016252,
            "tmdate": 1699636401010,
            "mdate": 1699636401010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g4YxyimoFS",
                "forum": "7jWiBAWG0b",
                "replyto": "e5tiMlzyMS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to\u00a0you for\u00a0your\u00a0valuable comments and constructive suggestions. The modifications mentioned in our response are all displayed in red font in our new manuscript.\nConsidering the strict page limit, most modifications are shown in Appendix and we will demonstrate their specific locations. \n\n**Q1:** What is the valid range of values for $c$ in Table 1?...\n\n**A1:** Thanks for your constructive comments to benefit for the comparisons with current stability bounds in Table 1. \n\nFirstly, [1] provided a uniform stability bound $\\mathcal{O}\\left((\\beta n)^{-1}L^{\\frac{2}{\\beta c+1}}T^{\\frac{\\beta c}{\\beta c+1}}\\right)$, where the constant $c$ is just set to be greater than 0. Thus, under the same assumption with [1], our bound (Theorem 4.2) is tighter than it when $c$ satisfies the following inequality\n$L^{\\frac{2}{\\beta c+1}} T^{\\frac{\\beta c}{\\beta c+1}} \\geq LT^{1/2}\\log T$,\ni.e., $0<c\\leq \\frac{1}{\\beta}\\left(\\frac{1}{\\log_{L^2T^{-1}}(LT^{-1/2}\\log T)}-1\\right)$. \n\nSecondly, [2] provided a uniform stability bound $\\mathcal{O}\\left((\\beta n)^{-1}L^2T^{\\frac{\\beta c}{\\beta c+1}}\\right)$, where the constant $c=\\frac{1}{\\mu}$ ($\\mu$ is the parameter of PL condition). In general, $\\mu$ is typically a very small value (Examples 1 and 2 in [3]) which leads to a large value of $c$. Thus, $T^{\\frac{\\beta c}{\\beta c+1}}$ is closer to $T$ than $T^{1/2}\\log T$ in our bound (Theorem 4.2). In other words, our bound is tighter than [2].\n\nThe aforementioned detailed analysis has been added behind Theorem 4.2 of our modified manuscript to improve readability.\n\n[1]W. Shen, et al. Stability and optimization error of stochastic gradient descent for pairwise learning. arXiv, 2019.\n\n[2]Y. Lei, et al. Generalization guarantee of SGD for pairwise learning. NeurIPS, 2021b.\n\n[3]Y. Lei and Y. Ying. Sharper generalization bounds for learning with gradient-dominated objective functions. ICLR, 2021.\n\n***\n**Q2:** Have the stability bounds of this paper been established in the pointwise setting ...?\n\n**A2:** \nAs far as we know, there is a gap for the non-convex stability-based generalization work under the sub-Weibull gradient noise setting in pointwise learning. [4] utilized uniform convergence tools to study high probability guarantees of non-convex pointwise SGD under the sub-Weibull gradient noise setting. Therefore, we try to analyze non-convex pairwise SGD with algorithmic stability tools. Our results can be directly extended to the case of pointwise learning. In the following, we make comparisons with some stability bounds of non-convex pointwise learning [5,6] in **Vs. Theorem 4.2** and a discussion about the non-convex stability-based generalization work with heavy tails [7,8] in **Vs. Theorems 4.4, 4.6, 4.9**. \n\n**Vs. Theorem 4.2** [5] developed the uniform stability bound $\\mathcal{O}\\left((\\beta n)^{-1} L^{\\frac{2}{\\beta c + 1}} T^{\\frac{\\beta c}{\\beta c + 1}}\\right)$ under similar conditions, where the order depends on the smoothness parameter $\\beta$ and a constant $c$ related to step size $\\eta_t$. If $\\log T \\leq L^{\\frac{2}{\\beta c + 1} - 1} T^{\\frac{\\beta c}{\\beta c + 1} - \\frac{1}{2}}$, the bound of Theorem 4.2 is tighter than theirs. A stability bound $\\mathcal{O}\\left((nL)^{-1}\\sqrt{L+\\mathbb{E}_S[v_S^2]}\\log T \\right)$ [6] was established in the pointwise setting. Although it is $T^{1/2}$-times larger than the bound of Theorem 4.2, [6] made a more stringent limitation to the step size $\\eta_t$, i.e., $\\eta_t=\\frac{c}{(t+2)\\log(t+2)}$ with $0<c<1/L$. If we make the same setting, we will get a similar bound with [6].\n\n**Vs. Theorems 4.4, 4.6, 4.9** For the heavy-tailed distributions except for sub-Weibull distributions, e.g., $\\alpha$-stable distributions, there are a few papers [7,8] studied the stability-based generalization bounds and made the conclusion that the dependence of generalization bound on the heavy-tailed parameter is not monotonic. Especially, [7] analyzed the dependencies of several constants on heavy-tailed parameter. Our monotonic dependence on heavy-tailed parameter $\\theta$ essentially belongs to the dependence of the variance of the gradient for the loss function on heavy tails. Inspired by [7], we will further study the dependencies of other parameters (e.g., smoothness parameter $\\beta$) on $\\theta$, except for the monotonic dependence in our bounds.\n\nWe have added these statements in Appendix C.7.3 of our modified manuscript. \n\n[4]S. Li and Y. Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. ICML, 2022.\n\n[5]M. Hardt, et al. Train faster, generalize better: Stability of stochastic gradient descent. ICML, 2016.\n\n[6]Y. Zhou, et al. Understanding generalization error of SGD in non-convex optimization. Machine Learning, 2022.\n\n[7]A. Raj, et al. Algorithmic stability of heavy-tailed SGD with general loss functions. ICML, 2023b.\n\n[8]A. Raj, et al. Algorithmic stability of heavy-tailed stochastic gradient descent on least squares. ALT, 2023a.\n***"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292511905,
                "cdate": 1700292511905,
                "tmdate": 1700292511905,
                "mdate": 1700292511905,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GstMOmy2cp",
                "forum": "7jWiBAWG0b",
                "replyto": "e5tiMlzyMS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***\n**Q3:** ... how does the PL allow a transition from $T^{1/2}$ to $T^{1/4}$?\n\n**A3:** Thanks. The key of the transition from $T^{1/2}$ to $T^{1/4}$ is the upper bound of step size $\\eta_1$. \n\nFor Theorem 4.4, we set $\\eta_1 \\leq \\frac{1}{2\\beta}$ with the reason that the term $\\|\\nabla F_S(w_t)\\|^2$ can be removed without any cost, which can be found in the second inequality of Appendix C.4.\n\nFor Theorem 4.6, the upper bound of $\\eta_1$ is set to be tighter than Theorem 4.4. In this case, if we directly remove $\\|\\nabla F_S(w_t)\\|^2$, the bound is equal to Theorem 4.4, which is meaningless. So we introduce the additional assumption, PL condition, to decompose $\\|\\nabla F_S(w_t)\\|^2$. The bound of Theorem 4.6 demonstrates that the tighter upper bound of $\\eta_1$ combined with PL condition can lead to the tighter stability bound. The above statement has been added to improve readability in Appendix C.7.1 of our modified manuscript.\n\n***\n**Q4:** What is the dependence of the bounds in Theorems and Corollaries 4.6 to 4.11 on $\\mu$? ... Similarly, it seems like the dependence on $\\beta$ is hidden ...\n\n**A4:** **1)Dependence on $\\mu$:**\nThe dependence on $\\mu$ is indeed hidden. After the modifications on Theorems and Corollaries 4.6 to 4.11, this dependence $1-\\prod\\limits_{i=1}^t \\left(1-\\frac{1}{2} \\mu \\eta_i\\right)$ has been uncovered in our modified manuscript. With this dependence, we can not let $\\mu \\rightarrow 0$ to prove the same results without the PL condition due to the tighter upper bound of $\\eta_1\\leq \\frac{1}{4\\beta}$. **A3** provides the consideration about the upper bounds of step size $\\eta_1$.\n\n**2)Dependence on $\\beta$:** \nAll dependencies on smoothness parameter $\\beta$ are hidden in our results. In our modified manuscript, we have also uncovered them. For all results from Theorem 4.2 to Theorem 4.11, the dependencies are $\\beta^{-1}$, which are similar to [9,10]. We have added the above discussions in Appendix C.7.1 of our new manuscript.\n\n[9]W. Shen, et al. Stability and optimization error of stochastic gradient descent for pairwise learning. arXiv, 2019.\n\n[10]Y. Lei, et al. Generalization guarantee of SGD for pairwise learning. NeurIPS, 2021b.\n\n***\n**Q5:** ... It might be useful to add a discussion on the possibility of establishing high probability bounds ... \n\n**A5:** Thanks for your constructive comments. Our current proofs of the bounds in expectation can be developed to establish high probability bounds, which has been demonstrated at the beginning of \u201cMAIN RESULTS\u201d section. Limited by the response time, we have only provided a relationship between generalization error and on-average model stability, which removes the expectation of Theorem 4.1 (b), and the corresponding high probability generalization bound for Theorem 4.4 in Appendix C.8 of our new manuscript. The orders of these high probability bounds are similar to our previous bounds in expectation. After the end of Rebuttal, we will give the high probability bounds for all cases.\n \n***\n**Q6:** It seems that a term $|\\mathbb{E}[F_S(w(S))] - F(w^*)|$ is missing from the RHS of Equation (4).\n\n**A6:** The term $|\\mathbb{E}[F_S(w(S))] - F(w^*)| = 0$ due to the expectation w.r.t. the training set $S$, which has been emphasized before Equation (4) to make it clearer.\n\n***\n**Q7:** Why is SGD initialized at zero in Definition 3.1?\n\n**A7:** Since our proof can handle arbitrary initialization, we choose a simple initialization $w_1=0$ for convenience.\n\n***\n**Q8:** ... why Definition 3.5 is called $\\ell_1$ on-average stability...\n\n**A8:** Thanks. Our pairwise $\\ell_1$ on-average model stability is defined following Definition 4 [11]. It should be noted that $\\ell_1$ means the exponent of the term $\\left\\|A(S_{i,j}) - A(S)\\right\\|$ is 1, which is not related to the $\\ell_2$ norm.\n\n[11]Y. Lei and Y. Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. ICML, 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292550583,
                "cdate": 1700292550583,
                "tmdate": 1700292550583,
                "mdate": 1700292550583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9DiTA23dQj",
                "forum": "7jWiBAWG0b",
                "replyto": "e5tiMlzyMS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 6TFb,\n\nAs the author-reviewer discussion phase is nearing its conclusion, we would like to inquire if there are any remaining concerns or areas that may need further clarification. Your support during this final phase, especially if you find the revisions satisfactory, would be of great significance. We are looking forward to your reply.\n\nregards,\n\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699756943,
                "cdate": 1700699756943,
                "tmdate": 1700699756943,
                "mdate": 1700699756943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rQrOVUojeK",
                "forum": "7jWiBAWG0b",
                "replyto": "GstMOmy2cp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Reviewer_6TFb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Reviewer_6TFb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response and elaboration. After reading the revision, I still agree with Reviewer dGQ5 in that I'm not sure why the contributions are stated for pairwise SGD even though they are novel for pointwise SGD, a setting that would attract much more attention from the community. I believe a new version of this submission with more highlights on technical novelties in comparison with prior work would further strengthen the work and help the readers, which is why I'm keeping my original score.\n\nOn a side note, I'm not sure I completely understand why $\\mathbb{E}[F_S(w(S))] = F(w^*)$ as $w(S)$ has a dependence on the training set $S$ and is not the population minimizer."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705690421,
                "cdate": 1700705690421,
                "tmdate": 1700705690421,
                "mdate": 1700705690421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LwH7vAWU40",
                "forum": "7jWiBAWG0b",
                "replyto": "e5tiMlzyMS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4323/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 6TFb,\n\nThanks for checking our rebuttal and providing insightful inquiries! \n\n**For your first question \u201cI'm not sure why the contributions are stated for pairwise SGD even though they are novel for pointwise SGD\u201d:** \n\n>As we mentioned in **A3** of our response to Reviewer dGQ5, [1] had provided the generalization guarantees under the same setting of pointwise SGD with uniform convergence analysis. Although pointwise learning attracts much more attention than pairwise learning from the community, the latter has many applications, such as metric learning[2], ranking[3] and AUC maximication[4]. This is why we study the generalization guarantees of pairwise SGD. As for **\u201ceven though they are novel for pointwise SGD\u201d**, our extension to pointwise SGD is not the first generalization analysis of pointwise SGD with sub-Weibull tails but the first stability-based generalization analysis. Except for the comparisons with some pairwise results, we made **some comparisons with some pointwise results including [1]** in our main text (in **Table 2, the Remarks of Corollary 4.5, Theorem 4.8, and Appendix C.7.3**).\n\n**For your second question \u201cI'm not sure I completely understand why** $\\mathbb{E}[F_S(w(S))] = F(w^*)$**:\u201d**\n\n>We\u2019re very sorry for the incorrect explanation. We would like to make a detailed decomposition, which follows the same idea as in [5], to correct it as follows:\n\\begin{align}\n\\left|\\mathbb{E}_S[F(A(S)) - F(w^*)]\\right|\n\\leq & \\left|\\mathbb{E}_S[F(A(S)) - F_S(A(S)) + F_S(A(S)) - F_S(w(S)) + F_S(w(S)) - F_S(w^*) + F_S(w^*) - F(w^*)]\\right|\\\\\\\\\n= & \\left|\\mathbb{E}_S[F(A(S)) - F_S(A(S)) + F_S(A(S)) - F_S(w(S)) + F_S(w(S)) - F_S(w^*)]\\right|\\\\\\\\\n\\leq & \\left|\\mathbb{E}_S[F(A(S)) - F_S(A(S)) + F_S(A(S)) - F_S(w(S))]\\right|\\\\\\\\\n\\leq & \\left|\\mathbb{E}_S[F(A(S)) - F_S(A(S))]\\right| + \\left|\\mathbb{E}_S[F_S(A(S)) - F_S(w(S))]\\right|,\n\\end{align}\nwhere the first equality is due to $\\mathbb{E}_S[F_S(w^*)] = F(w^*)$, and the second inequality is derived from $\\mathbb{E}_S[F_S(w(S)) - F_S(w^*)] \\leq 0$ and $\\mathbb{E}_S[F(A(S)) - F(w^*)] \\geq 0$.\n\nPlease feel free to let us know if these address your concerns\uff01\n\nregards,\n\nAuthors\n\n[1]S. Li and Y. Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. ICML, 2022.\n\n[2]E. Xing, et al. Distance metric learning with application to clustering with side-information. NIPS, 2002.\n\n[3]W. Rejchel. On ranking and generalization bounds. Journal of Machine Learning Research, 2012.\n\n[4]Y. Ying, et al. Stochastic online AUC maximization. NIPS, 2016.\n\n[5]Y. Chen, et al. Stability and convergence trade-off of iterative optimization algorithms, 2018."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712947201,
                "cdate": 1700712947201,
                "tmdate": 1700713355590,
                "mdate": 1700713355590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]