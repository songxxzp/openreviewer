[
    {
        "title": "The LLM Surgeon"
    },
    {
        "review": {
            "id": "OqD3oiG1WH",
            "forum": "DYIIRgwg2i",
            "replyto": "DYIIRgwg2i",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2739/Reviewer_2GDn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2739/Reviewer_2GDn"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces LLM Surgeon, a method that enhances the scalability of Kronecker-factored curvature approximations of the targeted loss landscapes, designed for pruning LLMs at arbitrary sparse patterns. The authors demonstrate that this proposed methodology consistently improves the PPL performance of current methods across a range of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-organized and easy to comprehend.\n\n2. The fundamental idea for estimating the curvature of the loss landscape is reasonable and innovative."
                },
                "weaknesses": {
                    "value": "1. Since the authors adopt a multi-shot sparse approach, it would be beneficial to quantitatively compare the time costs and GPU memory consumption with SparseGPT.\n\n2. While the authors emphasize sparsity in large models, the largest model they utilize is of 7-billion parameters. It might provide readers with a clearer view if experiments involving larger model sizes were included.\n\n3. Global rank ordering is a sound strategy, but there seems to be a lack of an ablation experiment, that is, whether the suggested method outperforms SparseGPT under the same layer-wise sparsity situation.\n\n4. Although the authors underlines that LLM surgeon can be migrated to structured pruning, and as the authors stated, \"To the best of our knowledge, this is the first method to successfully perform structured pruning for LLMs,\" they do not discuss nor compare their approach to the already presented structured LLM pruning method[1]. This lack of discussion appears less meticulous. Furthermore, the authors should also consider comparing their work with another state-of-the-art large model sparse method, Wanda [2], which can also be adapted for pruning LLMs at any patterns.\n\n[1] LLM-Pruner: On the Structural Pruning of Large Language Models. In NeurIPS, 2023\n[2] A Simple and Effective Pruning Approach for Large Language Models. In Arxiv, 2023"
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637125275,
            "cdate": 1698637125275,
            "tmdate": 1699636216367,
            "mdate": 1699636216367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "op2AypxEuz",
                "forum": "DYIIRgwg2i",
                "replyto": "OqD3oiG1WH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and help to improve the paper. In particular, we thank the reviewer for noting the paper is well-organised and easy to comprehend and deem our fundamental contributions reasonable and innovative.\n\n1. compute and memory\n\nWe agree with the reviewer that we should include a quantitative comparison between methods. Compared to other methods (e.g. SparseGPT), we are able to obtain better compression results but only at the expense of longer compression time. We will make sure this is absolutely clear from the main text. The table below shows compression time on Llama-v2 7B and 13B models. We will add these results to the paper appendices:\n\n|           | SparseGPT  (baseline)           | Unstructured LLM-Surgeon (ours) |\n| --------- | --------------------- | ------------------------ |\n| Llama-7B  | <5m / 1 GPU (32GB)    | 2d8h16m / 4xH100 80 GB   |\n\n|           | K-OBD   (baseline)              | Structured LLM-Surgeon (ours)   |\n| --------- | --------------------- | ------------------------ |\n| Llama-7B  | 16h58m / 4xH100 80 GB | 17h08m / 4xH100 80 GB    |\n| Llama-13B | 1d6h5m / 8xH100 80 GB | 1d9h26m / 8xH100 80 GB   |\n\nThe method is highly parallelizable and more GPUs can be traded for a linear speed-up in time, if desired. Compression time measured using the required memory:\n\n|                                       |Runtime|PPL 90%|PPL 80%|PPL 70%|PPL 60%|PPL 50%|\n|---------------------------------------|-------|-------|-------|-------|-------|-------|\n|Unstructured baseline (SparseGPT)      |**<5m**    |5.49   |5.58   |5.71   |5.94   |6.51   |\n|Unstructured LLM Surgeon (ours)        |2d8h16m|**5.13**   |**5.20**   |**5.36**   |**5.66**   |**6.08**   |\n|Structured baseline (K-OBD)            |**16h58m** |5.48   |9.14   |15.43  |28.03  |46.64  |\n|Structured LLM Surgeon (ours)          |17h08m |**5.25**   |**6.18**   |**7.83**   |**10.39**  |**15.38**  |\n|Structured baseline (K-OBD) Llama-13B  |**1d6h5m** |4.908  |6.294  |10.08  |13.06  |16.06  |\n|Structured LLM Surgeon (ours) Llama-13B|1d9h26m|**4.692**  |**5.286**  |**6.207**  |**7.245**  |**9.428**  |\n\nFurther engineering efforts may further speed up compression, especially for unstructured pruning. As mentioned, our method does come with a higher computational cost compared to other approaches. Importantly, compression of LLMs in general only needs to happen once after which a pruned model can be deployed infinitely many times without further cost. This motivates our method which takes longer to run but provides much better final test performance.\n\n2. Larger models\n\nWe agree with the reviewers that it would be interesting to assess performance on even larger models. The model sizes were chosen to demonstrate compression on OPT models with millions of parameters but also on the Llama architecture, demonstrating pruning of models in the order of billions of parameters. Following the suggestions by the reviewers (zzzW, 2GDn), we now also have results for the larger Llama-v2 13B model and evaluated its performance on downstream tasks (see results below). We find that our method also outperforms baselines on this larger model, as well as on downstream tasks.\n\n3. Global rank ordering\n\nWe thank the reviewer for raising this point. Our proposed improved curvature approximations use gradient information that relates curvatures of individual layers to the final global objective. The gradient terms can be interpreted as the relative influence layer outputs have on the final objective. For this reason, our improved curvature estimates are especially useful in conjunction with global thresholding, which allocates weight to be pruned globally rather than for layers separately. Nevertheless, we will run an ablation study on this and include it in the appendix.\n\n4. Structured pruning related work\n\nWe thank the reviewer for bringing other structured sparsification works to our attention. Both works are going to be published at NeurIPS 2023 and appeared a few months before the ICLR deadline on arXiv so should be considered concurrent work. Further, [2] does not seem to include any experiments on structured pruning of rows and columns and only considers semi-structured pruning (e.g. 2:4). The other paper [1] does include such experiments, but for -20% sparsity on Llama-v2 reports a very large degradation in performance from 12.62 to 74.63, whereas our method on the same model, sparsity and data achieves test perplexity from 5.12 to 6.18. We would like to emphasise that structured pruning is a very hard task on which our method outperforms any existing approach by a large margin. Regardless, we deem [1, 2] relevant prior literature and will add them to the related work section. \n\n[1] LLM-Pruner: On the Structural Pruning of Large Language Models. In NeurIPS, 2023\n\n[2] A Simple and Effective Pruning Approach for Large Language Models. In Arxiv, 2023\n\n** Edit 20 Nov: added timing results for Llama-v2 13B and GPU memory"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243539370,
                "cdate": 1700243539370,
                "tmdate": 1700508240600,
                "mdate": 1700508240600,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5p08CzxrTn",
                "forum": "DYIIRgwg2i",
                "replyto": "FycNIsraIV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2739/Reviewer_2GDn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2739/Reviewer_2GDn"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the author's responses and efforts, which partially addressed my concerns. However, my unease about the efficiency of the proposed method persists. It is excessively costly for a LLM pruning method to compress llama-7b requiring 4xH100 80 GB and 2d8h16m, respectively. For instance, under the same conditions, SparseGPT only takes minutes, and Wanda even takes mere seconds with single GPU. While I comprehend the authors' assertion that their method, though more time-consuming, yields better performance, I question why we cannot directly use lora fine-tuning for performance recovery? As reported in [1], a 50% sparse llama7b can be elevated from a PPL of 7.26 to 6.84 merely by 4h of fine-tuning. This is akin to the performance gap between llm-surgen and SparseGPT, not to mention that lora only requires one GPU. Consequently, although I acknowledge the authors' comprehensive response, my apprehension concerning the method's efficiency prevents me from recommending this paper for acceptance.\n\n[1] A Simple and Effective Pruning Approach for Large Language Models. In Arxiv, 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527710180,
                "cdate": 1700527710180,
                "tmdate": 1700527710180,
                "mdate": 1700527710180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SjBsQNwy1r",
                "forum": "DYIIRgwg2i",
                "replyto": "OqD3oiG1WH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are very glad that most of the concerns have been addressed. In this reply, we hope to take away the remaining hesitation regarding model efficiency.\n\nOverall, our method was developed for structured pruning, where it shines. Here we are roughly as fast as the baseline (K-OBD) and much better (17h08m versus 16h58m on Llama-v2). For unstructured, it is also better but indeed a bit slow, but this is not the main point of the method. We included this comparison to have a well-established baseline to compare to and because the method outperformed pruning performance on all structure types.\n\n- why not replace costly pruning with finetuning\n\nThis is indeed an important question. However, we would like to push back on the possibility of using finetuning as an alternative for our approach. It is indeed that for unstructured and semi-structured pruning finetuning can be beneficial and can sometimes work [1], although not always reliably. The reason for this is that limited data used for compression is not always effective in preserving the capabilities of an existing pre-trained model. Empirically, we found that first-order updates (e.g. LoRA) can not always make up for removals of weights (see also Sec. 4.2), a problem which gets much worse in the case of structured pruning, in which the degradation in loss can be very large if weights are removed without updates that take weight correlations into account.\n\nCompared to finetuning, our approach has two big benefits, namely: (a) it is hyperparameter free, (b) allows dynamic allocation which weights to be removed, and (c) correlated weight updates which allow retaining as much information as possible from the existing pre-trained model, even with limited data.  In experiments, we found that finetuning to not sufficient to recover performance and are not aware of literature using finetuning to work for structured pruning. We believe this difficulty to be the reason why relatively few papers even consider structured pruning in their experiments. As such, our method has important benefits compared to finetuning approaches. We do think we could have made this point more clear in the paper and will improve this.\n\n- computational cost\n\nOur method takes longer to run but achieves higher performance.\n\nWe very much admire efforts such as [1] or SparseGPT as these works demonstrate how much compression can be achieved only using activations and weights. Nevertheless, only relying on activations and weight strength implies strong assumptions on the curvature of the loss and makes these methods unsuitable for more constrained structure types. Our improved curvature allows for dynamic allocation of weight removal and improved correlated weight updates. As a result, our method can be used for structured pruning and results in higher overall performance. \n\nAlthough we are proud of the efficiencies that allow scaling very rich Kronecker structures to LLMs, our resulting method does remain much more computationally expensive compared to existing pruning methods in relative terms. From a practical perspective, however, we argue that the compute time may not be 'excessive' at all! The reason for this is that a model only needs to be pruned once after which it can be deployed infinitely many times. Therefore, it can very well be economically advantageous to temporarily spend multiple GPU resources on compression to achieve a better performance after pruning. In absolute terms, the use of multiple large GPUs is very common practice in the field of large language models and many more GPUs are typically used to train and deploy large language models. Moreover, our method can be trivially parallelized in case further speed-ups or larger models are required. We hope this provides a better context and emphasises the trade-off that should be made by the practitioner.\n\nFurther, the trade-off between performance and computational cost might very well be fundamental (therefore something that we may not be able to fix). Given that, we sincerely hope that the computational cost is not a ground for rejection, as our method offers significant benefits including structured pruning and state-of-the-art pruning performance.\n\nWe thank the reviewer and do agree that the trade-off should be communicated clearly to the reader and practitioners who may wish to use the method. We promise to make an effort to ensure this is very clear from the main text."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536959366,
                "cdate": 1700536959366,
                "tmdate": 1700586336101,
                "mdate": 1700586336101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a4WMew260w",
            "forum": "DYIIRgwg2i",
            "replyto": "DYIIRgwg2i",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2739/Reviewer_zzzW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2739/Reviewer_zzzW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called \"LLM Surgeon\" for efficient pruning and compression of large pretrained language models like OPT and LLAMa. It scales up second-order Hessian-based pruning methods like Optimal Brain Surgeon using Kronecker-factored approximations of the Fisher information matrix. It derives closed-form solutions for removal costs and correlated weight updates when pruning multiple weights jointly. Experiments show the method can prune OPT and LLAMa models by 20-30% with minor performance loss and outperforms prior work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and clearly presented; \n- The paper provides a general pruning framework applicable to different structured and unstructured schemes. \n- The paper provides careful derivation of update rules that consider correlations between weights, theoretically principled and extends classical segundo-order pruning methods.\n- The proposed methods achieves state-of-the-art pruning results on large language models, especially for structured pruning. \n- Detailed ablation regarding the low-rank components, approximation methods, as well as the qualitative sparsity level analyses are provided to show the comprehensiveness of the proposed methods and design choices;"
                },
                "weaknesses": {
                    "value": "- The paper still uses approximations for computational tractability which limits pruning performance.\n- Structured pruning leads to irregular sparsity patterns which are difficult to accelerate. The real inference speedup or memory savings after pruning is unknown; \n- Additional FLOPs for approximation and updating offsets gains during deployment are needed, while the detailed comparison and discussion might be missing.\n- Some related works might also be good to include [1];\n\n[1] Yu, Shixing, et al. \"Hessian-aware pruning and optimal neural implant.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022."
                },
                "questions": {
                    "value": "- Could the authors provide both inference speed and additional cost for approximation and updating the offsets?\n- Could larger model sizes also be included and evaluated throughout?\n- Besides the PPL, could the author also provide the 0shot performance degradation regarding the OPT/LLaMA models for a comprehensive evaluation;"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842749416,
            "cdate": 1698842749416,
            "tmdate": 1699636216288,
            "mdate": 1699636216288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YDEbobc5kQ",
                "forum": "DYIIRgwg2i",
                "replyto": "a4WMew260w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and help to improve the paper. We thank the reviewer for finding the paper is well written, clearly presented and theoretically principled. We further value that the reviewer appreciated the 'careful derivations of correlated update rules' and noted the strength of the work and state-of-the-art pruning results, especially on structured pruning.\n\n> Use of approximations\n\nDue to the high dimensionality of LLM models, the constraint optimization problem considered in this work is inherently intractable. This means that it can be mathematically proven that the problem can not be solved in an exact manner and the sheer use of an approximation should thus not be considered a 'weakness' but rather a necessity. In fact, most other methods (even magnitude pruning) can be seen as some sort of crude approximation to the Hessian of the global loss and are strictly more approximate than ours. Thus, compared to other methods we use the most accurate curvature approximation. We expect this to be a misunderstanding and will make this more clear in the main text to prevent confusion.\n\n> Irregular sparsity patterns\n\nWe thank the reviewer for raising this as the computational benefits of the considered pruning structures might not have been emphasised enough in the paper. It is not true that structured pruning results in irregular sparsity patterns. Rather the opposite, it provides the most regular sparsity patterns - hence the name 'structured' - and is very easy to accelerate. As complete rows and columns are removed, structured pruning of X% of weights means that matrix dimensions can directly be reduced by X%. The resulting smaller matrices have simply lower dimensions and are thus directly amenable to existing standard CUDA/and hardware-accelerated kernels. We recognize that we only very briefly mention this in Sec. 2. We thank the reviewer and will make sure this point is made more clear in the main text.\n\n> Savings in terms of FLOPS / compute time\n\nWe are not exactly sure what is meant by 'updating offsets gains', but will assume the question is whether reported weight reduction relates to actual computational and memory savings at deployment. Unlike most other sparsity patterns, structured pruning allows for a direct reduction in the dimensionality of weight matrices by removing complete rows and columns. Therefore, a reduction of weights will result in the same reduction in the number of FLOPS. This is not generally true for most other sparsity patterns, and in fact an important benefit and in part the motivation for structured sparsity patterns. Below, we provide compression time of compressed models which we will add to the paper appendices.\n\n> Inference time overhead\n\nA major advantage of our method is that there is no overhead at inference time. All savings are absorbed into the weight of the neural network and there are no separate offsets to be separately stored. We have not mentioned this benefit explicitly and will make sure it is clear in the paper.\n\n> Related work\n\nWe thank the reviewers for pointing out this work. We find the reference relevant and will add it to an updated related work section.\n\n> Questions / additional experiments\n\n1. Performance\n\nThe table below shows compression runtimes on the Llama-7b model. We will add these results to the paper appendices.\n\n| |Runtime|PPL 90%|PPL 80%|PPL 70%|PPL 60%|PPL 50%|\n|-|-|-|-|-|-|-|\n|Unstructured baseline (SparseGPT) Llama-v2 7B |**<5m**|5.49|5.58|5.71|5.94|6.51|\n|Unstructured LLM Surgeon (ours)  Llama-v2 7B  |2d8h16m|**5.13**|**5.20**|**5.36**|**5.66**|**6.08**|\n|Structured baseline (K-OBD)  Llama-v2 7B  |**16h58m** |5.48|9.14|15.43|28.03|46.64|\n|Structured LLM Surgeon (ours)  Llama-v2 7B |17h08m |**5.25**|**6.18**   |**7.83**   |**10.39**|**15.38**|\n|Structured baseline (K-OBD) Llama-v2 13B|**1d6h5m** |4.908|6.294  |10.08  |13.06  |16.06|\n|Structured LLM Surgeon (ours) Llama-v2 13B|1d9h26m|**4.692**|**5.286**|**6.207**|**7.245**|**9.428**|\n\nOur method is most efficient for structured pruning, but it must be noted that engineering efforts may further improve speed for unstructured pruning. The focus of the paper is structured pruning, on which we achieve state-of-the-art compression rates. Importantly, compression of LLMs only needs to happen once after which a pruned model can be deployed infinitely many times without further cost. This motivates our method which takes longer to run but provides much better final test performance. We will add the timing results to the paper and make the last point clear from the text.\n\n2. Large model sizes\n\nWe agree that it would be interesting to assess performance on even larger models. Following the suggestions by the reviewers (zzzW, 2GDn), we now also include results for the larger Llama-v2 13B model and evaluated performance on downstream tasks (see reply to Reviewer 2GDn). We find that our method also outperforms baselines on this larger model, as well as on downstream tasks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244508101,
                "cdate": 1700244508101,
                "tmdate": 1700508798025,
                "mdate": 1700508798025,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TtxnA4nbva",
            "forum": "DYIIRgwg2i",
            "replyto": "DYIIRgwg2i",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2739/Reviewer_zF62"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2739/Reviewer_zF62"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a way to prune the weights of a pretrained LLM with a negligible loss in performance by iteratively solving a quadratic optimization problem using the curvature of a local minimum. To save the memory of materializing hessian, the work calculates the covariance layer-wise with Kronecker factorization. Also, the curvature is calculated incrementally and the remaining weights are corrected with a first-order term as more weights are pruned so that the weight remains in a local minimum."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The approach is sound and is presented well. Not having to materialize the hessian makes this amenable for LLM pruning."
                },
                "weaknesses": {
                    "value": "While it is believable that this method would generalize, the setup is unsatisfying in that the dataset used for compression is drawn from wikitext-2 which is a narrow domain (meaning the loss landscape may be easier to optimize and prune than a broader distribution), and the final model is evaluated only on the test perplexity of the same dataset. SparseGPT uses C4 and reports downstream performance on various standard benchmarks. The paper could really strengthen its claim by repeating the same setup as SparseGPT. Downstream benchmarks are required for a fair comparison and acceptance of the work."
                },
                "questions": {
                    "value": "Equation 2 should read `- log(D | theta)`. There is a typo in `General solution  We denote ... e_{q_k}` => `e_{k_q}`.\n\nRelated Work could include more previous works on LLM compression."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2739/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2739/Reviewer_zF62"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885185836,
            "cdate": 1698885185836,
            "tmdate": 1699636216210,
            "mdate": 1699636216210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xBiWlgev4x",
                "forum": "DYIIRgwg2i",
                "replyto": "TtxnA4nbva",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and help to improve the paper. We appreciate that the reviewer found the approach sound and well presented, further recognising our contributions to Hessian-based pruning amenable to LLMs.\n\n> Training with C4 instead of wikitext2\n\nUnlike SparseGPT which used the C4 dataset, we used the wikitext2 dataset for compression in our experiments. Although the amount of data is the same, C4 can be regarded slightly more general in that it contains multiple languages. We agree with the reviewer that it would be interesting to evaluate performance with C4. Following these comments, we have therefore also evaluated the use of the C4 dataset as reference data for compression in the case of structured pruning. We find that our model in this case still outperforms our baselines.\n\n**The additional results on downstream tasks can be found in our reply to Reviewer zzzW (also includes training with suggested C4 data)**\n\nFurther, we would like to stress that all results in our paper (including the SparseGPT) were trained on wikitext2 to enable fair comparison. To do so, we used the same dataloader and evaluation script as the official SparseGPT repo and reran all SparseGPT results to be trained on wikitext2. This even resulted in better scores for our SparseGPT baseline than the C4 trained results from the original SparseGPT paper. Yet, we find that our method using improved curvature estimates still outperformed the baselines. We are aware that these efforts might not have been made clear enough and will ensure this is clearly stated in the final version.\n\n> Downstream benchmarks\n\nThe reviewer raised the need for downstream benchmarks 'for a fair comparison and acceptance of the work'. Although perplexity scores in prior work tend to correlate well with downstream performance, we recognise the need for such additional experiments to offer a complete comparison between methods. We have performed downstream benchmarks on structured pruning compressed using wikitext-2 but also the C4 dataset.\n\n**The additional results on downstream tasks can be found in our reply to Reviewer zzzW**\n\nWe find that our method not only performs well in terms of test perplexity but also correlates well with downstream performance, outperforming the baselines on these downstream tasks.\n\n> Related work\n\nWe agree with the suggestion by the reviewer to extend the related work so that all important pruning work on large language model compression is included. In particular, we will include related work on structured pruning and other LLM pruning works that use a curvature-based approach.\n\nThis is indeed a typo. Typos will be fixed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245018884,
                "cdate": 1700245018884,
                "tmdate": 1700245018884,
                "mdate": 1700245018884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]