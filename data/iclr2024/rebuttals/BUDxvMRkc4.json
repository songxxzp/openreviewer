[
    {
        "title": "BLG: BALANCED LANGUAGE DISTRIBUTION AS GUIDANCE FOR ROBUST LONG-TAILED VISION CLASSIFICATION"
    },
    {
        "review": {
            "id": "Nryvk99hTD",
            "forum": "BUDxvMRkc4",
            "replyto": "BUDxvMRkc4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a framework designed to enhance CLIP in addressing long-tailed visual recognition challenges. This framework integrates a supervised contrastive loss mechanism, grounded on the transport plan, to fortify visual feature extraction. Several evaluations conducted on benchmarks corroborate that this proposed method significantly facilitates discriminative visual feature learning and achieves SOTA performance in long-tailed recognition tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The idea of this paper is clear and easy to follow.\n1. Experimental results show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Lack of innovation. The approach in this paper provides a more balanced prototype for visual pre-training models to guide the learning of visual feature extractors and designs supervised comparative learning loss. However a similar approach has appeared in previous long-tail methods [1]. The differences in this paper are: 1. A more robust pre-training model, CLIP, is utilized. 2. A text-based prototype design approach is used to replace the target anchor. These innovations are more limited.\n\n2. Some modules are without good motivation.\nFor example:\n- Why do we need a learnable linear classifier? The purpose of its existence seems to be the matching of visual features with textual features. However, the weights of the classifier will change during training, which does not narrow the gap between template label text and image feature distributions.\n-  In the unsupervised prototype-guided feature learning part, why choose cos similarity as the distance metric instead of other metric methods such as minimum entropy?\n-  For the modules of learnable classifier, unsupervised prototype-guided feature learning, and supervised contrastive loss, it seems to be an incremental improvement and not complementary, so why use them to train models together?\n\n**Reference**\n[1] Li T, Cao P, Yuan Y, et al. Targeted supervised contrastive learning for long-tailed recognition[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 6918-6928."
                },
                "questions": {
                    "value": "please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772634930,
            "cdate": 1698772634930,
            "tmdate": 1699636067587,
            "mdate": 1699636067587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W4v4t4q0TS",
                "forum": "BUDxvMRkc4",
                "replyto": "Nryvk99hTD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to pMKo: replies to the questions and suggestions"
                    },
                    "comment": {
                        "value": "Thanks for your kind suggestion and for providing related work to help refine our work.\n\n1. Reply to #W1. \n\nThis paper introduces a targeted contrastive learning loss (TSC) aimed at aligning class features with target features on the vertices of a regular simplex. We identify four key distinctions between our approach and TSC:\n\n**(1) Learning Targets and Complexity:**\n\nTSC employs SGD to learn targets, posing challenges in finding the optimal solution on a hypersphere [1,2]. This method imposes a demanding requirement that is difficult to fulfill perfectly. In contrast, our approach is more straightforward, avoiding complex assumptions and calculations to obtain balanced prototypes. Our prototypes, derived from textual features, carry rich semantic information, enhancing the discriminative power of image features. Unlike TSC, which constructs targets without class semantics, we leverage a more abstract and balanced text modality. **Therefore, our methods differ fundamentally, despite sharing a general motivation**. Additionally, TSC requires prior knowledge of the number of classes, whereas our method is more flexible.\n\n**(2) Implementation Complexity:**\n\nTSC faces the challenge of assigning class indices to each unlabeled anchor point, adding complexity to the method. In contrast, our approach constructs prototypes in a more natural and intuitive manner.\n\n\n**(3) Incorporation of Components:**\n\nTSC relies solely on supervised contrastive learning loss, while our method incorporates various components. TSC updates the model by minimizing the distance between visual features and assigned targets. In contrast, our method transforms balanced textual prototypes into a linear classifier, employing optimal transport for unsupervised alignment of visual features and prototypes. Furthermore, we introduce a supervised contrastive learning loss based on the transport matrix to enhance supervision signals and intra-class information. **Therefore, our contribution extends beyond a supervised contrastive learning loss. From the ablation study, our method can still perform best even without the proposed supervised contrastive loss.**\n\n\n**(4) Modality and Multimodality:**\n\nTSC operates as an image-only method, while our approach is multimodal. To assess TSC's impact, we integrated its method into our work and conducted a performance comparison. Indicating our proposed three losses as $L_{CE}$, $L_{OT}$ and $L_{SCT}$, we compare the performance replacing $L_{SCT}$ of $L_{TSC}$ based on ViT-B/16 and Places-LT dataset. As shown in the table below, **using textual prototypes to initialize anchor points and applying TSC in Phase B led to a significant performance drop, demonstrating that our method is not a mere substitution of anchor points in TSC with textual features.**\n\n| Methods            | Many-Shot | Medum-Shot | Few-Shot | Overall |\n| ------------------ | --------- | ---------- | -------- | ------- |\n| OURS               | 49.8      | 52.6       | 54.2     | 51.9    |\n| $L_{CE}$ + $L_{SCT}$ | 48.9 | 50.8   | 50.5 | 50.1    |\n| $L_{CE}$ + $L_{OT}$ + $L_{SCT}$ | 48.9 | 51.5   | 53.8 | 50.5    |\n\n\nIn summary, these four aspects highlight the substantial differences between our method and TSC, reinforcing the unique contributions of our approach.\n\n[1] https://arxiv.org/pdf/physics/0609231.pdf\n[2] https://arxiv.org/abs/cond-mat/0506616"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209227613,
                "cdate": 1700209227613,
                "tmdate": 1700209227613,
                "mdate": 1700209227613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FuEl5VKvql",
                "forum": "BUDxvMRkc4",
                "replyto": "Nryvk99hTD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "2. Reply to #W2 and Reply to #W3"
                    },
                    "comment": {
                        "value": "2. Reply to #W2.\n\nThe linear classifier plays a pivotal role in this context, serving as a crucial bridge. **Its utilization is essential for maintaining coherence between visual and textual features.** This necessity arises because, in the second phase, an additional residual module is introduced to refine biased visual features. Subsequently, the linear classifier serves a dual purpose in the final classification and is ultimately employed to guide the refinement of biased visual features using balanced textual initialized prototypes.\n\nWhen we use textual features to initialize the classifier, our classification process no longer depends on the text prompt and text encoder. In the learning process of the linear classifier, we initialize with textual features and use one-hot labels and cross-entropy loss to optimize it. **This approach means that the decision boundary constantly adapts to the input mini-batch.** As performance ascends, the alignment between the textual features initialized classification boundary and visual features achieves a more nuanced congruence. Consequently, the task can be comprehended as the reduction of the gap between textual and visual features. Finally, the parameters of the classifier are harnessed for prototype-guided matching to enhance visual features.\n\nIn instances where the linear classifier is not employed, our model structure aligns with BALLAD. **The results of the ablation study in Table 4 reveal that without the use of a trainable linear classifier, our approach outperforms BALLAD on ImageNet-LT and Places-LT by 1.1\\% and 0.3\\%, respectively.** Nevertheless, the incorporation of the classifier, accompanied by continuous refinement of the classification boundary to strengthen the coherence between textual and visual features, leads to further performance enhancements.\n\nFurthermore, we supplement our findings by presenting experimental results when the classifier is fixed, and only the optimization of residual links to improve visual features is pursued. As indicated in the table below, when the classifier is frozen, a certain performance impact is observed, resulting in a 1.5\\% decrease. Therefore, a learnable classifier can help continually and adaptively keep the consistency between textual features (prototypes) and the input visual features, from the perspective of both motivation and experiment results.\n\n | Methods            | Many-Shot | Medum-Shot | Few-Shot | Overall |\n | ------------------ | --------- | ---------- | -------- | ------- |\n | Learnable classifier | 49.8      | 52.6       | 54.2     | 51.9    |\n | Frozen classifier | 49.0 | 51.4   | 50.8 | 50.4    |\n\n**Besides, we update a visualization result in Appendix E.3 to demonstrate how the learnable linear classifier benefits our method and keeps the consistency of visual features and the corresponding textual prototypes (the weight of the classifier).**\n\n3. Reply to #W3.\n\n A cost matrix serves as a representation of the cost function in a discrete optimal transport problem, where goods or resources are distributed among a finite number of sources and destinations. This matrix is structured with a row for each source and a column for each destination, where each entry denotes the cost of transporting one unit of goods or resources from the corresponding source to the respective destination. In practical terms, various measurements capable of computing the distance between two distributions can function as a cost matrix. Examples include cosine similarity, Euclidean distance, or a matrix derived through learning in a specific manner (reverse OT problem).\n\nIt is worth noting that minimum entropy might be unsuitable as a measurement for the cost matrix in this context. For our purposes, we opt for cosine similarity due to its universality and superior overall performance compared to other distance metrics. The comparison results of cosine similarity and Euclidean distance, employed as a cost matrix based on Places-LT and ViT/B-16, are presented in the following table:\n\n| Methods            | Many-Shot | Medum-Shot | Few-Shot | Overall |\n| ------------------ | --------- | ---------- | -------- | ------- |\n| Cosine Similarity               | 49.8      | 52.6       | 54.2     | 51.9    |\n| Euclidean Distance | 49.0 | 52.1   | 54.5 | 51.7    |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209694183,
                "cdate": 1700209694183,
                "tmdate": 1700405195335,
                "mdate": 1700405195335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XsJdf2yiuM",
                "forum": "BUDxvMRkc4",
                "replyto": "Nryvk99hTD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer pMKo,\n\nGiven the constrained timeframe for further discussion, we genuinely appreciate any feedback you may have on our revised submission. If there are additional questions or points of clarification needed, we are more than willing to promptly address them.\n\nAgain, we express our gratitude for your time, extensive efforts, and valuable insights. Thank you for the opportunity to engage in this discussion.\n\nBest regards, The Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443557902,
                "cdate": 1700443557902,
                "tmdate": 1700443557902,
                "mdate": 1700443557902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pLlhCw985R",
                "forum": "BUDxvMRkc4",
                "replyto": "FuEl5VKvql",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer pMKo"
                    },
                    "comment": {
                        "value": "For #W2, from the above explanation, there is no reasonable reason why using text initialization would achieve your goal. It is mentioned that it \"initializes with textual features and uses one-hot labels and cross-entropy loss to optimize it.\" This suggests that the method utilizes long-tailed data as input and employs a CE loss optimization object. This optimization goal may not be appropriate as a means of addressing the mismatch issue between refined visual features and textual features. Can you provide some more plausible explanations?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641414613,
                "cdate": 1700641414613,
                "tmdate": 1700641414613,
                "mdate": 1700641414613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LV0kuY3Dgg",
                "forum": "BUDxvMRkc4",
                "replyto": "7zYYiMVd8a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_pMKo"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer pMKo"
                    },
                    "comment": {
                        "value": "In addition, it is important to align the representations learned by the model with the distribution of the test data. How do you ensure it while aligning the long-tailed data features with textual features?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642727785,
                "cdate": 1700642727785,
                "tmdate": 1700642727785,
                "mdate": 1700642727785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zaEftd5y5f",
            "forum": "BUDxvMRkc4",
            "replyto": "BUDxvMRkc4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
            ],
            "content": {
                "summary": {
                    "value": "This study discovers that the fine-tuned CLIP's textual features are more balanced and discriminative compared to its visual counterparts. Building on this, the research proposes utilizing balanced textual features as prototypes to guide the learning of robust representations for biased visual features. The CLIP is further fine-tuned through contrastive learning, followed by the optimization of biased visual representations using linear adapters and the introduction of optimal transport distance to help decouple biased visual features. Additionally, a supervised contrastive learning loss based on the transport plan is designed. Experimental results indicate that the approach excels in leveraging visual-language information for imbalanced visual recognition, achieving state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Extensive experiments on ImageNet-LT, Places-LT, and iNaturalist 2018 have demonstrated the effectiveness of the proposed method.\n2. Comprehensive visualizations and ablation studies were conducted to validate the impact of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The experiment results indicate that the method underperforms for \u201cmany\u201d classes in long-tail data.\n2. The proposed method employs a two-stage training process and fine-tunes the Full-CLIP, which requires significant computational resources and has a prolonged training duration.\n3. The proposed method doesn't seem to have a specific design tailored for long-tail data. The approach of using textual features as guidance for better image features can be applied to situations with limited image feature quality for various reasons, such as long-tail, few-shot, noisy data, generated data, low-resolution data, and so forth."
                },
                "questions": {
                    "value": "1. Why does the proposed method underperform in \u201cmany\u201d classes of LT dataset? An analysis of the underlying reasons would be appreciated.\n2. The experimental results show that the proposed method underperforms in \u201cmany\u201d classes of LT dataset. Does this imply that the method is primarily effective for situations with the few-shot scenario (i.e., \u201cmedium\u201d and \u201cfew\u201d classes in long-tail datasets)?\n3. Balanced sampling is a fundamental operation in long-tail methods. Why is random sampling used when fine-tuning the CLIP in the initial stage? Is it to intentionally obtain a CLIP encoder with strong biases caused by imbalance?\n4. Reference [1] also leverages CLIP's text features to enhance the discriminative power of image features. In [1], directly using text features and image features in concurrent training a linear classifier can achieve significant improvements in few-shot tasks. However, compared to the method in this paper, the method in [1] is much simpler, with much faster computation speeds and far less computational overhead.\n[1] https://arxiv.org/abs/2301.06267"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774062941,
            "cdate": 1698774062941,
            "tmdate": 1699636067513,
            "mdate": 1699636067513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "huagX0rxtM",
                "forum": "BUDxvMRkc4",
                "replyto": "zaEftd5y5f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to nBTe: replies to the questions and suggestions"
                    },
                    "comment": {
                        "value": "Thanks for your kind suggestion and for providing related work to help refine our work.\n1. Reply to #W1, #Q1 AND #Q2:\n\nIt involves a trade-off in performance among three different shot divisions in long-tail. Despite this, our method boasts several key advantages:\n\n(1) **We view imbalanced recognition as a task primarily concerned with enhancing the classification performance of tail classes, minimizing the performance gap with head classes, and achieving superior overall classification performance.** While BALLAD and VL-LTR excel in overall performance by mainly improving head class results, they fall short in addressing the tail. For instance, when based on RN50 and additional texts for training, VL-LTR achieves 77.8% accuracy in the Many-shot in ImageNet-LT but only 50.8% accuracy in the Few-shot, resulting in a substantial 27.0% performance gap. In contrast, our method exhibits a significantly lower performance gap of 8.7%.\n\n(2) **Our approach outperforms in medium-shot, few-shot, and overall performance, effectively narrowing the performance gap between these divisions and many-shot.** This aligns with our motivation to attain a more balanced visual feature distribution, resulting in a more balanced classification performance. Notably, we achieve a more substantial performance improvement in few-shot scenarios compared to previous methods. On ImageNet-LT, our method showcases a 10% increase in the few-shot category. Similarly, on Places-LT, the improvement exceeds 12%.\n\n**Furthermore, we wish to draw a distinction between the few-shot division in long-tailed recognition and few-shot learning in the general sense.** The former serves as a means and perspective for performance evaluation after data division under the long-tail problem, indicating performance on minority class samples. In contrast, the latter focuses more on the disparity between base classes and novel classes, involving learning with limited samples after the introduction of new classes to achieve improved performance. In the former, each arrival constitutes a completely imbalanced dataset, requiring the model to learn and classify the entire dataset. Ultimately, our evaluation considers performance under three different divisions and overall performance.\n\n2. Reply to #W2:\n\nFully fine-tuning of the CLIP encoders indeed introduces a large computation cost. We notice there are some parameter-efficient methods developed for tuning  CLIP encoders. We will focus on this issue and try more efficient ways to improve our method. \n\n3. Reply to #W3:\n\nThanks for your kind suggestion. We will try our method on more scenarios, such as few-shot learning and noisy data. Due to the rebuttal time limit, we may not give comprehensive results now.\n\n4. Reply to #Q3:\n\nWe employ a random sampling strategy when fine-tuning CLIP encoders in the first stage, driven by three key considerations.\n\n**Firstly, as indicated in related works [1,2], the feature extractor's impact on classification performance may not be significant.** The pivotal factor in imbalanced classification accuracy lies in the learning of the classifier. Consequently, utilizing random sampling during encoder training or fine-tuning, even with skewed visual features, remains a viable approach. Following this, methods often choose to fix encoders and employ new sampling strategies to train the classifier. Based on this concept, we adopt a random sampling strategy during encoder fine-tuning in the first stage. In the second stage, we shift to a balanced sampling strategy to refine visual features and train a balanced classifier. This decision aligns with our motivation.\n\n**Secondly, experiment results indicate that using a balanced sampling strategy to fine-tune encoders in the first stage is not the optimal approach.** Updated experiment results on sampling strategy in Appendix H support this observation. Notably, employing random sampling in the first stage and balanced sampling in the second stage yields the best results, outperforming the strategy of using balanced sampling in both stages. This finding is consistent with conclusions drawn in previous related work, such as BALLAD.\n\n**Finally, a new t-SNE visualization result in Appendix H compares visual features extracted from the same class under two different sampling strategies.** Even when a balanced sampling strategy is used in Phase A, it does not significantly enhance tail classes' feature distribution distinctiveness. The visual feature distribution obtained by balanced sampling does not markedly differ from that obtained by random sampling on tail samples. However, for head samples, visual features of the head class are less densely clustered than those obtained by random sampling. This underscores the necessity of employing different sampling strategies in distinct stages from an alternative perspective.\n\n[1] Decoupling representation and classifier for long-tailed recognition.\n[2] Improving calibration for long-tailed recognition."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205904709,
                "cdate": 1700205904709,
                "tmdate": 1700205904709,
                "mdate": 1700205904709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gKVolFmJ4z",
                "forum": "BUDxvMRkc4",
                "replyto": "zaEftd5y5f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer nBTe,\n\nGiven the constrained timeframe for further discussion, we genuinely appreciate any feedback you may have on our revised submission. If there are additional questions or points of clarification needed, we are more than willing to promptly address them. \n\nAgain, we express our gratitude for your time, extensive efforts, and valuable insights. Thank you for the opportunity to engage in this discussion.\n\nBest regards, The Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443536754,
                "cdate": 1700443536754,
                "tmdate": 1700443536754,
                "mdate": 1700443536754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NofMchiboE",
                "forum": "BUDxvMRkc4",
                "replyto": "zaEftd5y5f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_nBTe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for your diligent efforts and feedback, which have resolved some of my queries. However, I still hold reservations regarding the following points:\n\n1. Same as what I mentioned in weakness. As a paper to solve the LTR problem, the proposed method does not appear to have a specific design tailored for handling long-tail data distributions.\n\n2. The authors have only compared the results of combining their method with [1], without comparing it with the results of using [1] in isolation. \nAdditionally, the results indicate that the method in [1] only slightly decreases the performance for 'many' and 'medium' split sets while enhancing the 'few' set. However, the complexity of the method in [1] is significantly lower than that of the proposed method. Moreover, it employs a CLIP linear probe without the need to fine-tune the entire CLIP model, making it a more efficient and direct approach to leveraging textual features.\n\n[1] https://arxiv.org/abs/2301.06267\n\n3. There is a substantial domain gap between textual and image features. Is it reasonable to use textual features as initial centroids to guide the clustering of image features? \nSpecifically, while textual features possess strong discriminative power, the distance between textual features and image features (in the test set) remains considerably larger than that between image features in the training and test sets. Using textual features to guide the training set image features might yield more discriminative features and decision boundaries. However, this approach could increase the distance between the original (training set) image features and the (test set) image features, potentially leading to overall poorer performance."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633755252,
                "cdate": 1700633755252,
                "tmdate": 1700633755252,
                "mdate": 1700633755252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jDuKGefjRg",
                "forum": "BUDxvMRkc4",
                "replyto": "zaEftd5y5f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nBTe: replies to the questions"
                    },
                    "comment": {
                        "value": "Dear Reviewer nBTe,\n\nThanks for your insightful question and informative feedback to help refine the paper.\n\n\n1. Reply to #W1\n\nAllow us to elaborate on how our approach aligns with the challenges posed by long-tailed recognition (LTR), both in motivation, method design, and experimental outcomes.\n\n(1) **Motivation:**\n\n**Our motivation is grounded in the persistent balance and separability observed in text feature distributions even after processing imbalanced datasets through CLIP encoders.** Recognizing the inherent challenges of long-tailed data, we aim to harness this balance in text features to guide the learning of highly discriminative visual representations, thereby enhancing the performance of long-tailed vision recognition.\n\n(2) **Method Design:**\n\n**In the design of our method, we initiate the classifier (prototypes) with balanced text features and employ a balanced data sampling strategy during model training.** By utilizing optimal transport to match visual features with their prototypes, our approach aims at achieving a delicate balance between learning discriminative visual features and addressing the imbalances inherent in long-tailed datasets. The incorporation of a residual structure refines biased visual features, promoting their balance and separability in the long-tailed context.\n\n(3) **Experimental Insights:**\n\n**The experimental outcomes corroborate the effectiveness of our method in the long-tailed recognition scenario, showcasing state-of-the-art performance.** In response to the reviewer's inquiry about the applicability to limited data scenarios, our additional experiments demonstrate the adaptability of our method to few-shot learning tasks. While the versatility of our approach is evident, it is essential to underscore that **the primary strength of our method lies in its substantial improvement in addressing the challenges posed by long-tailed datasets in visual recognition.**\n\n**In summary, our method exhibits a nuanced approach tailored to the intricacies of long-tailed recognition.** By leveraging the balance in text features, our method not only specifically excels in imbalanced vision recognition but also extends its effectiveness to the nuanced domain of few-shot learning, reinforcing its applicability across various challenges in visual recognition tasks.\n\n\n2. Reply to #W2\n\nWe appreciate the opportunity to provide further clarification and additional insights regarding the comparison with the referenced work [1], and we acknowledge the need for a more comprehensive and accurate depiction of our method's nuances.\n\nFirstly, we would like to revisit the essence of [1]. **The primary objective of [1] is to enhance CLIP's performance in few-shot learning tasks by incorporating paired visual and textual features as input.** This method utilizes cross-modal information to train a robust classifier, ultimately improving CLIP's classification performance on few-shot tasks. The core of [1] lies in the introduction of textual information as input, a strategy we also explored in our experiments.\n\nIn our previous response, we conducted experiments by **incorporating the idea of introducing text as input into our existing method**, which involves the full fine-tuning of CLIP encoders, the vision adapter, and the utilization of prototype-guided matching loss. **Therefore, the outcome of these experiments revealed a marginal decrease in performance compared to our original method.**\n\nTo provide more clarity on the differences, we provide additional experiments. We utilized the released code from [1] and evaluated its performance on the Long-Tailed Recognition (LTR) task. Without fine-tuning CLIP encoders and without employing any of our proposed losses, [1] achieved an overall performance of only 42.9% on Places-LT based on ViT-B/16. This result underscores that **while [1] is effective and insightful for few-shot learning, it is not inherently tailored for the challenges posed by LTR tasks, and its performance does not significantly contribute to improving classification accuracy.**\n\nFurthermore, when fine-tuning CLIP encoders and exclusively using cross-modal input for training the classifier, [1] achieved an overall performance of 48.20%. **This outcome, although indicative of the effectiveness of [1] in few-shot learning, falls short in comparison to the performance achieved by our method in LTR tasks.**\n\n**In summary, while [1] offers a valuable approach for few-shot learning, its adaptability to LTR tasks is limited, as evidenced by the comparative experiments.** Our method, designed specifically for LTR tasks, demonstrates **superior performance and effectiveness in addressing the challenges inherent in long-tailed datasets.**\n\n| Methods             | Overall |\n| ------------------ |  ------- |\n| Linear Probing + [1] | 42.89    |\n| Fully Fine-tuned + [1] | 48.20    |\n| OURS | 51.90    |"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707698142,
                "cdate": 1700707698142,
                "tmdate": 1700707724653,
                "mdate": 1700707724653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "coBpu7Tn71",
            "forum": "BUDxvMRkc4",
            "replyto": "BUDxvMRkc4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
            ],
            "content": {
                "summary": {
                    "value": "After the advent of vision-language pre-training, numerous works have adapted the pre-trained vision-language model to various vision tasks, including long-tailed recognition. This paper first presents empirical evidence that textual features remain balanced even after fine-tuning in the context of long-tailed classification. Based on this, the authors propose a framework that leverages balanced textual features as a guide to obtain more robust visual features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The empirical finding that, during the fine-tuning of the entire vision-language pre-trained model on long-tailed data, textual features tend to achieve balance is quite intriguing. This paper goes beyond this observation and contributes to the community by proposing a concrete methodology that leverages balanced textual features to rectify imbalanced visual features.\n2. The thorough ablation study conducted on the elements comprising \"Phase B,\" proposed in this work, effectively underscores that the suggested $L_{\\text{OT}}$ and $L_{\\text{SCT}}$ indeed enhance performance."
                },
                "weaknesses": {
                    "value": "1. The overall structure of this paper, which deals with challenges in contrastive learning methods due to class imbalance and suggests remedies, evokes thoughts of Suh and Seo (2023). Nevertheless, the current paper does not include any discourse on this topic.\n2. Moreover, while one could mention Kang et al. (2021) as a seminal work on achieving a balanced and discriminative feature space in long-tailed classification scenarios, this is also not discussed.\n\n---\nKang et al., 2021, Exploring Balanced Feature Spaces for Representation Learning.  \nSuh and Seo, 2023, Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels."
                },
                "questions": {
                    "value": "1. Could you offer some informed speculation about why there is a tendency for textual features to be balanced?\n2. Since comparing performance between different architectures does not hold much significance, it would be better to provide results for RN50 and ViT-B/16 in separate groups.\n3. Does the proposed approach result in any additional training expenses? For instance, what are the costs associated with setting up an optimal transport plan?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823633079,
            "cdate": 1698823633079,
            "tmdate": 1700329326204,
            "mdate": 1700329326204,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cQjsCxx3r0",
                "forum": "BUDxvMRkc4",
                "replyto": "coBpu7Tn71",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to V37f: replies to the questions and suggestions"
                    },
                    "comment": {
                        "value": "Thanks for your kind suggestion and for providing related work to help refine our work.   \n\n1. Reply to #w1 and #w2:\n\nThank you for sharing these papers. We noticed with them before, as they share a similar motivation to ours: learning balanced and discriminative visual features for long-tailed recognition. Despite these similarities, there are significant differences between our approaches.\n\n**The first paper** maximizes mutual information between visual latent features and ground truth labels, employing a contrastive learning method based on image modality. In contrast, our method leverages complementary textual information, to facilitate the learning of robust visual features. **Notably, our approach does not explicitly model or utilize label frequency**, as seen in the first paper, where it is used to compute logit adjustment terms and construct class-wise queues. Instead, our focus is on achieving high separability between classes based on textual features. **Moreover, our method does not rely on the teacher-student network structure**. **Furthermore, our proposed supervised contrastive learning loss is only part of our contribution**, which serves as a complementary for the OT-based prototype-guided matching. As shown in Table 3, our method can still perform best even without the proposed $\\mathcal{L}_{\\text{SCT}}$.\n\n**The second paper** employs k-positive samples as anchors for contrastive loss, deviating from using augmented instances. This approach, dependent on the number of samples in the minority class, cannot guarantee the distance between k-positive samples from different classes, potentially causing tail classes to be pushed closer together. **In contrast, our method is independent of the number of samples and consistently identifies prototypes with strong discriminability based on textual features.** Additionally, our approach is multimodal, utilizing optimal transport to align visual features and prototypes in an unsupervised manner. We introduce class-level supervision signals based on the transport matrix to further enhance feature learning.\n\n**In summary**, while these referenced papers effectively aim to learn more discriminative visual distributions, our work distinguishes itself through a **focus on multimodal data, the incorporation of optimal transport for balanced prototype-guided visual representation learning and class-level supervision signals based on transport plan**. We will discuss more detailed differences in the related works .\n\n2. Reply to #Q1: \n\n**Firstly**, we argue that text features are a more high-level abstract semantic representation, which inherently has better discriminability. Therefore, they can serve as queries in the Vision Language Model to perform tasks such as classifying the input image features. This discriminability, in our work, is manifested as balance. That is, the language encoder is less affected by the tail class. As shown in the visualization in Appendix A, textual features always maintain good separability and balance, regardless of whether the language encoder of CLIP has been fine-tuned or not. **Secondly**, when we construct the text prompt for the input image, we use the class name information. This kind of information already provides high separability for textual features. From the text features, we can often directly infer which object a sentence is describing by the classname. Therefore, they can serve as prototypes with good classification properties. We aim to make visual features better cluster around the corresponding prototypes. **In addition**, when we use balanced textual features to build prototypes, each class uses the same number of text prompts. From this perspective, the prompts between different classes are balanced. The head class does not have more prompts than the tail class. **Finally**, when we fine-tune CLIP, we only use the InfoNCE loss in the image-retrieval direction, because our goal is to use text to classify images, which can also reduce the negative impact on textual features during Phase A to some extent.  \n\n3. Reply to #Q2. \n\nThanks for your kind reminder. We will update our tables for a clear comparison in the revision.  \n\n4. Reply to #Q3. \n\nYes. The introduction of OT loss brings some lightweight additional computational burden. The added computational burden mainly comes from the need to calculate the optimal transport (OT) distance when aligning visual features and prototypes. We have comprehensively compared our method with BALLAD and VL-LTR in Appendix G. As shown in Table 14, our method is comparable to other methods in terms of the training cost. Compared with BALLAD, we added 10 epochs in the training phase and required more time, but we needed less inference time and had a significant improvement in performance (51.9% vs 49.5%). Compared with VL-LTR, we require fewer epochs for training, less training time, and no additional data. Still, we achieve better performance (51.9% vs 50.1%) than VL-LTR."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204058474,
                "cdate": 1700204058474,
                "tmdate": 1700204058474,
                "mdate": 1700204058474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zqm6bCYnZX",
                "forum": "BUDxvMRkc4",
                "replyto": "coBpu7Tn71",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "General responce and Discussion about the connection with contrastive learning"
                    },
                    "comment": {
                        "value": "**Connection with contrastive learning.** \n\nWe sincerely appreciate the thoughtful reviews and the valuable insights provided by each reviewer. We have carefully considered your comments regarding the connections and differences between our method and previous works based on contrastive learning for imbalanced image classification. We acknowledge the similarities but would like to emphasize the essential differences that set our approach apart.\n\n**(1) Distinguishing from Contrastive Learning:**\n\nOur method is not solely based on contrastive learning. While we use CLIP and propose the use of a supervised contrastive learning loss function, leveraging the optimal transport matrix to enhance model learning and improve visual features, this aspect represents only a supplementary component within our broader method. Furthermore, our experiments demonstrate that even without the proposed contrastive supervision loss function, our method achieves state-of-the-art performance.\n\n**(2) Optimal Transport Matching vs. Contrastive Learning:**\n\nWhen addressing the challenge of improving biased visual features to achieve better balance and separability, we uniquely formulate this problem as an optimal transport matching problem. This approach stands out differently from traditional contrastive learning. Our objective is to minimize the optimal transport distance, ensuring a closer alignment between visual features of the same type and their prototypes while creating a more significant separation between different types.\n\n\n**(3) Leveraging Complementarity of Multi-Modal Information:**\n\nIn the context of imbalanced image classification, effectively leveraging the complementarity between multi-modal information for robust visual feature learning remains an open challenge. Our experiments in rebuttal reveal that naively employing text features as traditional positive samples or anchor points in contrastive learning methods does not directly contribute to effective model learning. Therefore, we find it imperative to design and implement more nuanced methods to address imbalanced image classification under multi-modal conditions.\n\n\nWe believe these distinctions underscore the uniqueness of our method and its contributions to addressing the complexities of imbalanced image classification. We appreciate your thorough evaluations and welcome any further suggestions or inquiries."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213693632,
                "cdate": 1700213693632,
                "tmdate": 1700213693632,
                "mdate": 1700213693632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aSfE273Md8",
                "forum": "BUDxvMRkc4",
                "replyto": "zqm6bCYnZX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1398/Reviewer_V37f"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "My main issue was the absence of discussions regarding prior works. It is crucial for the researchers to clearly identify any similar existing studies concerning their newly proposed ideas, delineate the points of similarity, and distinctly outline what aspects are novel. The authors addressed this concern through the revision, so I am accordingly raising the score.\n\nAdditional Note: it seems that Figure 7 incurs some delays in the image rendering process. One possible solution may be substituting it with a PNG format."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329311683,
                "cdate": 1700329311683,
                "tmdate": 1700329311683,
                "mdate": 1700329311683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]