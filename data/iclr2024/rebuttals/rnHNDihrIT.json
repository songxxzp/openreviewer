[
    {
        "title": "Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets"
    },
    {
        "review": {
            "id": "7FGEtuhj3m",
            "forum": "rnHNDihrIT",
            "replyto": "rnHNDihrIT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_BZGK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_BZGK"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an EM-inspired algorithm called Stylized Offline RL (SORL) to extract diverse strategies from heterogeneous offline RL datasets. Based on the learned behavior policies, the paper then applies an advantage-weighted style learning algorithm to improve their performance further. The authors demonstrated their algorithm's effectiveness with experiments on six Atari games and one online mobile basketball game, where SORL outperforms other baselines regarding quality, diversity, and consistency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Extracting diverse behaviors from an offline RL dataset is an interesting problem. SORL efficiently solves the problem following an EM-based approach. The proposed evaluation criteria, considering quality, diversity, and consistency, provide a nice guideline for other researchers to follow. The algorithm also performs better than other existing baselines in multiple offline RL datasets, including the \"Dunk City Diversity\" dataset, which contains extremely diverse behaviors. Finally, the paper is overall well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "1. There is no theoretical ground for naively replacing $A^{\\mu^{(i)}}$ with $A^\\mu$ without importance sampling corrections. At least an empirical ablation study should be provided if it is difficult to devise a theoretical justification.\n\n2. It isn't easy to understand the proof presented in Appendix B.\n\n    (1) $\\pi^{(i)}$ needs to satisfy the constraint $\\int \\pi^{(i)}(a\\mid s)\\,da=1$ for all $s$. The optimal solution might not be a critical point.\n\n    (2) $A^\\mu(s, a)-\\lambda \\mu^{(i)}(a\\mid s)+\\lambda \\pi^{(i)}(a\\mid s)+\\lambda=0$ does not imply $\\pi^{(i)*}(a\\mid s)\\propto \\mu^{(i)}(a\\mid s)\\exp(\\frac{1}{\\lambda}A^\\mu(s, a))$.\n\n    (3) The normalization constant for $\\pi^{(i)*}$ is ignored in (14).\n\n3. The diversity metric proposed by the authors does not consider how different the styles are. For example, consider the case where $\\pi^{(i)}(a=k\\mid s)=\\frac{1}{K}+\\epsilon_k(s)$ where $K$ is the number of possible actions and $\\epsilon_k(s)$ is a small number chosen arbitrarily. Then $\\hat{p}(z=j\\mid traj)$ would be determined by the values of $\\epsilon_k(s)$, so $p_{popularity}$ would be close to a uniform distribution, which is the distribution that maximizes the entropy. However, the learned styles are far from being diverse.\n\n### Minor comments:\n\n1. How about using $\\tau$ instead of $traj$? I think this notation is widely accepted.\n\n2. The $-$ sign seems missing in (9).\n\n3. \u00a75.1 Off-RLMPP \u2192 Off-RLPMM (appears twice on the fourth line)\n\n4. Appendix B: Unmatched parentheses $($ in (13) and on the first line of p.15\n\n5. Appendix B: $exp$ \u2192 $\\exp$ on the second line of p.15 and in the second and third equation of (14)\n\n6. Appendix B: In (14), $t$ is the index of summation, but it does not appear in the summand.\n\n7. Appendix B: Second $=$ \u2192 $\\approx$"
                },
                "questions": {
                    "value": "1. \u00a75.3 states that the character has to combat against an AI opponent. What is the strategy of the opponent? Is SORL robust to the changes in the opponent's strategy?\n\n2. All experiments were conducted on environments with discrete action spaces. How does SORL perform in continuous action environments?\n\n3. How was the diversity metric measured for InfoGAIL? To my knowledge, InfoGAIL does not explicitly split the policy into multiple clusters but instead learns a multi-modal policy.\n\n4. In Appendix C, the paper states that\n\n   > Besides, in order to ensure balanced learning among all the styles, we share the main network and use a lora module to discriminate different styles.\n\n   The explanation on the \"lora\" module seems missing. Also, I recommend moving this part to the main paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Reviewer_BZGK",
                        "ICLR.cc/2024/Conference/Submission1156/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697640171281,
            "cdate": 1697640171281,
            "tmdate": 1700656249142,
            "mdate": 1700656249142,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SlySHwTIVI",
                "forum": "rnHNDihrIT",
                "replyto": "7FGEtuhj3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your thoughtful review. We have carefully considered the comments and have provided detailed clarification. We appreciate it if you have any further feedback.\n\nQ1: There is no theoretical ground for naively replacing $A_{\\mu^{(i)}}$ with $A_\\mu$ without importance sampling corrections. At least an empirical ablation study should be provided if it is difficult to devise a theoretical justification.\n\nA1: We conducted a comparison of performance (referred to as quality) using $A_{\\mu^{(i)}}$ and $A_\\mu$. We employ a soft clustering approach where each trajectory has a probability of belonging to each cluster. During the training of $A_{\\mu^{(i)}}$, the dataset trajectories are weighted based on the probability obtained from the soft clustering method. In the experiment:\n\n|  | SpaceInvaders | MsPacman | MontezumaRevenge |\n| ---- | ---- | ---- | ---- |\n| SORL with $A_\\mu$          | $387.1$ | $622.2$ | $306.7$ |\n| SORL with $A_{\\mu^{(i)}}$ | $292.2$ | $551.3$ | $0.0$ |\n\nThe empirical results indicate that utilizing $A_{\\mu}$ during the learning process enhances the quality of policies. The reason that $A_\\mu$ generally outperforms $A_{\\mu^{(i)}}$ is that it only focus on part of the dataset.\n\nQ2: It isn't easy to understand the proof presented in Appendix B.\n\nA2: We sincerely appreciate the reviewer for pointing it out. We've refined the proof, which is now in Appendix D.\n\nQ3: The diversity metric proposed by the authors does not consider how different the styles are.\n\nA3: We appreciate the reviewer to point this out. We agree that one limitation of the current metric of diversity is that it relies on the clustering $\\hat{p}(z=j|\\tau)$. Comparing the diversity of two sets of policies are not trivial. Previous research, such as InfoGAIL, also did not propose a specific metric for evaluating diversity. Instead, they relied on visualization to showcase the diversity of learned policies.\nIn order to provide a more comprehensive representation of the diversity of learned policies, we have proposed additional diversity metrics, under which the described problem does not exist. We have incorporated three additional metrics into our testing: the **skill metric**, the **OT (optimal transport) metric**, and the **discrimination metric**. The skill metric measures the dissimilarity of actions, while the OT metric quantifies the disparity in state distribution. The discrimination metric assesses whether the policies can be distinguished by a neural network. The table below showcases the diversity of different style learning methods in the grid shooting environment, measured using various metrics. Higher values indicate greater diversity. For a detailed description of these metrics, please refer to the Appendix F. Based on the results of the additional diversity metrics presented in Table below, we can conclude that the SORL algorithm is capable of obtaining diverse and high-quality policies. InfoGAIL fails to learn diverse policies, while Off-RLPMM has considerably lower winning rate.\n\n| | Popularity | Skill | OT | Discrimination | Winning rate |\n| ---- | ---- | ---- | ---- | ---- | ---- |\n| SORL                       | $0.60$ | $15.7$ | $29.14$ | $0.75$ | $55.1$% |\n| SORL w/o advantage | $0.61$ | $21.2$ | $31.61$ | $0.97$ | $45.2$% |\n| Off-RLPMM              | $0.68$ | $23.2$ | $29.87$ | $0.99$ | $40.4$% |\n| InfoGAIL                    | $0.00$ | $2.0$ | $28.53$ | $0.69$ | $55.3$% |\n\nQ4: \u00a75.3 states that the character has to combat against an AI opponent. What is the strategy of the opponent? Is SORL robust to the changes in the opponent's strategy?\n\nA4: The opponent is an agent with a fixed strategy that move randomly, and shoot with a probability once the shooting action cools down.\n\nTo check whether the agent is robust to the changes in the opponent's strategy, we introduce a moderately strong opponent with DQN training for 20,000 steps.\n\n|  | Reward (shoot) | Reward (star) | Winning rate |\n| ---- | ---- | ---- | ---- |\n| Policy 1 | $1.1$ | $0.11$ | $19$% |[\u6570\u5b66\u516c\u5f0f]\n| Policy 2 | $0.1$ | $1.27$ | $54$% |\n\nDespite the opponent's increased strength, the policies employed by our agent remained stylized. Consequently, the winning rate experienced a decrease compared to previous opponents, but it remained competitive. Specifically, the winning rate of policy 1, which excels at shooting enemies, declined to $19$% due to the increased difficulty of successfully shooting and winning against the stronger opponent. On the other hand, the winning rate of policy 2, which specializes in collecting stars and focuses on escaping from the enemy's shooting, was not significantly affected."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455925073,
                "cdate": 1700455925073,
                "tmdate": 1700455925073,
                "mdate": 1700455925073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xxEPBY1MWv",
                "forum": "rnHNDihrIT",
                "replyto": "7FGEtuhj3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Part 2)"
                    },
                    "comment": {
                        "value": "Q5: All experiments were conducted on environments with discrete action spaces. How does SORL perform in continuous action environments?\n\nA5: Although we don't include environment with continuous action space, our algorithm can be easily adapted to the continuous action space. The adaptation process involves representing action probability generated by the policy network as a continuous distribution (often Gaussian). Our method is not limited to discrete action space.\nThe main contribution of SORL is to propose a novel framework that learns diverse and high-quaity qualities from offline datasets. The distinction between discrete and continuous action spaces in experiments lies outside the primary focus of this paper. We do recognize its importance for future work.\n\nQ6: How was the diversity metric measured for InfoGAIL? To my knowledge, InfoGAIL does not explicitly split the policy into multiple clusters but instead learns a multi-modal policy.\n\nA6: We set the latent space of InfoGAIL as an $m$-dim one-hot vector, where $m$ is the number of styles. For example, when $m=3$, we get the 3 policies using the one-hot latent codes $(0,0,1),(0,1,0),(1,0,0)$.\n\nQ7: The explanation on the \"lora\" module seems missing.\n\nA7: We appreciate the reviewer's advice to include a explanation of LoRA module [1] used. We have provided a more detailed explanation in Appendix E. We share the main network among policies of all styles and each policy has its own LoRA module.\n\nReferences:\n\n1.Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456031043,
                "cdate": 1700456031043,
                "tmdate": 1700456031043,
                "mdate": 1700456031043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ob5UdpOEEt",
                "forum": "rnHNDihrIT",
                "replyto": "xxEPBY1MWv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_BZGK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_BZGK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I still think the proof in Appendix D is incomplete. For example, the optimality of $\\pi^{(i)}$ with respect to the Lagrangian does not automatically make it the optimal solution to the primal problem. Other than that, most of my concerns were resolved, so I raised my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656214101,
                "cdate": 1700656214101,
                "tmdate": 1700656214101,
                "mdate": 1700656214101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bM5aNPW6X8",
            "forum": "rnHNDihrIT",
            "replyto": "rnHNDihrIT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_HX1W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_HX1W"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the problem of extracting diverse as well as high-quality policies from multi-modal datasets via offline reinforcement learning. The core of the proposed method lies in clustering trajectories within the dataset. Behavior policyies to induce such clusters are learned, which are later used for constraining policy learning to ensure that the offline RL policies are high-performing as well as aligning with the diverse multi-modal dataset. Extensive experiments are conducted, and results seem positive. But I still have some concerns for this paper, please refer to the weaknesses."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is straightforward and easy to comprehend.\n2. Extensive experiments are conducted. Resutls on all three benchmarks show the proposed method SORL achieves balance between performance and diversity of the learned policies.\n3. Procedure of SORL is clearly described."
                },
                "weaknesses": {
                    "value": "1. Transformation from the true posterior to Eq. 2 needs more explanation. The current context is too weak. And I assume the basic assumption for this transformation is that all behavior policies $\\mu_{1,..,m}$ are diverse enough, because the authors use transtion-wise action probability to replace the trajectory probability. This makes sense if behavior policies are diverse enough that they take different actions for each step. But if the policies only slightly differ from each other, the consecutive multiplication of all steps within the trajectory will make their trajectory distributions very different from each other (while the action distribution is not much different). As a result, Eq. 2 provides very inaccurate estimation of the posterior.\n2. The proposed SORL needs to know the number $m$ of policy primitives constituting the dataset in order to learn the diverse policies. But it is hard to know this prior under many ciucumstances. I think there should be a study about how sensitive SORL is  to this hyperparameter.\n3. How diverse are the induced policies? The case studies are great but there should be a quantitive study. My further question on this is, if we set $m$ larger than the actual number of policy primitives $\\mu_{1,..,m}$, what will the resulted policies be like?\n4. Some typos, e.g. line 10 in Algorithem 1: $\\mu^{i}$ instead of $\\mu^{1}$\n5. As the author claims the induced policies are high-performing, the baselines should include some strong offline RL methods for comparison. This will also show SORL's advantage in policy diversity compared to them. The current baselines are too few.\n6. How do the authors collect online user data? Where is the user agreement to collect this data? This should appear at least in the appendix."
                },
                "questions": {
                    "value": "Please refer to the weaknesses. If the authors can address my concerns, I'm happy to increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Reviewer_HX1W"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647019066,
            "cdate": 1698647019066,
            "tmdate": 1700626262858,
            "mdate": 1700626262858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KBT55IMhTm",
                "forum": "rnHNDihrIT",
                "replyto": "bM5aNPW6X8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the valuable feedback provided by the reviewer. We have carefully considered the comments and have addressed them in our responses. If you have any additional questions or concerns, please don't hesitate to let us know.\n\nQ1: Transformation from the true posterior to Eq. 2 needs more explanation.\n\nA1: We appreciate the reviewer for pointing out that 'if the policies only slightly differ from each other, the consecutive multiplication of all steps within the trajectory will make their trajectory distributions very different from each other'. We plot the distribution of $\\hat{p}(z=1|\\tau)$ for all trajectories in the dataset in Figure 11, Appendix J. Figure 11(a) uses consecutive multiplication, while Figure 11(b) is the average of all steps. The results show that using consecutive multiplication causes a trend of polarization in clustering, which leads to unexpected numerical instability. The experiment was conducted in the grid shooting environment with the number of clusters set to $m=2$. To address this issue, we have chosen to average all the steps instead of multiplication in order to alleviate the problem of numerical instability.\n\nQ2: The proposed SORL needs to know the number of policy primitives constituting the dataset in order to learn the diverse policies. But it is hard to know this prior under many ciucumstances. I think there should be a study about how sensitive SORL is to this hyperparameter.\n\nA2: We appreciate the reviewer for pointing out the importance of conducting sensitivity analysis on the number of policies $m$ in SORL, as it is difficult to know the exact value of $m$ in real circumstances. In order to address this concern, we have conducted experiments to evaluate the diversity, quality, and consistency of SORL under different values of $m$. We also add two more diversity metrics that are unrelated to $m$, and their detailed descriptions can be found in Appendix F. The performance of SORL remains similar across different $m$ values, indicating that the algorithm is robust to the hyperparameter $m$.\n\n|  | $m=2$ | $m=3$ | $m=4$ | $m=5$ |\n| ---- | ---- | ---- | ---- | ---- |\n| Diversity       | $0.61$      | $0.85$     | $0.65$      | $0.69$ |\n| Diversity (skill metric) | $15.7$ | $20.3$ | $14.0$ | $14.5$ |\n| Diversity (OT metric)   | $29.1$ | $28.3$ | $28.5$ | $29.2$ |\n| Quality          | $55.1$% | $50.3$% | $52.5$% | $52.2$% |\n| Consistency | $86.4$% | $88.5$% | $88.7$% | $88.7$% |\n\nQ3:  How diverse are the induced policies? The case studies are great but there should be a quantitive study. My further question on this is, if we set  m  larger than the actual number of policy primitives  $\\mu_{1,\\cdots,m}$ , what will the resulted policies be like?\n\nA3: The quantitative results of the grid shooting environment can be found in the table provided below (also available in Appendix A).\n\n|  | Quality | Diversity | Consistency |\n| ---- | ---- | ---- | ---- |\n| SORL          | $55.1\\pm4.6$% | $0.60\\pm0.07$ | $86.4\\pm0.4$% |\n| Off-RLPMM | $40.4\\pm0.7$% | $0.68\\pm0.01$ | $90.5\\pm0.0$% |\n| InfoGAIL      | $55.3\\pm2.1$% | $0.00\\pm0.00$ | $84.0\\pm0.3$% |\n\nWe provide additional results in the grid shooting game as $m$ increases. The actual number of policy primitives is two. The reward distribution of the learned policies is presented in the following table, taking $m=4$ as an example. For brevity, the resulting policies for $m=3$ and $m=5$ can be found in Appendix I.\n\n| Learned policies | Policy 1 | Policy 2 | Policy 3| Policy 4 |\n| ---- | ---- | ---- | ---- | ---- |\n| Reward (shoot) | $1.3$     | $2.2$     | $0.2$    | $3.1$ |\n| Reward (star)   | $6.0$    | $3.2$     | $7.6$    | $0.5$ |\n| Winning rate    | $57$% | $57$% | $56$% | $40$% |\n\nWhen taking larger $m$ values, the learned policies still differs a lot from each other. In the case when $m=4$, one policy learns to focus on shooting, one policy concentrates on star collection, and the remaining policies' preferences lie between the above two extremes."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455477362,
                "cdate": 1700455477362,
                "tmdate": 1700455477362,
                "mdate": 1700455477362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BEhuYncQfT",
                "forum": "rnHNDihrIT",
                "replyto": "bM5aNPW6X8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Part 2)"
                    },
                    "comment": {
                        "value": "Q4: As the author claims the induced policies are high-performing, the baselines should include some strong offline RL methods for comparison. This will also show SORL's advantage in policy diversity compared to them. The current baselines are too few.\n\nA4: We add experimental results of standard offline RL methods (CQL, AWR and IQL). We conduct comparison in Atari games and results are shown below. Notably, SORL learns a set of policies while the remaining methods learn a single policy. The diversity metric cannot be calculated for methods that only generate a single policy. The scores of SORL are averaged over the full set of learnt policies. We have not yet completed all the experiments on AWR. However, we will finish them and include them in Appendix G before the deadline.\n\n|        | SORL | CQL | AWR | IQL |\n| ---- | ----     | ----   | ----   | ----  |\n| SpaceInvaders            | $387.1\\pm33.3$    | $136.5\\pm27.5$   | $351.3\\pm102.6$ | $361.5\\pm86.9$ |\n| MsPacman                  | $622.2\\pm65.3$    | $513.7\\pm325.5$  | $320.0\\pm68.53$ | $505.0\\pm161.2$ |\n| MontezumaRenvenge | $306.7\\pm19.1$    | $113.4\\pm196.3$  | $210.0\\pm119.9$ | $166.7\\pm32.1$ |\n| Enduro                        | $371.1\\pm2.9$       | $0.0\\pm0.1$          | $301.7\\pm66.1$ | $290.1\\pm44.2$ |\n| Riverraid                     | $1931.2\\pm432.1$ | $1127.7\\pm160.6$ | $1439.0\\pm280.6$ | $1909.3\\pm332.1$ |\n| Frostbite                     | $2056.0\\pm391.9$ | $76.3\\pm16.7$     | $1165.0\\pm470.6$ | $2184.7\\pm377.5$ |\n\nThe results above shows that the quality performance of SORL is still generally higher than offline RL baselines. We hypothesize that the human datasets are multi-modal, and utilizing multiple policies instead of a single policy to fit the dataset yields better results.\n\nQ5: How do the authors collect online user data? Where is the user agreement to collect this data? This should appear at least in the appendix.\n\nA5: The game environment and the dataset used in this paper are public. The link to the game environment is <https://github.com/FuxiRL/DunkCityDynasty>, and the link to the dataset is <https://huggingface.co/datasets/FUXI/DunkCityDynasty_Dataset>. To our best knowledge, this paper is the first to conduct experiments of learning both diverse and high-quality behaviors from a large human dataset of video games."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455612869,
                "cdate": 1700455612869,
                "tmdate": 1700455612869,
                "mdate": 1700455612869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fBXoapD8Mi",
                "forum": "rnHNDihrIT",
                "replyto": "bM5aNPW6X8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_HX1W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_HX1W"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for conducting the additional experiments. My concerns have been addressed and I have increased my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626349442,
                "cdate": 1700626349442,
                "tmdate": 1700626349442,
                "mdate": 1700626349442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n0d2ViUODr",
            "forum": "rnHNDihrIT",
            "replyto": "rnHNDihrIT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new approach, Stylized Offline RL (SORL), which seeks to derive high-quality, stylistically diverse policies from offline datasets with distinct behavioral patterns. While most reinforcement learning (RL) methodologies prioritize either online interactions or policy performance, SORL combines the Expectation-Maximization (EM) algorithm with trajectory clustering and advantage-weighted style learning to promote policy diversification and performance enhancement. Through experiments, SORL has been shown to outperform previous methods in generating high-quality and diverse policies, with a notable application being in the basketball video game \"Dunk City Dynasty\". The effectiveness of SORL is evaluated in various settings, including a basketball video game. Compared to other methods, SORL consistently yields better-performing policies that also maintain distinct behavior patterns. The paper's contributions include:\n* The introduction of SORL, a framework that combines quality and diversity into the optimization objective, addressing limitations in both diverse RL and offline RL methods.\n* Extensive evaluations showing SORL's ability to generate high-quality, stylistically diverse policies from diverse offline datasets, including human-recorded data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe structure of the paper is clear and easy to understand. \n2.\tThe idea of using the EM framework to do trajectory clustering and policy optimization is interesting. \n3.\tUsing offline RL to solve real-world tasks using a human-generated dataset shows the scalability of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tNo standard Offline RL baseline compared. As quality and diversity are both metrics for evaluation, it would be good to compare the performance with other standard offline RL methods, e.g., CQL, TD3+BC, AWR. \n2.\tThe motivation for increasing the diversity of policy is not clear. In the related work section, the authors only discuss the importance of diversity in online RL settings, for example, encouraging exploration, better opponent modeling, and skill discovery. However, in the offline RL setting, there is no exploration problem or skill discovery since the dataset is fixed. In addition, in the preliminary section, the authors aim to \u201clearn a set of high-quality and diverse policies\u201d without any explanation of the advantage of learning a set of diverse policies over a single policy with diverse behaviors (e.g., using multi-modal distribution as policy distribution).\n3.\tMany details are missing in the experiment of the \u201cDunk City Dynasty\u201d. The code does not include this experiment."
                },
                "questions": {
                    "value": "1.\tThe metric for evaluating diversity seems to rely on the learned clustering p. I wonder why don\u2019t evaluate the diversity of the learned policy? Otherwise, this metric cannot be used for algorithms that don\u2019t learn the clustering of datasets. In addition, the goal of clustering the dataset is to learn diverse policies for online evaluation, so the diversity of the policy is what we really care about.\n2.\tCould the authors provide some visualization or example of the mean of the clusters in Atari games? It is not intuitive how the diverse behavior looks like in those games. Similarly, in the Dunk City Dynasty game, the visualization in Figure 3 is too simple. Could the authors plot the shooting positions of each policy? Also, besides the shooting position, are there other differences between these policies?\n3.\tCould the authors provide more details about the setting of the Dunk City Dynasty experiments? For example, the action space, and model structure. Appendix C only describes the structure of the first two experiments. \n4.\tWhat does it mean by \u201cwe share the main network and use a lora module to discriminate different styles.\u201d In Appendix C?\n5.\tResults in Table 3 show that SORL has a very high variance (5.3 \u00b1 3.4) in terms of quality, which only slightly outperforms InfoGAIL (5.0 \u00b1 0.8). Does this mean pursuing high-quality sacrifices for the performance of the policy? Then, what do we gain from the high diversity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727917180,
            "cdate": 1698727917180,
            "tmdate": 1700676448301,
            "mdate": 1700676448301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aX1Eeb3eUl",
                "forum": "rnHNDihrIT",
                "replyto": "n0d2ViUODr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for the valuable feedback. We appreciate the reviewer's constructive comments and would like to address your concerns. Please let us know if you have any further questions or comments.\n\nQ1: No standard Offline RL baseline compared. As quality and diversity are both metrics for evaluation, it would be good to compare the performance with other standard offline RL methods, e.g., CQL, TD3+BC, AWR.\n\nA1: We add experimental results of standard offline RL methods (CQL, AWR and IQL). We exclude TD3+BC, because it works for the continuous action setting, different from the experiment settings with discrete action spaces. Instead, we include IQL (Offline Reinforcement Learning with Implicit Q-Learning), which is a strong offline RL algorithm. We conduct comparison in Atari games and results are shown below. Notably, SORL learns a set of policies while the remaining methods learn a single policy. The scores of SORL are averaged over the full set of learnt policies. We have not yet completed all the experiments on AWR. However, we will finish them and include them in Appendix G before the deadline.\n\n|        | SORL | CQL | AWR | IQL |\n| ---- | ----     | ----   | ----   | ----  |\n| SpaceInvaders            | $387.1\\pm33.3$    | $136.5\\pm27.5$   | $351.3\\pm102.6$ | $361.5\\pm86.9$ |\n| MsPacman                  | $622.2\\pm65.3$    | $513.7\\pm325.5$  | $320.0\\pm68.53$ | $505.0\\pm161.2$ |\n| MontezumaRenvenge | $306.7\\pm19.1$    | $113.4\\pm196.3$  | $210.0\\pm119.9$ | $166.7\\pm32.1$ |\n| Enduro                        | $371.1\\pm2.9$       | $0.0\\pm0.1$          | $301.7\\pm66.1$ | $290.1\\pm44.2$ |\n| Riverraid                     | $1931.2\\pm432.1$ | $1127.7\\pm160.6$ | $1439.0\\pm280.6$ | $1909.3\\pm332.1$ |\n| Frostbite                     | $2056.0\\pm391.9$ | $76.3\\pm16.7$     | $1165.0\\pm470.6$ | $2184.7\\pm377.5$ |\n\nThe results above show that the quality performance of SORL is still generally higher than offline RL baselines. We hypothesize that this is because the human datasets are multi-modal, and utilizing multiple policies instead of a single policy to fit the dataset yields better results.\n\nQ2: The motivation for increasing the diversity of policy is not clear.\n\nA2: We have discussed the motivation of learning diverse policies from offline datasets in Lines 4-7 in the introduction. Additionally, we have added further content discussing the utilization of learning diverse policies from offline datasets in opponent modeling in Lines 8-10. We appreciate the reviewer for pointing out that the motivation needs a more detailed discussion. Learning diverse policies from offline datasets is beneficial for various real-world applications, such as game AI and opponent modeling. In game AI, offline data collected from human gameplay allows AI bots to mimic human-like behavior. In online games, deploying such AI bots with varyied motion styles can enrich the gaming environment and enhance player engagement. Additionally, in opponent modeling, high-quality diverse opponents that resemble real opponents can significantly improve the performance of the learned policy. It is worth noting that learning diverse policies from offline datasets is an area that has not been extensively explored in previous research. In the related work, we mention encouraging exploration and skill discovery because they are the motivation of promoting diversity in online settings rather than offline settings.\n\nQ3: Many details are missing in the experiment of the \u201cDunk City Dynasty\u201d. The code does not include this experiment.\n\nA3: We have updated the supplementary materials for \"Dunk City Dynasty\" and included the algorithm codes. However, please note that the code for large-scale efficient experiments cannot be made public at this time due to institution regulations. We will make it available to the public as much as possible in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454500892,
                "cdate": 1700454500892,
                "tmdate": 1700454500892,
                "mdate": 1700454500892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "67aUM96Msd",
                "forum": "rnHNDihrIT",
                "replyto": "n0d2ViUODr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Part 2)"
                    },
                    "comment": {
                        "value": "Q4: The metric for evaluating diversity seems to rely on the learned clustering p.\n\nA4: We admit that the diversity metric employed in the paper relies on clustering $p$. Comparing the diversity of two sets of policies are not trivial. Previous research, such as InfoGAIL, also did not propose a quantitative metric for evaluating diversity when learning policies from datasets. Instead, they relied on visualization to showcase the diversity of learned policies.\n\nIn order to provide a more comprehensive investigation of the diversity of learned policies, we have proposed additional diversity metrics that do not require clustering: the **skill metric** measuring the dissimilarity of the skill set and the **OT (optimal transport) metric** quantifing the disparity in state distribution. For a detailed description of these metrics, please refer to the Appendix F. Moreover, we will present visualization results of the Atari environment in the response to the next question.\n\nThe table provided below showcases the diversity of various style learning methods in the grid shooting environment utilizing these two additional metrics as well as popularity metric used in the original paper. The two new metrics are not dependent on clustering. Higher values indicate greater diversity. Based on the results of the additional diversity metrics presented in Table below, we can conclude that the SORL algorithm is capable of obtaining diverse policies. The results demonstrate that SORL learns a set of diverse and high-quality policies. InfoGAIL fails to learn diverse policies, while Off-RLPMM learns low-quality diverse policies and has considerably lower winning rate.\n\n| | Popularity metric | Skill metric | OT metric | Winning rate |\n| ---- | ---- | ---- | ---- | ---- |\n| SORL                      | $0.60$ | $15.7$ | $29.14$ | $55.1$% |\n| SORL w/o advantage | $0.61$ | $21.2$ | $31.61$ | $45.2$% |\n| Off-RLPMM              | $0.68$ | $23.2$ | $29.87$ | $40.4$% |\n| InfoGAIL                     | $0.00$ | $2.0$ | $28.53$ | $55.3$% |\n\nQ5: Could the authors provide some visualization or example of the mean of the clusters in Atari games? Could the authors plot the shooting positions of each policy? Also, besides the shooting position, are there other differences between these policies?\n\nA5: We have generated visualizations of the clusters in several Atari environments. Detailed descriptions of the environment and the visualization can be found in Appendix H. In SpaceInvaders, the shooter can move horizontally, and there are three stationary bunkers positioned above the shooter. To evaluate the diversity of learned policies, we calculate the frequency of the shooter's position over 30,000+ steps for each style. The table below demonstrates that different styles of policies tend to prefer different positions.\n\n|  | Leftmost - Bunker 1 | Bunker 1 - Bunker 2 | Bunker 2 - Bunker 3 | Bunker 3 - Rightmost|\n| ---- | ---- | ---- | ---- | ---- |\n| Style 1 | $23$% | $19$% | $36$% | $22$% |\n| Style 2 | $10$% | $58$% | $15$% | $18$% |\n| Style 3 | $32$% | $38$% | $26$% | $4$% |\n\nIn MsPacman, the map and starting position are fixed. To evaluate the diversity of learned policies, we calculate the frequency of the beginning trajectory until the first corner for each style, based on 30 trajectories per style. There are a total of six possible corners. The table below displays the different preferred corners for each style:\n\n| Which corner | 3rd left | 2nd left | 1st left | 1st right | 2nd right | 3rd right |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| Style 1 | $0$% | $33$% | $67$% | $0$% | $0$% | $0$% |\n| Style 2 | $20$% | $7$% | $7$%   | $3$% | $37$% | $27$% |\n| Style 3  | $0$% | $97$% | $3$%   | $0$%  | $0$%  | $0$% |\n\nAs for the Dunk City Dynasty, the plot of shooting positions are added to Appendix B. Besides the style difference of shooting positions, there also exists the style difference of ball passing and ball possesion, as shown in the following table. Style 2 has the fewest number of passes and the lowest proportion of goals scored directly after a pass.\n\n|  | Style 1 | Style 2 | Style 3 |\n| ---- | ---- | ---- | ---- |\n| #passes | $41$ | $29$ | $39$ |\n| Proportion of goals directly after a pass | $27.7$% | $22.2$% | $44.4$% |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454910474,
                "cdate": 1700454910474,
                "tmdate": 1700454910474,
                "mdate": 1700454910474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bIr1VUnXVm",
                "forum": "rnHNDihrIT",
                "replyto": "n0d2ViUODr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Part 3)"
                    },
                    "comment": {
                        "value": "Q6: Could the authors provide more details about the setting of the Dunk City Dynasty experiments? For example, the action space, and model structure. Appendix C only describes the structure of the first two experiments.\n\nA6: We appreciate the reviewer for pointing it out. We have provided details of the experiments in Dunk City Dynasty in Appendix B. The state space dimension is $468$, including global state, allies' states and enemies' states. The action space is a discrete space with $52$ actions. The network structure of 'Dunk City Dynasty' is a 3-layer MLP. The opponent in evaluation is an agent with moderate strength, that is learned by vanilla Behavioral Cloning for similar training steps (100,000 steps).\n\nQ7: What does it mean by \u201cwe share the main network and use a lora module to discriminate different styles.\u201d In Appendix C?\n\nA7: The detailed description of LoRA module is as follows. In order to ensure balanced learning among all the styles, we share the main network and use a LoRA module to discriminate different styles. LoRA [1] is a widely used network structure, that substitutes the original matrix of the linear layer by a matrix and a low-rank multiplication of two other matrices. Further details of LoRA module is added in Appendix E.\n\nQ8: Results in Table 3 show that SORL has a very high variance (5.3 \u00b1 3.4) in terms of quality, which only slightly outperforms InfoGAIL (5.0 \u00b1 0.8). Does this mean pursuing high-quality sacrifices for the performance of the policy? Then, what do we gain from the high diversity?\n\nA8: The focus of our work is to learn diverse and high-quality policies. These are two distinct objectives and pursuing diversity may sacrifice the quality of the policy. The goal of this paper is to maximize both objectives and trade off between diversity and quality. In Table 3, although SORL shows higher variance in performance, it signficantly outperforms the baselines in terms of disversity. Similarly, in the other two environments, we also observe a trade-off between quality and diversity. For instance, Figure 2 in the main text shows that the SORL algorithm does not always outperform the baselines in terms of quality, but it consistently achieves a good balance between high quality and diversity. By combining the pursuit of both quality and diversity, we demonstrate that the SORL algorithm is superior to the baselines. It provides a more comprehensive solution that achieves both high performance and a diverse set of policies.\n\nThe motivation of learning diverse policies from offline datasets has been expained in the response to question 2. To recapitulate briefly, it is useful in applications including learning human-like behaviors in game AI and opponent modeling.\n\nReferences\n\n1.Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455079727,
                "cdate": 1700455079727,
                "tmdate": 1700455079727,
                "mdate": 1700455079727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zvcymmDRlF",
                "forum": "rnHNDihrIT",
                "replyto": "bIr1VUnXVm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions"
                    },
                    "comment": {
                        "value": "I really appreciate the efforts made by the authors. The experimental results and explanations added in the rebuttal stage make this paper much better. **Actually, I feel that the version after the rebuttal looks more like a formal submission to ICLR while the previous version seems incomplete with only preliminary results.** However, I am not sure if this is fair to other submissions as the rebuttal phase should only be used for clarifying unclear questions rather than completing the paper.\n\nI still have some follow-up questions:\n\nQ1: \u201cWe hypothesize that this is because the human datasets are multi-modal, and utilizing multiple policies instead of a single policy to fit the dataset yields better results.\u201d Could the authors provide more evidence for this hypothesis? For example, the performance of individual policies. Actually, I also want to ask why using the Atari game to evaluate the proposed method. Although it is a widely used benchmark for offline RL, I am unsure if it is reasonable to say it is a heterogeneous dataset. Why not use a real heterogeneous dataset and show that a diverse policy works well? I suggest that this paper should only focus on the Dunk City Dynasty experiment, which seems to be more consistent with the motivation and complex enough to compare different methods.\n\n\nQ2: Increasing diversity in online settings is indeed a widely investigated topic, but I am still not fully convinced to maximize diversity in offline settings. The authors give me two examples:\n* (1)\tin game AI, diverse behavior enriches the gaming environment and enhances player engagement.\n* (2)\tin opponent modeling, high-quality diverse opponents that resemble real opponents can significantly improve the performance of the learned policy.\n\nThese two motivations seem reasonable to me on a general level, but both are hard to evaluate. I suggest that the authors design some examples to support the motivation. Or maybe just focus on the Dunk City Dynasty experiment and provide more analysis and ablation studies.\n\n\nQ4: The proposed two metrics, skill metric, and OT metric, seem to have inconsistent results. Why does InfoGAIL have a similar OT score but a much lower skill score than SORL? Which metric leads to the conclusion \u201cInfoGAIL fails to learn diverse policies\u201d?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600021273,
                "cdate": 1700600021273,
                "tmdate": 1700600021273,
                "mdate": 1700600021273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "618DyFo3nr",
                "forum": "rnHNDihrIT",
                "replyto": "T0Ja7AX0EX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Reviewer_Xp9f"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for providing more evidence to support their points. I don't have further questions and I think the discussion so far has provided enough ingredients to make this paper a good one. I tend to accept this paper now and will increase my score. However, I highly suggest the authors spend some effort to organize the results and clarification during the rebuttal stage to improve the quality of the manuscript."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676420342,
                "cdate": 1700676420342,
                "tmdate": 1700676420342,
                "mdate": 1700676420342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XqlyjYvfXr",
            "forum": "rnHNDihrIT",
            "replyto": "rnHNDihrIT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_pT1t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1156/Reviewer_pT1t"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of learning diverse policies based on datasets of trajectories collected by humans. This is particularly relevant in the context of video gaming, where the goal is to develop bots that are not only proficient but also exhibit varied behavioral patterns based on human player data. The authors introduce a purely offline solution that eliminates the need for environmental interaction. This approach is underpinned by a dual-step method. Initially, a clustering technique, leveraging the EM algorithm, assigns trajectories to different clusters by learning  a style-sensitive policy. Subsequently, to foster policies that are both effective and stylistically aligned, Advantage Weighted Regression (AWR) is employed in conjunction with a style-regularization component based on the style-sebsitive policies. The effectiveness of this method is demonstrated through a series of tests conducted in a simplistic environment, a handful of Atari games, and a commercial video game, all of which confirm the algorithm's capability to generate diverse and competent policies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and will be of the interest of a large audience. The model is quite simple (clustering then offline learning) and easy to apply to different use-cases, it can be a good baseline for many future works.  More importantly, as far as I know, this paper is the first one to propose a set of experiments on a real video game and a large dataset of collected traces which is certainly where this paper has the most value and the dataset and  environment will be release (can you confirm I am right on that point ?)"
                },
                "weaknesses": {
                    "value": "The paper presents a compelling methodology, yet it notably omits a benchmark against \"robust imitation of diverse behaviors,\" which is a reference work within this domain. Although primarily an online training paper, like infoGAIL, its principles could potentially be adapted for offline training, serving as a relevant comparison.\n\nThere appears to be an implied relationship between what the authors denote as 'style' and the rewards associated with a particular trajectory. Commonly, one might categorize trajectories by skill level, segregating expert from intermediate or novice plays. However, in such a scenario, the operation of the Advantage Weighted Regression (AWR) on these distinct clusters is not thoroughly explained. The connection between the 'style' of play and the 'reward' outcome merits a deeper examination.\n\nThe simplicity of the clustering model raises concerns regarding its ability to discern more nuanced styles, such as specific repetitive actions (e.g., \"jump twice\"). A more critical discussion on the model's capacity to identify and differentiate between complex styles would enhance the paper.The algorithm seems limited in capturing policies that would need memory to characteriwe their styles.\n\nRegarding the implementation of AWR, it seems to be applied to each cluster individually. This approach suggests that in a situation where ten clusters are identified, only one-tenth of the training trajectories are utilized during the AWR phase for each cluster. This potentially limits the method's scalability when dealing with numerous styles, possibly making it impractical for extensive style differentiation.\n\nLastly, the paper could explore the potential of employing more advanced offline reinforcement learning algorithms in the second step of the methodology. Such a discussion could provide insights into improving the efficiency and effectiveness of the learning process in diversifying policies."
                },
                "questions": {
                    "value": "(see previous section)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no concerns"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698940255487,
            "cdate": 1698940255487,
            "tmdate": 1699636041783,
            "mdate": 1699636041783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gKiBtX2j7e",
                "forum": "rnHNDihrIT",
                "replyto": "XqlyjYvfXr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, and we provide clarification on your concerns as follows. We would appreciate it if you have any further feedback.\n\nQ1: Will the video game environment and the dataset be released?\n\nA1: The game environment and the dataset used in this paper are public. The link to the game environment is <https://github.com/FuxiRL/DunkCityDynasty>, and the link to the dataset is <https://huggingface.co/datasets/FUXI/DunkCityDynasty_Dataset>. To our best knowledge, this paper is the first to conduct experiments of learning both diverse and high-quality behaviors from a large human dataset of video games.\n\nQ2: The connection between the 'style' of play and the 'reward'.\n\nA2: We agree with the reviewer that 'the connection between the 'style' of play and the 'reward' outcome merits a deeper examination'. In our perspective, there are two types of policy diversity: different policies of comparable performances and policies of novice/veteran distinction. In the first case, although the policies are diverse, they may still have similar expected returns. In this paper, we primarily focus on this case. For the latter case, one simple approach is to split the dataset into subsets with respect to the cumulative return of trajectories, and learn different policies on corresponding subsets. In the experiments (Section 5.3), we find that our method discovers two distinct strategies for winning the grid shooting game: shooting the enemy and collecting stars. The motivation of this paper is to enhance the quality concerning the first kind of diversity.\n\nQ3: The simplicity of the clustering model raises concerns regarding its ability to discern more nuanced styles. A more critical discussion on the model's capacity to identify and differentiate between complex styles would enhance the paper.\n\nA3: Regarding to your concerns on its ability to discern more nuanced styles, we acknowledge that the MLP utilized in our current implementation cannot model policies that need memory to characterize styles, since it doesn't memorize historical states. We believe that future work could incoperate network structures with memory, such as transformers and recurrent neural networks (RNNs).\n\nTo discuss more on the model's capacity to identify and differentiate between complex styles, we provide an additional experiment on different numbers of styles $m$, especially when $m$ is set larger than the actual number of styles of the dataset.\n\nWe have conducted experiments to evaluate the diversity, quality, and consistency of SORL under different values of $m$. We also add two more diversity metrics that are unrelated to $m$, and their detailed descriptions can be found in Appendix F. The results show that the performance of SORL remains consistent across different values of $m$, indicating that the algorithm has sufficient capacity to generate diverse policies.\n\n|  | $m=2$ | $m=3$ | $m=4$ | $m=5$ |\n| ---- | ---- | ---- | ---- | ---- |\n| Diversity       | $0.61$      | $0.85$     | $0.65$      | $0.69$ |\n| Diversity (skill metric) | $15.7$ | $20.3$ | $14.0$ | $14.5$ |\n| Diversity (OT metric)   | $29.1$ | $28.3$ | $28.5$ | $29.2$ |\n| Quality          | $55.1\\%$ | $50.3\\%$ | $52.5\\%$ | $52.2\\%$ |\n| Consistency | $86.4\\%$ | $88.5\\%$ | $88.7\\%$ | $88.7\\%$ |\n\nQ4: Regarding the implementation of AWR, it seems to be applied to each cluster individually.\n\nA4: Our method is scalable with respect to the number of clusters. Firstly, in our algorithm, we employ a soft clustering approach where each trajectory has a probability of belonging to each cluster. This partially mitigates the problem by allowing trajectories to contribute to multiple clusters. Secondly, policies of different styles share a common part of the network parameters, which helps in learning better stylized policies. Further details about the shared network structure can be found in the Appendix E.\n\nQ5: The paper omits a benchmark against \"robust imitation of diverse behaviors\".\n\nQ6: The paper could explore the potential of employing more advanced offline reinforcement learning algorithms in the second step of the methodology.\n\nA5,6: Thank you for bringing to our attention the benchmark \"robust imitation of diverse behaviors\" and the potential use of advanced offline RL algorithms in the second step of our methodology. We will definitely consider these suggestions for future improvements."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453822272,
                "cdate": 1700453822272,
                "tmdate": 1700453822272,
                "mdate": 1700453822272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]