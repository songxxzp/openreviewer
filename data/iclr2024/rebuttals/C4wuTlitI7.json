[
    {
        "title": "Understanding Masked Autoencoders From a Local Contrastive Perspective"
    },
    {
        "review": {
            "id": "HZOrnIROK4",
            "forum": "C4wuTlitI7",
            "replyto": "C4wuTlitI7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_iWK2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_iWK2"
            ],
            "content": {
                "summary": {
                    "value": "CL and MIM are the two important self-supervised pre-training methods for computer vision. This paper discusses the connection between them and proposes a new method, which may benefit the community. This paper is well-organized and well-written."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is interesting and the writing is good. The authors provide an interesting finding that MIM and CL methods have a close relationship. It motivates the authors to propose a new method by combining CL and MIM. Moreover, The experiments show the fair performance of the proposed method."
                },
                "weaknesses": {
                    "value": "1. There are currently too few experiments to demonstrate the effectiveness of the approach. At present, the lack of experiments includes longer-epoch training, benchmark object detection, benchmark semantic segmentation, and other experiments.\n2. The paper should outperform other contrastive + masked methods, e.g., MST[1], iBoT[2] which were proposed two years ago.\n\n[1] Mst: Masked self-supervised transformer for visual representation. NeurIPS2021.\n[2] iBOT: Image BERT Pre-Training with Online Tokenizer. ArXiv2021."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698030166752,
            "cdate": 1698030166752,
            "tmdate": 1699636305209,
            "mdate": 1699636305209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "aWXMBtefEd",
            "forum": "C4wuTlitI7",
            "replyto": "C4wuTlitI7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_bqZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_bqZg"
            ],
            "content": {
                "summary": {
                    "value": "This paper empirically analyzes the behaviors of the decoders in MAE. The author finds that 1) the first layer of the decoder primarily relies on the positional information while the subsequent layers obtain higher-level semantic information \n2) the receptive field of the decoder is limited. Based on that, this paper reformulates the objective of MAE and proposes a new architecture that combines the spirits of MAE and contrastive learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The writing is well and easy to follow. And the main messages based on the analysis look solid and insightful.\n2. The representations learned by Uni-SSL (without masking techniques) are more similar to the mask models instead of contrastive models, which is interesting and verifies the analysis proposed in this paper."
                },
                "weaknesses": {
                    "value": "1. As shown in Figure 3, the attention distance increases with the larger mask ratio. However, both the fine-tuning and linear accuracy are not monotonous in MAE. Is it possible to discuss the trade-off in the choice mask ratio based on the analysis in this paper?\n2. In Figure 5(b), the fine-tuning accuracy of DINO is higher than MAE and Uni-SSL while in Table 2 it is the opposite. What differences have I missed?\n3. The paper focuses on analyzing the behavior of the decoders in MAE. Is it possible to provide some insights about how to design a better decoder?\n4. The connections between the analysis of the decoder, the analysis of the training objective, and the design of the new framework are a little confusing. It would be better to provide a more detailed discussion about that."
                },
                "questions": {
                    "value": "see my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582325095,
            "cdate": 1698582325095,
            "tmdate": 1699636305137,
            "mdate": 1699636305137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Yts3HJ1lDV",
            "forum": "C4wuTlitI7",
            "replyto": "C4wuTlitI7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_pPsy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_pPsy"
            ],
            "content": {
                "summary": {
                    "value": "This work has two main contributions: 1)  a comprehensive understanding of the role of the decoder part in MAE, uncovering the fact that the reconstructive decoder part learns local features within a limited receptive field. This work statistically analyzes the similarity among the learned attention maps for all mask tokens on the ImageNet validation set. These findings elucidate that the initial decoder layer predominantly depends on token positional data, whereas in the subsequent layers, the decoder progressively combines more advanced semantic information while maintaining positional guidance. 2) proposing a Siamese architecture combining MIM and contrastive learning in a unified manner."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work has explored the decoder\u2019s role of MAE in helping the encoder learn \u201crich hidden representations\u201d in a generative manner, uncovering the fact that the decoder part enables to learn local features."
                },
                "weaknesses": {
                    "value": "### Comparison to prior works\n\nThis work proposed a combination of Masked Image Modeling (MIM) and contrastive learning (CL) using Siamese architecture, however, this strategy has already been explored in several methods (iBOT [1], CAE [2], and CMAE [3]).  Moreover, this work just commented on these works, not comparing the proposed methods with these prior works. It seems that outstanding points of this work do not exist compared to the prior works.\n\n### Weak experiments\n\n1) very short training epoch: This work was trained for only 100 epochs, which is very short to compare with state-of-the-art methods\n\n2) lacks comparison with prior works (iBOT [1], CAE [2], and CMAE [3]). It would be better to compare with the [MIM+CL] combination methods.\n\n[1] Zhou et al., iBOT: Image BERT Pre-Training with Online Tokenizer, ICLR 2022.  \n[2] Chen, et al., Context Autoencoder for Self-Supervised Representation Learning, Arxiv, 2023.  \n[3] Huang et al., Contrastive Masked Autoencoders are Stronger Vision Learners, Arxiv, 2023."
                },
                "questions": {
                    "value": "No questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761680879,
            "cdate": 1698761680879,
            "tmdate": 1699636305065,
            "mdate": 1699636305065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "VKZsgxgTsP",
            "forum": "C4wuTlitI7",
            "replyto": "C4wuTlitI7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_zrzk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3517/Reviewer_zrzk"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses MAE is a local-level contrastive learning, and propose a approach to self-supervised learning. This paper is well-written."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors provide an finding that MAE is local contrastive learning, though some previous paper express the same opinion. Moreover, the paper propose a new method by combining contrastive learning and MAE. Finally, the writing is good."
                },
                "weaknesses": {
                    "value": "1. The paper should cite and compare with related work, like MST, iBoT, CMAE, and so on. They are contrastive + masked methods.\n\n\n2. The main opinions of this paper is proposed by previous work. Hence, the authors should not ignore them.\n\n3. Current experimental results cannot demonstrate the method is effective. The authors should show detection and segmentation experiments in MAE to fairly compare with MAE and other related work."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849587459,
            "cdate": 1698849587459,
            "tmdate": 1699636304995,
            "mdate": 1699636304995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]