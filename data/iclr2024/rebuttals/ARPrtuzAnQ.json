[
    {
        "title": "On the hardness of learning under symmetries"
    },
    {
        "review": {
            "id": "CcXKfWf3zW",
            "forum": "ARPrtuzAnQ",
            "replyto": "ARPrtuzAnQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2889/Reviewer_kFXh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2889/Reviewer_kFXh"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the computational hardness of learning equivariant networks using gradient descent.  They show that enforcing symmetries like permutation invariance does not make learning any substantially easier, and that their hardness results hold even for shallow 1-layer GNNs and CNNs.  They provide statistical query (SQ) lower bounds that scale exponentially with feature dimensions for various architectures.  Additionally, the authors provide an efficient non-gradient based algorithm for learning sparse invariant polynomials, separating SQ and correlational SQ complexity.  Lastly, they perform numerous experiments to verify their results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality:\n\nThe authors prove numerous new results on the sample complexity of learning in neural networks, and provide ample empirical support for their work.\n\nQuality/clarity:\n\nThe authors sketch their proofs using careful, clear technical arguments.  Additionally, their experiments are simple, but clear demonstrations of the practical difficulty of learning networks within the families they authors study.\n\nSignificance:\n\nThe author's work significantly advances progress on the hardness of learning symmetric networks, opening the door to clear avenues of future, follow-up work."
                },
                "weaknesses": {
                    "value": "I would've liked a _slightly_ more thorough empirical treatment, if only to make sure that the failure to learn was not due to poor hyperparameter choices / poor initialization etc."
                },
                "questions": {
                    "value": "Could the authors comment more on the applicability of \"worst-case\" reasoning re: the likelihood of these function classes to well-describe nature?  It seems plausible that the worst case could be significantly harder than the typical case for problems that we care about.  In practice, these sorts of hardness results don't seem to impact practitioner's usage of these model classes much at all!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2889/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443844685,
            "cdate": 1698443844685,
            "tmdate": 1699636232281,
            "mdate": 1699636232281,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ZfUySieDg",
                "forum": "ARPrtuzAnQ",
                "replyto": "CcXKfWf3zW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2889/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2889/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and insightful comments. We respond to specific points below.\n\n> I would've liked a slightly more thorough empirical treatment, if only to make sure that the failure to learn was not due to poor hyperparameter choices / poor initialization etc.\n\nIn our experiments, we found little to no difference in the results based on the choice of hyperparameters. For example, for the varying levels of overparameterization (e.g. more layers) and various learning rates that we tested, the results were essentially the same. Some of this is reported in the appendix, Section H (Figure 3). We have also included an additional plot showing that the performance of the GNN is consistent across three different optimizers (Adam, SGD, RMSprop) and different learning rates for those optimizers.\n\n\n> Could the authors comment more on the applicability of \"worst-case\" reasoning re: the likelihood of these function classes to well-describe nature? It seems plausible that the worst case could be significantly harder than the typical case for problems that we care about. In practice, these sorts of hardness results don't seem to impact practitioner's usage of these model classes much at all!\n\nThe reviewer raises a good question that we hope forms the basis for future studies. Indeed, as pointed out by the reviewer and in the discussion section of our paper (Section 8), the functions we see in the real world are clearly learnable, and definitely not in the form of the rather adversarial ones studied here. Something has to be changed in the theoretical setting to tie it closer to practice; this is also the case for the existing line of work on SQ/CSQ lower bounds for fully-connected neural networks. To proceed formally, we see two paths for future study. First, worst-case settings such as SQ/CSQ can give too much flexibility to produce hard functions, which are often rather contrived. Some ways to make the setting more practical are to change the data input model (e.g. data that falls on a low dimensional manifold), or to introduce noise to the function class (e.g. weights are randomly chosen or subject to random perturbation). Second, we can reduce the size of the function class to rule out impractically adversarial learning tasks, e.g. by placing restrictions on the weights of the architectures to make them more tied to realistic functions. Such restrictions could also include bounds on the condition number, rank, etc. Promising work in these directions has been done for simple architectures, and we hope these ideas are expanded and studied in the symmetric setting as well [1-4]. \n\n**References:**\\\n[1] Chen, Sitan, et al. \"Learning narrow one-hidden-layer relu networks.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023.\n\n[2] Diakonikolas, Ilias, and Daniel M. Kane. \"Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials.\" arXiv preprint arXiv:2307.12840 (2023).\n\n[3] Daniely, Amit, Nathan Srebro, and Gal Vardi. \"Most Neural Networks Are Almost Learnable.\" arXiv preprint arXiv:2305.16508 (2023).\n\n[4] Goel, Surbhi, et al. \"Recurrent Convolutional Neural Networks Learn Succinct Learning Algorithms.\" Advances in Neural Information Processing Systems 35 (2022): 7328-7341."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2889/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105779668,
                "cdate": 1700105779668,
                "tmdate": 1700105779668,
                "mdate": 1700105779668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6zP8ThkuVD",
            "forum": "ARPrtuzAnQ",
            "replyto": "ARPrtuzAnQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2889/Reviewer_Fgas"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2889/Reviewer_Fgas"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the problem of learning symmetric Neural Networks.  The\nauthors provide hardness results for Statistical Query (SQ) and Correlational\nStatistical Query (CSQ) algorithms (and an NP-Hardness result for properly\nlearning Graph Neural Networks (GNNs)).  An example of a symmetric neural\nnetwork is a 2-layer GNN that maps an input graph $A$ to $g(f(A))$, where\n$f:\\{0, 1\\}^{n \\times n} \\mapsto R^k$ first aggregates $k$ permutation\ninvariant features of the input graph $A$ and $g$ is a one-hidden layer MLP.\n\nThe first result is an SQ hardness result for two-layer GNNs showing that for\nthe above class of GNNs $\\tau^2 2^{n^{\\Omega(1)}}$ queries of tolerance $\\tau$\nare required.  The result follows by designing a 2-layer GNN where the\n$i$-output of the first layer counts how many nodes have $i-1$ outgoing edges\nand the second layer selects a subset of those counts and computes its parity.\nBy using properties of GNP graphs, the authors reduce the problem to the\nwell-known hard problem of learning parity functions over the uniform\ndistribution on the $n$-dimensional Boolean hypercube.\n\nThe second result considers GNNs that take as input a $n \\times d$ feature\nmatrix $X$ and then compute $1_n^T \\sigma(A(G) X W) a$, for an adjacency matrix\n$A(G) \\in \\{0,1\\}^n$,a weight $d \\times 2 k$ matrix $W$ and a $2k$-dimensional\nweight vector $a$.  They give a $d^k$ CSQ lower bound for this problem.  This\nresult follows from adapting the hard instances of the CSQ lower bound\nconstruction of [2].\n\nThe third result shows that for CNNs (and more general frame-averaged networks)\nof the form $f(X) = 1/|G| \\sum_{g \\in G} a^T \\sigma (W^T g^{-1} X) 1_d$ where\n$X$ is a $n \\times d$ input matrix, $G$ is a group acting on $R^n$ (e.g., could\nbe cyclic shifts) either requires $2^{n^{\\Omega(1)}}/ |G|^2$ queries or a query\nwith precision $|G| 2^{-n^{\\Omega(1)}} + \\sqrt{|G|} n^{-\\Omega(k)}$.  The proof\nof this results also adapts the construction of [2] For more general\nframe-averaged networks, the authors use the techniques developed in [1] to\nshow a super-polynomial CSQ lower-bound (for any constant c either $n^{\\log n}$\nqueries are needed or a query with accuracy $n^{-c}$).\n\n\n[1] Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans. Superpolynomial lower bounds for learning one-layer neural networks using gradient descent.\nICML 2020.\n\n[2] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, and Nikos Zarifis. Algorithms and sq lower bounds for pac learning one-hidden-layer relu networks. COLT 2020."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem considered in this work is interesting and well-motivated. Most theoretical prior works on learning neural networks focused on fully connected shallow networks; investigating the learnability of popular and practically relevant classes of neural networks such as GNNs and CNNs (that have more restricted symmetric structure) is a natural\nnext step.\n\n2. The paper provides hardness results for various classes of ``symmetric'' neural networks in the SQ and CSQ models that are general models of computation capturing, for example, stochastic gradient descent algorithms.\n\n3. I found the paper to be well-organized and written. The authors clearly state what results of prior works they rely on to get their results."
                },
                "weaknesses": {
                    "value": "1. The novelty of the technics and arguments used in the lower bounds provided in this work may be limited in the sense that most of the claimed results rely heavily on machinery developed in the prior works [1,2]."
                },
                "questions": {
                    "value": "1. See weaknesses. \n\n2. While the authors clearly state which lemmas and proofs of the prior works they are using, I think a more detailed high-level explanation of the arguments and the differences from prior work should appear in the main body of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2889/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2889/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2889/Reviewer_Fgas"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2889/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700039826806,
            "cdate": 1700039826806,
            "tmdate": 1700039826806,
            "mdate": 1700039826806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dAbTo0MtJZ",
                "forum": "ARPrtuzAnQ",
                "replyto": "6zP8ThkuVD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2889/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2889/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful and detailed feedback, and respond to their specific questions and criticisms below.\n\n\n> The novelty of the techniques and arguments used in the lower bounds provided in this work may be limited in the sense that most of the claimed results rely heavily on machinery developed in the prior works [1,2].\n\nThe reviewer is correct in noting that we based many of our own proofs on the rather useful and elegant proof techniques already in Diakonikolas et al., Goel et al., and others. Indeed, in our efforts to prove hardness results, we found that the techniques of these works were useful backbones to our proofs. Nonetheless, as shown in the appendix, extending these techniques to invariant settings often required careful and detailed changes or enhancements to the existing results. Finally, we should note one exception to our application of previous machinery: we could not prove the lower bound (Theorem 3) for Erdos-Renyi graph inputs via the Diakonikolas et al. or Goel et al. constructions. There, a more customized GNN had to be constructed to achieve orthogonality and SQ hardness, which may be of independent interest.\n\n> While the authors clearly state which lemmas and proofs of the prior works they are using, I think a more detailed high-level explanation of the arguments and the differences from prior work should appear in the main body of the paper.\n\nWe thank the reviewer for pointing this out. Noting the space limits, we tried our best to provide high level explanations in the paper of the results describing our techniques early on in the introduction and adding proof sketches to the main text. Nonetheless, we recognize that following along with the arguments can be challenging at times and more details would without a doubt be helpful. For the updated draft, we have incorporated additional explanation in the text under the paragraph **Our contributions**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2889/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105688663,
                "cdate": 1700105688663,
                "tmdate": 1700105688663,
                "mdate": 1700105688663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xvgzdc1Rz7",
            "forum": "ARPrtuzAnQ",
            "replyto": "ARPrtuzAnQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2889/Reviewer_aiE7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2889/Reviewer_aiE7"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the hardness of learning certain two-layer or one-hidden-layer neural networks under symmetrized architecture/algorithmic designs on Gaussian inputs, via the Statistical Query (SQ) lower bound techniques. It provides several results characterizing the hardness of learning GNNs and CNNs via leveraging correlational SQ (CSQ) lower bounds for learning boolean functions and by connecting them with learning parity functions. It also discussed when CSQ lower bounds can be different than SQ lower bounds."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The question studied in this paper is closely related to a core question in understanding deep learning, that is: can deep learning benefit from symmetry-inspired algorithmic designs? In this sense I deem the question studied in the paper valuable and this paper's attempt to deal with it respectful.\n2. The technical contribution of this paper, although still depended on some prior works, is novel enough to my understanding to be nontrivial. This paper constructed function classes that were not studied before to specifically deal with their problems, and proved hardness of learning these classes, which is a notable effort. \n3. This paper covers both GNN and CNNs, and discussed the difference between CSQ and SQ in certain scenarios, which is good for completeness."
                },
                "weaknesses": {
                    "value": "The weaknesses listed below are, in my opinion, secondary to the contributions of this paper.\nThe approach of this paper in studying the hardness of learning symmetry-enhanced neural networks has certain limitations. It cannot account for all neural architectures at once and requires specific construction whenever the problem formulation changes by a little bit. And the hard function classes, although are well designed for the proof, are not very intuitive in terms of broader impact to people who are outside of the learning theoretic community. Perhaps there could be more illustrative explanation for the intuition behinds the constructions and also its possible impacts outside pure theory."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2889/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2889/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2889/Reviewer_aiE7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2889/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700525770740,
            "cdate": 1700525770740,
            "tmdate": 1700525770740,
            "mdate": 1700525770740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qJAR1uFnz8",
                "forum": "ARPrtuzAnQ",
                "replyto": "xvgzdc1Rz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2889/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2889/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the positive feedback and thoughtful review. \n\nWe acknowledge the reviewer's valid points regarding the limitations of our approach, specifically in terms of the lack of intuitiveness in the constructed function classes. As mentioned in other responses and in our discussion, we believe future work could focus on refining these classes or adapting the learning setting to enhance practicality. Additionally, we appreciate the observation that our approach may require specific constructions when the problem formulation undergoes changes. While we recognize this limitation, we aimed to be as broad as possible in our setting with this work and hope that our methods can serve as a basis for extensions in other settings."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2889/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687469752,
                "cdate": 1700687469752,
                "tmdate": 1700687469752,
                "mdate": 1700687469752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]