[
    {
        "title": "Fine-Tuning Language Models for Factuality"
    },
    {
        "review": {
            "id": "HxnXA8MZrV",
            "forum": "WPZ2yPag4K",
            "replyto": "WPZ2yPag4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies approaches to improve the factuality of language models (LMs) by fine-tuning the LMs using reinforcement learning (the specific algorithm is DPO), and the two main types of the reward function are: 1) reference-based FactScore (referred to as \u201cDPO-FS\u201d) and 2) reference-free uncertainty measure based on self samples (referred to as \u201cDPO-MC\u201d). \n\nThe authors conduct the experiments on two tasks: 1) free-form generation of biographies and 2) medical question answering. The datasets were crafted specifically for their experiments, hence resulting in the small size (e.g., biography train/test = 296/59 and medical QA train/test = 150/50 instances). The base LM is Llama1 and Llama2. The main experimental results show that both DPO-FS and DPO-MC generate responses with a higher \u201ccorrect\u201d percentage than baselines (SFT and inference-time methods such as ITI and DOLA). Also, DPO-FS and DPO-MC achieve a higher correct percentage than Llama-2-chat. Lastly, the authors perform a human evaluation to validate the findings previously evaluated using GPT3.5."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper shows that DPO can be applied to improve the factuality of LMs as shown by DPO-FS and DPO-MC achieving better factuality, and to the best of my knowledge, the factuality-based reward has not been investigated yet. The paper also investigates both reference-free and reference-based, and shows the effectiveness of both methods."
                },
                "weaknesses": {
                    "value": "1. Although existing work may have not used a factuality-based reward, the results in this paper are mostly the expected observations (e.g., applying RL-based training improves target rewards). For example, (Lu et al., 2022) applied RL (PPO) with a reward based on an external metric to improve toxicity, repetition, etc.\n\n2. The main findings (Tables 2, 3, 4, 5) are all based on GPT3.5 evaluation, and coupled with the fact that the test sets are small (e.g., 59 instances for biographies & 50 instances for medical QA), I\u2019m not certain how reliable the results are. Also, there is not much information in Section 5.5 about human evaluation, e.g., inter-annotator agreement, or how many annotators were employed.\n\n3. How does the DPO fine-tuned model perform on out-of-domain tasks? For example, when fine-tuning to improve factuality on biographies, does it also improve factuality on medical QA? And does its general performance change?\nThere is also a recent survey paper (Pan et al., 2023) about aligning LLMs for different aspects (including hallucination/factuality), and I think it would be useful for authors to incorporate additional relevant papers (i.e., those that apply RL to improve LMs) \n\n4. Weak base LM: This work uses Llama-7B as the base model, and at this size, the model may not yet be highly capable of long-form generation / medical QA. Previous works such as (Manakul et al., 2023) and (Mundler et al., 2023) investigated LLM hallucination with much larger LLMs (e.g., GPT3.5/4). It would be interesting to see, for example, when using larger models (either open-source such as larger Llama / Falcon-180B or private ones such as GPT-4), if the model still makes as many factual errors (because if they don\u2019t \u2013 due to the emergent ability when scaling up \u2013 fine-tuning may not be necessary or have little impact).\n\nThere is a recent survey paper (Pan et al., 2023) about aligning LLMs for different aspects (including hallucination/factuality), and I think it would be useful for authors to incorporate additional relevant papers (i.e., those that apply RL to improve LMs) \n\n*References*\n- (Lu et al., 2022) QUARK: Controllable Text Generation with Reinforced Unlearning\n- (Manakul et al., 2023) SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models\n- (Mundler et al., 2023) Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation\n- (Pan et al., 2023) Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"
                },
                "questions": {
                    "value": "My questions are related to the points in the weaknesses section. I'm looking forward to seeing your responses to the weaknesses above, especially point number 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58",
                        "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697758264819,
            "cdate": 1697758264819,
            "tmdate": 1700739511286,
            "mdate": 1700739511286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CGOjk2mcGJ",
                "forum": "WPZ2yPag4K",
                "replyto": "HxnXA8MZrV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your thoughtful assessment of our work! To address your concerns:\n\n**Regarding novelty.** While our work indeed leverages DPO, an existing algorithm, usage of model confidences as a proxy for a learning signal for RL is to our knowledge completely novel, and we show this technique is very useful for improving factuality. A priori, it is not obvious that LLMs contain a strong enough internal model of what is true and false that they would be able to learn to increase their factuality in a generalizable way. That is, the novelty of our findings is that **we can directly fine-tune language models to improve their factuality from tractably-sized datasets, without human labels or test-time retrieval/repeated sampling.** Further, we also note the call for papers lists societal considerations including \"fairness, safety, privacy\" as specifically relevant topics to ICLR; in light of the important reliability and safety risks of language model hallucinations, we feel that our work showing substantial mitigation of this problem is highly relevant and will be of strong interest to the ICLR community.\n\n**Re: evaluation metric.** To clarify our evaluation, we use FactScore to measure factuality, which has an error rate of <2% compared to human annotations [1]. GPT-3.5 is used only to parse the claims from the text, not to evaluate their correctness directly (this is done by comparing the claim with a wikipedia article). Our human study additionally supports the reliability of FactScore.\n\n[1] FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. Min et al., 2023.\n\n**Re: evaluation set size.** We generated newer, larger test sets using the same methodology as the originals. On these larger held out sets of 300 medical questions and 300 biography prompts, we find similar results as in our original test sets:\n\nOn 300 medical questions & Llama2: \n| Method | Correct | Total | % Correct |\n| --- | --- | --- | --- |\n| SFT | 9.16 | 15.9 | 0.582 |\n| FactTune-FS | 12.6 | 16.2 | 0.783 |\n| FactTune-MC | 11.3 | 16.9 | 0.671 |\n| RLHF | 8.68 | 16.2 | 0.559 |\n\nOn 300 biographies & Llama2: \n| Method | Correct | Incorrect | % Correct |\n| --- | --- | --- | --- |\n| SFT | 12.3 | 7.10 | 0.648 |\n| FactTune-FS | 16.0 | 3.13 | 0.841 |\n| FactTune-MC | 11.5 | 3.40 | 0.783 |\n| RLHF | 20.6 | 6.81 | 0.746 |\n\nThe relative performance of our methods remains similar \u2013 FactTune still surpasses SFT and RLHF. \n\n**Re: generalization across domains.** In a new experiment, we show that factuality tuning generalizes very well across data distributions. We evaluate the result of training on one data distribution (e.g, bios) and evaluating on a completely different one (e.g., medQA). **In summary, factuality tuning on bios and evaluating on medQA (or vice versa) increases factuality more than RLHF, indicating that factuality tuning makes a meaningfully general improvement to the model's factuality.**\n\nEval on Bios\n| Method | Correct | Incorrect | % correct |\n| --- | --- | --- | --- |\n| RLHF | 19.0 | 6.41 | 0.748 |\n| FactTune-FS | 21.0 | 4.50 | 0.824 |\n| (OOD-med) FactTune-FS | 21.0 | 5.31 | 0.799 |\n\nEval on MedQA\n| Method | Correct | Incorrect | % correct |\n| --- | --- | --- | --- |\n| RLHF | 9.63 | 5.50 | 0.636 |\n| FactTune-FS | 9.50 | 5.63 | 0.680 |\n| (OOD-bio) FactTune-FS | 8.81 | 5.12 | 0.658 |\n\n**Re: factuality's impact on general sample quality.** To check for decreased capabilities, we measure the fluency of each model (fine-tuned from Llama-7b on biographies) compared to SFT, using GPT-4 as a judge. We find that **our approach leaves fluency of generated text essentially unchanged.** Further, DoLA harms model sample fluency (often through increased repetition).\n\n- DOLA: Preferred 34%\n- FactTune-FS (ours): Preferred 50%\n- FactTune-MS (ours): Preferred 48%"
                    },
                    "title": {
                        "value": "Response 1/2"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646057640,
                "cdate": 1700646057640,
                "tmdate": 1700646099929,
                "mdate": 1700646099929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ndgtiLwkx7",
                "forum": "WPZ2yPag4K",
                "replyto": "PdgV6hOhfZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_mN58"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response & further experimental results to validate the findings, especially the generalizability across domains. All the points I made in my review have been addressed by the authors (e.g., evaluation, generalizability, model choice), and I've raised my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739977576,
                "cdate": 1700739977576,
                "tmdate": 1700739977576,
                "mdate": 1700739977576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EMBilxuGJs",
            "forum": "WPZ2yPag4K",
            "replyto": "WPZ2yPag4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_7K7x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_7K7x"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes fine-tuning language models to improve their factuality. Specifically, one reference-based and one reference-free method are explored to estimate the truthfulness of different model responses, the scoring/preference of which are then used to fine-tune the LMs with direct preference optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper writing is of high quality and easy to follow\n\n- The proposed method, regardless of reference-based or reference-free, shows improved factuality than the SFT baseline and the highest correct% among the compared methods.\n\n- The paper provides analysis and ablations of different variants such as fine-tuning the pretrained/chat models and combining with inference-time decoding method."
                },
                "weaknesses": {
                    "value": "- [major] I have some concerns about the evaluation\n  - The test sets (50 and 59 examples in each domain, respectively) look very limited, making it bit hard to understand the actual improvement of model factuality.  How reliable are the results? Is 75% -> 81% a lot? I can't really answer these questions after reading the paper.\n  - I noticed that the total number of claims are often different for different methods. Could generation style (e.g., length) contribute to the seemingly better/worse results? I wonder if the authors have considered such factors.\n  - There is also no evidence indicting the improved factuality doesn't come at the expense of performance in other aspects. I understand the authors may not have enough labor/compute for a more comprehensive eval like GPT or Llama but LLMs, in my experience, can behave in mysterious ways when you over index on one specific objective.\n\n- [minor] The method is somewhat straightforward, which is not necessarily a bad thing if the evaluation can show meaningful improvements (that are worth fine-tuning specifically for factuality) than methods that modify decoding only."
                },
                "questions": {
                    "value": "- I'm a little confused why choosing the largest bin to measure truthfulness in the reference-free setting. Does that mean the atomic claim doesn't really matter (\"Yo-Yo Ma was born in 1951\" and \"Yo-Yo Ma was born in 1955\" would both be converted to \"What year was Yo-Yo Ma born\")? So as long as two responses make a claim on the same fact, regardless if it's correct or wrong, they will receive the same truthfulness score? If the hypothesis is \"a language model\u2019s confidence in a generated answer is highly correlated with the probability that the answer is correct\", why not use the distribution to cross-check like in the reference-based setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Reviewer_7K7x"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458750306,
            "cdate": 1698458750306,
            "tmdate": 1700756117087,
            "mdate": 1700756117087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EyJuSSJ3L6",
                "forum": "WPZ2yPag4K",
                "replyto": "EMBilxuGJs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful feedback! To address your concerns:\n\n**Regarding evaluation set size.** We generated newer, larger test sets using the same methodology as the originals. On these larger held out sets of 300 medical questions and 300 biography prompts, we find similar results as in our original test sets:\n\nOn 300 medical questions & Llama2: \n| Method | Correct | Total | % Correct |\n| --- | --- | --- | --- |\n| SFT | 9.16 | 15.9 | 0.582 |\n| FactTune-FS | 12.6 | 16.2 | 0.783 |\n| FactTune-MC | 11.3 | 16.9 | 0.671 |\n| RLHF | 8.68 | 16.2 | 0.559 |\n\nOn 300 biographies & Llama2: \n| Method | Correct | Incorrect | % Correct |\n| --- | --- | --- | --- |\n| SFT | 12.3 | 7.10 | 0.648 |\n| FactTune-FS | 16.0 | 3.13 | 0.841 |\n| FactTune-MC | 11.5 | 3.40 | 0.783 |\n| RLHF | 20.6 | 6.81 | 0.746 |\n\nThe relative performance of our methods remains similar \u2013 FactTune still surpasses SFT and RLHF. \n\n\n**Gaining intuition about the results.** Another way to gain intuition about the results is considering error rate: an improvement of 75% to 81% correctness corresponds to a reduction in error rate from 25% to 19%. In other words, **the fraction of a model's response that is factually incorrect has decreased by 24%.** Our Llama-2 experiments show even more substantial improvement of a **50% reduction in hallucinations for biography generation and answering medical questions** on the extended test set.\n\n**Impact of length on results.** To address concerns that length is contributing to performance, we note that FactTune-FS produces a strict improvement over typical supervised fine-tuning, unlike RLHF. This fact is clearer in the following newly-added Figure 3, which shows that **only factuality tuning enables strict factuality improvement, defined as a simultaneous increase in factual statements AND decrease in incorrect statements**: https://imgur.com/a/bhrxGZX.\n\n**Impact of factuality tuning on general model sample quality.** As one way to check for decreased capabilities, we measure the fluency of each model (fine-tuned from Llama-7b on biographies) compared to SFT, using GPT-4 as a judge. We find that, for example, DoLA harms model sample fluency (often through increased repetition), while our approach leaves fluency essentially unchanged.\n\n- DOLA: Preferred 34%\n- FactTune-FS (ours): Preferred 50%\n- FactTune-MS (ours): Preferred 48%\n\n**Regarding the more general point of reward overoptimization.** While our previous two points have shown that factuality tuning does not come through exploiting length of responses or compromising the fluency of model samples, we also show that we do not over-exploit the training data by training on one data distribution (e.g, bios) and evaluating on a completely different one (e.g., medQA). **In summary, factuality tuning on bios and evaluating on medQA (or vice versa) increases factuality more than RLHF, indicating that factuality tuning makes a meaningfully general improvement to the model's factuality.**\n\n\nEval on Bios\n| Method | Correct | Incorrect | % correct |\n| --- | --- | --- | --- |\n| RLHF | 19.0 | 6.41 | 0.748 |\n| FactTune-FS | 21.0 | 4.50 | 0.824 |\n| (OOD-med) FactTune-FS | 21.0 | 5.31 | 0.799 |\n\nEval on MedQA\n| Method | Correct | Incorrect | % correct |\n| --- | --- | --- | --- |\n| RLHF | 9.63 | 5.50 | 0.636 |\n| FactTune-FS | 9.50 | 5.63 | 0.680 |\n| (OOD-bio) FactTune-FS | 8.81 | 5.12 | 0.658 |\n\n**Regarding novelty.** While our work indeed leverages DPO, an existing algorithm, usage of model confidences as a proxy for a learning signal for RL is to our knowledge completely novel, and we show this technique is very useful for improving factuality. A priori, it is not obvious that LLMs contain a strong enough internal model of what is true and false that they would be able to learn to increase their factuality in a generalizable way. That is, the novelty of our findings is that **we can directly fine-tune language models to improve their factuality from tractably-sized datasets, without human labels or test-time retrieval/repeated sampling.**  Further, we also note the call for papers lists societal considerations including \"fairness, safety, privacy\" as specifically relevant topics to ICLR; in light of the important reliability and safety risks of language model hallucinations, we feel that our work showing substantial mitigation of this problem is highly relevant and will be of strong interest to the ICLR community."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648418381,
                "cdate": 1700648418381,
                "tmdate": 1700648418381,
                "mdate": 1700648418381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D4AH2d95MH",
            "forum": "WPZ2yPag4K",
            "replyto": "WPZ2yPag4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_Pzp4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_Pzp4"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose to finetune Llama models for factuality in long-form generation tasks using DPO on automatically constructed preference pairs. Authors explore 2 methods for generating preference ratings: 1) Reference-based: Extracts atomic facts using GPT-3.5 and then use Lama-1-7B-based NLI model to determine correctness of each atomic fact with respect to the reference. Percentage of correct atomic facts is used to compare the factual correctness of samples. 2) Reference-free: Extracts facts using GPT-3.5, then use GPT-3.5 to convert a fact to a question (uses few-shot prompting). Then, through sampling answers multiple times from the model, they estimate the model's uncertainty for the actual answer. The model's uncertainty is used to compare the factual correctness of samples.\n\nThey evaluate their approach on two tasks: biography generation and open-ended medical QA. To accommodate for the reference-based metrics, they generate data based on individuals (for biographies) and medical conditions that have Wikipedia pages.\n\nResults show superior factual accuracy for DPO-based models on both tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- New results to show the benefit of using automated feedback for improving LLMs, targeting factuality for long-form open-ended generation.\n\n- Paper is well-written, experimental settings are well-defined, human evaluation is performed.\n\n- Paper also shows that DPO-finetuning is complementary to decoding-time factuality improvement method (DOLA), (DPO + DOLA outperforms DPO)"
                },
                "weaknesses": {
                    "value": "- Idea itself is not novel, RLAIF has been consistently shown to be useful (here, authors used DPO instead of PPO). Though the application is new.\n\n- Including more fine-tuning based baselines can help understand the role of automated metrics. E.g., directly using prompts to compare factuality of two outputs w.r.t. the wikipedia article instead of extracting atomic facts. \n\n- Both DPO variants reduce number of correct facts on biography generation. This does not seem very surprising, given the optimized metric is the percentage of correct atomic facts. For example, a sample with 10 correct and 5 incorrect is preferred over 11 correct and 6 incorrect. Can this bias be removed from the fine-tuned model, maybe by changing the metric or comparing samples of similar lengths? Or is it the bias of evaluation metric?"
                },
                "questions": {
                    "value": "Check questions in the Weakness section.\n\n- Between reference-free and reference-based metrics, there is a significant difference in the total number (correct + incorrect) of generated facts (almost 30% fewer) on biographies. What's the source of this bias, any possible explanations?\n\n- Could you provide statistics on the number of tokens in wining vs losing samples in all cases (dataset/model/metric)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Reviewer_Pzp4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785524107,
            "cdate": 1698785524107,
            "tmdate": 1699637051588,
            "mdate": 1699637051588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2oXE35Oeib",
                "forum": "WPZ2yPag4K",
                "replyto": "D4AH2d95MH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your thoughtful feedback on our work! \n\n**Regarding novelty.** While our work indeed leverages DPO, an existing algorithm, usage of model confidences as a proxy for a learning signal for RL is to our knowledge completely novel, and we show this technique is very useful for improving factuality. A priori, it is not obvious that LLMs contain a strong enough internal model of what is true and false that they would be able to learn to increase their factuality in a generalizable way. That is, the novelty of our findings is that we can directly fine-tune language models to improve their factuality from tractably-sized datasets, without human labels or test-time retrieval/repeated sampling. Further, we also note the call for papers lists societal considerations including \"fairness, safety, privacy\" as specifically relevant topics to ICLR; in light of the important reliability and safety risks of language model hallucinations, we feel that our work showing substantial mitigation of this problem is highly relevant and will be of strong interest to the ICLR community.\n\n**Regarding other fine-tuning baselines.** We asked GPT-3.5 to provide factuality preference pairs over our responses with CoT, but found that these preference pairs agreed with the FactScore-baed preference pairs only 55% of the time, suggesting that naively prompting GPT-3.5 does not have strong enough factuality signal. \n\n**Regarding impact of length on results.** To address concerns that length is contributing to performance: While other methods for factuality and RLHF either improve number of correct facts or decrease the number of incorrect responses compared to supervised fine-tuning, we note that FactTune-FS instead is able to improve both. This fact is clearer in the following newly-added Figure 3, which shows that factuality tuning is the only method that enables strict factuality improvement, defined as a simultaneous increase in factual statements AND decrease in incorrect statements: https://imgur.com/a/bhrxGZX.\n\nIn some applications, if generating an additional false statement is more harmful than not generating an additional correct fact, in the trade-off between longer responses and more accurate responses, may also prefer the slightly shorter but more accurate ones from a model like FactTune-MC. Viewing generations of varying lengths as a selective prediction mechanism on generating accurate facts, we may want a model to adaptively learn (such as from its own confidences) to improve accuracy by learning when to stop generating more facts. \n\nLastly, here are some length statistics:\nAverage # words/sample in train set: 100 words \nFact Score: \n- Avg # words in winning vs losing samples: 96 vs 105 words  \n- Avg difference in each pair: winning has 9.4 more words than losing \nModel Confidence: \n- Avg # words in winning vs losing samples: 96 vs 105 words  \n- Avg difference in each pair: winning has 8.7 fewer words than losing \n\nAverage # facts/sample in train set: 24 \nFact Score:\n- Avg # facts in winning vs losing samples: 22.6 vs 25.1\n- Avg diff (facts) in each pair: winning has 2.4 fewer facts than losing \nModel Confidence:\n- Avg # facts in winning vs losing samples: 23.0 vs 24.7\n- Avg diff (facts) in each pair: winning has 1.7 fewer facts than losing"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653080927,
                "cdate": 1700653080927,
                "tmdate": 1700653080927,
                "mdate": 1700653080927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oiDasYwPxq",
            "forum": "WPZ2yPag4K",
            "replyto": "WPZ2yPag4K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors construct a direct preference optimization (DPO) dataset for improving factuality using reference-based and reference-free truthfulness annotation techniques. Through the proposed method, they improve accuracy in two tasks (Biographies and Medical QA) without human factuality labels. The authors demonstrate that the proposed method (DPO-FS and DPO-MC) can be applied to Llama-2 and Llama2-Chat, and combined with a factuality-decoding approach (e.g., DOLA)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Motivation is intuitive and easy to understand\n- The proposed method improves the truthfulness of LLM without human factuality labels\n- The proposed method can be augmented with existing orthogonal approaches for factuality"
                },
                "weaknesses": {
                    "value": "- Because the framework is simple and the method of scoring truthfulness and fine-tuning technique uses existing approaches, the proposed method appears to have limited contributions"
                },
                "questions": {
                    "value": "In Biographies and Medical QA tasks, are DPO-FS, DPO-MC, and SFT fine-tuned on the training set of each dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run",
                        "ICLR.cc/2024/Conference/Submission8435/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808718382,
            "cdate": 1698808718382,
            "tmdate": 1700727151440,
            "mdate": 1700727151440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gZCnUXDklE",
                "forum": "WPZ2yPag4K",
                "replyto": "oiDasYwPxq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8435/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your feedback on our work! To address your questions:\n\n**Regarding novelty.** While our work indeed leverages DPO, an existing algorithm, usage of model confidences as a proxy for a learning signal for RL is to our knowledge completely novel, and we show this technique is very useful for improving factuality. A priori, it is not obvious that LLMs contain a strong enough internal model of what is true and false that they would be able to learn to increase their factuality in a generalizable way. That is, the novelty of our findings is that **we can directly fine-tune language models to improve their factuality from tractably-sized datasets, without human labels or test-time retrieval/repeated sampling.** Further, we also note the call for papers lists societal considerations including \"fairness, safety, privacy\" as specifically relevant topics to ICLR; in light of the important reliability and safety risks of language model hallucinations, we feel that our work showing substantial mitigation of this problem is highly relevant and will be of strong interest to the ICLR community.\n\n**Train sets.** DPO-FS, DPO-MC, and SFT are all fine-tuned with the train set of each dataset."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644982581,
                "cdate": 1700644982581,
                "tmdate": 1700644982581,
                "mdate": 1700644982581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QwHvj99QVs",
                "forum": "WPZ2yPag4K",
                "replyto": "gZCnUXDklE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8435/Reviewer_2run"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comment. After reading the comment and additional results in the general response, I have raised my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727230868,
                "cdate": 1700727230868,
                "tmdate": 1700727230868,
                "mdate": 1700727230868,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]