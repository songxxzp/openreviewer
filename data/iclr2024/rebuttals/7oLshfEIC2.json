[
    {
        "title": "TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting"
    },
    {
        "review": {
            "id": "s1Nhe5CuOm",
            "forum": "7oLshfEIC2",
            "replyto": "7oLshfEIC2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
            ],
            "content": {
                "summary": {
                    "value": "To better forecast complicated time series that include distinct patterns in each scale, this paper proposes TimeMixer which is a fully MLP-based architecture. This model incorporates Past-Decomposable-Mixing(PDM) and Future-Multipredictor-Mixing (FMM). As for PDM, it first decomposes time series into seasonal and trend parts. Subsequently, PDM processes seasonal and trend in a fine-to-coarse and coarse-to-fine manner. After PDM processes input time series, FDM forecasts future observations with multiple predictors. TimeMixer equipped with PDM and FMM shows the best results in various datasets for short-term and long-term forecasting. Furthermore, it proves the efficacy of the decomposition, different-way processing, and multiple predictors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**1. Easy to understand**  \nIn simpler terms, the paper is written well. It explains its ideas clearly and in a way that anyone can understand. This makes it easier for readers to grasp the concepts and improves the paper's overall clarity.\n\n**2. Good performance with simple models**  \nEmpirically, TimeMixer exhibits remarkably low computational and memory costs because it consists of only simple linear models. However, it's important not to overlook its performance, which should not be underestimated.\n\n**3. Good observations**  \nFrom the thought that fine-scale seasonality can provide coarse-scale one with detailed information while coarse-scale trend can provide fine-scale one with more denoised information, TimeMixer processes trend and seasonality in a different way. The experiment section proves the efficacy of this design and thought. I think that this can broadly affect how to utilize multiple scales in time series.\n\n**4. Enough experiments**  \nIn the experimental section, the paper presents numerous forecasting results, including outcomes for both long-term and short-term forecasting tasks."
                },
                "weaknesses": {
                    "value": "In spite of the good aspects of this paper, I hesitate to give acceptance because of some minor but important concerns.\n\n**1. Absence of some important baselines**\nBecause this paper is based on decomposition, I think the authors have to include methods based on decompositions [1] into baselines. Furthermore, (although they are not accepted to any conference, ) it is beneficial to include methods based on MLP-Mixer [2,3]. I think that this absence makes the second and fourth strengths fade.\n\n**2. Insufficient experiment in ablation studies**\nI think the biggest contribution of this paper, or the main reason for acceptance, is different-way processing (coarse-to-fine and fine-to-coarse). Although the thought leading to this asymmetric design is not quite intuitive, Table 5 provides empirical results. However, the used datasets are insufficient. Considering its importance, more datasets should be used so this design has to be tested more rigorously. Furthermore, because small $M=1$ is employed for short-term forecasting such as M4, M4 cannot prove the efficacy of the asymmetric design sufficiently. Finally, there isn't an ablation setting where seasonal is coarse-to-fine and the trend is fine-to-coarse. These insufficient ablation studies make the third strength vanish.\n\nIf these main concerns are resolved (include more related baselines and design more strict ablation studies to prove the efficacy of asymmetric design), I will consider raising my points.\n\n\n[1] Shabani et al., Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting, 2023, ICLR\n[2] Li et al., MTS-Mixers: Multivariate Time Series Forecasting via Factorized Temporal and Channel Mixing, 2023\n[3] Chen et al., TSMixer: An All-MLP Architecture for Time Series Forecasting, 2023"
                },
                "questions": {
                    "value": "1. Most datasets you used are multivariate datasets but I am curious about why didn't you design MLPs to capture inter-feature connections. \n\n2. I think the equation (6) is not aligned with Figure 4. An average of Figure (b,c,d,e) ($\\hat{x}=\\frac{1}{M+1} \\sum_{m=0}^{M}\\hat{x}_m $) can produce figure 4. However, just summing Figure (b,c,d,e) doesn't result in Figure (a). When I checked your code, not average but the sum was implemented. Can you explain this situation?\n\n3. In table 4, I am confused about the meaning of x mark in pasting mixing and future mixing columns. What does it mean in each column in detail? In other words, how did you implement the method with x mark in each column?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698286170610,
            "cdate": 1698286170610,
            "tmdate": 1699635995218,
            "mdate": 1699635995218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRHytYPfmI",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ixvB [Part 1]"
                    },
                    "comment": {
                        "value": "Many thanks to Reviewer ixvB for providing the insightful review and comments.\n\n> **Q1:** Add Scaleformer, MTS-Mixers, TSMixer in comparison.\n\nIn the original submission, we compared TimeMixer with 15 baselines. Following the reviewer's request, we have included Scaleformer, MTS-Mixers, and TSMixer into comparison.\n\nAs we stated in the $\\underline{\\text{Section 4 of main text}}$, we experiment with these baselines with their official code but align the hyperparameters to ensure a fair comparison. All the results have been included in the $\\underline{\\text{Appendix H of revised paper}}$. Here are some key experiments.\n\nIn addition to the quantitive results, showcase comparisons have also been added in $\\underline{\\text{Appendix J of revised paper}}$ for an intuitive comparison.\n\n| Averaged from 4 prediction lengths (MSE \\| MAE) | Scaleformer    | MTS-Mixer      | TSMixer        | TimeMixer              |\n| ----------------------------------------------- | -------------- | -------------- | -------------- | ---------------------- |\n| Weather                                         | 0.416 \\| 0.423 | 0.258 \\| 0.286 | 0.253 \\| 0.304 | **0.240** \\| **0.271** |\n| Solar-Energy                                    | 0.323 \\| 0.378 | 0.261 \\| 0.300 | 0.280 \\| 0.348 | **0.216** \\| **0.280** |\n| Electricity                                     | 0.203 \\| 0.315 | 0.201 \\| 0.293 | 0.220 \\| 0.327 | **0.182** \\| **0.272** |\n| Traffic                                         | 0.578 \\| 0.352 | 0.558 \\| 0.375 | 0.546 \\| 0.372 | **0.484** \\| **0.297** |\n| ETTh1                                           | 0.495 \\| 0.488 | 0.482 \\| 0.481 | 0.466 \\| 0.467 | **0.447** \\| **0.440** |\n| ETTh2                                           | 0.451 \\| 0.460 | 0.426 \\| 0.435 | 0.394 \\| 0.412 | **0.364** \\| **0.395** |\n| ETTm1                                           | 0.438 \\| 0.440 | 0.415 \\| 0.419 | 0.408 \\| 0.416 | **0.381** \\| **0.395** |\n| ETTh2                                           | 0.303 \\| 0.351 | 0.296 \\| 0.342 | 0.290 \\| 0.344 | **0.275** \\| **0.323** |\n\n> **Q2:** Complete the ablation studies to prove the efficacy of asymmetric design.\n\nIn the original submission, we conducted experiments on M4 and ETTh1, which are two representative datasets in long- and short-term forecasting. Thanks for the reviewer's suggestion, we have completed the ablations in all the benchmarks. Besides, we also add a new ablation setting, where we adopt the bottom-up mixing for trend and top-down mixing for the seasonal part.\n\n**Overall, we have conducted 10 types of ablations in all 18 benchmarks.** Here are part of the main results. The complete results have been included in $\\underline{\\text{Table 5 and Table 15,16,17 of revised paper}}$. In all benchmarks, our official design performs best consistently, which can verify the efficacy of our asymmetric design.\n\nIt is also notable that completely reversing mixing directions for seasonal and trend parts leads to a serious performance drop. This may come from that the essential microscopic information in finer-scale seasons and macroscopic information in coarser-scale trends are ruined by unsuitable mixing approaches.\n\n| Ablations                                                    | ETTm1 (Predict-336) MSE \\|MAE | M4 SMAPE \\|MASE \\|OWA             | PEMS04 MAE \\|MAPE \\|RMSE        |\n| ------------------------------------------------------------ | -------------------------------- | ------------------------------------ | ----------------------------------- |\n| Bottom-up mixing for trend and top-down mixing for seasonal part | 0.412 \\| 0.429                   | 13.012 \\| 1.657 \\| 0.954             | 22.27 \\| 15.14 \\| 34.67             |\n| TimeMixer                                                    | **0.390** \\| **0.404**           | **11.723** \\| **1.559** \\| **0.840** | **19.21** \\| **12.53** \\| **30.92** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039441964,
                "cdate": 1700039441964,
                "tmdate": 1700039453030,
                "mdate": 1700039453030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tG1xGWh4fX",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe kindly remind you that it has been 3 days since we posted our rebuttal. Please let us know if our response has addressed your concerns. \n\nFollowing your suggestion, we have answered your concerns and improved the paper in the following aspects: \n\n- We have **added 3 new baselines (Scaleformer, MTSMixer, TSMixer) in all 18 benchmarks** to demonstrate the advancement of TimeMixer. Both **quantitive results and showcases** are provided in the revised paper.\n- We can **complete 10 types of ablations in all 18 benchmarks**, which verifies the effectiveness of our design in TimeMixer. The implementation details of each ablation have also been included.\n- We have also provided the implementation details for the visualization experiments. \n\nIn total, we have added more than 100 new experiments. **All of these results have been included in the $\\underline{\\text{revised paper}}$.**\n\nThanks again for your valuable review. We are looking forward to your reply and are happy to answer any future questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303269624,
                "cdate": 1700303269624,
                "tmdate": 1700303269624,
                "mdate": 1700303269624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ThIxGTpZCQ",
                "forum": "7oLshfEIC2",
                "replyto": "tG1xGWh4fX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author"
                    },
                    "comment": {
                        "value": "Thank you for your effort to resolve my concerns, including abundant experiments.\n\nHowever, there are still some nonsensical parts to me. The followings are some concerns that I still maintain. \n\n**1. Concerns about your empirical evidence for the asymmetric design.**  \nAs I said before, I think the only novel part of your method is the asymmetric design. However, your intuitions (\"essential microscopic information in finer-scale seasons and macroscopic information in coarser-scale trends\") for this design aren't very convincing. For example, microscopic information in finer-scale seasons can still contain a lot of noise, leading to useless information. Therefore, very rigorous tests are required. However, despite additional experiments, there are still non-sensical parts in your evaluation settings of ablation studies.\n\n- (1) Because small M is employed for short-term forecasting such as M4, M4 cannot prove the efficacy of the asymmetric design sufficiently.   \n &rarr; Ablation studies (Table 5) must include short-term forecasting results with large M.\n- (2) In long-term forecasting experiments, a relatively small input length, 96, is used. I think the effects of decomposition and making information coarse and fine might be significant in long input length. I think that with a small input length, it is hard to recognize the effect of your design. For this reason, the efficacy of your method against TSMixer, MTSMixer, and Scaleformer has to be proven in long input length.   \n&rarr; Ablation studies (Table 5) in long-term forecasting have to include the results with a large search space of the input length, such as in Table 14. Also, TSMixer, MTSMixer, and Scaleformer have to be compared to TimeMixer in Table 14.\n\n**2. Concerns about evaluations in Figure 4**  \nI understand your evaluation settings. However, I think this setting makes some changes in the original design and the changes are not trivial. With this significant change, I think the exact behavior of your method cannot be caught. Maintaining your original design, this figure becomes meaningful. \n\n**3. Additional question**\nAfter identifying the exact design of your method, I am curious about the reason why the final future multi-perdictor mixing is designed with not averaging but summing. Considering the scales of input in each predictor, averaging is more natural. Are there any intuition, theoretical reasons, or empirical results for this design?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447977473,
                "cdate": 1700447977473,
                "tmdate": 1700447977473,
                "mdate": 1700447977473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4OC09KiYHs",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Second Response to Reviewer ixvB [Part 2]"
                    },
                    "comment": {
                        "value": "> **Q2: Concerns about evaluations in Figure 4.** \n\nThe original motivation of Figure 4 is to **\"provide an intuitive understanding of the forecasting skills of multiscale series\", which means that our experiment subject is the learned multiscale features, not predictors.** Thus, we think our previous fix-PDM-retrain-FMM design can verify this point.\n\nBut as per your request, **we have also provided the case of directly visualizing the results of each predictor in $\\underline{\\text{Appendix A (Figure 7) of revised paper}}$ as a supplement, which is also intuitive in verifying the different forecasting capabilities of series in different scales.** \n\nBut since we use the sum ensemble, the scale of each predictor output is around $\\frac{1}{M+1}$ of real series. More discussions are included in the **Q3**.\n\n> **Q3: Are there any intuition, theoretical reasons, or empirical results for this design?**\n\nFirstly, we would like to highlight that in our design, the loss is calculated based on the ensembled results, not each predictor, that is $MSE(\\mathbf{x},\\hat{\\mathbf{x}})$=$MSE(\\mathbf{x},\\sum_{m=0}^{M}\\hat{\\mathbf{x}}_{m})$.\n\nThus, if you change the ensemble strategy as \"average\", the loss will be $MSE(\\mathbf{x},\\hat{\\mathbf{x}})$=$MSE(\\mathbf{x},\\frac{1}{M+1}\\sum_{m=0}^{M}\\hat{\\mathbf{x}}_{m})$. Obviously, the difference between \"average\" and \"mean\" is only a constant multiple.\n\n**Theoretical intuition:** It is common sense in the deep learning community that deep models can easily fit constant multiple. For example, if we replace the \"sum\" with \"average\", under the same supervision, the deep model can easily fit this change by learning the parameters of each predictor equal to the $\\frac{1}{M+1}$ of the \"sum\" case, which means these two designs are equivalent in learning final prediction under the deep model aspect. And we do expect the reviewer can think about this design from the deep learning perspective.\n\n**Empirical results:** Following your suggestion, we also provide the empirical results here. We can find that the performances of these two strategies are very close.\n\n|                     | M4 (SMAPE \\| MASE \\| OWA) | PEMS04 (MAE \\| MAPE \\| RMSE) | ETTm1 (MSE \\| MAE) |\n| ------------------- | ------------------------- | ---------------------------- | ------------------ |\n| TimeMixer (sum)     | 11.723\\|1.559\\|0.840      | 19.21\\|12.53\\|30.92          | 0.390\\|0.404       |\n| TimeMixer (average) | 11.742\\|1.573\\|0.851      | 19.17\\|12.45\\|30.88          | 0.391\\|0.407       |\n| Relative fluctuation on the first metric |  **0.16%**     | **0.20%**          |   **0.25%**     |\n\nThese results have also been included in $\\underline{\\text{G.4 of revised paper}}$.\n\nThanks again for your detailed and prompt response, which is very important to us. **All the results for Q2 and Q3 are inclued in $\\underline{\\text{revised paper highlighted in purple}}$. The new results for Q1 will come soon.**"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485302463,
                "cdate": 1700485302463,
                "tmdate": 1700487251659,
                "mdate": 1700487251659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BQtHh320NV",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Second Response to Reviewer ixvB [Part 4]"
                    },
                    "comment": {
                        "value": "> **Q1-Part 2: \"Ablation studies (Table 5) in long-term forecasting have to include the results with a large search space of the input length, such as in Table 14.\"**\n\nIn Table 5, we provide the long-term forecasting results of ETTm1 in the input-96-predict-336 setting. Following the reviewer's suggestion, we also conducted the ablations on input-336 (searched input length). Here are the results. We can find that under the input-336 configuration, the relative promotion is more significant than the smaller input setting.\n\nAs for the \"noise in finer-scale seasons\" that the reviewer mentioned, we would like to recall that the **bottom-up mixing layer will downsample the seasonal information**, and the downsample operation can play a denoising role in principle. However since we attempt to propose a practical model, the theoretical proof is left as future work. In fact, it is hard to find strict proof for deep models. We can only provide the intuitive understanding and empirical results (extensive ablations that we have provided).\n\n| Metric (MSE\\|MAE)                    | ETTm1 with input-96 (Table 5 of main text) | Relative Promotion for input-96 in MSE | ETTm1 with input-336 (Table 21 of appendix) | Relative Promotion for input-336 in MSE |\n| ------------------------------------ | ------------------------------------------ | -------------------------------------- | ------------------------------------------- | --------------------------------------- |\n| 1 (Official design of TimeMixer)     | **0.390 \\| 0.404**                         | -                                      | **0.360 \\| 0.381**                          | -                                       |\n| 2 (Remove FMM)                       | 0.402 \\| 0.415                             | 2.9%                                   | 0.375 \\| 0.398                              | 4.1%                                    |\n| 3 (Remove seasonal mixing)           | 0.411 \\| 0.427                             | 5.1%                                   | 0.390 \\| 0.415                              | 7.7%                                    |\n| 4 (Remove trend mixing)              | 0.405 \\| 0.414                             | 3.7%                                   | 0.386 \\| 0.410                              | 6.8%                                    |\n| 5 (Change seasonal mixing direction) | 0.392 \\| 0.413                             | 0.5%                                   | 0.371 \\| 0.389                              | 3.0%                                    |\n| 6 (Change trend mixing direction)    | 0.396 \\| 0.415                             | 1.5%                                   | 0.370 \\| 0.388                              | 2.7%                                    |\n| 7 (Change all the mixing direction)  | 0.412 \\| 0.429                             | 5.3%                                   | 0.384 \\| 0.409                              | 6.2%                                    |\n\n> **Q1-Part 3: \"Also, TSMixer, MTSMixer, and Scaleformer have to be compared to TimeMixer in Table 14.\"**\n\nFollowing the reviewer's request, we reproduce these baselines with their official code and conduct a comprehensive hyperparameter search, where TimeMixer still performs best. Here are the results. (See $\\underline{\\text{Table 23 of revised paper}}$ for full results)\n\n| Averaged from 4 prediction lengths (MSE \\| MAE) | Scaleformer    | MTS-Mixer      | TSMixer        | TimeMixer          |\n| ----------------------------------------------- | -------------- | -------------- | -------------- | ------------------ |\n| Weather                                         | 0.248 \\| 0.304 | 0.254 \\| 0.278 | 0.240 \\| 0.269 | **0.222 \\| 0.262** |\n| Solar-Energy                                    | 0.270 \\| 0.333 | 0.230 \\| 0.277 | 0.229 \\| 0.283 | **0.192 \\| 0.244** |\n| Electricity                                     | 0.191 \\| 0.298 | 0.179 \\| 0.286 | 0.167 \\| 0.262 | **0.156 \\| 0.246** |\n| Traffic                                         | 0.443 \\| 0.307 | 0.539 \\| 0.354 | 0.415 \\| 0.290 | **0.387 \\| 0.262** |\n| ETTh1                                           | 0.468 \\| 0.466 | 0.461 \\| 0.464 | 0.418 \\| 0.432 | **0.411 \\| 0.423** |\n| ETTh2                                           | 0.412 \\| 0.432 | 0.397 \\| 0.422 | 0.350 \\| 0.399 | **0.316 \\| 0.384** |\n| ETTm1                                           | 0.406 \\| 0.421 | 0.393 \\| 0.405 | 0.350 \\| 0.377 | **0.348 \\| 0.375** |\n| ETTh2                                           | 0.288 \\| 0.336 | 0.284 \\| 0.335 | 0.270 \\| 0.326 | **0.256 \\| 0.315** |\n\nDuring the rebuttal period, following the instruction from the reviewer, we have done our best to provide double experiment results for both settings for scientific rigor. We hope our effort in rebuttal (**more than 200 new results in total, 9 new pages, 7 days experiment with 32 A100 GPUs**) can resolve the reviewer's concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567649776,
                "cdate": 1700567649776,
                "tmdate": 1700567706750,
                "mdate": 1700567706750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5SAXUQfZRF",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The discussion period ending soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThanks again for your valuable and constructive review, which helped us improve the paper in completing ablations and comparisons. All the updates have been included in the  $\\underline{\\text{revised paper highlighted in purple}}$.\n\n**We kindly remind you that the reviewer-author discussion phase will end in 3 hours. May we know if our response addresses your main concerns? After that, we will not have a chance to respond to your comments.**\n\nSincere thanks for your dedication! We eagerly await your reply."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730679877,
                "cdate": 1700730679877,
                "tmdate": 1700731005421,
                "mdate": 1700731005421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bhtk77u1JC",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[The discussion period will end in one hour] & [Summary of rebuttal]"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nMany thanks for your valuable review. With the deadline for the author-reviewer discussion phase drawing near (**less than 1 hour**), we wish to ensure that our response sufficiently addressed your concerns. After this hour, we may not have a chance to answer your question. \n\nFor clearness, we summary our revision that we made towards your concerns here.\n\n**(1) New baselines**: we have added 3 baselines that you mentioned in all 18 benchmarks under both unified and search hyperparameter settings.\n\n**(2) New ablations:** we have completed 10 types of ablations in all 18 benchmarks under the unified hyperparameter setting and provided the requested ablations in two specific settings faithfully following the reviewer's request.\n\n**(3) New visualizations:** we have provided visualization for FMM in two types of configurations and clarified that ensemble strategies that the reviewer mentioned are equivalent in the deep model perspective.\n\nIn total, we have provided extensive experiments (**more than 200 new results, 9 new pages**) to address every concern that you proposed. Hope our effort can address your concerns to your stastification and looking forward to your reply."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739580514,
                "cdate": 1700739580514,
                "tmdate": 1700740256387,
                "mdate": 1700740256387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6zO3wQab0Q",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_ixvB"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, I want to express my gratitude for your effort. \n\nI read all of the authors' responses and the discussions with other reviewers.\n\nHowever, there are three points where I am reluctant to raise the point.\n\n1) Actually, I found that the performance scores of TSMixer in your paper are different from those of TSMixer officially reported in [1] whereas PachTST is the same in both cases. This inconsistency is quite critical in believing the scores in the paper. \n\n2) Furthermore, when comparing the scores of TimeMixer to those of TSMixer reported in [1], I don't think there is a large margin between them. At this point, I don't know that the module the authors proposed is truly required to boost forecasting performance when considering that TimeMixer is similar to TSMixer without some methods. \n\n3) Finally, in this case where performance improvement is mediocre, I believe that more ablation studies are required to emphasize the effectiveness of asymmetric design.\n\nBecause of these points, I decide to uphold the score as before. (Still, I respect the different opinions of other reviewers.)\n\n[1] Chen et al., TSMixer: An All-MLP Architecture for Time Series Forecasting, 2023"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740445718,
                "cdate": 1700740445718,
                "tmdate": 1700740590891,
                "mdate": 1700740590891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XoeKPNIq9o",
                "forum": "7oLshfEIC2",
                "replyto": "s1Nhe5CuOm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Many thanks for your response.\n\n> Inconsistency with TSMixer offical results comes from the training epochs.\n\nAs for the comparison with TSMixer, we would like to highlight that the difference is from the training epoch, where **we only employ 10 training epochs due to the time limitation in rebuttal, while TSMixer employs 100 epochs.** Note that training 10 epochs is a convention in this area.\n\nBeisdes, as we always emphasized, we have made a great effort to ensure fair comparison. The unified hyperparameter setting results are also important, given many works have followed this setting, such as Scaleformer, MTSMixer, TimesNet.\n\n> About ablations.\n\nSince many preivous papers only provide the input 96 settings and we have provided ablations in both input settings strictly following your suggestions, we do hope the reviewer can take the double workload into consideration.\n\nOverall, we have made our best to ensure fair compairson. Hope the reviewer can reconsider the score."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741054708,
                "cdate": 1700741054708,
                "tmdate": 1700741451504,
                "mdate": 1700741451504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wvY48GhWNH",
            "forum": "7oLshfEIC2",
            "replyto": "7oLshfEIC2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission680/Reviewer_ansv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission680/Reviewer_ansv"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackled the challenge of temporal variations in time series forecasting, recognizing that real-world time series frequently present complex temporal fluctuations, which intensify the intricacy of forecasting. In response, they introduced TimeMixer: a model grounded in a multiscale mixing architecture, integrating both Past-Decomposable-Mixing and FutureMultipredictor-Mixing components. Central to their approach is the notion of capturing variations across different time scales. This design leverages disentangled variations while harnessing complementary forecasting strengths. Empirical evaluations revealed that TimeMixer consistently achieved state-of-the-art results for both long-term and short-term forecasting, with its MLP-centric architecture offering remarkable runtime efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Solid Motivation**: \n    - The motivation behind the paper is robust and well-justified. \n    - The significance of **TIME SERIES FORECASTING** is inherently evident and requires no further validation.\n    - Echoing the authors' sentiments, the multiscale analysis paradigm stands out as a classic yet crucial methodology to model the intricate temporal variations inherent in time series data.\n\n2. **Coherent Conceptual Framework**: \n    - The core idea of decomposing the signal into various scales and subsequently aligning trends across these scales is both intuitive and compelling.\n    - This seemingly straightforward approach not only resonates with the fundamentals of time series analysis but has also demonstrated its efficacy in the authors' experiments.\n\n3. **Rigorous Experimental Evaluation**:\n    - The experiments conducted in the paper are methodologically sound, lending further credence to the proposed approach.\n    - The ablation study is particularly convincing, illuminating the individual contributions of different components.\n    - It's worth noting the diverse range of benchmarks utilized and the comparison with competitive models, which further underscores the robustness and generalizability of the proposed method.\n\n4. **Clear Presentation**:\n    - The paper is commendably articulated. \n    - The authors present their ideas with a blend of intuitive explanations supplemented with coherent textual descriptions, making the content both accessible and insightful for readers."
                },
                "weaknesses": {
                    "value": "1. **Analysis and Explanation of the Results**:\n    - While the empirical experiments, inclusive of the ablation study, provide evidence of the effectiveness of TimeMixer, the underlying reasons for its superior performance remain somewhat opaque. Specifically, when juxtaposed against competing models, it's not lucidly expounded how TimeMixer excels in capturing temporal variations. A deeper dive into this comparative analysis would have been enlightening. Introducing spectral analysis or similar methodologies might offer theoretical insights that bridge this understanding gap.\n\n2. **Alternative Decomposition Methods**:\n    - The paper seems to sidestep the exploration of alternative decomposition techniques that are prevalent in the domain. For instance, methods such as the Discrete Fourier Transformer (DFT) have been widely recognized in time series analysis. How does the TimeMixer's Past Decomposable Mixing distinguish itself fundamentally from these frequency-based techniques? A comparative discourse delving into the nuances between the proposed method and established frequency-based approaches would enrich the paper's content and address potential queries from the readership. \n    - Average pooling is only one of the pooling methods one could try. Is that possible that any alternatives could be better or how the authors could prove that average pooling is the optimal one."
                },
                "questions": {
                    "value": "In relation to the second weakness mentioned, how does TimeMixer's Past Decomposable Mixing fundamentally differentiate itself from frequency-based methods like the Discrete Fourier Transformer (DFT)? I wonder if, while it might compute variations across domains, it might not efficiently capture temporal dynamics without specific design considerations. Furthermore, as I've highlighted, could there be alternatives to Average pooling that might perform better? How can the authors demonstrate that average pooling is indeed the optimal choice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Reviewer_ansv",
                        "ICLR.cc/2024/Conference/Submission680/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719460784,
            "cdate": 1698719460784,
            "tmdate": 1699642262417,
            "mdate": 1699642262417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "poAaykDDgK",
                "forum": "7oLshfEIC2",
                "replyto": "wvY48GhWNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ansv [Part 1]"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer ansv for providing the valuable feedback.\n\n> **Q1:** \"A deeper dive into this comparative analysis would have been enlightening. Introducing spectral analysis or similar methodologies might offer theoretical insights that bridge this understanding gap.\"\n\nThanks for the reviewer's valuable suggestion. We have provided the spectral analysis in the $\\underline{\\text{Appendix I of revised paper}}$. It is observed that TimeMixer outperforms other baselines significantly in capturing different frequencies. \n\n> **Q2-part 1:** \"How does TimeMixer's Past Decomposable Mixing fundamentally differentiate itself from frequency-based methods like the Discrete Fourier Transformer (DFT)?\"\n\n**(1) Ablations on decomposition methods**\n\nIn this paper, we adopt the moving-average-based season-trend decomposition, which is widely used in previous work, such as Autoformer, FEDformer and DLinear. Following the reviewer's suggestion, we also try the DFT-based decomposition as a substitute. Here we present two types of experiments.\n\n- DFT-based high- and low-frequency decomposition: We treat the high-frequency part like the seasonal part in TimeMixer and the low-frequency part like the trend part.\n- DFT-based season-trend decomposition: We replace the moving average with DFT-based seasonal part extraction, which the most significant frequencies are extracted as season. Then the rest is trend.\n\nThe results are shown as follows, which has also been included in $\\underline{\\text{Appendix G.2 of revised paper}}$.\n\n| Ablation on decomposition methods               | ETTm1 (Predict-336) MSE \\| MAE | M4 SMAPE \\| MASE \\| OWA | PEMS04 MAE \\| MAPE \\| RMSE |\n| ----------------------------------------------- | ---------------------------------- | -------------------------- | ------------------------------- |\n| DFT-based high- and low-frequency decomposition | 0.392 \\| 0.404                     | 12.054 \\| 1.632 \\| 0.862   | 19.83 \\| 12.74 \\| 31.48         |\n| DFT-based season-trend decomposition            | 0.383 \\| 0.399                     | 11.673 \\| 1.536 \\| 0.824   | 18.91 \\| 12.27 \\| 29.47         |\n| moving-average-based Season-trend decomposition | 0.390 \\| 0.404                     | 11.723 \\| 1.559 \\| 0.840   | 19.21 \\| 12.53 \\| 30.92         |\n\nWe can have the following observations:\n\n- Replacing the season-trend decomposition with high-low-frequency decomposition does not bring performance gain. Since we only explore the proper mixing approach for the former decomposition in the paper, the bottom-up and top-down mixing strategies may be not suitable for high- and low-frequency parts. New visualizations like $\\underline{\\text{Figure 3 and 4 of original submission}}$ are expected to provide insights to the model design. Thus, we would like to leave the exploration as the future work.\n\n- Enhancing season-trend decomposition with DFT perfroms better than moving average. However, since moving average is quite simple and easy to implement with PyTorch, we eventually chose the moving-average-based season-trend decomposition in TimeMixer, which can also achieve a favorable balance between performance and efficiency.\n\n**(2) Discussion about season-trend decomposition and DFT-based methods**\n\nFirstly, we would like to highlight that **we propose TimeMixer towards a simple but effective method**, which we have already emphasized in the Introduction section. **In the spirit of building a practical model, proving that seasonal-trend decomposition is optimal or fundamentally different from other decomposition methods is out of the scope of our paper.** \n\nBut thanks for the reviewer's suggestion. We would like to leave this discussion of the optimality and completeness of our model as future work, which has been added to the $\\underline{\\text{Appendix F of revised paper}}$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039144013,
                "cdate": 1700039144013,
                "tmdate": 1700039357382,
                "mdate": 1700039357382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uEYoiiy46b",
                "forum": "7oLshfEIC2",
                "replyto": "wvY48GhWNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ansv [Part 2]"
                    },
                    "comment": {
                        "value": "> **Q2-part 2:** \"Could there be alternatives to average pooling that might perform better? How can the authors demonstrate that average pooling is indeed the optimal choice?\"\n\nAs per the reviewer's request, we replaced the average pooling with 1D convolutions with stride as 2. Here are the results. we can find that the complicated 1D-convolution-based outperforms average pooling slightly. But considering both performance and efficiency, we eventually use average pooling in TimeMixer.\n\n| Ablation on downsampling methods | ETTm1 (Predict-336) MSE \\|MAE | M4 SMAPE \\|MASE \\|OWA | PEMS04  MAE \\|MAPE \\|RMSE |\n| -------------------------------- | -------------------------------- | ------------------------ | ---------------------------- |\n| Moving average                   | 0.390 \\|  0.404                  | 11.723 \\| 1.559 \\| 0.840 | 19.21 \\| 12.53 \\| 30.92      |\n| 1D convolutions with stride as 2 | 0.387 \\|  0.401                  | 11.682 \\| 1.542 \\| 0.831 | 19.04 \\| 12.17 \\| 29.88      |\n\nAgain, we just want to build a practical model. Although average pooling may be not optimal, it obviously an effective and easy-to-implement choice. The above ablation has also been included in $\\underline{\\text{Appendix G.3 of revised paper}}$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039195338,
                "cdate": 1700039195338,
                "tmdate": 1700039367666,
                "mdate": 1700039367666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gc25Db7LQZ",
                "forum": "7oLshfEIC2",
                "replyto": "poAaykDDgK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_ansv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_ansv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for addressing my questions and concerns. Although I still believe that emphasizing optimalism for this work is crucial in the end, I appreciate your efforts in conducting new experiments to explore the methods I suggested. I anticipate your future work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591566034,
                "cdate": 1700591566034,
                "tmdate": 1700591566034,
                "mdate": 1700591566034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fe0DSHOHHU",
            "forum": "7oLshfEIC2",
            "replyto": "7oLshfEIC2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission680/Reviewer_P2SD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission680/Reviewer_P2SD"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed TimeMixer, which employs a multiscale mixing architecture to address the complex temporal variations in time series forecasting. By utilizing Past-Decomposable-Mixing and Future-Multipredictor-Mixing blocks, TimeMixer leveraged disentangled variations and complementary forecasting capabilities. Additionally, thanks to its fully MLP-based architecture, TimeMixer demonstrated efficient runtime processing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The paper has a clear and easily understandable structure.\n\nS2. The experiments are extensive, involving long time series forecasting without the use of highly noisy exchange-rate and illness datasets. Additionally, a new solar-energy dataset is introduced, and the provided code and configurations enhance the credibility of the experimental results."
                },
                "weaknesses": {
                    "value": "W1. In general, upsampling results in more data points, while downsampling results in fewer data points (as illustrated in Figure 1, leftmost). Based on this, I believe the descriptions 'Up-mixing' and 'Down-Mixing' in Figure 2 by the authors may not be appropriate and should perhaps be reversed.\n\nW2. The paper lacks significant innovation. (1) Decoupling of multiscale [1], seasonal-trend disentanglement [2] are common modules that have already been proposed.  (2) The so-called FMM module appears to be essentially a linear layer mapping the concatenated multiscale features back to the prediction length in the time dimension. In my view, the author's main contribution seems to be the integration of these two [1,2]  modules.\n\n[1] MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting\n\n[2] Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting\n\nW3. The improvement in experimental performance is marginal. Table 10 (the best results among all baselines) reveals that the actual enhancement by TimeMixer is not substantial, especially in three larger datasets, 'traffic,' 'weather,' and 'Electricity,' where it only holds a slight advantage at the third decimal place."
                },
                "questions": {
                    "value": "Please see my listed weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission680/Reviewer_P2SD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698933180987,
            "cdate": 1698933180987,
            "tmdate": 1700664482200,
            "mdate": 1700664482200,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4f7CHp2iBD",
                "forum": "7oLshfEIC2",
                "replyto": "Fe0DSHOHHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P2SD [Part 1]"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer P2SD for providing a detailed review and insightful suggestions. \n\n> **Q1:** \"The descriptions 'Up-Mixing' and 'Down-Mixing' in Figure 2 may not be appropriate and should perhaps be reversed.\"\n\nSorry for these two inaccurate formalizations. Following your suggestion, we have rephrased 'Up-Mixing' and 'Down-Mixing' to 'Bottom-Up-Mixing' and 'Top-Down-Mixing'  in both figures and text of the $\\underline{\\text{reviserd paper}}$.\n\n\n\n> **Q2-part1:** \"The paper lacks significant innovation. In my view, the author's main contribution seems to be the integration of these two modules from MICN and Autoformer.\"\n\n**(1) TimeMixer is clearly distinct from MICN and Autoformer.**\n\nFirstly, TimeMixer is a **MLP-based** model, while MICN is **convolution-based** and Autoformer is **Transformer-based**.\n\nSecondly, although multiscale analysis and decomposition have been used in previous models, we would like to highlight that **their usage in TimeMixer is distinct from MICN and Autoformer in both motivation and technical design,** which is summarized in the following tables:\n\n| Motivation comparison | Why use decomposition?                                       | Why use multiscale analysis?                                 |\n| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Autoformer            | Disentangle intricate variations                             | **/**                                                        |\n| MICN                  | Disentangle intricate variations                             | Aggregate **local information** in different size regions with multiple convolutional kernels |\n| **TimeMixer (Ours)**  | We find that **mixing directions should be different for seasonal and trend parts**. | **1. Disentangle intricate variations** **2. Utilize complementary forecasting capabilities in multiscale series** |\n\n| Design comparison    | How to use decomposition?                                    | How to use multiscale analysis?                              |\n| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Autoformer           | Decompose **one-scale input series and one-scale intermediate results** | **/**                                                        |\n| MICN                 | Decompose **one-scale input series**                         | **Only for the seasonal part**                               |\n| **TimeMixer (Ours)** | Decompose **multiscale deep features**                       | **1. For both seasonal and trend features** **2. Propose to use different mixing directions for multiscale seasonal and trend features** |\n\nWe do appreciate that MICN and Autoformer introduce multiscale analysis and decomposition into deep time series forecasting. But obviously, these three models are quite different in both model architecture and usage of the aforementioned two modules.\n\n**(2) The design in adopting different mixing directions for decomposed parts should not be overlooked.**\n\nIn the paper, **we propose to utilize top-down and bottom-up mixing directions for seasonal and trend parts respectively**, which is a key contribution in our model. This design is well supported by:\n\n- Motivation analysis in $\\underline{\\text{3rd paragraph in Introduction}}$ and $\\underline{\\text{Section 3.2}}$.\n- Ablations in $\\underline{\\text{Table 5}}$, which verifies that our proposed mixing approach performs best.\n- Visualizations in $\\underline{\\text{Figure 3}}$, which demonstrates that seasonal and trend parts should utilize different mixing directions.\n\nThus, TimeMixer is far beyond simple integration of multiscale analysis and decomposition. To highlight this point, we have rephrased the listed contributions in the $\\underline{\\text{Introduction of revised paper}}$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039024320,
                "cdate": 1700039024320,
                "tmdate": 1700039321400,
                "mdate": 1700039321400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gzcAFLRBpY",
                "forum": "7oLshfEIC2",
                "replyto": "Fe0DSHOHHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P2SD [Part 2]"
                    },
                    "comment": {
                        "value": "> **Q2-part 2:** \"The so-called FMM module appears to be essentially a linear layer mapping the concatenated multiscale features back to the prediction length in the time dimension.\"\n\n**(1) There might be some misunderstandings of FMM (Future-Multipredictor-Mixing).** \n\nAs presented in $\\underline{\\text{Equation 6 of main text}}$, we adopt **different predictors** to regress the future from past features in different scales separately.\n\nThus, we think the review might be misconceived in the following two points:\n\n- We do not concatenate multiscale features, but keep the past information in their original length and directly regress the future separately. *(This is also different from the multiscale design in MICN.)*\n- FMM is not a single linear layer mapping, but an ensemble of multiple predictors.\n\nFor clearness, we have rephrased $\\underline{\\text{Section 3.3 in the revised paper}}$ to clarify the misleading parts.\n\n**(2) FMM can utilize complementary forecasting capabilities in multiscale series, not just align the time dimension.** \n\nAs we presented in $\\underline{\\text{Figure 4 of original submission}}$, different scales present complementary forecasting capabilities, which is why we adopt multiple different predictors in FMM. To our best knowledge, this point is firstly explored by TimeMixer, which is also an important design in our paper.\n\n> **Q3:** \"The improvement in experimental performance is marginal, especially in three larger datasets, 'Traffic', 'Weather,' and 'Electricity'.\"\n\nWe believe that to measure the value of a deep model, we should consider $\\underline{\\text{performance, hyperparameter tuning cost and efficiency}}$ simultaneously.\n\n**(1) In this unified hyperparameter setting, TimeMixer surpasses other baselines significantly.**\n\nAs shown in $\\underline{\\text{Table 2,3,4 of original submission}}$, TimeMixer is clearly better than the previous state-of-the-art PatchTST, SCINet, and TimesNet in both performance and efficiency. This advantage is meaningful since sometimes we do not have enough time or sources for hyperparameter searching in real-world applications.\n\n**(2) In the hyperparameter-search setting, TimeMixer is much more efficient than the second-best model PatchTST.**\n\nAs pointed out by the reviewer, after a comprehensive hyperparameter-search, the relative improvement of TimeMixer against PatchTST is smaller than the unified hyperparameter setting. But we have to highlight that, TimeMixer still holds a significant advantage in efficiency, making it a valuable model to this community and applications.\n\nTo make this clearer, we list some results from $\\underline{\\text{Table 8 and Table 14 of original submission}}$ as follows. As presented in $\\underline{\\text{Table 6 of original submission}}$, Solar-Energy is the largest dataset and Weather is the second. We can find that TimeMixer surpasses PatchTST with 25% and 7.8% relative promotion in these two large benchmarks even after the hyperparameter searching. Besides, TimeMixer is over 2 times faster than PatchTST. In addition, we also present the results of DLinear, which is comparable to TimeMixer in efficiency. But TimeMixer beats it with more than 10% performance gain.\n\nThese analyses have also been included in the $\\underline{\\text{Appendix E of revised paper}}$.\n\n| Model     | Solar-Energy (averaged MSE) Largest dataset | Weather (averaged MSE) Second Large dataset | Electricity (averaged MSE) | Traffic (averaged MSE) | GPU memory (predict-384) | Running Time (predict-384) |\n| --------- | ----------------------------------------------- | ---------------------------------------------- | -------------------------- | ---------------------- | ------------------------ | -------------------------- |\n| PatchTST  | 0.256                                           | 0.241                                          | 0.159                      | 0.391                  | 2097 MiB                 | 0.019 s/iter               |\n| DLinear   | 0.329                                           | 0.246                                          | 0.166                      | 0.434                  | 1021 MiB                 | 0.003 s/iter               |\n| TimeMixer | **0.192**                                       | **0.222**                                      | **0.156**                  | **0.387**              | 1043 MiB                 | 0.007 s/iter               |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039087932,
                "cdate": 1700039087932,
                "tmdate": 1700039340760,
                "mdate": 1700039340760,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8M3JyD3aLr",
                "forum": "7oLshfEIC2",
                "replyto": "Fe0DSHOHHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThanks for your valuable and constructive review, which has inspired us to improve our paper further substantially. This is a kind reminder that it has been 3 days since we posted our rebuttal. Please let us know if our response has addressed your concerns. \n\nFollowing your suggestions, we have provided the following revisions to our paper:\n\n- **Elaborate the difference between TimeMixer and MICN, Autoformer** in using decomposition and multiscale analysis.\n- **Revise Figure 2** and text of the main text to resolve inaccurate formalizations.\n- **Revise the listed contributions in the $\\underline{\\text{Introduction section}}$** to highlight our contributions in proposing new mixing directions for decomposed components and utilizing multiple predictors for multiscale series.\n- **Revise the $\\underline{\\text{Future-Mixing section}}$** to clarify our design in Future-Multipredictor-Mixing block.\n- Highlight that although under hyperparameter searching, the relative promotion of TimeMixer is smaller than the unified hyperparameter setting, **TimeMixer is 2x faster than the second-best model PatchTST.**\n\nIn this paper, we propose the TimeMixer as a simple but effective model and provide extensive experiments, visualization, and ablations to support our insight. All the revisions are included in the $\\underline{\\text{revised paper}}$.\n\nSincere thanks for your dedication! We are looking forward to your reply."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303407078,
                "cdate": 1700303407078,
                "tmdate": 1700303407078,
                "mdate": 1700303407078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n9QAvrzYkD",
                "forum": "7oLshfEIC2",
                "replyto": "Fe0DSHOHHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The discussion period ending soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThanks again for your valuable and constructive review, which helps us revise our contribution and advantages to a clearer stage.\n\n**We kindly remind you that the reviewer-author discussion phase will end in 24 hours. After that, we may not have a chance to respond to your comments**.\n\nBesides, during the rebuttal period, we also completed the ablations in all 18 benchmarks (**more than 100 new experiments**), which may be helpful to you in further justifying our contribution in PDM and FMM. **All of these results have been included in the $\\underline{\\text{revised paper}}$.**\n\nSincere thanks for your dedication! We are looking forward to your feedback."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625547613,
                "cdate": 1700625547613,
                "tmdate": 1700625547613,
                "mdate": 1700625547613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6rH3cZ3Vjj",
                "forum": "7oLshfEIC2",
                "replyto": "n9QAvrzYkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_P2SD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission680/Reviewer_P2SD"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author's Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed rebuttal, which further clarify the value of the work and supplement richer experiments. Although I feel there is a little lack of innovation,  the experiments are solid enough and maybe this paper will inspire anyone else. The open-sourcing of the code will also benefit the time-series community. Therefore, I increase my score from 5 to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664430167,
                "cdate": 1700664430167,
                "tmdate": 1700664430167,
                "mdate": 1700664430167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]