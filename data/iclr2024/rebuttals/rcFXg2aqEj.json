[
    {
        "title": "LMDX: Language Model-based Document Information Extraction and Localization"
    },
    {
        "review": {
            "id": "KjO4YvfECS",
            "forum": "rcFXg2aqEj",
            "replyto": "rcFXg2aqEj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to extract information from visually rich documents with the information and their position. The method extracts information from the OCR'ed document by prompting the PaLM2-S LLM to perform completion tasks. With the LLM with entity extraction training, the model can achieve strong performance even with no training data, comparable to or better than a few baselines. With a few-shot setting, the method shows a high performance with a large margin compared to existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposes a novel method to document information extraction from visually rich documents using LLMs. \n- The absolute performance is greatly higher than existing methods, and some of the proposed enhancements are shown effective through the ablation study. \n- The paper is well-written with the details of algorithms, schemas, sample outputs, etc."
                },
                "weaknesses": {
                    "value": "- The model's performance highly depends on the off-the-shelf OCR and PaLM2-S large language model, but they are unavailable, so the results are not reproducible. Also, there is no detailed explanation or evaluation of these modules. \n- The authors mention the support of the hierarchical entity and entity localization, but their effect is not directly evaluated since there is no evaluation without them."
                },
                "questions": {
                    "value": "- Is there any performance assessment of the OCR model? \n- How does the model differ from baselines in, e.g., the number of parameters and runtime? Although the authors remark on using open-source LLMs as future work, how is it difficult to run the model with publicly accessible OCR and existing LLMs? \n- Did the authors try other prompts, schema, or target formats during the development? How are the current settings chosen?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No problem."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2034/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698284517643,
            "cdate": 1698284517643,
            "tmdate": 1699636135299,
            "mdate": 1699636135299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fyNEQW9rtG",
                "forum": "rcFXg2aqEj",
                "replyto": "KjO4YvfECS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5Y8U (Part 1)"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer 5Y8u for taking the time to read our paper in detail and giving valuable feedback! Please see our response below:\n\n> The model's performance highly depends on the off-the-shelf OCR and PaLM2-S large language model, but they are unavailable, so the results are not reproducible. Also, there is no detailed explanation or evaluation of these modules.\n\n**[Response]** We agree we did not make this clear in the submitted paper, but all our experiments use the OCR provided by the benchmark for all models and baselines. As such the reported performance improvements are entirely due to the model and LMDX methodology, and we can fairly compare it to the baselines. We will make this clearer in the next version of the paper.\n\n> The authors mention the support of the hierarchical entity and entity localization, but their effect is not directly evaluated since there is no evaluation without them.\n\n**[Response]** With regards to hierarchical entity evaluation, we believe this is directly evaluated: we highlight the F1 score on line_item entity (the hierarchical entity in VRDU) for all models and baselines in Table 2 and a provide a discussion for it in Section 3.3 *Performance on Hierarchical Entities*. The table below synthetizes this information and shows the line_item F1:\n\n| Dataset Size | 0     | 10    | 50    | 100   | 200   |\n|--------------|-------|-------|-------|-------|-------|\n| FormNet      | N/A   | 5.72  | 19.06 | 18.80 | 21.86 |\n| LayoutLM     | N/A   | 6.95  | 19.50 | 21.26 | 23.90 |\n| LayoutLMv2   | N/A   | 9.96  | 20.98 | 23.52 | 25.46 |\n| LayoutLMv3   | N/A   | 5.92  | 19.53 | 22.08 | 24.51 |\n| LMDX         | 21.21 | 39.35 | 65.42 | 69.77 | 72.09 |\n\nOverall, previous SOTA methods (LayoutLM / FormNet) plateau around ~20-25% F1 due to their reliance on heuristics that do not benefit from additional training data, unlike LMDX. Please let us know if there are more that would be useful to show with regards to hierarchical entity evaluation.\n\nWe also acknowledge that in the submitted paper version, entity localization is not directly/independently evaluated, and we thank reviewer 5Y8U for pointing it out. For all models and baselines that can do entity localization, on VRDU Registration Form and Ad-Buy Form Mixed, we've computed the localization accuracy, following the formula below:\n\n$$Accuracy_{Localization} = TotalEntityCount_{CorrectlyExtractedAndLocalized} \\div TotalEntityCount_{CorrectlyExtracted} $$\n\nSince LMDX localizes at the line level, localization verification is done at the line-level as well, i.e. localization is considered correct if the prediction bounding box is covered by the groundtruth line-level bounding box by more than 80%. See the results below.\n\nVRDU Registration Form Mixed:\n\n| Train Dataset Size / Model | LayoutLM | LayoutLMv2 | LayoutLMv3 | **LMDX (Ours)**   |\n|----------------------|----------|------------|------------|--------|\n| 0 doc                | N/A      | N/A        | N/A        | 93.21% |\n| 10 doc               |   98.71% |     99.00% |     99.20% | 99.75% |\n| 50 doc               |   99.69% |     99.54% |     99.39% | 99.87% |\n| 100 doc              |   99.63% |     99.72% |     99.72% | 99.92% |\n| 200 doc              |   99.69% |     99.75% |     99.67% | 99.87% |\n\nVRDU Ad-Buy Form Mixed:\n\n| Train Dataset Size / Model | LayoutLM | LayoutLMv2 | LayoutLMv3 | **LMDX (Ours)**   |\n|----------------------|----------|------------|------------|--------|\n| 0 doc                | N/A      | N/A        | N/A        | 88.18% |\n| 10 doc               |   92.60% |     93.95% |     90.68% | 94.51% |\n| 50 doc               |   95.24% |     95.64% |     95.28% | 98.28% |\n| 100 doc              |   95.09% |     95.72% |     95.88% | 98.69% |\n| 200 doc              |   95.38% |     95.78% |     95.95% | 98.65% |\n\nThis evaluates the localization quality independently of the extraction quality. We will update the next version of the paper to include those results.\n\n> Is there any performance assessment of the OCR model?\n\n**[Response]** In all our experiments, we simply use the publicly provided benchmark's OCR for all models and baselines, thus we do not explicitly evaluate the performance of the OCR model. This means that the quality improvements showcased come from the parsing model (powered by LMDX, FormNet, LayoutLMv1/2/3, etc) and not the OCR itself, leading to a fair comparison between LMDX and baselines.\n\nFor details regarding the OCR, the VRDU authors mention they used Google Vision OCR (per page 5 of [1]).\n\n[1] Wang, Zilong, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep Tata. \"Vrdu: A benchmark for visually-rich document understanding.\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 5184-5193. 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507487528,
                "cdate": 1700507487528,
                "tmdate": 1700507487528,
                "mdate": 1700507487528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cnKXXK4Dix",
                "forum": "rcFXg2aqEj",
                "replyto": "4fwo0gtddm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and responses. Since my concerns on the dependence of models without details remain, I will keep my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714462576,
                "cdate": 1700714462576,
                "tmdate": 1700714462576,
                "mdate": 1700714462576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B05L9EkvJG",
            "forum": "rcFXg2aqEj",
            "replyto": "rcFXg2aqEj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2034/Reviewer_W15R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2034/Reviewer_W15R"
            ],
            "content": {
                "summary": {
                    "value": "When applying LLM to Visually Rich Document (VRD) understanding, many methods use the two staged approaches: first execute the text recognition/serialization step and then execute the parsing step. However, lots of methods suffer from the need for large training data or are not able to predict hierarchical entities or hallucinations in domains other than text-only data. These problems are due to the absence of layout encoding within LLMs and the absence of a grounding mechanism ensuring the answer is not hallucinated. To overcome these challenges, the authors propose the five staged frameworks: OCR - chunking - prompt generation - LLM inference - decoding. The suggested framework is experimented with PaLM 2-S and compared to several publicly available baseline models on Visually Rich Document Understanding (VRDU) and Consolidated Receipt Dataset (CORD), resulting in a bigger performance margin than baseline methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Suggest reasonable methods to tackle the challenges of visual document understanding.\n- Provide rich information to reproduce experiments"
                },
                "weaknesses": {
                    "value": "* Though the suggested method seems agnostic to specific LLM, the authors experimented only with PaLM 2-s. To verify the superiority of the suggested framework, additional experiments using LLMs other than PaLM are needed (I think the additional experiment would enhance the presentation of the robustness of the proposed method)."
                },
                "questions": {
                    "value": "* In document representation, when generating prompts, how well do coordinate tokens work? Line-level segments with 2 coordinates are enough for various VRD data?\n* Schema representation is important in the perspective of getting information in VRD. However, it would be vulnerable to hallucination. Does LLM properly parse JSON format?\n* When doing Top-K sampling, we can choose Top-K sampling for individual N chunks and then merge, or do Top-K sampling for entire N chunks. I guess the latter method is better for the semantic integration quality (the similar reason that authors used the entire predicted tree value from a single LLM completion for hierarchical entities), but the authors used the former method. Is there a reason? I think the comparison may be interesting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Reviewer_W15R"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2034/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824294042,
            "cdate": 1698824294042,
            "tmdate": 1699636135224,
            "mdate": 1699636135224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "blKS274zH3",
                "forum": "rcFXg2aqEj",
                "replyto": "B05L9EkvJG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W15R (Part 1)"
                    },
                    "comment": {
                        "value": "Firstly, we would like to thank reviewer W15R for taking the time to review our paper and providing great feedback! Please see our response below:\n\n> Though the suggested method seems agnostic to specific LLM, the authors experimented only with PaLM 2-s. To verify the superiority of the suggested framework, additional experiments using LLMs other than PaLM are needed (I think the additional experiment would enhance the presentation of the robustness of the proposed method).\n\n**[Response]** We agree with the feedback that evaluation numbers with other LLMs would better show the generality of our method. What we meant by LMDX being generic to the LLM is that it simply requires a simple text-in, text-out interface, which all LLMs provide (no extra input/output, or reliance on specific tokenization scheme). We will rephrase this in the next version of our paper to make this clearer.\n\n> In document representation, when generating prompts, how well do coordinate tokens work? Line-level segments with 2 coordinates are enough for various VRD data?\n\n**[Response]** Coordinate tokens are very important to a high quality extraction: this is showcased in the ablation study atTable 4, where we replace the coordinate tokens by line index (See appendix A.6 for how the prompts look). This allows us to not communicate any layout information to the model, while still using LMDX prompts and localizing the entities. Overall, coordinate tokens improves micro-F1 by an absolute 12-15% across data regimes on VRDU Ad-Buy Form Mixed.\n\nLine-level segments with 2 coordinates are indeed enough based on our experiments. We\u2019ve tried with 4 coordinates, word-level and finer coordinate quantization buckets on an internal dataset which were all quality-neutral or negatives. We posit that this is due to the fact that those schemes add more coordinate tokens to the text sequence, which makes it further and further from the type of data the LLM was pretrained on. It becomes then harder for the LLM to correctly interpret the LMDX-style text sequences."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506569474,
                "cdate": 1700506569474,
                "tmdate": 1700506569474,
                "mdate": 1700506569474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mYU2MJAyl3",
            "forum": "rcFXg2aqEj",
            "replyto": "rcFXg2aqEj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes LMDX \u2014 a mechanism for information extraction from documents leveraging off-the-shelf Optical Character Recognition service and LLM prompt engineering approach with PALM LLM for processing the extracted information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n* The paper shows potential of using LLMs for information extraction from documents\n* Ablation studies are interesting and show the value of fine-tuning the PALM LLM for document information extraction"
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n* This paper proposes a mechanism for information extraction from documents leveraging off-the-shelf Optical Character Recognition service and complicated LLM prompt engineering approach for processing the extracted information. The main underlying assumption driving the complexity of the prompt engineering approach is limited context length of LLMs. However, models like Claude 2 are capable of working with 100K token context windows. Additionally, methods like RoPE scaling and other context length expansion approaches allow to increase the context size for other LLMs including open-source models. As there are effective ways to address the context length limitation, the presented prompt engineering approach is a somewhat incremental engineering contribution, especially given its complexity. The fine-tuned model, however, is of interest.\n* While the proposed approach outperforms other baselines on VRDU and CORD benchmarks, the performance advantage clearly comes from using a powerful LLM. It would be important to compare this method to OCR+long-context LLMs such as Claude 2.\n* Another reasonable baseline with the potential to achieve high performance on these benchmarks is a multi-modal vision-text LLM, for example GPT-4. It has potential to work out of the box, without requiring fine-tuning, and significantly outperform other baselines.\n* Code is not provided.\n* Many unexplained abbreviations: e.g., IOB, NER. Readers would benefit from expanding these abbreviations the first time they are used."
                },
                "questions": {
                    "value": "Questions:\n* In-context learning is likely to significantly improve performance on this task, have you tried any experiments with in-context demonstrations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no ethics concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2034/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698905360679,
            "cdate": 1698905360679,
            "tmdate": 1700549401827,
            "mdate": 1700549401827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PirPqYbds2",
                "forum": "rcFXg2aqEj",
                "replyto": "mYU2MJAyl3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2034/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2034/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Vt8a (Part 1)"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer Vt8a for taking the time to review the paper and give us insightful feedback! See our response below:\n\n> This paper proposes a mechanism for information extraction from documents leveraging off-the-shelf Optical Character Recognition service and complicated LLM prompt engineering approach for processing the extracted information. The main underlying assumption driving the complexity of the prompt engineering approach is limited context length of LLMs. However, models like Claude 2 are capable of working with 100K token context windows. Additionally, methods like RoPE scaling and other context length expansion approaches allow to increase the context size for other LLMs including open-source models. As there are effective ways to address the context length limitation, the presented prompt engineering approach is a somewhat incremental engineering contribution, especially given its complexity. The fine-tuned model, however, is of interest.\n\n**[Response]** To clarify, the complexity of the prompt engineering is not driven by a goal to reduce the number of tokens due to limited context length, but rather to communicate the layout information of the document to the model (necessary for a high-quality extraction on many visually-rich documents), and to derive the extracted entities\u2019 bounding boxes (a major requirement in document information extraction which we want our method to handle). Both are achieved through the coordinate tokens contained in the prompt.\n\nThe chunking stage in the pipeline (Section 2.3), which reduces the number of tokens sent in each prompt, is detailed so that ML practitioners can use the methodology on LLMs with context length limitations or on LLMs that can not use their full context well [1]. However, that stage can be entirely skipped for LLMs that supports long context and can use it effectively. We will ensure to clarify this in the next version of our paper. \n\nNonetheless, we do make an effort to reduce the number of tokens used by our method whenever it is quality neutral so that it can be used by ML practitioners at a lower cost and faster inference speed, but this is not the primary goal.\n\nOverall, we hope this clarifies details around the complexity of the prompt!\n\n[1] Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. \"Lost in the middle: How language models use long contexts.\" arXiv preprint arXiv:2307.03172 (2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506947709,
                "cdate": 1700506947709,
                "tmdate": 1700506947709,
                "mdate": 1700506947709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OUadXOWGBx",
                "forum": "rcFXg2aqEj",
                "replyto": "HzPIPFSTF4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your thorough response"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you very much for your excellent response and additional experiments! In particular, thank you for the clarification regarding the context length and highlighting the importance of your coordinate token approach (both through the explanation in your response and additional experiments on a vision-text LLM). Thank you for including the GPT-3.5 results as well -- they justify the complexity of the proposed method and alleviate my concerns about the potential to achieve a similar level of performance out-of-the-box. I also welcome your points about limited accessibility of Claude and GPT-4 and I agree that your experiments provide an excellent alternative. In light of the new experiments and clarifications, I am happy to recommend acceptance and raise my score.\n\nThank you!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2034/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549357392,
                "cdate": 1700549357392,
                "tmdate": 1700549357392,
                "mdate": 1700549357392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]