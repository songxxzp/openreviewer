[
    {
        "title": "Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders"
    },
    {
        "review": {
            "id": "aM2cV6swrY",
            "forum": "4zZFGliCl9",
            "replyto": "4zZFGliCl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_1CHE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_1CHE"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors analyze the issue of posterior mode collapse for conditional and hierarchical linear VAEs. They demonstrate conditions required to observe posterior collapse, which provides meaningful insight into the design and training of more complex VAEs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Technical derivations seem correct, and the paper methodology is solid.\n- Theoretical insights into the understanding and design of VAEs."
                },
                "weaknesses": {
                    "value": "The reviewer appreciates little contribution in the paper. Yes, the authors generalize the results obtained for the linear VAE by Wang & Ziying (2022) and Dai et al. 2020 to linear CVAEs and hierarchical linear VAES, and this contribution is acknowledged. However, I wonder if generalizing the analysis to these new models resulted in different conclusions or insights about avoiding posterior collapse. In other words, is anything in Table 1 significantly different from what we learned from Wang and Ziying? At the end, the conclusion is that both $\\beta$ and $\\eta_{dec}$ should be small ..."
                },
                "questions": {
                    "value": "Please clarify what new designing insights are obtained from your results that couldn't be guessed from other works in the literature on analytical understanding methods for VAEs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Reviewer_1CHE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705175330,
            "cdate": 1698705175330,
            "tmdate": 1700817610030,
            "mdate": 1700817610030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UKNvsyedDS",
                "forum": "4zZFGliCl9",
                "replyto": "aM2cV6swrY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1CHE - Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1CHE,\n\nThank you for your thoughtful review and valuable feedback. Below we address your concerns. We have also revised the current manuscript according to the suggestions of the reviewers. The changes are in blue color. Please kindly see the revised manuscript, as well as our General Response and Summary of Revision.\n\n**Q1**: The reviewer appreciates little contribution in the paper. Yes, the authors generalize the results obtained for the linear VAE by Wang & Ziyin (2022) and Dai et al. 2020 to linear CVAEs and hierarchical linear VAES, and this contribution is acknowledged.  However, I wonder if generalizing the analysis to these new models resulted in different conclusions or insights about avoiding posterior collapse. In other words, is anything in Table 1 significantly different from what we learned from Wang and Ziyin? At the end, the conclusion is that both $\\beta$  and $\\eta_{dec}$  should be small.\n\n**A1**: We respectfully disagree with the reviewer that our contributions are little. Please allow us to clarify our contributions below.\n\n1/ **Implications of our theoretical results for CVAE and MHVAE.** \n\nFor CVAE, our theoretical analysis is the first to prove the existence of posterior collapse in CVAE, an important and popularly used variant of VAE [1, 2, 3]. We rigorously study the effect of parameters to the rank of the models and the level of posterior collapse at optimality, including $\\theta$\u2019s (the singular values of matrix $Z = \\mathbb{E}(x \\tilde{x}^{\\top})$, defined at Theorem 2), $\\beta$ (scaling scalar of the KL term of ELBO at Equation (5) in our paper) and $\\eta_{dec}$ (magnitude of the variance of the generating distribution). The results imply that a sufficiently small $\\beta$ and/or $\\eta_{dec}$ can mitigate posterior collapse. Especially, we prove that **the correlation of the input condition and output of training data relates to the collapse level** (see the paragraph after Theorem 2 in our manuscript). This insight obviously cannot be derived from the standard VAE setting which does not incorporate input conditions into the generative process.     \n\nFor MHVAE, our theoretical analysis is again the first to prove the existence of posterior collapse in MHVAE, another important and popularly used variant of VAE [4, 5, 6, 7]. We also rigorously study the effect of parameters to the rank of the models and the level of posterior collapse at optimality, including $\\theta$\u2019s, $\\beta_1$, $\\beta_2$ and $\\eta_{dec}$. Moreover, since the ELBO loss of MHVAE consists of multiple KL-regularized terms, we study the effect of each KL-regularized term and the trade-offs between latents via the magnitude of the hyperparameters $\\beta_1$ and $\\beta_2$. We find that **decreasing $\\beta_2$ can help to alleviate posterior collapse and increase the rank of the model, while decreasing $\\beta_1$ will have the opposite effect**. This insight cannot be derived from the standard VAE setting which has only one latent and one KL-regularized term in the ELBO. In addition, we observe that **when any latent variable of MHVAE suffers complete collapse, its higher-level latents become useless**. Thus, we suggest creating additional mapping between the input data and every latent to prevent this adverse situation.\n\nFurthermore, we find that **unlearnable encoder variance can prevent posterior collapse** (see Section 3 in our paper) for VAE, CVAE and MHVAE. This insight is also novel. \n\n2/ **As per Reviewer vZoP comment, our paper \u201cprovides an important step forward for the understanding of the inner workings of VAEs and provides the basis and theoretical tool set for future research\u201d**. Specifically, current state-of-the-art VAE models are mostly CVAE and HVAE [1, 2, 3, 4, 5, 6, 7], not standard VAE with only one latent, so our settings are more practical compared with prior works and provide the basis and theoretical tool set for future research of CVAE and HVAE. Furthermore, as the discussion in Section 4.2 in our manuscript, diffusion models share many similarities with deep MHVAE model where the encoding process of diffusion models also consists of consecutive linear maps with injected noise and with unlearnable isotropic variance, and their training loss function is also ELBO. Therefore, our framework potentially sheds light on the understanding of diffusion models."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350061398,
                "cdate": 1700350061398,
                "tmdate": 1700350061398,
                "mdate": 1700350061398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uPkNhF1X1g",
                "forum": "4zZFGliCl9",
                "replyto": "6jUKVO7FhO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Reviewer_1CHE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Reviewer_1CHE"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks for you detail response. Upon reading all the answers, I am willing to admit that I probably overlooked all the paper contributions, which are also better emphasized in the revised version of the paper.  In this regard, I am willing to increase my rating of the paper once the rebuttal period finishes."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405329408,
                "cdate": 1700405329408,
                "tmdate": 1700405329408,
                "mdate": 1700405329408,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xZWICdRqGb",
            "forum": "4zZFGliCl9",
            "replyto": "4zZFGliCl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_fUP2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_fUP2"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the first theoretical analyses of posterior collapse in the conditional VAE and markovian hierarchical VAE. Both models contrast with standard VAEs, which have theoretical results related to posterior collapse, in that they have more complex latent structure. Mathematical analyses derive conditions under which we should see posterior collapse for both classes of models. Qualitative observations are extracted from the theorems. Experimental results on MNIST test and validate the qualitative observations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The VAE is a widely used, classic generative modeling framework. Theoretical results providing a better understanding of performance, and lack thereof, are interesting and valuable. \n- The theoretical results and qualitative observations provide satisfying and interesting insights into the behavior of the model. \n- The empirical experiments provide a nice, if modest, evaluation of the predictions."
                },
                "weaknesses": {
                    "value": "- The primary weakness was in exposition. The paper tends to discuss intuitions after theorems, which makes the theorems a bit challenging on first read. \n- Moreover, at several point the authors assume a lot of the reader. Specifically, consider the case of unlearnable Sigma, and comparisons between theorem 1 and the previous work (the result from which is not stated in the text). \nTaken together the paper was more difficult than need be to understand. \n\nMinor comments: \n\"Interestingly, we find that the correlation of the training input and training output is one of the factors that decides the collapse level\" Why is this interesting? \n- \"We study model with\" typo\n- \"beside the eigenvalue\" typo\n- \" i) we characterize the global solutions of linear VAE training problem for unlearnable \u03a3 case, which generalizes the result in (Wang & Ziyin, 2022) where only the unlearnable isotropic \u03a3 is considered, and ii) we prove that for the case of unlearnable \u03a3, even when the encoder matrix is low-rank, posterior collapse may not happen. Thus, learnable latent variance is among the causes of posterior collapse, opposite to the results in Wang & Ziyin (2022) that it is not the cause of posterior collapse.\" This warrants some explanation. Why is the conclusion opposite? \n- What does it mean for Sigma to be unlearnable? This isn't properly explained. \n- What is the meaning of Theorem 1? It isn't yet clear to me what we are proving or why? \n- \"We note that our results generalize Theorem 1 in (Wang & Ziyin, 2022) where \u03c3i\u2019s are all equal to a constant.\" It would really be helpful to the reader to present the previous result and explain clearly why. \n- Ok the explanation after the theorem is helpful. It would be preferable to help readers see what is coming ahead of time, too. \n- I am still unsure about what an unlearnable Sigma is."
                },
                "questions": {
                    "value": "My main questions are related to the limitations. I would like to hear how the authors would plan to revise to clarify and sharpen exposition."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709133269,
            "cdate": 1698709133269,
            "tmdate": 1699636801989,
            "mdate": 1699636801989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7qSizAcCaV",
                "forum": "4zZFGliCl9",
                "replyto": "xZWICdRqGb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fUP2 - Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer fUP2,\n\nThank you for your thoughtful review and valuable feedback. Below we address your concerns. We have also revised the current manuscript according to the suggestions of the reviewers. The changes are in blue color. Please kindly see the revised manuscript, as well as our General Response and Summary of Revision.\n\n**Q1**: The primary weakness was in exposition. The paper tends to discuss intuitions after theorems, which makes the theorems a bit challenging on first read. \n\n**Q2**: Ok the explanation after the theorem is helpful. It would be preferable to help readers see what is coming ahead of time, too.\n\n**A1 + A2**: Thanks for your suggestions. We have added the description before every theorem of what the incoming theorems are about and what they try to accomplish to facilitate the reading.\n\n**Q3**: Moreover, at several points the authors assume a lot of the reader. Specifically, consider the case of unlearnable Sigma, and comparisons between theorem 1 and the previous work (the result from which is not stated in the text). Taken together the paper was more difficult than need be to understand. \n\n**Q4**: \"We note that our results generalize Theorem 1 in (Wang & Ziyin, 2022) where \u03c3i\u2019s are all equal to a constant.\" It would really be helpful to the reader to present the previous result and explain clearly why.\n\n**A3 + A4**: Thank you for your valuable comment. We have revised the paper to clarify the previous results whenever we mention them. For the comparison between our Theorem 1 and previous results, we have updated the paper to describe explicitly the differences between our Theorem 1 and Proposition 2 in (Wang & Ziyin, 2022), please see the paragraphs before and after Theorem 1 in our manuscript. \n\nSpecifically, our Theorem 1 allows for arbitrary predefined values of $\\\\{ \\sigma_i \\\\}\\_{i=1}^{d_1}$, thus is more general than the Proposition 2 in (Wang & Ziyin, 2022) where $\\sigma_i$'s are all equal to a constant. Under broader settings, there are two notable points from Theorem 1 that has not been captured in the previous result of (Wang & Ziyin, 2022): i) at optimality, the singular matrix $\\mathbf{T}$ of the encoder map $\\mathbf{V}$ sorts the set $\\\\{ \\sigma_i \\\\}\\_{i=1}^{d_1}$ in non-decreasing order, and ii) singular values $\\omega\\_{i}$ of the decoder map $\\mathbf{U}$ and $\\lambda\\_{i}$ of the encoder map $\\mathbf{V}$ are calculated via the $i$-th smallest value $\\sigma^{\\prime}\\_{i}$ of the set $\\\\{ \\sigma_i \\\\}\\_{i=1}^{d_1}$, not necessarily the $i$-th element $\\sigma_i$.\n\n**Q5**: \"Interestingly, we find that the correlation of the training input and training output is one of the factors that decides the collapse level\" Why is this interesting? \nAnd some typos: \"We study model with\", \"beside the eigenvalue\"\n\n**A5**: Thanks for your suggestion and for pointing out the typos. We used the word \u201cInterestingly\u201d because we found that the finding was novel. In our revision, we removed this word. We have also fixed the typos that the reviewer mentioned.\n\n**Q6**: What is the meaning of Theorem 1? It isn't yet clear to me what we are proving or why?\n\n**Q7**: \u201cThus, learnable latent variance is among the causes of posterior collapse, opposite to the results in Wang & Ziyin (2022) that it is not the cause of posterior collapse.\" This warrants some explanation. Why is the conclusion opposite?\n\n**A6 + A7**: Our Theorem 1 characterizes the global minima of the ELBO training problem, which includes the parameters from the matrix $\\mathbf{U}$ (the decoder), $\\mathbf{W}$ (the encoder) as optimization variables. The encoder variance $\\mathbf{\\Sigma}$ is fixed/unlearnable in this setting. We prove that the optimal parameters $(\\mathbf{U}^{\\ast}, \\mathbf{W}^{\\ast})$ have their singular values can be calculated via the closed-form formula stated in Theorem 1 in our manuscript. Thus, the ranks of the encoder map and the decoder map depend on $\\theta$\u2019s (the singular values of the matrix $\\mathbf{Z} := \\mathbb{E}(x \\tilde{x}^{\\top})$ ), $\\beta$ (scaling factor of the KL term in ELBO in Eqn. (3)) and $\\eta_{dec}$ (the variance magnitude of the generating distribution $p(x | z)$). After Theorem 1, in Section 3, we discuss the conditions for posterior collapse to occur, including the rank of the encoder/decoder and learnability of the encoder variance."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348954313,
                "cdate": 1700348954313,
                "tmdate": 1700348954313,
                "mdate": 1700348954313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PXBTqfhxQi",
                "forum": "4zZFGliCl9",
                "replyto": "hHEuIbKHD7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Reviewer_fUP2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Reviewer_fUP2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669953092,
                "cdate": 1700669953092,
                "tmdate": 1700669953092,
                "mdate": 1700669953092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kSqEbwn70y",
            "forum": "4zZFGliCl9",
            "replyto": "4zZFGliCl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_VR5S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_VR5S"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a theoretical and empirical analysis of the problem of posterior collapse in two types of VAE, specifically conditional and hierarchical. Under a linear VAE setting they prove certain specific conditions as contributors to posterior collapse. They then conduct various experiments on MNIST, focusing in their empirical work on non-linear VAEs, and demonstrate that their theoretical results line up with the experimental findings of what settings lead to collapse."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper puts forward theoretical analysis of posterior collapse in previously unstudied models that are more complex than vanilla VAEs\n- The paper performs experiments that corroborate the theoretical claims, albeit in a more realistic nonlinear setting"
                },
                "weaknesses": {
                    "value": "- Much of the empirical results are relegated to the appendix with the paper itself being somewhat light on experiments. This isn\u2019t necessarily a bad thing but it would be nice to see more thorough experiments in the main paper, e.g. vanilla VAE, linear CVAE/MHVAE, learnable vs. unlearnable variance, just for comparison.\n- The theoretical results are generally restricted to highly specific scenarios, such as linear and two latent for MHVAE, and the setting of both variances being learnable in MHVAE is not considered. It would be nice to see more theoretical motivation for why these results might generalize to more realistic training settings, but instead that is largely directed towards prior work.\n- Even in the empirical setting the majority of models studied are restricted to fairly shallow networks. The results are entirely on a simple dataset, MNIST, and therefore it\u2019s hard to get a sense for how the experiments would behave in a more complex setting with deeper networks, modern architectures, and less standardized data.\n- One of the most noticeable absences is that this paper doesn\u2019t consider the effects of SGD and optimization strategies. The setting where global solutions are directly obtainable is not really representative of most real world VAEs which would involve deep nonlinear networks that are most likely to only reach a local optimum. While the experiments themselves do use Adam to train the models, there\u2019s not much analysis of how the optimization strategy interacts with other phenomena.\n- Many of these weaknesses could be levied against prior work in this space. For what it\u2019s worth this paper does go beyond those works in that they do rigorous theoretical analysis of more complex model types."
                },
                "questions": {
                    "value": "You claim that Wang & Ziyin 2022 arrived at the opposite conclusion as you regarding the role of learnable latent variance. Can you say more about why you would have found different results and what that means?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Reviewer_VR5S"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816882780,
            "cdate": 1698816882780,
            "tmdate": 1699636801867,
            "mdate": 1699636801867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "03NSJm7YSw",
                "forum": "4zZFGliCl9",
                "replyto": "kSqEbwn70y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VR5S - Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer VR5S,\n\nThank you for your thoughtful review and valuable feedback. Below we address your concerns. We have also revised the current manuscript according to the suggestions of the reviewers. The changes are in blue color. Please kindly see the revised manuscript, as well as our General Response and Summary of Revision.\n\n**Q1**:  Much of the empirical results are relegated to the appendix with the paper itself being somewhat light on experiments. This isn\u2019t necessarily a bad thing but it would be nice to see more thorough experiments in the main paper, e.g. vanilla VAE, linear CVAE/MHVAE, learnable vs. unlearnable variance, just for comparison.\n\n**A1**: Thank you for your comment. We have moved an additional experiment to the main paper, where we compare the level of posterior collapse for VAE with learnable and unlearnable encoder variance to validate our claim about the role of learnability encoder variance to posterior collapse (Section 3). Please kindly see Figure 2 in the revised manuscript. Due to the main text\u2019s space limit of 9 pages and our work consists of many theoretical results that require detailed explanation, we currently include other experimental results in the Appendix.\n\n**Q2**: The theoretical results are generally restricted to highly specific scenarios, such as linear and two latent for MHVAE, and the setting of both variances being learnable in MHVAE is not considered. It would be nice to see more theoretical motivation for why these results might generalize to more realistic training settings, but instead that is largely directed towards prior work.\n\n**A2**: We agree with the reviewer that our theoretical results focus on the settings of linear networks. However, the theoretical analysis of nonlinear networks is very challenging and, in fact, there has been no rigorous theory for deep nonlinear networks yet to the best of our knowledge. Even for the linear case, a complete theoretical understanding of the VAE setting is still lacking. We aim to fill this gap and use the insights from our work for deep linear VAEs to empirically examine and improve deep nonlinear VAEs: we have experiments at Figure 2 + Figure 3 in our main paper and more experiments on nonlinear networks in the Appendix that support our theoretical insights. \n\nWe also want to emphasize that **analyzing deep linear networks has been an important step in studying deep nonlinear networks**. [1, 2, 3, 4] show that the optimization of deep linear models exhibits similar properties to those of the optimization of deep nonlinear models. In practice, deep linear networks can help improve the training and performance of deep nonlinear networks [5, 6, 7]. Specifically, [5] empirically proves that linear overparameterization in nonlinear networks improves generalization on classification tasks (see Section 4 in [5]). In particular, [5] expands each linear layer into a succession of multiple linear layers and does not include any non-linearities in between, which results in a considerable increase in performance. [6] applies a similar strategy for compact networks, and their experiments show that training such expanded networks yields better results than training the original compact networks. We discuss these importances of studying linear networks in the Related Work section in Appendix B in our paper, which is also our theoretical motivation to study this setting.\n\n**Regarding the motivation for the settings of CVAE and MHVAE**, we note that current state-of-the-art VAE models are mostly deep HVAE and CVAE [8, 9, 10, 11, 12], not standard VAE with only one latent, so our settings are practical and we provide the basis and theoretical tool set for future research of CVAE and HVAE (as per Reviewer vZoP comment). Furthermore, as discussed in Section 4.2 in our paper, **diffusion models** share many similarities with MHVAE model where the encoding process of diffusion models also consists of **consecutive linear maps** with injected noise and **with unlearnable isotropic variances**, and their training loss function is also the ELBO. Therefore, our framework potentially sheds light on the understanding of diffusion models. \n\nWe note that MHVAE with two latents is cumbersome to study theoretically. The complexities added when increasing the number of latent variables from one (standard VAE) to two can be seen via the ELBO training problem of two models in our paper: $L_{VAE}$ in Eqn. (3) and $L_{HVAE}$ in page 6. Due to some technical challenges, we currently leave the setting of both learnable encoders as future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346732828,
                "cdate": 1700346732828,
                "tmdate": 1700346732828,
                "mdate": 1700346732828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wQr2fOpy21",
            "forum": "4zZFGliCl9",
            "replyto": "4zZFGliCl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_Bn1w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_Bn1w"
            ],
            "content": {
                "summary": {
                    "value": "This work extends previous theoretical work on the linear variational autoencoder (VAE), showing the conditions under which linear conditional VAEs (CVAEs) and Markovian hierarchical VAEs (MHVAEs) will exhibit posterior collapse. Empirical experiments are conducted that supports theoretical results in both linear and nonlinear cases."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **The results are potentially novel and can have interesting implications.** This work extends the previous results on more complex VAE models which are widely used in practice. This might give practical directions for people deploying these variant of VAEs that previous work did not cover.\n- **Experiments clearly support the main theoretical results.** Both linear and nonlinear models are trained for different parameters and subsets of data. The results very clearly support the theoretical predictions and shows evidence on how the linear results can be predictive of the general nonlinear case."
                },
                "weaknesses": {
                    "value": "- **The paper seems to claim a result that has already been proven.** Theorem 1 seems to be exactly identical to proposition 2 in (Wang & Ziying, 2022). Maybe the functional form of the result is identical but the authors have more general assumptions on $\\sigma_i$ (I doubt this after having scanned the proof in Wang & Ziying). In either case, the statement that this theorem is one of the novel contributions is false or at best misleading.\n- **It is unclear how much novelty there is in extending the proof to CVAEs and MHVAEs.** Since I did not follow the proof for theorem 2 and 3 closely, and I am also not familiar with the literature, I cannot be certain whether extending the existing results on linear VAEs to CVAEs and MHVAEs is methodologically novel. In particular, the expression for $\\omega^*$ in theorem 2 seems quite similar to that of theorem 1 (which I do not consider novel, see previous point), and it is unclear to me if the derivation follows almost identical strategies. I think it might be helpful if the authors can discuss why extending the proof to the CVAE and MHVAE cases are interesting and nontrivial. This might just be a minor concern as one can argue that the implications of the results might be the more important and interesting aspect of this work.\n- **Graphics are not very easy to read.** It is difficult to see how the results shown in the plots support the theoretical results. This is only a minor critique and can be easily improved by using a different coloring scheme or just rearranging the labels in a vertical list and/or remind the reader what the theoretical predictions are (larger $\\beta1$ and smaller $\\beta2$ is good, etc.)."
                },
                "questions": {
                    "value": "- Can you clarify on the novelty claim for theorem 1?\n- On the hierarchical VAE results, you mentioned that it is advisible to create separate maps between the input data and each level of hierarchy, which seems closely related to the ideas in (S\u00f8nderby et al., 2016; Zhao et al., 2017; Vahdat & Kautz, 2020). Maybe it will be helpful to discuss these models since the theoretical results here seem to provide motivation for them?\n\nReferences:\n\nCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Ladder variational autoencoders. Advances in neural information processing systems, 29, 2016.\n\nZhao, S., Song, J., & Ermon, S. (2017). Learning hierarchical features from generative models. arXiv preprint arXiv:1702.08396.\n\nArash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. Advances in neural information processing systems, 33:19667\u201319679, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6893/Reviewer_Bn1w",
                        "ICLR.cc/2024/Conference/Submission6893/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699171960818,
            "cdate": 1699171960818,
            "tmdate": 1700820608969,
            "mdate": 1700820608969,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8rHcuWgA9L",
                "forum": "4zZFGliCl9",
                "replyto": "wQr2fOpy21",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Bn1w - Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer Bn1w,\n\nThank you for your thoughtful review and valuable feedback. Below we address your concerns. We have also revised the current manuscript according to the suggestions of the reviewers. The changes are in blue color. Please kindly see the revised manuscript, as well as our General Response and Summary of Revision.\n\n**Q1**: The paper seems to claim a result that has already been proven. Theorem 1 seems to be exactly identical to proposition 2 in (Wang & Ziying, 2022). Maybe the functional form of the result is identical but the authors have more general assumptions on $\\sigma_i$. In either case, the statement that this theorem is one of the novel contributions is false or at best misleading.\n\n**Q2**: Can you clarify on the novelty claim for theorem 1?\n\n**A1 + A2**: Thank you for your comment. We agree with the reviewer that the word \u201cnovel\u201d may be inappropriate here. In our revision, we have changed the wording from \u201cOur novel contributions\u201d to \u201cOur contributions\u201d in Section 3. Please allow us to clarify the contributions of Theorem 1 in our manuscript and the differences with Proposition 2 in (Wang & Ziyin, 2022) as follows.\n\nFirst, compared to Proposition 2 in (Wang & Ziyin, 2022), our Theorem 1 characterizes the global solutions of standard linear VAE for unlearnable $\\mathbf{\\Sigma}$ with arbitrary elements $\\sigma_i$ on the diagonal while Proposition 2 and Theorem 1 in (Wang & Ziyin, 2022) studies the isotropic $\\mathbf{\\Sigma} = \\sigma^2 \\mathbf{I} $ case only. **Another new and interesting result from Theorem 1 in our manuscript is that we prove the left singular matrix $\\mathbf{T}$ of the encoder map $\\mathbf{V}$ is the sorting matrix that sorts the values of $\\mathbf{\\Sigma}$ in non-decreasing order**. This property of $\\mathbf{T}$ is one of the factors that decide whether posterior collapse happens or not, which has not been considered in (Wang & Ziyin, 2022).\n\nSecond, after Theorem 1 in (Wang & Ziyin, 2022), they claim that \u201coptimizable encoder variance is not essential to posterior collapse problem\u201d (Section 4) and \u201ca learnable (data-dependent or not) latent variance is not the cause of posterior collapse\u201d (Section 4.5). Moreover, in Section 4.2 of (Wang & Ziyin, 2022), after Theorem 1, under the setting of unlearnable isotropic variance $\\sigma_i = \\eta_{enc}, \\forall i$, they claim the existence of posterior collapse when the learned model is low-rank (i.e., some singular values $\\lambda$ and $\\theta$ are 0\u2019s). In particular, they argue \u201c...the learned model becomes low-rank. Namely, some of the dimensions collapse with the prior\u201d. **We observe that these claims are not totally correct**, and we explain in our manuscript why posterior collapse may not happen in the above unlearnable setting (see the paragraph after our Theorem 1). For convenience, we repeat the explanation below. \n\nWe first let the SVD of the encoder matrix $\\mathbf{V}$ (i.e, the encoder map) to be $\\mathbf{V} =  \\mathbf{T} \\Lambda \\mathbf{S}^{\\top}$ then at the global minimizer of the training problem, we prove in Theorem 1 that $\\mathbf{T}$ is the matrix that sorts the diagonal values of $\\Sigma$ in non-decreasing order. When $\\mathbf{V}$ is low-rank, $\\Lambda$ will have zero rows, then $\\Lambda \\mathbf{S}^{\\top}$ will have zero rows. However, in the case of isotropic $\\mathbf{\\Sigma} = \\eta_{enc}^{2} \\mathbf{I}$ (as considered in Theorem 1 in (Wang & Ziyin, 2022)), $\\mathbf{T}$ can be any orthonormal matrix (since $\\sigma_i$\u2019s are all equal and in non-decreasing order already), and thus, $\\mathbf{V} = \\mathbf{T} \\Lambda \\mathbf{S}^{\\top}$ may have no zero rows. Then, for any data $x$, the mean $\\mathbf{V} \\tilde{x}$ of the approximate posterior $q(z|x) =  \\mathcal{N} (\\mathbf{V} \\tilde{x}, \\Sigma)$ might have no zero component, and thus the **posterior may not collapse to the prior $\\mathcal{N}(0, \\eta^2_{enc})$**. On the other hand, if all values $\\sigma_i$\u2019s are distinct, Theorem 1 suggests that $\\mathbf{T}$ is a permutation matrix and hence, $\\mathbf{V}$ will have zero rows, which corresponds with the dimensions of latent $z$ that collapse.\n\nFor the case of **learnable** $\\mathbf{\\Sigma}$, low-rank condition leads to posterior collapse. This is because at optimality, the diagonality of $\\mathbf{\\Sigma}$ makes $\\mathbf{U}^{\\top} \\mathbf{U}$ diagonal. Therefore, $\\mathbf{U}$ has zero columns, and subsequently,  $\\mathbf{V}$ will have zero rows (see detailed explanation in the paragraph before Section 4 in our manuscript). Finally, the corresponding dimensions of $z = \\mathbf{V} \\tilde{x} + \\epsilon$ collapse to its prior $\\mathcal{N}(0, \\eta^2_{\\text{enc}})$. Thus, our findings suggest that the learnability of encoder variance plays an important role in the existence of posterior collapse. We have updated the manuscript to describe carefully the differences between our Theorem 1 and Proposition 2 in (Wang & Ziyin, 2022) in the paragraphs after Theorem 1 in our revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343559815,
                "cdate": 1700343559815,
                "tmdate": 1700343601760,
                "mdate": 1700343601760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ebz6aH8GKM",
            "forum": "4zZFGliCl9",
            "replyto": "4zZFGliCl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_vZoP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6893/Reviewer_vZoP"
            ],
            "content": {
                "summary": {
                    "value": "This work provides a theoretical analysis of posterior collapse in Variational Autoencoder (VAE) models. First the authors extend previous results for the linear VAE case (Variational Autoencoders with only a single linear layer as encoder and decoder) and provide a novel condition for posterior collapse. Then these results are extended and the authors provide conditions for posterior collapse for two further VAE models: the Conditional VAE (CVAE), and the Markovian Hierarchical VAE (MHVAE)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This work provides important theoretical insights into the conditions for posterior collapse in Variational Autoencoders. Although these conditions are only presented for the linear case, and for MHVAE the case that only one of the two encoder variances is learnable, this work provides an important step forward for the understanding of the inner workings of VAEs and provides the basis and theoretical tool set for future research. The empirical evaluation confirms that the theoretical insights gained for the single layer encoder and decoder case extend, at least empirically, to the nonlinear (multi layer) case."
                },
                "weaknesses": {
                    "value": "The conditions provided in the paper are only for the linear (single layer encoder and decoder) case, which of course limits the practical applicability of the framework. Yet, I believe that the work provides an important step forward in the understanding of VAEs and do not see a big drawback in this."
                },
                "questions": {
                    "value": "In Fig. 3 (b) the case for beta_1=1.0 and beta_2=1.0 has been left out. Just for completeness it might be interesting to visualize the result for this \"standard\" ELBO (beta_1=beta_2=1.0)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical concerns"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699635891706,
            "cdate": 1699635891706,
            "tmdate": 1699636801621,
            "mdate": 1699636801621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fhbjwSu6Hq",
                "forum": "4zZFGliCl9",
                "replyto": "ebz6aH8GKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6893/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vZoP"
                    },
                    "comment": {
                        "value": "Dear Reviewer vZoP,\n\nThank you for your thoughtful review and valuable feedback. Below we address your concerns. We have also revised our manuscript according to the suggestions of the reviewers. The changes are in blue color. Please kindly see the revised manuscript, as well as our General Response and Summary of Revision.\n\n**Q1**: The conditions provided in the paper are only for the linear (single layer encoder and decoder) case, which of course limits the practical applicability of the framework. Yet, I believe that the work provides an important step forward in the understanding of VAEs and do not see a big drawback in this.\n\n**A1**: Thank you for your comment and appreciation of our work. We agree with the reviewer that our theoretical results focus on the settings of linear networks. However, the theoretical analysis of nonlinear networks is very challenging and, in fact, there has been no rigorous theory for deep nonlinear networks yet to the best of our knowledge. Even for the linear case, the complete theoretical understanding in the VAE setting is still lacking. We aim to fill this gap and use the insights from our work for deep linear VAEs to empirically examine and improve the deep nonlinear VAEs: we have experiments at Figure 2 + Figure 3 in our main paper and more experiments on nonlinear networks in the Appendix that support our theoretical insights. Throughout this process, our frameworks and theorems play an important role as a starting point for further theoretical and empirical study of the nonlinear conditional VAEs, hierarchical VAEs or even diffusion models, potentially. \n\nWe also want to note that analyzing deep linear networks has been an important step in studying deep nonlinear networks. For example, using only linear regression, [1] can recover several phenomena observed in large-scale deep nonlinear networks, including the double descent phenomenon [2]. [3, 4, 5, 6] show that the optimization of deep linear models exhibits similar properties to those of the optimization of deep nonlinear models. In practice, deep linear networks can help improve the training and performance of deep nonlinear networks [7, 8, 9]. Specifically, [7] empirically proves that linear overparameterization in nonlinear networks improves generalization on classification tasks (see Section 4 in [7]). In particular, [7] expands each linear layer into a succession of multiple linear layers and does not include any non-linearities in between, which results in a considerable increase in performance. [8] applies a similar strategy for compact networks, and their experiments show that training such expanded networks yields better results than training the original compact networks. [9] shows that linear overparameterization, i.e., the use of a deep linear network in place of a classic linear model, induces on gradient descent a particular preconditioning scheme that can accelerate optimization. The preconditioning scheme that deep linear layers introduce can be interpreted as using momentum and adaptive learning rate.\n\n**Q2**: In Fig. 3 (b) the case for beta_1=1.0 and beta_2=1.0 has been left out. Just for completeness it might be interesting to visualize the result for this \"standard\" ELBO (beta_1=beta_2=1.0).\n\n**A2**: Thanks for your suggestion. We already have this setting in the paper. In particular, we conducted the experiment for either $\\beta_1$ or  $\\beta_2$ is 1 and included the results in Figure 4 on page 14 of the Appendix. We agree with the reviewer that this setting is standard, and we have moved it back to the main text in our revision.\n\n\n**References**:\n\n[1] Trevor Hastie et al. \u201cSurprises in High-Dimensional Ridgeless Least Squares Interpolation\u201d, Annals of statistics, 2022\n\n[2] Preetum Nakkiran et al. \u201cDeep Double Descent: Where Bigger Models and More Data Hurt\u201d, Journal of Statistical Mechanics, 2021\n\n[3] Andrew M. Saxe, James L. McClelland, Surya Ganguli. \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks\u201d, 2013\n\n[4] Kenji Kawaguchi. \u201cDeep Learning without Poor Local Minima\u201d, NeurIPS 2016\n\n[5] Moritz Hardt, Tengyu Ma. \u201cIdentity Matters in Deep Learning\u201d, ICLR 2017\n\n[6] Thomas Laurent, James von Brecht. \u201cDeep linear neural networks with arbitrary loss: All local minima are global\u201d, ICML 2018\n\n[7] Minyoung Huh et al. \u201cThe Low-Rank Simplicity Bias in Deep Networks\u201d, TMLR 2022\n\n[8] Shuxuan Guo, Jose M. Alvarez, Mathieu Salzmann. \u201cExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks\u201d, NeurIPS 2020\n\n[9] Sanjeev Arora, Nadav Cohen, Elad Hazan. \u201cOn the Optimization of Deep Networks: Implicit Acceleration by Overparameterization\u201d, ICML 2018"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6893/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342913155,
                "cdate": 1700342913155,
                "tmdate": 1700343006820,
                "mdate": 1700343006820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]