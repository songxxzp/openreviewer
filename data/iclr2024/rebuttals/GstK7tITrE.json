[
    {
        "title": "AniHead: Efficient and Animatable 3D Head Avatars Generation"
    },
    {
        "review": {
            "id": "i21EiSTZKY",
            "forum": "GstK7tITrE",
            "replyto": "GstK7tITrE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of text-guided 3D face generation and proposes a method to achieve 3D head generation with one feed-forward pass without test-time optimization.  Here 3D parametric head model and texture maps are used to represent 3D heads. This work first generates texture maps and shape parameters based on text prompts by optimizing the 3D head parameters and texture maps using standard SDS loss. In this way, a set of samples with text and corresponding 3D heads are generated. These samples are then used to train models to directly predict the 3D head representation from text. Here, the shape parameters are predicted by an MLP with CLIP text embedding as input, and the texture maps are generated by fine-tuning a stable diffusion model. This method achieves a better CLIP score and faster inference compared with prior art."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n\n- The proposed method is technically sound. Most design choices are well-motivated."
                },
                "weaknesses": {
                    "value": "- The baseline DreamFace seems to generate higher-quality results. Also, DreamFace considers 4K resolution texture maps while the proposed method only considers 256x256. \n\n- All generations have the same skin color, e.g. Mark Zuckerberg and Morgen Freeman in Figure 8. \n\nThis paper\u2019s results are qualitatively not as impressive as its baseline DreamFace,  with significantly lower resolution and skin color variation. I also have concerns regarding the fact that the training data selection step."
                },
                "questions": {
                    "value": "Overall, this paper\u2019s results are qualitatively not as impressive as its baseline DreamFace, with significantly lower resolution and skin color variation. I also have concerns regarding the fact that the training data selection step. My questions include:\n\n- Can the proposed method be used for 4K generation or is there any fundamental limitation?\n- Why do the generations have very limited skin color variations? \n- The authors mention that they use SDS optimization to obtain 600 samples while selecting only 50 samples for training to \u201censure a balance of gender, ethnicity, and age\u201d. Why selection is needed, and why not just balancing the input text prompts? Also, 50 training samples sound very limited. Why not use more samples?\n- How does the proposed method compare with the SDS optimization pipeline which is used for training data generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1766/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698606746061,
            "cdate": 1698606746061,
            "tmdate": 1699636105902,
            "mdate": 1699636105902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "in0EBOYwJ4",
                "forum": "GstK7tITrE",
                "replyto": "i21EiSTZKY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3kTT"
                    },
                    "comment": {
                        "value": "Thank you for your valueable comments. Please refer to the general response for information about 4K resolution results. Here are the answers to the other questions:\n\n__Q1: skin color__\n\n__A1__: Besides texture resolution, reviewers are also concerned about the skin color generated by our method. \nThis issue is partly caused by the lighting settings in rendering software, such as Blender. We have updated the relevant images in the main paper and the appendix to better highlight the skin color variations.\n\n__Q2: Why selection of training examples is needed__\n\n__A2__: We use only 50 samples because, generally, LoRA training requires just a few dozen samples. Besides, due to the inductive bias of SD, the generated data by SDS is not very fair in terms of gender, ethnicity, and age. Therefore we perform a simple selection from 600 sample to form our training set. Moreover, we empirically find that using more sample for LoRA finetuning not only leads to longer finetuning time, but also results in overfitting problem. The finetuned model will concentrate on generating normal-looking humans, while having poor generation of contents that are not part of the dataset and require large areas of color blocks, such as The Joker and Batman.\n\n\n__Q3: Comparison with SDS__\n\n__A3__: We have updated the results in our appendix. As shown in Fig.10 in the appendix, the data generated by SDS does contain some blurry samples, but the final generated results are clearer, thus showing the effectiveness and necessity of our method."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302177831,
                "cdate": 1700302177831,
                "tmdate": 1700302177831,
                "mdate": 1700302177831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2feJZmvON9",
                "forum": "GstK7tITrE",
                "replyto": "in0EBOYwJ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_3kTT"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer 3kTT"
                    },
                    "comment": {
                        "value": "Thank authors for their efforts in addressing my concerns. However, the updated 4K textures do not seem to capture high-resolution details, e.g. in Figure 3, the proposed method generates less details than DreamFace. Regarding skin color variation, although the authors provide generations with other skin colors, these colors seem to be 4 discrete options (in Figure 8 and 9) lacking fine-grained variations."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678902907,
                "cdate": 1700678902907,
                "tmdate": 1700678902907,
                "mdate": 1700678902907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hIvb9h3f2K",
            "forum": "GstK7tITrE",
            "replyto": "GstK7tITrE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1766/Reviewer_ggv2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1766/Reviewer_ggv2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach for text-guided 3D animatable head avatar generation where a 3D avatar with desired facial characteristics is generated based on input textual prompts. It draws inspiration from recent works on diffusion-based text-to-3D approaches such as DreamFusion. The authors propose learning shape parameters of a FLAME-based 3D head model using a pretrained CLIP text encoder. A pretrained Latent Diffusion model is fine-tuned using an additional mean-texture token for generalized learning of the facial texture. The proposed method adopts SDS technique to generate training data for training the shape and texture generator. The main contribution is the reduction of inference time complexity for 3D avatar generation. The proposed method also does not require 3D annotated data for training. Qualitative and quantitative comparison results are presented with state-of-the-art methods on text-to-3D methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe proposed method claims the lowest test time complexity among existing test-to-3D models that can generate 3D faces. There is substantial reduction in inference time (1 min) compared to the most efficient method DreamFace, which takes around 5 mins for the optimization. However the texture resolution is lower than DreamFace.\n2.\tQualitative results denote decent quality of 3d faces.  The reconstructed avatar of celebrity faces are bearing resemblance to the real people.\n3.\tThe method supports specific prompts for tasks such as generating special characters (animation) and editing shape and style."
                },
                "weaknesses": {
                    "value": "\u2022\tNovelty and Significance of contributions:\nThe novelty of the proposed method appears slightly limited. Similar to DreamFace, pretrained CLIP and LDM models are used in the avatar generation with independent geometry(shape) and texture generators. The idea of using a mean texture token is novel, but similar to ideas have been explored in the form of pre-defined identity token in DreamBooth, and domain-specific prompt tuning in DreamFace, Introduction point 2 mentions challenges in animation due to implicit representations as limitation of existing text-to-3D methods. However DreamFace uses the ICT-FaceKit face model that can be integrated with existing animation pipelines, so the benefit obtained from using FLAME model in the current work is not clear. DreamFace also additional benefits of hair selection and video-driven animation generation. It is not evident from the paper how much the state-of-the-art in text-guided-3D face avatar generation will be advanced by the proposed method. Although the paper claims reduction in inference time, there are doubts about the generalization ability of the method in generating arbitrary high-fidelity avatars of varying age, skin colours etc, given that the training data consists of manually selected 50 training samples generated by SDS optimization\nWriting Issues:  \n\u2022\tUnclear writing: \no\t(Page 2) \u201cneed for a cumbersome two-stage generation process\u201d \u2013 what two-stage generation process.\no\t(Page 2) \u201cmeticulously crafted to encapsulate essential human head texture information.\u201d - How\no\t(Page 3) \u201cwe further propose other specific design to generate high-quality animatable 3D head avatars\u201d \u2013 what designs.\no\t\u201cthe renewed text prompts can contribute to fine-grained personalized characteristics with high fidelity of identity\u201d- doesn\u2019t make sense\no\t\u201ccommon texture-wise features shared by human beings.\u201d\no\tEpsilon is not defined in Equation 1.\n \u2022\tTypos: \n\t\u201cour propose generalized shape\u201d  in Page 5\n         Equation 2 \\phi() needs to be replaced by e_\\phi()\n\n\u2022\tMissing citations : \no\t\u201cExisting methodologies [??] typically leverage SDS\u201d \no\t \u201cremarkable strides achieved in diffusion-based text-to-3D models [??]\u201d\no\t\u201cWhile these [??] SDS-based approaches\u201d\no\t\u201cLeveraging readily available off the-shelf models [??]\u201d\no\t\u201c[Articulated Diffusion]\u201d\n\nExperimental Results:\n\u2022\t3D view (other than frontal) should have been included similar to existing works such as DreamFace. In the absence of a supplementary video it is hard to assess the qualitative results.\n\u2022\tUser Study needed to assess the perceptual quality of the generated results.\n\u2022\tMore detailed ablation study should be presented, the significance of the mean texture token should be justified using quantitative metrics.\n\u2022\tSome failure cases should be present to illustrate limitations of the method."
                },
                "questions": {
                    "value": "1.\tThe description \u201cdata-free\u201d strategy appears ambiguous as it also mentioned that SDS is used to generate training data. More clarity is needed on the training strategy in Section 3.3. Is a pretrained stable diffusion model being finetuned for the training data preparation?  What kind of candidate text prompts are used for the geometry and UV texture generation (few examples) How is it ensured that the \u201ctraining data\u201d generated using SDS sufficiently accurate for generalized performance at inference time.\n2.\tWhich pre-trained LDM models are used for finetuning?\n3.\tThe significance of the mean-texture token is not clear from the results. How is the mean-texture token prompt obtained at test time?  \n4.\tIs the mean-texture token sufficient to finetune a pretrained LDM (trained on diverse images) to the specific task of the Face UV texture generation. How is it ensured that the generated texture is consistent with face geometry? In the absence of UV texture ground truth to finetune pretrained LDM for texture how is the accuracy ensured at inference time?\n5.\tThe paper mentions \u201cwe set this parameter to a relatively low value and obtain more realistic, real-life outcomes.\u201d Is there any ablation done on the guidance scale parameter to justify this statement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1766/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772281266,
            "cdate": 1698772281266,
            "tmdate": 1699636105829,
            "mdate": 1699636105829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S9Xgsk98V2",
                "forum": "GstK7tITrE",
                "replyto": "hIvb9h3f2K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ggv2"
                    },
                    "comment": {
                        "value": "We thank reviewer ggv2 for the valuable time and constructive feedback.\n\n__Q1: Novelty in terms of pretrained CLIP and LDM__\n\n__A1__: Thank you. We would like to stress that using pretrained foundation models like CLIP and LDM is common in the current vision tasks due to their strong capability. We have not listed such techniques as our contribution. Instead, as mentioned in the general response, our novel pipeline for 3D head avatar generation along with the data-free training strategy and the usage of mean texture token for representing general human facial details are the key contribuion of our paper.\n\n__Q2: Mean texture token v.s. DreamBooth and domain-specific prompt tuning__\n\n__A2__: We have explained the difference between our mean texture token and identifier in DreamBooth in Sec.3.2. We quote the original content here: \"It is noteworthy that while the proposed mean-texture token is similar to the unique subject identifier utilized in DreamBooth, the actual effect is indeed different. The identifier in DreamBooth is aimed to represent a specific subject. In contrast, our mean-texture token helps complement text prompts by representing common texture-wise features shared by human beings.\" Moreover, our method differs from DreamBooth in implementation. DreamBooth finetunes the entire large model and requires a prior preservation loss. In contrast, our approach employs the LoRA method for training, which adds low-rank layers to the freeze original Stable Diffusion, reducing overfitting and training consumption due to fewer parameters.\n\nAs for the domain-specific prompt tuning in DreamFace, we think it has essential difference with our mean texture token. The domain-specific prompt tuning was proposed to help model identify the unwanted domain data resulted from the collection. In short, the method is used for data filtering. On the contrary, our mean texture token is proposed to represent the prior knowledge of human face textures, as mentioned in Sec.3.2.\n\n__Q3: ICT-FaceKit v.s. FLAME__\n\n__A3__: Thank you. The advantage of FLAME introduced in our paper is in comparison to latent representation methods like Nerf and DMTet. With this linear shape parametric model, we can easily control the shape using 100-dimensional parameters and easily integrate with a CG pipeline to achieve downstream tasks, such as video-driven animation.\n\nBoth our FLAME model and DreamFace's ICT-FaceKit belong to the 3DMM parametric model category and share the advantages mentioned above. The differences lie in:\n\n(1) The texture coverage of ICT-FaceKit does not include the entire face, covering only the ears and the frontal face without eyes, while our FLAME model's texture covers the entire head, which can be seen in the videos in our supplementary material.\n\n(2) Our usage of the 3DMM model also differs from DreamFace. DreamFace samples shape parameters from a multivariate normal distribution N(0,1) to obtain one million candidates, followed by 300 carving steps. In contrast, we design the AniHead model pipeline to directly learn shape parameters, inferring the shape parameter vector directly from the prompt. This not only results in higher shape-text matching (as shown in Tab.1 and Fig.3) but also reduces inference time (ours can generate shape within seconds).\n\n__Q4: more generation results demonstrating varying age, skin colors__\n\n__A4__: As both our method and DreamFace use parametric models as shape model, they can share a CG pipeline to achieve effects such as hair selection and video-driven animation, as demonstrated in the gif in Fig.5. We have updated Fig.4 in the main paper to demonstrate the editing capabilities for age and skin colors. As can be seen from the results, our model is versatile to multiple types of editing on different people.\n\n__Q5: Unclear writing and missing citations__\n\n__A5__: Thanks for pointing out. We have updated these problems in the new version.\n\n__Q6: 3D view__\n\n__A6__: We have provided the required videos in the supplementary material. The results are consistent with the original results in our paper, supporting the efficacy of our method.\n\n__Q7: data-free training strategy__\n\n__A7__: Thank you. We would like to kindly point out that 'data-free' refers to the fact that manual data collection is not required. In the SDS generation process, the only preparation needed is providing a prompt, such as \"a portrait of {name} with neck and shoulder, without hair\", which can be easily created. Moreover, the generalization ability has been shown in results in Fig.4. Given limited training data generated by SDS, these results further prove the effectiveness of our proposed pipeline.\n\nWe would like to clarify that the Stable Diffusion model is freeze during the SDS optimization process, because SDS only needs to leverage the prior information from Stable Diffusion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301993873,
                "cdate": 1700301993873,
                "tmdate": 1700302029252,
                "mdate": 1700302029252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MUc7hlAND4",
                "forum": "GstK7tITrE",
                "replyto": "mOBpt2cLNA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_ggv2"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_ggv2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response to the questions. It seems that the qualitative results in Fig. 3 in the revised submission have been updated with 4K texture. It appears that the results reported in the original submission had better texture quality in the original resolution. The 4K texture has noticeable artifacts in (e.g mouth) in Figs. 3 and 4."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674290309,
                "cdate": 1700674290309,
                "tmdate": 1700674290309,
                "mdate": 1700674290309,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZMHWpYLP47",
            "forum": "GstK7tITrE",
            "replyto": "GstK7tITrE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1766/Reviewer_9Wmk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1766/Reviewer_9Wmk"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a comprehensive pipeline for the generation of 3D heads. Their approach begins with the application of a Score Distillation Sampling (SDS) technique to create training data for FLAME-based models. Subsequently, they employ this paired dataset to train generators for both shape and texture. To evaluate the efficacy and efficiency of their method compared to baseline techniques, the authors conducted a series of experiments, the results of which are presented in the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This innovative method presents a unique pipeline for text-to-3D head generation that distinguishes itself in several ways. Notably, it does not rely on annotated datasets for training, making it exceptionally versatile. Additionally, the utilization of FLAME as the 3D representation in this method contributes to faster inference times, setting it apart from other baseline approaches."
                },
                "weaknesses": {
                    "value": "My apprehension revolves around the generative quality constrained by the use of FLAME. It appears that the resulting shape and texture may fall short of the realism achieved by DMTet-based or Nerf-based methods. Moreover, there seems to be a limitation in the ability to synthesize 3D hair components.\n\nFurthermore, it's worth noting that the methods employed in this approach draw heavily from existing techniques. For instance, the process of generating the training dataset bears a resemblance to DreamFusion, albeit with the incorporation of the FLAME representation."
                },
                "questions": {
                    "value": "I'd like to pose two questions:\n\nIn Figure 4, I'm curious about how the model manages to synthesize 3D hair for \"Taylor Swift.\" It seems like a noteworthy achievement, and I'm interested in understanding the underlying techniques.\n\nIn Figure 2, during the training data preparation stage, there appears to be a differentiation in the input for Stable diffusion, involving both shader images and textured images. I'd like clarification on the purpose of these distinct inputs for various steps and how they relate to the rendering equation and the overall model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1766/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815377802,
            "cdate": 1698815377802,
            "tmdate": 1699636105746,
            "mdate": 1699636105746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fBbUg5f3Zg",
                "forum": "GstK7tITrE",
                "replyto": "ZMHWpYLP47",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Wmk"
                    },
                    "comment": {
                        "value": "We thank reviewer 9Wmk for the valuable time and constructive feedback.\n\n__Q1: synthesize 3D hair for \"Taylor Swift.\"__\n\n__A1__: Thank you for your comment. As mentioned in Sec 3.2, the addition of accessories such as hats and hair are solved by post-processing steps. This procedure is similar to DreamFace, where candidate hair is selected and added to the generated avatars. Specifically, we put the generated 3D head into Blender and add a default hair accessory for it. We have updated the caption of Fig.1 for better understanding.\n\n__Q2: difference in the input for SD during data preparation__\n\n__A2__: We utilize SDS to generate both shape and texture training data. Originally, SDS optimizes latent 3D parameters like NeRF for generation. Since we want to generate decoupled shape and texture instead of single 3D shape, we have to optimize two sets of parameters controling shape and texture respectively. Specifically, shape paramter $S\\in R^{1 \\times 100}$\n is optimized with SDS. $S$ can be mapped to 3D head shape by fixing the pose and expression parameters to 0 in the FLAME model. Formally, $\n T, M_{normal} = FLAME(S)$, where $T$ is mesh shape and $M_{normal}$ is the according normal map.\n Then, we use a differentiable renderer $R$ with a random camera pose $C$ to obtain the shader image $I = R(T, C, M_{normal})$. We then encode $I$ into the latent space to obtain $z_t$, and we use the SDS loss to get the optimized shape $S'$.\n\nFor texture, we fix the shape parameters $\\hat{S} = S'$ and set the parameter to be optimized as texture map $M_{\\text{tex}}$, which is initialized as the mean texture for four different skin colors. Similarly, we obtain $I = R(T, C, M_{\\text{tex}})$, which reflects the effect of mapping the texture map to be optimized onto the 3D shape.\n\nWe have updated this detail in the appendix.\n\n\n__Q3: quality constrained by the use of FLAME__\n\n__A3__: It is true that FLAME may have limitations in representing certain accessories such as hair or hats. However, using a parametric model has its advantages:\n\n(1) The parametric model incorporates a human head prior, providing constraints that enable the generation of more reasonable results, while implicit representation methods struggle or fail to generate reasonable results (as shown in Fig.3). Both shape and texture are more controlable and reasonable, which had also mentioned in our overall respose.\n\n(2) Our approach is animatable, allowing for easy integration with off-the-shelf driving methods to create animations, as demonstrated in the gif in Fig.5. In contrast, implicit methods, although capable of representing hair, are not animatable and lack downstream applications.\n\n(3) For static 3D head avatars, adding hairs by post-processing, as adopted by us and DreamFace, would not lead to significant performance gap with latent-based methods.\n\nWe have updated the discussion in our appendix."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301849374,
                "cdate": 1700301849374,
                "tmdate": 1700301849374,
                "mdate": 1700301849374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bvbj1uFvaH",
                "forum": "GstK7tITrE",
                "replyto": "fBbUg5f3Zg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_9Wmk"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1766/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1766/Reviewer_9Wmk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1766/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631910944,
                "cdate": 1700631910944,
                "tmdate": 1700631910944,
                "mdate": 1700631910944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]