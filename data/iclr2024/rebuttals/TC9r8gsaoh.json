[
    {
        "title": "Nuisance-Robust Weighting Network for End-to-End Causal Effect Estimation"
    },
    {
        "review": {
            "id": "fO5tgl1BP7",
            "forum": "TC9r8gsaoh",
            "replyto": "TC9r8gsaoh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8811/Reviewer_KLj8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8811/Reviewer_KLj8"
            ],
            "content": {
                "summary": {
                    "value": "The research introduces the Nuisance-Robust Transformed Outcome Regression Network (NuNet) within a standard causal inference framework, which aims to discern between factual and counterfactual potential outcomes using observational data. In empirical tests, many established methods showcased an inclination towards optimism particularly under conditions of noise heterogeneity. NuNet distinguishes itself by merging nuisance estimation and target estimation into a singular step, guided by the pessimism principle. The primary goal is to pinpoint a potential outcome function to determine the causal e\ufb00ect of a treatment action. Accuracy is gauged through the PEHE, contingent upon three foundational assumptions: the Stable Unit Treatment Value, unconfoundedness, and overlap.\n\nEmpirical tests claim that NuNet oLen surpasses or parallels baseline plug-in methods, particularly in diverse noise seMngs and real-world datasets. However, it faces challenges with techniques prioritizing joint optimization. Inspired by pessimism in o\ufb04ine reinforcement learning, this causal inference method o\ufb00ers a di\ufb00erent approach. Most conventional techniques adopt a plug-in estimation, but this may show sub-optimality if nuisance accuracy isn't accounted for. The signi\ufb01cance of addressing the gap between optimistic and pessimistic errors is emphasized, leading to adherence to the pessimism principle. The study also delves into various methods used for estimating the CATE. Notable methods include Transformed Outcome Regression, PWNet, Doubly Robust Learner (DRNet), and NuNet. These strategies aim to enhance robustness against nuisances and unobserved confounders, ultimately aiming for reliable and accurate estimations. For future exploration, the authors suggested constructing a pessimism-based theoretical structure and delving into principled learning avenues could propel the evolution of causal inference methodologies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Clear mention of shortcomings of the conventional statistical and end-to-end adversarial learning networks.\n2. Evaluation of robustness of the approach under conditions including heterogeneous noise, AN setting, MN setting and real-world datasets."
                },
                "weaknesses": {
                    "value": "Considerable di\ufb00erence between the results of NuNet and SNet (best performing) pertaining to PEHE on the additive noise dataset."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8811/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699072854164,
            "cdate": 1699072854164,
            "tmdate": 1699637108019,
            "mdate": 1699637108019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GDQhxTusaj",
                "forum": "TC9r8gsaoh",
                "replyto": "fO5tgl1BP7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer KLj8, many thanks for your positive feedback and for concerning the empirical comparison.\n\n## Q1. Considerable di\ufb00erence between the results of NuNet and SNet (best performing) pertaining to PEHE on the additive noise dataset.\n\nCompared to SNet, our base model DRNet is also outpaced in the AN setting (NuNet generally outperforms DRNet).\nSNet has a special network architecture designed for CATE estimation (shared representation for $f$s and $\\mu$), which may serve as an effective inductive bias.\nThe exploration of architecture is an important aspect that is orthogonal to our considerations.\nCombining such an architecture is an important topic for future work as we wrote in the conclusion.\n\n> On the other hand, it cannot be applied to methods that are originally\nformulated as joint optimization of models and weights, such as representation decomposition. Such\nmodels that have representation shared by the model and weights often exhibit effective inductive\nbias, which is another aspect of the combination of the weighting approach and DNNs. Deriving\na pessimism-based theoretical framework for such methods and investigating principled learning\nmethods would be a promising future direction for versatile causal inference methods.\n\nAgain, thank you for your time and enthusiasm in reading our paper and providing feedback."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078440632,
                "cdate": 1700078440632,
                "tmdate": 1700078440632,
                "mdate": 1700078440632,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WVO73ovz82",
            "forum": "TC9r8gsaoh",
            "replyto": "TC9r8gsaoh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8811/Reviewer_S5uy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8811/Reviewer_S5uy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for estimating CATE. In contrast to previous two-stage estimators, the propensity score is simultaneously optimized with the second-stage regression. This is done in an adversarial manner to ensure robustness regarding estimation errors in the propensity score. The method is evaluated using simulated an real-world data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written.\n- CATE estimation is an important problem, with applications in various domains.\n- The proposed method performs well empirically."
                },
                "weaknesses": {
                    "value": "- I am not convinced of the advantages of the proposed method. The robustness of CATE estimation with respect to estimated nuisance functions is central to recent well-established works on CATE estimation that make use of semiparametric estimation theory, e.g., by Chernozhukov et al. (2018), Foster and Syrgkanis (2019), Nie and Wager (2021), Kennedy (2023). One of the key results is that CATE estimators with Neyman orthogonal loss functions (e.g., DR-learner, R-learner) are robust with respect to estimation errors of nuisance parameters (response surfaces, propensity score) in the sense of a fast guaranteed convergence rate. It is not clear to me that the proposed adversarial end-to-end approach improves on that. The proposed method already uses the Neyman orthogonal loss of the DR learner which makes the adversarial approach of optimizing for a pessimistic propensity score seem redundant.\n- Despite being central to the topic of the paper, three of the four works mentioned above are not cited in the paper. Neither is the R-learner considered as a baseline.\n- Furthermore, I do not understand why the proposed approach only performs adversarial learning w.r.t. the propensity score in combination with the doubly robust loss. Why not also for the response surfaces? Only accounting for the estimation errors in one nuisance parameter while ignoring the others seems arbitrary.  \n- A property of the DR loss is that only requires **either** the propensity score **or** the response surfaces to be estimated correctly to achieve a fast convergence rate (Kennedy, 2023). Again, this would make the proposed approach redundant if the response surface estimators converge sufficiently fast.\n- While the method performs well in the experiments, the datasets seem to favor methods that focus on response function estimation rather than propensity score. PW-Net has a huge variance and the estimation error seems to grow with sample size for some reason. I suspect that there might be possible overlap violations in the simulated data. I could imagine that the proposed method offers some advantages in dealing with overlap violations, which might lead to an alternative way to frame the paper. However, this would require additional intuition and experiments.\n- In summary, I think the problem of CATE estimation (or more generally, statistical estimation with nuisance parameters) is already quite well understood regarding the robustness to nuisance errors. I am not convinced that the proposed approach adds much benefit to the existing state-of-the-art.\n\nMinor points\n\n- The introduction puts a lot of emphasis on the CATE literature on representation learning. Personally, I do not think this literature stream is very relevant to the paper as it does not focus on representation learning, but CATE estimation as a statistical estimation problem with nuisance components. The same holds for the related work.\n- The related work on CATE could be expanded. \n- There are existing works on ATE estimation that estimate nuisance functions and ATE in an end-to-end manner (Shi et al. 2019, Frauen et al. 2023) which should be mentioned in the related work. However, these works perform end-to-end estimation to \"target\" the model parameters to fulfill estimation equations from semiparametric efficiency theory.\n- In the literature, $\\mu$ is usually used for the response functions and $\\pi$ for the propensity score\n- The consistency assumption is missing in Sec. 2"
                },
                "questions": {
                    "value": "-Was data-splitting/ cross-fitting performed for the proposed method and the DR learner?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8811/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8811/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8811/Reviewer_S5uy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8811/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699564939930,
            "cdate": 1699564939930,
            "tmdate": 1699637107899,
            "mdate": 1699637107899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oiNqpTk6CH",
                "forum": "TC9r8gsaoh",
                "replyto": "WVO73ovz82",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer S5uy, we appreciate your detailed feedback. We understand the concerns and hope to address them below.\n\n## Q1. The proposed method already uses the Neyman orthogonal loss of the DR learner which makes the adversarial approach of optimizing for a pessimistic propensity score seem redundant.\n\nThank you for raising this quite important point of discussion.\nSince the current manuscript mainly discussed methods with deep models, there was only a limited description of parametric statistical methods in the introduction as follows.\n\n> Various countermeasures have been tried to alleviate this problem, such as doubly robust (DR) methods (Kang et al., 2007; Kennedy, 2020; Dud\u00edk et al., 2014), which are clever combinations of outcome prediction models and only weighting its residuals using estimated propensity. Nevertheless, the IPW-based approach\u2019s limitation is the instability of the two-step procedure in non-asymptotic situations (Athey et al., 2018).\n\nAs implied above, we believe that these (semi-)parametric analyses based on asymptotics assuming $p < N$ (at least w.r.t. the target $\\tau$) have much room for improvement when applied to a complex (deep) model.\nThe larger the hypothesis space, the more likely a hypothesis with an optimistic loss value will exist within it (and such a hypothesis is to be selected).\nIn addition to the large hypothesis space, the extreme weights (even if accurate) exacerbate the effective sample size (Tanimoto et al. 2022, reference in the manuscript), which is usually ignored in the convergence rate analyses as a constant coefficient.\nTherefore we need a theory and a method that take care not only of the inaccuracy of the nuisance propensity model but also the estimation variance due to extreme weights.\n\nA series of DNN-based CATE estimation methods including (Shalit et al., 2017) avoid weighting, but this too has been pointed out to be theoretically limited (Johansson et al., 2019; Zhao et al., 2019; Wu et al., 2020).\nSome DNN-based methods such as (Shi et al. 2019; Hassanpour and Greiner, 2019) adopt simultaneous training of the model and weights; however, they adopt naive simultaneous training of them and lack a principle of their joint objectives.\n\nWe would like to expand the description of the manuscript in this comparison.\n\n## Q2. Despite being central to the topic of the paper, three of the four works mentioned above are not cited in the paper. Neither is the R-learner considered as a baseline.\n\nI would like to refer to the references you mentioned and clarify clarify along with our answer to Q1.\n\nWe added the R-Learner as a baseline. See the overall response.\nR-Learner (RNet) exhibited similar trends with the DR-Learner.\nOur NuNet outperformed RNet in most settings.\n\n## Q3. Why not also (adversarial) for the response surfaces? (S5uy)\n\nEstimation error is not the only reason for introducing the adversarial perturbation to the nuisance.\nBasically, the adversarial formulation would involve optimizing the concave objective function (maximizing the MSE), which reduces the stability of the training.\nNevertheless, we believe that there is great merit in the adversarial formulation for the propensity, for the reasons stated in the above Q1 section.\nOn the other hand, there is no such reason for the surfaces.\n\nIn addition, the true effect and surfaces have a linear relation $\\tau^*(x) = \\hat f^*_1(x) - \\hat f^*_0(x),$ which means that if the adversary adds perturbations $\\Delta f_1$ and $\\Delta f_0$, the learner can adjust as $\\tau \\leftarrow \\tau + \\Delta f_1 - \\Delta f_0$ (ignoring the error correction terms), and the advantage for the adversary vanishes.\n\nWe would like to clarify this in the manuscript.\n\n## Q4. A property of the DR loss is that only requires\u00a0**either**\u00a0the propensity score\u00a0**or**\u00a0the response surfaces to be estimated correctly to achieve a fast convergence rate (Kennedy, 2023). Again, this would make the proposed approach redundant if the response surface estimators converge sufficiently fast.\n\nActually, deep models are highly expressive and converge even with modeling surface learning alone.\nIn order to speed up the convergence, models with flexible induction bias for the response surface are being explored.\nSNet is one such consideration.\nIn practice, however, DRNet and NuNet with weighting often outperform SNet even without such a sophisticated model architecture (Figs. 2 and 3 in the manuscript).\nTherefore, it is important to have a framework that can utilize the expressive power of DNNs with weighting."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078405746,
                "cdate": 1700078405746,
                "tmdate": 1700097123938,
                "mdate": 1700097123938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEBbgO7bz1",
                "forum": "TC9r8gsaoh",
                "replyto": "WVO73ovz82",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We hope this message finds you well. We are writing to follow up on the rebuttal we submitted. We are keen to know if our rebuttal has adequately addressed your concerns.\n\nIf our response has been satisfactory, we would greatly appreciate it if you could update the review status accordingly. If there are any further questions or additional clarification needed, please feel free to let us know. We welcome any additional feedback that can help improve our work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554069396,
                "cdate": 1700554069396,
                "tmdate": 1700554110271,
                "mdate": 1700554110271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2rnYTzlmLv",
            "forum": "TC9r8gsaoh",
            "replyto": "TC9r8gsaoh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8811/Reviewer_DQuR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8811/Reviewer_DQuR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a CATE estimation method using doubly robust estimators and machine learning.\n\nHere the outcome and propensity models are learned with ML in a \"targeted\" way to minimize the MSE of the CATE estimator, rather than being fit separately and plugged in. They then derive and bound/regularize additional loss terms that account for the contribution of nuisance mis-estimation to estimator bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It's important to bridge the gap between ML and traditional yet challenging estimation problems such as CATE from observational data\n- Theory looks good/correct\n- Go beyond deriving a standard DR estimator and characterization additional issues with nuisance mis-estimation and what to do about it\n- Experiments are pretty complete."
                },
                "weaknesses": {
                    "value": "There seems to be substantial discussion of related work missing.\n\nIn particular, there is a lot of existing work that also directly estimates the nuisance functions with ML, and even does so in a regularized way to directly target the estimand.\n\nSome examples include:\n- Adapting Neural Networks for the Estimation of Treatment Effects, https://arxiv.org/abs/1906.02120\n- RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests https://arxiv.org/abs/2110.03031\n\n(EDIT: I saw that the Shi work was added in the revision. See \"Questions\")\n\nMore generally, as mentioned in questions below, I think there are some clarity issues, such as incomplete sentences, that make the work hard to understand. Some of those unclear sentences appear exactly when there is an important differentiation about related work to be made.\n\nI would be willing to raise my score if the other reviewers believe that the related work has been clarified in the revision, and if the other reviewers and AC believe that unclear sentences/discussion points such as those above could be clarified easily."
                },
                "questions": {
                    "value": "1)\n\n(EDIT after revision) I saw that the updated draft includes the passage: \"Joint optimization approaches have also been proposed for ATE estimation (Shi et al., 2019), though it may lead to cheating by less weighting to noisy regions especially under noise heterogeneity\"\n\nI am not sure what \"cheating\" means and what \"less weighting\" means?\n\n2) \n\nIn the baseline section, in passing, the authors mention \"DeR-CFR\" (Wu et al., 2022) as another method that optimizes weights and outcome model simultaneously, but do not clarify why it is not compared against.\n\n3)\n\n\"Although f0 and f1 are also nuisance parameters, the uncertainty of them do not differ among the target parameter space, thus we need not take care\"\n\nThis sentence is very hard to understand; it is not complete and certain phrases are not defined \"uncertainty does not differ\" and \"not taking care\". Which uncertainty?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8811/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8811/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8811/Reviewer_DQuR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8811/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700849047171,
            "cdate": 1700849047171,
            "tmdate": 1700849229221,
            "mdate": 1700849229221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]