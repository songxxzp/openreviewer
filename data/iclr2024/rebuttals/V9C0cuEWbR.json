[
    {
        "title": "Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization"
    },
    {
        "review": {
            "id": "JaDr4AEXao",
            "forum": "V9C0cuEWbR",
            "replyto": "V9C0cuEWbR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_RwMk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_RwMk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Soft Convex Quantization (SCQ) scheme to substitute the vector quantization (VQ) module in generative modeling. The authors first outline the challenges in VQ, including gradient approximation, codebook collapse, and lossy quantization, as well as recent related works for alleviating these issues. The proposed SCQ leverages convex optimization to perform soft quantization, which acts as an improved drop-in replacement for VQ that addresses many of the challenges. Experimental results demonstrate that SCQ is superior to existing VQ methods on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to follow.\n2. The proposed SCQ is a novel method to mitigate the challenges in VQ mentioned above. First, the optimization of SCQ (via Eq. (8)) selects a convex combination of codebook vectors for each embedding, which means that the codebook capacity can be sufficiently utilized to avoid codebook collapse. Second, SCQ is implemented via Differentiable Convex Optimization (DCO), which enables effective and efficient backpropagate through soft quantization.\n3. Extensive experiments are conducted to validate the effectiveness of the proposed SCQ. SCQ outperforms recent state-of-the-art methods on two different datasets in image reconstruction task in terms of three measurements. Analysis of SCQ presents that SCQ improves codebook perplexity and convergence during training. Besides, when combined with the training first-stage models, the authors show that SCQGAN performs better than VQGAN."
                },
                "weaknesses": {
                    "value": "1. Soft quantization is also used to solve the gradient approximation challenge discussed in section 2.2.1, which is not mentioned in the paper.\n2. The authors claim that SCQ is implemented with DCO. However, the relationship between the optimization of SCQ and DCO is ambiguous, especially the relationship between Eq. (7) and Eq. (8).\n3. The analysis of how SCQ addresses the lossy quantization challenge compared with existing VQ methods is not convincing. More theoretical analysis or experiments are encouraged to demonstrate this point.\n4. Some details are missing. For example, in Eq.(8), the codebook used to obtain \\tilde(P) is not specified. Besides, in section 4.2, the training loss of SCQGAN is not given."
                },
                "questions": {
                    "value": "1. What is the relationship between the optimization of SCQ and DCO?\n2. Could the authors provide more detailed analysis on how SCQ mitigates the lossy quantization challenge?\n3. Could the author give the missing details mentioned in weaknesses #4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673958761,
            "cdate": 1698673958761,
            "tmdate": 1699636212937,
            "mdate": 1699636212937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nVj9grtQHI",
                "forum": "V9C0cuEWbR",
                "replyto": "JaDr4AEXao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We address the reviewer\u2019s concerns and questions below.\n\n**Regarding weaknesses:**\n1. We would appreciate it if the reviewer could provide more clarity on which soft quantization method is being referred to.\n2. The SCQ is formulated as an instantiation of a DCO layer. Specifically, in Eq. (7) we introduce the general notation for a DCO layer as proposed in (Amos & Kolter, 2017). In Eq. (8) we introduce the SCQ instantiation. In the forward pass we solve for the matrix $P$ containing convex combination weights in its columns and in the backward pass we update matrix $C$ which represents the parameters of the DCO layer. We then proceed to introduce a scalable version of the SCQ algorithm which relaxes the optimization given in Eq. (8). This scalable algorithm eliminates the need to solve a quadratic program in the forward pass to carry out the soft quantization. The scalable algorithm is presented in Alg. 2.\n3. SCQ sees an improvement in the quantization performance over VQ. To make this clearer, we first consider how the quantization error is incurred for VQ: the error is measured between the input feature and the \u201cclosest\u201d codebook vector. For SCQ, any individual input feature can be exactly reconstructed if it lies within the convex hull of the set of codebook vectors. This is a consequence of how the SCQ optimization in Eq. (8) is formulated. Thus, for any input feature that does not coincide exactly with a codebook vector and lies within the convex hull of codebook vectors, we incur no quantization error in SCQ whereas we incur nonzero error in the VQ method. For input features outside the convex hull of the set of codebook vectors, we again incur smaller error for SCQ as we measure the error with respect to the projection onto the convex hull. We would be happy to formalize this analysis and include it in the paper. We also demonstrate the improvement in quantization error empirically: in Table 1 (pg. 7, reproduced in General Comments section above), we have included the quantization error as a metric to evaluate SCQ\u2019s quantization performance over competing VQ variants. The quantization error measures the incurred MSE between the encoder outputs and quantized counterparts. The table summarizes results on the image reconstruction performance on CIFAR-10 and GTSRB datasets. Across both datasets SCQ significantly outperforms competing methods on the induced quantization error. \n4. We thank the reviewer for bringing this to our attention. The codebook used to obtain $\\tilde{P}$ is the same codebook used throughout the SCQ process, i.e. codebook $C$. $\\tilde{P}$ represents the output of the VQ method given the same codebook. We regularize the SCQ optimization in Eq. (8) with $\\tilde{P}$ to bias the SCQ solution towards a one-hot encoding (this is to ensure compatibility with downstream autoregressive generation tasks). The loss curves of SCQGAN are given in Figures 6-8 in Appendix B of the paper. \n \n\n**Regarding questions:**\n\n1. Please see the response to (2) above.\n2. Please see the response to (3) above.\n3. Please see the response to (4) above."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028692905,
                "cdate": 1700028692905,
                "tmdate": 1700028692905,
                "mdate": 1700028692905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vqq4NPkDwh",
            "forum": "V9C0cuEWbR",
            "replyto": "V9C0cuEWbR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_7Ro3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_7Ro3"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a differentiable and convex optimization layer as a direct substitute for vector quantization (VQ). It addresses three major challenges in classical VQ: a) non-differentiable k-means is replaced by softmax and gumbel sampling; b) codebook collapse through stochasticity; c) transforms the NP-hard quantization centroid generation into a convex hull over codebook vectors. The results outperforms other VQ-based architectures for image processing tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work demonstrated a great alternative to k-means based VQ training which is differentiable and convex. \n\n2. Clean formulation on the optimization goals and algorithms.\n\n3. Clear performance wins over other VQ techniques for neural networks."
                },
                "weaknesses": {
                    "value": "1. In each comparison table, the non-quantized baseline numbers are missing.\n\n2. Author emphasizes the problem of codebook collapse, but there were no quantitative support for how SCQ performs better than traditional k-means or VQ."
                },
                "questions": {
                    "value": "1. Please elaborate on how SCQ prevents the codebook collapse problem.\n\n2. What is the runtime latency of this work when compared to the uncompressed baseline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2708/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2708/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2708/Reviewer_7Ro3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722390901,
            "cdate": 1698722390901,
            "tmdate": 1699636212869,
            "mdate": 1699636212869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lk3MJl0XmS",
                "forum": "V9C0cuEWbR",
                "replyto": "Vqq4NPkDwh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We address the reviewer\u2019s concerns and questions below.\n\n**Regarding weaknesses:**\n1. For the CIFAR-10 and GTSRB experiments (see Section 4.1), we provide the non-quantized baseline numbers below. The same base autoencoder (AE) was used without any quantization bottleneck. Thus, we report only the final test MSE over both datasets as the other metrics are not applicable (i.e. there is no quantization or codebook). We would like to highlight that in Table 1 (pg. 7) we demonstrate that SCQ comes closest to this non-quantized MSE. Moreover, we would also like to emphasize that this baseline non-quantized autoencoder is not compatible with downstream generative applications due to the missing latent structure provided by a quantization bottleneck.\n\n**Results on CIFAR-10**\n| Method      | MSE ($10^{-3}$)$\\downarrow$ | Quant Error$\\downarrow$ | Perplexity$\\uparrow$ | Avg Quant Time (ms)$\\downarrow$ |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| Non-quantized AE  | 0.44 | - | - | - |\n\n**Results on GTSRB**\n| Method      | MSE ($10^{-3}$)$\\downarrow$ | Quant Error$\\downarrow$ | Perplexity$\\uparrow$ | Avg Quant Time (ms)$\\downarrow$ |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| Non-quantized AE  | 1.25 | - | - | - |\n\n2. In Table 1 (pg. 7, reproduced in General Comments section above) we include the perplexity metric to quantitatively measure SCQ\u2019s improvement over VQ and other variants with regards to the codebook collapse problem. The table compares methods on an image reconstruction task for CIFAR-10 and GTSRB and reports the average performance over 5 independent training runs. In the context of vector quantization, the perplexity metric measures the average usage of each codebook vector over a batch of samples: higher perplexity scores indicate better coverage of the entire codebook in the quantization process. Across both datasets SCQ significantly outperforms competing methods on the perplexity score.\n\n**Regarding questions:**\n1. For every individual input feature, VQ utilizes the single \u201cclosest\u201d codebook vector in the quantization. Thus, during training, the subset of codebook vectors that have been selected for a batch of inputs receive training signals and are updated. The codebook vectors that have not been selected do not receive any update signal. The codebook collapse problem is a consequence of this phenomenon, where only a small fraction of codebook vectors are repeatedly \u201cchosen\u201d and updated by VQ whereas the remainder are unused. SCQ overcomes this issue by representing each individual input feature with a (convex) combination of the entire codebook. Thus, during backpropagation, the entire codebook will receive a signal to update. We appreciate the reviewer\u2019s question and hope this provides sufficient clarification. We also strive to make this more clear within the paper.\n2. Please see the general comments for this question. We have extended Table 1 (pg. 7) with latency measures of the quantization methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030031701,
                "cdate": 1700030031701,
                "tmdate": 1700030031701,
                "mdate": 1700030031701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dSnGBJnUdR",
                "forum": "V9C0cuEWbR",
                "replyto": "Lk3MJl0XmS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Reviewer_7Ro3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Reviewer_7Ro3"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their rebuttal. However, for the first question on the \"codebook collapse\", please support your arguments with a quantitative study. Explanation without numbers is a bit thin for your claim.\n\nI also share same concern with reviewer Jz15 and d7nB. Authors should choose one of the two options in presenting this work:\na) Demonstrate quality under the same bit-rate (current SCQ is using way more bits than VQ), or\nb) focus a particular downstream application that is only feasible/tractable using SCQ."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493698265,
                "cdate": 1700493698265,
                "tmdate": 1700493698265,
                "mdate": 1700493698265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7W9cOAOaRW",
            "forum": "V9C0cuEWbR",
            "replyto": "V9C0cuEWbR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_d7nB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_d7nB"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose soft convex quantization (SCQ) as a drop-in replacement for vector quantization (VQ) for generative modeling with architectures such as VQVAEs and VQGANs. \n\nSCQ learns a codebook of vectors similar to VQ. However, SCQ uses its code vectors to define a convex polytope. Then, to \"quantize\" an arbitrary vector, SCQ projects it onto the closest point of this convex polytope. In particular, SCQ \"quantizes\" vectors inside the polytope with essentially zero error. This approach starkly contrasts with VQ, which quantizes an arbitrary vector to its closest code vector.\n\nWhile SCQ's projection operation is differentiable (as opposed to VQ, which needs to use a straight-through estimator to perform gradient descent-based optimization), the authors note that it is not scalable. Thus, they develop a scalable approximation to it.\n\nThe authors perform image reconstruction experiments on several datasets and show that their method compares favorably to VQ-based methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I found using the code vectors to define a convex polytope and \"quantize\" vectors by projecting them onto the polytope quite interesting.\n\nFurthermore, the authors compare their methods across several datasets using several different generative models."
                },
                "weaknesses": {
                    "value": "I also find soft convex quantization a misnomer because the method doesn't quantize vectors (i.e., it doesn't map them to a discrete set of representations); it projects them onto a polytope. This latter issue is especially problematic because it means that comparing SCQ to VQ is meaningless.\n\nFrom the perspective of autoencoders, SCQ is a worse model because it has zero error inside its convex polytope, but it introduces a projection error outside of it. Furthermore, it is computationally more expensive to use SCQ than just using a standard autoencoder.\n\nHowever, theoretical issues aside, my primary problem with the paper is that the model's use cases are unclear. \n\nAs I understand, VQ is usually used for generative modeling or data compression. Since SCQ produces continuous \"quantized\" representations, it cannot be used for compression, so its use case seems to be restricted to generative modeling. However, the authors do not perform any experiments on generative modeling, only image reconstruction, so SCQ's performance for this task is unclear.\n\nCould the authors please correct me if I misunderstood their approach? If not, could they please clarify what use case they intend for their method?\n\nBesides this, the writing needs to be improved. The authors use non-standard terminology, e.g. they use the terms \"first-stage\" and \"second-stage\" models for inference and generative networks, respectively. There are many strangely worded sentences that are difficult to understand.\n\nThey also seem to misuse the term \"sparsity\" in section 3.1. A sparse vector has most of its entries be exactly zero, while the authors seem to mean close to zero.\n\nIn the same paragraph, the symbol C is overloaded to mean the number of input channels and the codebook matrix.\n\nFinally, the authors don't provide any formal analysis for or perform any ablation studies between the \"proper\" convex optimization procedure in Algorithm 1 and the relaxed version they propose in Algorithm 2, so the error introduced by this approximation is unclear."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766872970,
            "cdate": 1698766872970,
            "tmdate": 1699636212793,
            "mdate": 1699636212793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S7swYT683k",
                "forum": "V9C0cuEWbR",
                "replyto": "7W9cOAOaRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We address the reviewer\u2019s concerns below.\n\n**SCQ Motivation**\n\nWe appreciate the reviewer\u2019s concern regarding the name of our proposed algorithm. We believe that the best way of justifying the chosen name is by motivating it again. We propose SCQ as a drop-in replacement that relaxes VQ. VQ operates as a parametric online K-means algorithm: it quantizes individual input features with the single \u201dclosest\u201d learned codebook vector. However, due to challenges that arise from discretization with a single codebook vector (e.g. codebook collapse, inexact backpropagation, lossy quantization) our aim was to relax the hard quantization constraint of VQ to a \u201csoft\u201d one that involves multiple codebook vectors simultaneously. The \u201cconvex\u201d aspect to our algorithm was born because we wanted probabilities as weights when combining codebook vectors to represent input features. This is a relaxation of VQ which is restricted to using one-hot encodings. We decided on using \u201cconvex\u201d weights/probabilities as this would make the latent space compatible with downstream autoregressive generative models. We understand the reviewer\u2019s concern that SCQ does not perform hard quantization. However, we maintain that our algorithm is indeed a relaxation/softening of the hard VQ, where we can trivially recover the VQ solution through SCQ by taking hyperparameter $\\lambda$ in Eq. (8) to infinity. Thus, we believe that within this context it is also fair to view VQ as an instantiation of SCQ and directly compare the methods.\n\n**SCQ Application**\n\nAs mentioned above, we intend for SCQ to be used as a drop-in replacement for VQ in the generative modeling framework. We regard this paper as the first in a sequence of works that explores the potential of the SCQ algorithm in this context. Specifically, in this paper our aim was to introduce SCQ as a relaxation of VQ that addresses many of the latter\u2019s drawbacks. The goal was not to focus on a downstream generation application, but rather demonstrate SCQ\u2019s potential as a soft quantization layer that enables training superior VAE architectures on several vision datasets whilst remaining compatible (by design) with downstream generative models. SCQ is readily compatible with latent diffusion processes that operate directly on the encoder embeddings, as well as autoregressive models that require supervision from a categorical distribution over the latent space. On the other hand, uncompressed baseline autoencoders are not compatible with downstream autoregressive models as there is no characterization of a categorical distribution over the latent space. We believe that the far superior performance of SCQ-embedded VAEs will lend itself naturally to improved results for downstream generation applications that utilize these models. In particular, we demonstrate in Section 4.2 how SCQGANs retain higher performance over VQGANs when considering smaller latent resolutions. This observation is particularly useful for generation tasks, as operating on smaller resolutions significantly reduces the required downstream computation. In future work, we aim to couple SCQ-embedded models with latent generative processes and showcase improved performance of SCQ on downstream applications.      \n\n**Other**\n\nWe thank the reviewer for pointing out some unclear/non-standard terminology, and overloaded notation. We aim to correct the writing accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028957419,
                "cdate": 1700028957419,
                "tmdate": 1700028957419,
                "mdate": 1700028957419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dsno8niyTc",
                "forum": "V9C0cuEWbR",
                "replyto": "S7swYT683k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Reviewer_d7nB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Reviewer_d7nB"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their rebuttal. However, sadly, it did not change my view of the paper in its current form. I understand that SCQ is a relaxation of quantization but a very significant relaxation. It changes the range of the \"quantization\" layer from discrete (in the case of VQ) to continuous and allows for cases that couldn't remotely occur for VQ, such as having zero quantization error for every point inside the convex hull of the codebook vectors. This fact makes the comparison between VQ and SCQ unfair.\n\nAs such, it seems like SCQ gets the worst of both worlds: 1) it is not as powerful as a simple autoencoder since it introduces an error for representations outside the convex hull of the codebook vectors, yet it is more challenging to train. 2) On the other hand, it uses continuous latent representations, so it cannot be used for compression.\n\nTherefore, I cannot see any situation in which I would choose to use SCQ over either VQ or simply not quantizing the latents. Hence, the authors should demonstrate what advantageous theoretical/empirical properties SCQ has (e.g., what is gained by restricting the decoder's domain to be compact / to be a polytope? What do the codebook vectors decode to compared to VQ?) or rewrite the paper to focus fully around a particular downstream application that is only feasible/tractable using SCQ."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310570787,
                "cdate": 1700310570787,
                "tmdate": 1700310570787,
                "mdate": 1700310570787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q59Qj9K0H6",
            "forum": "V9C0cuEWbR",
            "replyto": "V9C0cuEWbR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_Jz15"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2708/Reviewer_Jz15"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a continuous relaxation of vector quantization, where rather than assigning a vector to a single codebook element, the vector can be assigned to anywhere within the convex hull of codebook elements. This makes the resulting VQ variant easier to integrate into SGD-based training routines, increases codebook utilization due to the use of soft assignments, and increases the quality of the quantized representation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Strong experimental results and useful illustrations for image reconstruction\n1. Strong mathematical exposition"
                },
                "weaknesses": {
                    "value": "1. The comparison against VQ-based quantizers for image reconstruction quality needs to be justified more. Using VQ on a K-vector codebook leads to $\\log K$ bits per codeword, while this technique seems to use $O(K)$ bits, due to its soft assignments (unless the assignments are rounded later on, which I don't believe is the case). It therefore seems rather obvious and trivial that this much higher-bitrate representation leads to better image reconstruction; it simply isn't much of an information bottleneck, relative to traditional VQ. Either:\n\n    * Justify the comparison on image reconstruction quality, when the baseline algorithms seem to be at a severe disadvantage given their much lower bitrate.\n    * Show superior results on downstream tasks such as image generation, where compressed representation isn't the goal but rather a means to achieve the desired result.\n1. No mention of training time. The scalable relaxation's runtime of $O(K^3)$ still seems rather expensive. How does this compare to traditional VQ-based methods?\n1. Minor weakness in notation: $C$ is both the codebook and the number of channels."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2708/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2708/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2708/Reviewer_Jz15"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811315340,
            "cdate": 1698811315340,
            "tmdate": 1699636212677,
            "mdate": 1699636212677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GTQbkWD48F",
                "forum": "V9C0cuEWbR",
                "replyto": "Q59Qj9K0H6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We address the reviewer\u2019s concerns below.\n\n**Regarding weaknesses:**\n1. SCQ is proposed as a drop-in replacement for VQ within the generative modeling framework. Specifically, our focus in this paper is to introduce SCQ as a relaxation of VQ that effectively addresses several limitations associated with the latter. Through the formulation in Eq. (8) it is clear that VQ is a particular instantiation of SCQ - taking hyperparameter $\\lambda$ to infinity exactly recovers the VQ solution. However, by keeping $\\lambda$ finite, we obtain a balance between the one-hot VQ solution compatible with downstream generative processes and the improved backpropagation, codebook collapse prevention capabilities of the DCO. Thus we maintain the validity of comparing SCQ with VQ and its variants. \nIn this paper the focus was not on any specific downstream generation task, but rather to illustrate SCQ's capabilities as a soft quantization layer that facilitates the training of superior VAE architectures whilst maintaining compatibility with downstream generative models. We anticipate that the significantly enhanced performance of SCQ-embedded VAEs will naturally translate into improved results for downstream generation applications employing these models. In Section 4.2, we demonstrate how SCQGANs consistently outperform VQGANs, especially at smaller latent resolutions. This observation holds valuable implications for downstream generation tasks, where operating on smaller resolutions substantially reduces computational requirements. Looking ahead, our future work aims to integrate SCQ-embedded models with latent generative processes, showcasing the enhanced performance of SCQ in downstream applications.\n2. Please see the general comments for this concern. We have extended Table 1 (pg. 7) with latency measures of the quantization methods.   \n3. We thank the reviewer for feedback on the overloaded notation. We will correct this."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029483172,
                "cdate": 1700029483172,
                "tmdate": 1700029483172,
                "mdate": 1700029483172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VzGI6AuEJa",
                "forum": "V9C0cuEWbR",
                "replyto": "GTQbkWD48F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2708/Reviewer_Jz15"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2708/Reviewer_Jz15"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for providing quantization latencies.\n\nI believe Reviewer d7nB largely shares the same concerns as I do. It still doesn't seem fair to me to compare VQ to SCQ when the former compresses the data, and the latter does not. It is a trivial result for SCQ to give better VAE performance; taken to the extreme, if we completely remove the information bottleneck and replace it with an identity transformation, we could expect even better VAE performance, and with less compute too.\n\nCan you elaborate on your response to d7nB \"SCQ is readily compatible with latent diffusion processes that operate directly on the encoder embeddings, as well as autoregressive models that require supervision from a categorical distribution over the latent space\"? How exactly does this soft \"quantized\" representation work with downstream generative applications (specifically when $\\lambda$ is finite)? Demonstrating this in the paper is critical, because this downstream compatibility is the main thing differentiating SCQ from similarly non-bottlenecked transformations, like identity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700181189033,
                "cdate": 1700181189033,
                "tmdate": 1700181189033,
                "mdate": 1700181189033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]