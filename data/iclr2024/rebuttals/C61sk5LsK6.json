[
    {
        "title": "InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning"
    },
    {
        "review": {
            "id": "ZPomEw18ew",
            "forum": "C61sk5LsK6",
            "replyto": "C61sk5LsK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission247/Reviewer_DemB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission247/Reviewer_DemB"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel dynamic data pruning, aiming to remove (with a predefined probability) the samples with lower loss score. The algorithm does not require to sort the losses, but it does require to train the model with all samples in the final epochs. The experimental results show a significant speedup in the training procedure, with minimal (or zero) performance drop."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**originality**: Although the idea of dynamic pruning is not novel (as clearly stated by the authors), but the solution provided is original enough.\n\n**quality**: The experimental results show the algorithm is able to obtain a clear speedup training, and also remarkable performance results, with almost no accuracy drop.\n\n**clarity**: The idea is simple and easy to implement. The ablation study also provides a solid explanation about how each novel idea affects the overall result.\n\n**significance**: Speeding up training procedures is of huge interest, as it can save a lot of time and energy."
                },
                "weaknesses": {
                    "value": "**originality**: The idea is somehow similar to other dynamic pruning approaches. It does not provide a different point of view in the matter.\n\n**quality**: In the ablation study, I would like to see if different threshold selections (apart from the mean value) can affect the algorithm. I find it a little bit odd that there is no discussion regarding to this point.\n\n**clarity**: The text is too dense. The figures are too small to be read in a paper. I suggest the authors to increase the figures, while removing the text surrounding them. Some sections, like 2.3, can be summarized to make room for the adjustment.\n\n**significance**: the authors claim their threshold value can be established in constant time, whereas the state-of-the-art methods require a sorting part (the complexity should be $\\mathcal{O}(N \\log N)$. However, I think this improvement is no significant, as the speedup produced is residual compared to the time needed to train the network."
                },
                "questions": {
                    "value": "- Why there is no discussion regarding to the threshold selection procedure? Different solutions like taking the mean plus a factor of the standard deviation can led to interesting results.\n\n- Did the authors experiment with a pruning probability that depends on the sample score?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I think the authors should address the issue that can cause a bias in the final training, as removing certain samples that can cause this issue."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698223654507,
            "cdate": 1698223654507,
            "tmdate": 1699635950489,
            "mdate": 1699635950489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FC0z51g9AW",
                "forum": "C61sk5LsK6",
                "replyto": "ZPomEw18ew",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to DemB (1/3)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer DemB for the careful review and valuable comments/questions.\nFor the concerns and questions, we make responses as follows.\n\n**Q1: Idea similar to other dynamic pruning approaches, not providing a different point of view in the matter.**\n\n**A1:** Thanks for the comment. In section 1 paragraph 5, we stated: \n* \"Directly pruning data may lead to a biased gradient\nestimation as illustrated in Fig. 1a, which affects the convergence result. This is a crucial factor that\nlimits their performance, especially under a high pruning ratio (corresponding results in Tab. 1)\"\n\nTo better illustrate the difference, we show a table here for comprehensive analysis:\n\n| method                       | soft prune | rescale | anneal | unbiased| Task                                         | lossless saving ratio |\n|------------------------------|------------|---------|--------|---------|----------------------------------------------|------------------------|\n| Other Dynamic Pruning [1][2] | no         | no      | no     | no      | Classification                               | ~20%                   |\n| InfoBatch                    | yes        | yes     | yes    | yes     | Classification, Segmentation, Diffusion, LLM | 40%                    |\n\n\n**Analysis:** \n* Previous methods don't use soft pruning. It could lead to a biased gradient direction (see Appendix B.1, or B.2 in the updated revision).\n* Previous methods don't use rescaling. This leads to a lack in total update, which is more severe at a higher pruning ratio.\n* Previous methods don't use annealing. Therefore a remaining bias could be left (Appendix B.2 or updated revision B.3 further discussed annealing).\n* Previous methods mainly focus on classification. InfoBatch can be applied to a broader range of tasks.\n\n\n**Conclusion:** \nInfoBatch proposes a **probabilistic framework** aiming to achieve unbiased dynamic pruning (with soft pruning, rescaling, and annealing), which differs from\nother dynamic pruning works that propose **metrics** and use **sort** to select more important samples.\n\n\n**Q2: Ablation of threshold selection.**\n\n**A2:** Thanks for the comment. We conduct these ablations on CIFAR-100 ResNet-50 as follows:\n\n**Setting:**  All results are averaged across three runs with std reported.\nWe report their acc and pruning ratio here.\n\n| threshold      | Acc. (%) | Prune. (%) |\n|----------------|----------|------------|\n| <mean(default) | 80.6\u00b10.1 | 32.8       |\n| <mean+0.5*std  | 80.2\u00b10.1 | 35.4       |\n| <mean+ 1*std   | 80.1\u00b10.1 | 38.2       |\n| <mean+ 2*std   | 80.1\u00b10.1 | 41.5       |\n| <75%           | 80.6\u00b10.1 | 32.8       |\n| <85%           | 80.3\u00b10.1 | 36.9       |\n\n**Analysis:** \n* mean + c*std as a threshold with c>0 can achieve a higher saving ratio but with degraded performance.\n* percentile threshold selection can also lead to a reasonable performance. \n* Specifically, the <75% threshold gets a similar saving ratio and performance as using mean. \n* <85% threshold achieves both higher saving ratio and performance than \nmean+0.5*std\n\n**Discussions:**\n* According to the inference in 2.3, the unbiasedness doesn't depend on the threshold selection. **The experiment result matches the theoretical analysis**\nas using the 75 percentile threshold also gives a lossless result.\n* std factor is not preferred, properly because entropy loss is not Gaussian and long-tailed at convergence \n(we add a visualization in revised Appendix E.1).\n* if assuming Gaussian distribution for some other metrics behind loss, percentile of loss can do the same thing as using means + c*std.\n\n**Conclusion:** InfoBatch is robust to different threshold selection methods. Empirically, the std factor is not preferred for loss. Percentile thresholds could also be a choice."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468551745,
                "cdate": 1700468551745,
                "tmdate": 1700468665923,
                "mdate": 1700468665923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OQahd7lC1b",
                "forum": "C61sk5LsK6",
                "replyto": "ZPomEw18ew",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer DemB (2/3)"
                    },
                    "comment": {
                        "value": "**Q3: Did the authors experiment with a pruning probability that depends on the sample score?**\n**A3:** Thanks for the question. To improve data efficiency, we use the percentile method to prune part of the data (20%) with a more aggressive ratio r (0.75)\nas the default setting on several datasets, as claimed in section 3.1.\n\nBy doing so, a higher pruning ratio can be achieved without performance loss. In Table 6 with this improvement,\nCIFAR-100 ResNet-50 can achieve 41.3% saving ratio, compared to 33% in Table 5 with uniform r.\n\nA theoretical related analysis is in Appendix B.3 (updated revision B.4), suggesting that $\\mathbb{E}\\_\\text{rescaled}[G\\_{z}^2/(1-\\mathcal{P}\\_t(z))]\\leq {\\mathbb{E}}\\_{\\mathcal{D}}[G_z^2]$\ncould be a condition for healthy rescaling. This indicates that we can prune and rescale more aggressively for \nsamples with low loss (gradients).\n\nTo better illustrate the effect of using different pruning probabilities depending on the sample score,\nwe verified different combinations as follows:\n\n**Setting:** InfoBatch on CIFAR-100 ResNet-50 with different thresholding method. \nAll results are averaged across three runs with std reported.\nWe report their acc and pruning ratio here.\n\n| range:ratio(r)                                                         | saving ratio (%) | performance (%)  |\n|------------------------------------------------------------------------|--------|--------------|\n| full data (r=0)                                                        | 0      | 80.6\u00b10.1     |\n| <mean:0.5                                                              | 33     | 80.6\u00b10.1     |\n| [0%,20%):0.75,[20%,85%):0.5                                            | 41.3   | **80.6\u00b10.2** |\n| [0%,30%):0.75,[30%,90%):0.5                                            | 45     | 80.4\u00b10.2        |\n| [0%,36%):0.75,[36%,95%):0.5                                            | 49     | 80.3\u00b10.2        |\n| [0%,20%):0.75\uff0c[20%,40%):0.7\uff0c[40%,60%):0.65\uff0c[60%,80%):0.6\uff0c[80%,90%):0.5 | 51.3   | 80.1\u00b10.2         |\n\n\n**Analysis:** \n* Adaptive r with [0%,20%):0.75,[20%,85%):0.5 gets the 41.3% saving ratio with lossless performance. \n* Adaptive r depending on the score could require a corresponding parameter search.\n* Moving thresholds rightward would cause degraded performance.\n\n**Conclusions:**\n* A pruning probability depends on sample score could further improve data efficiency.\n* It is not straightforward to find a better adaptive r with manually tuning\n\n**Future work:** **To easily find adaptive r values,** we plan to propose a cheap estimator for gradient G and try setting adaptive r based on it ensuring $\\mathbb{E}\\_\\text{rescaled}[G\\_{z}^2/(1-\\mathcal{P}\\_t(z))]\\leq {\\mathbb{E}}\\_{\\mathcal{D}}[G_z^2]$.\n\n\n**Q4: Summarize 2.3 and increase the figures.**\n\n**A4:** Thanks for the comments. We shortened the 2.3 and rearranged the layout to enlarge the figures (in the updated revision). The corresponding change \nis as follows:\n* Sec 1, we enlarge Fig 1 and move it to the top of the page.\n* Sec 2.3, we shorten the Theoretical analysis and move part of the proof to B.1.\n* Sec 3, we put the previous Fig 3 and Fig 4 together to the top, making them no longer surrounded by text.\n* Sec 3.5, we increase the size of Table 8 and Table 9.\n\n\n**Q5: Overhead acceleration not significant compared to current dynamic pruning with sort**\n\n**A5:** Thanks for the question. Sort is O(NlogN) for full data and amortized O(logN) for each sample, ours is O(N) and \namortized O(1). We design the operation considering it could differ in a large factor when N is very big, as shown in Appendix D table 11.\n\nThe overhead saving of operation compared to UCB is 10 times (0.03h to 0.0028h) on ImageNet-1K; however we agree it is \nnot a significant improvement compared to training time saving (17.5h->10h). Our main saving comes from saving the forward and backward\ncomputation, which is shown in the table below:\n\n**Setting:** ResNet-50 on CIFAR-100. All results are averaged across three runs with std reported.\nWe report their acc and pruning ratio here. \n\n|           | Overhead (s) | Saving      | Acc. (%) |\n|-----------|--------------|-------------|----------|\n| UCB       | < 2          | 20%, ~800s  | 80.4\u00b10.2 |\n| UCB       | < 2          | 33%, ~1320s | 79.8\u00b10.2 |\n| UCB       | < 2          | 41%, ~1640s | 79.5\u00b10.2 |\n| InfoBatch | < 0.1        | 33%, ~1320s | 80.6\u00b10.1 |\n| InfoBatch | < 0.1        | 41%, ~1640s | 80.6\u00b10.2 |\n\n**Analysis**: \n* At lossless performance of ResNet-50 on CIFAR-100, InfoBatch can save 41% cost compared to 20% of UCB. The saved time is a 100% (800s->1600s) improvement.\n* At the same saving ratio, UCB has a much higher performance degradation\n\nWe degraded the claim of the part on time complexity, which can be visible in the new revision. The main modifications can \nbe summarized as:\n1. in section 4 related works, we change \"which could be a non-negligible overhead\" -> \"which could be an overhead\""
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468592262,
                "cdate": 1700468592262,
                "tmdate": 1700726216097,
                "mdate": 1700726216097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jp85n3PCYf",
                "forum": "C61sk5LsK6",
                "replyto": "wMwrcqXnbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_DemB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_DemB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I suggest the authors to put a disclaimer in the paper related to how data pruning can cause an increase in the bias. More experiments need to be done before concluding that data pruning is not prone to cause bias. Besides that, I am satisfied with the answers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478876243,
                "cdate": 1700478876243,
                "tmdate": 1700478876243,
                "mdate": 1700478876243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eEfti6EyVm",
                "forum": "C61sk5LsK6",
                "replyto": "ZPomEw18ew",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer DemB on bias (Ethics)"
                    },
                    "comment": {
                        "value": "Thank you for the advice. We agree that ethical issue should be studied with care and not overstated.\nWe would evaluate the ethical issue carefully with extended experiments in final revision. We add a disclaimer \nin limitations (in the updated revision), claiming the potential bias should be considered when applying this research:\n* In Section 5 limitations, we add \"Removing samples may cause bias in model predictions. It is advisable to consider \nthis limitation when applying InfoBatch to ethically sensitive datasets.\"\n\nWe are welcome to further discussions if there are further questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502098729,
                "cdate": 1700502098729,
                "tmdate": 1700503594674,
                "mdate": 1700503594674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pRfCHbiEEc",
            "forum": "C61sk5LsK6",
            "replyto": "C61sk5LsK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission247/Reviewer_8u8Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission247/Reviewer_8u8Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a dynamic data pruning scheme to reduce the cost of stochastic gradient training. It does so by stochastically removing lower loss data points and correspondingly rescaling their gradients to maintain an unbiased overall estimates. In conjunction with training on all data towards the end of training, the scheme is able to reduce training cost by about 20-40% on a range of datasets and architectures, including some large scale ones such as ImageNet and a LLaMA model.\n\nOverall, this is a simple but highly pragmatic and practical approach. The scheme solely relies on quantities that are computed during training anyway, so incurs minimal overhead. Unfortunately, I believe that the comparison with the baselines is not entirely applies-to-apples and there are some minor issues with the write-up, so that all things considered I would lean towards rejecting the paper. Nevertheless, I hope these will be addressed over the course of the rebuttal and am open to increasing my score.\n\nEDIT: the extensive additional results for the rebuttal have addressed my concerns and I would now recommend acceptance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The approach is simple but pragmatic, I appreciate the care that is taken to not incur substantial overheads for additional computation such as thresholding by the mean score. This could be a broadly applicable technique for speeding up training, both for researchers and practitioners.\n* The method is described well, I think it would be straight-forward to implement this even without code being provided.\n* There is a broad range of experiments, including some larger scale setting involving ImageNet and language models, emphasizing the potential relevance of the approach."
                },
                "weaknesses": {
                    "value": "* The comparison with the baselines does not seem entirely apples-to-apples to me due to the \"annealing\" period on the whole dataset. I suspect (intuitively and based on the ablations in Table 4 plus the pruning rule seemingly being irrelevant in Table 5) that ensuring the total length of the optimization trajectory remains comparable to that on the full dataset (by rescaling the gradients) in conjunction with the fine-tuning on all data towards the end of training is the \"secret sauce\" to making a dynamic pruning method perform without a degradation in test accuracy. I'm not familiar with the (Raju et al., 2021) paper, but would expect that at least the annealing period could be incorporated into this method without further issue. At the moment the paper presents its selection rule leading to performance matching that of full-data training as a core contribution, however if this can be achieved relatively easily with other selection techniques as well, I think it becomes more about the re-scaling/tuning as general purpose techniques and the cost comparison between different selection approaches being featured more prominently. I don't think this would worsen the paper at all, although it would change the key takeaways a fair bit and I think it is important that the latter accurately reflect the empirical results.\n* On a related note, I am a little bit concerned that the hyperparameters on e.g. ResNets for CIFAR10 are not tuned for achieving the final test performance as quickly as possible. I think it would be worth adding a baseline that trains on all data with a reduced number of epochs/learning rate decay milestones but increased learning rate corresponding to the computation saved by pruning (so hypothetically for 20% saved computation, train for 80 instead of 100 epochs but with learning rate 1.25 instead of 1 and halve it after 40 instead of 80 epochs). This is to ensure that pruning approaches meaningfully speed up training rather than benefitting from slack in the canonical hyperparameter choices for benchmark problems.   \n* I don't entirely follow what the theoretical analysis is trying to achieve in 2.3. Isn't this just showing that the soft-pruned and rescaled gradient is unbiased? Isn't this completely obvious from having independent Bernoullis multiplied onto the terms of a sum (the total gradient over the dataset) and the expectation of a Bernoulli being its probability (so that if we divide by the probability, we get an expectation of 1 and the sum remains unchanged)?\n* I found the paper to be fairly different to what the title made me expect. \"Info\" and \"lossless\" imply a connection with information theory and lossless compression to me, which is of course not present in the method. I appreciate that this is entirely subjective, but would suggest reconsidering the title of the paper. In particular, I would argue that the \"lossless\" part is a bit misleading since this is not theoretically guaranteed by the method, but merely and empirical observation in the experiments. Of course matching performance to the full dataset can always be achieved by letting $r \\rightarrow 0, \\delta \\rightarrow 1$, but this would remove any cost savings.\n* Similarly, I think the paper overstates its relationship with coreset/data selection methods a little bit. These are generally not for speeding up an initial training run, but subsequent ones e.g. for continual learning or hyperparameter tuning and typically incur an upfront cost. On the contrary, the proposed method speeds up a given training run without producing any artefacts (a coreset) that are useful downstream. So to me this seems more like a curriculum learning paper (although I am not particularly familiar with this branch of the literature, so this is a somewhat speculative statement)."
                },
                "questions": {
                    "value": "* I would like to see results for Random*, $\\epsilon$-greedy and UCB with annealing and gradient re-scaling (for Random*; for the other two as applicable). As much as possible for Table 1 and ideally Table 2 (ResNet-18 instead of 50 is perfectly fine if that makes it more realistic). My hypothesis here would be that all baselines will match InfoBatch in discriminative performance, which would necessitate the main claims in the paper being updated (again, I don't think this affects the value of the contribution). If all the baselines are already using annealing and rescaling, this point is of course void.\n* Add a full data baseline with reduced epochs as proposed in the weaknesses.\n* Is there anything more to section 2.3 than showing unbiasedness?\n* I would be curious what fraction of the data are soft-pruned throughout training? With the losses being bound below by 0, I would expect the distribution to become quite asymmetric as training proceeds. Could e.g. the median or some percentile be preferable? The median (not sure about arbitrary percentiles) can be computed in linear time, although I don't know if it is possible to update it online as for the mean.\n* Do you have any thoughts on how to set $r$ and $\\delta$ on a new dataset/architecture? The benchmarks in the paper are of course well-studied, but I think it would be a nice addition to the paper to discuss some heuristics for finding close-to-optimal values without needing a full training run (although suboptimal values already present a cost saving of course).\n\nTypos:\n- p1, \u00a73: \"constrainT computation resources\" -> \"constrainED computation resources\"\n- p1, \u00a74: \"cubersome\" -> \"cumbersome\"\n- Section 3.3, \u00a71: \"see appendix\" -> missing specific reference\n- B4. last \u00a7: \"bond\" -> \"bound\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission247/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission247/Reviewer_8u8Q"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698245781056,
            "cdate": 1698245781056,
            "tmdate": 1700587467442,
            "mdate": 1700587467442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SQtvArVQoz",
                "forum": "C61sk5LsK6",
                "replyto": "pRfCHbiEEc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 8u8Q (1/4)"
                    },
                    "comment": {
                        "value": "We sincerely thank the review 8u8Q for the meticulous review and responsible attitude. \nWe fix those typos in the revised PDF, thank you.\n\nFor the concerns and questions, here are our responses:\n\n**Q1: Are rescaling and annealing all the key points of InfoBatch?**\n\n**A1:** Thanks for the question. This is partially right. **Only with soft (probabilistic) pruning, recaling can take effect\nwith no bias.** Rescaling for hard pruning would harm the performance. Annealing can be used in all settings, which helps to \nachieve lossless performance when rescaling is unbiased. To verify this, we conduct the following experiment on CIFAR-100 ResNet-50.\n\n| method                     | saving ratio  (%)| performance (%) |\n|----------------------------|------------|-------------|\n| InfoBatch w/o soft pruning | 33         | 80.1        |\n| InfoBatch w/ soft pruning | 33         | 80.6        |\n\nNote: training on the whole dataset achieves 80.6% acc.\n\n**Conclusion: Soft pruning is important to unbiased rescaling and achieving lossless results.**\n\nThis question is not fully answered yet at this point. The further insight of InfoBatch's sample selection is discussed \nin answer A2 and A3.\n\n**Q2: (Generalization) Results for other dynamic baselines with annealing and rescaling**\n**A2:** Thanks for the question. The random in Table 5 (CIFAR-100 ResNet-50) is the random* with annealing and rescaling.\nIts result is improved as expected over table 4, since random* is also a kind of soft pruning. \n\n\nHowever on UCB which uses (sort and) hard pruning, the thing is different. The rescaling can only be applied to all remaining \nsamples which are all higher score samples, being statistically different from pruned samples. \nWe show the corresponding experimental results here:\n\nSetting: Train ResNet-50 on CIFAR-100 using UCB and random*. We report their acc and pruning ratio here.\n\n| method                                      | saving ratio (%) | performance  (%) | compared to InfoBatch |\n|---------------------------------------------|--------------|--------------|-----------------------|\n| UCB                                         | 33%          | 79.9         | -0.7                  |\n| UCB                                         | 41%          | 79.5         | -1.1                  |\n| UCB+anneal                                  | 33%          | 80.1         | -0.5                  |\n| UCB+anneal                                  | 41%          | 79.6         | -1.0                  |\n| UCB+rescale+anneal                          | 33%          | 79.9         | -0.7                  |\n| UCB+rescale+anneal                          | 41%          | 79.5         | -1.1                  |\n| random*                                     | 33%          | 79.7         | -0.9                  |\n| random*+rescale+anneal<br/>(Table 5 random) | 33%          | 80.5         | -0.1                  |\n| random*+rescale+anneal                      | 41%          | 80.3         | -0.3                  |\n\n\n**Analysis:**\n* UCB's performance is increased by annealing using the same saving ratio (by controlling the pruning fraction), but much lower than\nInfoBatch\n* Rescaling with annealing improves the performance of random* using the same saving ratio; but adding rescaling leads to degraded performance\nof UCB\n\n**Conclusion:** **Annealing could help to improve performance on a broader range of\nsample selection methods, but rescaling can only take effect if it is using soft pruning.**\n\n**Q3: Point beyond soft pruning, rescaling, and annealing**\n\n**A3:** Thanks for the comment.\n\n**It is possible to use different (static or dynamic) thresholds with different pruning ratios** \n\n* According to the proof in Section 2.3, the expectation rescale equation would hold independent of the threshold selection and probability.\n\n**It is possible to use a more aggressive ratio r on lower score (gradient) samples.**\n\n* According to Appendix B.3 (B.4 in revised version), $\\mathop{\\mathbb{E}}\\_{rescaled}[G\\_{z}^2/(1-\\mathcal{P}\\_t(z))]\\leq \\mathop{\\mathbb{E}}\\_{\\mathcal{D}}[G_z^2]$\ncould potentially lead to health rescaling. \n* We use the percentile method to prune part of the data (20%) with a more aggressive ratio r (0.75) as the default setting as claimed in 3.1.\nThis leads to a higher pruning ratio without performance loss. \nIn Table 6 with this improvement, CIFAR-100 ResNet-50 can achieve 41.3% saving ratio, compared to 33% in Table 5 using uniform r.\n\n**Summarization:**\n* Rescaling (w/ soft pruning) and annealing are our contributions which do generalize to difference threshold selection\n* We further explore better data efficiency with better utilization of scores from both theoretical and experimental aspects."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467116170,
                "cdate": 1700467116170,
                "tmdate": 1700549890802,
                "mdate": 1700549890802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "poQl3gKGQi",
                "forum": "C61sk5LsK6",
                "replyto": "pRfCHbiEEc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 8u8Q (2/4)"
                    },
                    "comment": {
                        "value": "**Q4: Add a full data baseline with reduced epochs and adjusted learning rate.**\n**A4:** Thanks for the question. Here we add the tuned learning rate baseline with reduced epoch numbers. \n\n\n| settings with adjusted learning rate         | saving ratio      | performance (%) | InfoBatch performance |\n|----------------------------------------------|-------------------|-----------------|-----------------------|\n| CIFAR-10 ResNet-18 baseline     | 30%   (140 epoch) | 94.8            | 95.6 \u2b61 0.8            |\n| CIFAR-10 ResNet-18 baseline     | 50%   (100 epoch) | 94.6            | 95.1 \u2b61 0.5            |\n| CIFAR-10 ResNet-18 baseline     | 70%   (60 epoch)  | 92.7            | 94.7 \u2b61 2.0            |\n| CIFAR-100 ResNet-18 baseline    | 30%   (140 epoch) | 77.0            | 78.2 \u2b61 1.2            |\n| CIFAR-100 ResNet-18 baseline    | 50%   (100 epoch) | 76.9            | 78.1 \u2b61 1.2            |\n| CIFAR-100 ResNet-18 baseline    | 70%   (60 epoch)  | 76.3            | 76.5 \u2b61 0.2            |\n| ImageNet-1K ResNet-50 baseline  | 60%   (54 epoch)  | 72.8            | 76.5 \u2b61 3.7            |\n\n**Analysis** \n* InfoBatch outperforms the tuned baseline. This is properly because higher loss samples are \nmore sensitive to learning rate scaling as discussed in Appendix B.3. \n* Based on that, when scaling the learning rate to match\nthe original training progress, scaling lower loss samples is still preferred than higher loss samples. \n\n[//]: # (We add this baseline to Table 1 in the revision.)\n\n**Conclusions:**\n* With the same computation, InfoBatch has higher performance than the tuned baseline.\n* According to the table and the point above, the improvement of InfoBatch is not caused by a lack of tuning of the benchmark.\n\n\n**Q5: Purpose of 2.3**\n\n**A5:** Thanks for the question. Previously, considering the different backgrounds of reviewers, we wrote 2.3 with more detailed \nsteps. We summarize it to make it brief (in the updated revision), the changes are as follows:\n\n* We merge previous Eqn. (6) and (7) to revised Eqn. (6), and move the middle step to B.1.\n* We merge previous Eqn. (8) and (9) to revised Eqn. (7).\n\n**Q6: \"Info\" and \"lossless\" in the title**\n\n**A6:** Thanks for the comment. \n* We call our method \"InfoBatch\" because it takes advantage of the current learning status (loss) to do selective data pruning. It is \"informative\"\ninstead of purely random.\n* We achieve lossless results on many tasks with substantial savings, which is an important milestone. It is true InfoBatch cannot\nguarantee a lossless result in all settings, but the empirical hyperparameters have already achieve lossless results on image classification,\nsemantic segmentation, image generation, and language model finetuning.\n\nWe are welcome to further discussion on this problem during the author reviewer discussion period.\n\n\n**Q7: What fraction of the data are soft-pruned throughout training?**\n\n**A7:** Thanks for the question. We plot the saving fraction curve of InfoBatch in CIFAR-10 ResNet-50 training in \nrevised Appendix E.1 Fig.6.\n\nFor **entropy loss**, InfoBatch (mean) usually **starts with a relatively lower ratio,\nand then the ratio keeps increasing till the end**. This is probably because maximizing log probability tends to converge to a \nlong-tailed loss distribution.\n\nWe show the loss distribution visualization during training in revised Appendix E.1 Fig.7. We can see the loss distribution\nis initially right-skewed and finally left-skewed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467366093,
                "cdate": 1700467366093,
                "tmdate": 1700467654604,
                "mdate": 1700467654604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ab8J1sqoLm",
                "forum": "C61sk5LsK6",
                "replyto": "pRfCHbiEEc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 8u8Q (4/4)"
                    },
                    "comment": {
                        "value": "**Q10: How to set r and $\\delta$ on a new dataset/architecture?**\n\n**A10:** Thanks for the good question. \n\nWe consider the r value in the future work (see it in Appendix B.4), we plan to use \n$\\mathbb{E}\\_\\text{rescaled}[G\\_{z}^2/(1-\\mathcal{P}\\_t(z))]\\leq {\\mathbb{E}}\\_{\\mathcal{D}}[G_z^2]$\nto find near-optimal r. We theoretically analyze the reasonableness of choosing r according to this equation.\n* Our proposed procedure is to measure the $\\mathop{\\mathbb{E}}_{\\mathcal{D}}[G_z^2]$ \n(or some cheaper indicator values instead) after warmup, then use \nthe statistics to quickly select near optimal r values. \n\nFor delta, our default value 0.875 is actually corresponding to the annealing stage of the one cycle scheduler \nwe used for the learning rate schedule. It corresponds to a drop in loss during normal training. \nIn general, it is the last 17.85% of a cosine annealing. As cosine annealing is prominent in \nmany tasks, we suggest **last 0.1785 fraction of cosine annealing could be used as default**."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467626097,
                "cdate": 1700467626097,
                "tmdate": 1700468111820,
                "mdate": 1700468111820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AGmZpYmWMD",
                "forum": "C61sk5LsK6",
                "replyto": "i6SzzYeoGZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_8u8Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_8u8Q"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the extensive additional results. These largely address my concerns and I will increase my score.\n\nI'm happy to believe that the soft pruning and rescaling/annealing techniques in the paper indeed lead to a meaningful saving in compute costs (and that this is not just due to suboptimal default hyperparameters for benchmarks). The one aspect that I remain a little bit sceptical on is that of the decision criterion. I appreciate that this is based on a fairly limited number of results, but it does look to me like random*+rescale+anneal does essentially as well as pruning based on loss values. I think it would be insightful for the camera-ready paper to test this baseline in a couple more settings and see if the observation holds across the board (in which case section 2.3 should be updated) or if e.g. for larger savings ratios we see the informative pruning criterion performing better."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587420696,
                "cdate": 1700587420696,
                "tmdate": 1700587420696,
                "mdate": 1700587420696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J96e1kPjLR",
                "forum": "C61sk5LsK6",
                "replyto": "pRfCHbiEEc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 8u8Q,\n\nThanks for acknowledging our work. We agree with the advice, and we are running the corresponding experiments to report the \nobservations (how much does loss-guided pruning differ from random pruning in performance under our framework) as follows:\n1. With **various architectures**, we plan to evaluate the performance on CIFAR-10 with random/loss-guided pruning + rescaling + annealing at default saving ratio\n\n| architecture\\selection | Random | Loss Guided |\n|------------------------|--------|-------------|\n| VGG                    | *      | *           |\n| ResNet-18              |95.3     | 95.6         |\n| ResNet-50              | 95.3      | 95.6          |\n| Swin-Tiny               | *      | *           |\n\n**Note:** * denotes running experiment waiting for the result. We will keep updating the results till the end of the reviewer-author discussion period. We will discuss these results in the revision.\n\n2. On **various datasets**, we plan to evaluate the performance with ResNet-50 at default saving ratio\n\n| dataset\\selection | Random | Loss Guided |\n|-------------------|--------|-------------|\n| CIFAR-10          | 95.3      | 95.6          |\n| CIFAR-100         | 80.5  $\\pm$ 0.2  | 80.6 $\\pm$ 0.1          |\n| ImageNet-1K       | *      |  76.5        |\n\n\n**Note:** * denotes running experiment waiting for the result. We will keep updating the results till the end of the reviewer-author discussion period. We will discuss these results in the revision.\n\n3. For **higher saving ratios**, we plan to evaluate the corresponding performance on CIFAR-10 ResNet-18 (using the aforementioned quantile method to control the ratio for loss-guided pruning)\n\n| ratio\\selection | Random | Loss Guided |\n|-----------------|--------|-------------|\n| 0%(baseline)    | 95.6    | 95.6         |\n| 30%             | 95.3      | 95.6           |\n| 40%             | 95.3      | 95.5           |\n| 50%             | 94.9      | 95.2           |\n| 60%             | 94.8      | 95.1           |\n| 70%             | *      | *           |\n\n\n**Note:** * denotes running experiment waiting for the result. We will keep updating the results till the end of the reviewer-author discussion period. We will discuss these results in the revision.\n\nOnce again, we appreciate the insightful and constructive comments on our work. We will be glad to improve our work if \nthere is any following advice. Thanks again for your comments and time."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596894135,
                "cdate": 1700596894135,
                "tmdate": 1700740686135,
                "mdate": 1700740686135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cWu01OjXs6",
            "forum": "C61sk5LsK6",
            "replyto": "C61sk5LsK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents InfoBatch, a novel data pruning approach which dynamically determines pruning probability over the course of training. InfoBatch soft prunes data with small loss value leading to negligible training cost overhead, and rescales remaining data so as to achieve unbiased gradient expectation. By conducting experiments across a wide range of tasks, the paper demonstrates the effectiveness and robustness of InfoBatch as a state-of-the-art data pruning technique in terms of tradeoff between performance and computational cost."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper tackles a practically-relevant problem supported by a fair amount of experiments conducted across various tasks in the image domain. \n- InfoBatch is simple yet has a distinctive benefit over existing dynamic data pruning approaches: (i) replacing sorting operation with mean-thresholding significantly reduces the overhead cost, and (ii) gradient expectation bias is well addressed supported by theoretical analysis. \n- The proposed method demonstrates superior performance compared to the baselines, and the paper provides a comprehensive review of relevant previous works.\n- Overall, the paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- InfoBatch improves over UCB via throwing away the dataset sorting operation. However, in Table 2, the practical overhead cost saving seems negligible compared to the wall clock time and the total node hour. Also, how was the overall saved cost calculated in Tables 6 and 7?   \n- To my understanding, annealing utilizes the whole dataset without pruning for the last 0.125% of total training epochs. This raises several concerns: (i) As the wall clock time of UCB and InfoBatch in Table 2 are both 10.5h, is this value taking the annealing process into account? (ii) Regarding Table 1, all the baselines and InfoBatch are compared under the same dataset pruning ratio. I wonder whether this is a fair comparison when annealing is involved in InfoBatch. (iii) Why did the authors utilize annealing only as a means of stabilizing the optimization, rather than leveraging the full dataset at the very beginning of optimization when we know that the early epochs of training can heavily influence the convergence of the loss landscape [1]?  \n- The authors may need to provide further clarification regarding how annealing contributes to the stabilization of the rescaling process, especially if it does not seem to significantly impact the variance of the results in Table 4.\n- The authors claim that the use of loss values in pruning conditions serves two purposes: (i) it reflects the learning status of samples, and (ii) it theoretically ensures unbiased gradient expectations. However, in Table 5, it is observed that even a random pruning criterion achieves nearly the same performance as the original pruning condition. This result raises questions about the necessity and effectiveness of using loss values as a pruning criterion and may require further discussion or clarification in the paper."
                },
                "questions": {
                    "value": "- How many random seeds are used throughout the experiments? \n \n\n[1] Fort et al., \u201cDeep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel.\u201d 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL",
                        "ICLR.cc/2024/Conference/Submission247/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838093527,
            "cdate": 1698838093527,
            "tmdate": 1700618535844,
            "mdate": 1700618535844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zRSkNe0RVZ",
                "forum": "C61sk5LsK6",
                "replyto": "cWu01OjXs6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer nCAL (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer nCAL for the valuable questions and comments. \nFor the concerns and questions, here are our responses:\n\n**Q1: Overhead cost saving over UCB seems negligible compared to training**\n\n**A1:** Thanks for the question. Sort is O(NlogN) for full data and amortized O(logN) for each sample, ours is O(N) and \namortized O(1). We design the operation considering it could differ by a large factor when N is very big.\n\nThis overhead saving of operation compared to UCB is 10 times (0.03h to 0.0028h) on ImageNet-1K; however we agree it is \nnot a significant improvement compared to training time saving (17.5h->10h). Our main saving comes from saving the forward and backward\ncomputation, which is shown in the table below:\n\n\n**Setting:** ResNet-50 on CIFAR-100. All results are averaged across three runs with std reported.\nWe report their acc and pruning ratio here. \n\n|           | Overhead (s) | Saving      | Acc. (%) |\n|-----------|--------------|-------------|----------|\n| UCB       | < 2          | 20%, ~800s  | 80.4\u00b10.2 |\n| UCB       | < 2          | 33%, ~1320s | 79.8\u00b10.2 |\n| UCB       | < 2          | 41%, ~1640s | 79.5\u00b10.2 |\n| InfoBatch | < 0.1        | 33%, ~1320s | 80.6\u00b10.1 |\n| InfoBatch | < 0.1        | 41%, ~1640s | 80.6\u00b10.2 |\n\n**Analysis**: \n* At lossless performance, InfoBatch can save 41% compared to 20% of UCB. The saved time is a 100% improvement.\n* At the same saving ratio, UCB has a much higher performance degradation\n\nWe degraded the claim of the part on time complexity, which can be visible in the new revision. The main modifications can \nbe summarized as:\n1. in section 4 related works, we change \"which could be a non-negligible overhead\" -> \"which could be an overhead\"\n\n\n\n**Q2: How was the overall saved cost calculated in Tables 6 and 7**\n\n**A2:**\nThanks for the question. As the overhead is negligible compared to training, we measured wall clock time and found the sample iteration saving\nis basically the same as wall clock saving. So the **overall saved cost is both wall clock time and sample iteration**.\n\n**Q3: Is annealing taken into account for saving?**\n\n**A3:** Thanks for the question. The answer is yes. Our time = overhead + pruned training time + annealing training time.\nFor table 1, InfoBatch saves slightly more cost than reported, with annealing already taken into account.\n\n**Q4: Using full dataset at warmup**\n\n**A4:** Thanks for the question. Only in the experiment of Timm Swin-Tiny training, we warmup with the full dataset for 5 epochs to avoid training instability. \n\nOn other tasks,the initial pruning ratio is lower, which serves as a pruning warmup for CNN-based experiments. A fraction\nis visualized in revised Appendix E.1 Fig.6. It is properly due to early stage loss distribution is right-skewed and later left-skewed,\nwhich is visualized in revised Appendix E.1 Fig.7.\nViT-based networks seem more sensitive to this warmup stage than CNN-based ones.\n\nThank you for pointing it out, we make the following modification to make it more clear:\n* In sec A.3 paragraph 2, add \"We also warmup InfoBatch for 5 epochs, only recording scores without pruning.\""
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466731698,
                "cdate": 1700466731698,
                "tmdate": 1700466731698,
                "mdate": 1700466731698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JomQ7rj26R",
                "forum": "C61sk5LsK6",
                "replyto": "cWu01OjXs6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review nCAL (2/2)"
                    },
                    "comment": {
                        "value": "**Q5: Further clarification of how annealing contributes to stabilization**\n\n**A5:** Thanks for the question. We found Table 4 with a low number of precision digits is hard to illustrate this point clearly.\nWe draw a plot in the revised E.2 Fig.8. All experiments\nuse the same saving ratio.\n\n**Analysis:** In this plot, compared to only rescaling, adding annealing leads to an increased mean performance and slightly reduced performance variance.\n\n**Conclusion:** This plot indicates how annealing helps stabilize the training.\n\n**Q6: Necessity of using loss value**\n**A6:** Thanks for the comment.\n\nBy reasonably utilizing loss, we can further improve data efficiency, based on the inference in Appendix B.3 (revised version B.4).\n\nNoted that in Table 5 (CIFAR-100 R-50), the lossless pruning ratio using mean is 33%. In Section 3.1 experiment details\nwe claim to use a more aggressive pruning probability r(0.75) for smaller loss samples (lowest 20%) on CIFAR-100;\nthat leads to a 41.3% saving ratio in Table 6.\n\nWe conduct the following experiment to elaborate:\n\n**Setting:** InfoBatch on CIFAR-100 ResNet-50 with increased r for certain samples. \nAll results are averaged across three runs with std reported.\nWe report their acc and pruning ratio here.\n\n| method                     | saving ratio (%) | performance (%) | time    |\n|----------------------------|------------------|-----------------|---------|\n| lower loss higher r(0.75)  | 41.3             | 80.6\u00b10.2        | 1.48h   |\n| higher loss higher r(0.75) | 41.3             | 79.9\u00b10.3        | 1.48h   |\n| random                     | 41.3             | 80.3\u00b10.2        | 1.48h   |\n\n**Analysis:** \n* Using a higher pruning ratio r for higher loss samples significantly degrades performance.\n* A related discussion is in Appendix B.3 (revised version B.4) about why lower-loss samples are preferred. \n$\\mathbb{E}\\_\\text{rescaled}[G\\_{z}^2/(1-\\mathcal{P}\\_t(z))]\\leq {\\mathbb{E}}\\_{\\mathcal{D}}[G_z^2]$ could be \nthe condition for a healthy rescaling.\n* Compared to random selection, there is almost no overhead to use loss, as it can be obtained with no extra cost.\n\n**Conclusions:** \n* **Loss is preferred, especially when using an adaptive high pruning ratio** which further improves data\nefficiency. \n* The **overhead of loss is negligible**, so its **improvement is of almost no cost** and thus also preferred in settings \nwithout adaptive high pruning ratio. \n\n\n**Q7: How many random seeds are used throughout the experiments**\n\nA7: Thanks for the question. For CIFAR-10/100 experiments, we measured at least 3 trials for those values with std."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466854053,
                "cdate": 1700466854053,
                "tmdate": 1700468174592,
                "mdate": 1700468174592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gbtKkhmneu",
                "forum": "C61sk5LsK6",
                "replyto": "cWu01OjXs6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to the reply"
                    },
                    "comment": {
                        "value": "Dear reviewer nCAL:\n\nThanks so much again for the time and effort in our work. \nAccording to the comments and concerns, we conduct the corresponding experiments and further discuss the related points.\nBesides, we have revised our claim of overhead compared to dynamic methods. We also provided visualization plots in Appendix E.1 to further illustrate how annealing contributes to stabilization. \n\nAs the discussion period is nearing its end, please feel free to let us know if there are any other concerns. Thanks again for your time and efforts."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546038712,
                "cdate": 1700546038712,
                "tmdate": 1700546161055,
                "mdate": 1700546161055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yWQWw0ifif",
                "forum": "C61sk5LsK6",
                "replyto": "gbtKkhmneu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their detailed response and the supplementary experiments. As the authors addressed most of my concerns, including clarification of total saving overhead cost and warmup stage with full dataset, I will raise my score from 5 to 6, and vote for the acceptance. \n\n- I have one more lingering question regarding the stabilization effect of annealing. While I agree with the most of the authors' response, I still remain uncertain whether the primary role of annealing is to reduce variance. In Fig. 8, it appears that annealing notably improves performance, yet it doesn't seem to stabilize performance as effectively as when it is used solely. I would appreciate if the authors could clarify why they mainly portray annealing as a stabilizing factor in Section 3.4?"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618507493,
                "cdate": 1700618507493,
                "tmdate": 1700618507493,
                "mdate": 1700618507493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GrnGLjmnyW",
                "forum": "C61sk5LsK6",
                "replyto": "cTl84sz3BC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_nCAL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the further clarification. With my concerns fully addressed, I vote in favor of acceptance."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646292920,
                "cdate": 1700646292920,
                "tmdate": 1700646292920,
                "mdate": 1700646292920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WX3m8IJD0h",
            "forum": "C61sk5LsK6",
            "replyto": "C61sk5LsK6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission247/Reviewer_Nce2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission247/Reviewer_Nce2"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a dynamic data pruning approach that can obtain lossless performances with less training cost.\nIt achieved the unbiased gradient update by randomly pruning a portion of less informative samples and rescaling the gradient of the remaining samples. The proposed approach consistently obtains lossless training results on various ML tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Clear presentation and easy-to-follow writing.\n- The proposed method is theoretically-supported and, more importantly, very efficient and easy to implement.\n- The evaluation, together with the analysis, is extensive and convincing."
                },
                "weaknesses": {
                    "value": "The paper conducts a complete study on dynamic data pruning, and the following weakness is relatively minor.\n- Missing recent works: 1) static data pruning [a,b,c], 2) dynamic data pruning [d]\n\n---\n[a] Active learning is a strong baseline for data subset selection. NeurIPS workshop, 2022\n\n[b] Moderate: Moderate coreset: A universal method of data selection for real-world data-efficient deep learning. ICLR, 2023\n\n[c] CCS: Coverage-centric Coreset Selection for High Pruning Rates. ICLR, 2023\n\n[d] Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. ICML, 2022"
                },
                "questions": {
                    "value": "- How to illustrate the gradient trajectory with landscape in Fig 1? Is it an illustration or a real plot on some dataset?\n- Which dataset is used for Table 4? maybe ImageNet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839321922,
            "cdate": 1698839321922,
            "tmdate": 1699635950118,
            "mdate": 1699635950118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fnhsZB7DVL",
                "forum": "C61sk5LsK6",
                "replyto": "WX3m8IJD0h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Nce2"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer Nce2 for pointing out the missing references as well as a potential improvement. We make responses as follows.\n\n**Q1: Add missing references.**\n\n**A1:** Thanks for the advice. We update the section 1 and 4 to add those references(see the revision). The main changes are:\n* Sec 1, paragraph 2: add reference \" Park et al., 2022; Xia et al., 2023; Zheng et al., 2023\"\n* Sec 4, paragraph 1: add \"AL (Park et al., 2022) propose\nto use active learning methods to select a coreset. Moderate (Xia et al., 2023) proposed to use the\nmedian of different scores as a less heuristic metric. Coverage-centric Coreset Selection (Zheng et al.,2023) \nadditionally considers distribution coverage beyond sample importance. \"\n* Sec 4, paragraph 2: add \"Mindermann et al. (2022) propose Reducible Holdout Loss Selection which\nprioritizes samples neither too easy nor too hard. It emphasizes training learnable samples.\"\n\n**Q2: How to illustrate the gradient trajectory with landscape in Fig 1? Is it an illustration or a real plot on some dataset?**\n\n**A2:** Thanks for the question. Fig 1 is an illustration instead of a real plot. To avoid misunderstanding, we modify the caption \n\"visualiztion\" -> \"illustration\".\nThe corresponding accuracy values are from Table 1 CIFAR100 experiment, comparing InfoBatch and EL2N-2 at 50% pruning ratio. \n\n**Improvement Plan:**\nWe notice that there could be a potential improvement of this illustration using the method proposed in [1].\nIt would express the same idea but with a more accurate visualization. We found in [2] Fig 1 they show the gradient sketch\non such a loss landscape. We are welcome to further discussion on this in the auther reviewer discussion period (whether change our Fig 1 like that).\n\n[1] Li, Hao, et al. \"Visualizing the loss landscape of neural nets.\" Advances in neural information processing systems 31 (2018).\n[2] Liu, Zhuang, et al. \"Dropout Reduces Underfitting.\" arXiv preprint arXiv:2303.01500 (2023).\n\n**Q3: Which dataset is used for Table 4?**\n\n**A3:** Thanks for the comment. In section 3.4 first paragraph, we claim: _if not stated, the experiments are conducted on CIFAR-100 by default_.\nTable 4 is conducted on CIFAR100 as default dataset for ablation. We update its caption to make it more clear in the updated revision."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466311538,
                "cdate": 1700466311538,
                "tmdate": 1700466311538,
                "mdate": 1700466311538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "97sgNnML3R",
                "forum": "C61sk5LsK6",
                "replyto": "fnhsZB7DVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_Nce2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission247/Reviewer_Nce2"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "Great work! The authors addressed all my concerns and please include the real plot (as in [2] Fig 1) in the final version. Concerning InfoBatch's broad applicability, I believe there is a substantial impact. I will keep my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533606566,
                "cdate": 1700533606566,
                "tmdate": 1700533606566,
                "mdate": 1700533606566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]