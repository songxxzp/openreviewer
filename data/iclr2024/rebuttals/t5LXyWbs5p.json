[
    {
        "title": "Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals"
    },
    {
        "review": {
            "id": "3dC42g0pcS",
            "forum": "t5LXyWbs5p",
            "replyto": "t5LXyWbs5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_hfvK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_hfvK"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of learning a reliable pretraining approach against domain shifts in multimodal biosignal processing. Proposed method harnesses frequency-domain features in multimodal representation alignment, using a transformer-based masked autoencoder. Simulations are conducted on various multimodal biosignal datasets (e.g, EEG, EOG, EMG, ...), and the approach is demonstrated to be beneficial in several experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approach is novel and effective in its application to multimodal representations and alignment in the frequency domain.\n- The paper is written well with a good storyline and overview illustrations that give the main intuition clearly."
                },
                "weaknesses": {
                    "value": "- Reproducibility in biosignal processing studies is often a concern. Therefore authors should consider strengthening their randomized experiment setup, and open sourcing their code and simulations reported in this submission."
                },
                "questions": {
                    "value": "1) Regarding the downstream tasks: What is the true impact of the used dataset split folds? Did the authors investigate this in simulations where the randomness is controlled (they mention a number of seed repetitions here), or is the appendix Table 7 a one-time setup where test performances are only presented for? What is the performance deviation in such repetitions? This needs to be clarified a bit further to support the strength/reliability of the results.\n\n2) How influential is the architecture capacity on the results? The design of the network backbone seems like an arbitrary choice, as it is not really justified much or studied in the ablations. In comparison to the other approaches, what would be the size of the network (or parameters to be optimized) in the proposed model?\n\n3) What is the comparative computational overhead of bioFAME at training and test time, as opposed to previous alternatives (e.g., TS-TCC, PatchTST..)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698241475272,
            "cdate": 1698241475272,
            "tmdate": 1699636671850,
            "mdate": 1699636671850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wy8VxYA1Xo",
                "forum": "t5LXyWbs5p",
                "replyto": "3dC42g0pcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hfvK"
                    },
                    "comment": {
                        "value": "Dear reviewer hfvK, we appreciate your time and insightful feedback. Especially, thank you for evaluating our work as \u201cnovel and effective\u201d, and \u201cwell written\u201d. We greatly appreciate your positive comments!\n\n---\n### **Reproducibility and randomness control**\n\nWe totally agree that reproducibility is often a concern in the physiological signal field, and plan to publicize all related code soon with the exact settings. Thank you for your suggestion about the randomness control. While we have provided the model performance different across different random seeds under the same dataset splits in Appendix Table 5,  we agree that the data splits are another important source of randomness. However, because we performed many experiments across a diverse range of datasets, we believe the diversity in experiments could compensate for the data splits; and thus did not conduct additional experiments due to time constraints. We will make sure to note this in the limitation of the work.\n\n---\n### **Architecture capacity analysis and additional ablations**\n\nThank you for your question regarding the architecture capacity and additional ablations! \n\nTo understand the size of the model, we compare the total number of parameters (Params) and FLOPs of our approach and baselines.\n\n| Methods | TS2vec  | TFC | TS-TCC | PatchTST | **bioFAME** (Ours) |\n| -------- | ------- | -------- | ------- | ------- | ------- |\n| Params | 632K | 1.18M | 140K | 612K | 243K |\n| FLOPs | 0.69B | 1.38B | 1.95B | 35.0B | 9.42B |\n\nWe can see that, bioFAME is very parameter-efficient due to its fix-size frequency filter design. With the same depth (4), heads (8), and dimensionality (64), bioFAME contains only ~40% parameters of the transformer baseline PatchTST. The parameter size of bioFAME also stands competitive than many CNN-based architectures, with only TS-TCC smaller than it.\n\nTo gain a better understanding of the throughput of the model, we used the fvcore package to compute the FLOPs of the models over a batch of SleepEDF data (batch size = 64). We found that the FLOPs of bioFAME is significantly lower than the transformer baseline PatchTST (<30%); yet greater than CNN-based architectures.\n\nTo understand the sensitivity of our architecture to hyperparameters, we also conducted additional ablation experiments to examine how the performance would change with respect to the changes in numbers of layers and the latent dimensionality. We observed that increasing the latent dimensionality could further improve the performance; while increasing the network depth gives little performance gains.\n\n| Latent dim | 32 | 64 | 128  | 256 |\n| -------- | ------- | -------- | ------- | ------- |\n| EMG | 91.1 | 98.05 | 96.48  | 97.78  |\n| FDB | 76.74  | 76.58 | 78.14  | 80.87  |\n| Avg. | 83.92 | 87.32 | 87.31 | 89.33 |\n\n| Network Depth | 3 | 4 | 5  | 6 |\n| -------- | ------- | -------- | ------- | ------- |\n| EMG | 77.54 | 76.58 | 76.79 | 78.99 |\n| FDB | 97.78 | 98.05 | 95.55 | 92.59 |\n| Avg. | 87.66 | 87.32 | 86.17 | 85.79 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185761828,
                "cdate": 1700185761828,
                "tmdate": 1700185761828,
                "mdate": 1700185761828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7aqAgUZOCE",
                "forum": "t5LXyWbs5p",
                "replyto": "3dC42g0pcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up to the Rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you once again for your insightful feedback. We have carefully revised our manuscript in line with your suggestions and believe we have comprehensively addressed your concerns. Could you kindly confirm whether the revisions align with your expectations? We are available to engage in further discussion or answer any additional questions you may have.\n\nThanks!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587423498,
                "cdate": 1700587423498,
                "tmdate": 1700587423498,
                "mdate": 1700587423498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1qzr3MlV1a",
                "forum": "t5LXyWbs5p",
                "replyto": "7aqAgUZOCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_hfvK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_hfvK"
                ],
                "content": {
                    "title": {
                        "value": "post-rebuttal response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their responses.\n\nIn general I'm happy with the way the manuscript is written, its novelty and scope. My only concern is on the significance of the results, which is still open. The fact that all experimental results are currently bound to certain train-test splits (App. Table 10) is slightly weighing in negatively. I think it should be somewhat simpler to just shuffle these arbitrary train/test splits of one dataset to run a few of the supplementary experiments in order to discard any suspicion on model's capabilities. The fact that there is more than one task/dataset (with again pre-defined arbitrary splits) does not really prove this generalization in the same sense. Therefore I am currently around the borderline, but in general at the positive side. I will keep my score as it is."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733575708,
                "cdate": 1700733575708,
                "tmdate": 1700733575708,
                "mdate": 1700733575708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zfuYzXZFm2",
            "forum": "t5LXyWbs5p",
            "replyto": "t5LXyWbs5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a frequency-aware MAE design for multimodal pretraining on time-series biosignals in the frequency domain. They use a frequency-aware transformer encoder and a frequency-maintain pretraining strategy with masked autoencoding in the latent space. Specifically, the proposed encoder mainly consists of a frequency-domain feature extractor after DFT transformation and an attention-based dynamic fusion mechanism. The frequency-maintain pretraining strategy mainly addresses that masked autoencoding happens in the latent space and channel-independent feature encoding. Experimental results on uni-modal and multi-modal biosignals are provided to demonstrate the advantage of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors tackled an interesting and practical problem in the domain of multi-modal pretraining of time-series sensing signals, with resiliency and robustness considerations.\n\n2. Extensive evaluation results on both uni-modal and multi-modal scenarios are reported.\n\n3. The presentation and writing of the paper is of good quality. It is generally easy to follow and understand the paper.\n\n4. The robustness test presented in the experiment helps justify the performance of the approach."
                },
                "weaknesses": {
                    "value": "1. The authors claimed the frequency-domain analysis and feature extraction for biosignals (being time series) as one of the main contributions in this paper, which actually have been well studied in the sensing community [1, 2]. Besides, the Transformer encoder, the masked autoencoding paradigm, and the attention-based fusion mechanism are original and novel contributions made in this paper. The channel-independent design, as mentioned in the paper, has been studied in several previous works. For this reason, I feel the novelty of the proposed approach is limited.\n\n2. The approach of this paper seems to be generally applicable to various time-series sensing modalities, and there is no special design unique to the biosignals. I am not sure why you position the paper as one, especially for multi-modal pretraining on biosignals.\n\n3. There lack of comparisons to existing multi-modal contrastive frameworks in the literature [3, 4], so it would be hard to understand its performance in the multi-modal collaboration scenario.\n\n4. There is a comparison between channel-independent learning and conventional approaches that simultaneously consume all channels. In my personal opinion, channel-independent learning might be less efficient in computation (reduced parallelizability) and might not convey the global information (related to all channels) very well, which could be hard to model by simply concatenating extracted channel features.\n\n[1] Yao, Shuochao, et al. \"Stfnets: Learning sensing signals from the time-frequency perspective with short-time Fourier neural networks.\" The World Wide Web Conference. 2019.\n\n[2] Li, Shuheng, et al. \"Units: Short-time Fourier inspired neural networks for sensory time series classification.\" Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. 2021.\n\n[3] Tian, Yonglong, et al. \"Contrastive multiview coding.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16. Springer International Publishing, 2020.\n\n[4] Poklukar, Petra, et al. \"Geometric multimodal contrastive representation learning.\" International Conference on Machine Learning. PMLR, 2022"
                },
                "questions": {
                    "value": "1. How many labels do you use during the fine-tuning stage? What finetuning strategy do you use? Do you only finetune the appended classification layers or update the whole encoder parameters?\n\n2. What is the main design that distinguishes your approach from existing frequency-domain encoders for time-series signals? There is no discussion or experiment on this comparison in the paper.\n\n3. Could you provide an ablation study on turning on/off the channel independent learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734498859,
            "cdate": 1698734498859,
            "tmdate": 1699636671708,
            "mdate": 1699636671708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oCJqB336tU",
                "forum": "t5LXyWbs5p",
                "replyto": "zfuYzXZFm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BHTD"
                    },
                    "comment": {
                        "value": "Dear reviewer BHTD, we appreciate your time and thoughtful feedback. Especially, thank you for evaluating our work as \u201cinteresting and practical\u201d, having \u201cextensive evaluation\u201d, and recognizing our \u201crobustness tests\u201d. We greatly appreciate your positive comments!\n\n---\n### **Novelty of our approach, and why we emphasize pretraining on biosignals**\n\nThank you very much for pointing us to works in the sensing community! We will make sure to include and discuss them in the revised text. Also, thank you for recognizing the novelty of our transformer encoder, MAE paradigm, and attention-based fusion mechanism.\n\nAlthough the channel-independence technique **itself** isn\u2019t novel, we would like to emphasize that our work aims to present a **multimodal pretraining strategy** that could (i) effectively combine information across different modalities including Electroencephalogram (EEG), Electrooculography (EOG), Electromyography (EMG), and Respiration rates (Resp) at the same time during pretraining, and (ii) could be flexibly applied on many other (unseen) modalities (e.g. Electromechanics). To the best of our knowledge, we are the first work that demonstrates that using information from different physiological modalities could improve the performance on downstream tasks.\n\nWe agree that the proposed method is generally applicable to various time-series sensing modalities, and consider it as an advantage of our work. However, we think the challenges, and the enabled applications of our approach are unique in the biosignal field: \n\n1. Firstly, different physiological signals are recorded using different types of sensors placed at various locations on the body. The diversity in sensor technology and placement differentiates them as unique modalities. The signals also differ significantly in terms of their signal characteristics, sampling rate, and effective frequency bands.\n\n2. Secondly, each signal type is affected differently by various types of noise and artifacts, causing large distributional shifts within each modality. In practice, the available combination of those signals also varies, causing large distributional shifts across modalities. The challenges in mapping, aligning, and analyzing data under such distributional shifts are central to research in physiological signal processing.\n\nFor the novelty of our approach, aside from the motivation that we clarified above, we would like to emphasize that our approach is technically distinguished from other works from two perspectives:\n\n1. The most innovative aspect of bioFAME is its use of a fixed-size frequency filter. Inspired by FNO, our approach is specifically designed for biosignals and is significantly different from methods used in vision. The approaches in other works rely on frequency kernel of size $C^{N \\times D}$, where N is the number of patches and D is the dimensionality of each patch. Thus, it cannot be directly transferred on data of different sizes, while our method, which learns queryable frequency filters of size $C^{K \\times D}$, is adept at handling biosignals of varying lengths (e.g. 178 for Epilepsy, 5120 for FD-B) without performance degradation.\n\n2. Our model is also different from Fourier-based approaches that were previously used in physiological signals and the sensing community. Our approach **does not** rely on explicit frequency-space encoder or frequency-space decoder, and does not use reconstruction loss in the frequency space. We found our method more flexible, parameter-efficient, and also easier to implement than other approaches.\n\nWe hope the novelty of the proposed approach isn\u2019t shadowed due to the channel-independence technique we used to improve transferability. Such techniques are carefully selected and combined together to ensure that the final architecture is transferable and flexible!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185617919,
                "cdate": 1700185617919,
                "tmdate": 1700185617919,
                "mdate": 1700185617919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R6816BEvd4",
                "forum": "t5LXyWbs5p",
                "replyto": "zfuYzXZFm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **Other multi-modal contrastive frameworks as baselines**\n\nWe thank the reviewer for their suggestion. To the best of our knowledge, no existing multimodal contrastive framework demonstrated effective performance on multimodal physiological signals. Existing work [1] even suggests that training on multiple different physiological signals gives worse performance, due to the large variation across different signals. We believe that (i) the lack of such baselines **highlights the contribution of our work**, as we are the first work that could effectively use many modalities (EEG, EMG, EOG, Resp) for pretraining; (ii) it might be out of our scope to implement such baselines. \n\n[1] Zhang, Xiang, et al. \"Self-supervised contrastive pre-training for time series via time-frequency consistency.\" Advances in Neural Information Processing Systems 35 (2022): 3988-4003.\n\n---\n### **The pros and cons of channel-independent learning**\n\nWe are grateful for your insightful comments on the computational efficiency and global information representation in channel-independent learning. We agree that the usage of channel independence is a balance between transferability and capacity. To address the capacity concern, bioFAME incorporates a lightweight encoder (Encoder 2) in its architecture. We still selected channel independence to ensure transferability, but the lightweight encoder during pretrained helped the information across different channels and modalities mix and integrate. We note that, without channel independence, many of our experiments cannot be conducted due to the shifts and mismatch of the total amount of channels.\n\n---\n### **Details of experimental settings during fine-tuning stage**\n\nThank you for your question about the detailed experimental settings, we will make sure to revise the text to clarify such experimental details! For the amount of labels used during fine-tuning stage, we presented the information in Appendix Table 7. We conducted full-scale fine-tuning instead of linear probing, and applied such fine-tuning pipeline to all baselines, because we believe it is a more common practice for transformer backbones [1], and is also more effective for contrastive learning methods [2].\n\n[1] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[2] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185665105,
                "cdate": 1700185665105,
                "tmdate": 1700185958253,
                "mdate": 1700185958253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "atEtNDjfaZ",
                "forum": "t5LXyWbs5p",
                "replyto": "zfuYzXZFm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up to the Rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you once again for your insightful feedback. We have carefully revised our manuscript in line with your suggestions and believe we have comprehensively addressed your concerns. Could you kindly confirm whether the revisions align with your expectations? We are available to engage in further discussion or answer any additional questions you may have.\n\nThanks!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587406629,
                "cdate": 1700587406629,
                "tmdate": 1700587406629,
                "mdate": 1700587406629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U6Zb6BmrKo",
                "forum": "t5LXyWbs5p",
                "replyto": "atEtNDjfaZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_BHTD"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Authors' Responses"
                    },
                    "comment": {
                        "value": "Thanks to the authors for your responses. Upon carefully reading your explanations and justifications, as well as comments from other reviewers, my concerns related to its novelty remain. Therefore, I would like to keep my original rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703969228,
                "cdate": 1700703969228,
                "tmdate": 1700703969228,
                "mdate": 1700703969228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FFc2qSYfgI",
            "forum": "t5LXyWbs5p",
            "replyto": "t5LXyWbs5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a Transformer-inspired architecture to learn representations of biosignal time-series in unimodal and multimodal settings with a masked autoencoding pretraining task. The proposed Frequency-aware block performs token mixing in the frequency domain while the masked autoencoding task in the learned latent space is meant to preserve the structure of the learned frequency-based representation during pretraining. The model is pretrained on the SleepEDF dataset (on EEG and/or EMG and EOG channels) and then finetuned on different downstream tasks that contain different biosignal modalities (e.g. EMG, EOG, electromechanical measurements). Evaluation in different settings, e.g. with unimodal pretraining, multimodal pretraining, and with modality substitution or dropout, shows the proposed approach outperforms existing baselines and is robust to changing or missing data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The combination of a new frequency-based Transformer-like block along with a masked autoencoding pretraining task that enables multimodality processing appears novel.\n\nQuality: The manuscript is overall of good quality. The proposed methodology is well-motivated and evaluated in relevant settings.\n\nClarity: The text and results are mostly clearly presented, however some core concepts would benefit from a clearer presentation (see Weaknesses).\n\nSignificance: The presented results which suggest that pretraining on EEG allows a substantial improvement on downstream tasks on different modalities is impressive (in some cases with significantly higher results than reported baselines). If the reported results hold in different multimodal/transfer settings (see Weaknesses) the proposed approach might be very useful on various multimodal tasks."
                },
                "weaknesses": {
                    "value": "- The evaluation seems a bit limited given the claims in the abstract. Since the models are only pretrained on sleep data, I wonder if the downstream performance might reflect the nature of sleep data (or of the specific SleepEDF dataset) rather than the transferability of the representations. Reporting results on more diversified pretraining datasets (e.g. pretraining on epilepsy data to later test on sleep data) would ensure the proposed approach generalizes to additional settings.\n\n- Although the text is written clearly, some parts of the pipeline would benefit from additional explanations on the initial MLP (see Q1 below) and the architecture of the second encoder (Q3)."
                },
                "questions": {
                    "value": "1. What does the initial MLP described in Section 4.1 end up learning to do in practice (intuitively and/or actually)? My understanding is that it is mixing temporal information within each patch $s_i$, which would already scramble (some of) the temporal information in higher frequencies that could be relevant for the task and common across modalities. Second, are the MLPs shared between modalities? If the sampling rates are not the same then I assume different MLPs would be required unless patches capture different time lengths depending on the modalities. On this topic, what are $N$ and $P$ in practice for the different modalities/datasets?\n\n2. What is the architecture of the \u201csecond encoder\u201d as analyzed in Table 3? To confirm, is this the same thing as the \u201clightweight\u201d encoder of Section 4.2, first paragraph? Its exact role is unclear to me.\n\n3. Have the authors tried a linear probing evaluation instead of full finetuning as in Table 1? This would provide a clearer evaluation of whether the model truly learned a \u201cgeneric\u201d representation of biosignal time series during pretraining.\n\n4. How many parameters does the final encoder model(s) contain?\n\n5. Did the authors evaluate the impact of using normalization (e.g. layer norm) and/or positional/temporal encoding as is commonly done with Transformers, or does the frequency-aware module not require such components?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796838928,
            "cdate": 1698796838928,
            "tmdate": 1700564723764,
            "mdate": 1700564723764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7WZmfIXVQz",
                "forum": "t5LXyWbs5p",
                "replyto": "FFc2qSYfgI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YFSy"
                    },
                    "comment": {
                        "value": "Dear reviewer YFSy, we appreciate your time and insightful feedback. Especially, thank you for evaluating our work as \u201cnovel\u201d, \u201cwell-motivated and evaluated\u201d, and has \u201csubstantial improvements that are impressive\u201d. We greatly appreciate your positive comments!\n\n---\n### **Additional experiments to further support the claims**\n\nWe believe your comment \u201cmodels are only pretrained on sleep data, \u2026 might reflect the nature of sleep data\u201d is very insightful and spot on. We do believe that the performance of our model would be greatly affected by its pretraining dataset, similar as foundation models in many fields [1, 2, 3]. \n\nHowever, we believe that this would not dim the effectiveness of our approach. To validate that our pretraining strategy outperforms other strategies under diverse pretraining datasets, we conducted **two additional experiments**: we pretrain bioFAME, and a competitive baseline PatchTST on (i) FDA dataset; (ii) TUH EEG dataset, and transfer the learned representation on new datasets. \n\nThe below table shows the collected results, where bioFAME (S) represents the SleepEDF pretrained model for benchmarking; bioFAME (FDA) represents the FDA pretrained model; and bioFAME (TUH) represents the TUH EEG pretrained model.\n\n| Methods | FD-B   | Epilepsy | ExpEMG | Avg |\n| -------- | ------- | -------- | ------- | ------- |\n| PatchTST (S) | 67.03 | 95.01 | 92.68 | 84.91 |\n| bioFAME (S) | 76.58 | 95.51 | 98.05 | 90.05 |\n| $\\Delta$ | $\\uparrow$ 9.55 | $\\uparrow$ 0.50 | $\\uparrow$ 5.37 | $\\uparrow$ 5.14 |\n\n| Methods | FD-B   | Epilepsy | ExpEMG | Avg |\n| -------- | ------- | -------- | ------- | ------- |\n| PatchTST (FDA) | 88.42 | 84.95 | 89.45 | 87.61 |\n| bioFAME (FDA) | 90.46 | 92.94 | 95.55 | 92.98 |\n| $\\Delta$ | $\\uparrow$ 2.04 | $\\uparrow$ 7.99 | $\\uparrow$ 6.10 | $\\uparrow$ 5.37 |\n\n| Methods | FD-B   | Epilepsy | ExpEMG | Avg |\n| -------- | ------- | -------- | ------- | ------- |\n| PatchTST (TUH) | 50.08 | 82.98 | 78.65 | 70.57 |\n| bioFAME (TUH) | 77.91 | 90.02 | 94.58 | 87.50 |\n| $\\Delta$ | $\\uparrow$ 27.83 | $\\uparrow$ 7.04 | $\\uparrow$ 15.93 | $\\uparrow$ 16.93 |\n\nThere are two major observations: \n(i) bioFAME, as a robust pretraining strategy, outperforms the PatchTST baseline given the same pretraining data in diverse situations.\n(ii) The representation quality of bioFAME highly correlates with the quality of pretraining dataset, and our choice of multimodal sleep dataset gives high-quality representation. Interestingly, pretraining on FDA dataset gives large performance gain on FDB downstream task; while pretraining on the TUH EEG corpus in general gives worse performance, probably due to limited training time (see details below). \n\nBelow are the more specific training & dataset details:\n\n1. FDA dataset is another corpus of single-channel Electromechanics dataset. We divide the signal into chunks of length 5120, creating a training dataset of sample size 8,000, and pretrain the network on it for 200 epochs with learning rate 1e-3 using an Adam optimizer for both models. \n\n2. TUH EEG is a large-scale abnormal EEG corpus. We preprocess the signal following a public GitHub repo https://github.com/AITRICS/EEG_real_time_seizure_detection, and then chunk the signal into samples of length 4000, creating a 20-channel unimodal (EEG) dataset of sample size 102,903. Due to the limited amount of time, we pretrain the network for 10 epochs with learning rate 1e-3 using an Adam optimizer for both models.\n\nWe identify that large amount of high-quality dataset for pretraining is one of the major challenges of the biosignal field, and hypothesize that sleep data, with its diverse modalities and long period of time available, is an appropriate testbed for our proposed method. We hope our approach, by combining information across modalities for pretraining, can further advocate and encourage the production of such datasets, and benefit the physiological research field.\n\n[1] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[2] Kirillov, Alexander, et al. \"Segment anything.\" arXiv preprint arXiv:2304.02643 (2023).\n\n[3] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185317050,
                "cdate": 1700185317050,
                "tmdate": 1700185317050,
                "mdate": 1700185317050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WunTWUrqMx",
                "forum": "t5LXyWbs5p",
                "replyto": "4YmvoFShIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_YFSy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their answers. Training pipeline and model architecture details are clearer now and the additional results on different pretraining datasets answers my main question. I am improving my score to 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564697996,
                "cdate": 1700564697996,
                "tmdate": 1700564697996,
                "mdate": 1700564697996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "osoeVimV2u",
            "forum": "t5LXyWbs5p",
            "replyto": "t5LXyWbs5p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the pre-training on bio-signals (1D time series neurological signals) based on Masked Modeling strategies like MAE. To effectively pre-training with the bio-signals with potential distributional shifts, the authors design a frequency-aware masked auto-encoder, dubbed bioFAME, to learn bio-signal representations in the Fourier domain. Specifically, a frequency-aware transformer is designed, which leverages a fixed-size Fourier-based operator for token mixing. Meanwhile, the authors propose a frequency-maintain pre-training strategy that performs masking and reconstruction in the latent space to sustain the frequency components. Conducting pre-training and fine-tuning experiments, results on five bio-signal datasets show performance gains of the uni-modality and multi-modality versions of bioFAME."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(**S1**) This paper proposes a frequency-aware Transformer architecture and a latent-space masked auto-encoder pre-training strategy for bio-signals. Analysis and evaluation of the pre-training strategy are extensive. Experiment results demonstrate the effectiveness of both the proposed methods with performance gains and transferring abilities.\n\n(**S2**) The overall writing is easy to follow, and the presentation of texts, tables, and figures is well-arranged and informative."
                },
                "weaknesses": {
                    "value": "(**W1**) Lack of novelty and discussion with existing methods. The proposed method borrows the idea of MAE with the frequency modeling and reconstruction of neurological signals. The proposed frequency-aware transformer seems to borrow a similar design from FNO [1] and Ge$^2$AE relevant works [2, 3, 4] in computer vision. More importantly, the idea of this paper is quite similar to neuro2vec [5] proposed in 2022, which designs a masked modeling pre-training framework for neurological signals in both the spatiotemporal and Fourier domains and conducts experiments on open-source neurological datasets. The authors should discuss the relationship between these existing works and the proposed bioFAME, and point out the necessity of designing the frequency-aware modeling framework for bio-signal representations.\n\n(**W2**) The proposed frequency-aware Transformer encoder lacks analysis. Since the architecture also brings significant performance gains compared to previous networks, the details of network design should be discussed soundly, e.g., the layer number, the embedding dimension, and the utility of the multi-head frequency filter layer.\n\n(**W3**) Concerns about performance gains and experimental settings. Firstly, Compared to prior arts, the improvement of bioFAME is marginal. Also, since not all arts utilize multimodal training, it is only fair to compare bioFAME unimodal with other approaches. The experimental setting of unimodal vs multimodal is very confusing. The authors state that ExpEMG is a dataset of single-channel EMG recordings, how is the multimodal approach applied in this case? This same problem also applies to other datasets.\nSecondly, bio-signal datasets are usually of small size, which leads to the problem of high variance over different random seeds. How did the authors manage to solve this? The authors claim to follow the setup of Zhang et al. [6], yet they [6] reported mean and variance. Why did the authors fail to do so? \nThirdly, since there are contrastive learning baselines (e.g., TS-TCC), do the authors conduct linear evaluations of the learned representations (e.g., linear probing)? Meanwhile, it is better to provide more empirical analysis of the learned embedding, e.g., embedding visualization by tSNE, and reconstruction result visualizations.\n\n(**W4**) The connection between the motivation and approach is vague. The authors claim substantial distributional shifts between the pretraining and inference datasets. How can FREQUENCY-AWARE MASKED AUTOENCODERS solve this? What is expected to be learned from FREQUENCY-AWARE MASKED AUTOENCODERS, and how is this linked to the architecture design of the model?\n\n(**W5**) Details of pre-training and fine-tuning settings on each dataset of bioFAME are vague. Hyper-parameter settings and sensitivity analysis are not available in the main text and appendix.\n\n### Reference\n[1] Li et al. Fourier neural operator for parametric partial differential equations. In ICLR, 2021.\n\n[2] Liu et al. The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training. In AAAI, 2023.\n\n[3] Li et al. Architecture-Agnostic Masked Image Modeling - From ViT back to CNN. In ICML, 2023.\n\n[4] Xie et al. Masked Frequency Modeling for Self-Supervised Visual Pre-Training. In ICLR, 2023.\n\n[5] Wu et al. neuro2vec: Masked Fourier Spectrum Prediction for Neurophysiological Representation Learning. In arXiv, 2022.\n\n[6] Zhang et al. Self-supervised contrastive pre-training for time series via time-frequency consistency. In NeurIPS, 2022."
                },
                "questions": {
                    "value": "(**Q1**) Why did the authors call bioFAME multimodel pre-training? Training on multiple channels on the same modality should not be called multimodal.\n\n(**Q2**) Did the authors provide details of pre-training and fine-tuning settings on each dataset of bioFAME? Hyper-parameter settings and sensitivity analysis are not available in the main text and appendix.\n\n================== Post-rebuttal Feedback ==================\n\nThanks for the detailed rebuttal feedback. However, my main concerns about novelty were not well solved, and I decided to maintain my rating. I apologize again for my late review and reply!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699266806457,
            "cdate": 1699266806457,
            "tmdate": 1700819375940,
            "mdate": 1700819375940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eHLP4nQntS",
                "forum": "t5LXyWbs5p",
                "replyto": "osoeVimV2u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GTFm"
                    },
                    "comment": {
                        "value": "Dear reviewer GTFm, we appreciate your time and valuable comments. Especially, thank you for evaluating our experimental analysis as \u201cextensive\u201d, and recognizing the effectiveness of our proposed method.\n\nIn response to the major criticism about multimodal experimental settings and the confusion regarding \u201chow is the multimodal approach applied\u201d to \u201csingle-channel EMG recordings\u201d, we must clarify that the goal of the paper is to present a **multimodal pretraining strategy** that could (i) effectively combine information across different modalities including Electroencephalogram (EEG), Electrooculography (EOG), Electromyography (EMG), and Respiration rates (Resp) at the same time during pretraining, and (ii) could be flexibly applied on many other (unseen) modalities (e.g. Electromechanics). This is fundamentally different from works like neuro2vec [1], which represents a robust strategy that pretraining and testing on the same physiological modality, and does not evaluate how much adding additional information during pretraining could further help with building robust representation. We hope our approach can fundamentally address the challenge and unify learning on many different physiological signals, which is a problem that has not been systematically tackled before in this community.\n\n[1] Wu, Di, et al. \"neuro2vec: Masked Fourier spectrum prediction for neurophysiological representation learning.\" arXiv preprint arXiv:2204.12440 (2022).\n\n---\n### **W1 - Novelty of our approach and additional discussion with existing methods**\n\nFor the novelty of our approach, aside from the motivation that we clarified above, we would like to emphasize that our approach is technically distinguished from other works from two perspectives:\n\n1. The most innovative aspect of bioFAME is its use of a fixed-size frequency filter. Inspired by FNO [1], our approach is specifically designed for biosignals and is significantly different from methods used in vision [2, 3, 4]. The approaches in vision rely on frequency kernel of size $C^{N \\times D}$, where N is the number of patches and D is the dimensionality of each patch. Thus, it cannot be directly transferred on data of different sizes, while our method, which learns queryable frequency filters of size $C^{K \\times D}$, is adept at handling biosignals of varying lengths (e.g. 178 for Epilepsy, 5120 for FD-B) without performance degradation.\n\n2. Our model is also different from Fourier-based approaches that were previously used in physiological signals [5]. Our approach **does not** rely on explicit frequency-space encoder or frequency-space decoder, and does not use reconstruction loss in the frequency space. We found our method more flexible, parameter-efficient, and also easier to implement than other approaches.\n\nWe thank the reviewer for the comment one more time. We will revise the manuscript to extend the discussion of our approach and its differences with existing methods. We will also make sure to cite and discuss papers [3, 5], as [1, 2, 4] are already discussed in the related works and method preliminaries.\n\n[1] Li et al. Fourier neural operator for parametric partial differential equations. In ICLR, 2021.\n\n[2] Liu et al. The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training. In AAAI, 2023.\n\n[3] Li et al. Architecture-Agnostic Masked Image Modeling - From ViT back to CNN. In ICML, 2023.\n\n[4] Xie et al. Masked Frequency Modeling for Self-Supervised Visual Pre-Training. In ICLR, 2023.\n\n[5] Wu et al. neuro2vec: Masked Fourier Spectrum Prediction for Neurophysiological Representation Learning. In arXiv, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184985441,
                "cdate": 1700184985441,
                "tmdate": 1700184985441,
                "mdate": 1700184985441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vg0zSlrv65",
                "forum": "t5LXyWbs5p",
                "replyto": "osoeVimV2u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-Up to the Rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you once again for your insightful feedback. We have carefully revised our manuscript in line with your suggestions and believe we have comprehensively addressed your concerns. Could you kindly confirm whether the revisions align with your expectations? We are available to engage in further discussion or answer any additional questions you may have.\n\nThanks!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587366614,
                "cdate": 1700587366614,
                "tmdate": 1700587366614,
                "mdate": 1700587366614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "URY9kvBK4E",
                "forum": "t5LXyWbs5p",
                "replyto": "osoeVimV2u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6179/Reviewer_GTFm"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed and comprehensive responses, and sorry for the late reply at the end of the rebuttal period! I appreciate the technical contributions and well-organized writing of this work. However, the authors' responses have not addressed all my concerns, and I list them as follows:\n\n* (W1, W4, Q1) The novelty is my main concern. Firstly, I knew that these works (e.g., works [1-4] mentioned in the authors' responses) I mentioned were proposed in different fields, and this work aims to tackle the pre-training problem of bio-signals in multi-modalities. However, as a new method for the AI community, it is necessary to provide strong motivations and reasons if the new method directly utilizes existing techniques to tackle special problems in downstream applications. Otherwise, Otherwise, I can arrange and combine some related techniques to make another new paper. I suggest the authors clarify these points in the main text (better to use empirical analysis and comparison). Secondly, the relationship and difference between bioFAME and neuro2vec [5] mentioned in the response are not convincing to me. And, I suggest the authors add this discussion in the related works (or some sections in the appendix).\n\n* (W2, W5) These issues were well addressed and added to the revised version. A minor issue is that the FLOPs of bioFAME are relatively high, which will be a limitation of the proposed network architecture.\n\n* (W3, W5, Q2) These issues were almost addressed. A minor issue is the performance gains of bioFAME are not significant on Epilepsy and SleepEOG. Is there any explanation? Since the performance gain of bioFAME comes from both the network architecture and the pre-training, it is better to clarify whether bioFAME can consistently achieve superior performances on different scenarios, or the limitations should be discussed and explained.\n\nOverall, I decided to keep my current rating based on the above issues."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740113829,
                "cdate": 1700740113829,
                "tmdate": 1700740220113,
                "mdate": 1700740220113,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]