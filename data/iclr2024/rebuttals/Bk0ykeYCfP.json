[
    {
        "title": "Analyzing the Effects of Emulating on the Reinforcement Learning Manifold"
    },
    {
        "review": {
            "id": "Ye1EcpYxI1",
            "forum": "Bk0ykeYCfP",
            "replyto": "Bk0ykeYCfP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the robustness of deep neural policies in adversarial settings. It evaluates the robustness of the deep reinforcement learning algorithm and contrasts it with both imitation learning and inverse reinforcement learning methods. The paper also discusses scenarios in which the optimal trajectory changes due to action perturbations in the policy within the inverse reinforcement learning context."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The domain and motivation are compelling, with a focus on understanding the manifold and assessing minor changes for generalization.\n- Attempts have been made to offer both theoretical motivation and empirical evaluation."
                },
                "weaknesses": {
                    "value": "- Throughout the paper, numerous instances of ambiguous language complicate understanding.\n- The connections between Lemmas, Propositions, Corollaries, and their proofs to the paper's central message are unclear.\n- The experiments are missing crucial details, including specifics on the policy architecture, and the explanations of the results are unclear."
                },
                "questions": {
                    "value": "The paper intends to analyze the reinforcement learning manifold. However, the meaning is not evident from the text. Could you clarify what this entails and how the theoretical and empirical analyses relate to it?\n\nIn several sections, the paper mentions \"with reward\" and \"without reward.\" However, their exact meanings are unclear. Does \"without reward\" refer to imitation and inverse RL? Furthermore, \"without reward\" is commonly associated with unsupervised RL, where the agent does not receive a reward signal. Could you please provide clarification? It's crucial to maintain this distinction consistently throughout the paper.\n\nThroughout the paper, several instances of vague language make comprehension challenging. For instance, does \u201cthat focus on learning via emulating affect\u201d refer to imitation learning?\nAnd what exactly is meant by \"learning from exploration\"? Is it referring to regular reinforcement learning?\n\n\n\"Our paper is the first to focus on the adversarial vulnerabilities of deep neural policies that can learn without a reward function.\" Could you clarify how the proposed method learns without a reward function?\n\n\nThe related work is presented without clearly delineating how the current work differs or aligns with it. Providing these comparisons would enhance the related work section.\n\n\nWhile the paper references the general term \"deep reinforcement learning,\" the analysis primarily centers on Q-learning-based approaches. The examination of another form of algorithm, specifically policy gradient, is absent. I suggest the author specify the exact type of algorithm analyzed in the paper for clarity and completeness.\n\nThroughout the paper, I find it challenging to link the various Lemmas, Propositions, Corollaries, and their proofs to the central message of the paper. For instance, the purpose and significance of Lemma 3.1 (and its proof) remain unclear to me. The necessity and meaning behind Propositions 3.3 and 3.5 are also ambiguous. What is the intent of Definition 4.1? How does it relate to the main objective of the paper?\n\nThe caption for Table 1 is unclear and requires significant revision. Both Table 1 and Table 2 mention the tasks Pong and Seaquest, yet the results display three different games. How were these results derived?\n\n\u201cThe state-of-the-art imitation and inverse reinforcement learning policy is trained via the inverse Q-learning algorithm described in Section 2.\u201d However, I do not see this detailed in the paper. How can both imitation and IRL be trained via Q-learning? The explanation seems to be absent and unclear.\n\nFor Figure 1, which environment is being depicted? Is it an average of all three games (Seaquest, BeamRider, and Breakout)? The purpose and message of Figure 1 are unclear.\n\n\nOverall, the writing needs revision. It predominantly consists of lengthy sentences, making comprehension challenging. Addressing this by crafting shorter, simpler sentences would be beneficial.\n\nImplementation details appear to be missing in the paper. Which architecture is used to represent the policy? Which specific algorithms are utilized for IRL and imitation? Without these specifics, it's challenging to justify the performance presented.\n\nWhile the paper seems to concentrate on high-dimensional cases, how do the outcomes vary in a low-dimensional representation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777577129,
            "cdate": 1698777577129,
            "tmdate": 1699636923705,
            "mdate": 1699636923705,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UEeyHFW6cz",
                "forum": "Bk0ykeYCfP",
                "replyto": "Ye1EcpYxI1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for the time you have allocated in providing feedback on our paper.  \n\n---\n\n1. *\u201cIn several sections, the paper mentions \"with reward\" and \"without reward.\" However, their exact meanings are unclear. Does \"without reward\" refer to imitation and inverse RL?\u201d*\n\nYes, without reward refers to imitation and inverse RL.\n\n---\n\n2. *\u201dFurthermore, \"without reward\" is commonly associated with unsupervised RL, where the agent does not receive a reward signal. Could you please provide clarification? \u201c*\n\nYes, as mentioned above without reward refers to imitation and inverse reinforcement learning.\n\n---\n\n3. *\u201cFor instance, does \u201cthat focus on learning via emulating affect\u201d refer to imitation learning?\u201d*\n\nYes, it refers to imitation learning.\n\n---\n\n4. *\u201cWhat exactly is meant by \"learning from exploration\"? Is it referring to regular reinforcement learning?\u201d*\n\nYes, learning from exploration refers to regular reinforcement learning.\n\n---\n\n5. *\u201cCould you clarify how the proposed method learns without a reward function?\u201d*\n\nThe method mentioned here is the inverse Q-learning algorithm which is also explained in Page 3 between the lines 12 and 19 of the Section titled \u201cLearning without Rewards\u201d.\n\n---\n\n6. *\u201cThroughout the paper, I find it challenging to link the various Lemmas, Propositions, Corollaries, and their proofs to the central message of the paper. For instance, the purpose and significance of Lemma 3.1 (and its proof) remain unclear to me. The necessity and meaning behind Propositions 3.3 and 3.5 are also ambiguous.\u201d*\n\nLemma 3.1, Proposition 3.3, and Proposition 3.5 together theoretically demonstrate  that learning only from expert trajectories results in highly non-robust policies in a natural setting. In particular, Lemma 3.1 proves that states in expert trajectories have large projections onto a subspace implicitly defined by the actions leading to high reward. Proposition 3.3 proves that training on states from the subspace described in Lemma 3.1. results in a policy that is essentially untrained (i.e. takes actions no better than random guessing) outside this subspace.  Proposition 3.5 then uses Proposition 3.3 to analyze a natural setup in which a policy trained in the subspace is highly non-robust. The non-robustness and vulnerabilities of these policies are further justified empirically in Section 4 and Section 5.\n\n---\n\n 7. *\u201cThe paper intends to analyze the reinforcement learning manifold. However, the meaning is not evident from the text. Could you clarify what this entails and how the theoretical and empirical analyses relate to it?\u201d*\n\nThe reinforcement learning manifold is represented by the graph of the function $J_{\\pi}(s)$, and thus implicitly given by the policy $\\pi$.\nHere $J_{\\pi}(s)$ is the expected cumulative rewards obtained by policy $\\pi$ starting from state $s$.\nThe paper both theoretically and empirically analyzes how small perturbations from states $s$ on the learned trajectory yield large changes in $J_{\\pi}(s)$ when $\\pi$ is trained from expert trajectories, equivalently corresponding to the manifold represented by $J_{\\pi}(s)$ having steeper slopes near the learned trajectory.\n\n---\n\n\n8. *\u201cWhat is the intent of Definition 4.1? How does it relate to the main objective of the paper?\u201d*\n\nDefinition 4.1 describes the algorithm and MDP independent adversarial direction setting which is a state-of-the-art adversarial framework for producing black-box adversarial perturbations in deep reinforcement learning. \n\n---\n\n9. *\u201cBoth Table 1 and Table 2 mention the tasks Pong and Seaquest, yet the results display three different games. How were these results derived?\u201d*\n\nThe games in the first and fourth row of the Table 1 and 2 represent the MDPs in which the adversarial perturbations are computed. As already described in detail in definition 4.1. the algorithm and MDP independent adversarial direction $\\mathcal{A}_{alg+\\mathcal{M}}^{random}$ computes an adversarial direction independent from the MDP in which the perturbations are added to the state observations.\n\n---\n\n10. *\u201cThe state-of-the-art imitation and inverse reinforcement learning policy is trained via the inverse Q-learning algorithm described in Section 2.\u201d However, I do not see this detailed in the paper. How can both imitation and IRL be trained via Q-learning?\u201d*\n\nPlease see the **supplementary material** for the training details. Furthermore, see the response to bullet point 5.\n\n---\n\n11. *\u201cImplementation details appear to be missing in the paper. Which architecture is used to represent the policy? Which specific algorithms are utilized for IRL and imitation? Without these specifics, it's challenging to justify the performance presented.\u201d*\n\nThis is already currently present in the **supplementary material**. Please see supplementary material."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699970784323,
                "cdate": 1699970784323,
                "tmdate": 1699970784323,
                "mdate": 1699970784323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6e4FbQGiPy",
                "forum": "Bk0ykeYCfP",
                "replyto": "Ye1EcpYxI1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, authors, for the response. However, my assessment of the paper, particularly the weaknesses mentioned earlier, remains unchanged.\n\nMy concern regarding the implementation details and their impact on the reproducibility of the study persists. In point 11, the author refers to supplementary materials for specifics. While I can find the hyperparameters of the algorithms, there is no mention of the policy architecture. For instance, is a neural network used to represent the Q-network, and if so, how many layers does it have, and what are the dimensions of these layers? Since no code is provided, these implementation details are crucial for replicating the experiments. This is also emphasized by reviewer uuJm, who noted that \"...the experimental protocol is not mentioned clearly in the main paper nor the appendix,\" indicating that the lack of detail presents challenges for reproducibility.\n\nCould you please respond to the following question:\n\u201cFor Figure 1, which environment is being depicted? Is it an average of all three games (Seaquest, BeamRider, and Breakout)? The purpose and message of Figure 1 are unclear.\u201d\n\nIn point 10, upon reviewing the supplementary materials, I was unable to locate this specific analysis. Could you mention it here or indicate where this detail can be found?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584864579,
                "cdate": 1700584864579,
                "tmdate": 1700622152207,
                "mdate": 1700622152207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ijs7DoB8bN",
                "forum": "Bk0ykeYCfP",
                "replyto": "wzkoEwAdKj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Reviewer_HmFk"
                ],
                "content": {
                    "comment": {
                        "value": "Dear AC (ujaP),\n\nI have elaborated on my concerns about the paper in my revised response. I appreciate your attention to the details I initially overlooked. I hope that my expanded feedback will be of use to the author."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622601227,
                "cdate": 1700622601227,
                "tmdate": 1700622601227,
                "mdate": 1700622601227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ftJNVDY0D",
            "forum": "Bk0ykeYCfP",
            "replyto": "Bk0ykeYCfP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7613/Reviewer_99sz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7613/Reviewer_99sz"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates the robustness of vanilla reinforcement learning algorithms compared to methods learning from expert demonstrations, such as inverse reinforcement learning (IRL). The study starts from the formulation of theoretical analysis that shows learning from demonstration can produce lower robustness. In particular, the authors argue that perturbations can cause agents to transition into states where the generated reward from IRL and the states are not correlated, eventually decreasing the agent's robustness by making it not achieve optimal returns in the task of interest. The authors then formulated a way to generate perturbations that cause this highlighted problem and showed vanilla reinforcement learning algorithms yield a lower drop in returns compared to methods learning from expert demonstration."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**\n\nTo the best of my minimal knowledge on the topic, the paper appears to be novel. I have not seen other works that establish theoretical analysis on the robustness of inverse reinforcement learning algorithms towards state perturbations.\n\n**Quality - Experiments**\n\nWhile I did not extensively check the theoretical contributions made in this paper, the experiments that empirically demonstrate the issues with the perturbations proposed in the earlier (theoretical) sections seem to be sound. It believe it empirically demonstrates the claim regarding the robustness issues of IRL methods.\n\n**Clarity**\n\nIn general, the paper is well-written. Despite having introduced plenty of theorems throughout the document, the authors did a good job outlining the role of each theorem in highlighting the weaknesses of IRL methods in terms of their robustness. Similarly, I found the experiments (and their analysis) were well written in terms of explaining the overall argument of the paper. \n\n**Significance**\n\nIn general, I find that the problem being tackled in this paper could provide highly valuable results for the broader ICLR community. Most reinforcement learning researchers would be highly concerned with the robustness of the policies they trained. While the theoretical analysis seems limited to inverse RL methods, it's still valuable knowledge that perhaps can spur further research in this area. At least, this paper provides the broader RL community with something to consider when choosing between vanilla RL and IRL from expert demonstrations."
                },
                "weaknesses": {
                    "value": "**Clarity - Perturbation used in experiments**\n\nWhile it may be tricky to produce, it may be useful to show what the perturbations used in the environment really look like. I believe this could help readers further understand the type of \"noises\" introduced to demonstrate the claims in this paper."
                },
                "questions": {
                    "value": "I do not have further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848878093,
            "cdate": 1698848878093,
            "tmdate": 1699636923596,
            "mdate": 1699636923596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h0CuLlHJyv",
                "forum": "Bk0ykeYCfP",
                "replyto": "7ftJNVDY0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your well-considered and thoughtful review. \n\n---\n\n*\u201cWhile it may be tricky to produce, it may be useful to show what the perturbations used in the environment really look like\u201d*\n\nYes, we will add this information to the supplementary material. Thank you very much for the suggestion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699996792565,
                "cdate": 1699996792565,
                "tmdate": 1699996792565,
                "mdate": 1699996792565,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9ppgrUPmjs",
            "forum": "Bk0ykeYCfP",
            "replyto": "Bk0ykeYCfP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7613/Reviewer_uuJm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7613/Reviewer_uuJm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the robustness of policies derived from expert demonstrations in the context of high-dimensional Markov Decision Processes (MDPs). The key contribution of this study is the demonstration that deep reinforcement learning policies, trained using the actual reward signal, exhibit much greater robustness against adversarial attacks and perturbations, as compared to policies obtained through inverse reinforcement learning.\n\nThe authors further ground their empirical findings with a theoretical framework. They illustrate that in the context of inverse soft-Q-learning with linear policies, the learned rewards are random for states not included within the manifold visited by the expert demonstrations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper studies a relevant problem in light of the current success of RLHF methods and the misalignement problem.\n* The findings of the paper clearly point out a substantial lack of robustness of policies learned by IRL to both adversarial attacks and random perturbations.\n * Section 3 offers interesting insights into the shortcomings of linear Q-function approximators in the Inverse Soft Q-Learning setting."
                },
                "weaknesses": {
                    "value": "* The comparisons between the vanilla policies and those derived from IRL are unfair as the vanilla policy is trained on hundreds of thousands of tranisitions whereas its IRL counterpart gets to see only few thousands of transitions.\n\n* The case of studied linear Q-function approximators in the theoretical part is quite restrictive. The assumptions used for Proposition 3.5 are very strong. A potentially more general direction would be to study smooth Q-approximators in the case of Lipschitz MDPs.\n\n* Another weakness is that the authors do not offer no solutions to the problem of lack of robustness. This could be in the form of better training methods for the policy like regularization or better strategies to collect demonstrations."
                },
                "questions": {
                    "value": "I have a few questions for the authors :\n\n1. In the background paragraph, you do not distinguish between Imitation Learning and Inverse Reinforcement Learning. Although IRL covers a broader range of algorithms than can learn even from suboptimal data. Could you please clarify your choice?\n\n2. Could you clarify what you mean by \"Manifold setting\" in the introduction?\n\n3. Could you clarify what $\\phi$ stands for in the inverse Q-learning objective?\n\n4. Relating to the weaknesses, could you provide plots for the evolution of the performance of the IRL policy under adversarial and or perturbation setting in an online setting where it can perform X transitions?\n\n5. I am aware this might not be possible for the current time window. It would've been interesting see a comparison of the robustness of policies learned from expert demonstrations and those learned by ranking of trajectories. As the latter case covers sub-optimal states it can be a potential solution to the robustness problem. Could you add such experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7613/Reviewer_uuJm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699550283131,
            "cdate": 1699550283131,
            "tmdate": 1699636923495,
            "mdate": 1699636923495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yboXUMusUO",
                "forum": "Bk0ykeYCfP",
                "replyto": "9ppgrUPmjs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part I"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time to provide kind feedback on our paper.\n\n---\n\n1. *\u201dThe comparisons between the vanilla policies and those derived from IRL are unfair as the vanilla policy is trained on hundreds of thousands of tranisitions whereas its IRL counterpart gets to see only few thousands of transitions.\u201d*\n\nWe would like to kindly highlight that this statement is **false**. The IRL counterpart experiences up to and more than a million transitions. \n\n---\n\n2. *\u201cThe case of studied linear Q-function approximators in the theoretical part is quite restrictive. The assumptions used for Proposition 3.5 are very strong. A potentially more general direction would be to study smooth Q-approximators in the case of Lipschitz MDPs.\u201d*\n\nAn important point here is that the theoretical part is demonstrating a failure mode of training from expert trajectories. Typically this is done by producing a counter-example MDP in which the failure occurs. Thus, a theoretical demonstration of such a counter-example in a simple setting is a stronger result, as it demonstrates that the algorithm fails even in very simple MDPs. MDPs with bounded, linearly parameterized rewards and transitions are immediately Lipschitz, and thus our counter-example setting does indeed correspond to a Lipschitz MDP. \n\nFurthermore, linear function approximation is currently at the frontier of research that develops a rigorous theoretical understanding of reinforcement learning algorithms [1,2,3,4,5,6,7,8]. Thus, our theoretical results are demonstrated in the most general setting currently amenable to provable mathematical analysis. \n\n[1] Bilinear classes: A structural framework for provable generalization in RL. ICML, 2021.\n\n[2] Reinforcement learning from partial observation: Linear function approximation with provable sample efficiency. ICML 2022.\n\n[3] Provably efficient reinforcement learning with linear function approximation. Conference on Learning Theory. PMLR, 2020.\n\n[4] Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. Conference on Learning Theory. PMLR, 2021.\n\n[5] Learning near optimal policies with low inherent bellman error. ICML, 2020.\n\n[6] Reinforcement learning in linear MDPs: Constant regret and representation selection. NeurIPS 2021.\n\n[7] Nearly minimax optimal reinforcement learning with linear function approximation. ICML, 2022.\n\n[8] First-order regret in reinforcement learning with linear function approximation: A robust estimation approach. ICML 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976181641,
                "cdate": 1699976181641,
                "tmdate": 1699976565837,
                "mdate": 1699976565837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ujTTn2X1IW",
                "forum": "Bk0ykeYCfP",
                "replyto": "9ppgrUPmjs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part II"
                    },
                    "comment": {
                        "value": "---\n\n3. *\u201cAnother weakness is that the authors do not offer no solutions to the problem of lack of robustness. This could be in the form of better training methods for the policy like regularization or better strategies to collect demonstrations.\u201d*\n\nPlease note that in the adversarial machine learning literature there is a line of work dedicated solely to discussing the explicit and concrete vulnerabilities of the models [1,2,3,4,5,6,7], and there is a line of work dedicated to addressing these problems described in these studies [8,9]. We believe these two lines of research progress side by side, without the expectation on one single paper carrying both of these perspectives at the same time.\n\n\n[1] On Adaptive Attacks to Adversarial Example Defenses, NeurIPS 2020.\n\n[2] Label-Only Membership Inference Attacks, ICML 2021.\n\n[3] Poisoning and Backdooring Contrastive Learning, ICLR 2022.\n\n[4] Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness, AAAI 2023.\n\n[5] EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples, AAAI 2018.\n\n[6] Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs, AAAI 2022.\n\n[7] Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning, AAAI 2020.\n\n[8] Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations, NeurIPS 2020.\n\n[9] Robust Deep Reinforcement Learning through Adversarial Loss, NeurIPS 2021.\n\n---\n\n4. *\u201cCould you clarify what you mean by \"Manifold setting\" in the introduction?\u201d*\n\nManifold settings here refers to the first paragraph of the introduction describing the current fields that reinforcement learning is being utilized in, such as autonomous vehicles, finance and large language models.\n\n\n---\n5. *\u201cCould you clarify what $\\phi$ stands for in the inverse Q-learning objective?\u201d*\n\n$\\phi:\\mathbb{R}\\to\\mathbb{R}$ is a concave function corresponding to a choice of distance metric for regularization in inverse Q-learning. For example, setting $\\phi(x) = x$ corresponds to total variation distance, and setting $\\phi(x) = x - x^2/4$ corresponds to $\\chi^2$ distance. Please see [1] for more details.\n\n[1] IQ-Learn: Inverse soft-Q Learning for Imitation, NeurIPS 2021.\n\n---\n\n6. *\u201cRelating to the weaknesses, could you provide plots for the evolution of the performance of the IRL policy under adversarial and or perturbation setting in an online setting where it can perform X transitions?\u201d*\n\nPlease see the response to bullet point 1.\n\n---\n\n7. *\u201dI am aware this might not be possible for the current time window. It would've been interesting see a comparison of the robustness of policies learned from expert demonstrations and those learned by ranking of trajectories. As the latter case covers sub-optimal states it can be a potential solution to the robustness problem. Could you add such experiments?\u201d*\n\nIn the RLHF setting, the trajectories selected for ranking are the outputs of an LLM (e.g. GPT-4), and hence these trajectories analogously are the \u201cexpert trajectories\u201d. Thus, the analysis of our paper extends to the setting of ranking trajectories. Furthermore, note that LLMs have had hard limits applied to the number of rounds of dialogue with users, due to the observations that unlimited conversations steer towards harmful behavior by the LLM (i.e. insulting users, manipulating, or lying) [1].\n\n[1] New York Times. Microsoft to Limit Length of Bing Chatbot Conversations, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976441910,
                "cdate": 1699976441910,
                "tmdate": 1699976616048,
                "mdate": 1699976616048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rPLxNcGPE5",
                "forum": "Bk0ykeYCfP",
                "replyto": "ujTTn2X1IW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Reviewer_uuJm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Reviewer_uuJm"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the authors for their response. However i have some doubts regarding some claims.\n\n> \u201dThe comparisons between the vanilla policies and those derived from IRL are unfair as the vanilla policy is trained on hundreds of thousands of tranisitions whereas its IRL counterpart gets to see only few thousands of transitions.\u201d\nWe would like to kindly highlight that this statement is false. The IRL counterpart experiences up to and more than a million transitions.\n\nThis point deserves further clarification. Indeed, the experimental protocol is not mentioned clearly in the main paper nor the appendix.  Indeed, it is not stated clearly if the experiments follow an offline or online setting for IQ-L. How many expert trajectories and online samples are used.\n\nAccording to the IQ-learn paper, for the Atari games, 20 expert trajectories are used supplemented by up-to 1 Million online interactions with the environment. This is still a magnitude lower than the tens of millions of trajectories used in the Atari games [1].Hence, it would be worth mentioning the number of samples used for the vannila RL method and that used by IQ-Learn. Or even better to perform an ablation on the robustness of IQ-L as a function of the number of online samples.\n\nThe experimental section should be rewritten to better clarify the protocol used for the DQN and IQ-Learn baselines.\n\n> \u201dI am aware this might not be possible for the current time window. It would've been interesting see a comparison of the robustness of policies learned from expert demonstrations and those learned by ranking of trajectories. As the latter case covers sub-optimal states it can be a potential solution to the robustness problem. Could you add such experiments?\u201d\nIn the RLHF setting, the trajectories selected for ranking are the outputs of an LLM (e.g. GPT-4), and hence these trajectories analogously are the \u201cexpert trajectories\u201d. Thus, the analysis of our paper extends to the setting of ranking trajectories. Furthermore, note that LLMs have had hard limits applied to the number of rounds of dialogue with users, due to the observations that unlimited conversations steer towards harmful behavior by the LLM (i.e. insulting users, manipulating, or lying) [1].\n\nIt is not clear how the analysis of the paper extends to rankings of trajectories. Also in a general RL setting like a locomotion task or Atari games, one can imagine having access trajectories of varying performance and their respective rankings. Such trajectories could potentially cover more effectively the state space of MDP and exhibit more robustness. Indeed, in the experimental section, it is only normal for the agent to fail once it visits a single sub-optimal state as it has never encountered such states during training."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570565757,
                "cdate": 1700570565757,
                "tmdate": 1700570565757,
                "mdate": 1700570565757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7kgRTjjcMK",
                "forum": "Bk0ykeYCfP",
                "replyto": "9ppgrUPmjs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7613/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1.** *\u201dIndeed, the experimental protocol is not mentioned clearly in the main paper nor the appendix\u2026How many expert trajectories and online samples are used?...The experimental section should be rewritten to better clarify the protocol used for the DQN and IQ-Learn baselines.\u201d*\n\nThis is indeed mentioned in our supplementary material. Please see Line 14 (i.e. item 9), and Line 18 respectively of the Section Titled \u201cHYPERPARAMETER DETAILS AND ARCHITECTURES\u201d of the supplementary material.\n\n---\n\n**2.** *\u201dIt is not clear how the analysis of the paper extends to rankings of trajectories. Also in a general RL setting like a locomotion task or Atari games, one can imagine having access trajectories of varying performance and their respective rankings. Such trajectories could potentially cover more effectively the state space of MDP and exhibit more robustness. Indeed, in the experimental section, it is only normal for the agent to fail once it visits a single sub-optimal state as it has never encountered such states during training.\u201d*\n\nRanking trajectories requires human feedback (as in RLHF training for LLMs). Thus, increased coverage of the state-space via ranked trajectories comes at a very significant human labor cost, this is a substantially different level of cost than only having a few expert demonstrations, as used in the IQ learn algorithm we study."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682937381,
                "cdate": 1700682937381,
                "tmdate": 1700682955517,
                "mdate": 1700682955517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]