[
    {
        "title": "$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States"
    },
    {
        "review": {
            "id": "6Cl5jxEWTJ",
            "forum": "OUeIBFhyem",
            "replyto": "OUeIBFhyem",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_btvx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_btvx"
            ],
            "content": {
                "summary": {
                    "value": "**Edit: I have raised my score to 6 to reflect the author's updates to the submission.**\n\nThis paper proposes a generalization of diffusion models for data living in infinite dimensional Hilbert spaces. The main motivation for doing so is to enable models which can be trained and sampled at arbitrary resolutions and to enable scaling of diffusion models to high-dimensional data. Section 3 discusses some theoretical concerns for developing such models, Section 4 proposes an architecture well-suited for this task on natural image data, and Section 5 provides an empirical evaluation of the proposed methodology."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed neural architecture is novel and a significant contribution to the literature on infinite-dimensional models. In particular, the proposed model shows significant performance gains in comparison to several previously proposed infinite-dimensional generative models (Table 1) in terms of FID scores.\n- The experiments are generally well-executed and convincing in terms of the conclusions drawn from these throughout the paper."
                },
                "weaknesses": {
                    "value": "- A major weakness of this paper is how the authors frame their contributions within the existing literature. \n     - For example, in the abstract the authors write \"Unlike prior infinite-dimensional models, which use point-wise functions requiring latent compression, our method employs non-local integral operators to map between Hilbert spaces, allowing spatial context aggregation.\" Similarly, in Section 1, the authors write \"We introduce a new Gaussian diffusion model defined in an infinite-dimensional state space that allows infinite resolution data to be generated (see Fig. 2)\". However, numerous recent works have posed infinite-dimensional diffusion models which precisely use these integral operators and a similar theoretical framework [1, 2, 3, 4, 5, 6, 7]. \n     - The authors are clearly aware of this work (see Section 6) but do not appropriately frame their contributions, i.e. many of these prior works develop the theory of infinite-dimensional models which is very closely related to the proposed theory, but the authors of this submission do not state this. The authors claim that these works are concurrent, but the earliest of these works appeared in December 2022 [4], February 2023 [1], and March 2023 [2, 3], more than 6 months prior to the ICLR submission deadline. In addition, the authors are missing a citation to Kerrigan et al. [4] who previously develop a theory for discrete-time diffusion models which is closely related to the theory the authors propose in Section 3, as well as references to several other infinite-dimensional diffusion models in continuous time [5, 6, 7].\n\n- The theory in Section 3 is imprecise to the degree of incorrectness. \n     - The authors write \"The Radon-Nikodym theorem tells us the density for a measure $\\nu$ absolutely continuous with respect to a base measure $\\mu$\". This is not what the Radon-Nikodym says -- the Radon-Nikodym theorem states the *existence* of a density given the absolute continuity. To actually compute this density, you require stronger results, such as the Cameron-Martin and Feldman-Hajek theorems. See [1, 4] for a discussion of these theorems in the context of diffusion models.\n     - The authors miss key regularity assumptions necessary on the Gaussian noise in order to obtain a well-posed model, see again [1, 4] for a discussion of these requirements.\n\n\n### Minor\n- The discussion of the \"diffusion autoencoder framework\" in Section 5 could use a short description for those unfamiliar with the work, i.e. it was not entirely clear to me how model is trained based on the description provided alone.\n\n\n### References:\n[1] [Lim et al., Score-Based Diffusion Models in Function Space](https://arxiv.org/abs/2302.07400)\n\n[2] [Franzese et al., Continuous-Time Functional Diffusion Processes](https://arxiv.org/abs/2303.00800)\n\n[3] [Hagemann et al., Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation](https://arxiv.org/abs/2303.04772)\n\n[4] [Kerrigan et al., Diffusion Generative Models in Infinite Dimensions](https://arxiv.org/abs/2212.00886)\n\n[5] [Lim et al., Score-based Generative Modeling through Stochastic Evolution Equations\n](https://neurips.cc/virtual/2023/poster/72191)\n\n[6] [Baldassari et al., Conditional score-based diffusion models for Bayesian inference in infinite dimensions](https://arxiv.org/abs/2305.19147)\n\n[7] [Pidstrigach et al., Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2302.10130)"
                },
                "questions": {
                    "value": "See weakness section.\n\nOverall, I think this work provides an important and novel practical contribution (namely an empirically sound architecture for infinite-dimensional diffusions), but the contributions regarding the theory are over-stated and the framing of the work with regards to prior work in this area needs to be significantly improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Reviewer_btvx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4814/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705633577,
            "cdate": 1698705633577,
            "tmdate": 1700502093033,
            "mdate": 1700502093033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5xfvVFJNUA",
                "forum": "OUeIBFhyem",
                "replyto": "6Cl5jxEWTJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer btvx"
                    },
                    "comment": {
                        "value": "> **Numerous recent works have posed infinite-dimensional diffusion models\u2026 [The authors] do not appropriately frame their contributions**\n\nThe ICLR guidelines state that these works are considered contemporaneous, and we are not required to compare our paper to them. Specifically, it states that if a paper has not been published at a peer-reviewed venue by May 28th then it is considered contemporaneous. The papers we discuss as concurrent work either exist only on arXiv, or were published after this date (e.g. at NeurIPS 2023). Additionally, our paper was published on arXiv during the months the reviewer states as the \u201cearliest of these works\u201d, in fact, some of these works cite our paper as concurrent work. As such, we believe it is fair to describe our paper as a concurrent work. \n\n> **\u201cThe authors are missing a citation to Kerrigan et al. [4]\u201d and \u201cThe contributions regarding the theory are over-stated\u201d**\n\nThanks for pointing out this missing citation; we were unaware of it and did not intend to overstate our contributions. While our approach was developed entirely independently, with our paper published on arXiv early this year before the conference proceedings of this work, we believe it is important to properly credit prior work. As such, in the introduction, we mention that concurrent works also developed diffusion models in infinite-dimensions; at the start of Section 3 we state that these concurrent works exist, and that they delve deeper into the theory than our paper; and finally, we have expanded the discussion on the concurrent works in Section 6. Throughout the paper we have reduced the strength of our contribution claims and/or also mentioned concurrent work (including those pointed out by the reviewer), and in parts where details are limited compared to these concurrent works (such as on the mentioned Radon-Nikodym theorem and regularity assumptions) we have added references to appropriate papers and stated their contributions.\n\nNonetheless, in terms of the practical aspect, we believe our approach offers substantial contributions extending these works since it is a major challenge to efficiently scale such models to enable practical modelling of complex data. Kerrigan et al. evaluate their approach only on small 2D curve datasets, while the other concurrent works evaluate on simple datasets such as Gaussian mixtures or MNIST (with FID scores substantially worse than baseline finite-dimensional approaches). Our work instead focuses on scaling to substantially more complex, high resolution datasets, developing a practical architecture, subsampling coordinates to achieve speedup/memory gains, and achieving state-of-the-art results for infinite-dimensional models by a wide margin, which presents a very large challenge as highlighted by the other reviewers. As such, with emphasis on such different aspects, we believe our work and each of these concurrent works are very complementary. \n\n> **Discussion on diffusion autoencoder framework**\n\nWe have added more details on the diffusion autoencoder framework to this section to explain how they can be used to reduce stochasticity and allow training with smaller batch sizes. Specifically, that an encoding network is introduced (which in our case operates on sparsely sampled coordinates) to map data points to small latent vectors; our mollified diffusion process is then conditioned on these latent vectors meaning that at the high time steps when the input to the denoising network is mostly noise, the latent provides additional information that allows better estimation of the denoised data, reducing gradient stochasticity. Subsequently, a small diffusion model with MLP backbone can be quickly trained to model these latents to allow unconditional sampling."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482756486,
                "cdate": 1700482756486,
                "tmdate": 1700482756486,
                "mdate": 1700482756486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dBQsHYue2S",
                "forum": "OUeIBFhyem",
                "replyto": "5xfvVFJNUA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_btvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_btvx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. I would like to clarify that I do not expect the authors to perform an experimental comparison to the methods listed, given their (relative) contemporaneity. I believe the changes made to the submission regarding the existing literature in this area is sufficient.\n\nI am willing to raise my score to a 6 given these changes to the submission. However, there still remain a number of errors with regards to the theoretical claims in Section 3. While this paper is primarily a practical contribution, these errors would ideally be fixed in a later version of the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502047427,
                "cdate": 1700502047427,
                "tmdate": 1700502047427,
                "mdate": 1700502047427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fc2TW7sYvB",
            "forum": "OUeIBFhyem",
            "replyto": "OUeIBFhyem",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_iaub"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_iaub"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the generative modeling problem and the diffusion models in particular. The proposed novel diffusion model is defined in an infinite-dimensional Hilbert space in order to possibly model infinite resolution data. The model is trained only on randomly sampled subsets of coordinates and denoising data only there. It allows for learning a continuous function for arbitrary resolution sampling. Whereas the standard infinite-dimensional models use point-wise functions requiring latent compression, the proposed model employs non-local integral operators to map between Hilbert spaces and, as such, allows spatial context aggregation. The method is compared against the state-of-the-art models across different tasks achieving comparable results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper has a few significant strengths overall, which I will outline below:\n1. Proposed model achieve the best or comparable results to the state-of-the-art models.\n2. Formulating the generative diffusion model in an infinite-dimensional Hilbert space and allowing to denoise data only on a subset of coordinates is very interesting. I would assume that it might be to complex problem but maybe the mollification is why the model is able learn?\n3. Formulating the diffusion model in a Hilbert space in a strict mathematical regime, which seems to be correct.\n4. Including additional experiments on not so common tasks like inpainting.\n5. The presentation is very clear. Overall, the flow of the manuscript is well-organized."
                },
                "weaknesses": {
                    "value": "However, despite the strengths, the paper has a few major and minor weaknesses:\n1. The works based on similar ideas are mentioned only in the Discussion section, but the proposed model is not compare against them in an experimental way. \n2. The lack of information regarding the computational costs of both training and inference for the proposed model. \n3. I couldn\u2019t find the information which is actual upscaling procedure limit, which will be significant from the practitioner point of view."
                },
                "questions": {
                    "value": "I would like to see especially the following experiments and improvements regarding specifically to the Weaknesses section:\n1. It will be great if the authors might compare the works mentioned in the Discussion section against their model in the same experimental settings.\n2. I would like to see a new experiment comparing the computational costs of proposed model on different resolutions (e.g., in FLOPs).  \n3. The authors show the results on up to 8x subsampling rate but I\u2019m considering what will be the highest subsampling rate (and resolution) when the model is still doing well. Please, if you could include such comparison."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Reviewer_iaub"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4814/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786142410,
            "cdate": 1698786142410,
            "tmdate": 1699636464791,
            "mdate": 1699636464791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LXsRuskw5N",
                "forum": "OUeIBFhyem",
                "replyto": "Fc2TW7sYvB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iaub"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very helpful feedback and positive comments. \n\n> **The proposed model is not compared against [the similar works] in an experimental way**\n\nOur approach is designed to address the problem of efficient training on high resolution datasets, which we achieve through the combination of our mollified diffusion framework and efficient sparse architecture design. In contrast, the architectures alone used in these concurrent works do not scale well, making training on these impractical (in terms of memory and compute), where these works only consider very simple toy datasets such as Gaussian mixtures or MNIST with FID scores substantially worse than baseline finite-dimensional approaches. These works are considered contemporaneous, which we are not required to compare against.\n\n> **Lack of information regarding the computational costs of both training and inference**\n\nWe agree that more information on computational costs, beyond the details in Section 5 and Table 3, would be useful. Particularly when applied to different resolutions, to assess how our approach scales. Sampling enough images for FID calculations is particularly slow at the highest resolutions, so will add more experiments into the final manuscript. Specifically, we will make a table with various combinations (e.g. data resolution, number of sampled coordinates, and inner grid resolution) and show run-time and memory usage.\n\n> **[What is the] actual upscaling procedure limit?**\n\nTheoretically, the potential maximum resolution is infinite (technically bounded only by computation constraints and floating point precision), since the nature of the model is continuous. While at higher resolutions than the training data, you can\u2019t expect results that contain substantially greater detail than the training data, we do find in Figure 7 sampling at higher resolutions does achieve good FID scores. However, as observed in Figure 2, we find that samples converge in terms of details and as observed by reviewer RCWm, can contain artefacts. Although this is, to the best of our knowledge, the first class of infinite-dimensional diffusion model that can capture non-blurry results higher than the training distribution. In the final manuscript we will add FID scores for even higher resolutions and observe how FID is affected."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482723706,
                "cdate": 1700482723706,
                "tmdate": 1700482723706,
                "mdate": 1700482723706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IuLMhoQdln",
                "forum": "OUeIBFhyem",
                "replyto": "LXsRuskw5N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_iaub"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_iaub"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for their response to my concerns and suggestions. I will be looking forward to see the additional experimental details (like FID scores, FLOPs, etc.) that have been promised by the authors. From my perspective, the most important question related to the paper is the actual scaling limits of the proposed method. I agree that we don\u2019t have any theoretical constraints, but as always, there might be some limits (due to variety of biases) that prevent the method from scaling over the specific threshold. It will be great to see the extended discussion on this topic in the revised or final version of the manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498705021,
                "cdate": 1700498705021,
                "tmdate": 1700498705021,
                "mdate": 1700498705021,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G8TiMBRnBs",
            "forum": "OUeIBFhyem",
            "replyto": "OUeIBFhyem",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_RCWm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_RCWm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an infinite-dimensional diffusion model to handle images at arbitrary resolutions. The authors introduce an architecture based on neural operators that achieves state-of-the-art FID among infinite-dimensional approaches at resolutions up to 256x256."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The network architecture seems to have been well designed, and the results are very convincing. In particular, the use of irregular grids seems to be a good tradeoff between efficiency and retaining fine detail information (particularly if the sampling grid varies from training point to training point, which is not clear)."
                },
                "weaknesses": {
                    "value": "- The paper lists the extension of diffusion models to infinite-dimensional spaces as one of its contributions, but there are already many works on this topic [1-5] (of which only [2-4] are briefly mentioned in the paper). The authors should discuss the relationship of their framework to this related work in greater detail (and they should be mentioned before the discussion). Also, the mathematical treatment of the extension to infinite dimensions in the paper is lacking compared to [1-5] and thus cannot be considered a contribution.\n- While the paper aims to work at any resolution, the use of \"mollification\" (I suggest to simply call this blurring which is a better-known term in the NeurIPS community) on the data distribution effectively limits the maximum resolution of the generated images. A close inspection of Figure 2 also reveals artifacts on the images generated at resolution larger than the training resolution. On a related note, the authors never discuss the mechanisms by which the score network should be expected to generate to higher resolutions while never having observed high-resolution images. Could the authors expand on this point?\n- The central contribution of the paper seems to be its architecture. While its components are described rather abstractly in Section 4, I do not understand what is being implemented exactly. The discretization of the integrals is never discussed in detail, as well as the grid on which the various intermediate activations are represented. When using a regular grid, what is the difference between the proposed architecture and a regular CNN such as UNet? As another important point, how discretization invariance is obtained when using images with different resolutions is not discussed. Does the number of points used in the finite-sum approximation of the integrals changes?\n- Super-resolution and inpainting are rather unfaithful to the original images: downsampling or cropping the generated image does not yield back the input image.\n\n[1] Kerrigan, Gavin, Justin Ley, and Padhraic Smyth. \u201cDiffusion Generative Models in Infinite Dimensions.\u201d arXiv, February 24, 2023. https://doi.org/10.48550/arXiv.2212.00886.\n\n[2] Lim, Jae Hyun, Nikola B. Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, et al. \u201cScore-Based Diffusion Models in Function Space.\u201d arXiv, February 14, 2023. https://doi.org/10.48550/arXiv.2302.07400.\n\n[3] Hagemann, Paul, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, and Nicole Tianjiao Yang. \u201cMultilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation.\u201d arXiv, April 29, 2023. https://doi.org/10.48550/arXiv.2303.04772.\n\n[4] Franzese, Giulio, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi. \u201cContinuous-Time Functional Diffusion Processes.\u201d arXiv, July 7, 2023. https://doi.org/10.48550/arXiv.2303.00800.\n\n[5] Pidstrigach, Jakiw, Youssef Marzouk, Sebastian Reich, and Sven Wang. \u201cInfinite-Dimensional Diffusion Models.\u201d arXiv, October 3, 2023. http://arxiv.org/abs/2302.10130."
                },
                "questions": {
                    "value": "- What is the \"pointwise\" constraint that is mentioned in the abstract and conclusion? Perhaps related, could the authors elaborate what is meant by \"coordinates are treated independently\" in neural fields?\n- In section 4.1, what does the notation $c \\in {m \\choose D}$ means?\n- In equation (13), I am confused by the use of both $x$ and $v_l$. I do not understand if the operator represents a function from $x$ to $s$ or $v_l$ to $v_{l+1}$.\n- I did not understand the second paragraph of section 5 on diffusion autoencoder framework. What do the authors mean by \"using the first half of our architecture\"?\n- What do the authors mean by \"each model being trained to optimize validation loss\" in Appendix A? Is validation data used to train the networks on top of training data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Reviewer_RCWm",
                        "ICLR.cc/2024/Conference/Submission4814/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4814/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810061405,
            "cdate": 1698810061405,
            "tmdate": 1700540461138,
            "mdate": 1700540461138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XfZca5xk3m",
                "forum": "OUeIBFhyem",
                "replyto": "G8TiMBRnBs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RCWm"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive feedback and noting the convincing results. The feedback is much appreciated and strengthened our paper; we\u2019ve addressed the weaknesses and questions point-by-point below:\n\n> **Already many works on this topic [1-5]\u2026 The authors should discuss the relationship of their framework to this related work in greater detail.**\n\nIn the updated paper we have expanded the discussion on these works in Section 6, expanding on each paper, including a discussion on [1], and how our paper fits in. We have also added references throughout the paper to indicate additional mathematical treatment in these works.\n\n> **Mollification\u2026 effectively limits the maximum resolution of the generated images**\n\nBy mollifying, we in effect suppress high-frequency details beyond a specific scale yielding smoother functions with characteristics resembling those from a finite-dimensional subspace. However, it importantly facilitates the recovery of high-frequency details under certain conditions, an advantage over information loss induced by coarse discretisation\u2014as showcased in our super-resolution results (Fig. 8). In practice, we only need to use a very small variance blur, which corresponds to very high resolutions, far exceeding the memory of the subsampled coordinates (up to 8x tested in Table 2), yet still lying in Hilbert space to allow for arbitrary resolution sampling (Figs. 2 and 7) and super-resolution (Fig. 8).\n\n> **Generations at higher resolutions**\n\nThe field of neural operators focuses on learning discretisation invariant operators and has been shown in prior work to generalise to resolutions greater than the training data (Li et al. 2021). While there is always potential for artefacts when sampling at such higher resolutions (also observed with other infinite-dimensional models), in Fig. 7 we evaluate FID at higher resolutions and observe good scores, showing that samples are high quality.\n\n> **How the architecture is implemented**\n\nAt the sparse level integrals are Monte-Carlo approximated: Eqn. 16 becomes $x(c) \\approx \\frac{|N(c)|}{|\\mathbf{c}|}\\sum_{y\\in\\mathbf{c}} \\kappa(c-y)v(y)$. Sampled coordinates do vary across training points, and the same coordinates are used through the forward pass. We found global operators worse and be less scalable than local operators so we interpolate points to/from a regular grid where a model such as a U-NO (Rahman et al. 2022) can be used for global context.\n\nWhen on a regular grid, there are some key differences to regular UNets. Primarily that each layer has a different formulation than typical CNN blocks, see Equation 14. The design of these layers is crucial for generalisation; for example, in Table 2 we show that if the operator kernel is non-linear (the overall operator is still non-linear), it is unable to generalise to different numbers of coordinates/resolutions.\n\n> **Super-resolution and inpainting are unfaithful to the original images**\n\nObtaining better faithfulness when inpainting with diffusion models is a popular research area. Diffusion only in masked areas leads to edge artefacts so approximation approaches such as DDIM inversion (Song et al. 2021) and guidance (Ho et al. 2022b; which we use) were proposed, avoiding artefacts at the expense of perfect reconstruction. More recent work substantially improving this [1] can be directly applied to our method.\n\n> **Elaborate what is meant by \"coordinates are treated independently\"**\n\nNeural fields use an MLP to directly map coordinates to pixel values, e.g for $c \\in [0,1]^2$, the corresponding pixel is $p=\\text{MLP}(c)$ (Sec 2.2). Hence they are independent (pointwise) because the mapping function does not transform over multiple points unlike convolutions/transformers. We have clarified this in Section 2.2.\n\n> **What does $c \\in \\left(\\begin{smallmatrix}D\\\\\\\\m\\end{smallmatrix}\\right)$**\n\nIt is a subset of length $m$ from $D$, where $D=[0,1]^2$ is the set of all coordinates. In practice we uniformly sample m coordinates, but other methods exist (Sec 4.2). We have clarified this in the updated manuscript.\n\n> **Confused between $x$ to $s$ or $v_l$ to $v_{l+1}$**\n\nWe use $\\mathcal{F}$ to denote the entire neural operator, mapping from $\\mathcal{X}$ to $\\mathcal{S}$, and use $v$ to indicate the activations of the network. Therefore $x=v_0$ and $s=v_L$. We have clarified this in this section.\n\n> **What does \u201cfirst half of our architecture\u201d mean?**\n\nFollowing Preechakul et al., 2022, the encoder is the downsampling stage of our architecture with the upsampling and skip connections removed: looking at Figure 4, it is only the left half of the image. We have added more details on diffusion autoencoders to Section 5.\n\n> **What does optimise validation loss mean?**\n\nWe only train on the training data. We use a kept-out validation set to tune hyperparameters, in this case when to stop.\n\n[1] Huberman-Spiegelglas et al. An Edit Friendly DDPM Noise Space: Inversion and Manipulations. arXiv:2304.06140 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482685642,
                "cdate": 1700482685642,
                "tmdate": 1700482685642,
                "mdate": 1700482685642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kiWftbpXcM",
                "forum": "OUeIBFhyem",
                "replyto": "XfZca5xk3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_RCWm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_RCWm"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed answer which addresses my questions. I have raised my score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540447639,
                "cdate": 1700540447639,
                "tmdate": 1700540447639,
                "mdate": 1700540447639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UonpK8Ti6O",
            "forum": "OUeIBFhyem",
            "replyto": "OUeIBFhyem",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_gZH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4814/Reviewer_gZH8"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the mollified diffusion models is proposed to process infinite resolution data efficiently and effectively, to overcome the weakness of low sampling speed of existing diffusion models, where the mollification of data, Fourier neural operators, multi-scale architectures and efficient sparse operators are applied. The experiments are conducted accordingly with impressive results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Originality: It is a novel idea to apply mollification for diffusion models  to enable the data to be processed in the Hilbert space and allow diffusion models to generate high resolution or even infinite resolution data with efficacy. Smoothing using mollification can ensure the regularity of data and enhance enable the modelling in the Hilbert space to reduce the sampling speed.\n\n2. Quality: The experimental results seem to be promising and impressive quantitatively and qualitatively. FIDs of the proposed $\\infty$-Diff are comparative to those of finite-dimensional methods. \n\n3. Significance: In this paper, regularity is discussed in diffusion models. It opens the horizon of future theoretical analysis of diffusion models."
                },
                "weaknesses": {
                    "value": "The writing really needs to be polished. A lot of typos should be corrected, for example, Page 4 Section 3.2 Paragraph 1 Line 11 $\\mu_\\theta: \\mathcal{H}, \\mathbb{R} \\rightarrow \\mathcal{H}$ \u2192 $\\mu_\\theta: \\mathcal{H} \\times \\mathbb{R} \\rightarrow \\mathcal{H}$; Page 5 Section 4 .1  Paragraph 2 Line 1-2 \u201crespectfully\u201d \u2192 \u201crespectively\u201d; etc."
                },
                "questions": {
                    "value": "Q1. What does $x_t(x_0, \\psi)$ mean in Equation (11)? What is the difference between $x_t(x_0, \\psi)$ and $x_t$?\n\nQ2. Could the authors provide any theoretical justification and guarantee on why mollification is necessary and suitable for diffusion models to analyze infinite resolution data?\n\nQ3. Would the authors clarify what $v_0$ means above Equation (14)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4814/Reviewer_gZH8",
                        "ICLR.cc/2024/Conference/Submission4814/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4814/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831733304,
            "cdate": 1698831733304,
            "tmdate": 1700554859760,
            "mdate": 1700554859760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7yXBTSbm9E",
                "forum": "OUeIBFhyem",
                "replyto": "UonpK8Ti6O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gZH8"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and helpful suggestions and for noting the originality, quality, and significance of our approach. We address the weaknesses and questions below point-by-point.\n\n> **Typos should be corrected**\n\nThanks for spotting these typos, we have corrected them and revised similar cases in the updated paper.\n\n> **What does $x_t(x_0, \\xi)$ mean? What is the difference between $x_t(x_0, \\xi)$ and $x_t$**\n\n$x_t(x_0, \\xi)$ is a reparameterization of $x_t \\sim q(x_t | x_0)$. That is, for a data point $x_0$ and Gaussian noise sample $\\xi$, $x_t(x_0, \\xi)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\xi$. We have added this definition to the updated paper.\n\n> **Theoretical justification and guarantee on why mollification is necessary and suitable**\n\nGaussian white noise for finite-dimensional diffusion models is defined such that each dimension is independent. In infinite-dimensional Hilbert spaces, white noise defined similarly exhibits discontinuities that mean that it does not lie in this space. Specifically, this is because it does not satisfy the trace-class requirement, that is, that the covariance operator $C$ must satisfy $\\int_{\\mathcal{H}} \\| x \\|_{\\mathcal{H}} d\\mu(x) = \\text{tr}(C) < \\infty$ (Da Prato and Zabczyk, 2014). Mollification involves convolving white noise with a mollifier kernel  $k(s)>0$ that corresponds to a linear operator $T$, resulting in white noise that can be described by $\\mathcal{N}(0, TT^*)$ (Higdon 2002). This alters the white noise to ensure that it lies within Hilbert space and satisfies the trace-class condition (Sec. 3.1). Without mollification, it is much more difficult to tune the noise hyperparameters, leading to the trained model being unable to generalise across different coordinate subsampling rates or different resolutions. We have clarified this in Section 3.2.\n\n> **Clarify what $v_0$ means**\n\n$v_0$, which lies in Banach space, is the input to the deep neural operator-based network. Each subsequent $v_l$ are the activations of the network, which also lie in Banach space, with $v_l \\mapsto v_{l+1}$ being a single neural operator layer. We have clarified this in the updated paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482602854,
                "cdate": 1700482602854,
                "tmdate": 1700482602854,
                "mdate": 1700482602854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PL2OUEfin0",
                "forum": "OUeIBFhyem",
                "replyto": "7yXBTSbm9E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_gZH8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4814/Reviewer_gZH8"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their answers which serve as great clarification of their proposed model. I have raised my score by 1."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4814/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554822892,
                "cdate": 1700554822892,
                "tmdate": 1700554822892,
                "mdate": 1700554822892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]