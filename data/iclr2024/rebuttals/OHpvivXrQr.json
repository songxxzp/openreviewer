[
    {
        "title": "Protein Multimer Structure Prediction via PPI-guided Prompt Learning"
    },
    {
        "review": {
            "id": "cNPmw7OuPM",
            "forum": "OHpvivXrQr",
            "replyto": "OHpvivXrQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce an interesting task of multimer structure prediction in the form of assembly graph where each node is a monomer and each edge represents an assembly action. They propose to first pretrain a model to predict the TM-score between the structure obtained from the given assembly graph and the ground truth, then finetune with prompt and meta-learning to perform link prediction to construct the assembly graph step by step. The prompt is crafted with $l=3$ path to form a 4-node graph so that the link prediction can be implemented as graph-level prediction on small graphs which is well aligned with the pretraining phase."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written and clear. I really enjoy reading the paper.\n2. The paper introduces an interesting task (i.e. multimer structure prediction) to the community with clear formalization (i.e. prediction of the assembly graph given pairwise dimers).\n3. The experiments are solid, testing the performance on multimers ranging from 3 chains to 30 chains. The authors also compare the results when given ground-truth dimers or alphafold-predicted dimers as inputs. The results are promising, exhibiting obvious improvement over baselines."
                },
                "weaknesses": {
                    "value": "1. The $l=3$ graph prompt is proposed to tackle the distribution shift of chain numbers. However, I notice that in section 4.2 the initial embeddings are obtained from the last layer of the pretrained GIN encoder with the full assembly graph as input. This step may already suffer from the distribution shift and produces out-of-distribution embeddings.\n2. The ablation of the pretraining phase is missing. An experiment without pretraining should be conducted to demonstrate the necessity of the proposed pretraining strategy."
                },
                "questions": {
                    "value": "1. Can you show the correlation between the number of chains and the node degrees to directly validate the claim \"multimers with more chains are more likely to yield assembly graphs with high degrees\" in section 3.3?\n2. How is the ablation of the C-PPI modelling strategy implemented?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2316/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2316/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697599315690,
            "cdate": 1697599315690,
            "tmdate": 1699636164031,
            "mdate": 1699636164031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2WMKLMveNJ",
                "forum": "OHpvivXrQr",
                "replyto": "cNPmw7OuPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer y71p (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer **y71p**'s recognition of our paper and his/her valuable time. We will respond to reviewer's insightful suggestions point-by-point.\n\n**[Cons. 1 GIN encoder distribution shift]** We thank the reviewer for the valuable comments.\n\nThe comment relates to the core motivation of this paper. To avoid distribution shift caused by multimers of different scales (chain numbers) in the target task of link prediction, we re-model the link prediction task of **any scale** as a graph-level task for an $l=3$ path (graph). Please kindly note that the node number of such a path is fixed at **4**. \n\nTo also avoid distribution shift in the source task (graph-level regression), we should ensure that the scale $N$ of the source data for pre-training the GNN model is around **4**. We mentioned this operation (in our original manuscript) at the end of the section \"Reformulating the Target Link Prediction Task\" as **\"the pre-trained model should always be fed with graphs of similar scales (i.e., node number ranges from 2 to 7) to the source data.\"**\n\nIn fact, to aviod distribution shift on the GIN encoder, we have tried **(1)** only using multimers with $N=4$ for pre-training; **(2)** using sufficient multimer data of $2\u2264N\u22647$ for pre-training. Empirically, we found that the latter (option) led to significantly better model performance. This might be due to the presence of not only gaps but also common knowledge that can be utilized between different scales. Thus, selecting multimer data close to the $N=4$ scale is the optimal choice.\n\nWe would like to thank the reviewer once again for the insightful comments, and we will provide an analysis of this issue in the revised manuscript.\n\n**[Cons. 2 Ablation of the pre-training]** Thanks for the valuable suggestion. To address the reviewer's concerns, we establish the baseline without pre-training in the following experiments.\n1. **Ours:** Our complete version of the PromptMSP model.\n2. **Ours w/o pt:** Our method without pre-training the GIN encoder $\\theta$ and the task head $\\phi$.\n\nSpecifically, the prompt model still exists in **Ours w/o pt**. Unlike **Ours**, the GIN encoder $\\theta$ and the task head $\\phi$ are initialized on the target task before training and their parameters are then updated together with the prompt model $\\pi$.\n\nThe experimental results **(with AFM-produced dimers)** are as follows:\n\n| Chain number $N$ |  3   |  4   |  5   | 6 | 7 | 8 | 9 | 10 |Overall|\n| -------------- |:----:|:----:|:----:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| Ours w/o pt | 0.64 | 0.60 | 0.45 | 0.38  | 0.40  | 0.33  | 0.34  | 0.34 |0.43|\n| Ours  | 0.71 | 0.66 | 0.54 | 0.50  | 0.46  | 0.46  | 0.45  | 0.40 |0.51|\n\nIt can be seen that pre-training the graph-level regression model has a significant effect on almost all chain numbers and average performance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579290810,
                "cdate": 1700579290810,
                "tmdate": 1700579343352,
                "mdate": 1700579343352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lmtXOzG7z9",
                "forum": "OHpvivXrQr",
                "replyto": "yx5TTiOwcg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response, which largely alleviates my concerns. I will keep my recommendation for acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642310620,
                "cdate": 1700642310620,
                "tmdate": 1700642310620,
                "mdate": 1700642310620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sydWKlDPJr",
            "forum": "OHpvivXrQr",
            "replyto": "OHpvivXrQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_UpSS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_UpSS"
            ],
            "content": {
                "summary": {
                    "value": "The paper treats the problem of multimer assembly: given a set of sequences, and the structure of all possible dimers (e.g. from AF2), we wish to assemble the multimer by iteratively selecting the next chain and aligning dimer structures\u2014represented with an assembly graph. The paper proposes a multi-stage solution to this problem. (1) A GNN is pre-trained to predict the multimer TMScore from an assembly graph (2) The \u201cnext link\u201d prediction problem is  framed as a TMScore prediction over a fictitious assembly graph, i.e., akin to \u201cprompting\u201d the pretrained GNN. This fictitious assembly graph is created by a \u201cprompting model\u201d and its design is inspired by network-based PPI prediction in bioinformatics. (3) The prompting model\u2013which is specific for each multimer size\u2013is obtained via meta-learning, where the meta-training tasks are small multimer sizes, and the meta-tuning tasks are large multimer sizes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper proposes a novel solution to the difficult problem of multimer structure prediction. Multiple strategies are employed to make this extremely data-scarce problem tractable for deep learning. These strategies are impressive in their sophistication and the bar for originality / novelty has clearly been surpassed.\n* The experimental results are good in terms of both performance and runtime relative to the best existing methods.\n* The paper is a nice illustration of the concept of learning on top of foundation models such as AF2, a paradigm which arose in NLP and is becoming increasingly useful in biological ML."
                },
                "weaknesses": {
                    "value": "* The paper integrates multiple technical ideas with a complex problem domain, but unfortunately the presentation is very confusing.\n   * The paper relies on many ideas that are less familiar to the average reader in protein ML. There should be an extensive background section explaining meta-learning, prompt learning, L=3 PPI prediction, etc.\n    * For a procedure with this many moving parts, it is absolutely essential to provide an explicit inference algorithm somewhere.\n    * The paper is made even more confusing by certain particular choices of emphasis which serves only to distract the reader on a first pass.\n        * It is not clear why it is important to emphasize C-PPI vs I-PPI. Perhaps the authors are trying to draw a distinction with MCTS, but this is really not necessary or within scope. Fully appreciating the difference would require a detailed explanation of the MCTS method, which the paper has no time (or need) to fully explain.\n       * The extended discussion in Section 3.3 seems disconnected from the context of the paper and serves only to make it more confusing.\n       * The authors repeatedly distinguish between oligomers and multimers based on size, which is very unconventional and should be fixed.\n    * L=3 PPI prediction is not obvious and is very confusing when referred to in-passing the first few times it is brought up. \n\n* The pipeline seems unnecessarily complicated and poorly justified. All else held equal, solutions to hard problems should be as simple as possible, and complexity (even if novel) should at least be sensible and easy to justify once understood. Here, it is really not clear why the problem requires such a complex formulation. The so-called source task is a nice way of framing the multimer assembly problem to make it much more data-rich. But then, the most natural solution would seem to be to run the TMScore predictor on all possible next-link additions to the current assembly graph. It seems quite convoluted to instead obtain a prompting model to convert each possible next-link prediction to a fictitious 4-graph when a real (N+1)-graph would also seem to work.\n\nJustification for score. Although I like the wealth of ideas presented in the paper, the presentation is too unclear and the complexity insufficiently justified to recommend acceptance in its current form."
                },
                "questions": {
                    "value": "* Can the authors confirm that there is only one pretrained model, despite the discussion in section 3.3?\n* Where are the node embeddings H is used in prompt model? Are only $H_u, H_d$ used?\n* Is there precedent for learning a prompt _model_ that generates a different prompt for each input, as opposed to simply learning a _prompt_?\n* Are the encoder parameters $\\theta$ and task head parameters $\\phi$ ever separated? If not, then denoting them separately only makes the paper more confusing.\n* How is runtime calculated? I assume the dimer structures are completed \"lazily.\" What explains the large gap in runtime relative to MCTS? It would be nice to report the total number of dimer structures \"required\" by MCTS vs the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2316/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2316/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2316/Reviewer_UpSS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698184716977,
            "cdate": 1698184716977,
            "tmdate": 1699636163929,
            "mdate": 1699636163929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y3Ht2tHbWe",
                "forum": "OHpvivXrQr",
                "replyto": "sydWKlDPJr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer UpSS (1/3)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer's insightful suggestions on our paper and his/her valuable time. We sincerely appreciate the reviewer's recognition for the contribution and novelty of this paper, and we apologize for any confusion caused by our presentation.\n\n**[Cons. 1. Background introduction]** Thank you for bringing this important issue to our attention. It is necessary to provide an introduction to the background knowledge involved. Therefore, we will add relevant explanations in the revised manuscript.\n\nFor clarity, we briefly introduce the background knowledge here.\n* Meta-learning is a technique that aims to enable models to quickly and efficiently learn from and adapt to new data. Meta-learning is a training technology rather than a deep neural network. **It is well-known for its ability to learn common knowledge from sufficient data and quickly adapt to (new) scarce data.** For the multimer structure prediction (MSP) task, there is a severe imbalance in the number of scales (chain numbers) among multimers, which can be intuitively well-addressed by meta-learning techniques.\n* In Section 2, we have introduced the background of prompt learning. In short, prompt learning can narrow the gaps between different data or tasks by **introducing learnable flexibility to the model without the need for fine-tuning the pre-trained model.** For the MSP task, we verified that regardless of whether the problem is modeled as TM-Score prediction or link prediction, there is a knowledge gap caused by the difference in chain numbers (scales) among multimers. This challenge leads to poor performance of training a unified model using data from all scales (please kindly refer to the experimental results in **[Cons. 5]**). **Motivated by this, we aim to use prompt learning to establish connections between data of multiple scales, which allows us to train a unified (prompt) model to handle all samples.**\n* **$l=3$ PPI path:** We apologize for the missing explanation. We respectfully explain that $l=3$ path is an important identifier for PPI prediction. The existence of such a path implies that the proteins at both ends of the path will undergo PPI. Since $l=3$ path always has only 4 nodes, this allows us to re-model the link prediction task for multimers of different scales as a 4-node graph-level regression task (TM-Score prediction).\n\nWe sincerely thank the reviewers once again for bringing to our attention the importance of these key background knowledge.\n\n**[Cons. 2. Inference process]** Thanks for the valuable comment. We have reorganized Figure 5 (framework diagram) and added the description of the inference process (Section 4.4).\n\nThe inference process is multi-step. For an $N$-chain multimer, we randomly choose a starting chain and then perform $N-1$ assembly steps in order. During each step, we predict linking probabilities of all possible chain pairs and then select the most possible pair for assembly.\n\n**[Cons. 3. C-PPI, Section 3.3 and scales]** We thank for the valuable suggestions. We respectfully merge these three questions together because they are closely connected.\n\nOne of the purposes of emphasizing C-PPI is indeed to distinguish our method from MCTS and to demonstrate that using only I-PPI modeling for MSP tasks is biased. However, another crucial factor to consider is that predicting C-PPI (i.e. link prediction) with different chain numbers of multimers can lead to the out-of-distribution (OOD) situation for model's generalization. For clarity, we use \"10+1\" to denote predicting the next right link on an already assembled 10-chain multimer. **Such OOD issue means that we struggle to use a model trained on \"10+1\" (C-PPI) data to perform the \"29+1\" prediction.** This is why we emphasize the importance of \"scale\" in Section 3.3 and throughout the entire paper.\n\nWe apologize for any confusion caused by our unclear presentations. We will remove the wording \"oligomers\" and restate the relationship between C-PPI and scale in the introduction part."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561105930,
                "cdate": 1700561105930,
                "tmdate": 1700561616603,
                "mdate": 1700561616603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AKQldrUZUQ",
            "forum": "OHpvivXrQr",
            "replyto": "OHpvivXrQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_6yks"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_6yks"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a sequential protein complex assembly method called PromptMSP. In each assembly step, PromptMSP predicts where a protein should be assembled to the current complex. During training, PromptMSP learns a continuous score for a given protein assembly graph and during testing, it uses the learned score model to find the most likely assembly graph. To avoid training and testing distributional mismatch, PromptMSP employs prompt learning to reduce the gap of input formats. PromptMSP is compared with existing multimer prediction baselines and outperforms AlphaFold-multimer baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed method outperforms AlphaFold-Multimer (AFM), which is impressive.\n* The evaluation setting is comprehensive. It includes both ground truth dimer setting and predicted dimer setting, which ensures a fair comparison with AFM.\n* Ablation studies show that each proposed component is effective.\n* Incorporation of L=3 PPI rule into the inference procedure is an interesting contribution."
                },
                "weaknesses": {
                    "value": "* The method description is very confusing. Figure 5 is very crowded and rather uninformative.\n* It is very hard to understand what meta-learning part (section 4.3) is actually doing. A visual step-by-step illustration of prompt fine-tuning can be helpful.\n* The introduction of prompt fine-tuning seems an overkill. A simpler approach should work equally well. For example, we can adopt a standard autoregressive link prediction algorithm to this problem. In each step, you predict the link between a pair of proteins and train the model to predict the right link given different prefix graphs.\n* Analysis in section 3.3 is unclear. How did you compute Centered Kernel Alignment between two models?\n* It's unclear how a new protein is docked to the current assembly in each step. Did you use EquiDock? If so, how do you ensure that EquiDock is not trained on any of your test set instances?"
                },
                "questions": {
                    "value": "* At test time, what prompt do you provide to the model? It seems that the prompt is basically the assembly graph that model predicted. I don't see why prompt engineering is useful during training.\n* It would be helpful to report model performance for each number of chain (from 3 to 30)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791868932,
            "cdate": 1698791868932,
            "tmdate": 1699636163828,
            "mdate": 1699636163828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDEkbli0Rr",
                "forum": "OHpvivXrQr",
                "replyto": "AKQldrUZUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer 6yks (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer **6yks** for acknowledging our method as \"impressive\", our experiments as \"comprehensive\" and our prompt method as \"interesting contribution\". Many thanks for the constructive feedback especially for the questions about the presentation and our detailed settings. **These all help us to further enhance the readability and the quality of our paper.** \n\nTo address reviewer 6yks\u2019s concerns, we provide point-wise responses below.\n\n**[Cons 1. Method description and Figure 5's organization]** We sincerely apologize for the confusion caused. To address this issue, we have **(1) clearly described the proposed method and (2) made revisions to Figure 5 and the description of meta-learning part. We sincerely appreciate your suggestions, as they have indeed improved the readability and quality of our paper.**\n\n1. Descriptions: \n\n1.1 **The motivation of our paper:** Under the multimer structure prediction (MSP) task, our main goal is to address the impact of knowledge gap of multimer scales (chain numbers) on C-PPI prediction. \n\n1.2 **C-PPI knowledge gaps:** In Section 3.3, we validated the existence of gaps and connections in C-PPI knowledge implied by multimers of different scales (chain numbers).\n\n1.3 **$\\mathscr{l}=3$ rule prompting (main contribution):** We first **pre-train** the GNN model, which takes an assembly graph as input. Its encoder $\\theta^*$ outputs the node-level embeddings and the head $\\phi^*$ outputs the predicted TM-Score. During tuning, we update only the prompt model $\\pi$ based on the target data (conditional link prediction). For clarity, we refer to the combination of these three modules as a `pipeline`, denoted as $f_{\\pi|\\theta^*,\\phi^*}$. In short, the `pipeline` $f_{\\pi|\\theta^*,\\phi^*}$ outputs the linking probability conditioned on an already assembled portion of the multimer and a newly added chain.\n\n1.4 **Meta-learning:** We respectfully clarify that the meta-learning method we apply is **a training strategy**, rather than **a neural network**. It is responsible for reliable prompt tuning, i.e., to update $\\pi$ within $f_{\\pi|\\theta^*,\\phi^*}$. \n\n2. Revisions: \n\n2.1 **We have re-organized Figure 5,** where we remove the meta-learning part and clearly show the specific process of training (pre-training, prompt-tuning) and testing.\n\n2.2 We explicitly state in Section 4.3 that meta-learning is **a training strategy** and **explain the purpose** of applying this technique.\n\n2.3 We have added a schematic diagram (Figure 7) to show the process for obtaining prompt tuning results.\n\n2.4 We have included a part (Section 4.4) to introduce the inference stages.\n\n**All revisions can be found in our revised manuscript. We sincerely apologize for any confusion we may have caused.**\n\n**[Cons. 2 The meta-learning part's readability]** Thanks for your valuable suggestions. To make the applied meta-learning technique much clearer, we have revised the entire Section 4.3 and **visualized specific cases to illustrate how it is responsible for prompt tuning.**\n\n**In response, we describe the process as follows.**\n* Assuming that the pre-trained model has been well-trained, now we have the GIN encoder $\\theta^*$ and the task head $\\phi^*$. **They will be frozen from now on.**\n* Our goal of prompt tuning is to obtain 2 different prompt models $\\tilde{\\pi}$ and $\\pi^*$ for scales $3\\leq N\\leq 7$ and $N>7$, respectively. For example, to infer a $10$-chain multimer, we perform 9 assembly steps. In each of the first 6 steps, we apply the `pipeline` $f_{\\tilde{\\pi}|\\theta^*,\\phi^*}$ to predict the\nlinking probabilities of all pairs of chains and select the most likely pair for assembly. Upon this assembled $7$-chain multimer, we apply the `pipeline` $f_{\\pi^*|\\theta^*,\\phi^*}$ to sequentially add the remaining 3 chains.\n* **The process of getting $\\tilde{\\pi}$:** We construct a pool of **tasks** from the link prediction datapoints created from multimers of $3\\leq N\\leq 7$. Each task contains two sets (a support set and a query set) of link prediction datapoints. **In each epoch, we do three things in order**. (1) Randomly sample $B$ tasks from the task pool; (2) for each of the $B$ support sets, update a separate model starting from $\\pi$, resulting in $\\pi^{(i)},i-1,...,B$; (3) compute the sum of loss ($\\mathcal{L}\\_{meta}$) of these all $\\pi^{(i)},i=1,...,B$ on the query sets and update $\\pi$ with the gradient of $\\mathcal{L}_{meta}$ with respect to $\\pi$. **After multiple epochs of iteration, we obtain prompt model $\\tilde{\\pi}$.**\n* **The process of getting** $\\pi^*$: We create link prediction datapoints from multimers of $8\\leq N\\leq 30$. Over multiple epochs, we update the previously obtained $\\tilde{\\pi}$ to $\\pi^*$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544271481,
                "cdate": 1700544271481,
                "tmdate": 1700544271481,
                "mdate": 1700544271481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eOv5gyVAsb",
                "forum": "OHpvivXrQr",
                "replyto": "xQt7Rhrin0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Reviewer_6yks"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Reviewer_6yks"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response. Overall, it is still hard to understand the methodology of this paper because it has so many components. Therefore, I would like to keep my original score"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598597885,
                "cdate": 1700598597885,
                "tmdate": 1700598597885,
                "mdate": 1700598597885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mcl1J5QTWG",
                "forum": "OHpvivXrQr",
                "replyto": "AKQldrUZUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up response"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback. We sincerely believe that we have made every effort to organize the paper and that the revised version has greatly improved its clarity and readability.\n\nOverall, our method includes two parts: the model part and the meta-learning part. The basic definition of meta-learning is **Model-Agnostic**, which is a technique that enables the prompt model to obtain more reliable training. In addition, existing work **[re1]** has already demonstrated that meta-learning can significantly improve the effectiveness of prompt techniques.\n\nIf you could provide more specific information on what is unclear, we would be more than happy to continue improving the readability of the paper. Besides, we will release the code and checkpoints to facilitate the reproducibility and further research in this area.\n\n**[re1]** All in One: Multi-Task Prompting for Graph Neural Networks. SIGKDD (best research paper award), 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739531799,
                "cdate": 1700739531799,
                "tmdate": 1700739877724,
                "mdate": 1700739877724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IDjhIDh9CG",
            "forum": "OHpvivXrQr",
            "replyto": "OHpvivXrQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_PqHe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2316/Reviewer_PqHe"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new algorithm to predict multimer structure with multiple chains via a pre-training and prompt tuning framework. The overall idea is novel and interesting. Different from MoLPC, where proteins docking are independent without the consideration of other protein, this method considers the influence of third-party proteins when performing docking. This paper compared several baselines on N chains datasets (N>=3). The experimental results show improvement on AlphaFold-Multimer and MoLPC. Although this paper introduces some new idea, many details are unclear. Also, the baselines are so weak and the experimental setting is not realistic. \nI vote to reject this paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Solving multimer structure prediction via pre-training and prompt tuning is interesting.\n\n- It's reasonable to consider conditional docking for multiple protein."
                },
                "weaknesses": {
                    "value": "- defintion 1 is problematic. Because in real-world setting for docking, monomer's ground-truth structures could not be provided. So that the correctness could never be 1 in real-world setting.\n\n- the baselines are so weak. when taking ground-truth structure as input, HDock[2] and xTrimoDock[3] are strong baselines. \n\n- it could be interesting if you can compare different baselines over different the number of chains. The performance could be reduced when increasing the number of chains. \n\n- missing some related references: [1], [3], [4] [5]\n\n[1] Ghani, Usman, et.al. Improved docking of protein models by a combination of alphafold2 and cluspro.\n\n[2] Yumeng Yan, et.al. The HDOCK server for integrated protein\u2013protein docking.\n\n[3] Yujie Luo, et.al. xTrimoDock: Rigid Protein Docking via Cross-Modal Representation Learning and Spectral Algorithm.\n\n[4] Mohamed Amine Ketata et.al. DIFFDOCK-PP: RIGID PROTEIN-PROTEIN DOCKING WITH DIFFUSION MODELS. use torsional diffusions to solve rigid protein docking, and the source code is released. \n\n[5] Lee-Shin Chu et.al. Flexible Protein-Protein Docking with a Multi-Track Iterative Transformer."
                },
                "questions": {
                    "value": "- when comparing with AlphaFold-Multimer, do you input monomer's ground-truth structure as the template?\n\n- how does your method perform when using predicted monomer structure? is the method robust?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699534831634,
            "cdate": 1699534831634,
            "tmdate": 1699636163758,
            "mdate": 1699636163758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P1hSHwwpUe",
                "forum": "OHpvivXrQr",
                "replyto": "IDjhIDh9CG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2316/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer PqHe (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer **PqHe** for acknowledging our methodology is \u201cinteresting\u201d and our motivation is \u201creasonable\u201d. We sincerely appreciate the reviewer\u2019s valuable time and comments. We provide point-wise responses below.\n\n**[Cons. 1 Monomer's ground-truth structures]** We acknowledge and apologize for the ambiguity in the task objective stated under Definition 1. We will revise it as: *With the above definitions, our paper aims to predict assembly graphs (>>2 proteins chains) that maximize the TM-Scores, taking as inputs the residue sequences of proteins and pre-calculated dimer structures (=2 protein chains).*\n**However, we respectfully clarify that pre-calculated dimer structures can be obtained from AlphaFold-Multimer without the need for providing ground-truth monomers.**\n\nWe strongly agree that the ground-truth monomer or ground-truth dimer structures should not be used in real-world setting. We respectfully emphasize that **our approach did not rely on ground-truth monomer to perform inference in real-world setting**.\n\n|Setting|Inference inputs|Inference intermediate variables |Inference outputs|\n|-|-|-|-|\n|AFM Dimer|Amino acid sequences|Predicted docking path, Dimer structures predicted by AlphaFold-Multimer|Predicted multimer structure|\n|GT Dimer|Amino acid sequences, Ground-truth dimer structures|Predicted docking path|Predicted multimer structure|\n\nIndeed, in this work we propose a framework to predict the best possible protein multimers (>>2 protein chains), taking as inputs the pre-calcuated dimer structures (=2 protein chains). For the pre-calcuated dimer structures, we considered two settings in our **original experiments:** \n* The \"AFM Dimer\" setting does not involve ground-truth monomer nor ground-truth dimer structures. We predict dimers with AlphaFold-Multimer by inputting only amino acid sequences and our model also only takes amino acid sequences as inputs for predicting the docking path. This setting is applicable to real-world scenarios and consistent with the experimental setup of MoLPC **[re1]** (our baseline). \n* The \"GT Dimer\" setting may be less realistic under the application scenarios, but we respectfully think that it is not completely meaningless. The performance under this \"GT Dimer\" setting suggests the effectiveness of the docking paths predicted by ours and the MoLPC baseline.\n\n**[Cons. 2 HDock and xTrimoDock]** We sincerely thank the reviewer's comment. **Firstly, we respectfully clarify once again that our \"AFM Dimer\" setting does not require any ground-truth monomers as input for real-world inference.** \n* **HDock and xTrimoDock may not be suitable for serving as the baselines.** We have checked HDock's local version and online server, and inquired the HDock team about the potential of performing multimer structure prediction (MSP). We find that HDock is currently unable to perform hetero-protein MSP tasks. We also checked all versions related to xTrimoDock **[re2,re3,re4]**, and even went through the supplementary materials in its NeurIPS'23 submission, but we could not find any available code. **Thus, we could not use HDock and xTrimoDock as baselines to directly predicting multimer structures.** \n* However, we greatly appreciate the suggestion for taking HDock into consideration and have added the \"HDock Dimer\" setting, where **we use HDock to pre-calculate dimer structures**.\n* To further demonstrate that our method is not dependent on ground-truth monomers or ground-truth dimers, we also added the \"ESMFold Dimer\" setting, where **we use sequences to predict dimer structures with ESMFold**. Under all three settings, our method consistently and significantly outperforms the MoLPC and AlphaFold-Multimer baselines. \n\n**Overall, we sincerely appreciate the suggestion about HDock which helps yield surprisingly satisfactory experimental results. We consider HDock as a setting to provide pre-calculated dimer structures, rather than a baseline that can directly perform MSP.**\n\n| Setting | AFM Dimer | AFM Dimer | HDock Dimer | HDock Dimer |ESMFold Dimer | ESMFold Dimer |\n|---|---|---|---|---|---|---|\n| Model | Mean TM-Score | Mean RMSD | Mean TM-Score | Mean RMSD | Mean TM-Score | Mean RMSD |\n| AFM | 0.46 | 31.17 | 0.46 | 31.17 | 0.46 | 31.17 |\n| MoLPC | 0.42 | 34.21 | 0.50 | 30.76 | 0.39 | 40.73|\n| **Ours** | **0.51** | **27.77** | **0.54** | **25.39** | **0.49** | **28.34** |\n\nDetailed experimental results will be shown in Table 9 in the Appendix of our revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2316/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142045641,
                "cdate": 1700142045641,
                "tmdate": 1700142045641,
                "mdate": 1700142045641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]