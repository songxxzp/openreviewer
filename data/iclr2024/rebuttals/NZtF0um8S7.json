[
    {
        "title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners"
    },
    {
        "review": {
            "id": "NivLwTO0AD",
            "forum": "NZtF0um8S7",
            "replyto": "NZtF0um8S7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_SaZz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_SaZz"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the potential of Seq2Seq models as robust few-shot learners. A few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. The paper proposes two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, the approach outperforms a decoder-only\nmodel that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written regarding the language and organization.\n2. The experimental evaluation validates their claims.\n3.  The paper performs few-shot learning on a variety of tasks, indicating that the seq-to-seq model can have certain advantages, which is indeed a contribution.\n4. Their analysis in the experimental parts is comprehensive."
                },
                "weaknesses": {
                    "value": "1. The paper might want to explain a bit more about the specific tasks of few-shot learning.\n2. The paper should explain why the seq-to-seq model is powerful in related tasks, from a machine-learning perspective.\n3. Similarity, the paper might want to analyze in-depth why early fusion sometimes yields better performance than late fusion, from a machine-learning perspective."
                },
                "questions": {
                    "value": "No other question, but the model proposed seems too simple. However, the experimental analysis and finding is nontrivial."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3152/Reviewer_SaZz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747238125,
            "cdate": 1698747238125,
            "tmdate": 1699636262926,
            "mdate": 1699636262926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E1Hkm5Yg22",
                "forum": "NZtF0um8S7",
                "replyto": "NivLwTO0AD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SaZz"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer SaZz, for your positive opinion. In the following, let us try to address your concerns.\n\n> The model proposed seems too simple. However, the experimental analysis and finding is nontrivial.\n\n\nThanks again for recognizing the value of our findings. Our work's primary contribution, as recognized by the reviewers, is the rediscovery of in-context few-shot learning capabilities within seq2seq models, realized through comprehensive experimentation. Our endeavor aims to highlight that seq2seq models inherently possess the capability for in-context learning, rather than introducing a novel methodology. Therefore, instead of employing specialized and potentially non-generalizable methods, we focus on applying straightforward and universally applicable methods that are tailored to the innate structure and objectives of seq2seq models. The effectiveness of our proposed objective-aligned prompting and fusion-based approach, while seemingly straightforward, is attributed to their simple yet carefully crafted design, allowing for seamless integration within the seq2seq framework.\n\nAdditionally, a critical aspect of in-context learning capabilities lies in the model itself, which needs to be trained with high-quality data and a well-defined objective to possess higher-order skills. Recently, research focus has shifted predominantly towards decoder-only models, resulting in a relative neglect of seq2seq models in the area of in-context learning. Recognizing this gap, we employed a direct yet impactful strategy to showcase the capabilities of seq2seq models, which we consider to be a meaningful endeavor. Through our work, we expect that seq2seq models will be more deeply researched in the future as Large Language Models. We would be grateful if this aspect is acknowledged as our contribution.\n\n---\n\n> The paper might want to explain a bit more about the specific tasks of few-shot learning.\n\nThank you for pointing it out. We acknowledge that Section 4 of the main paper lacks detailed explanations of each task. To address this, we will include these details in the Appendix of the revised paper.\n\n---\n\n> The paper should explain why the seq-to-seq model is powerful in related tasks, from a machine-learning perspective.\n\nIn the comparison between encoder-decoder and decoder-only models, a key inductive bias of the encoder-decoder models is their bidirectional encoding capability, especially evident in the processing of examples and queries. This bidirectionality facilitates a deeper understanding of the relationships between examples and queries, allowing the encoder to more precisely extract relevant information and the decoder to generate outputs more effectively based on this information. Furthermore, as highlighted in Section 5.4 of the main paper, encoder-decoder models have a strength in handling permutation bias, inherent to their architecture, meaning their performance is less affected by variations in the order of examples.\n\n---\n\n> Similarly, the paper might want to analyze in-depth why early fusion sometimes yields better performance than late fusion, from a machine-learning perspective.\n\nGenerally, the pretraining objective of encoder-decoder models is designed such that the decoder aggregates the outputs of the encoder. This process maximizes the joint probability of the input text sequence for the decoder, achieved through the encoder-decoder attention module. In this context, the early-fusion method implicitly selects examples that assist in resolving the test query by fully leveraging the encoder-decoder attention module. On the other hand, an encoder-decoder model employing a late-fusion method does not differentiate whether individual examples aid in solving the test query; it simply aggregates all responses. In essence, the late-fusion method does not fully utilize the encoder-decoder attention module. We think that this distinction sometimes enables the early-fusion method to outperform the late-fusion method. We will add these details in Section 5 of the revised paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476842643,
                "cdate": 1700476842643,
                "tmdate": 1700476842643,
                "mdate": 1700476842643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JHEEWptyeS",
            "forum": "NZtF0um8S7",
            "replyto": "NZtF0um8S7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_aYaC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_aYaC"
            ],
            "content": {
                "summary": {
                    "value": "An extensive evaluation of zero-shot to few-shot performance of seq2seq models across a wide range of evaluation set is presented. The authors make a case for strong seq2seq model performance for generation and understanding tasks when compared to decoder only models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The primary strength of this works seems to come from experimentally demonstrating that the seq2seq model can outperform the decoder-only model with 6 times larger parameters across diverse datasets."
                },
                "weaknesses": {
                    "value": "Would've liked to see some evaluations around more varied generative tasks like Math/Coding which are more practically useful."
                },
                "questions": {
                    "value": "Are there any tasks where the seq2seq few shot performance was inferior to decoder only models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3152/Reviewer_aYaC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809003065,
            "cdate": 1698809003065,
            "tmdate": 1699636262855,
            "mdate": 1699636262855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b00LFiiQWK",
                "forum": "NZtF0um8S7",
                "replyto": "JHEEWptyeS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aYaC"
                    },
                    "comment": {
                        "value": "We thank Reviewer aYaC for your positive opinion. Below are our thoughts to further address your questions.\n\n> Would've liked to see some evaluations around more varied generative tasks like Math/Coding which are more practically useful. Also, are there any tasks where the seq2seq few shot performance was inferior to decoder only models?\n\nThank you for the insightful question. As you suggested, assessing emerging abilities like math or coding could be a valuable indicator of the practical utility of encoder-decoder models, extending beyond conventional language understanding and generative tasks. However, our method depends on pretrained models, and currently, seq2seq pretrained models face two main issues. \n\nFirstly, they haven\u2019t received as much attention as decoder-only models recently, leading to a lack of models trained on knowledge-intensive data. Our baseline, T5, is solely trained on Common Crawl data, which does not include math and code datasets, making it challenging to reveal these capabilities through in-context learning. For instance, GSM8k score of ****our model(1.6)**** is higher than that of **OPT-66B(1.14)** reported on the Open LLM Leaderboard [1]. However, meaningful comparisons are difficult unless knowledge-intensive datasets are included in the pretrained data. \n\nSecondly, the pretraining objectives of existing seq2seq models aren\u2019t ideally configured for free response generation; their focus was more on improving downstream task capabilities through span corruption and local reconstruction objectives, rather than on language modeling objectives. Consequently, for generative tasks, they might be less effective compared to decoder-only models. To illustrate this, we conducted comparisons with decoder-only baselines on representative generation tasks, XSum and WebNLG, with the results detailed below.\n\n**XSum**\n| Model    | 1-shot (R1/R2/RL) |       5-shot (R1/R2/RL)       |\n|----------|:-----------------:|:-----------------------------:|\n| T5*      |  13.72/2.46/11.97 |         7.57/0.66/6.34        |\n| T5       |  25.12/8.69/20.72 |        26.39/8.99/21.59       |\n| T5-early |         -         | **30.31**/**11.82**/**25.24** |\n| BLOOM-7B |  21.50/4.75/16.33  |        21.96/5.06/17.04       |\n| OPT-13B  |  28.37/9.93/22.46 |       31.32/11.52/24.72       |\n| OPT-30B  |  27.61/10.06/22.20 |       31.54/12.06/25.19       |\n| OPT-66B  | 29.31/10.64/23.45 | **32.52**/**12.86**/**26.19** |\n\n**WebNLG**\n| Model    | 1-shot (R1/R2/RL) |       32-shot (R1/R2/RL)      |\n|----------|:-----------------:|:-----------------------------:|\n| T5*      |  11.44/4.85/10.05 |         0.73/0.12/0.72        |\n| T5       |  49.20/29.22/40.79 |        39.33/23.75/32.72      |\n| T5-early |         -         | **51.11**/**31.16**/**42.52** |\n| BLOOM-7B |  62.77/38.54/51.19|        68.07/43.97/56.10       |\n| OPT-13B  |  55.03/33.04/45.53|       66.26/43.44/55.85       |\n| OPT-30B  |  58.00/35.35/47.78|       66.57/43.07/55.22       |\n| OPT-66B  |  61.44/37.54/50.53| **68.77**/**45.37**/**57.75** |\n\nThe asterisk(\\*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denote ROUGE-1,2,L, respectively. We highlight the scores of both our model and the one exhibiting the best performance. Our best model, T5-early, outperforms OPT-13B in terms of ROUGE-2 and OPT-30B in terms of ROUGE-L on the XSum dataset, despite having only 11B in size. However, it shows slightly lower overall performance compared to OPT-66B. For the WebNLG dataset, our model generally scores lower than decoder-only models. Notably, the significant difference in 5-shot performance between T5\\* and our proposed T5-early model indicates that our method meaningfully enhances the generation performance of the seq2seq model, irrespective of the pretrained language model\u2019s performance. As previously mentioned, with improvements in the seq2seq model\u2019s pretraining data and objectives, we expect a considerable boost in performance. We will include these experiments in the revised paper and mention the limitations.\n\n[1] https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476461101,
                "cdate": 1700476461101,
                "tmdate": 1700489892144,
                "mdate": 1700489892144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CXLFBMHk1N",
            "forum": "NZtF0um8S7",
            "replyto": "NZtF0um8S7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_6VXy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_6VXy"
            ],
            "content": {
                "summary": {
                    "value": "This paper pays attention to the in-context few-shot learning capabilities of seq2seq models. Specifically, this paper conducts comprehensive experiments with an in-context evaluation toolkit to investigate the performance of seq2seq models in few-shot scenarios. In addition, an objective-aligned prompting strategy and a fusion-based approach are proposed. Through extensive experiments, some interesting conclusions are also obtained."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper is well organized and easy to follow. \n2.\tThe motivation is reasonable and experiments are abundant.\n3.\tThe findings and conclusions about in-context few-shot learning capabilities of seq2seq models will be interesting to the community."
                },
                "weaknesses": {
                    "value": "Several main concerns are as follows:\n\n1.\tThis paper claims that the objective-aligned prompting strategy is its one key contribution. However, this strategy seems to be very straightforward and some recent state-of-the-art works have already introduced such a strategy. In this sense, this contribution is somewhat limited.\n\n2.\tThe second contribution of this work is a fusion-based approach, which also comes from the existing works, such as RAG and Fid. Therefore, what\u2019s the main difference and contribution of this work? In addition, in the abstract, the sentence \u201cour approach outperforms a decoder-only model that is six times larger\u2026\u201d shows that the proposed models will be much larger than the competitors. Is it not a significant limitation? \n\n3.\tCan we consider the proposed fusion-based approach as a simple ensemble strategy? If so, the authors may need to explain more for this part.\n\n4.\tAre there any evidences to support the hypothesis in the sentence of \u201cWe hypothesize that the encoding of relations between demonstrations does not significantly impact in-context learning performance.\u201d?"
                },
                "questions": {
                    "value": "Please kindly refer to the above comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845173879,
            "cdate": 1698845173879,
            "tmdate": 1699636262779,
            "mdate": 1699636262779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xwibZpNL3k",
                "forum": "NZtF0um8S7",
                "replyto": "CXLFBMHk1N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6VXy (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review and helpful feedback. We are also pleased that you found our paper as well-organized, reasonable, and interesting to the NLP community. In the following, let us try to address your questions. \n\n> The contribution comes from objective-aligned prompting and fusion-based approach is somewhat straightforward and limited. Some recent state-of-the-art works have already introduced strategies similar to object-aligned prompting. Also, what\u2019s the main difference and contribution of this work compared to FiD and RAG?\n\nOur work's primary contribution, as recognized by the reviewers, is the rediscovery of in-context few-shot learning capabilities within seq2seq models, realized through comprehensive experimentation. Our endeavor aims to highlight that seq2seq models inherently possess the capability for in-context learning, rather than introducing a novel methodology. Therefore, instead of employing specialized and potentially non-generalizable methods, we focus on applying straightforward and universally applicable methods that are tailored to the innate structure and objectives of seq2seq models. The effectiveness of our proposed objective-aligned prompting and fusion-based approach, while seemingly straightforward, is attributed to their simple yet carefully crafted design, allowing for seamless integration within the seq2seq framework.\n\nIn exploring the objective-aligned prompting strategy, we realized that within the seq2seq architecture, there are various ways for structuring inputs. While the approach of simply listing examples as inputs aligns well with the objectives of decoder-only models, it may not be as intuitive for currently trained seq2seq models. We observed that in existing studies, this aspect has often been overlooked, with details of how examples are input remaining undisclosed. Therefore, through our experiments with various ablations (for the first time in our knowledge), we have concluded that aligning with the model\u2019s pretraining objective can indeed yield the most effective results.\n\nIn the exploration of the fusion-based approach, we particularly focused on one of the significant structural advantages of seq2seq models: the capability to configure the encoding part in parallel in an intuitive manner. Consequently, we indeed adopted methods from FiD and RAG. While following the design of this previous work, we diverged in purpose by using multiple encoders instead of retrieval and scoring modules and applied this to the encoder-decoder few-shot scenario. As a result of effectively customizing modules that previously served entirely different functions, we derived novel insights and achieved impressive performance gains. We would be grateful if this aspect is acknowledged as our contribution.\n\n---\n\n> The sentence \u201cour approach outperforms a decoder-only model that is six times larger\u2026\u201d shows that the proposed models will be much larger than the competitors. Is it not a significant limitation?\n\nAs shown in Table 3 of the main paper, our method, utilizing the T5-11B model, surpasses the performance of the OPT-66B model. This means that our approach yields better results than a decoder-only model that is six times larger, indicating that our model is much smaller than the competitors.\n\n---\n\n> Can we consider the proposed fusion-based approach as a simple ensemble strategy? If so, the authors may need to explain more for this part.\n\nAs per your observation, It seems plausible that our approaches could be interpreted as an ensemble strategy. Following your insightful advice, we will add an interpretation from an ensemble perspective in Section 3 of the revised paper. Thank you for your perceptive and valuable advice."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476018487,
                "cdate": 1700476018487,
                "tmdate": 1700493883646,
                "mdate": 1700493883646,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N7rz9AhYfF",
                "forum": "NZtF0um8S7",
                "replyto": "CXLFBMHk1N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6VXy (Part 2/2)"
                    },
                    "comment": {
                        "value": "> Are there any evidences to support the hypothesis in the sentence of \u201cWe hypothesize that the encoding of relations between demonstrations does not significantly impact in-context learning performance.\u201d?\n\nThank you for raising a great question. The key reason why increasing the number of shots improves performance in in-context few-shot learning is that it allows for better generalization of the pattern necessary to answer a given question, by repeatedly exposing the model to multiple examples similar to the query, rather than learning the relationship between the examples.\n\nTo demonstrate this, we conducted experiments to assess performance using an 8-shot learning scheme across four benchmarks: CB, RTE, WSC, and COPA. CB and RTE are designed to evaluate the ability to infer relationships between two sentences, WSC tests commonsense reasoning ability, and COPA measures causal reasoning and sentence completion skills. Our ablation studies included configurations of 1-shot * 8-encoders, 2-shots * 4-encoders, and 4-shots * 2-encoders, aiming to evaluate the impact of the relationship among few-shots within a single encoder. To ensure a fair experiment that neutralizes the effect of the length extrapolation, we designed the experiments so that the average length entering an encoder does not exceed 512 tokens, which is the maximum pretrained length for a T5 encoder. However, we observed maximum input lengths of 620 for the RTE-4 shot and 526, 617, and 763 for the CB-1,2,4 shot, respectively.\n\n| T5-early     |  CB  |  RTE  |  WSC  |  COPA  |\n|--------------|:----:|:-----:|:-----:|:------:|\n| _1-shot*8-enc_ |****76.79****|****84.00****|****64.26****|****67.31****|\n| _2-shot*4-enc_ |****76.79****|82.00|60.29|54.81| \n| _4-shot*2-enc_ |55.36|82.00|55.23|50.00|\n\n| T5-late      |  CB  |  RTE  |  WSC  |  COPA  |\n|--------------|:----:|:-----:|:-----:|:------:|\n| _1-shot*8-enc_ |****75.00****|****83.00****|****64.62****|****62.50****|\n| _2-shot*4-enc_ |71.43|82.00|61.01|48.08| \n| _4-shot*2-enc_ |53.57|80.00|54.51|49.04|\n\nWe highlight the scores for each task that exhibits the best accuracy. Our methodologies, separating each shot into different encoders, attained the highest scores in all scenarios. This experiment supports our assertion that the relationship between few-shot examples does not significantly influence the performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476162926,
                "cdate": 1700476162926,
                "tmdate": 1700490239774,
                "mdate": 1700490239774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kQuIBtVGJ5",
                "forum": "NZtF0um8S7",
                "replyto": "N7rz9AhYfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Reviewer_6VXy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Reviewer_6VXy"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the authors' detailed response. I think most of my concerns have been addressed. Unfortunately, I am not very familiar to the NLP field, so I will discuss with other reviewers to obtain the final score. Thank you."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669415768,
                "cdate": 1700669415768,
                "tmdate": 1700669415768,
                "mdate": 1700669415768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wc7pmEeWrE",
                "forum": "NZtF0um8S7",
                "replyto": "CXLFBMHk1N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive feedback! We have rigorously addressed all of your concerns and clarified points of confusion in our detailed rebuttal. We would also be delighted to provide any further clarifications. As the discussion period draws to a close, we kindly request your consideration in revising the score for our work. Thank you."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734216353,
                "cdate": 1700734216353,
                "tmdate": 1700734560389,
                "mdate": 1700734560389,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RfBA6Ym1Gr",
            "forum": "NZtF0um8S7",
            "replyto": "NZtF0um8S7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_6fMx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3152/Reviewer_6fMx"
            ],
            "content": {
                "summary": {
                    "value": "This paper performs a first-ever extensive experiment comparing the in-context few- shot learning capabilities of decoder-only and encoder-decoder (seq2seq) models on a broad range of tasks. The authors further propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. They show their methods significantly outperform decoder-only models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This work develops an in-context evaluation toolkit for seq2seq models and conduct extensive experiments to investigate the performance of seq2seq models in zero-shot to few-shot scenarios.\n\n+ The author explore prompting strategies and fusion-based approaches in encoder-decoder models, which reveals their ability of zero/few-shot learning.\n\n+ The comprehensive experiments of comparison between decoder-only and encoder-decoder models could be very useful for researchers in this field."
                },
                "weaknesses": {
                    "value": "- The technical novelty of this work is a bit weak. The proposed objective-aligned prompting and fusion-based approach are straightforward.\n\n- The detailed description of the objective-aligned prompting method is missing."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699068803873,
            "cdate": 1699068803873,
            "tmdate": 1699636262716,
            "mdate": 1699636262716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yWSbljb8Iz",
                "forum": "NZtF0um8S7",
                "replyto": "RfBA6Ym1Gr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6fMx"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s constructive comments on our work. Below, we provide our responses to further address your concerns.\n\n> The technical novelty of this work is a bit weak. The proposed objective-aligned prompting and fusion-based approach are straightforward.\n\nOur work's primary contribution, as recognized by the reviewers, is the rediscovery of in-context few-shot learning capabilities within seq2seq models, realized through comprehensive experimentation. Our endeavor aims to highlight that seq2seq models inherently possess the capability for in-context learning, rather than introducing a novel methodology. Therefore, instead of employing specialized and potentially non-generalizable methods, we focus on applying straightforward and universally applicable methods that are tailored to the innate structure and objectives of seq2seq models. The effectiveness of our proposed objective-aligned prompting and fusion-based approach, while seemingly straightforward, is attributed to their simple yet carefully crafted design, allowing for seamless integration within the seq2seq framework.\n\nAdditionally, a critical aspect of in-context learning capabilities lies in the model itself, which needs to be trained with high-quality data and a well-defined objective to possess higher-order skills. Recently, research focus has shifted predominantly towards decoder-only models, resulting in a relative neglect of seq2seq models in the area of in-context learning. Recognizing this gap, we employed a direct yet impactful strategy to showcase the capabilities of seq2seq models, which we consider to be a meaningful endeavor. Through our work, we expect that seq2seq models will be more deeply researched in the future as Large Language Models. We would be grateful if this aspect is acknowledged as our contribution.\n\n---\n\n> The detailed description of the objective-aligned prompting method is missing.\n\nThank you for the valuable suggestion. Objective-aligned prompting is an abstract concept designed to maximize performance during inference by aligning the prompting design with a model\u2019s pretraining objective. Owing to its dependency on pretrained models, we chose to illustrate this concept through practical examples from well-known models (e.g., T5, UL2), rather than providing a theoretical description of the method. However, as you rightly pointed out, the third paragraph of Section 2 lacked a detailed explanation of the connection between a model\u2019s pretraining objectives and objective-aligned prompting. In response, we will enrich the content of Section 2 in the revised paper, offering a more detailed description of this connection."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475985809,
                "cdate": 1700475985809,
                "tmdate": 1700475985809,
                "mdate": 1700475985809,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]