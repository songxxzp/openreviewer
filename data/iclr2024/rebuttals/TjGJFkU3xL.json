[
    {
        "title": "Doubly Robust Proximal Causal Learning for Continuous Treatments"
    },
    {
        "review": {
            "id": "ZT0rrJo9Vr",
            "forum": "TjGJFkU3xL",
            "replyto": "TjGJFkU3xL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4616/Reviewer_eqxp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4616/Reviewer_eqxp"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a kernel-based doubly robust estimator designed for proximal causal learning, particularly adept at handling continuous treatments. The authors demonstrate that this estimator is a consistent approximation of the influence function. They also introduce an innovative method for efficiently resolving nuisance functions. Additionally, the paper includes an in-depth convergence analysis in relation to the mean square error.\n\nThe paper proposes a novel and intriguing problem within the proximal causal learning framework.  Despite the motivation not being particularly compelling, the problem itself remains highly intriguing and of considerable importance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well-structured and flows naturally.\n* This paper proposes an intriguing research topic within proximal causal inference work.\n* Theoretical guarantee is also provided."
                },
                "weaknesses": {
                    "value": "* The inference issue seems to be ignored without the analysis of asymptotical distribution for causal effect. \n\n* The paper\u2019s rationale for the calculation of the influence function appears to be lacking, as it directly selects a specific submodel. It might be more appropriate to refer to the function derived in this paper as the efficient influence function, given that the previous doubly robust estimator for binary treatment is efficient.\n\n* The empirical coverage probability is not given. MSE only contains both bias and variance terms, which may not display the true statistic estimation acuraccy and precision."
                },
                "questions": {
                    "value": "The authors have well clarified their setting and their problem is also novel. I only have several comments below.\n\n* It will be more appropriate to call Assumption 3.2 the \"latent ignorability\" instead of \"conditional randomization\". \n* Although one single optimization reduces computational burden, but asymptotical normality of causal effect estimate may not hold using the proposed kernel estimation method. So please make the motivation clear about why this paper does not focus on the inference issue.\n* Why does the projected residual mean squared error make sense? Does it mean \\hat{q} is consistent for q0?\n* this paper seems to consider the high-dimensional setting in experiments, what about theoretical properties?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4616/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4616/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4616/Reviewer_eqxp"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678017609,
            "cdate": 1698678017609,
            "tmdate": 1699636440575,
            "mdate": 1699636440575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vLu1Xj0c01",
                "forum": "TjGJFkU3xL",
                "replyto": "ZT0rrJo9Vr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eqxp"
                    },
                    "comment": {
                        "value": "We appreciate your efforts in reviewing our paper and constructive comments. We have revised our manuscript based on your advice. We address your concerns in the following.\n\n**Q1.** The paper\u2019s rationale for the calculation of the influence function appears to be lacking, as it directly selects a specific submodel. It might be more appropriate to refer to the function derived in this paper as the efficient influence function, given that the previous doubly robust estimator for binary treatment is efficient.\n\n**A.** While the DR estimator for binary treatment is shown to be the effective influence function, it is unclear whether it still holds for continuous treatments without explicit computation. To verify, we calculate the influence function. \n\n**Q2.** Why does the projected residual mean squared error make sense? Does it mean $\\hat{q}$ is consistent for $q_0$?\n\n**A.** The projected residual mean squared error measures how much $\\hat{q}$ violate Eq. (3). This performance metric holds significant prominence in the theoretical analysis of minimax problems, as noted in references [1, 2, 3]. When the measures of the ill-posedness of inverse problems are bounded, $\\hat{q}$ is consistent for $q_0$. Please refer to Remark 6.3 for details.\n\n**Q3.** Please make the motivation clear about why this paper does not focus on the inference issue about the analysis of asymptotically distribution for a causal effect.\n\n**A.** Currently, we cannot obtain the asymptotic normality due to the error introduced in kernel approximation. With this error, we can show in Theorem E.9 that our estimator is $n^{2/5}$-consistent, while the asymptotic normality means $\\sqrt{n}$-consistent. Besides, according to [4], since the estimand is non-regular, therefore it may not enjoy the properties of $\\sqrt{n}$-consistent and asymptotically normality. We illustrate this point through an empirical study in Appendix E.5.\n\n**Q4.** The empirical coverage probability is not given. MSE only contains both bias and variance terms, which may not display the true statistic estimation accuracy and precision.\n\n**A.** Thank you for your suggestions. We provide in Theorem E.9 that our estimator is $n^{2/5}$-consistent, which means with high probability, the error is $O(n^{-2/5})$. \n\n**Q5.** This paper seems to consider the high-dimensional setting in experiments, what about theoretical properties?\n\n**A.** In this context, we follow the setting in [5,6], in which the term \"high-dimensional\" merely indicates a scenario with relatively more covariates compared to those in section 7.1.1, without implying that the number of samples is smaller than the number of features. Therefore, our theory still applies to this case. \n\n[1] Dikkala, Nishanth, et al. \"Minimax estimation of conditional moment models.\" Advances in Neural Information Processing Systems 33 (2020): 12248-12262.\n\n[2] Ghassami, AmirEmad, et al. \"Minimax kernel machine learning for a class of doubly robust functionals with application to proximal causal inference.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\n[3] Qi, Zhengling, Rui Miao, and Xiaoke Zhang. \"Proximal learning for individualized treatment regimes under unmeasured confounding.\" Journal of the American Statistical Association (2023): 1-14.\n\n[4] Colangelo, Kyle, and Ying-Ying Lee. \"Double debiased machine learning nonparametric inference with continuous treatments.\" arXiv preprint arXiv:2004.03036 (2020).\n\n[5] Xu, Liyuan, Heishiro Kanagawa, and Arthur Gretton. \"Deep proxy causal learning and its application to confounded bandit policy evaluation.\" Advances in Neural Information Processing Systems 34 (2021): 26264-26275.\n\n[6] Kompa, Benjamin, et al. \"Deep learning methods for proximal inference via maximum moment restriction.\" Advances in Neural Information Processing Systems 35 (2022): 11189-11201."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403373131,
                "cdate": 1700403373131,
                "tmdate": 1700403373131,
                "mdate": 1700403373131,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7FvOxTDgCO",
            "forum": "TjGJFkU3xL",
            "replyto": "TjGJFkU3xL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4616/Reviewer_sZEk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4616/Reviewer_sZEk"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for proximal causal inference when dealing with continuous treatments. The proposed method is doubly robust in terms of estimating nuisances."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is technically sound and strong. \n2. The experimental studies are well done. A sufficient amount of empirical evidence for the proposed method is provided."
                },
                "weaknesses": {
                    "value": "1. I believe that the statement of Theorem 4.5 is incorrect. More precisely, the influence function of $B(a)$ does not exist, meaning that $B(a)$ is not pathwise differentiable. However, $B(a; P^{\\epsilon,h_{bw}})$ is pathwise differentiable, indicating that the influence function does exist. Therefore, to accurately state this, the term \"lim\" should be removed and the statement should be made with \"for any $h_{bw} > 0$\".\n2. A practical guide is needed to solve the optimization problem in Equations (8,9) and apply these methods in practice."
                },
                "questions": {
                    "value": "1. I believe that the statement of Theorem 4.5 is incorrect. More precisely, the influence function of $B(a)$ does not exist, meaning that $B(a)$ is not pathwise differentiable. However, $B(a; P^{\\epsilon,h_{bw}})$ is pathwise differentiable, indicating that the influence function does exist. Therefore, to accurately state this, the term \"lim\" should be removed and the statement should be made with \"for any $h_{bw} > 0$\". \n2. I understand that $q_0$ represents the maximum value for the equation in Lemma 5.1. However, I am unsure about the connection between Lemma 5.1 and Equation (8). In other words, why do we need to minimize the empirical quantity for the equation in Lemma 5.1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683554321,
            "cdate": 1698683554321,
            "tmdate": 1699636440500,
            "mdate": 1699636440500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FHJmBhHP4S",
                "forum": "TjGJFkU3xL",
                "replyto": "7FvOxTDgCO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sZEk"
                    },
                    "comment": {
                        "value": "We appreciate reviewer sZEk for the time and effort in reviewing our paper. We address your concerns below.\n\n**Q1.** I believe that the statement of Theorem 4.5 is incorrect. More precisely, the influence function of $B(a)$ does not exist, meaning that $B(a)$ is not pathwise differentiable. However, $B(a,P^{\\varepsilon,h_{bw}})$ is pathwise differentiable, indicating that the influence function does exist. Therefore, to accurately state this, the term `lim' should be removed and the statement should be made with for any $h_{bw}>0$.\n\n**A.** While $\\beta(a,P^{\\varepsilon,h_{bw}})$ is pathwise differentiable, its influence function is not of our interest. Our goal is to obtain the influence function for $\\beta(a)$. To obtain this, we take the gradient with respect to $\\varepsilon$ and let $h$ go to 0. Such a treatment has been similarly adopted in [1]. \n\n**Q2.** A practical guide is needed to solve the optimization problem in Equations (8,9) and apply these methods in practice.\n\n**A.** Equations (8,9) are a minimax optimization problem, and we can solve it in different ways, depending on what function space class we use. For example, We can parameterize $q$ (resp. $h$) and $m$ (resp. $g$) as reproducing kernel Hilbert space (RKHS) with kernel function or neural networks. For the former, we derive their closed solutions in Appx. F. In the latter case, we can employ Generative Adversarial Networks [2]. We have appended this discussion to make it clearer. \n\n**Q3.** I understand that $q_0$ represents the maximum value for the equation in Lemma 5.1. However, I am unsure about the connection between Lemma 5.1 and Equation (8). In other words, why do we need to minimize the empirical quantity for the equation in Lemma 5.1?\n\n**A.** We have modified the description of Lemma 5.1 and added a paragraph to discuss its connection to Eq. (3) and Eq. (8). Simply speaking,  We first show that solving $q_0$ from data is equivalent to minimizing $\\mathcal{L}_q(q;p):=\\mathbb{E} [ \\left( \\mathcal{R} _q\\left( q,p \\right) \\right) ^2 ]$ over $q$, where $\\mathcal{L}_q(q;p)$ is equivalent to the maximization form in Lemma 5.1. \n\n[1] Hidehiko Ichimura and Whitney K Newey. The influence function of semiparametric estimators.\nQuantitative Economics, 13(1):29\u201361, 2022.\n\n[2] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems 27 (2014)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372515663,
                "cdate": 1700372515663,
                "tmdate": 1700372515663,
                "mdate": 1700372515663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pCHsU5aw48",
            "forum": "TjGJFkU3xL",
            "replyto": "TjGJFkU3xL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4616/Reviewer_jsr3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4616/Reviewer_jsr3"
            ],
            "content": {
                "summary": {
                    "value": "This work studies two very important settings in causal inference: continuous treatment & unmeasured confounding. This work introduces a new kernel-based DR estimator designed for continuous treatments, and it presents an efficient approach to solving nuisance functions and demonstrates the estimator's effectiveness through synthetic and real-world data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Clear writing & solid theoretical results"
                },
                "weaknesses": {
                    "value": "The reviewer is not an expert in deep learning for causal inference but would assume there are working targeting the studies setting: causal effect estimation for continuous treatment under potential missing confounders. \n\nThe reviewer understands the page limitation and would recommend a detailed comparison to existing literature in the appendix --- how is this method novel and why this novel kernel modification is necessary to handle the pitfalls in the current literature?\n\nThe reviewer would like to raise the score once there are more literature survey on that direction and added numerical experiments: comparing with **SOTA** DL method on **benchmark** datasets in the revision.\n\nA minor comment: will it be better to call section 2 \"background\" instead of \"related works\"? And it is not clear to the reviewer why \"in this paper\" is highlighted in the second paragraph of section 2..."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4616/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4616/Reviewer_jsr3",
                        "ICLR.cc/2024/Conference/Submission4616/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695172383,
            "cdate": 1698695172383,
            "tmdate": 1700631056042,
            "mdate": 1700631056042,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2FOdKYuOYB",
                "forum": "TjGJFkU3xL",
                "replyto": "pCHsU5aw48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jsr3"
                    },
                    "comment": {
                        "value": "Thank you for the positive assessment and valuable suggestions on our paper. We address your questions below.\n\n**Q1.** Recommending a detailed comparison to the existing literature in the appendix  --- how is this method novel and why this novel kernel modification is necessary to handle the pitfalls in the current literature?\n\n**A.** We have claimed in our **contribution** and the second but last paragraph in the introduction that our method for the first time can estimate the causal effect over continuous variables, even if the unboundedness assumption is violated. To better stand out our uniqueness, we have appended a related work section in **Appx. B** that gives a comprehensive introduction to related works and how our method differs from it. \n\nWe use kernel modification because its kernel approximation is consistent (Theorem 4.2); more importantly, it can approximate the influence function according to Theorem 4.5. \n\n**Q2.** More  numerical experiments.\n\n**A.**  We conduct experiments on four additional settings, encompassing three synthetic data with different data-generating mechaninisms, as well as a time-series forecasting setting. Besides, we compare with two additional DL-based baselines *MINMAX* and *NMMR*. Results in Tab. 1 and Tab. 3 (Appx. G) suggest that our methods can outperform others in these settings. For completeness, we attach the table of additional experiments in the Appx. G to the rebuttal console:\n\n| Dataset     |            |  Size | PMMR      | KPV       | DFPV      | MINIMAX   | NMMR-V    | POR       | PKIPW     | PKDR      |\n|-------------|------------|-------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|   |\n|    \"Hu et al.  | Scenario 1 | 1000  | 0.25\u00b10.05 | 0.25\u00b10.05 | 0.59\u00b10.35 | 1.45\u00b11.32 | 0.17\u00b10.09 | 0.16\u00b10.23 | 0.29\u00b10.12 | **0.16\u00b10.22** |\n|     (2023)\"        | Scenario 2 | 1000  | 0.16\u00b10.02 | 0.16\u00b10.02 | 0.22\u00b10.17 | 0.88\u00b10.29 | **0.05\u00b10.04** | 0.08\u00b10.07 | 0.15\u00b10.06 | 0.07\u00b10.06 |\n|             | Scenario 3 | 1000  | 0.10\u00b10.02 | 0.10\u00b10.02 | 0.28\u00b10.39 | 0.45\u00b10.29 | 0.21\u00b10.10 | 0.22\u00b10.20 | **0.09\u00b10.03** | 0.21\u00b10.19 |\n| Time series |            | 500   | 0.11\u00b10.04 | 0.13\u00b10.12 | 0.20\u00b10.14 | 0.21\u00b10.09 | 0.18\u00b10.12 | 0.10\u00b10.14 | 0.21\u00b10.05 | **0.09\u00b10.12** |\n|             |            | 1000  | 0.10\u00b10.05 | **0.08\u00b10.07** | 0.16\u00b10.21 | 0.22\u00b10.06 | 0.18\u00b10.10 | 0.12\u00b10.12 | 0.20\u00b10.06 | 0.12\u00b10.10 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309303928,
                "cdate": 1700309303928,
                "tmdate": 1700309303928,
                "mdate": 1700309303928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hPaXADiH58",
                "forum": "TjGJFkU3xL",
                "replyto": "2FOdKYuOYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4616/Reviewer_jsr3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4616/Reviewer_jsr3"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer acknowledged that the concerns are addressed and increased the score accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631115947,
                "cdate": 1700631115947,
                "tmdate": 1700631115947,
                "mdate": 1700631115947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]