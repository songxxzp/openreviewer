[
    {
        "title": "Identifying Policy Gradient Subspaces"
    },
    {
        "review": {
            "id": "AryShRBFJc",
            "forum": "iPWxqnt2ke",
            "replyto": "iPWxqnt2ke",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_862E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_862E"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the *existence* of gradient subspaces in policy gradient algorithms. More specifically, they conduct an empirical campaign on two relevant methods (i.e., PPO and SAC) to verify that there exist directions with high curvature spanning a subspace that stays relatively stable throughout the training and that contains the gradient. To this end, the authors propose several numerical setups that verify such an existence. All experiments are conducted both for the actor and the critic gradients. Finally, the authors discuss how the existence of these subspaces could be used in practice to advance the state-of-the-art in policy search algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The existence of gradient subspaces has gathered consistent attention in the supervised learning community. This work aims to empirically show that such subspaces exist in RL as well. Given the numerous challenges that are introduced in RL, the existence of gradient subspaces is not trivial. In this sense, the work done by the authors aims at filling this gap within the literature, and, therefore, I retain it to be interesting for the community. \n\n- The paper is well-written. The main concepts and idea are easy to understand."
                },
                "weaknesses": {
                    "value": "1. The contribution of the paper is focused on the **empirical existence** of gradient subspaces. Although the authors, in Section 5, discuss how the existence of this sub-space could be leveraged in practice, it remains an open question to provide empirical evidence of these eventual benefits.\n2. **Limited experimental campaign**. It has to be noticed that the contributions of this paper are only empirical. As a consequence, I would have expected more experiments to prove the empirical existence of these sub-spaces. Results are limited to 6 domains (i.e., Finger-spin, Ball_in_cup, Ant, HalfCheetah, Pendulum, Walker2D). I invite the authors to expand the set of domains considered.  \n3. **Clarity and writing (minor)**. I would encourage the authors to introduce and follow a more rigorous and formal definition of the elements that are used in the paper (e.g., subspace and so on). All the discussion is, indeed, rather informal."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7121/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7121/Reviewer_862E",
                        "ICLR.cc/2024/Conference/Submission7121/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698160774992,
            "cdate": 1698160774992,
            "tmdate": 1700520125978,
            "mdate": 1700520125978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5idJjNyTD1",
                "forum": "iPWxqnt2ke",
                "replyto": "AryShRBFJc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for this precise evaluation and the insightful suggestions. The report helped to improve our paper. We carefully respond to each point below.\n\n> Although the authors, in Section 5, discuss how the existence of this sub-space could be leveraged in practice, it remains an open question to provide empirical evidence of these eventual benefits.\n\nThere are potentially many ways to exploit subspaces in the context of PG (parameters space exploration and second order optimization being only two of them). Creating methods that effectively exploit this knowledge is a significant effort that is independent of the analysis. The goal of this paper is to understand gradient subspaces in PG methods and share our findings with the RL community. We hope to spark interest in leveraging this phenomenon for the design of PG methods, similarly to how Gur-Ari (2018) encouraged research on gradient subspaces in supervised learning that resulted in a wealth of practical applications(see the methods mentioned in the related work section). For this reason, we hope for your understanding, that advancing PG algorithms with the findings of this paper is left for future work.\n\n\n> I invite the authors to expand the set of domains considered.\n\nWe doubled the number of domains by applying our analysis to six more tasks:  BipedalWalker, Hopper, LunarLanderContinuous, Reacher, and Swimmer from OpenAI Gym (Brockman et al., 2016) and the FetchReach from Gym Robotics (Plappert et al., 2018).\nPlease note that the selection of tasks encompasses a wide variety of different characteristics of RL tasks. The tasks range from low-dimensional classical control tasks (Pendulum) to complex high-dimensional locomotion tasks for diverse embodiments (e.g., Ant, Swimmer) and robotics-inspired tasks (FetchReach). Furthermore, the selection includes sparse (Ball_in_cup and FetchReach) and dense reward (e.g., Ant, Walker2d), as well as single-goal (e.g., Ant, Walker2d) and goal-conditioned (FetchReach and Reacher) tasks.\nWe expanded the plots in Figure 2 to show results for four (instead of the previous two) tasks. Appendix C displays detailed results for all twelve tasks. Consistently across all tasks, a significant fraction of the gradient lies in the high-curvature subspace. Furthermore, on all tasks, there is a substantial overlap between the subspaces at different timesteps. The consistency of these results across such a diverse set of tasks suggests that stable subspaces are a general phenomenon in PGs rather than being task-specific.\n\n> I would encourage the authors to introduce and follow a more rigorous and formal definition of the elements that are used in the paper (e.g., subspace and so on)\n\nWe will add a section to the preliminaries that formally introduces the concept of a subspace and the projections into and out of the subspaces. This section will also state important properties of these matrices. Furthermore, this section will explain the relation between curvature and the Hessian of the objective.\n \nMoreover, we have expanded on the intuition of the criteria in eq. (5) and (7) (eq. (5) and (8) in the revised paper) that we use to evaluate our hypotheses. We added a derivation to Appendix A that shows that the gradient subspace fraction criterion is equivalent to a perhaps more intuitive term (1 - the relative projection error).\n\nWe appreciate the insightful comments by the reviewer and hope to have addressed all doubts about our work. We are committed to responding to the remaining questions, if any exist. We would be truly thankful if the reviewer considered raising the score in case no question remains.\n\n\n### References:\n\nGuy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv preprint arXiv:1812.04754, 2018.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\n\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495558519,
                "cdate": 1700495558519,
                "tmdate": 1700495558519,
                "mdate": 1700495558519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ezuOBJxzkv",
                "forum": "iPWxqnt2ke",
                "replyto": "5idJjNyTD1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_862E"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_862E"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "I thank the authors for their exhaustive comments. In particular, I appreciated the new set of extensive experiments. I have increased my score accordingly. I have no further questions for the authors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520100088,
                "cdate": 1700520100088,
                "tmdate": 1700520100088,
                "mdate": 1700520100088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fj8rBmCgzd",
            "forum": "iPWxqnt2ke",
            "replyto": "iPWxqnt2ke",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_CUg2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_CUg2"
            ],
            "content": {
                "summary": {
                    "value": "Previous literature demonstrated that the gradient in supervised learning could be dominated by some of several high-curvature subspaces. Inspired by this, the authors investigate whether that is true in policy gradient methods and find that similar phenomena appears in both PPO and SAC by checking the gradient subspace fraction. Moreover, the authors demonstrate that these high-curvature subspaces remain stable across the training process with empirical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly presented and easy-to-follow, the motivation and the methodology are clearly described.\n- The work has demonstrated that the subspace exists in policy gradient methods as well, similar to what people have discovered in the supervised learning setup. This is done by relatively comprehensive experiments including different approaches to estimate the policy gradient and Hessian matrix and consider both the policy model and the critic model.\n- The authors also discuss how to leverage the insights to improve RL algorithms such as subspace-based optimization or parameter-space exploration."
                },
                "weaknesses": {
                    "value": "One of the perspectives to make the paper stronger and more convincing is to show the unique conclusion and domain-specific insight for policy gradient learning, since most of the conclusions actually come from the gradient subspace paper under a supervised learning setup."
                },
                "questions": {
                    "value": "One of the perspectives to make the paper stronger and more convincing is to show the unique conclusion of the policy gradient since this is an extension of the gradient subspace in the supervised learning setup. I could imagine more in-depth discussion in the paper could be helpful. For instances:\n\n- Investigate and explain why the PPO/on-policy method has a much lower gradient fraction in Figure 2, and how much of this is due to the data distribution shift.\n\n - Have more experiments/configurations in Figure 2 to make the conclusion and discussion more sound, for instance, whether the PPO low gradient fraction is consistent across more RL tasks or not.\n\n - Have demonstrative experiments to explore one of the potential RL applications (using the insight obtained from this experiment) to make it more convincing that these insights will be helpful for RL training."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825773196,
            "cdate": 1698825773196,
            "tmdate": 1699636842246,
            "mdate": 1699636842246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CIr79mBXzE",
                "forum": "iPWxqnt2ke",
                "replyto": "fj8rBmCgzd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for this insightful evaluation, which helped to improve our paper. We carefully answer each point below.\n\n> [S]how the unique conclusion and domain-specific insight for policy gradient learning\n\nThe paper already contains insights that are specific to the domain of policy gradient / actor-critic methods.\nSections 4.2 and 4.3 highlight the differences in the quality of the gradient subspace for the actor and the critic.\nThe sections further point out differences between on-policy and off-policy learning.\nThe potential application to parameter-space exploration that we discuss in the conclusion is also specific to reinforcement learning.\nAdditionally, we are currently running more experiments that directly compare on-policy and off-policy learning by considering an on-policy variant of SAC. These experiment investigate to which extent the differences in the analysis results of SAC and PPO stem from the fact that SAC reuses data and PPO does not.  The results will allow disentangling the effects of on-policy/off-policy learning from the algorithmic details of the two RL algorithms.\n\n> Investigate and explain why the PPO/on-policy method has a much lower gradient fraction in Figure 2, and how much of this is due to the data distribution shift\n\nAs mentioned in the response to the first question, we will add experiments that consider an on-policy variant of SAC. Like PPO, the on-policy SAC variant collects a small dataset of on-policy transitions at every update, which is discarded after the update. Since the rest of the algorithm stays the same, these experiments will shed light on whether the differences in the results between PPO and SAC are due to the stronger distribution shift in on-policy learning or due to other details of the algorithms.\n\n\n> Have more experiments/configurations in Figure 2\n\nTo increase the confidence that gradient subspaces are a general phenomenon in PG methods, we doubled the number of tasks on which we conducted our analysis by adding the tasks BipedalWalker, Hopper, LunarLanderContinuous, Reacher, and Swimmer from OpenAI Gym (Brockman et al., 2016) and the FetchReach task from Gym Robotics (Plappert et al., 2018). We extended Figure 2 with results for  Ant and LunarLanderContinuous while describing the remaining tasks in Appendix C due to space constraints. Further, we are currently conducting an evaluation of the influence of the hyperparameter configurations on the analysis results, which will be added to the appendix in the revised paper.\n\n\n> Have demonstrative experiments to explore one of the potential RL applications\n\nThere are potentially many ways to exploit subspaces in the context of PG (parameter-space exploration and second-order optimization being only two of them). Thus, creating concrete applications of these insights for improving RL requires substantial further investigation and effort. The goal of this paper is to understand gradient subspaces in PG methods and share our findings with the RL community. We hope to spark interest in leveraging this phenomenon for the design of PG methods, similar to how Gur-Ari (2018) encouraged research on gradient subspaces in supervised learning that resulted in a wealth of practical applications (see the methods mentioned in the related work section). For this reason, we hope for understanding that advancing PG algorithms with the findings of this paper is left for future work.\n\nWe appreciate the insightful comments by the reviewer and hope to have clarified the aspects in question. We are committed to responding to the remaining questions, if any exist. We would deeply appreciate if the reviewer considered raising the score in case no further doubt remains.\n\n### References:\nGuy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv preprint arXiv:1812.04754, 2018.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\n\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint\narXiv:1802.09464, 2018."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495800979,
                "cdate": 1700495800979,
                "tmdate": 1700496689444,
                "mdate": 1700496689444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1zyrK9TgEp",
                "forum": "iPWxqnt2ke",
                "replyto": "fj8rBmCgzd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We hope that our answers and additional experiments address the raised questions to the reviewer's satisfaction. We welcome additional questions if there are any. Should there be no more questions and the reviewer is satisfied, we would be delighted if they considered increasing the score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658621637,
                "cdate": 1700658621637,
                "tmdate": 1700658621637,
                "mdate": 1700658621637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "32xcd7xJoM",
                "forum": "iPWxqnt2ke",
                "replyto": "fj8rBmCgzd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_CUg2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_CUg2"
                ],
                "content": {
                    "comment": {
                        "value": "I am grateful for the authors' recent updates and revisions to the paper, including the updated explanations and the newly conducted experiments."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685002759,
                "cdate": 1700685002759,
                "tmdate": 1700685002759,
                "mdate": 1700685002759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oGTOQ9s37i",
            "forum": "iPWxqnt2ke",
            "replyto": "iPWxqnt2ke",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_qF4u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_qF4u"
            ],
            "content": {
                "summary": {
                    "value": "The papers verifies experimentally the existence of gradient subspace in reinforcement learning in the on-policy algorithm PPO and off-policy algorithm SAC."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Current literature on identifying gradient subspace focus on supervised learning, and related work in RL focus on identifying parameter subspace rather than gradient subspace. Therefore, this work is the first to identify gradient subspace in the context of RL and is informative for training RL algorithms. Project codes are provided for reproducibility."
                },
                "weaknesses": {
                    "value": "It is unclear to which extent the contribution of identifying gradient subspace comparing to existing works in RL that focus on identifying parameter subspace (e.g. Gaya et al 2023 in the references) is significant, since both approaches have the same goal of improving training efficiency of policy parameters."
                },
                "questions": {
                    "value": "- Could you elaborate in more detail the motivation of identifying gradient subspace in comparison to parameter subspace, for the goal of guiding parameter-space exploration? This seems to be overlapping with Gaya et al. 2023's claimed benefit of identifying parameter subspace and it is not clear there what would be the benefits of using gradient subspace instead of parameter subspace in that case.\n- Is it possible to experimentally verify in a realistic example that the methods of Gaya et al. fail and the methods in current paper succeed?\n\n\n=============================\n\nPost-rebuttal: I thank the authors for the clarification on their contribution. I have raised my score accordingly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7121/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7121/Reviewer_qF4u",
                        "ICLR.cc/2024/Conference/Submission7121/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831226007,
            "cdate": 1698831226007,
            "tmdate": 1700496954572,
            "mdate": 1700496954572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I4Orr6Xy8e",
                "forum": "iPWxqnt2ke",
                "replyto": "oGTOQ9s37i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's critical observation and acknowledge that our initial presentation may not have sufficiently highlighted the distinct differences between our work and that of Gaya et al. (2022 and 2023). We're grateful for the opportunity to clarify these distinctions more explicitly.\n\n> It is unclear to which extent the contribution of identifying gradient subspace comparing to existing works in RL that focus on identifying parameter subspace (e.g. Gaya et al 2023 in the references) is significant, since both approaches have the same goal of improving training efficiency of policy parameters.\n\nBoth works consider different subspaces and, hence, identifying each subspace can have distinct, maybe even orthogonal, benefits. Gaya et al. generate a new policy through a convex combination of policy parameters that are trained to be distinct (Eq. (6) in Gaya et al. 2022) and maximize the RL objective (Eq. (4) in Gaya et al. 2022). These parameters (referred to as anchor parameters) span the subspace. We, on the other hand, analyze the subspace spanned by the dominant eigenvectors of the Hessian matrix (second derivative of the RL objective wrt. the policy parameters = derivative of the policy gradient). \nThus, Gaya et al. focus more on generalizing policies, and we appreciate that this line of work also reports benefits of using subspaces. Our work aims to accelerate the vast field of policy gradients methods by using subspaces. In that sense, there might exist multiple ways to exploit subspaces for more efficient policy gradient training. Guiding parameter-space exploration or enabling second-order optimization are just two ways that we propose for future work. In fact, since the works by Gaya et al. from 2022 and 2023 use PPO (line 5 in Figure 2 in Gaya et al. 2022) and SAC (line 8 in Algorithm 1 in Appendix C.1 in Gaya et al. 2023) within their framework, our works could even be combined.  \n\n> Could you elaborate in more detail the motivation of identifying gradient subspace in comparison to parameter subspace, for the goal of guiding parameter-space exploration?  This seems to be overlapping with Gaya et al. 2023's claimed benefit of identifying parameter subspace and it is not clear there what would be the benefits of using gradient subspace instead of parameter subspace in that case.\n\nAs stated above, we identify and analyze gradient subspaces to make policy gradients more efficient, and the line of work by Gaya et al. proposes to use subspaces of policy parameters to recombine them into new policies. Parameter-space exploration, as formulated in Plappert et al., 2017, can be one way how policy gradient subspace can help advance policy gradient algorithms. We believe that some aspects of the work by Gaya et al. could also be applied for parameter-space exploration. However, it has not been applied to this problem yet. \n\n> Is it possible to experimentally verify in a realistic example that the methods of Gaya et al. fail and the methods in current paper succeed?\n\nGaya et al. consider policy adaptation (increase the performance of the policy on similar but different MDPs at test time (Gaya et al., 2022)) and continual RL (training agents on sequences of tasks (Gaya et al., 2023)) settings, whereas our work addresses efficiency in single-task learning. So, the objectives are not fully aligned. In fact, the approach by Gaya et al. builds upon learned single-task policies. For that reason, it is not obvious how to create a scenario where both methods can be compared fairly. \n\nWe would like to thank the reviewer again for this comment. We hope that this response addresses all of raised doubts about our work. We are committed to answering the remaining ones, if any exist. If there are no further questions, we would really appreciate if the reviewer considered raising the score.\n\n### References:\n\nJean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for online\nadaptation in reinforcement learning. In International Conference of Learning Representations\n(ICLR), 2022.\n\nJean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, and Roberta\nRaileanu. Building a subspace of policies for scalable continual learning. In International Conference of Learning Representations (ICLR), 2023.\n\nMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. \"Parameter space noise for exploration.\" arXiv preprint arXiv:1706.01905, 2017."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496042366,
                "cdate": 1700496042366,
                "tmdate": 1700496922161,
                "mdate": 1700496922161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8qkNNZ5AD",
                "forum": "iPWxqnt2ke",
                "replyto": "oGTOQ9s37i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy that we could clarify the difference between our and Gaya et al.'s work and would like to express our gratitude that the reviewer raised the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563145720,
                "cdate": 1700563145720,
                "tmdate": 1700563145720,
                "mdate": 1700563145720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hlLmtcp2f1",
            "forum": "iPWxqnt2ke",
            "replyto": "iPWxqnt2ke",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
            ],
            "content": {
                "summary": {
                    "value": "**EDIT: After the rebuttal, I am raising my score. The authors have promised that they have included additional explanations in the paper (and will add the rest) along with some experiments that they are running, and have modified the abstract to clearly specify the scope of this paper. With these additions, I feel that this paper can be a worthwhile addition to this conference.**\n\nThe paper empirically analyzes two popular policy gradient methods (PPO and SAC) on standard RL benchmark tasks and demonstrates (for the first time, to the best of my knowledge) that the actor and critic gradients lie in a tiny subspace (containing about 1% of the original network parameters) and that this subspace changes very slowly. Similar knowledge, as the paper discusses, about the gradients of neural networks in supervised learning have already resulted in various methods to improve training. This paper has the potential to result in similar advances for RL.\n\nI propose a weak accept for this paper because \n(0) The findings seem novel and can have a positive impact on PG algorithm development\n(1) I find the empirical methods of this paper ad-hoc and not well justified; and\n(2) Many empirical details are missing (although, maybe they are all minor details, and I'm over-estimating their importance, especially since the authors released their code);\n\nNote to the authors: I have a limited experience in deep learning, and as such my knowledge of most of the references and the methods employed in this paper is very limited. So if you feel that I don't understand the significance of your analysis, I would appreciate if you could point it out in your rebuttal (and finally in the revised paper). Thanks!"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality and significance:** The paper (empirically) shows that for the first time that policy gradients (and the critic gradients) of the widely popular deep RL methods (PPO and SAC) lie in a very tiny subspace and this subspace remains somewhat stable during the course of learning. This result is interesting in itself as it gives us additional insights into the deep RL methods. Further, such a result can have significant implications on developing policy gradient methods, as outlined by the authors: Since deep learning methods often have a large number of parameters, this paper's insights can speed up existing RL methods by restricting optimization to a small fraction of those parameters. It can also result in better exploration techniques.\n\n**Quality and clarity:** The paper does a great job of outlining existing research: it's literature survey and references to relevant works is very thorough. The main ideas of the paper themselves are presented in an easy to follow manner, complemented by clear graphs and intuitive explanations. The writing itself is free from any grammatical mistakes (which is uncommon in the papers I have reviewed; so that is nice!)."
                },
                "weaknesses": {
                    "value": "# Major issues (these affect my score significantly):\n\nThe major weakness of the paper is a lack of rigor and concrete results. \n\n## 1. There is a very weak link between the experimental results and the claims made in the paper: ##\n\n(a) For example, see the following claims of the paper:\n- Section 1 (last paragraph): \"(i) parameter-space directions with significantly larger curvature exist in PG\": --> why can we say that the curvature is significantly larger? Figure 1 is highly qualitative in nature. In fact, Figure 2 is essentially qualitative as well. (And larger than what?)\n- Section 1 (last paragraph): \"(iii) the subspace is sufficiently stable to be useful for training\" --> how do the experiments justify the sufficiency? As far as I understand, Figure 3 only shows that the magnitude of the largest eigenvector projected to the subspace is, say, 0.4 for most part of the learning. What does 0.4 mean? How does that imply sufficient stability for learning based on methods that make updates in this subspace?\n- Section 1 (last paragraph): \"observe that the value function subspace often exhibits less variability and retains a larger portion of its gradient compared to the PG subspace\" --> inconclusive (Figure 2 shows very similar trends for actor and critic)\n\n(b) The results for the paper are specifically for well-tuned PPO and SAC agents on specific RL benchmarks, while the conclusions are drawn for general results. For instance, can we be sure that the subspace would also exist for a random hyperparameter configuration? The existence of subspace is even more important for a wide range of hyperparameters, since we won't know the optimal choice apriori for a random problem. Would the subspace be as restricted (for instance a dimensionality of 0.1% of the total parameter values) for a random SAC agent?\n\n(c) Section 4.3 (paragraph 3): \"Similar to the gradient subspace fraction results from Section 4.2, the subspace overlap is more pronounced for the critic than the actor.\" --> why? Are the scales comparable for the actor and critic graphs in Figure 3? Is this \"extra\" overlap really useful for designing algorithms? (Maybe it is, but without any explanations or additional experiments, this information is just speculative.)\n\n## 2. There is no rigorous motivation about various metrics used: ##\nIt seems that the paper adopted the metrics from Gur-Ari et al. (2018) for its experiments. However, these metrics seem somewhat arbitrary, and it is unclear how they relate to the usefulness/existence of the gradient subspace.\n\n(a) Gradient subspace fraction (Eq. 5) seems uninformative. Why showing that the gradient norm in the projected subspace is similar to the original gradient norm helpful? For instance, from the [Johnson-Lindenstrauss lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma) we know that any random projection matrix will ensure that the projected gradient norm is not too far away from the actual gradient norm. My point being, the metric in Eq. 5 doesn't seem to be useful right away. Maybe one way to make it more informative would be to establish a scale (so compare the norm of the projected gradient to the Hessian eigenvector subspace and the norm of the gradient projected to random subspaces).\n\n(b) Subspace overlap (Eq. 7): This metric is very complicated and I don't clearly see why it helps with showing the stability of the subspace and what that means for restricting training there. Maybe additional references could help with this.\n\n## 3. Missing details about the implementations: ##\n\nI give some examples below:\n- Section 4.3 (the equation for S_overlap): what is $k$ in the experiments? Does $k$ change with $t$ in the graph?\n- Section 4.3: \"where $v_i$ is the ith largest eigenvector at timestep t\" --> how is the largest eigenvector determined? Is it by the size of the corresponding eigenvalue?\n- Section 4 (paragraph 2): This section is unclear and doesn't provide sufficient details for reproduction (I understand the code does that, but without some details, the paper's results are very difficult to reason about). For instance, SAC usually has multiple Q networks; which ones do the experiments analyze?\n- Page 6, Figure 2: I was not able to understand the difference between the three categories: \"Estimated/true\" gradient, \"estimated/true\" Hessian, and why does that matter (probably Hessian is used to find the projection matrix $P_k$; if so, please explicitly mention it somewhere)? For instance, why do we care about estimated gradients? I understand that estimated Hessian can be important because we will use it to identify the policy gradient subspace and then use it downstream in algorithms. Some more justification about this could be nice.\n- Page 6 (first paragraph): Why was 2%, 0.14%, and 0.07% chosen? These seem highly arbitrary choices. How do the trends change when these numbers are changed?\n- Page 5: cutoffs for Equation 6 seem arbitrary.\n\n# Minor issues / suggestions (these do not affect my score as much; please ignore them if you don't agree):\n- The abstract seems incomplete/misleading in its present form, and it overclaims the contributions of the paper. For instance, it says \"we demonstrate the existence of such gradient subspaces for policy gradient algorithms.\" While this is not technically wrong to say that, the paper only empirically shows these subspaces, and that too for a very limited class of algorithms, i.e. deep policy gradient methods (in fact just two algorithms PPO and SAC). This could be easily rectified, for instance, by including this line from the introduction: \"This paper conducts a comprehensive empirical evaluation of gradient subspaces in the context of PG algorithms, assessing their properties across various simulated RL benchmarks.\" Further, the line \"Our findings reveal promising directions for more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.\" seems to be suggesting that the paper also introduces methods for exploiting these subspaces, whereas that is just suggestion for future research.\n- The appendices are not properly referenced anywhere in the paper. At the very least, each section of the appendix should be referenced at the appropriate place in the main paper, explaining what additional details are there. Currently, appendices are a dump of graphs with no accompanying textual explanations (other than the caption). \n- Do you think the \"LoRA: Low-Rank Adaptation of Large Language Models\" (https://arxiv.org/abs/2106.09685) paper could be added to the related works section as well?\n- In Section 3.1: can $\\gamma$ be 1? Is the setting episodic? \n- In Section 3.1 (paragraph 2): The phrase, \"expected cumulative reward\" could be replaced by \"expected (discounted) cumulative reward\"\n- In Section 3.1 (paragraph 2): \"advantage function ... and can also be defined as\" --> why say \"can also be defined as\"? Isn't that the definition of advantage function? Maybe rephrase the sentence..\n- Section 3.2, first line: Maybe give a reference for the objective function $J(\\theta)$? \n- Section 3.2, first line: Also, the definition of the expectation is unclear: in particular, in its current form, the expression $J = \\mathbb{E}[ \\pi(a_t | s_t) \\hat A_t]$ seems to depend on the timestep $t$. That shouldn't be the case..\n- Section 3.2, first paragraph: What \"estimator of advantage function\" is being used? It is not specified.  \n- Section 3.2, first paragraph (above equation 2): what is the target value for the critic?\n- Section 3.3 (above Eq. 4): \"exponential of the learned Q-function\" --> isn't saying something like a Gibbs distribution or softmax distribution more appropriate?\n- Figure 2: since the \"true gradient\" is just the gradient computed using more state-action pairs, maybe calling it \"true\" is not accurate?"
                },
                "questions": {
                    "value": "I would really appreciate if the authors could clarify the following points (please refer to the Weaknesses section for details):\n\n1(b) Is my understanding correct here? Would these conclusions continue to hold for randomly chosen hyperparameters?\n\n2(a) Why can Eq. 5 helpful? How would the projections on random subspaces look like?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7121/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7",
                        "ICLR.cc/2024/Conference/Submission7121/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699575863674,
            "cdate": 1699575863674,
            "tmdate": 1700688439484,
            "mdate": 1700688439484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5JuVDbzYig",
                "forum": "iPWxqnt2ke",
                "replyto": "hlLmtcp2f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer u1A7 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for this detailed and thorough evaluation of our work and the insightful questions. The report improved our paper significantly. We carefully answer each point below.\n\n## 1.\n(a)\n\n> [W]hy can we say that the curvature is significantly larger?\n\nThe curvature is determined by the magnitude of the Hessian eigenvalues. In Figure 1, we show for the actor and critic losses that a handful Hessian eigenvalues exist with a significantly larger magnitude than the rest. Therefore, a small number of parameter-space directions dominate the curvature of the objective.\n\n> And larger than what?\n\nWe added \u201ccompared to the other parameter-space directions\u201d to make this point clearer.\n\n> \"(iii) the subspace is sufficiently stable to be useful for training\" --> how do the experiments justify the sufficiency?\n\nOur current experiments do not yet evaluate the sufficiency of the subspace for downstream applications. We changed the formulation to \u201c(iii) the subspace remains relatively stable throughout the RL training.\u201d Although it is yet to be shown that the stability of the subspace throughout the training is useful, we believe this result by itself is already interesting due to the high variance nature of PG algorithms.\n\n> \"observe that the value function subspace often exhibits less variability and retains a larger portion of its gradient compared to the PG subspace\" --> inconclusive (Figure 2 shows very similar trends for actor and critic)\n\nWe added results for more tasks to Figure 2. Furthermore, we fixed a bug in our implementation that changed the analysis results for the Walker2D task (see the general response for details about this bugfix). The updated Figure 2 shows the trend that the critic gradient tends to lie better in the respective subspace more clearly. This effect is particularly strong for PPO, but the average gradient subspace fraction is also slightly higher for the critic in SAC. Please note that this trend is also noticeable in the results for all tasks (see Appendix C) and is not limited to the four tasks displayed in Figure 2.\n\n(b)\n\nThis is an important point since hyperparameters in RL generally influence the learning performance significantly. We are currently running an experiment, where we sample hyperparameters in the bounds that we also use for hyperparameter tuning to obtain suboptimal but realistic hyperparameters. We will add the analysis results for these configurations to the revised paper once the experiments are finished.\n\n(c) \n> \"Similar to the gradient subspace fraction results from Section 4.2, the subspace overlap is more pronounced for the critic than the actor.\" --> why? Are the scales comparable for the actor and critic graphs in Figure 3?\n\nYes, the scales are comparable. We applied the same criterion and used the same subspace dimensions for both the actor and the critic. The criterion is inherently normalized to the range [0, 1] (since we assume the eigenvectors to be norm 1). Furthermore, both networks have the same hidden layer size and, thus, contain roughly the same number of parameters (there is a small difference due to the different input/output dimensions).\n\nThe results in Figure 3 and the detailed results in Figures 7 and 8 of Appendix C show that the subspace overlap tends to be larger for the critic than for the actor.\n\n> Is this \"extra\" overlap really useful for designing algorithms?\n\nThe design of algorithms that exploit the subspace phenomenon in the context of PG algorithms can take various forms and, hence, requires substantial experimental validation, which is left for future works. Because there is a significant overlap between the subspaces at different timesteps, the subspace does not need to be determined anew at every step during training. Downstream applications could exploit this fact to increase the computational efficiency of the method. The fact that the subspace overlap is on average higher for the critic than the actor could be exploited for further efficiency gains. The scale of these efficiency gains, however, depends on the specific algorithm."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498609721,
                "cdate": 1700498609721,
                "tmdate": 1700498609721,
                "mdate": 1700498609721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6sFne6YfxD",
                "forum": "iPWxqnt2ke",
                "replyto": "hlLmtcp2f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We hope that our response clarifies all of the reviewer's questions. If any aspect of our paper remains unclear, we encourage the reviewer to pose further questions. Should the reviewer be satisfied with our response and have no more questions, we would be delighted if they considered increasing the score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659253456,
                "cdate": 1700659253456,
                "tmdate": 1700659253456,
                "mdate": 1700659253456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "njEVcXlm2f",
                "forum": "iPWxqnt2ke",
                "replyto": "5JuVDbzYig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed comments. This makes sense. While I cannot check if all these additional experiments / additional explanation has been inclined in the paper, I believe that adding these details will make the paper much more useful and accessible. (It's even worthwhile just to place these in the appendix, if shortage of space is a concern.)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687997327,
                "cdate": 1700687997327,
                "tmdate": 1700687997327,
                "mdate": 1700687997327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VCtF7JAyxx",
                "forum": "iPWxqnt2ke",
                "replyto": "bgdQkJLRxG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "content": {
                    "comment": {
                        "value": "I appreicate the explanation of point 2(a). This additional explanation (and the results of the sanity check about the random matrix) would be super useful if added to the paper. \n\nPoint 2(b) remains unclear to me."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688104239,
                "cdate": 1700688104239,
                "tmdate": 1700688104239,
                "mdate": 1700688104239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ddP00N2yuc",
                "forum": "iPWxqnt2ke",
                "replyto": "oDsAjdRx6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "content": {
                    "comment": {
                        "value": "This was very helpful. Thanks!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688230095,
                "cdate": 1700688230095,
                "tmdate": 1700688230095,
                "mdate": 1700688230095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "83PyUrODxY",
                "forum": "iPWxqnt2ke",
                "replyto": "6sFne6YfxD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7121/Reviewer_u1A7"
                ],
                "content": {
                    "comment": {
                        "value": "Your comments were very thorough. I am thankful for that. \n\nI have raised the scores. But again, I implore you to include additional details and experiments in  the revised paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688403571,
                "cdate": 1700688403571,
                "tmdate": 1700688403571,
                "mdate": 1700688403571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]