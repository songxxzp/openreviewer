[
    {
        "title": "Compositional Image Decomposition with Diffusion Models"
    },
    {
        "review": {
            "id": "zJFyA003H3",
            "forum": "88FcNOwNvM",
            "replyto": "88FcNOwNvM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4201/Reviewer_t1av"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4201/Reviewer_t1av"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the image decomposition task and proposes a new approach, i.e., Decomp Diffusion, to decompose a scene into a set of factors represented as separate diffusion models. The proposed method can decompose scenes into both global and local concepts. These concepts can further be flexibly composed to generate a variety of scenes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of leveraging the connection between Energy-based models and diffusion models for image decomposition is interesting and effective. The compositional concepts from images can be discovered in an unsupervised manner. The experimental results show that the proposed method can discover both global and local concepts, and be used for component compositions across multiple datasets and models."
                },
                "weaknesses": {
                    "value": "1. The quantitative evaluation is not thorough. The current quantitative evaluation only focuses on the global factors, while the quantitative evaluation for the local factors and cross dataset generalization is missing. In contrast, the existing work (COMET) contains quantitative comparisons for the object-level decomposition.\n2. As the proposed method contains a set of diffusion models, the computational cost of the proposed method and existing works should be discussed in the paper.\n3. For training details in the supplemental, each model is trained on an NVIDIA V100 or an NVIDIA RTX 2080 with the same hours. I was wondering whether the model performance would be different. In addition, is the memory of NVIDIA RTX 2080 24GB or 8GB?"
                },
                "questions": {
                    "value": "1. For the ablation study, why use MSE and LPIPS to evaluate the reconstruction quality, rather than the metrics used in Table 1? How about the results of the ablated versions on other datasets used in the paper?\n2. How to determine the types of factors that can be inferred from the image? For example, I am not sure whether the second to the fourth columns correspond to shadow, objects, and background respectively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4201/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4201/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4201/Reviewer_t1av"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675560082,
            "cdate": 1698675560082,
            "tmdate": 1699636386670,
            "mdate": 1699636386670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZuLnCvtoww",
                "forum": "88FcNOwNvM",
                "replyto": "zJFyA003H3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments. We have added quantitative metrics for local factor decomposition and cross-dataset generalization as well as for the ablation study. In addition, we answer questions about the computational cost, machine information, and inferring factors in more detail.  \n\n**Q1) Quantitative metrics for local factors and cross-dataset generalization**\n\nTo quantitatively evaluate our method on local factors, we compare our method with several baselines for local factors (CLEVR) in the table below (we have also updated it in Table 1 in our main paper). Our method achieves the best performance across all three metrics, achieving the best performance on local factor decomposition.\n\n| Model | FID &darr; | KID &darr; | LPIPS &darr;\n|  :----------------  |  :------:  |  :------:  | :------:  |\n| Beta-VAE | 316.64 | 0.383 | 0.651\n| MONet | 60.74 | 0.063 | 0.118\n| COMET | 103.84 | 0.119 | 0.141\n| Slot Attention | 27.07 | 0.026 | 0.031\n| Hessian Penalty | 25.40 | 0.016 | ---\n| GENESIS-V2 | 318.46 | 0.403 | 0.631\n| Ours | **11.49** | **0.011** | **0.012**\n\nFor cross-dataset evaluation (CLEVR combined with CLEVR Toy), because there is no ground truth for recombined images, we computed FID and KID scores of generated recombination images against the original CLEVR dataset as well as the original CLEVR Toy dataset. We report in the table below and have added it in the Appendix. Our approach achieves better scores for both original datasets compared to COMET, which suggests that our generations are more successful in recombining objects from the original datasets. \n\n| Model | CLEVR  FID  | CLEVR KID  | CLEVR Toy  FID | CLEVR Toy KID |\n|  :----------------  |  :------:  |  :------:  | :------:  |  :------:  |\n| COMET | 98.27 | 0.110 | 192.02  | 0.250 |\n| Ours | **75.16**  | **0.086** | **52.03**  | **0.052** |\n\n\n**Q2) Computational cost**\nPlease see general response Section 4. Our method uses $K$ diffusion models, so the computational cost is $K$ times that of a normal diffusion model. In practice, the method is implemented as $1$ denoising network that conditions on $K$ latents, as opposed to $K$ individual denoising networks. One could significantly reduce computational cost by fixing the earlier part of the network and only condition on latents in the second half of the network. This would likely achieve similar results while reducing computation. In principle, we could also parallelize $K$ forward passes for each factor, thus reducing both training and inference time.  We have added this detail in the Section E.3 of the Appendix. Regarding the computational cost of other approaches, Beta-VAE, MONet, COMET, and Hessian Penalty generally used less training time until convergence, while Slot Attention and GENESIS-V2 use a comparable training time of 24 hours.  \n\n**Q3) Machine specs**\nOur models were trained with the same batch size of 32 images, so the machine type should not affect model performance. The memory of a NVIDIA RTX 2080 machine is 24GB.\n\n**Q4) Ablation study metrics**\n\nThank you for your question. Our method not only decomposes images into a set of factors, but also enables recombination of all inferred factors to reconstruct the input image. As a result, we used MSE and LPIPS to measure both pixel-wise errors and perceptual similarity of the original input image and reconstructed image. In addition to MSE and LPIPS, we have also added FID and KID to the ablation study in the table below and update it in the main paper accordingly (Table 2):\n\n| Dataset | Multiple Components | Predict x_0 | MSE &darr; | LPIPS &darr; | FID &darr; | KID &darr;\n|  :----------------  |  :------:  |  :------:  | :------:  | :------:  | :------:  | :------:  |\n| CelebA-HQ | YES | NO | 105.003 | 0.603 | 155.46 | 0.141 \n| CelebA-HQ | NO | YES | 88.551 | 0.192 | 30.10 | 0.022\n| CelebA-HQ | YES | YES | **76.168** | **0.089** | **16.48** | **0.013** \n| CLEVR | YES | NO | 56.179 | 0.3601 | 42.72 | 0.033\n| CLEVR | NO | YES | 26.094 | 0.2236 | 24.27 | 0.023\n| CLEVR | YES | YES | **6.178** | **0.0122** | **11.54** | **0.010**\n\nIn the table above, we have also included ablation study for the CLEVR dataset. The non-ablated version of the model, with predicting $x_0$ and using multiple components (i.e., 4 components), achieves the best scores on both CelebA-HQ and CLEVR datasets across all metrics, indicating that the conclusion is consistent across different datasets.\n\n\n**Q5) How are the types of inferred factors determined?**\nPlease see general response Section 3. Since our method learns to decompose images in an unsupervised manner, there is no way to know what type of factors would be inferred beforehand. As a result, we provide a large number of example images and name inferred factors through visual inspection in order to help readers understand what each component represents. We have updated the Experiment section to clarify this."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247381363,
                "cdate": 1700247381363,
                "tmdate": 1700256448032,
                "mdate": 1700256448032,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RituFpLsle",
            "forum": "88FcNOwNvM",
            "replyto": "88FcNOwNvM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses compositional image generation through denoising diffusion models. The unsupervised approach decomposes the input image into several primitives, and the model is able to recompose these primitives together. Experiments are conducted on simple object scenes and human faces, and demonstrate superior performance than SOTAs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper addresses compositional modeling for images using denoising diffusion models. The recomposition quality seems promising. \n+ The paper shows that energy functions are additive of primitives."
                },
                "weaknesses": {
                    "value": "+ The method seems to be similar to [1]\n+ What is the computational cost? It may takes more space and computational resources with K diffusion models\n\n\n\n[1] Du et al, Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC, ICML 2023"
                },
                "questions": {
                    "value": "+ Is the learned encoder Enc\u03b8(x) pre-trained or trained with diffusion model jointly? \n+ Suppose it is jointly trained, how does the network learn to decompose the image into shadow image, object image, background image, etc? Is there any specific constraint for learning these different properties?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695490487,
            "cdate": 1698695490487,
            "tmdate": 1699636386566,
            "mdate": 1699636386566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "USz63UodZl",
                "forum": "88FcNOwNvM",
                "replyto": "RituFpLsle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for the feedback. We address the comparison to a related work, and answer other questions about the computational cost, encoder, and inferred factors in more detail.  \n\n**Q1) The method seems similar to [1]**\nThough both works involve diffusion models representing concepts, the goal of this paper is to discover and decompose images into component diffusion models, while [1] focuses on composing pre-trained diffusion models. To accomplish this decomposition, we propose a new training-time objective to find each component. \n\n**Q2) What is the computational cost?**\nThank you for your question. Our method computes $K$ conditional scores for a total of $K$ latents, so the computational cost is $K$ times that of a normal diffusion model. In practice, the method is implemented as $1$ denoising network that conditions on $K$ latents, as conditioning on different latents can make arbitrary functions in principle. One could significantly reduce computational cost by fixing the earlier part of the network since latents are only conditioned in the second half of the network. In addition, in practice, we parallelize $K$ forward passes for each factor, thus keeping training and inference time constant. \n\nWe have added this detail in the Section E.3 of the Appendix.\n\n\n**Q3) How is the encoder learned?**\nOur method jointly learns the latent encoder and the denoising network jointly. We have also updated Section 3.2 in the paper to clarify this choice.\n\n**Q4) How does the model learn to decompose images into different factors?**\nWe have described how the model learns to disentangle components in general response Section 2, and explained how we name our inferred factors in general response Section 3. To further clarify, the model learns to extract meaningful latents primarily through the information bottleneck principle. The encoder, which is jointly trained, compresses the inputs $x$ into a latent representation $z$ consisting of $K$ low-dimensional sub-latents $z_k$. Since these sub-latents are very low-dimensional, to most effectively denoise images, each of the different functions $\\epsilon_\\theta(x_i^t, t, z_k)$, focuses on different aspects of an image (as if any two functions focus on the same aspect, then the information capacity between the two latents is not being effectively used). This encourages disentanglement in the learned functions across $K$ sub-latent representations, where each sub-latent captures specific, independent aspects of the input images $x$. An interesting direction of future work is to add other constraints to enforce orthogonality between different sub-latents, though we had limited success when trying this. We have added this information in Section 3.2.\n\n\nSince our method learns to decompose images in an unsupervised manner, there is no way of knowing what type of factors would be inferred beforehand. As a result, we provide a large set of example images and name inferred factors through visual inspection in order to help readers understand what each component represents. We have updated the Experiment section to clarify this."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246998000,
                "cdate": 1700246998000,
                "tmdate": 1700256411437,
                "mdate": 1700256411437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VC4YgrLs3u",
                "forum": "88FcNOwNvM",
                "replyto": "USz63UodZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your reply. It is good to see that the encoder and decoder are trained jointly to decompose an image into a fixed number of elements. I am curious to see what would it be to use GANs to unsupervised decompose an image?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348775676,
                "cdate": 1700348775676,
                "tmdate": 1700348775676,
                "mdate": 1700348775676,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GoSIyP4Q6P",
                "forum": "88FcNOwNvM",
                "replyto": "Veuv6EpBMb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Yes, this is a good point. GANs may have mode collapse problem. Now let's say VAE. VAE can be additive to composition. I assume VAE can also decompose an image into different primitives, right?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351184946,
                "cdate": 1700351184946,
                "tmdate": 1700351184946,
                "mdate": 1700351184946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "StIQ1mrhTO",
                "forum": "88FcNOwNvM",
                "replyto": "ZIKD3yVUOs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_Rmft"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "This is good. However, I only find quantitative comparison on reconstruction quality in Table 1. I do not see any visual comparisons of disentanglement of primitives, which I believe is quite important to validate your method."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407569988,
                "cdate": 1700407569988,
                "tmdate": 1700407569988,
                "mdate": 1700407569988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qJjVuUmVqh",
                "forum": "88FcNOwNvM",
                "replyto": "RituFpLsle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Hi, yes in Figure XVIII of the original paper you can see a visual comparison of disentanglement of primitives with baselines. We've also just added a revision to the paper to provide additional visual comparisons of disentanglement with MONet in Figure XIX ([screenshot here](https://ibb.co/Z6Qssyt)) and recombination in Figure XX ([screenshot here](https://ibb.co/pQH0JF6))."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441438843,
                "cdate": 1700441438843,
                "tmdate": 1700582789476,
                "mdate": 1700582789476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OLO5F7faGu",
            "forum": "88FcNOwNvM",
            "replyto": "88FcNOwNvM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4201/Reviewer_W4iB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4201/Reviewer_W4iB"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses unsupervised image decomposition/re-composition with diffusion models. The authors show equivalence between previous decomposition work COMET, based on energy minimization and gradient descent optimization framework, and the recent diffusion models (DDPM) denoising steps iteration.  They consequently 'substitute' the EM model with a diffusion model conditionned on a set of latent variables z_k. The z_k's are inferred by an Encoder, and are associated to the different factors of the decomposition. \nExperimental results are illustrated on several classical benchmarks (CelebA, Virtual Kitti, Falcor3D, also synthetic data such as CLEVR and Tetris), compared qualitatively and quantitatively to related work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Unsupervised image intrinsic decomposition/re-composition is very challenging and one of the most fundamental open issues in computer vision. Using diffusion models for this purpose seems a natural choice (given the success of DM in natural image generation, and in learning semantic image properties). The authors give a rigorous justification of their choices from a mathematical point of view.  The paper's idea is well argued. The illustrated results show the strong potential of the approach.  I enjoyed reading the article."
                },
                "weaknesses": {
                    "value": "Qualitative results are promising but still leave room for improvement. Reconstructed images appear blurry, and at low resolution. But at this stage this is not a major issue and that might be improved by further work."
                },
                "questions": {
                    "value": "1) I did not find in the paper an explanation about the encoder z = Enc_\\theta(x).  How Enc_() is learnt? What ensures that the decomposition is at the local (ie objects, things) or global (ie, illumination, stuffs) level?   What ensures the disentanglement of the decomposition ? (no additional constraints are enforced during the learning stage).  Those aspects might have been discussed in the original paper COMET (I did not read it), however, it is worth to discuss them again in the current paper since it is key for the understanding and  analysis of the success/failures of the proposed approach. \n\n2) Some details in the approach that are not clear to me. \n2.1 The authors argue that they 'learn a set of different denoising functions to recover an image x_i' (page 4). However, the denoising function \\epsilon_\\theta is not, in eq.9, parameterized by k.  The only dependance to k is in the input latent variable z_k. It would imply that there is a single denoising function, but with different input argument (in particular the z_k). Please clarify. \n2.2 The encoder Enc_\\theta() and the denoising function \\epsilon_\\theta, are both parameterized by \\theta. This is probably a typo, the two networks being parameterized by two sets of independent weights, \\theta_1 and \\theta_2. Please correct as needed in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical issues beyond existing public generative models."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827461180,
            "cdate": 1698827461180,
            "tmdate": 1699636386461,
            "mdate": 1699636386461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ELeO50aEYV",
                "forum": "88FcNOwNvM",
                "replyto": "OLO5F7faGu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and positive evaluation. We have added a description of the encoder training and explanation of how the approach learns to disentangle latents in the text. \n\n**Q1) How is the encoder learned?**\nThank you for your question. As addressed in the general response 1, our encoder and denoising network are learned jointly using the denoising loss. The main reason behind this design choice is that we can naturally enable the model to learn different types of latent representations from various datasets. \n\nWe have also clarified this in the method section accordingly.\n\n**Q2) What ensures the type of decomposition?**\nSince our method is an unsupervised approach to decompose images into a set of latents, we aren\u2019t able to know the types of inferred concepts those learned latents would be ahead of time. Based off visual inspection, we provided a 'psuedolabel' for each component based off the visualization of the component. To more clearly display each component, we provide a set of visualizations in our experiments and appendix to help readers understand inferred concepts. \n\nWe have also updated this in the Experiment section to avoid any further confusion.\n\n**Q3) How is disentanglement ensured?** \nPlease see our general response 2. Disentanglement is primarily ensured by information bottleneck. The encoder encodes the information in $x$ into a compact low-dimensional latent representation $z$ consisting of $K$ sub-latents $z_k$. To most effectively denoise an image given these low-dimensional latents, each function $\\epsilon_\\theta(x_i^t, t, z_k)$ learns to focus on distinct information in an image. This enforces diversity and disentanglement in the learned functions across $K$ sub-latent representations. While we explored additional constraints to enforce independence in components, we found that it gave similar performance to just using a low dimensional latent. \n\nWe have added this information in Section 3.2.\n\n\n**Q3) Why is one single denoising network used instead of $K$**\nThank you for your question. In principle, a model can learn arbitrary functions by conditioning on different latent representations. Thus one single denoising network that conditions on $K$ different latents should achieve similar performance as $K$ denoising networks. In addition, our method is more memory-efficient as it only requires one network instead of $K$.\n\n**Q4) Typos**\nThank you for pointing that out. We have updated the encoder parametrization notation to be $Enc_\\phi$ instead of $Enc_\\theta$. We will make sure that our notations are consistent in our next version of the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246791858,
                "cdate": 1700246791858,
                "tmdate": 1700363241838,
                "mdate": 1700363241838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6vxBgfnWDu",
                "forum": "88FcNOwNvM",
                "replyto": "ELeO50aEYV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_W4iB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4201/Reviewer_W4iB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the clarifications. They mainly address my concerns. \nThe approach developed in the paper is theoretically well founded and the main concept clearly justified/discussed. Despite its limitations,  I do believe that the paper paves the way for future work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727009535,
                "cdate": 1700727009535,
                "tmdate": 1700727009535,
                "mdate": 1700727009535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]