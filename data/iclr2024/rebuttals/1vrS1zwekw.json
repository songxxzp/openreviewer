[
    {
        "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following"
    },
    {
        "review": {
            "id": "VevF2hSeiF",
            "forum": "1vrS1zwekw",
            "replyto": "1vrS1zwekw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new paradigm of \"Scaling Tasks per Input\" for curating instruction-following datasets. Existing methods for improving language models' instruction-following capabilities have several limitations. Scaling-Inputs require extensive input-output pairs per task, causing oversensitivity to inputs. Scaling Input-Free Tasks expands tasks without inputs but struggles when inputs are needed. To alleviate these issues, this work instead diversifies tasks for each input text using a facet-based approach to enhance model's instruction-following capability.\n\nThe authors construct a dataset MUFFIN implementing this paradigm. They generate tasks by identifying textual facets of inputs to stimulate relevant instructions and reusing existing instructions matched to inputs. Careful balancing of classification and generation tasks is also proposed. Comprehensive experiments demonstrate MUFFIN's superiority over strong prior datasets, approaching human-annotated data quality. Ablation studies and human evaluation further verify the effectiveness of the techniques and the overall paradigm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The proposed `Scaling Tasks per Input` paradigm is novel and unique compared to prior instruction dataset curation schemes. By controlling the input and diversifying tasks, it helps models focus on instructions.\n\nQuality: The paper is technically strong, with a systematic data collection pipeline considering input diversity, instruction generation, output annotation, and task balancing. The analyses thoroughly verify the quality, diversity, and validity.\n\nSignificance: Thorough experiments across four benchmarks convincingly demonstrate MUFFIN's superiority, compared to prior synthetic instruction dataset, over strong baselines spanning different paradigms. Both the automatic and human evaluations demonstrated the merit of this dataset."
                },
                "weaknesses": {
                    "value": "While the input-facet-oriented instruction generation approach is interesting, a major concern is the reliability of facets identified by the LLMs. As we know, hallucination remains an issue with LLMs. Though the model may correctly recognize textual attributes in most of time, some \"facets\" or the model generated *outputs* of these facet-based question could arise from hallucination. I don't see a specific mechanisms to handle this in the paper, except human audit on a small portion of samples. It's unclear whether these noise data, if it indeed exist, would hurt the model's reasoning capabilities. *One potential evidence is the substantially degraded performance on BBH benchmark compared to other baselines in Table 1.*\n\nI see similarities between this work and the self-instruct paper in the overall approach and principles for data generation, experiment design, and analysis approaches. Without addressing the trustworthiness issue I mentioned above, I would be hesitate to advocate the contribution of this work. \n\nI would like to see the authors to share some insights about the limitation of this work."
                },
                "questions": {
                    "value": "1. In the facet-based instruction generation and output annotation, is there a specific strategy or mechanism used to guarantee the trustworthy of the facets or output annotation, except for human inspection? \n2. For facet-based generation, how is facet diversity quantified? And is this diversity measured with respect to ground truth facets? Some analysis confirming facet coverage would be reassuring.\n3. Could you share some insights about why the MUFFIN model performs much worse than other baselines on BBH benchmark in Table 1?Since BBH focuses on assessing reasoning skills, this seems to indicate there is still room for improvement on those capabilities. To provide more insights into the diversity of the dataset, it would be helpful to breakdown the facet into fine-grained categories, such as elaboration,  numeric calculation, and commonsense reasoning etc. By doing so, it could give a clearer picture of the diversity of this dataset, meanwhile also help to isolate the reason behind the degraded performance. \n4. The motivation behind balancing classification and generation tasks is clear, but how exactly is the decision made on which tasks undergo classification expansion? Prompting the model to make the decision?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6820/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6820/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645022724,
            "cdate": 1698645022724,
            "tmdate": 1700732695982,
            "mdate": 1700732695982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fk225iY9jS",
                "forum": "1vrS1zwekw",
                "replyto": "VevF2hSeiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer#4 --- Part 1 (Q1)"
                    },
                    "comment": {
                        "value": "We are grateful for your detailed review and constructive comments! We're pleased you found our work novel, the proposed technique strong, and the experiments solid. \n\nSome of your suggestions are helpful and have been taken by us seriously. In this response, we add more experiments and try to address your concerns one by one:\n\n---\n\n> Q1: Are there any specific mechanisms to guarantee the trustworthiness of the facets and output generation?\n\nThanks for pointing this concern out. \n\n1. First, for the output annotation, we do admit that there is no specific mechanism to guarantee trustworthiness (except human verification), which is a common headache that almost all of the previous works suffered from. However, since the **main objective of this paper is to prove the effectiveness of the proposed paradigm** instead of simply achieving SOTA with a higher-quality dataset, we try to avoid any sophisticated strategies to cherry-pick training instances (e.g., using a small neural model to help filter hallucinated outputs). But of course, your suggestion is highly appreciated; it shall be our next work to release an even higher-quality dataset with the proposed paradigm (e.g., human-annotated Muffin-v2).\n\n2. As for the facets generation, since it is one of the **unique designs and critical steps** for creating the dataset with the proposed paradigm (i.e., `Scaling Task per Input`), we have some simple yet effective operations in our framework to improve its reliability:\n\n      - **Explicit Prompting**: we added some explicit constraints in the prompt of facets generation, to ensure the quality and diversity of the facets, e.g., \u201c*each facet should clearly describe one specific textual attribute\u2026*\u201d and \u201c*instructions should strictly be based on the facet\u2026*\u201d (pls see Table 4 and Table 5 in Appendix C).\n      - **Joint Filtering**: since the generated facets are eventually used for creating instructions, the proposed \u201c*Instruction Filtering*\u201d can also jointly filter those hallucinated facets that may lead to unanswerable instructions (pls see the second paragraph at page 4).\n\n     We proved the effectiveness of the above methods in Q2.\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361184732,
                "cdate": 1700361184732,
                "tmdate": 1700361184732,
                "mdate": 1700361184732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xB4qOvWv06",
                "forum": "1vrS1zwekw",
                "replyto": "VevF2hSeiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer#4 --- Part 2 (Q2)"
                    },
                    "comment": {
                        "value": "> Q2: This work lacks a clear big picture of the generated facets (the reliability and diversity of facets should be quantified); showing the facets with fine-grained categories may help.\n\nWe highly agreed with your concern and realized the importance of showing the quantification of generated facets. We carefully took your comments and tried to address your concern by elaborating on the following three dimensions: **statistics**, **quality**, and **diversity**.\n\n---\n\n1. **Facets Statistics**.\n\nWe hope the most straightforward way to show a big picture of the generated facets is to illustrate its statistics. As shown below, similar to Table 9 in our paper, we report additional statistics related to facets (the statistics after conducting \u201c*Instruction Filtering*\u201d).\n\n| statistics |  |\n|---|---|\n| # of inputs | 1,463 |\n| # of instructions (from facets-based   brainstorm) | 33,720 |\n| # of facets | 11,382 |\n| avg # of facets per input | 7.78 |\n| max # of facets per input  | 30 |\n| min # of facets per input  | 2 |\n| avg # of instructions per facet | 2.96 |\n| max # of instructions per facet  | 18 |\n| min # of instructions per facet  | 1 |\n\n---\n\n2. **Factes Quality**.\n\nTo estimate the quality and reliability of the generated facets, and also to demonstrate the effectiveness of our facets-controlling methods (in Q1), we conducted further human verifications on the facets.\n\nSpecifically, we randomly collected 100 inputs from Muffin, along with their corresponding facets (889 facets in total) and instructions (2,265 instructions in total). Then, we asked an experienced human annotator to evaluate the following two metrics:\n\n  - **Input-to-facet correctness**: whether each facet correctly described the given input (reflecting the quality and reliability of facts).\n  - **Facet-to-instruction correctness**: whether the subsequent instructions are reasonably related to the given facet (reflecting the utility of facets). \n\nThe final results are shown below:\n\n|   input-to-facet correctness   |   90.78%  |\n|---|---|\n|   **facet-to-instruction correctness**  |   **85.22%**  |\n\nThese results demonstrate the good quality and utility of the facets generated by our method. It's also worth noting that, after we conducted \"*Instruction Filtering*\", the input-to-facets correctness increased from 83.28% to 90.78% (the score above), proving the effectiveness of \"*Instruction Filtering*\" that can jointly filter a considerable amount of hallucinated facets.\n\n---\n\n3. **Facets Diversity**.\n\nAs for the diversity, we are sincerely grateful for your suggestions. Inspired by your comments, we asked the human annotator to **discover** the categories from the facets (i.e., clustering instead of classification), because it's hard to predefine the categories that may introduce personal bias.\n\nWe used the same instances as in the previous analysis (100 inputs and 889 facets). The human annotator was asked to summarize a \"keyword\" (category) for each LLM-generated facet. If the current category has already appeared before, the annotator also has to \"cluster\" those facets that belong to the same category together. In doing so, more and more novel categories can be discovered.\n\nWe then calculated the following two metrics:\n\n- **Intra-diversity**: how many unique categories there were for each input\u2019s facets, and reported an averaged ratio among all the inputs. \n- **Inter-diversity**: overall categories distribution among all the 889 facets, namely the global frequency of each unique facet.\n\nWe found that the generated facets achieve **91.55%** intra-diversity, meaning the facets of the same input are pretty diverse. \n\nMeanwhile, there are a total of **276** unique categories out of the 889 facets, which implies a high inter-diversity of the facets. The distribution plot of inter-diversity with detailed categories can be found at this **[Anonymous Link](https://anonymous.4open.science/api/repo/iclr2024_rebuttal-515F/file/inter_distribution.pdf)**. \n\nNote that the human-annotated facet categories cover a wide range of topics. Some facet categories may describe the structural attributes of the input (e.g., length, conversation format), while others may focus on the specific textual content (e.g., the input contains a location or time entity).\n\nWe will update these analyses in our next version and will try to elaborate on more details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362251075,
                "cdate": 1700362251075,
                "tmdate": 1700362313635,
                "mdate": 1700362313635,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5J6DLev4cQ",
                "forum": "1vrS1zwekw",
                "replyto": "VevF2hSeiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer#4 --- Part 4 (Q5~Q6)"
                    },
                    "comment": {
                        "value": "> Q5: Comparing this work with the previous works (e.g., self-instruct), it's hard to identify the differences.\n\nWe highlight the differences between this work and the previous works below:\n\n- **Motivation**.\n\nThe main motivation of almost all previous works is the same \u2014 *how to use LLMs to collect a large-scale instruction-following dataset automatically*. \n\nHowever, the essential motivation of this work is totally different \u2014 instead of simply bootstrapping more data, our research question is ***how to reformulate the current learning paradigm for a better instruction-following capacity of LMs***. \n\nWe first thought deeply about the scaling-input paradigm; it follows a conventional multi-task learning scheme, where each task instruction has multiple input-output pairs. We assumed that there is a potential harm: models are conditioned on the **same instruction** but **different inputs** to generate **different outputs**, which implicitly teaches models to focus on inputs rather than instruction (because the inputs seem to be the key information to generate correct outputs in this case). \n\nNaturally, we came up with the proposed paradigm, where models are conditioned on **different instructions** but the **same input** to generate totally **different outputs**. \n\nIn a word, we respectfully disagree with the opinion that the proposed paradigm is merely a simple incremental production from the previous work. In contrast, *we believe it aligns well with human intuition and can inspire future work in this area*.\n\n\n- **Method**\n\nFollowing the aforementioned motivation, our data curation method is also quite different from the previous works due to the **unique challenge** in our paradigm.\n\nMost of the previous works tried to use existing instructions (`I`) as demonstrations and asked LLMs to brainstorm new instructions (`I\u2019`), then created the subsequent input (`X`) and output (`Y`). The whole procedure can be succinctly described as `I \u21d2 I\u2019 \u21d2 (X,Y)`.\n\nContrastingly, we start our data curation procedure from the input text (`X`) and try to gather diverse instructions (`I\u2019`) for it, and then annotate the outputs (`Y`), namely `X \u21d2 I\u2019 \u21d2 Y`. \n\nIn addition to the different procedures, creating instructions from the input (`X \u21d2 I\u2019`) is much more challenging than generating from existing instructions (`I \u21d2 I\u2019`), because the former requires more restrictions (instructions should \u201cmatch\u201d the input, there is less freedom) and is also much harder to be controlled (due to the hallucination of the LLMs). That\u2019s why we proposed two different methods for curating enough high-quality instructions, and used input-oriented facets to guide the instruction brainstorming.\n\nTo the best of our knowledge, due to the *unique nature* of the proposed paradigm, *collecting the corresponding dataset requires more special treats and designs in the method*, which any previous work has never covered.\n\n---\n\n> Q6: Share the limitations of this work.\n\nWe are willing to shed light on the following two limitations of this work:\n\n1. Since our initial motivation is to drive LMs to learn to follow the instructions instead of mapping inputs to outputs (as mentioned before), the contribution of this work mainly focuses on the changes in the dataset paradigms. However, we still follow a traditional supervised fine-tuning scheme to train the LMs. Therefore, **it remains unclear if our paradigm shift can really make a difference during the learning procedure** (such as the difference in gradient). But of course, it's sometimes difficult for humans to interpret the \"learning of models\". Therefore, **a special learning objective designed for \u201c`Scaling Task per Input`\u201d may help to enhance our conclusion**.\n\n2. What's more, this work is still following a dataset-scaling scheme in this area; considering the recent findings on \"less is more\" [3], we think it's also worthwhile to investigate **whether we do need to \"`scaling task per input`\" or just try to sample \"`less task per input`\"** and achieves more efficient instruction learning.\n\nRegarding the above limitations, there could be considerable space in this area for the future to dig deep into.\n\n---\n\n### References:\n[1]. [Self-Instruct: Aligning Language Models with Self-Generated Instructions.](https://arxiv.org/abs/2212.10560) (*ACL 2023*)\n\n[2]. [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.](https://arxiv.org/abs/2306.04751) (*NeurIPS 2023 Datasets and Benchmarks*).\n\n[3]. [Lima: Less is more for alignment.](https://arxiv.org/abs/2305.11206) (*ArXiv 2023*)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364622376,
                "cdate": 1700364622376,
                "tmdate": 1700364881755,
                "mdate": 1700364881755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Syb9wZVu7S",
                "forum": "1vrS1zwekw",
                "replyto": "VevF2hSeiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder from Author"
                    },
                    "comment": {
                        "value": "Dear Reviewer#4 (od6V),\n\n---\n\nWe are writing to inquire if you have had the chance to review our detailed response to your comments. We greatly value your feedback and would appreciate any further questions or comments you might have.\n\n---\n\nSincerely,\n\nAuthors of Paper#6820"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633243824,
                "cdate": 1700633243824,
                "tmdate": 1700633243824,
                "mdate": 1700633243824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wYhFM605Eh",
                "forum": "1vrS1zwekw",
                "replyto": "Syb9wZVu7S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's detailed clarification and efforts to address my concerns. As all my raised issues have been satisfactorily resolved, I have adjusted my score accordingly. Looking forward to reading the final manuscript."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733401445,
                "cdate": 1700733401445,
                "tmdate": 1700733401445,
                "mdate": 1700733401445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "COxq8ABvoL",
            "forum": "1vrS1zwekw",
            "replyto": "1vrS1zwekw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_j2qL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_j2qL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a two-step facet based instruction brainstorming method: in the first step, they use ChatGPT to recognize the textual facets of a given input; in the second step, they instruct ChatGPT to brainstorm task instructions. Besides direct instruction synthesis, the paper also suggests extracting human-written instructions from the training set and employ GPT-4 for binary classification. GPT-4 discriminates if the instruction can align with the input to create a valid task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper contributes to a important problem: dataset and alignment. The paper is well written.\n\n- The proposed method works as a reasonable approach to create a diverse tasks and instructions per task given the same input. GPT-4 works as a reasonable critique model, selecting high quality examples. Overall, the method is very simple but effective. \n\n- The paper reported strong empirical results on SuperNI-test, MMLU, T0-Eval, etc., compared to related work. Human evaluation acceptance ratio is significantly improved with Muffin."
                },
                "weaknesses": {
                    "value": "- The work can have better ablation on whether the diversity in the dataset or the quality of the dataset helps more. For example, without the facet based brainstorming, how would it perform? On the other side, without the filtering (GPT4 critique) how would it perform?\n\n- The paper lacks a scaling analysis on the size of the finetuning dataset. How does the model performance scale with the number of examples in the dataset?\n\n- The discussion on task leaking is too short. I feel it is a very important problem to discuss in greater details. For example, what is the SoTA metric to detect data leaking? Why would semantic similarities be sufficient in detecting? Sentences can mean similarly while being semantically different. I feel the paper can be much stronger if this part is well addressed."
                },
                "questions": {
                    "value": "1. Are there any particular reasons that different GPT models are selected for different purposes? Why would you pick ChatGPT over GPT4 to work on the facet recognition and instruction brainstorm? \n\n2. Any particular reason not to compare with FLAN [1]?\n\n[1]: https://arxiv.org/abs/2301.13688"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791214620,
            "cdate": 1698791214620,
            "tmdate": 1699636788902,
            "mdate": 1699636788902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wLf4w0lAAc",
                "forum": "1vrS1zwekw",
                "replyto": "COxq8ABvoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer#3 --- Part 1 (Q1~Q4)"
                    },
                    "comment": {
                        "value": "Thanks for your efforts in reviewing! We are happy you found our paper well-written, the proposed method reasonable, and the empirical results solid.\n\nSome of your suggestions are very important to improve our work. We adopted your comments and tried adding more analyses during this discussion period.\n\nAs shown below, we'd like to address your concerns in detail.\n\n---\n\n> Q1: A better ablation on the trade-off between diversity and quality.\n\nThis is a great point for better evaluating the proposed dataset. Your suggestion on the ablation between \u201cfacets-based instruction brainstorm\u201d and \u201cGPT-4 based rematching\u201d is a good perspective to investigate this question. \n\nWe put the ablation results in Table 13 in Appendix M, where we tested the ablation performances on the development set of SuperNI to avoid cherry-picking (same as the previous work [4]). Here, for your convenience, we also put that table below.\n\n| Methods | EM (CLS) | ROUGE-L (GEN) | ROUGE-L (overall) |\n|---|---|---|---|\n|        Instruction Rematching  | 29.45 | 46.11 | 35.95 |\n|        Instruction Brainstorm  | 28.26 | 47.35 | 36.27 |\n|        Rematching + Brainstorm  | 29.77 | 48.12 | 37.53 |\n|        Rematching + Brainstorm + CLS Expansion  | **32.77** | **48.56** | **41.2** |\n\nWe found that **both these methods contribute to the final performance, and seem to show complementary effects on each other** (combining the two methods resulting in better performance).\n\nThis conclusion aligns with previous works [1][2], that both factors matter greatly in the final generalization of LMs. However, to our knowledge, it\u2019s still hard to quantify their exact weights because various variables can affect the conclusion (e.g., the base models and the downstream generalization tasks).\n\nWe will shed more light on this question in our next version.\n\n---\n\n> Q2: The paper lacks scaling analysis of the datasets.\n\nThanks for your reasonable suggestions. It is critical to investigate the scaling trends to prove the robustness of our dataset.\n\nWe plotted the performance trends of different datasets (including Muffin and those competitive baselines) with various scales (10%, 30%, 50%, 80% 100%). \n\nFor your convenience, **you can find the resulting figure at this** **[Anonymous Link](https://anonymous.4open.science/api/repo/iclr2024_rebuttal-515F/file/scaling_trend_comparison.pdf)**.\n\nThe results demonstrate that Muffin consistently achieves superior performances among various scales, and exhibits a stable performance boosting w.r.t. the scaling. However, for some other baseline datasets, the performance even decreased after a certain scale threshold (perhaps due to the effect of the noise).\n\n---\n\n> Q3: Insufficient discussion on the task leakage.\n\nWe do agree with your concern about the task leakage analysis. \n\nTo our knowledge, since there is no SOTA leakage detection method specifically designed for instruction tuning, we followed the previous work [3] **using ChatGPT to judge the task leakage of different training datasets**.\n\nGiven an instance from a training dataset, we asked ChatGPT whether it belongs to any task categories from the evaluation benchmark. For example, the SuperNI-Test has 12 task categories, and we requested ChatGPT to perform a 13-way classification (one \u201c`None of above`\u201d label) on the training instances. Finally, we report how many instances from this training dataset contain task leakage.\n\nThe following table illustrates the results, comparing Muffin with the *most competitive baseline datasets*. Though there are some shifts in the leakage ranking compared with our original analysis (Figure 6 in Appendix N), the overall conclusion is still the same \u2014 **Muffin demonstrates relatively low evaluation task leakage across all four benchmarks**.\n\n|  | SuperNI-Test | MMLU | T0-Eval | BBH |\n|---|---|---|---|---|\n| Muffin | 16.01% | 13.05% | 8.63% | 2.53% |\n| Self-Inst. | 11.62% | 10.37% | 11.27% | 5.29% |\n| Unnatural Inst. | 22.27% | 16.91% | 18.39% | 4.76% |\n| Dynosaur | 12.08% | 13.49% | 15.20% | 1.27% |\n\nThe main objective of our analyses is to address the impact of other factors that may affect the contribution of our proposed paradigm. Though these leakage analyses are not that perfect, **we believe the task leakage is not a serious issue affecting our conclusion**, according to the above consistent observations.\n\n---\n\n> Q4: Particular reasons that different GPT models are selected for different purposes.\n\nDuring our preliminary trials, we observed GPT -4's superiority over ChatGPT in \"*Instruction Rematching*\", which can produce much more precise pair matching than ChatGPT. While for \"*Instruction Brainstorm*\", we found that ChatGPT was good enough and much cheaper. That's why we use ChatGPT for \"*Instruction Brainstorm*\" and GPT-4 for \"*Instruction Rematching*\". We will highlight this point in our next version.\n\n---"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366930168,
                "cdate": 1700366930168,
                "tmdate": 1700369157308,
                "mdate": 1700369157308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j1nrUSYmt2",
                "forum": "1vrS1zwekw",
                "replyto": "COxq8ABvoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder from Author"
                    },
                    "comment": {
                        "value": "Dear Reviewer#3 (j2qL),\n\n---\n\nWe are writing to inquire if you have had the chance to review our detailed response to your comments. We greatly value your feedback and would appreciate any further questions or comments you might have.\n\n---\n\nSincerely,\n\nAuthors of Paper#6820"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633140737,
                "cdate": 1700633140737,
                "tmdate": 1700633140737,
                "mdate": 1700633140737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C9hpB5bNV2",
            "forum": "1vrS1zwekw",
            "replyto": "1vrS1zwekw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_cEte"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_cEte"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use multiple facets of an input to construction instruction-input-output pairs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Curating instructions from different facets is an interesting idea. It can be combined with other data augmentation techniques.\n2. The presentation is straightforward and easy to follow."
                },
                "weaknesses": {
                    "value": "1. This method heavily relies on the human-curated SuperNI dataset. The Instruction Brainstorming requires extracting Instruction-input pairs from SuperNI while instruction reconstruction simply use data from SuperNI but do compositional changes. However, after those changes, the results of finetuning an LLM become worse than originally finetuning with SuperNI, as in Table 1. Does this mean we are introducing noise?\n2. Comparisons to some baselines might not be fair, although being claimed to directly compared results in the paper. I tihink the comparison with Dynasour and Unnatural instruction is unfair as again, the curation of the new dataset relies on some higher quality human instruction in SuperNI while the other two work doesn't rely on modifying human instructions. I think to really demonstrate the effectiveness of this input-based instruction construction method, the authors should try to start from Dynasour or Unnatural instruction and create new instructions on inputs from those datasets instead of from SuperNI.\n3. The creation of the facets lack human control. This is a limitation in the methodology that the framework cannot control which facets will be generated from the first stage. \n4. This might be personal, but I have an overall negative attitude towards work like this one. This work basically falls into the category of exploiting the ability of LLMs to augment the instruction tuning data. The changes in this paradigm compared to previous ones look incremental, simply trying to change a bit on how we prompt LLMs to generate instruction-input-output pairs. Notice that this idea is also not new as in Dynasour, they also generate multiple instructions by asking LLMs to focus on different parts of the metadata. Most importantly, I don't know how this paradigm would really impact how people create instruction data. Does it worth trying this paradigm or simply using the queries to chatgpt to collect more data?"
                },
                "questions": {
                    "value": "1. See the weakness section. Maybe elaborate more on your opinions about point 1 and 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832652494,
            "cdate": 1698832652494,
            "tmdate": 1699636788772,
            "mdate": 1699636788772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GifDVKH0Kw",
                "forum": "1vrS1zwekw",
                "replyto": "C9hpB5bNV2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer#2 --- Part 1 (Q1~Q4)"
                    },
                    "comment": {
                        "value": "Your comments are very much appreciated! We are glad you found our idea interesting and the presentation clear.\nWe took your comments carefully and have added some missing experiments you mentioned.\nWe address your concerns one by one as follows:\n\n---\n\n> Q1: Muffin highly relies on the human-crafted SuperNI dataset.\n\nOur data curation method does utilize the resources from SuperNI, but it\u2019s hard to say our method \u201c*highly*\u201d relies on SuperNI, because: \n- For \u201c*Instruction Brainstorm*\u201d, we only used 3 instructions from SuperNI as demonstrations (which is similar to all the previous works); For \u201c*Instruction Rematching*\u201d, we extracted instructions and inputs from SuperNI **separately while NOT keeping the original correspondence** between them (the instruction-input correspondence should be the most critical part of human annotation).  \n- Additionally, our method can be practically applied to **any** textual resources and still results in comparable instruction-following performances (please refer to the *experimental results in Q3*).\n\n---\n\n>Q2: Muffin is sourced from SuperNI, but introduces more noise.\n\nWe agree with your concern and address it with the following points:\n- To our knowledge, all the LLM-generated datasets (e.g., alpaca, self-inst) introduced more noise compared with the source datasets. However, one main contribution of this area is to use LLMs to automatically curate instruction-following instances instead of relying on expensive human labor.\n- According to our human analysis, Muffin suffers less noise than previous works (pls see Figure 4 in Appendix E).  \n\n---\n\n> Q3: The authors should create new instructions on the inputs sourced from Dynosaur or Unnatural Inst. to ensure a fair comparison.\n\nThis is a very good suggestion! \n\nDuring this discussion period, we gathered the inputs from other LLM-generated datasets (including Dynosaur and Unnatural-Inst, as suggested by you), and applied our \u201c*Instruction Brainstorm*\u201d methods to them. Finally, we created two small-scale datasets (about 16k), namely *Muffin-Dynosaur* and *Muffin-Unnatural*, and compared their performances with Dynosaur and Unnatural-Inst (in the same size).\n\nThe results (with T5-3B models) can be found in the following table:\n\n| **Models** | **SuperNI (CLS)** | **SuperNI (GEN)** | **SupnerNI (overall)** | **MMLU (ACC)** | **MMLU (EM)** | **T0-Eval (ACC)** | **T0-Eval (EM)** | **BBH (EM)** | **AVG** |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| Unnatural Inst. | 23.94 | 44.36 | 37.12 | 24.35 | 22.33 | 45.93 | 35.88 | 8.69 | 30.33 |\n| Dynosaur | 25.03 | 42.08 | 32.55 | 26.88 | 25.26 | 38.56 | 39.13 | 12.09 | 30.2 |\n| Muffin-Unnatural | 27.53 | 47.7 | 38.86 | 33.12 | 24.73 | **46.08** | 43.63 | 12.91 | 34.32 |\n| Muffin-Dynosaur | 27.72 | 46.7 | 38.23 | **33.34** | 22.59 | 45.3 | **44.01** | **13.53** | 33.93 |\n| Muffin | **30.77** | **48.82** | **40.45** | 33.24 | **25.39** | 42.68 | 40.99 | 13.44 | **34.47** |\n\n---\n\n We observed that Muffin consistently demonstrates superior generalization performance, regardless of the text sources employed. It further suggests that Muffin's robust ability to follow instructions is not reliant on the input resources but rather **stems more from our crucial contribution\u2014the diversified instructions per input**. Therefore, we anticipate that our method can be ideally applied to any resources while still crafting diverse task instructions.\n\n---\n> Q4: Is it worth trying this paradigm or simply using the queries to chatgpt to collect more data?\n\nWe hope we can answer your question by plotting the scaling trends of different datasets. Specifically, we randomly sampled subsets from the original datasets and trained T5-3B on them to show the trends (10%, 30%, 50%, 80%, 100%). \n\nThe resulting figure can be found in this **[Anonymous Link](https://anonymous.4open.science/api/repo/iclr2024_rebuttal-515F/file/scaling_trend_comparison.pdf)**.\n\nIn the range of 68K, MUFFIN exceeds the baselines by a noteworthy margin (average scores on 4 evaluation benchmarks). Other baselines may only **be comparable to our data results when they continue to be scaled to several times the size of our data**. \n\nMore importantly, the performances of some datasets even decrease after scaling to a larger size (perhaps due to the noise in these LLM-generated datasets), such as Self-Inst. and Dynosaur. Therefore, we conjecture that our paradigm is more efficient than simply collecting more data from the other paradigms. \n\n---"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357585369,
                "cdate": 1700357585369,
                "tmdate": 1700360426354,
                "mdate": 1700360426354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p7ncBmONGj",
                "forum": "1vrS1zwekw",
                "replyto": "C9hpB5bNV2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder from Author"
                    },
                    "comment": {
                        "value": "Dear Reviewer#2 (cEte), \n\n---\n\nWe are writing to inquire if you have had the chance to review our detailed response to your comments. We greatly value your feedback and would appreciate any further questions or comments you might have.\n\n---\n\nSincerely,\n\nAuthors of Paper#6820"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633018625,
                "cdate": 1700633018625,
                "tmdate": 1700633018625,
                "mdate": 1700633018625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YrTmn0a8T8",
                "forum": "1vrS1zwekw",
                "replyto": "GifDVKH0Kw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_cEte"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_cEte"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts in conducting the strategies in this paper on Dynasour and Unnatural-Instruct. The results look good.\n\nHowever, I'm still concerned about how much this method relies on the existing human-annotated datasets. It looks like from Table 13 that the classification expansion is the most effective strategy among the three, which is the reason that differs MUFFIN from other baselines (without it the performance is ~37 which is incrementally better than baselines and much worse than SuperNLI). It has been known that the performance on SuperNLI-test set and other benchmarks like MMLU will be influenced a lot by the number of classification tasks, Adding more classification tasks, in my opinion, is a shortcut to improve the performance. Thus, I'm a bit concerned this method is only designed to improve the performance as it doesn't totally fit into the story. The story is to augment instructions, but classification expansion simply converts some human-annotated instruction-input-output triplets to another format.\n\nBesides, I'm still not sure whether this paradigm can be a widely adopted one because of its being hard to control. We will not be able to control which facets to use and how many generation tasks to convert to classification tasks to optimize model performance.\n\nSo, I prefer not to change my original scores. I'm pretty confidence about the current score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645260345,
                "cdate": 1700645260345,
                "tmdate": 1700645260345,
                "mdate": 1700645260345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2az7xFirvz",
            "forum": "1vrS1zwekw",
            "replyto": "1vrS1zwekw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a technique for synthesizing instruction fine-tuning data using LLMs (ChatGPT and GPT-4). In particular, the paper draws a distinction between past works in this area which have focused on either adopting an instruction+input format and scaling the number of inputs per instruction (Scaling-Inputs), or adopting an instruction-only format and scaling the number of total instructions (Scaling Input-Free Tasks). As an alternative, the technique and resulting dataset, MUFFIN, presented in this work adopts the instruction+input format but uses automated techniques to scale the number of instructions per input (Scaling Tasks per Input).\n\nExperimental comparisons to extensive baselines are presented on SuperNI, MMLU, T0, and BBH (each classified as either Scaling-Inputs, Scaling Input-Free Tasks, or Hybrid) and demonstrate the effectiveness of the proposed approach in all 3 settings. Additional experiments and results from human evaluation are also presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The topic of how to effectively scale synthetic instruction datasets is relevant and timely, and the dichotomy of scaling inputs vs instructions is interesting.\n2. The experiments are extensive and the inclusion of evaluation of datasets from multiple settings (scaling-inputs, input-free, etc) is appreciated.\n3. The methodology and techniques are mostly well-described and presented clearly."
                },
                "weaknesses": {
                    "value": "1. The muffin looks more like a cupcake to me.\n2. As noted previously, the dichotomy of scaling inputs vs instructions is interesting. However, the presentation makes it seem like one must choose between the two when really they are more like two extremes of a spectrum. This very naturally leads me to wonder whether a hybrid that scales both according to some mixture proportion would be most effective. This seems like a somewhat large omission given it would be very testable with the synthetic data here.\n3. All experiments use T5-3B or T5-11B. As many of the other datasets (Self-Instruct, Dolly, Alpaca) were developed and tested using more recent LMs (Llama, etc), it would be useful to see if/how the results change when fine-tuning such models. At minimum, it would be useful to see performance numbers for the original models developed with these datasets (if they are available) in the \"Existing Systems\" section of Table 1 to understand the performance drop due to changes in setup (model, hyperparams, etc).\n4. I think there are a few errors in the baseline discussion in section 5. In particular, this section says Dolly and Self-Instruct were collected with ChatGPT or GPT-4. As far as I know, Dolly is human-created (this is stated correctly in the Appendix) and Self-Instruct was produced using GPT-3.\n5. The presentation of human evaluation was somewhat confusing and left out key details. What model was used, 3B or 11B? Additionally, the evaluation is described as follows, \"we provide the volunteer with only task instruction, input, and model prediction.\" However, MMLU is classified as \"Input-Free\". What do annotators see as the input in this setting?\n\n**Minor Issues**\n\n1. The Scaling-Inputs, Scaling Input-Free Tasks, and Scaling Tasks per Input terminology is somewhat awkward. Have the authors considered something slightly simpler like Scaling-Inputs, Scaling-Input-Free, and Scaling-Instructions?\n2. I found the \"our MUFFIN\" terminology used throughout the paper somewhat annoying and distracting.\n\n------\n\nI have reviewed the authors' responses and raised my score accordingly."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6820/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6820/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858165636,
            "cdate": 1698858165636,
            "tmdate": 1700693029679,
            "mdate": 1700693029679,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D2IlpUVeWg",
                "forum": "1vrS1zwekw",
                "replyto": "2az7xFirvz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Reponse to Reviewer#1 --- Part 1 (Q1~Q3)"
                    },
                    "comment": {
                        "value": "Many thanks for your detailed review! We are glad you found this work interesting, the experiments solid, and the presentation clear. During this discussion period, we conducted more experiments per your suggestions to improve this work. As shown below, we summarize your main concerns and address them one by one:\n\n---\n\n> Q1: Whether a hybrid paradigm that scales both (`Scaling Inputs` & `Scaling Tasks per Input`) according to some mixture proportion would be most effective.\n\nThis is a great point to investigate!\n\nTo figure out the answer to this question, we mixed Muffin (`Scaling Tasks per Input`) with SuperNI-Train (`Scaling Inputs`) with various proportions (0.0, 0.1, 0.2, \u2026, 0.9, 1.0), where the proportion means how many instances are from Muffin. Then, we trained T5-3B on these mixtures and reported the average performances on the four benchmarks.\n\nFor your convenience, we plotted the figure at this **[Anonymous Link](https://anonymous.4open.science/api/repo/iclr2024_rebuttal-515F/file/mixture_trends.pdf)**.\n\nNote that, SuperNI-Train is created by humans and has much higher quality. That\u2019s why using only SuperNI-Train resulted in the best performance in this figure (i.e., `proportion == 0`).\n\nHowever, the most interesting finding is that, after mixing Muffin with more high-quality data from SuperNI-Train, the **model\u2019s performance first dropped and then increased**.\n\nThus, we draw mainly two conclusions here:\n1. **The dataset paradigm does affect the learning efficiency**. Different dataset paradigms do have obviously various impacts on the model\u2019s performance. Therefore, simply combining two different paradigms can even hurt the model\u2019s instruction-tuning efficiency, meaning the paradigm is a critical factor.\n2. **The superiority of the proposed paradigm**. Meanwhile, the effect of dataset paradigms is even greater than that of data quality (adding a small proportion of high-quality data from other paradigms even harms Muffin\u2019s performance), further implying the proposed paradigm's effectiveness.\n\nThis observation also inspires us to release an even higher-quality dataset in the future, that has both quality and paradigm superiority (as also suggested by reviewer#4).\n\n---\n\n> Q2: The performances with the most recent LLMs (e.g., LLaMA).\n\nWe agree that we have to adopt some other models besides T5.\n\nDuring this discussion period, we conducted experiments with the most advanced decoder-only LMs, namely **LLaMA-2 (13B)**, to further enhance the reliability of our conclusion. Since the training and inference of LLaMA-2-13B were time-consuming, we reported only the performances of Muffin and those most competitive baselines; meanwhile, we randomly selected 2k testing instances from each evaluation benchmark (every evaluation task was covered) to show the comparison, instead of the whole testing benchmarks (which will take more than two weeks).\n\nThe results are shown in the following table. The main conclusion is similar to T5\u2019s results in our paper, where Muffin consistently gains better overall performances compared with the baselines. \n\n| Models | SuperNI (CLS) | SuperNI (GEN) | SupnerNI (overall) | MMLU (ACC) | MMLU (EM) | T0-Eval (ACC) | T0-Eval (EM) | BBH (EM) | AVG |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| Self-Inst. | 27.34 | 40.9 | 33.78 | 33.56 | 34.28 | 56.78 | 51.55 | **33.77** | 38.99 |\n| Unnatural Inst. | 38.12 | 46.98 | 45.02 | 38.75 | 27.98 | 50.13 | 49.88 | 13.56 | 38.8 |\n| Dynosaur | **39.13** | 50.09 | 45.84 | 34.37 | 35.82 | 51.28 | 47.2 | 12.33 | 39.51 |\n| Muffin | 38.3 | **53.41** | **46.62** | **46.46** | **43.3** | **60.07** | **52.37** | 27.08 | **45.95** |\n\nWe will add the completed inference results of LLaMA-2-13B in our next version, and fill in the performances of the \"original models\" trained on those baseline datasets to the \"Existing Systems\" in Table 1.\n\n---\n\n> Q3: The presentation of human evaluation omits key details.\n\nWe are sorry for letting you get confused.\n\n- *For the models used in human evaluation*: We use **T5-3B** through all the analysis sections. We will highlight it by mentioning it in the table captions.\n- *What \u201cinput\u201d was provided to the human when facing \"input-free\" tasks*: For those \"input-free\" tasks, we just **left the \"input\" field empty** (pls refer to Table 12 in Appendix L). For a better illustration, we provide the figure of the human evaluation interface at this **[Anonymous Link](https://anonymous.4open.science/api/repo/iclr2024_rebuttal-515F/file/human_eval_interface.jpg)**.\n\n---"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550992397,
                "cdate": 1700550992397,
                "tmdate": 1700551788363,
                "mdate": 1700551788363,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WxR4Afb3Dx",
                "forum": "1vrS1zwekw",
                "replyto": "2az7xFirvz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder from Author"
                    },
                    "comment": {
                        "value": "Dear Reviewer#1 (BsoU), \n\n---\n\nWe are writing to inquire if you have had the chance to review our detailed response to your comments. We greatly value your feedback and would appreciate any further questions or comments you might have.\n\n---\n\nSincerely,\n\nAuthors of Paper#6820"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632801384,
                "cdate": 1700632801384,
                "tmdate": 1700632959374,
                "mdate": 1700632959374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NNNn7U65pb",
                "forum": "1vrS1zwekw",
                "replyto": "WxR4Afb3Dx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the thorough reply and addressing my questions. I will focus on Q2 as I feel my other questions have been addressed sufficiently.\n\nThe findings from the mixture experiment are quite interesting. I appreciate the authors' putting forward some hypotheses, but I'm not sure I can confidently draw the same conclusions (at least not with any confidence). Please ensure any such hypotheses put forward in the paper are clearly noted as such. I am particularly unsure how to interpret that \"adding a small proportion of high-quality data from other paradigms even harms Muffin\u2019s performance\" as this seems counter to my general intuition.\n\nI believe the paper has improved since my initial review and represents a valuable contribution; at minimum bringing attention to a previously unexplored dichotomy in the way instruction examples are formatted, providing a useful instruction tuning data set, and putting forward some interesting results related to the way mixing datasets harms performance. I will raise my score accordingly."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692941867,
                "cdate": 1700692941867,
                "tmdate": 1700692941867,
                "mdate": 1700692941867,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YMPaazivf2",
                "forum": "1vrS1zwekw",
                "replyto": "2az7xFirvz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's Follow-up Response to Reviewer#1"
                    },
                    "comment": {
                        "value": "Dear Reviewer#1 (BsoU),\n\n---\n\nThanks so much for reading our response and raising the score. It's our pleasure that our additional experiments and efforts can earn your recognition.\n\nRegarding the interesting results of mixing SuperNI, we will definitely attach it with more details and try to figure out more reasonable explanations in our next version. \n\nThanks again for your recognition of our paper's contributions.\n\n---\n\nBest regards,\n\nAuthors of Paper#6820"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717335250,
                "cdate": 1700717335250,
                "tmdate": 1700717374774,
                "mdate": 1700717374774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]