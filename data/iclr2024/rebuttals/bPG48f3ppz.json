[
    {
        "title": "SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network"
    },
    {
        "review": {
            "id": "c8T428qaus",
            "forum": "bPG48f3ppz",
            "replyto": "bPG48f3ppz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission560/Reviewer_EDPh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission560/Reviewer_EDPh"
            ],
            "content": {
                "summary": {
                    "value": "The author proposed a cross-modal SNN, named SpikeCLIP, which can perform feature extraction and alignment across multiple modalities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The author proposed a cross-modal model for SNN and explored some downstream tasks based on it."
                },
                "weaknesses": {
                    "value": "1. The author mainly adopted the idea of knowledge distillation to train SpikeCLIP, however, a similar scheme [1] has been proposed previously. In addition, regarding the algorithm and neuron model design of SNN, I think the contribution of this paper is very limited. I think this paper is more about directly transferring the concepts related to CLIP to the field of SNN and lacks technical contributions related to SNN.\n\n2. The performance of SpikeCLIP on downstream datasets (CIFAR-10, CIFAR-100) is not superior, and the author did not fully list recent works about single-modality SNN in Table 1. For example, [2] can achieve higher performance (CIFAR-10: 95.58%, CIFAR-100: 78.71%, 4 time-steps) by directly training on ResNet-19 than SpikeCLIP, which means that the author's pre-trained multi-modality model based on ImageNet-1k is even inferior to a single-modality SNN through direct training on ResNet-19.\n\n3. The training dataset (ImageNet-1k) and model parameter size (56.87M) used by the author in this paper are too small. Although this paper is a preliminary exploration of SNN cross-modal learning, the performance achieved by the author and the number of downstream tasks attempted have serious deficiencies.\n\n4. In Figure 2, the text encoder section of SpikeCLIP uses the LayerNorm layer. However, the floating-point multiplication operations involved in LayerNorm are usually not allowed in the inference stage of SNN.\n\n[1] Xu, Qi, et al. \"Constructing deep spiking neural networks from artificial neural networks with knowledge distillation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2] Duan, Chaoteng, et al. \"Temporal effective batch normalization in spiking neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 34377-34390."
                },
                "questions": {
                    "value": "See Weakness Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission560/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission560/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission560/Reviewer_EDPh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698136747518,
            "cdate": 1698136747518,
            "tmdate": 1699635983374,
            "mdate": 1699635983374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bw3H9sLEAb",
                "forum": "bPG48f3ppz",
                "replyto": "c8T428qaus",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you very much for your valuable comments!**\n\n**Q1: The work of training SNNs using KD principles has already been done, and SpikeCLIP lacks algorithms and neuron designs specific to SNNs, so is the contribution of SpikeCLIP relatively limited?**\n\n**R1:** The work you mentioned [1] employed KD principles to train single-modal CNN-based SNNs in a step-wise manner, whereas our work effectively trained multi-modal Transformer-based SNNs using the paradigm of aligned pre-training followed by dual-loss fine-tuning. The difficulties and emphasis between the two works are different. Due to the scarcity of SNNs in the field of natural language processing (mainly due to the absence of authoritative architectures), we attempted to represent both modalities' information effectively through direct training with spike signals, aligning and fusing them, which is more challenging than distilling single-modal information. To achieve this, we not only modified the original Spikingformer on the image encoder side (adding TSW, as shown in A.1) but also compared the effectiveness of different text encoder architectures. While it is indeed feasible to easily transition from CLIP to the SNN domain using methods such as weight transfer, our focus is on validating whether modal information represented by spike signals can effectively undergo modal fusion, similar to what ANNs achieve. Our work's contribution lies in emphasizing this point.\n\n\n\n\n**Q2: The performance of SpikeCLIP on downstream datasets is not superior, and the author did not fully list recent research achievements in single-modal SNNs in Table 1?**\n\n**R2:** We have included several representative works of single-modal SNNs in Table 1, but not all, as there is indeed a considerable amount of relevant research.  We have also listed the latest works on single-modal SNNs, such as [2] (CIFAR10: 95.95%; CIFAR100: 80.37%, 4-time steps), which outperforms the one you mentioned [3] and is currently the best-performing.  \n\nAdditionally, you noted that the performance of SpikeCLIP is inferior to a single-modal SNN trained directly through ResNet-19. This is inevitable because comparing the accuracy of a single-modal model with a multi-modal model is inherently unfair and meaningless. As SpikeCLIP is the first multi-modal SNN, we intentionally included in Table 1 a fair comparison between the best single-modal and multi-modal models in both the SNN and ANN domains to better assess its performance.  Table 1 compares the accuracy of models on CIFAR10/100.  In the ANN domain, the best single-modal model has an accuracy advantage of 0.68/4.50% over the best multi-modal model, while in the SNN domain, the best single-modal model outperforms SpikeCLIP by 1.47/2.68%. Compared to the gap in ANNs, the gap in SNNs is within an acceptable range, and on CIFAR100, the gap in SNNs is even smaller. We believe this sufficiently reflects the performance level of SpikeCLIP. Furthermore, SpikeCLIP possesses zero-shot learning capabilities, a feature absent in traditional single-modal SNNs.  Faced with new downstream tasks, SpikeCLIP can follow the fine-tuning paradigm without starting training from scratch, a characteristic worth considering. In conclusion, demanding that multi-modal SNNs surpass single-modal SNNs in performance while leveraging advantages unique to single-modal models is the next important challenge."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699771135328,
                "cdate": 1699771135328,
                "tmdate": 1699771135328,
                "mdate": 1699771135328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g6A8wb4taK",
                "forum": "bPG48f3ppz",
                "replyto": "c8T428qaus",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear commenter EDPh:**\n\n**We greatly appreciate your time and effort in reviewing our work.** We have considered your questions carefully and made the necessary changes.\nSpecifically, we have revised section **A.1** to elaborate in more detail on our introduction of TSW, an architectural setup that significantly improves SNNs performance. We have revised **3.3** to express in more detail the special insights and significance of our \"alignment pre-training + dual-loss fine-tuning\" training framework; We have added **Table 1** to include the baseline you mentioned in the comparison range; Finally, we give our considerations for choosing to use the LN layer in **A.3**. You can also refer to our official review for a more comprehensive response on the motivation and contribution to this study, the training framework, the performance of SpikeCLIP, and the implementation of LN in SNN.\n\nYour expertise and insights are invaluable and we are keen to ensure that our research meets the highest standards. We look forward to hearing more from you and will be happy to answer further questions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186608476,
                "cdate": 1700186608476,
                "tmdate": 1700186608476,
                "mdate": 1700186608476,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2vNr2PkshY",
                "forum": "bPG48f3ppz",
                "replyto": "c8T428qaus",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear Reviewer EDPh:**\n\nWe sincerely thank you for taking the time out of your busy schedule to conduct a thorough review of our paper and provide valuable comments.\n\nAs the Author-Review Discussion period is drawing to a close with only **two days** remaining, we would like to ensure that all your concerns have been adequately addressed. If there are any questions or unresolved issues, we are eager to provide further clarification or make necessary revisions.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544996147,
                "cdate": 1700544996147,
                "tmdate": 1700544996147,
                "mdate": 1700544996147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RuBhzuSJJB",
            "forum": "bPG48f3ppz",
            "replyto": "bPG48f3ppz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors introduce SpikeCLIP inspired by CLIP. To realize the SpikeCLIP, the authors provide an Alignment Pre-training + Dual-Loss Finetuning method. Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across various datasets commonly used for multimodal model evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This is the first work to transfer the CLIP to the SNN field.\n2. The authors provide the code, which is good."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. The two steps can be seen as the KD method. So the work just uses a KD method to convert a CLIP as SpikeCLIP.\n2. The results are not good. For image classification, the accuracy is worse than other SOTA methods. For the zero-shot task, since SpikeCLIP is not trained on a really large dataset, it is much worse than CLIP, thus the value of SpikeCLIP is limited, considering that the greatest value of CLIP is suitable for zero-shot tasks."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission560/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission560/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698593528725,
            "cdate": 1698593528725,
            "tmdate": 1699795274714,
            "mdate": 1699795274714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9eYsoDtuPY",
                "forum": "bPG48f3ppz",
                "replyto": "RuBhzuSJJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you very much for your valuable comments!**\n\n**Q1: The novelty seems limited; is the alignment pre-training + dual-loss fine-tuning just a knowledge distillation (KD) method?**\n\n**R1:** Currently, most research on SNNs is primarily concentrated in the field of computer vision, with an extremely limited presence in natural language processing and a complete absence in the application of SNN architectures in the multimodal domain. There is currently no existing work exploring the feasibility of SNN architectures in the multimodal domain and whether the modality information represented by pulse signals can fuse modalities as effectively as the floating-point information in ANNs. However, with the growing prevalence of multimodal applications and the enormous energy consumption pressure brought by ANNs, the application of SNN architectures in the multimodal domain is becoming inevitable. Our work, inspired by CLIP, is the first attempt to explore the feasibility and effectiveness of using the spike-based computing paradigm for modality fusion, urging researchers to pay more attention to the application of SNNs in the multimodal context.\nWe propose the alignment pre-training + dual-loss fine-tuning framework, which effectively trains the first competitive and zero-shot learning-capable multimodal SNN. \n\nWhile our framework draws inspiration from CLIP and incorporates the idea of knowledge distillation (KD), it successfully accomplishes the task on its own merit. Knowledge distillation is a universally effective concept, and the focus of our alignment pre-training + dual-loss fine-tuning framework is not on whether KD principles are used, but on how they are applied. In our paper, we argue that the initial alignment pre-training step enables SpikeCLIP to acquire general image/text representation capabilities. In the dual-loss fine-tuning stage, as described in Section 4.3, we determine the use of two different losses by discerning the impact at different data granularities.\nMoreover, SpikeCLIP is not merely a transformation of CLIP into SpikeCLIP. SpikeCLIP and CLIP have distinct architectures (beyond the SNNs vs. ANNs difference) and different training processes, among other differences. Since CLIP conducts representation comparison at the end of a dual-stream architecture when performing multimodal tasks, a condition also achievable by current SNNs, we chose CLIP as a guide for SpikeCLIP. SpikeCLIP is not presented as a direct transformation from CLIP, and perhaps a better \"conversion\" could be achieved through weight transfer. However, this is not the focus of our paper.\n\n\n\n\n**Q2: SpikeCLIP's performance is not good; Its accuracy is lower than other state-of-the-art (SOTA) methods, and it is much worse than CLIP.  Does SpikeCLIP have limited value?**\n\n**R2:** Without a doubt, SpikeCLIP's accuracy is expected to be lower than the current best single-modal SNNs and significantly worse than CLIP. However, this comparison is unfair and meaningless because, at the same level, single-modal models perform worse than multimodal models, and under the same conditions, SNNs perform worse than ANNs.\n\nAt the same level, multimodal models have zero-shot learning capabilities, a feature we validated in Section 4.2.2 for SpikeCLIP, but currently, all single-modal SNNs lack this capability.\nAt the same level, SNNs have lower accuracy than ANNs because, unlike ANNs, SNNs use integer operations to process data streams. While this results in an acceptable decrease in accuracy, it also comes with higher energy efficiency.  In Section 4.5, we demonstrate that SpikeCLIP reduces energy consumption by 77.06% to 78.66% compared to its ANN counterpart.\n\nTo more fairly evaluate SpikeCLIP's performance, we compare the performance gaps between the best single-modal and multimodal models in SNNs and ANNs on CIFAR10/100 in Table 1. SpikeCLIP is currently the only multimodal SNN. In Table 1, the performance gap between SpikeCLIP and Spikingformer is 1.47%/2.68%, while the gap between CLIP and ViT is 0.68%/4.50%. These comparisons indicate that, as a multimodal SNN, SpikeCLIP's performance is competitive.\nOf course, it is not our intention to simply discuss the performance of SpikeCLIP. Due to the gap in the topic of multimodal SNN, our work is only a preliminary attempt to determine whether modal information represented by SpikeCLIP can be fused, which is also claimed in our paper. We believe that our work will lead to more outstanding research focusing on the performance weaknesses and application challenges of multimodal SNNs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699771093978,
                "cdate": 1699771093978,
                "tmdate": 1699771093978,
                "mdate": 1699771093978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "srEL7lFjzi",
                "forum": "bPG48f3ppz",
                "replyto": "RuBhzuSJJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. Considering that all the concerns of reviewers are the same, Maybe I have been a bit strict with the authors, so I have changed my score. However, I agree with other reviewers that the more important thing is actually to find the specifics of spikes in those architectures or settings, not just apply the SNN to other fields. I advise that the authors could give more special insights and meanings for the alignment pre-training + dual-loss fine-tuning framework. About the performance, the performance of the SpikeCLIP on zero-shot tasks is really much worse. I don't think that just SNN can save energy thus its performance can be ignored. So the response can not convince me. In addition, BN can be folded into the weights in the inference, but LN can not be. So the authors' responses to the questions of other reviewers are not good enough."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699795242026,
                "cdate": 1699795242026,
                "tmdate": 1699795347372,
                "mdate": 1699795347372,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pHQQROh8mN",
                "forum": "bPG48f3ppz",
                "replyto": "RuBhzuSJJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear commenter njnp:**\n\n**We greatly appreciate your time and effort in reviewing our work.** We have considered your question carefully and made the necessary changes. Specifically, we have revised section **3.3** to elaborate in more detail on the specific insights and implications of our training framework. We have also revised **4.2.1** so that everyone can more clearly compare SpikeCLIP's performance from a fair point of view. Finally, we detail our considerations for choosing the LN layer in section **A.3**.\n\nYour expertise and insights are invaluable and we are keen to ensure that our research meets the highest standards. We look forward to hearing more from you and will be happy to answer further questions."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186590966,
                "cdate": 1700186590966,
                "tmdate": 1700186590966,
                "mdate": 1700186590966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9PSZlXICAE",
                "forum": "bPG48f3ppz",
                "replyto": "RuBhzuSJJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear Reviewer njnp:**\n\nWe sincerely thank you for taking the time out of your busy schedule to conduct a thorough review of our paper and provide valuable comments.\n\nAs the Author-Review Discussion period is drawing to a close with only **two days** remaining, we would like to ensure that all your concerns have been adequately addressed. If there are any questions or unresolved issues, we are eager to provide further clarification or make necessary revisions.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544973181,
                "cdate": 1700544973181,
                "tmdate": 1700544973181,
                "mdate": 1700544973181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QDCcHGpjcE",
            "forum": "bPG48f3ppz",
            "replyto": "bPG48f3ppz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission560/Reviewer_edZQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission560/Reviewer_edZQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SpikeCLIP, an image-text multi-modal SNN based on CLIP, and a two-stage training method to fine-tune it on downstream tasks. The resulting model can achieve comparable performance on mainstream image datasets with reduced energy consumption and maintains robustness on zero-shot classifications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper presents the SNN image-text multi-modal model.\n\n2. The two-stage fine-tuning approach retains the performance of the original CLIP model on various tasks, including classification with undefined class labels."
                },
                "weaknesses": {
                    "value": "1.The proposed model architecture for each modal separately is not innovative enough. The image encoder uses an existing SNN architecture (Spikingformer), while the text encoder is a simpler MLP structure, bypassing the difficulties of processing long sequences with SNNs. This design choice improves training efficiency but may limit the model's text-processing capabilities.\n\n2.The two-stage training process of distillation followed by task-specific fine-tuning lacks specific optimization for SNN computational characteristics.\n\n3.Due to the inaccessibility of CLIP's full pretraining dataset, this work uses the smaller ImageNet-1k for distillation pretraining. This restricts the model's generalization capability compared to the original CLIP, including both image and language modalities. More pretraining data would likely be necessary for the model to serve as a general-purpose multimodal foundation model, which may lead to more future challenges, such as convergence and training efficiency."
                },
                "questions": {
                    "value": "1. Does the simple MLP text encoder sacrifice generalization ability in language understanding? For example, the paper does not describe the text templates used in zero-shot classification. If a fixed template like \"a photo of a {label}\" is used throughout, the text encoder's role may be oversimplified and insufficient to handle other plausible templates such as \u201ca picture of a {label}\u201d. More details should be provided on text settings.\n\n2. ImageNet-1k is used for alignment pretraining before fine-tuning on other datasets. However, test accuracy after further fine-tuning on ImageNet itself is not reported. Does SpikeCLIP have adequate representational capacity and scalability for such large-scale image classification tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646017572,
            "cdate": 1698646017572,
            "tmdate": 1699635983195,
            "mdate": 1699635983195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wZ87dshqNP",
                "forum": "bPG48f3ppz",
                "replyto": "QDCcHGpjcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you very much for your valuable comments!**\n\n**Q1: Does the MLP text encoder sacrifice the generalization ability of language understanding? Regarding the use of text templates in zero-shot classification.**\n\n**R1:** In order to imbue the text decoder of SpikeCLIP with text representation capabilities and for the purpose of comparison with CLIP, our constructed dataset $D_{txt}$  is derived from 27 datasets used to evaluate CLIP's zero-shot learning ability. As mentioned in A.4, for each dataset, we combine its labels with the corresponding template set, thus constructing  $D_{txt}$ with a total of 115,708 entries. We sincerely apologize for not providing more detailed construction details of  $D_{txt}$ in the paper, causing confusion. However, these template sets encompass various templates, and you can refer to the supplementary material for a detailed examination.\nFor the image classification datasets used in this paper, training a text decoder using $D_{txt}$ can yield a sufficiently effective result. However, as you rightly pointed out, a simple MLP text encoder inevitably sacrifices the generalization ability of language understanding, despite our comparison of the effectiveness of two different architectures used as text encoders. Therefore, constructing a better text encoder structure remains an important problem we aim to address in our future work.\n\n\n**Q2: Why is the test accuracy after further fine-tuning on ImageNet itself not reported? Does SpikeCLIP have sufficient representational capacity and scalability?**\n\n**R2:** Following the paradigm of alignment pre-training + dual-loss fine-tuning, we deliberately set the downstream datasets to be different from the pre-training dataset. This setup aims to demonstrate the effectiveness of pre-training and the zero-shot learning capability of SpikeCLIP. In Section A.6, we report SpikeCLIP's performance when the distributions of the pre-training dataset and the downstream dataset are dissimilar, indirectly validating the good performance when using ImageNet-1k for both alignment pre-training and dual-loss fine-tuning. \n\nFurthermore, this is the first attempt to use the spike-based computing paradigm for multimodal classification tasks. Unlike CLIP, we do not have access to larger datasets for alignment pre-training, which indeed imposes significant limitations on SpikeCLIP's representational capacity and scalability, as you rightly mentioned. However, in Section 4.3, we experimentally verify the potential for improved SpikeCLIP performance with the existence of more data. Certainly, this is a challenging task, but it doesn't hinder us from confirming the feasibility of SNNs in multimodal scenarios."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699771042737,
                "cdate": 1699771042737,
                "tmdate": 1699771042737,
                "mdate": 1699771042737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9cK6LAnjfG",
                "forum": "bPG48f3ppz",
                "replyto": "QDCcHGpjcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear reviewer edZQ:**\n\n**We greatly appreciate your time and effort in reviewing our work.** We have considered your questions carefully and made the necessary changes.\nSpecifically, we further analyze our considerations for choosing the MLP architecture as the text encoder in section **A.3**.  Although the architecture is simple, in our work, there is also not much text information consisting of labels of data sets, which makes the simple MLP architecture suitable for the text processing capabilities of SpikeCLIP.  In addition, we provide more details of $D_{txt}$ in **A.4** (including different templates), and finally, you can refer to our official review for A more comprehensive summary of SpikeCLIP's performance.\n\nYour expertise and insights are invaluable and we are keen to ensure that our research meets the highest standards. We look forward to hearing more from you and will be happy to answer further questions."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186577873,
                "cdate": 1700186577873,
                "tmdate": 1700186577873,
                "mdate": 1700186577873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aeH6Xoy6Vh",
                "forum": "bPG48f3ppz",
                "replyto": "QDCcHGpjcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear Reviewer edZQ:**\n\nWe sincerely thank you for taking the time out of your busy schedule to conduct a thorough review of our paper and provide valuable comments.\n\nAs the Author-Review Discussion period is drawing to a close with only **two days** remaining, we would like to ensure that all your concerns have been adequately addressed. If there are any questions or unresolved issues, we are eager to provide further clarification or make necessary revisions.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544925830,
                "cdate": 1700544925830,
                "tmdate": 1700544925830,
                "mdate": 1700544925830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gq65g9FO9X",
            "forum": "bPG48f3ppz",
            "replyto": "bPG48f3ppz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission560/Reviewer_edTp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission560/Reviewer_edTp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed SpikeCLIP, a framework inspired by CLIP method to dually deal with both image and text input through spiking neural network. The SpikeCLIP is trained through a two-staged \u201calignment (CLIP) pre-training + Dual-Loss Fine-tuning\u201d, showing good results in classification tasks and zero-shot learning tasks. Compared to conventional CLIP, spikeCLIP shows theoretically high energy efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- SpikeCLIP is the first multimodal SNN architecture that shows the ability to deal with both text and image input. \n- The paper proposes an effective training framework to train SpikeCLIP. The alignment pre-training part finds a good way to transfer the representation learned in CLIP to SpikeCLIP.\n- The implementation of the framework and the experiments are solid. Moreover, the experiments results shows robust performance in image classification tasks (including zero-shot setting)."
                },
                "weaknesses": {
                    "value": "- Although this is the first paper (as far as I know) to realize spiking version of CLIP, the paper itself is lack of enough novelty. Nowadays, as the surrogate gradient based SNN training methods have been greatly developed, transferring or reproducing a specific architecture in conventional ANN(artificial neural network) to SNN is never a significant issue. More important thing is actually to find the specifics of spikes in those architectures or settings, rather than claiming \u201cwe are the first spiking version of xx\u201d. Unfortunately, I did not find such highlights in this paper.\n- One may argue that this paper shows the energy efficiency of SpikeCLIP compared with its counterpart ScratchCLIP, however, such comparison is not fair enough. As shown in the appendix, the authors only calculate the SOPs corresponding to each spike, while the updating process of membrane potential, the BN/LN operations are not included. It is worth to note that these are only the computing energy. Moreover, the additional energy that needed to maintain the membrane potential for each neuron, which is actually more severe in reality, is not considered in their calculation. No need to mentioning the factual energy lies mostly in data/weight transferring, which is not mentioned either in the paper (the weight amount is the same, and the data input is in fact more than ANN, given one needs to repeat T times of input). I understand this is a paper focusing on algorithms, discussing the real implementation is somewhat out of the scope. However, if the only novelty or advantage actually is built on energy consuming, such discussion is then unable to be ignored.\n- Overall, the performance gap compared to ANN is still very big, and noticing this in only in CIFAR10/100, for larger dataset or more complex tasks, I do believe the gap will be larger."
                },
                "questions": {
                    "value": "Please see above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762059717,
            "cdate": 1698762059717,
            "tmdate": 1699635983115,
            "mdate": 1699635983115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uhtHxxFU47",
                "forum": "bPG48f3ppz",
                "replyto": "Gq65g9FO9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you very much for your valuable comments!**\n\n**Q1\uff1aThe paper lacks novelty, SpikeCLIP architecture implementation claims \"the first......\" However, the bright spot could not be found ?**\n\n**R1:** Our work attempts to align the two modalities with the spiking computing paradigm, which is not done before. As mentioned in the paper, the current implementation methods of SNNs mainly include weight transfer and direct training using surrogate gradients. However, due to the sparsity of data and integer operation in SNN, direct training of SNNs is not easy. Our work has used alignment pre-training and dual-loss fine-tuning to effectively realize the alignment of the two modalities and achieve competitive performance. In previous research on SNNs, the vast majority of work in the field of computer vision has been done, so many questions about architecture design have been conducted around computer vision tasks. However, the lack of work on SNNs in the field of natural language processing makes it difficult to use SNNs for multimodal tasks. Considering that the advanced Spikingformer is based on Transformer, which is derived from natural language processing tasks, we have experimented with different text-side architectures in Transformer and MLP for multimodal classification tasks. \n\nWe sincerely apologize for emphasizing \"the first...\" in the paper. However, due to the lack of exploration in the field of SNNs in the multimodal context, our work represents only a preliminary exploration of the feasibility of this topic. We also hope that in the future, there will be more SNNs-related research that pays close attention to the implementation of multimodal tasks, similar to the attention given to computer vision tasks.\n\n\n**Q2: The comparison of energy efficiency between SpikeCLIP and its counterpart, ScratchCLIP, is unfair, and there are omissions in energy calculations?**\n\n**R2:** To ensure a fair comparison of the performance and energy consumption between SpikeCLIP and its counterpart ScratchCLIP, we configured both with identical settings (e.g., layer number, dimensions, training process) except for the spiking neurons. In this setting, energy consumption calculations were based on [1], where SNN, as a neural morphic computing algorithm, can smoothly execute on sparse neural morphic chips, requiring only spike-based accumulate (AC) operations, while ANNs involve numerous multiply-and-accumulate (MAC) operations. Therefore, the additional energy required for membrane potential update processes and maintaining the membrane potential of each neuron does not need to be considered. Specific chip implementation details can be found in [2] and [3]. Regarding BN/LN operations, as SpikeCLIP and ScratchCLIP architectures have identical BN/LN operations, their energy consumption cancels out when calculating the energy consumption ratio.\n\n While it is acknowledged that the data input for SNN is greater than that for the corresponding ANN, SNN can leverage this additional input to abandon more precise floating-point calculations, sacrificing precision for efficient energy consumption. SpikeCLIP, in its preliminary attempt at multimodal alignment tasks in SNNs, exhibits a performance gap with the single-modal Spikingformer within a reasonable range. At the same time, Spikeclip also shows zero-shot learning ability (which is not available in previous single-modal SNNs). Superior energy efficiency is only an inherent but indispensable advantage of SpikeCLIP."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699770977758,
                "cdate": 1699770977758,
                "tmdate": 1699770977758,
                "mdate": 1699770977758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JTcpJ8owKm",
                "forum": "bPG48f3ppz",
                "replyto": "Gq65g9FO9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear reviewer edTp:**\n\n**We greatly appreciate your time and effort in reviewing our work.** We have carefully considered your questions and made the necessary changes. Specifically, we went deeper into the special insights and implications of our \"alignment pre-training + dual-loss fine-tuning\" training framework in **3.3**, and we modified section **A.6** to make it clearer how energy consumption compares with ScratchCLIP. Finally, you can refer to our official review for a more comprehensive summary of our motivations and contributions to this study and SpikeCLIP's performance.\n\nYour expertise and insights are invaluable and we are keen to ensure that our research meets the highest standards. We look forward to hearing more from you and will be happy to answer further questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186560641,
                "cdate": 1700186560641,
                "tmdate": 1700186560641,
                "mdate": 1700186560641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1MhU2oPLnz",
                "forum": "bPG48f3ppz",
                "replyto": "Gq65g9FO9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission560/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Dear Reviewer edTp:**\n\nWe sincerely thank you for taking the time out of your busy schedule to conduct a thorough review of our paper and provide valuable comments.\n\nAs the Author-Review Discussion period is drawing to a close with only **two days** remaining, we would like to ensure that all your concerns have been adequately addressed. If there are any questions or unresolved issues, we are eager to provide further clarification or make necessary revisions.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544903951,
                "cdate": 1700544903951,
                "tmdate": 1700544903951,
                "mdate": 1700544903951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]