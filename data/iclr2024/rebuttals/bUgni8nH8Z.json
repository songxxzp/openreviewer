[
    {
        "title": "Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning"
    },
    {
        "review": {
            "id": "whOc7usDP2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
            ],
            "forum": "bUgni8nH8Z",
            "replyto": "bUgni8nH8Z",
            "content": {
                "summary": {
                    "value": "This paper proposed a novel approach to understanding and improving ReLU-based neural networks. The authors delve into the characteristic activation values of individual ReLU units within neural networks and establish a connection between these values and the learned features. They propose a geometric parameterization for ReLU networks based on hyperspherical coordinates, which separates radial and angular parameters. This new parameterization is demonstrated to enhance optimization stability, convergence speed, and generalization performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is written in a clear and easily comprehensible manner, making it easy for readers to follow.\n- The paper presents a unique and innovative approach to understanding ReLU networks by exploring the characteristic activation values. This fresh perspective sheds light on the inner workings of these networks, offering insights that were previously unexplored."
                },
                "weaknesses": {
                    "value": "see questions."
                },
                "questions": {
                    "value": "- Can the analysis apply to the existing advanced batch normalization improvements like IEBN [1], SwitchNorm [2], layer norm [3]. These missing works should be considered and added to the related works or analysis.\n\n- I am not very familiar with the topics covered in this article, I will consider these clarifications along with feedback from other reviewers in deciding whether to raise my score.\n\n[1] Instance Enhancement Batch Normalization: An Adaptive Regulator of Batch Noise, AAAI\n\n[2] Differentiable Learning-to-Normalize via Switchable Normalization, ICLR\n\n[3] Layer normalization, IJCAI"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4349/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4349/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697341354803,
            "cdate": 1697341354803,
            "tmdate": 1699636405943,
            "mdate": 1699636405943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "au1woXhye7",
                "forum": "bUgni8nH8Z",
                "replyto": "whOc7usDP2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n38z"
                    },
                    "comment": {
                        "value": "Thanks for your comments on our paper! We will respond to your comments point by point below:\n\n> Can the analysis apply to the existing advanced batch normalization improvements like IEBN [1], SwitchNorm [2], layer norm [3]? These missing works should be considered and added to the related works or analysis.\n\nThanks for pointing out these related works. Please note that LayerNorm is already discussed in the related work section (Section 2) under \u201cOther normalization methods\u201d. We have added [1,2] to the related work section in the updated manuscript. \n\nRegarding the applicability of our analysis, the proposed characteristic activation value analysis is designed to analyze neural network parameterizations. We showed in Section 2 that BatchNorm can be viewed as a kind of weight normalization, so our analysis will produce the same result for BatchNorm as that for weight normalization. Furthermore, since BatchNorm, IEBN, SwitchNorm, LayerNorm, and almost all existing normalization techniques are different variants of the same normalization technique which operate in the weight-space parameterization, they will have the same instability property as standard parameterization and weight normalization, as shown in Equations 12 and 14. We have clarified this in the updated paper manuscript.\n\nWe hope that this has sufficiently addressed all your concerns. Please let us know if you have any further questions or comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159102237,
                "cdate": 1700159102237,
                "tmdate": 1700160702743,
                "mdate": 1700160702743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ad804eqE8v",
                "forum": "bUgni8nH8Z",
                "replyto": "au1woXhye7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4349/Reviewer_n38z"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your detailed response; my concern has been addressed. I am inclined to keep my positive scores. Thank you for your efforts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729857695,
                "cdate": 1700729857695,
                "tmdate": 1700729857695,
                "mdate": 1700729857695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JkRlz1CQcR",
            "forum": "bUgni8nH8Z",
            "replyto": "bUgni8nH8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4349/Reviewer_cECr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4349/Reviewer_cECr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a geometric parameterization method for ReLU networks to improve their performance. Some experimental results show the performance of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a geometric parameterization method for ReLU networks to improve their performance. \n2. Some experimental results show the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "Although the paper is theoretically and experimental sound, there are still some questions need to be discussed in this paper:\n1.\tThe main contributions of this paper are to propose one geometric parameterization for ReLU networks and input mean normalization. But the input mean normalization proposed in this paper is very similar to the mean-only batch normalization (Salimans & Kingma, 2016). What\u2019s the advantage of the former against the latter?\n2.\tThe experimental results are not convincing. The authors should compare the performance of the proposed algorithm on more models and datasets.\n3.\tBoth the English language and equations in this paper need to be improved."
                },
                "questions": {
                    "value": "Although the paper is theoretically and experimental sound, there are still some questions need to be discussed in this paper:\n1.\tThe main contributions of this paper are to propose one geometric parameterization for ReLU networks and input mean normalization. But the input mean normalization proposed in this paper is very similar to the mean-only batch normalization (Salimans & Kingma, 2016). What\u2019s the advantage of the former against the latter?\n2.\tThe experimental results are not convincing. The authors should compare the performance of the proposed algorithm on more models and datasets.\n3.\tBoth the English language and equations in this paper need to be improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4349/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4349/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4349/Reviewer_cECr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699108531742,
            "cdate": 1699108531742,
            "tmdate": 1699636405871,
            "mdate": 1699636405871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2nBvDKgify",
                "forum": "bUgni8nH8Z",
                "replyto": "JkRlz1CQcR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cECr"
                    },
                    "comment": {
                        "value": "Thanks for your comments on our paper! We will respond to your comments point by point below:\n\n> The main contributions of this paper are to propose one geometric parameterization for ReLU networks and input mean normalization. But the input mean normalization proposed in this paper is very similar to the mean-only batch normalization (Salimans & Kingma, 2016). What\u2019s the advantage of the former against the latter?\n\nRegarding our contributions, please note that geometric parameterization and input mean normalization are only part of the contributions in our paper, which are natural products from our main contribution - the characteristic activation value analysis for neural network parameterizations. We used the proposed novel analysis to identify the instability of common neural network parameterizations such as standard parameterization and weight normalization. Motivated by this, we proposed geometric parameterization which has theoretical stability guarantees as shown by our analysis in Theorem 3.3. We verified the theoretical result with three illustrative experiments in Section 3.5.\n\nRegarding input mean normaliztion vs mean batch normalization, as discussed at the beginning of Section 4.2, the input mean normalization is a technique to address the covariate shift problem in multi-layer networks with geometric parameterization. We subtract the mean from the input to each layer because our proposed analysis assumes that input distribution is centered around the origin. The mean-only batch normalization (Salimans & Kingma, 2016) would not address this problem because it is applied to the pre-activation values (i.e., in this case the zero mean distribution will be destroyed after going through the ReLU activation, so the input distribution to the next layer will not be zero mean anymore). We have clarified this in the updated manuscript.\n\n> The experimental results are not convincing. The authors should compare the performance of the proposed algorithm on more models and datasets.\n\nWe have conducted extensive experiments with \n- three representative models: MLP, VGG, and ResNet;\n- several representative regression and classification benchmarks: ImageNet (ILSVRC 2012) classification dataset, ImageNet-32 classification dataset, 6 UCI regression datasets, 2D Banana classification dataset, and 1D Levy regression dataset. \n\nWe believe that these experiments have verified the usefulness of the proposed algorithm with different model architectures and on benchmarks from different domains.\n\n> Both the English language and equations in this paper need to be improved.\n\nWe\u2019d be grateful if the reviewer could point out the texts or equations that are confusing, and we will try our best to clarify them and improve our presentation. That said, we noticed that Reviewer n38z actually appreciated our clear writing in an easily comprehensible manner.\n\nWe hope that this has sufficiently addressed all your concerns. Please let us know if you have any further questions or comments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158803299,
                "cdate": 1700158803299,
                "tmdate": 1700158803299,
                "mdate": 1700158803299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6d4WuAwr9K",
                "forum": "bUgni8nH8Z",
                "replyto": "JkRlz1CQcR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for your effort during the reviewing process! We believe that we have addressed your stated concerns in our response and would like to ask if the reviewer thinks this is the case as well. If our response is affirmative, we would like to ask the reviewer if they are happy to increase their scores."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741464953,
                "cdate": 1700741464953,
                "tmdate": 1700741464953,
                "mdate": 1700741464953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1LGl5NZygc",
            "forum": "bUgni8nH8Z",
            "replyto": "bUgni8nH8Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4349/Reviewer_BSGv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4349/Reviewer_BSGv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes geometric parameterization (GmP) for ReLU networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduced the characteristic activation sets of individual neurons and a geometric connection between such sets and learned features in ReLU networks. \n2. This paper then introduced geometric parameterization (GmP) based on radial-angular decomposition in the hyperspherical coordinate system. It also proves that the change in the angular direction under perturbation $\\varepsilon$ is bounded by the magnitude of perturbation $\\varepsilon$. This property is not held for standard parameterization and weight normalization.\n3. The authors provide some experimental results to show the advantage of the proposed GmP for ReLU networks."
                },
                "weaknesses": {
                    "value": "1. The Gmp for the ReLU network with IMN in Eq.16 also seems applicable to the weight normalization (WN), i.e. change $u(\\theta)$ to $\\frac{w}{\\||w\\||_2}$. I am wondering what the result will look like. The authors should talk more about why they prefer optimization in the angular space instead of the weight normalization space since they are equivalent as shown in Eq. 9 ( $u(\\theta):=\\frac{w}{\\||w\\||_2}$).\n2. The theoretical proof only shows that the change in the angular direction under perturbation $\\varepsilon$ is bounded by the magnitude of perturbation $\\varepsilon$. Using $\\Delta \\phi$ as evidence for generalization is not theoretically justified. I think loss function values should be added as supporting evidence too.\n3. I found some of the experimental settings unsatisfactory. For example, the training settings of ResNet-50 are very different compared to standard settings. I expect to see a comparison of different methods with standard learning rate decay, i.e. first 30 epochs: 0.1, 30-60 epochs: 0.01, 60-90 epochs: 0.001. Under this setting, BN should achieve around 76.1% top-1 accuracy. Or, the authors should provide a convincing explanation why not use the standard training setting."
                },
                "questions": {
                    "value": "1. In Eq.16, only input mean normalization (IMN) is used, why not further normalize the input features with its variance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699148457162,
            "cdate": 1699148457162,
            "tmdate": 1699636405791,
            "mdate": 1699636405791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nFpzAnvdl2",
                "forum": "bUgni8nH8Z",
                "replyto": "1LGl5NZygc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BSGv"
                    },
                    "comment": {
                        "value": "Thanks for your comments on our paper! We will respond to your comments point by point below:\n\n> The authors should talk more about why they prefer optimization in the angular space instead of the weight normalization space since they are equivalent as shown in Eq. 9\n\nSection 3.4 talks about why optimization is preferred in the angular-space rather than the weight-space, which is the main theoretical contribution of this paper. In summary, Theorem 3.1 shows that the change of angular direction under small perturbation is bounded by the magnitude of perturbation, which is robust against noise during stochastic gradient optimization. In contrast, although Eq. 9 shows an equivalency between weight normalization and geometric parameterization, Eq. 14 shows that they have entirely different stability properties, i.e., weight normalization is as unstable as standard parameterization under small noise perturbation during stochastic gradient optimization. Furthermore, this further implies that any weight-space parameterization will suffer from this issue because they will all lead to Eq. 14. We have clarified this in the updated manuscript.\n\n> The theoretical proof only shows that the change in the angular direction under perturbation is bounded by the magnitude of perturbation. Using $\\Delta\\phi$ as evidence for generalization is not theoretically justified. I think loss function values should be added as supporting evidence too.\n\nWe did not use $\\Delta\\phi$ as evidence for generalization. Instead, we used it in our illustrative experiments in Section 3.5 to verify that our theoretical results about the stability of different parameterizations from Section 3.4 hold in practice. The evidence we used for generalization is the final test performance (RMSE and accuracy), i.e., Figure 2(d)-(g), Figure 3(j)-(m), Figure 4, and Tables 1, 2 and 3. The loss function values are consistent with these test performance, which we didn\u2019t report in the paper due to page limits.\n\n> I found some of the experimental settings unsatisfactory. For example, the training settings of ResNet-50 are very different compared to standard settings. I expect to see a comparison of different methods with standard learning rate decay, i.e. first 30 epochs: 0.1, 30-60 epochs: 0.01, 60-90 epochs: 0.001. Under this setting, BN should achieve around 76.1% top-1 accuracy. Or, the authors should provide a convincing explanation why not use the standard training setting.\n\nThis is because different parameterizations have different convergence speeds as shown in our ablation study (Figure 4). The fixed learning rate decay scheme mentioned by the reviewer may be useful for BatchNorm, but it can be suboptimal and thus slow down the convergence of geometric parameterization (because it converges faster and will reach the plateau faster at each stage than BatchNorm). Therefore, we used an adaptive learning rate decay scheme, Reduce LR On Plateau, for geometric parameterization. Then, we had to use Reduce LR On Plateau for BatchNorm and weight normalization as well for a fair comparison.\n\n> In Eq.16, only input mean normalization (IMN) is used, why not further normalize the input features with its variance?\n\nIMN is used to address the covariate shift problem in multi-layer networks with geometric parameterization, which ensure that the input distributions to the intermediate layers of multi-layer networks are centered around the origin, because this is an assumption of our proposed analysis. There is no need to further normalize the input features with its variance because our analysis does not assume that the input distribution has unit variance. That said, it would be harmless to normalize the variance in terms of the final test performance, but calculating the variance for the input to each layer in each forward pass would increase the computational cost and slow down training, and therefore we chose not to do that. We have clarified this in the updated manuscript.\n\nWe hope that this has sufficiently addressed all your concerns. Please let us know if you have any further questions or comments."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158607720,
                "cdate": 1700158607720,
                "tmdate": 1700158826225,
                "mdate": 1700158826225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DqXbdjr7wn",
                "forum": "bUgni8nH8Z",
                "replyto": "1LGl5NZygc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4349/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for your effort during the reviewing process! We believe that we have addressed your stated concerns in our response and would like to ask if the reviewer thinks this is the case as well. If our response is affirmative, we would like to ask the reviewer if they are happy to champion our paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741405609,
                "cdate": 1700741405609,
                "tmdate": 1700741405609,
                "mdate": 1700741405609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]