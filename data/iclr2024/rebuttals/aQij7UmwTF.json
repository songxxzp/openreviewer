[
    {
        "title": "Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching"
    },
    {
        "review": {
            "id": "SFxOpLbltA",
            "forum": "aQij7UmwTF",
            "replyto": "aQij7UmwTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_v5dd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_v5dd"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization bounds for neural networks with magnitude-based pruning. The approach is as follows. First, the paper proposes a magnitude-based pruning algorithm, which randomly sets the non-diagonal weights of neural networks to zero according to a Bernoulli random variable. This is followed by a discretization approach and a matrix sketching approach to improve the dependency of the bounds on the number of trainable parameters. The main result is a generalization bound based on a compression approach. Experimental analysis is also included to verify the assumptions used in the analysis, and to verify the theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposed generalization bounds based on magnitude-based pruning, which does not require structured compression as considered in the existing studies.\n\nThe paper considers sparse matrix sketching to decrease the number of trainable parameters, which improves the generalization bounds."
                },
                "weaknesses": {
                    "value": "Theorem 5.2 requires an assumption $\\gamma$ to be no smaller than a number. It seems that this number would be very large since it has exponential dependency on $L$, and $\\Gamma_l$ is also large according to the definition. If $\\gamma$ is large, then $\\hat{R}_\\gamma$ would be very large.\n\nTo make the high-probability bound in Theorem 5.2 meaningful, one needs to choose very large $\\epsilon_l,\\lambda_l$ and $p_l$. In this case, the generalization bound would also be large. Furthermore, Lemma B.5 requires $\\epsilon_l$ to be sufficiently small. This seems to lead to a contradiction.\n\nIn Theorem 5.2, the dependency of the generalization bound on $d$ seems to be $d^{3/4}$. If I understand correctly, if we do not use the matrix sketching, the dependency would be $d$. Therefore, the improvement is $d^{1/4}$, which seems not to be quite significant. As stated in the conclusion, the analysis still leads to a vacuous generalization bound.\n\nIn Lemma 5.1, the statement holds with probability $1-1/\\lambda_1-d^{-1/3}$. This dependency on $d$ is not appealing since it would require a very large $d$ to get a bound holding with large probability. For example, if we want the bound to hold with probability $1-0.01$, then $d$ should be as large as $10^6$."
                },
                "questions": {
                    "value": "Please see the comments above.\n\nMinor comments:\n\nSection 3.1: it seems that the definition of $\\hat{R}_\\gamma$ is not correct, i.e., $\\geq \\gamma$ should be $\\leq\\gamma$?\n\nTheorem 3.1 only shows the existence of a $A\\in\\mathcal{A}$. How to find such a $A$?\n\nLemma 4.1: $\\Gamma_l$ is used without giving its meaning"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698114620272,
            "cdate": 1698114620272,
            "tmdate": 1699636040753,
            "mdate": 1699636040753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sOStIexyLT",
                "forum": "aQij7UmwTF",
                "replyto": "SFxOpLbltA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "We thank this reviewer for their thoughtful response! We have addressed their concerns here. If these satisfy your concerns, we would greatly appreciate an increase in score!\n\n# Weaknesses\n1. *Theorem 5.2 requires an* \u2026\n\nA. When $\\gamma$ is large, $\\hat{R}_{\\gamma}$ is smaller as the acceptable margin is smaller. This is identical to the case of Arora et al. This is not an unusual statement unique to this paper. \n\n2. *To make the high-probability bound in* \u2026\n\nA. We don\u2019t choose $p_l$, that is the maximum dimension of the matrix of the $l$th layer which can be large. The other two constants are pretty common constants in many generalization bounds. You see them in Arora et al. 2023 for example and more. This is not a weakness of this paper, rather a tradeoff of using concentration inequalities in general. Moreover, it is not the loosensess of these constants that makes the generalization bound big empirically. Moreover, it is not a contradiction. You just have to set $\\epsilon$ accordingly. We found epsilon values which satisfy both empirically. \n\n3. *In Theorem 5.2, the dependency of the* \u2026\n\nA. This is not correct. Here, $d$ is a coefficient of the strength of the pruning. The dependence on $d$ is roughly $\\frac{1}{d^{.25}}$ so as $d$ increases and prunes more, the generalization bound gets tighter and tighter. In every other generalization bound, there is no mention of this $d$ and they do not get tighter as you prune more. $d$ is not the dimension of the matrix. For heavily pruned matrices, our bounds are significantly tighter. \n\n4. *In Lemma 5.1, the statement holds with* \u2026\n\nA. This is incorrect. We understand the confusion and will fix this in the camera-ready version. $d$ is not $d_1^l, d_2^l$. Rather, $d$ is the pruning coefficient, while the latter is the dimension. In Lemma 5.1, these hold with probability $1 - 1/\\lambda_1 - (d_1^l)^{-\u2153} - (d_2^l)^{-\u2153}$, which includes dependence on the matrix size. Therefore, this probability bound is strong in practice.\n\n# Minor comments:\n\n1. *Section 3.1* \u2026\n\nA. Yes, you are correct. This has been fixed in the updated version.\n\n2. *Theorem 3.1* \u2026\n\nA. Lemma 4.1 proves that using Algorithm 1 to prune a model finds such an $A$ that achieves a difference in output with small margin.\n\n\n3. *Lemma 4.1* \u2026\n\nA. We apologize for this. This was put into the appendix for space, but we will move it back for the Camera Ready Version. \n\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. CoRR, abs/1802.05296, 2018. URL http://arxiv.org/ abs/1802.05296."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700094805745,
                "cdate": 1700094805745,
                "tmdate": 1700121316745,
                "mdate": 1700121316745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xcqbtKSrZx",
            "forum": "aQij7UmwTF",
            "replyto": "aQij7UmwTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present a generalization bound derived from magnitude-based pruning (MBP) sparsity. \nThe central idea is that if (a) the pruned model exhibits good generalization, and (b) the original model's performance closely matches the pruned model, \nthen the original model also generalizes well. \nThe validity of point (a) has been established by previous work (Arora et al.), so the primary focus of this paper is to substantiate point (b).\nBy assuming that magnitudes follow a Gaussian distribution, the authors demonstrate that pruned and discretized parameters closely approximate the original parameters. Furthermore, the authors introduce the concept of matrix sketching to reduce parameter size, leading to a more favorable bound. Empirical experiments support the authors' claims, showing that their bounds exceed test loss and outperform prior bounds.\n\nDespite the paper's overall significance and the intuitiveness of the generalization bound based on MBP sparsity, there are still notable shortcomings. The most substantial concern lies in the limited ability to validate the proposed bound's practical effectiveness. The assumptions made, particularly regarding the Gaussian distribution of weights, lack robust justification in the experimental context. Additionally, the graphical verification of the bound may not be entirely convincing, as the predicted error bound substantially exceeds the actual error. This raises questions about the bound's validity and practical utility. Rather than focusing solely on the bound, it would be more insightful to verify the underlying assumptions.\n\nTherefore, unfortunately, I cannot recommend an acceptance at the time being."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors introduce the intuitive concept of utilizing magnitude-based pruning sparsity to address generalization concerns.\n2. Unlike prior approaches, this paper directly analyzes the generalization performance of the original model, rather than focusing solely on the pruned model.\n3. The authors illustrate the adaptability of their technique to various contexts."
                },
                "weaknesses": {
                    "value": "Overall, the proposed bound's practical effectiveness is challenging to confirm, representing a primary concern.\n\n1. The assumptions, especially those concerning Gaussian weight distribution, lack adequate empirical validation.\n2. The graphical verification of the bound might be misleading due to a significant predicted error bound compared to the actual error. A more rigorous approach is necessary to establish the validity of the assumptions.\n3. Considering the assumed Gaussian distribution of weights, it is worth exploring whether better discretization methods exist.\n4. The empirical verification of bounds in Fig~3 displays bound values that are excessively large, impairing their practical utility. It would be advisable to offer theoretical comparisons rather than relying solely on empirical evidence.\n\nMinor Concerns:\nThis paper could significantly benefit from improved writing clarity. The current version may be confusing for readers. Some specific points to consider include:\nIn the Abstract:\n1. Emphasize the necessity of Magnitude-Based Pruning and its advantages over structured pruning for clarity.\n2. The use of \"However\" in the abstract seems to have unclear logic.\nIn the Introduction:\n1. In the first paragraph, provide evidence supporting the claim that MBP significantly reduces memory requirements and inference time.\n2. Consider rephrasing the use of \"However\" in the second paragraph for improved clarity."
                },
                "questions": {
                    "value": "See limitations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1144/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1144/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698132864273,
            "cdate": 1698132864273,
            "tmdate": 1700695791232,
            "mdate": 1700695791232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SB5HnllzP8",
                "forum": "aQij7UmwTF",
                "replyto": "xcqbtKSrZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response"
                    },
                    "comment": {
                        "value": "We thank this reviewer for their thoughtful response! We have addressed their concerns here. If these satisfy your concerns, we would greatly appreciate an increase in score!\n\n# Weaknesses\n1. *Overall, the proposed bound's* \u2026\n\nA. There are two lemmas that use the Guassian assumption are Lemma 4.1 where pruning doesnt induce large error and that the sparsity is distributed. We verify empirically on MNIST and CIFAR10 that these are indeed the case. We verify the results of Lemma 4.1 in Figure 1 for example. Despite these assumptions being taken, we have verified that our lemmas do hold in practice. This is purely a weaker assumption than that of the Qian and Klabjan. \n\n2. *The graphical verification of the bound* \u2026\n\nA. We test both of the lemmas that come from our assumptions in Figures 1 and 2. We find that empirically, these bounds do indeed hold. Moreover, this Gaussian assumption is a purely weaker assumption than that done in Qian and Klabjan, which assume that the weights belong to a roughly uniform distribution. They take a stronger assumption to show a similar result that pruning does not induce large amounts of error. \n\n3. *Considering the assumed Gaussian distribution* \u2026\n\nA. We believe this reviewer has misinterpreted the point of discretization. We only discretize so that there are a limited number of parameterizations so that we can use the compression framework. Different discretization techniques are unlikely to improve the bounds significantly. \n\n4. *The empirical verification of bounds*\u2026\n\nA. As stated in this paper, all of the baselines do not capture the sparsity and do not incorporate sparsity into their generalization bounds. This means that our bounds are significantly tighter when the models are heavily pruned. \n\n## Minor Concerns: \n\nA. We thank this reviewer for their comments on clarity. We have taken the time to improve these elements of our paper. \n\nXin Qian and Diego Klabjan. A probabilistic approach to neural network pruning. CoRR, abs/2105.10065, 2021. URL https://arxiv.org/abs/2105.10065."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095689607,
                "cdate": 1700095689607,
                "tmdate": 1700121349514,
                "mdate": 1700121349514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jleMuTZKdF",
                "forum": "aQij7UmwTF",
                "replyto": "SB5HnllzP8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "I thank the authors for the timely responses. \n\n> There are two lemmas that use the Guassian assumption are Lemma 4.1 where pruning doesnt induce large error and that the sparsity is distributed. We verify empirically on MNIST and CIFAR10 that these are indeed the case. We verify the results of Lemma 4.1 in Figure 1 for example. Despite these assumptions being taken, we have verified that our lemmas do hold in practice.\n\nI believe that more experiments should be conducted to justify the Gaussian assumption.\nFor example, I suggest the authors *plotting the (empirical) weight distribution* and do some test to show that this distribution is, at least, close to Gaussian. \n\nAs for the Fig 1, as I said before, the bound seems too large. So although this bound might be  larger than true generalization in practice, it cannot convince me that the Gaussian assumption is valid. For example, if I just propose a bound with constant 1000, this bound is also true but meaningless. \n\nHope that this suggestion is helpful!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183547910,
                "cdate": 1700183547910,
                "tmdate": 1700183547910,
                "mdate": 1700183547910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "arIjwEalf8",
                "forum": "aQij7UmwTF",
                "replyto": "xcqbtKSrZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response about Gaussian Assumption"
                    },
                    "comment": {
                        "value": "Thank you for the responses! We now better understand your questions. We did not show this graphic since this is a well-known empirical phenomenon. For example, both Figure 7 by Han et al. 2015 and Figure 1 by Qian and Klabjan 2021 run experiments on the weight distributions of Feed-Forward Networks. After training, they *do* see that the distributions are indeed Gaussian approximately. They plot this graphical verification. We have reproduced their experiments and *added them to the newly uploaded version in Figure 1*. Please look at our figures. I believe these are what you are looking for. I appreciate all the help and quick response! Again, if this alleviates your concerns, we would greatly appreciate an increase in scores!\n\nXin Qian and Diego Klabjan. A probabilistic approach to neural network pruning. CoRR,\nabs/2105.10065, 2021. URL https://arxiv.org/abs/2105.10065.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\nefficient neural network. Advances in neural information processing systems, 28, 2015."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195468847,
                "cdate": 1700195468847,
                "tmdate": 1700196843424,
                "mdate": 1700196843424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gaYTTATfGQ",
                "forum": "aQij7UmwTF",
                "replyto": "arIjwEalf8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                ],
                "content": {
                    "title": {
                        "value": "Some more figures might be necessary"
                    },
                    "comment": {
                        "value": "I thank the authors for updating Figure 1. However, Figure 1 only contains the first layer. \nIt cannot convince me that all the layers have the same performances. \nCould the authors add some more figures (on other layers) about the Gaussian assumption in Appendix? I believe this could greatly enhance the current manuscript."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536026220,
                "cdate": 1700536026220,
                "tmdate": 1700536026220,
                "mdate": 1700536026220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vwz4aAJCOJ",
                "forum": "aQij7UmwTF",
                "replyto": "xcqbtKSrZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank this reviewer for interacting with us. This is a reasonable request, and we have added 6 new plots in Appendix I in the newly uploaded version. As you can tell, the initial and intermediate layer weights look very Gaussian for all datasets. The last layer looks less Gaussian but still resembles a Gaussian, especially for CIFAR-10. Thus, we feel that a Gaussian distribution is still a reasonable assumption. We note that the mean over the distribution of weights of the last layer is roughly $-0.028$ for MNIST and $-0.021$ for CIFAR-10, so the assumption of zero-mean is still also reasonable. We appreciate the feedback and look forward to your response!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550573891,
                "cdate": 1700550573891,
                "tmdate": 1700550581286,
                "mdate": 1700550581286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cMMOGDeGfx",
                "forum": "aQij7UmwTF",
                "replyto": "Vwz4aAJCOJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_TQ53"
                ],
                "content": {
                    "comment": {
                        "value": "I read the author's response and the other reviews. Based on the experiments the authors provide, I have become more natural to this paper. \n\nHowever, I still cannot find strong evidence that could let me recommend this paper with strong confidence, due to the current techniques and presentations. \nThis includes: (1) the techniques in this paper are not such novel, (2) the improvement for the bound seems not significant enough.\nBesides, I suggest that the authors could do some statistical tests to see whether the weight indeed follows Gaussian, I cannot tell that it is Gaussian only based on my eyes. \nSo overall, I still lean negative but have become more natural. I changed my score from 3 to 5. Thanks again for the authors' response."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695776368,
                "cdate": 1700695776368,
                "tmdate": 1700695776368,
                "mdate": 1700695776368,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eNLn4JwYp1",
            "forum": "aQij7UmwTF",
            "replyto": "aQij7UmwTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_ftR9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_ftR9"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to provide generalization bounds for pruned models using tools from sparse matrix sketching. The authors focus on developing a generalization bound for Magnitude based pruning methods but also show their proof methodology for other sparse subnetworks.\n\nThe proposed method first bounds the difference in layerwise activation norms between the dense and sparse model after pruning and discretization. The generalization bounds are then given by translating the sparse network into a small dense one using sparse matrix sketching and then applying bounds from Arora et al [1], which results in much tighter bounds."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose a novel idea of using sparse matrix sketching to develop generalization bounds for sparse networks.\n\n2. Their establishes a connection between sparse networks obtained via magnitude pruning and generalization."
                },
                "weaknesses": {
                    "value": "1. The proof follows a structure of bounding the layerwise error and then accounting for discretization of the parameters in order to conform with the setting of Theorem 3.1 based on the proof of Arora et al. [1]. However, the need for discretization has not been motivated clearly in the paper and can benefit from additional explanation regarding the same, which will make the proof easier to follow.\n\n2. The magnitude pruning algorithm assumed by the authors uses a Bernoulli based construction of the sparse mask, however, in practice magnitude pruning is done based on sorting the parameters in each layer (or the entire network). Does the $(j_r, j_c)$ structure in each layer hold then? For pruning methods like Iterative Magnitude Pruning, larger layers are known to have dead neurons (corresponding to all zeros in a row). This problem is especially amplified in a CNN where IMP sets most channels to zero while having very few dense channels. The current proof seems to be unable to handle these situations and how will the assumption change for CNNs.\n\n3. How will the generalization bounds change for different pruning methods, for eg: random pruning or other gradient based pruning criterions. Would random be strictly worse or similar? Insights on different pruning methods will help understand the relevance of the proposed bounds for different pruning criteria.\n\nThe key idea of the paper is novel and for the first time establishes generalization bounds for magnitude based pruning methods.\n\n[1] Arora, Sanjeev, et al. \"Stronger generalization bounds for deep nets via a compression approach.\" International Conference on Machine Learning."
                },
                "questions": {
                    "value": "See above section for questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1144/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1144/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1144/Reviewer_ftR9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746779069,
            "cdate": 1698746779069,
            "tmdate": 1699636040605,
            "mdate": 1699636040605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lJbLYt0T5J",
                "forum": "aQij7UmwTF",
                "replyto": "eNLn4JwYp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response"
                    },
                    "comment": {
                        "value": "We thank this reviewer for their thoughtful response! We have addressed their concerns here. If these satisfy your concerns, we would greatly appreciate an increase in score!\n\n# Weaknesses\n1. *The proof follows a structure* \u2026\n\nWe apologize that this has not been made clear. We discretize so that the number of different parameterizations is finite. We have made this clearer in the uploaded new version. \n\n2. *The magnitude pruning* \u2026\n\nThis is not true. There are different forms of Magnitude Based pruning. There are many forms of Magnitude Based pruning. For example, both random and threshold based Magnitude Based Pruning are discussed in Xian and Klabjan. This is not an abnormal form of pruning. However, our analysis can be slightly altered to prove for threshold based pruning as well. We only need that the pruning does not induce large error, which is known, and that there does not exist rows with many nonzeros, which we can also demonstrate. I am unsure what the second part of this weakness means. We only require that there are no rows with many nonzeros, i.e. distributed. Our analysis will hold if there are rows with no nonzeros. \n\n3. *How will the* \u2026\n\nAs long as one can prove that the pruning does not induce large error and that the sparsity induced is distributed, our analysis will hold. In fact, our method is not largely dependent on the type of pruning used, only that it is evenly distributed and does not induce large errors. This makes our results much more generally applicable. \n\n\nXin Qian and Diego Klabjan. A probabilistic approach to neural network pruning. CoRR, abs/2105.10065, 2021. URL https://arxiv.org/abs/2105.10065."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096173650,
                "cdate": 1700096173650,
                "tmdate": 1700121392256,
                "mdate": 1700121392256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J3j7G9cTUF",
                "forum": "aQij7UmwTF",
                "replyto": "lJbLYt0T5J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_ftR9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_ftR9"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their clarifications on the pruning criterion, discretization and the sparsity pattern.\n\nFrom the discussion with Reviewer zSMw, If I understand correctly, the simple parameter counting argument achieves similar bounds to sketching and one could completely get rid of the sketching mechanism.\n\nFurthermore, the authors suggested that as long as the pruning ensures low induced error and no rows with many nonzeros, their analysis holds for any pruning criteria, not just magnitude pruning. The generalization argument for sparse networks thus follows discretization and parameter counting followed by using the results by Arora et al [1].\n\nIt seems that the main contribution of the paper is then to combine the the compression result of Arora et al [1] with parameter counting for sparse networks, but this parameter counting too has been done partially by the Lottery Ticket kind of proofs by Malach et al [2], Pensia et al [3] and Burkholz [4]. \n\nHowever, I still believe that providing a generalization bound for sparse networks is an interesting contribution, but the authors must appropriately modify their writing to highlight their exact contribution.\n\n[1] Arora, Sanjeev, et al. \"Stronger generalization bounds for deep nets via a compression approach.\" International Conference on Machine Learning. 2018.\n\n[2] Malach, Eran, et al. \"Proving the lottery ticket hypothesis: Pruning is all you need.\" International Conference on Machine Learning. PMLR, 2020.\n\n[3] Pensia, Ankit, et al. \"Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient.\" Advances in neural information processing systems. 2020.\n\n[4] Burkholz, Rebekka. \"Most activation functions can win the lottery without excessive depth.\" Advances in Neural Information Processing Systems. 2022."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562935313,
                "cdate": 1700562935313,
                "tmdate": 1700562935313,
                "mdate": 1700562935313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3XlbEndRRo",
            "forum": "aQij7UmwTF",
            "replyto": "aQij7UmwTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_zSMw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1144/Reviewer_zSMw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves new generalization bounds for deep neural networks via a compression based approach. They assume that the weights of the neural network are normally distributed and from this argue that a simple weight pruning approach well approximates the original network. They they argue that after pruning, the network can be represented with few parameters. This allows them to apply a generalization bound of Arora et al. 2018 which depends on the compression error and parameters of the compressed network.\n\nThe paper uses two key tools: (1) for their approximation step, they apply random matrix theory results to bound the error introduced by pruning (which due to their assumption of random weights is essentially an iid random perturbation of the weight matrix). (2) To bound the parameters of the pruned network, they argue that its weight matrices are sparse and thus can be compressed with a \u2018sparse matrix sketching approach\u2019 which basically allows recovering a sparse matrix X from a sketch AXB^T where A and B have many fewer rows than X.\n\nUnfortunately, I found the results in the paper difficult to grasp \u2014 little intuition is given for the bounds and most of the results have undefined quantities which make them impossible to interpret. See my questions below. Further, it is not clear what role the two key technical tools actually have in establishing these bounds. Again, see my detailed questions below but: (1) it is not clear what the improvement is from applying random matrix theory bounds instead of simple Frobenius norm bounds to bound the spectral norm of th erandom perturbation (2) sparse matrix sketching does not represent a sparse matrix X with any fewer parameters than a naive sparse matrix format would. It is is clear that without additional assumptions, doing so would be impossible. Thus, it is unclear what role this tool is playing and why it is being used to bound the parameters of the compressed model."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See full review."
                },
                "weaknesses": {
                    "value": "See full review."
                },
                "questions": {
                    "value": "I have included comments/questions below. I starred points that I think are more important and would be helpful to have addressed during the author response period.\n\nQuestions/Comments:\n- The abstract is confusing as it seems to focus on ML in general. Never mentions the context \u2014 e.g. neural networks.\n- Overall, paper has lots of typos and grammatical errors. It would need significant proofing before final publication. The intro in particular is very vague and difficult to read. I could not understand from it what the paper was actually doing. E.g. what do you mean by sparse matrix sketching? From what I can tell, this is not a standard term and no citations are given in the intro when the term is introduced. Are you talking about random projection with sparse random matrices? Something else? How are you using this sparse matrix sketching? Is it an alternative to magnitude based pruning? A way of analyzing magnitude based pruning? Something else? Some of these questions become clearer later, but IMO the intro needs to be significantly reworked to be more concrete.\n- In 3.1, what is the different between R(M) and R_0(M). Both notations are used and both seem to mean the case when gamma = 0? I.e. no margin? \n-**  I didn\u2019t understand the role of the \u2018fixed string\u2019 s in Def 3.1. It is not used in either the definition of G_{A,s} nor in the definition of compressibility. It seems that dropping it would not change the definition nor Theorem 3.1 at all. So what is its role? Relatedly, G_A,s is defined differently in Def 3.1 and Thm 3.1, which I presume is a typo.\n- In Assumption 3.1, are the weights *independent* of each other? Or just marginally Gaussian but potentially correlated? (Later I see that independence is needed but this was not clear in the assumption)\n- In Remark 4.1, what is meant by \u2018diagonal elements\u2019 given that the weight matrices are not square.\n-** Given the explanation of the proof in 4.1, I\u2019m not clear on why an assumption of Gaussianity was needed. And I\u2019m not clear why randomized pruning was needed. As long as the original distribution was mean 0 and symmetric, if I just pruned all values below some threshold t, then Delta^l would have i.i.d. mean 0 entries with bounded moments right? And then random matrix theory results could be applied.\n- In Lemma 4.1, what is Gamma_l? I couldn\u2019t find where this was defined.\n- ** I also couldn\u2019t make intuitive sense of Lemma 4.1. I also don\u2019t see how the bound could possibly not depend on some norm bound for the original input points. Say I just have 1 layer and the non-linearity is actually just the linear function, then if I multiply my input by some arbitrarily large constant C, then x^L and hat x^L will also be multiplied by C and thus the error would be scaled by C. So in this case, the error bound must depend on the scaling of the input points.\n-** How does the error bound of Lemma 4.1 compared to the maximum size that ||x^L||_2 could be? Shouldn\u2019t this maximum size be roughly proportional to Product L_l ||A^l||_2. Thus, wouldn\u2019t this make the error bound very weak? As the error itself is large compared to ||x^L||_2?\n- Fig 1 \u2014 is Lemma B.5 meant to refer to Lemma 4.1?\n-** Where does the random matrix theory actually come in to Lemma 4.1? In particular, if I have a matrix with random mean 0 entries, then I can bound the spectral norm of that matrix trivially by the Frobenius norm, which by Markov\u2019s inequality is at most something like n*E[Var(random variable)] which good probability. Random matrix theory results improve this bound to something scaling more like \\sqrt{n}. If you instead used the trivial Frobenius norm bound, how would Lemma 4.1 change?\n- In Section 5 it was unclear of the Sparse Matrix Sketching idea was novel to this paper, or something from the literature? No citations are given.\n- **Say I have a p x p matrix with j*p nonzeros. Then to represent this matrix in sparse matrix format, I need j*p values for by non-zero entries, along with j*p indices indicating their positions. These indices take ~ log p bits each. Thus, I need roughly j*p*log p parameters to represent the whole matrix. I don\u2019t see how sparse matrix sketching is improving on this simple argument. In fact, since it compressed to a \\sqrt{jp}logp x \\sqrt{jp} log p sized dense matrix, it seems to lose a log factor since (\\sqrt{jp} log p)^2 = jp log^2 p parameters.\n- ** Related to the above question, an alternative bound based on a simple counting argument is given in Lemma D.1. The claim is that this bound is \u2018combinatorial\u2019. But it is not, given that {d_1 d_2 choose alpha} is in a log and thus bounded by roughly alpha*log(d_1 d_2).  I could not make sense of the bound of Theorem 5.2 enough to compare them but I couldn\u2019t understand why Lemma D.1 was obviously a worse bound. We should be able to bound the number of non-zero entries alpha directly using Lemma 5.1 by something like max(d_1,d_2)*max(j_r,j_c). Doing so looks like it would in fact give a stronger bound that Theorem 5.2\n- In Lemma 5.1, what is lambda_l? Without this being defined, it is impossible to interpret the theorem. E.g. we could have lambda_l = max(d_1,d_2). Also what roughly is Chi? \n- In Theorem 5.2, it says that d must be chosen to make some expression hold. However, that expression does not depend on d. So I don\u2019t understand what this is saying."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1144/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904900771,
            "cdate": 1698904900771,
            "tmdate": 1699636040543,
            "mdate": 1699636040543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8rApNMcKCk",
                "forum": "aQij7UmwTF",
                "replyto": "3XlbEndRRo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response Part 1"
                    },
                    "comment": {
                        "value": "We appreciate this reviewer's thoughtful review. We feel there are some parts miscommunicated. We hope these answers alleviate your concerns and raise your rating of our paper.\n\n# Weaknesses\n\n1. *The abstract is confusing as it seems*\u2026\n\nA. Magnitude-based Pruning is a very specific term as it applies to neural networks. We hope this clears up the confusion.\n\n\n2. *Overall, paper has lots of typos*\u2026\n\nA. Matrix Sketching is a well-known technique in matrix theory. I have added the relevant citations in the main text. We use Matrix Sketching to represent the space of sparse matrices in the set of smaller dense matrices to create tight generalization bounds from \u201cSpecifically, we show that our set of pruned matrices can be efficiently represented in the space of dense matrices of much smaller dimensions via the Sparse Matrix Sketching.\u201d  We have fixed this in the uploaded new version.\n\n3. *In 3.1, what is the difference between R(M) and* \u2026\n\nA. We state in the paper that \u201cThe population risk $R(M)$ is obtained as a special case of $R_{\\gamma}(M)$ by setting $\\gamma = 0.\u201d The fixed string is mentioned as we are repeating the lemma exactly from the work of Arora, which has the flexibility to use such fixed strings, but we do not use them. We have fixed this in the newly uploaded version. Moreover, yes, this is a small typo. We have dropped the string notation, as mentioned in the newly uploaded version.\n\n4. *In Assumption 3.1, are the weights independent of* \u2026\n\nA. We agree with this comment. We have specified the need for independence in the newly uploaded version.\n\n5. *In Remark 4.1, what is meant by \u2018diagonal elements\u2019 given that* \u2026\n\nA. The diagonal elements of a nonsquare matrix $A$ refer to any element $A_{i, i}$ for any $i$. We have made this more evident in the uploaded text. This intuition is where we believe this reviewer has misinterpreted this result. We need two facts: the error induced by pruning is small, and the sparsity is evenly distributed throughout the weights. If we assume gaussianity, which has been noticed in the past according to Han, both hold. Moreover, we study why randomized pruning leads to generalization, an empirical phenomenon. Magnitude-based pruning can be removing elements below a threshold or removing elements randomly with smaller values that are more probable (see Qian and Klabjan 2021). We can extend our results to deterministic pruning, but we still need the sparsity to be roughly distributed for matrix sketching to work. Thus, it is likely that a Gaussian assumption is still necessary even for deterministic pruning. We also apologize for not including the definition of $\\gamma_L$ in the main text; we moved its definition to the appendix for space reasons. We will move it back to the main text. We want to represent the possible space of parameterizations of sparse matrices as smaller dense matrices to generate tighter generalization bounds."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120863493,
                "cdate": 1700120863493,
                "tmdate": 1700120863493,
                "mdate": 1700120863493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1x6p7Uyoz7",
                "forum": "aQij7UmwTF",
                "replyto": "3XlbEndRRo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response Part 2"
                    },
                    "comment": {
                        "value": "# Weaknesses \n\n6. *I also couldn\u2019t make intuitive sense of Lemma 4.1*\u2026\n\nA. You are correct; there is a small typo in the proof. The norm of the input should scale with this error. We have corrected this in the newly uploaded version. This is a minor change. The latter part about the significant error compared to $\\|x^L\\|_2$ is invalid. As the error induced by pruning decreases, epsilon decreases, and the error decreases. The main point of this proof is that the error scales linearly with epsilon. This margin is not tight in practice, as seen in Figure 1a. This looseness is normal in margin bounds and suffices for our generalization bounds. This looseness and style of analysis are similar to the results of Arora et al. 2018. While one can trivially reduce this dependence using noise stability, such as Arora et al. 2018, this only slightly improves this bound. \n\n\n7, *Fig 1 \u2014 is Lemma B.5 meant to refer to Lemma 4.1?* -...\n\nA. Yes, there are many ways to prove that pruning does not induce too much error. We agree with this reviewer that there are many ways to do this. Again, analyzing the spectral norm of the difference between the original and pruned matrix is a straightforward way to analyze this and follows some of the analysis of the Probalistic pruning change. We do not believe alternative methods to proving similar bounds make this standard way any less strong. Random Matrix Theory is useful for demonstrating that the spectral norm of the difference between a pruned matrix and the original matrix is relatively small. There are many ways to show this. However, trivially applying Markov\u2019s Inequality results in a very poor bound. Using it results in an upper bound of $\\|\\hat{\\mathbf{A}} - \\mathbf{A}\\|_2 \\leq c\\|\\mathbf{A}\\|_2$ where $c$ is some large constant. This bound is weaker than ours. \n\n8. *In Section 5 it was unclear of the Sparse Matrix Sketching* \u2026\n\nA. In Theorem 5.1, we cite Dasarthy et al. Matrix Sketching is an idea from existing papers, but its use for generalization bounds is indeed novel. \n\n\n9. *Say I have a p x p matrix with jp nonzeros* \u2026\n\nA. This is an equivalent argument to the combinatorial argument in the appendix. We agree with this reviewer. We thank you for this catch. Indeed, the argument in the appendix is just as tight as matrix sketching. However, the argument in the appendix is still strong and improves on existing bounds. We hope this is enough in your opinion for acceptance.\n\n10. *Related to the above question*, \u2026\n\nA. We agree with this reviewer. Please see above.\n\n11. *In Lemma 5.1, what is lambda_l?* \u2026\n\nA. As stated, Lemma 5.1 holds for any $\\lambda$ in $\\mathbb{R}$. Moreover, $\\chi$ is exactly as stated in Lemma 5.1. \n\n\n12. *In Theorem 5.2, it says that d must be chosen* \u2026\n\nA. $\\epsilon_l$ depends on d from the pruning error. This comes from Lemma 4.2. \n\n\n# References \n\nXin Qian and Diego Klabjan. A probabilistic approach to neural network pruning. CoRR, abs/2105.10065, 2021. URL https://arxiv.org/abs/2105.10065.\n\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. CoRR, abs/1802.05296, 2018. URL http://arxiv.org/ abs/1802.05296.\n\nGautam Dasarathy, Parikshit Shah, Badri Narayan Bhaskar, and Robert D. Nowak. Sketching sparse\nmatrices. CoRR, abs/1303.6544, 2013. URL http://arxiv.org/abs/1303.6544."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121126964,
                "cdate": 1700121126964,
                "tmdate": 1700121126964,
                "mdate": 1700121126964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2epER9LCgE",
                "forum": "aQij7UmwTF",
                "replyto": "8rApNMcKCk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_zSMw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_zSMw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the explanations. \n\nI still am confused about the Gaussianity assumption. What would break in the analysis if I used any other bounded distribution which was independent and identical across entries instead of Gaussian? The error would still be small and the sparsity would be evenly distributed by the iid assumption no? \n\nAs a follow up question -- why would deterministic pruning not work if I have iid random weights? Again, even though the pruning threshold is deterministic, the pruned weights would be well-spread due to the random weight assumption."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149480074,
                "cdate": 1700149480074,
                "tmdate": 1700149480074,
                "mdate": 1700149480074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S3vAunqr04",
                "forum": "aQij7UmwTF",
                "replyto": "1x6p7Uyoz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_zSMw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1144/Reviewer_zSMw"
                ],
                "content": {
                    "comment": {
                        "value": "Just to follow up: how exactly would Lemma 4.1 change if we just used a naive bound on ||A-tilde A||? From what I can tell, the theorem already has a large constant in its bound anyways.\n\nAm I correct that applying sparse matrix sketching then does not give better generalization bounds then naive counting arguments? If this is the case I am worried about how the paper is framed. It focuses on using random matrix theory and sparse matrix sketching as tools for generalization bounds. But it sounds as if there is no need for at least one of these tools. \n\nI also think the description in the paper is incorrect then. In particular, don't the following paragraphs of the paper contradict the above response?\n\n\"Counting the number of parameters in small dense matrices\nis more efficient in terms of parameters than counting the number of large sparse matrices, thus\nproviding a way of evasion of the combinatorial explosion. We formalize this in the following section.\"\n\n\"Regrettably, such a bound is quite poor in its dependence on the size of the matrices, mainly due\nto the logarithm term of the factorial, which is a significantly large value. This is, in fact, worse\nthan many of the previous bounds in the literature. This is due to the combinatorial explosion of the\nnumber of sparse matrices. However, if there exists a way to instead represent the space of sparse\nmatrices within the space of dense matrices of much smaller dimensions, then we could avoid such a\ncombinatorial explosion of the number of parameters. This is the exact purpose of matrix sketching\""
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1144/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149868053,
                "cdate": 1700149868053,
                "tmdate": 1700149868053,
                "mdate": 1700149868053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]