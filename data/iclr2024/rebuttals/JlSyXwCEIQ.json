[
    {
        "title": "CodeIt: Abstract Reasoning with Iterative Policy-Guided Program Synthesis"
    },
    {
        "review": {
            "id": "o50OdUbUdT",
            "forum": "JlSyXwCEIQ",
            "replyto": "JlSyXwCEIQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a new program synthesis approach for solving the Abstraction and Reasoning Corpus (ARC). The authors take a 220m parameter T5 code pretrained language model, fine-tune it on handwritten solutions to the 400 training set tasks using a DSL designed for ARC as well as randomly mutated solutions to these tasks, and then train a policy by iteratively attempting to solve tasks and adding hindsight-relabelled solutions to a buffer of tasks to train the policy on. Their approach solves 40/400 tasks on the evaluation split, and improves over time."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The approach is excellent: well-designed, simple in principle yet careful in the details, and seems to be tuned well. The design decisions are carefully explained and justified, and the whole procedure is written out well in a way that is easy to follow. \n\nThis is a natural approach for ARC that has not been tried yet, and it is really exciting to see the results the authors have generated. Many ARC papers, as the authors note, are not good enough to use as a baseline, so the fact that this work succeeds in attempting to solve tasks from the evaluation set is a noteworthy accomplishment. Moreover, the approach is not simply brute-force search, and has the potential for much better performance if various parts of the system are tweaked and improved.\n\nThe related work is good. The ablation experiments are good. The discussion section is good. Really, it is a very nicely written paper."
                },
                "weaknesses": {
                    "value": "- Overall, the writing could use general revisions for clarity, mainly on little details of wordings rather than high level changes.\n- There is no mention of code being available or made open source.\n- The authors only evaluate their approach on ARC, even though it could be in principle compared to synthesis approaches in other domains. Due to the difficulty and uniqueness of ARC, I think only evaluating on ARC is more than sufficient, but showing performance on another domain would help elucidate what about the algorithm is working and not working as expected.\n- There is limited insight into understanding the capabilities of the model. For example, the paper does not convey well what exactly the model is improving at over time, or how competent the model is at generating syntactically correct code or using the full range of DSL operators once the fine-tuning stage is completed. \n\nOverall, despite the well-written nature of the paper and good design of the approach. I find it hard to have a good grasp of how well the approach actually worked. 40/400 tasks seems a bit low given the technique, and there's not much space in the paper devoted to understanding what exactly is going on with the model. The ablations help a bit. I think the best way to get a sense of this would be to have a link to a page which shows the tasks and generated solutions the model discovers over the course of training, and when each one was found. This would really help understand how well the approach is working. In addition, seeing some of the tasks it fails to solve, which we might expect it to solve (e.g. if the DSL can solve it in a few operators), would help too. While this might make the approach look less impressive, it would really increase the quality of the paper."
                },
                "questions": {
                    "value": "Questions\n- Why don't you evaluate on the hidden test set? Even if you do not beat the state of the art, it would be good to know the performance.\n- I don't understand how hindsight relabelling works with the mutation baseline in section 3.1 / appendix A.2 \u2014 for searching via mutation, it seems like you just need to store the program and the inputs into the buffer, no need to store the outputs?\n- Your main approach that combines mutation and policy just uses mutation for generating the data during fine-tuning, is that right? There's none of the iterative random mutation search to try to solve tasks?\n- Do you have any evidence that the pretrained model is comfortable using the DSL after pretraining? Or does it still struggle to use the correct input args, etc? not sure how best to convey this, but any information on this could help understand the model's capabilities better.\n- I'm wondering if a grid array number encoding works better for training, because it's more like how T5 has seen things encoded before, instead of a brand new encoding it doesn't really understand. did you try this at all, or just stick to the computationally cheaper representation?\n- You mention that including the mutated programs is a form of regularization, but I would prefer to call it data augmentation. Both imply preventing overfitting, but based on my understanding of the term, adding more data doesn't really regularize a model per se. \n- What % of the time do the sampled programs execute successfully?\n- 40/400 eval is not the state of the art on eval, is it? I think brute force approaches have solved a high fraction of the eval set \u2014 can you clarify? For example, I recall the original ARC kaggle competition winner saying they solved at least a hundred on the evaluation set using their DSL: https://github.com/top-quarks/ARC-solution/tree/master\n\nSuggestions (feel free to ignore if you feel otherwise, and no need to discuss in rebuttal)\n- I think the ARC example in figure 1 should be more complicated. This would both help audiences unfamiliar with ARC better understand the nature of the dataset, and would make the object encoding scheme easier to understand when looking at how the input is encoded \u2014 right now, the object encoding for Figure 1 has a lot of 0's, 1's, and 2's, which makes it hard to quickly understand how the example maps to the input.\n- My suggested improved title: Solving the Abstraction and Reasoning Corpus with Iterative policy-guided program synthesis. Justification: you're only evaluating on ARC, and calling your approach \"abstract reasoning\" is only true in the sense that it's applied to the abstraction and reasoning corpus. I don't think your LLM is doing much abstraction or reasoning, in the sense of forming new abstractions itself, or chaining multiple steps of thinking together to arrive at its solution (debatable though depending on your NN philosophy)\n- It might be good to include more examples of the DSL operators, so readers can have a rough sense of what your LM is generating without having to fully read Hodel's DSL explanation. Figure 4 is useful for this, but maybe you can show an example earlier on for this.\n- Some details to clarify in the main text: \n    - the mutated programs are evaluated on the inputs of the program it mutated off of to generate outputs\n    - some more details on fine-tuning: how many mutated programs, how many epochs of training. \n    - you mention weighing training on the handwritten solutions and solved tasks more often, so that you don't forget them, but is this explained further, or in the pseudocode in the appendix? if not, they should be included (maybe I missed it)\n    - I would rewrite the last sentence of the \"sampling stage\" paragraph in section 2.2 to be clearer that you sample n_p times total. \n- I would be careful not to use the word \"test\", as it might be mistaken for ARC's hidden test set. for example, in Table 2, I would call it Eval performance, not Test performance.\n- You have a \"policy only\" baseline in Figure 3, which should be described or at least listed in the \"baselines\" section. In particular, I am confused whether \"policy only\" means that the initial fine-tuning doesn't have the mutated tasks, or if the \"policy+mutation\" means that you're searching for new tasks solutions at each iteration with both with the policy and via random mutations. \n- Clarify that the solution programs were also written by Hodel \u2014 they deserve credit for that!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5606/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5606/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698078868858,
            "cdate": 1698078868858,
            "tmdate": 1699636578154,
            "mdate": 1699636578154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z5NWb3l23L",
                "forum": "JlSyXwCEIQ",
                "replyto": "o50OdUbUdT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their in-depth comments and constructive questions. We highlight updates to the paper based on this review in ```purple```.\n\n> There is limited insight into understanding the capabilities of the model.\n\nWe included additional analysis and results to provide more intuition for why the method works. For example, we include a comparison between solutions identified by the mutation baseline and CodeIt in Appendix D.1 and D.2, and analyze how CodeIt solutions change over time in Appendix D.3. We observe, for example, that in 81% of tasks for which CodeIt finds a solution, a shorter solution is identified in a later meta-iteration. This indicates that the method may use insights gained in some tasks to improve its solution for other tasks.\n\n> Why don't you evaluate on the hidden test set?\n\nAlthough this evaluation can provide another datapoint, this set is not available to the public, making it difficult to reproduce results. Not many published works report this performance, and it can only be obtained by submitting code to Kaggle or the ARC challenge. We did consider it however, and may do so at a later stage. \n\n> I don't understand how hindsight relabelling works with the mutation baseline in section 3.1 / appendix A.2\n\nFor the mutation baseline, we perform task relabeling, an exhaustive form of hindsight relabeling similar to [Gauthier 2022]. We run each program produced by mutation on *all* tasks, not just the task corresponding to the expert program we're mutating. Mutating is semi-random, so it sometimes results in a lucky hit, where we unexpectedly solve a different task. We do not perform this task relabeling procedure for CodeIt.\n\n> Do you have any evidence that the pretrained model is comfortable using the DSL after pretraining? \n\nWe do not test this explicitly, but we can infer this from the ablations in Figure 3, where we see that CodeIt without pretraining is unable to improve performance. \n\n> I'm wondering if a grid array number encoding works better\n\nWe picked our encoding scheme to minimize encoding length, as this would enable fitting this in the (limited) LLM context window. There is also some evidence that object-based representations of ARC tasks are easier to work with for LLMs [Xu 2023]. But it is possible that the best encoding for ARC tasks indeed depends on the base model and its pretraining data.\n\n> You mention that including the mutated programs is a form of regularization, but I would prefer to call it data augmentation.\n\nWe agree, this is a better choice of words. We updated the text.\n\n> What % of the time do the sampled programs execute successfully?\n\nWe did not track this number, as we filtered both programs that did not execute correctly and duplicate programs, so we have no way to distinguish between the two. After training we get about 5,000 programs for 24*400 = 9,600 sampled tasks, so it is likely that more than 50% of programs execute successfully.\n\n> I recall the original ARC kaggle competition winner saying they solved at least a hundred on the evaluation set using their DSL\n\nThis is correct, this method should give 129 correct programs. However, the DSL was designed by looking at the eval set. This means a human designer explicitly built priors into the DSL that would help with solving ARC evaluation programs. Michael Hodel first and foremost looked at the training set when designing the DSL, and thus stayed more true to the spirit of the ARC challenge as described by [Chollet 2019].\n\n> It might be good to include more examples of the DSL operators .. Clarify that the solution programs were also written by Hodel \u2014 they deserve credit for that!\n\nFully agreed, we did acknowledge this in Section 3. We added more details about the DSL and links to the GitHub repo of Hodel in Appendix B.4 - and explicitly mention that he built the 400 solution programs we used to kickstart CodeIt.\n\nOther comments:\n - we added more details about the hyperparameters and how we tuned these.\n - we added a description for the CodeIt variations in Figure 3, also see the main response above for details on the changes.\n\nReferences:\n- [Gauthier 2022]: \"Learning Program Synthesis for Integer Sequences from Scratch\", Gauthier & Urban, 2022\n- [Xu 2023]: \"LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations\", Xu et al., 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522102114,
                "cdate": 1700522102114,
                "tmdate": 1700591007542,
                "mdate": 1700591007542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0wMYXAERWx",
                "forum": "JlSyXwCEIQ",
                "replyto": "Z5NWb3l23L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment and for addressing my questions!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681554405,
                "cdate": 1700681554405,
                "tmdate": 1700681554405,
                "mdate": 1700681554405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z0pQIkZ6ud",
            "forum": "JlSyXwCEIQ",
            "replyto": "JlSyXwCEIQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_xY18"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_xY18"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes CodeIt, a program synthesis method that leverages learned prior for sampling and learning for ARC problem solving. To solve an ARC instance, the method uses a code-based pretrained network to sample program variants and used the augmented data to retrain the network for final program prediction. With good implementation, the method achieves good performance compared to previous state of the art, and the ablation studies show the contribution of proposed components."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method is essentially a simplified version of DreamCoder, the dream part in particular. The pretrained code-based network serves as the learned prior, and the sampling stage basically tries to augment the input-output-program triplets such that they could be used for fortifying the network, providing locally diversified data for additional training. This is a pretty intuitive way of attempting the problem. However, some questions still persist, see below."
                },
                "weaknesses": {
                    "value": "As the method still largely relies on the data it samples, I wouldn't be surprised to see it becomes better than previous methods. However, I'm also interested in hearing where the limit of the simple mutation baseline is: if you more extensively sample mutations and in the extreme case, the mutations cover all your newly sampled data, would your method becomes inferior? From my perspective, your method might only be better than the mutation baseline, because your pretrained policy network serves as some smart prior, and to get exactly the programs your policy samples, random mutation might simply take more than (less efficient). However, using a unverified prior would also risk data coverage, meaning that it might not cover as much data as the random mutation method. In this sense, the random mutation, while inefficient, when taken to the extreme, could possibly be better.\n\nAs the method is basically data-driven, but smartly, I would be expecting to see ablation on the amount of sampled data, rather than the context length. How would the model perform if you reduce the number of samples, or even better, can you show the curve of perf vs. num of samples per task? If you decrease that number, your performance might not be better than the mutation baseline.\n\nAnother problem regarding ARC in general is evaluation. There has been no equal footing as for the number of data one could use. I note that in your method, you have incorporated quite a lot of sampled programs, and I seriously doubt what would happen when the newly sampled programs are used by other methods. Also a pretrained CodeT5+ is used, which already sees quite a lot of data. In the extreme, one would like to sample the space as much as possible and feed them to a model, saving all the trouble in modeling.\n\nExperiments only on ARC are not necessarily sufficient to show the superior of the method. I knew of the Raven matrices that also stress abstract reasoning, and the RAVEN dataset has similarly structured data and a much simplified program structure. Would it be possible to show similarly improved performance on this task?\n\nOne thing I've been thinking about ARC is that the community has been doing program search on a fixed DSL for a while. Would it be possible to jointly search over the DSL space, maybe starting simple such as using a mutation method for the DSL space?"
                },
                "questions": {
                    "value": "See weaknesses. A lot of the questions may not be properly answered under the current climate, but please try to."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743333939,
            "cdate": 1698743333939,
            "tmdate": 1699636578045,
            "mdate": 1699636578045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NNsE14ScDy",
                "forum": "JlSyXwCEIQ",
                "replyto": "Z0pQIkZ6ud",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer xY18"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work and their insights. Changes to the paper related to his review are listed in ```orange```.\n\n> I\u2019m also interested in hearing where the limit of the simple\nmutation baseline is: if you more extensively sample mutations and in\nthe extreme case, the mutations cover all your newly sampled data, would\nyour method becomes inferior?\n\nOur experiments were indeed designed to maintain data-parity. If one were\nto increase the number of sampled mutations, one should also increase the\nnumber of policy samples to ensure fair comparison. It would be unfair\nto allow the mutation to sample new programs indefinitely and not allow\nCodeIt do to likewise.\n\n> ablation on the amount of sampled data, rather than the\ncontext length. How would the model perform if you reduce the number\nof samples, or even better, can you show the curve of perf vs. num of\nsamples per task?\n\nThere are two ways to interpret this question. First,\nif the reviewer asks about the effect of the sampled programs per task\n$n_\\rho$ per meta-iteration: we chose this number based on experiments on a\ncustom validation set. We did not ablate this choice here, as doing so\nwould require tuning other parameters as well (learning rate, batch size\nand/or number of iterations of continual learning).\n\nSecond, if the reviewer asks about performance over time, but shown on a\ndifferent axis: we sample equally many programs per task during search,\n$n_\\rho$. This means that dividing the number of sampled programs on the $x$-axis in\nFigure 3 by the number of tasks (400) will give the performance vs the number\nof sampled programs per task.\n\n>you have incorporated quite a lot of sampled programs, and\nI seriously doubt what would happen when the newly sampled programs\nare used by other methods. Also a pretrained CodeT5+ is used\n\nLikely, an approach in which a capable enough LM is trained on the programs\nsampled by CodeIt would be capable of imitating CodeIt\u2019s performance;\nbut this approach would amount to nothing more than a behavioral clone\nof CodeIt.\n\nWe are unable to control for the amount of data used to train CodeT5+,\nsince this information is not in the public domain (only the number of files used is reported, but not the amount of tokens in each). Our ablations show that using pre-trained weights does play an important role (Figure 3, right\nside).\n\nA recent work [Gendron 2023] reports a performance of 11.9% of GPT-4\non the ARC eval set. While this is hard to compare directly to our result\ndue to methodological differences (we do train on ARC, but see much less\ndata), it is telling that we match one of the most capable generalist LLMs in performance, despite using a much smaller CodeT5+\nmodel.\n\n> Experiments only on ARC are not necessarily sufficient to\nshow the superiority of the method. I knew of the Raven matrices that also\nstress abstract reasoning, and the RAVEN dataset has similarly structured\ndata and a much simplified program structure. Would it be possible to\nshow similarly improved performance on this task?\n\nThanks for pointing out this benchmark, we were not aware of it. We chose the ARC\nbenchmark because of its difficulty (as also remarked by reviewer NHnw),\nthe availability of neural baselines to compare with, and the possibility\nto efficiently represent the grids. While the RAVEN tests are similar in\nspirit, choosing how to represent its tasks is an open question, meaning\nthat applying our method would be a new research effort. We consider\nthis beyond the scope of this work, but will consider it for future work.\n\n>Would it be possible to jointly search over the DSL space,\nmaybe starting simple such as using a mutation method for the DSL\nspace?\n\nWe see learning or refining the DSL as an interesting area for\nfuture research. Previous work [Ellis 2020] proposes an approach called\nDreamCoder that can perform DSL refinement (referred to as abstrac tion), and work [Banburski 2020] applies this method to ARC; however,\nthis scaled poorly to the size of most ARC problems. We therefore deemed\nit to be beyond the scope of the present work, and focused on an approach\nthat scales well for a given DSL."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520717579,
                "cdate": 1700520717579,
                "tmdate": 1700555504052,
                "mdate": 1700555504052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3FqGPmohGW",
                "forum": "JlSyXwCEIQ",
                "replyto": "NNsE14ScDy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_xY18"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_xY18"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the authors' response. I believe some questions are unanswered, and yet some are not, eg. the performance curve. Therefore, I keep my initial rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705691408,
                "cdate": 1700705691408,
                "tmdate": 1700705691408,
                "mdate": 1700705691408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1U6rOzsms2",
                "forum": "JlSyXwCEIQ",
                "replyto": "Z0pQIkZ6ud",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reading our response. Apart from the question on additional benchmark tasks and refining of the DSL, which questions do you believe remain unanswered? We will do our best to respond now and/or address these in the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735570627,
                "cdate": 1700735570627,
                "tmdate": 1700735777419,
                "mdate": 1700735777419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GvIzis5M5i",
            "forum": "JlSyXwCEIQ",
            "replyto": "JlSyXwCEIQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_JPeY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_JPeY"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an iterative program search method to tackle the challenging benchmark, ARC, designed to measure AI skill acquisition and generalization ability. The main idea is similar to iterative policy improvement approaches, where at each iteration,1)  a set of solutions are sampled from current policy, 2) local search is performed around the sampled solutions (e.g., program mutation), 3) policy is improved via learning. The model is finetuned from the pretrained Code-T5 model, and the entire work is implemented based on the DSL manually designed for ARC. The experiment shows that the proposed method, CodeIt, outperforms previous SOTA method by large margin. Also, authors conduct various ablation studies to analyze policy\u2019s capacity to understand input-output grids (i.e., few-shot program inference ability), and the effect of policy update and pretrained weights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed idea is simple and reasonable\n* The empirical study clearly demonstrates the effectiveness of the proposed method\n* The paper reads well (but there are few missing details. Please refer to the Questions section)\n* The ablation experiments are adequately designed to analyze the effect of each component"
                },
                "weaknesses": {
                    "value": "* Limited contribution and novelty\n\n As described in the related work section, the proposed idea shares the main idea with the iterative policy improvement works: iterating between policy-guided search followed by imitation learning to improve the policy. It is indeed somewhat beneficial to the readers to show that applying existing idea to the new challenging domain works well. However, it would be more helpful to provide more intuition beyond that, given that the idea is mostly inspired by the existing work in other domains. For example, analyzing whether there is any unique challenge/benefit in applying iterative policy improvement ideas to the ARC domain compared to conventional RL domains could be an interesting contribution.\n\n* Scalability is limited\n\n In the discussion section, authors mention that \u201c..via program mutation and hindsight relabeling, thus requiring only a small amount of expert programs to start training\u201d. Although this is much better than requiring a large number of programs as data, the program mutation is only possible and effective if the DSL is efficiently designed by domain experts and also the program mutation algorithm is carefully designed by domain experts. Also for filtering, the execution engine for DSL is required as well. Overall, these requirements limit the scalability of the proposed approach, and it is important to study how well the proposed method will perform depending on the quality/availability of these prerequisites.\n\n* Comparison with baselines in common settings\n\n Although I recognize that there seems no widely accepted common experiment  setting exists, when it comes to comparing with other approaches, it would be more convincing if the proposed method and the compared methods are compared in a common setting. I do agree that the proposed DSL, grid representation, program mutation, and DSL execution engines are the important contributions of this work. However, still for evaluating the effectiveness of the proposed *learning framework* alone, including an apple-to-apple comparison result would greatly improve the paper\u2019s significance. Also, comparing with other learning-based baselines would be great.\n\n* Intuition behind recency-based sampling of input-output pairs\n\n It would be helpful to provide intuition behind the proposed design choice such as recency-based sampling of input-output pairs."
                },
                "questions": {
                    "value": "* Comparison with previous iterative policy improvement methods\n\n It is not clear how the proposed method differs with iterative policy improvement. Sampling from policy and performing local search (e.g., program mutation in CodeIt) around the sampled action is a common approach in iterative policy improvement. The related work only explicitly compares with ExIt and ReST among iterative policy improvement methods. \n\n* First paragraph of Section 3.1: the meaning of $n_\\rho$ and $n_{\\text{tasks}}$ are not defined\n\n* In Figure 3, the meaning of x-axis is undefined and unclear. Section 3.3 mentions \u201cacross meta-iterations in Figure 3.\u201d. Does it mean \u201cnumber of sampled programs\u201d is the same as meta-iterations?\n\n* The agents \u201cCodeIt: mutated+policy\u201d and \u201cCodeIt: policy only\u201d are not defined\n\n* Suggestion: population-based policy searching\n\nAs indicated by the ablation A2 experiment, policy improvement plays an important role in program searching. It would be an interesting future direction to try population-based policy searching approaches; i.e., maintaining multiple population of policy networks Q to enable more efficient exploration in program search space."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802000572,
            "cdate": 1698802000572,
            "tmdate": 1699636577911,
            "mdate": 1699636577911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rcZExehX30",
                "forum": "JlSyXwCEIQ",
                "replyto": "GvIzis5M5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments. Updates to the paper related to this review are listed in ```brown```.\n\n> Limited novelty; the proposed idea shares the main idea with the iterative policy improvement works\n\nWe respond to this point in the global response above, as novelty was raised by reviewer WdyM as well. \n\n> Scalability is limited: it is important to study how well the proposed method will perform depending on the quality/availability\nof [the DSL, mutation procedure, expert programs]\n\nIt is true that our method relies on access to some expert programs and a DSL amenable to mutation. We investigate the effect of the mutation procedure in Figure 3, and through additional ablations in Appendix C, also see the global response. We observed that mutation is an essential ingredient for fast training - it is not impossible to train CodeIt without it, but it will take longer. For many program synthesis works, access to an interpreter is a necessary requirement, and as such we do not see this as a limitation. \n\n> Comparison with baselines in common settings\n\nWe have added comparisons to additional neural baselines to the revised paper, in particular LLM-based methods that solve a variation of the ARC eval set. \n\n> Intuition behind recency-based sampling of input-output pairs\n\nWe saw some evidence that as we ran CodeIt, programs would become shorter over time. This likely meant those solutions were of higher quality, and that they should therefore be sampled more often. We include quantitative proof of this phenomenon in Appendix D.3, where we show that for 81% of tasks for which CodeIt finds a solution, a shorter solution is found in a later meta-iteration. We also include qualitative examples in Appendix D.2.\n\n> Population-based policy searching\n\nThanks for the suggestion to explore population-based search, this could indeed be a way to increase data diversity during exploration and prevent stagnation. For example, [Jung 2020] shows that population-based learning can be an effective way to improve off-policy RL performance. Given the limited rebuttal period, we decided to leave more advanced search approaches for future work.\n\n\nOther comments:\n- We added the definition of $n_\\rho$ (number of sampled programs per task per meta-iteration) and $n_{tasks}$ (number of tasks) to the text.\n- The x-axis of Figure 3 shows the total number of sampled programs. As we sample a fixed number of programs per task per meta-iteration, dividing this number by $( n_\\rho \\times n_{tasks}) $ gives the total number of meta-iterations.\n- We added more descriptive names for the methods in Figure 3. Also see the global response for more info on the updated Figure 3.\n\nReferences:\n- [Jung 2020] \"Population-Guided Parallel Policy Search for Reinforcement Learning\", Jung et al., ICLR 2020."
                    },
                    "title": {
                        "value": "Response to reviewer JPey"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519659623,
                "cdate": 1700519659623,
                "tmdate": 1700555638360,
                "mdate": 1700555638360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HO3mfW4m50",
            "forum": "JlSyXwCEIQ",
            "replyto": "JlSyXwCEIQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the task of programming by examples. The authors propose a method where a language-model-based policy network generates DSL programs given the input examples. The policy network is first pretrained on human-annotated examples-program pairs, and then iteratively trained on programs and input examples generated from the last iteration of the network. Program mutation and hindsight relabeling increase the diversity of the training data. The proposed method has achieved SOTA performance on the evaluation split of the ARC benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is simple and straightforward.\n- The proposed method achieves SOTA performance on the evaluation split of ARC. The ARC benchmark being tackled is known to be challenging. Most works do not evaluate the full evaluation dataset but instead focus on a simpler subset.\n- Ablation studies are also conducted to demonstrate the contribution of each component in the model."
                },
                "weaknesses": {
                    "value": "- Technical novelty is limited. As the related work mentions, iterative policy improvement and hindsight relabeling are not new ideas. In the regime of program synthesis, the idea sampling from the policy, filtering by execution, and then retraining is also explored in [1].\n- The effectiveness of the proposed pipeline is not convincing:\n    - From Table 1, we can see that the full version of CodeIt solves only 1% more programs compared with the \u201cmutation only\u201d baseline (40/400 vs 36/400). This naive baseline simply samples programs from the training dataset perturbed by changing one line of code. This indicates the DSL and the original reference program for the training dataset play a major role in achieving SOTA performance. The author also mentions this point in the discussion. But I believe it is important to expand on this.\n    - To better illustrate the performance of CodeIt, I strongly encourage the authors to provide the following results: 1. how many solved tasks are in common for CodeIt and the mutation baseline? 2. For tasks that both methods solve, are there any differences between the solutions of each method? Will CodeIt generate a more succinct program?\n\nOverall, I believe the technical novelty and the effectiveness of the method need more justification. I am willing to raise my score if my concern is addressed.\n\n[1] Language Models Can Teach Themselves to Program Better.  Patrick Haluptzok, et al. ICLR 2023"
                },
                "questions": {
                    "value": "- It would be helpful if the authors could provide quantitative results on how well CodeIt compresses programs like the example in Figure 4. How many correct programs are shorted throughout the iteration? Are there programs that become longer?\n- The task mutation procedure plays a crucial part in the procedure, I believe it\u2019s important to expand the pseudocode to explain more details about it. There is no explanation about functions in the pseudocode, though some can be inferred from the names. How is this mutation procedure determined? How sensitive are the results w.r.t to the hyperparameter $\\phi$?\n- How do authors determine the number of training epochs during continual learning? Will training for more than 1 epoch for each iteration degrade the performance?\n\nOther Comments:\n- Figure 4, it\u2019s hard to interpret the figure because the task being considered is not shown and the meaning of each function is unknown.\n- why the example input and output are named S_0 and S_N?\n- It is encouraged for authors to include the DSL in the paper, the original write-up of the DSL is not straightforward to find.\n\nTypo\n- The first sentence of Section 3.3: \u201cWe report performance of CodeIt after [sampling] 500,000 programs,\u201d\n- Second paragraph of Section 2.2: \u201cWe [finetune] the policy network on the resulting set, and initialize our replay buffer with it\u201d. \u201cfinetune\u201d should be \u201cpre-train\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5606/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM",
                        "ICLR.cc/2024/Conference/Submission5606/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814144408,
            "cdate": 1698814144408,
            "tmdate": 1700732414152,
            "mdate": 1700732414152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AgEiaOpUYL",
                "forum": "JlSyXwCEIQ",
                "replyto": "HO3mfW4m50",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for highlighting the simplicity of our method, as well as its performance and the exhaustiveness of our ablation studies. Updates to the paper related to this review are listed in `green`.\n\n> Technical novelty is limited. As the related work mentions, iterative policy improvement and hindsight relabeling are not new ideas. In the regime of program synthesis, the idea sampling from the policy, filtering by execution, and then retraining is also explored in Haluptzok et al. (2022)\n\nThanks for pointing out reference [Haluptzok 2022], we have included a comparison in the related work section. The key differences between that work and ours are the way tasks are proposed and how output programs are\nused. In their work, synthetic tasks are proposed by a language model,\nand programs that solve those puzzle correctly are saved. In our work\nwe use real task inputs only, but use hindsight relabeling to augment the\noutputs: we save all programs that can be executed and their outputs as\nnew, synthetic tasks. Whether these techniques are complementary is an\ninteresting question for future work.\n> The effectiveness of the proposed pipeline is not convincing:\n\nCodeIt (full eval) solves 48 tasks compared to 41 tasks in our\nMutation $d_1$ baseline. While this is only a 7 task difference, this represents an increase of 17%. Further, we designed the mutation procedure\nas a strong baseline that makes effective use of the DSL, and tuned its\nhyperparameters \u2013 it is not a naive random search. Although deployment\nit is not a focus of this work, also note that running inference on a never\nseen-before task is hard for the mutation baseline: the best one can do is\nexecute-and-check all found programs. On the other hand, CodeIt distills\nknowledge about the ARC tasks into model parameters, which enables\nrunning a forward pass on the new task. CodeIt\u2019s better performance\ndemonstrates that distilling knowledge about the tasks ultimately trumps\nbrute-force search of a solution, even with a DSL explicitly designed to\nenable search. While CodeIt (full eval) significantly outperforms Ainooson\net al. (2023) and Mirchandani et al. (2023), it also performs in line with\nGPT-4 Gendron et al. (2023). Achieving similar performance here speaks\nto the efficiency of the proposed pipeline since GPT-4 is a much larger\nmodel, with a substantially larger context window, and trained on a much larger amount of data.\n\n>  I strongly encourage the authors to provide the following\nresults: 1. how many solved tasks are in common for CodeIt and the\nmutation baseline? 2. For tasks that both methods solve, are there any\ndifferences between the solutions of each method? Will CodeIt generate a\nmore succinct program?\n\nWe thank the reviewer for this suggestion. We\nprovide an analysis in this spirit in appendix D.2 of the revised paper, and we refer to it\nin the global reply. In short: there are differences between the solutions,\nCodeIt programs tend to be shorter, and some tasks are only solved by\none of the two methods.\n\n> provide quantitative results on how well CodeIt compresses\nprograms like the example in Figure 4\n\nWe carried out this analysis and we provide it Appendix D.3 of the revised paper. We observe\nthat for 80% of tasks where a solution is found, CodeIt finds a shorter solution at a later point during the search.\n\n> expand the pseudocode [of the mutation procedure]\n\nWe have expanded and better outlined the mutation algorithm as requested.\n\n> How do authors determine the number of training epochs during continual learning\n\nWe tuned all of our hypeparameters on a custom validation set as described in Appendix B.2. On this set, we observed that training longer can lead to overfitting and is ultimately not beneficial.\n\n> Figure 4, it\u2019s hard to interpret the figure because the task being considered is not shown and the meaning of each function is unknown\n\nWe updated the figure and figure caption. We hope that the figure is easier to parse now.\n\n> why the example input and output are named $S_0$ and  $S_N$\n\nA CodeIt solution can be thought of as a sequence of $S$(tate) transformations, going from the 0th input grid to the desired output grid. This structure is effectively promoted by the design of the DSL. Since the number of transformation steps is unknown, we use a placeholder $N$ for the output grid, $S_N$\n\n> It is encouraged for authors to include the DSL in the paper, the original write-up of the DSL is not straightforward to find.\n\nWe provide additional info and a link to the original write-up on the DSL in Appendix B.4. As the full DSL would effectively be larger than the current paper+appendix, we chose not to include it in our manuscript, but provide a link to Hodel's GitHub instead.\n\nReferences: \n- [Haluptzok 2022]: \"Language models can teach themselves to program better\", ICLR 2023"
                    },
                    "title": {
                        "value": "Response to reviewer WdyM"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519176344,
                "cdate": 1700519176344,
                "tmdate": 1700524430408,
                "mdate": 1700524430408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7031XpKsHC",
                "forum": "JlSyXwCEIQ",
                "replyto": "AgEiaOpUYL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM"
                ],
                "content": {
                    "comment": {
                        "value": "Thank authors for addressing my concern. I will raise my score to 6. It is very interesting to see that the policy is able to compress programs during the process without any regularization. I encourage the authors to include a discussion about potential explanations for this. Also, it is strongly encouraged to include the example intput/output examples of programs in Table 5 and Table 6 of the appendix as they make the table more interpretable."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732398701,
                "cdate": 1700732398701,
                "tmdate": 1700732398701,
                "mdate": 1700732398701,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]