[
    {
        "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation"
    },
    {
        "review": {
            "id": "JJmEHrFvtv",
            "forum": "1k4yZbbDqX",
            "replyto": "1k4yZbbDqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_mwbp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_mwbp"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to address the limitations of diffusion models in text-to-image (T2I) generation, particularly their slow multi-step sampling process. The authors propose a novel one-step generative model derived from Stable Diffusion (SD) using a method called Rectified Flow. The core of Rectified Flow is its reflow procedure, which improves the coupling between noises and images and facilitates the distillation process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Well organized and clarified.\n2. The contribution is great. Making the conditional diffusions work with one or very few steps will greatly boost the development of diffusion community. \n3. The comparison experiments are carefully and fairly set up, and I appreciate that."
                },
                "weaknesses": {
                    "value": "I have several questions about this paper, and I hope the authors to clearly clarify them.\n\n1. **Storage Overhead**: In my opinion, it seems that either the distillation process or the reflow process actually requires us to create a relatively large (noise, image) pair dataset in advance, which would cause the additional storage overhead.\n\n2. **Intrinsic Difference between the so-called distillation and reflow process**: The distillation step aims to make the model predict the same as target computed by the ODE process of Stable Diffusion at the zero-timestep. While the reflow process seems to only change to make the distillation applied to all possible timesteps. \n\n3. **Noise Scheduler**: The noise scheduler of SD requires a normal diffusion noise scheduler. The reflow requires the \"linear\" (I call it \"linear\" just for convenience) scheduler. Wouldn't that cause trouble? Besides, the SD Unet requires time embedding, what time embedding do you use?\n\n4. Why do you choose to predict \"x1-x0\" instead of \"x1-xt\"? Do you have any considerations about this?"
                },
                "questions": {
                    "value": "I tend to accept the paper, considering its theoretical and technical contributions. However, I have several questions about this paper and hope the authors answer them for me to make the final decision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Reviewer_mwbp",
                        "ICLR.cc/2024/Conference/Submission3230/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573503267,
            "cdate": 1698573503267,
            "tmdate": 1700650338793,
            "mdate": 1700650338793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AoKayCRHaW",
                "forum": "1k4yZbbDqX",
                "replyto": "JJmEHrFvtv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you for the positive review and valuable comments! We will address your concerns below.\n\n**Q#1 (Storage Overhead)**: We admit additional storage space is required to apply our current pipeline. However:\n* Stable Diffusion is trained using more than 1B (text, image) pairs, while InstaFlow only uses 1.6M to achieve the current performance. The overhead is very small.\n* Storage in the training stage is relatively cheap. In contrast, after training, the inference cost of the deployed model usually exceeds the training cost (consider the case of GPT and Midjourney).\n* The distillation step can be further replaced by data-free distillation, like consistency distillation, to save the storage space.\n* Generating the reflow dataset is fast, since only forward inference is involved.\n\nOverall, we argue that the storage overhead is small compared with the original dataset, and the advantage in efficiency brought to the inference stage by our pipeline is significant and overweighs the storage overhead in training.\n\n**Q#2 (Intrinsic Difference between Distillation and Reflow)**: The intrinsic, fundamental difference is the coupling between the noise distribution and the image distribution. \nFor distillation, the one-step generative model is represented by, \n$$ f_{distill}(X_0) = X_0 + v_\\theta(X_0, 0)$$\n\nFor reflow, the continuous-time generative model is represented by,\n$$ f_{flow}(X_0) = X_0 + \\int_0^1 v_\\theta(X_t, t) d t $$\n\nWhen distilling from a existing probability flow, the objective is,\n$$ E_{X_0 \\sim \\pi_0,  X_1 := f_{flow}(X_0)} [D (f_{distill}(X_0), X_1)] $$\nThe coupling between noise and image $(X_0, X_1)$ is determined by the flow $f_{flow}$, and the distilled one-step student tries to imitate these couplings. If the couplings $(X_0, X_1)$ are suboptimal, it could be too difficult for the student distilled model to learn, and make the distillation performance bad. Instead, reflow optimizes the whole velocity field, get new probability flow $f_{flow}^{new}$, and refines the coupling $(X_0, X_1^{new}:=f_{flow}^{new}(X_0))$. The new coupling induced from the new flow is easier to distill for the one-step student model. We explained these subtle differences at the top of Page 6 in our submission.\n\n**Q#3 (Noise Scheduler)**: Thank you for the question! We will answer according to our understanding, but further elaboration on the question is welcomed! Rectified flow trains with a new linear interpolated trajectory instead of the old DDPM noise schedule. In our practice, we found the pre-trained Stable Diffusion neural networks have an amazing ability to adapt to the new trajectory without catastrophic forgetting of the learned contents. At the beginning of the project, just like you, we were expecting potential issues, but it turns out the pre-trained networks have superior adaptivity. We think this is a valuable observation to the community as well, since it makes researchers confident to fine-tune with many other alternative trajectories. We will add that to the paper. \n\n**Q#4 (Time Embedding)**:  We use the same time embedding as the original SD. The only difference is that their time steps are integers while we relax that to continuous values.\n\n**Q#5 (Network Prediction)**: We were following the practice in previous works. In fact, predicting $X_1 - X_0$ is equivalent to $X_1 - X_t$ up to a weight coefficient because,\n$$ X_1 - X_t = X_1 - (t X_1 + (1-t)X_0)= (1-t) X_1 - (1-t) X_0 = (1-t) (X_1 - X_0). $$\nTherefore,\n$$ ||X_1 - X_t - f_\\theta (X_t, t) ||^2 = || (1-t) (X_1 - X_0) - f_\\theta (X_t, t) ||^2 = (1-t)^2|| X_1 - X_0 - \\frac{f_\\theta (X_t, t)}{1-t}||^2. $$\nSo the only difference is the additional time-related weight coefficient $(1-t)^2$ if we parameterize $v_\\theta(X_t, t) = \\frac{f_\\theta (X_t, t)}{1-t}$. The weighting is important as discussed in previous works (e.g., [1]), but we leave that for future investigation as uniform weighting works perfectly in our current practice.\n\n[1] Denoising Diffusion Probabilistic Models, Ho et al."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220196809,
                "cdate": 1700220196809,
                "tmdate": 1700220196809,
                "mdate": 1700220196809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yn4itXEBlq",
                "forum": "1k4yZbbDqX",
                "replyto": "AoKayCRHaW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_mwbp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_mwbp"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback.  I will upgrade the rate to accept. \n\nHere is some additional advice. Nowadays, many personalized models based on SD are trained and widely used. So I think it's better for the author to train some versatile plug-and-play modules like LoRA that transforms normal diffusion model into instaflow-based diffusion model. This will definitely enlarge the impact of this work. \n\nI have seen many reviews in ICLR this year about AIGC. The impact in open-source community makes many papers much easier to get a high rate even with limited novelty."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650294660,
                "cdate": 1700650294660,
                "tmdate": 1700650294660,
                "mdate": 1700650294660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NTFw9qhBG0",
            "forum": "1k4yZbbDqX",
            "replyto": "1k4yZbbDqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_W54n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_W54n"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the InstaFlow, an application that applies the Rectified Flow to Stable Diffusion. The authors implement the rectified flow technique on Stable Diffusion and subsequently distill a one-step diffusion model from the rectified model. The rectified method makes the trajectories of Stable Diffusion straighter, thus making it much easier to distill the multi-step model to fewer or even one-step model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written, and the experiments conducted are both sufficient and convincing. The InstaFlow achieves amazing performance (1-step inference with reasonable quality in approximately 0.09 seconds)."
                },
                "weaknesses": {
                    "value": "While this model demonstrates impressive performance, it does involve a trade-off between inference speed and generation quality. From the supplementary document of InstaFlow, we can still observe various artifacts, which may be inherited from the 2-Rectified Flow (e.g., many faces are already distorted in the rectified model). Nevertheless, as the authors also mentioned, this model can be used for generating quick reviews, and then larger models can be employed for further generating high-quality images."
                },
                "questions": {
                    "value": "I don't have further questions regarding the experiments since they're satisfactory to me.\nHowever, given that this work involves practical applications, I encourage the authors to consider sharing the source code and pretrained models, as this would undoubtedly be of great benefit to the community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Reviewer_W54n"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673838921,
            "cdate": 1698673838921,
            "tmdate": 1699636271233,
            "mdate": 1699636271233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6cvcJ3iAeb",
                "forum": "1k4yZbbDqX",
                "replyto": "NTFw9qhBG0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you for the positive review and valuable comments! We will address your concerns below.\n\n**Q#1 (Performance loss)**: We observe the training of 2-Rectified Flow is not totally converged and most text-to-image models are trained using more than 1B text prompts (InstaFlow only uses 1.6M) and thousands of GPU days (InstaFlow only uses 199). Therefore, we believe InstaFlow can be hugely improved by investing more computation and resources, though that is beyond our reach. The emphasis of our research is that text-conditioned reflow has a decisive impact on distilling Stable Diffusion, and we get state-of-the-art one-step Stable Diffusion from that insight.\n\n**Q#2 (Open-source)**: We will open-source the training/inference codes along with pre-trained checkpoints upon acceptance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219006312,
                "cdate": 1700219006312,
                "tmdate": 1700219063425,
                "mdate": 1700219063425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bDfXvVQgjU",
                "forum": "1k4yZbbDqX",
                "replyto": "6cvcJ3iAeb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_W54n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_W54n"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback. While this paper seems to be an extensive work based on the Rectified Flow, it's not trivial to integrate this technique into the pre-trained Stable Diffusion (SD) and build a fast and practical model by utilizing SD's data prior. \nAfter reading other reviewers' comments and authors' responses, I tend to keep my rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537602247,
                "cdate": 1700537602247,
                "tmdate": 1700537602247,
                "mdate": 1700537602247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5SUbNXhSEd",
            "forum": "1k4yZbbDqX",
            "replyto": "1k4yZbbDqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_ZDe9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_ZDe9"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends a recently introduced framework of rectified flows to the distillation of the coupling learned by the pretrained diffusion model, e.g. Stable Diffusion (SD). \nWhile the recent work [1] reported the results of experiments on unconditioned generation (on CIFAR-10, LSUN, AFHQ, MetFace and CelebA-HQ datasets) as well as on img2img translation, the current submission focuses on text-conditional generation.\nThe paper reconfirms that a \"rectified\" ODE produces an easier target for 1-step distillation. \n\nThe main contribution is the InstaFlow model which essentially is a multi-step pipeline which takes a pretrained SD model as an input and outputs a 1-step generative model. \nIn addition, a novel type of architecture called Stacked U-Net is presented.\nAs the conducted evaluation shows, InstaFlow outperforms recent baselines such as Progressive Distillation of SD and StyleGAN-T in terms of FID and CLIP score.\n\n[1] Liu et al. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In ICLR, 2023."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing style of the paper is extremely clear.\nIt provides a very good introduction to the framework of rectified flows for a reader without deep knowledge of the topic, and overall the manuscript is quite self-contained. \nThe conducted experiments provide a sufficient support for the motivation of the method. I find the evaluation thorough enough.\nThe results achieved in the paper are definitely interesting for the broad community of ML researchers and practitioners due to the achieved combination of required computational resources for the model training and inference and its performance."
                },
                "weaknesses": {
                    "value": "1. First of all, this submission is more an extension of the previous work [1] rather than an independent work. The novelty of the presented ideas is definitely limited: all parts of the pipeline were actually introduced previously, and the submitted work applies the same pipeline to the coupling learned by SD model instead of independent coupling of noise and images. The proposed results are definitely valuable for applications. However, they look more like a technical exercise on top of the [1]. Overall, I find this work too incremental although helpful for practitioners.\n1. While the idea of Stacked U-Net is interesting, the paper lacks the study if this type of architecture is actually better than increasing the depth (or the number of channels) of the conventional U-Net model.\n1. The paper provides the results for latent models only. Taking the empirical nature of this work into account, I suggest adding any of the open cascaded models to the comparison to see, e.g. DeepFloyd IF [2].\n\n[2] https://github.com/deep-floyd/IF"
                },
                "questions": {
                    "value": "1. Please, address the limitations discussed above.\n1. The training pipeline described in the Appendix D, looks pretty complicated. \n    1. Why was it necessary to change the batch size (step 2), and why wasn't more common learning rate tuning applied instead? \n    1. What is the reasoning behind switching from L2 to LPIPS objective (step 4) instead of training with a combination of L2 and perceptual loss from the very beginning of the distillation phase? \n    1. How were switching points for steps 2 and 4 selected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775985155,
            "cdate": 1698775985155,
            "tmdate": 1699636271146,
            "mdate": 1699636271146,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yKEkG0WRIx",
                "forum": "1k4yZbbDqX",
                "replyto": "5SUbNXhSEd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you for the review, comments and valuable questions! We appreciate that the reviewer recognizes the contribution of our paper to the broad ML community. We will address your concerns below. Please kindly raise the rating if our response makes sense to you.\n\n**Q#1 (Novelty)**:  Besides the novelty/difference/interesting phenomenon mentioned in General Response #1, we would like to claim that scaling-up is a resource-consuming process with much back-and-forth, and successful scaling-up is indispensable for the modern machine learning community to acknowledge the value of a specific algorithmic framework. In our work, we use **new large-scale models, new large-scale datasets and new text-conditioned rectified flow framework**, which are not presented in the previous works. Moreover, we extensively provide new empirical comparison and state-of-the-art results on large-scale text-to-image generation, which is a significant improvement over the previous work that has experiments only on toy dataset CIFAR10. The framework and the results, in our opinion, are of great interest to the ICLR community. We will open-source the codes and pre-trained models to further advance the research of generative models.\n\n**Q#2 (Network Structure)**: Thank you for proposing the alternatives! We discuss them below:\n* *Increasing Depth*: Stacked U-Net is actually increasing the depth of the original U-Net by adding more layers. Please refer to Figure 12 c.\n* *Increasing the number of channels*: In our early experiments, we tried to double the number of channels in each layer of the original U-Net, and found no obvious improvement over the original U-Net. This suggests the importance of increasing depth. We will add discussion of this result in the future versions.\n\n**Q#3 (Deep Floyd)**: Thank you for the suggestions! We are definitely interested in applying our text-conditioned rectified flow to more advanced models like DeepFloyd and SDXL. However, for this work, our focus is to fairly compare different distillation methods to provide scientific insights on using text-conditioned reflow before distillation in large-scale text-to-image generation. Limited by the computational resources accessible to us, we leave applications to these advanced models for future development.\n\n**Q#4 (Training Pipeline)**: Thank you for your questions! The pipeline is a little bit complicated because we are limited by resources. While the original Stable Diffusion training adopts 32 * 8 A100 GPUs, we only have 8 A100 GPUs available. Therefore, the training pipeline is a mixture of empirical observations and trade-off between training time and training quality. Even for the original Stable Diffusion, their training pipeline is empirically divided into multiple stages (https://huggingface.co/runwayml/stable-diffusion-v1-5) due to the complexity of training large-scale models. For your questions, we will address the individual concerns below:\n* *Large batch size vs. small learning rate*: We started from training with a batch size of 64, and found the loss converged after around 70,000 iterations (step 1). Then we actually tried both increasing batch size and decreasing learning rate, and found increasing batch size can further reduce the loss while decreasing learning rate has no obvious effect. Increasing batch size is also a common practice when training modern large-scale models, as adopted in GPT-3, PaLM, GLM-130B, etc..\n\n* *Distillation loss*: Optimization on the L2 loss in the latent space of VAE is faster. It also consumes less GPU memory and thus allows larger batch size, because LPIPS loss is computed over the image and the gradient needs to be back-propogated through the VAE decoder. However, optimizing the LPIPS loss has an instant improvement on the generated image quality of the distilled model, so step 4 is indispensable. We actually tried the combination of L2 and LPIPS loss as you suggested, and found the visual quality is worse than using LPIPS loss only. Overall, step 3 is an efficient warm-up stage for step 4 given limited resources.\n\n* *Switching point*: As discussed in sub-question 1, we switch from step 1 to step 2 after observing the (moving average) loss converged with small batch size. In step 2, we train the model up to our computational budget. The loss for training 2-Rectified Flow with a batch size of 1024 is not converged and may improve further given longer training time. We switch from step 3 to step 4 after the quality of the generated images saturates with L2 loss.\n\nIf there are enough resources, the training pipeline could be simplified to step 2+step 4 only (large batch size + LPIPS loss for distillation), and the training duration can be expanded for further improvement. The training pipeline could be suboptimal due to resource limitation but will be a good starting point for future research with our open-source."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218370946,
                "cdate": 1700218370946,
                "tmdate": 1700218370946,
                "mdate": 1700218370946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zLx57nB8wZ",
                "forum": "1k4yZbbDqX",
                "replyto": "yKEkG0WRIx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_ZDe9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_ZDe9"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their feedback. I encourage the authors to include the discussion on the training strategy (Q#4) to the revised manuscript"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488397094,
                "cdate": 1700488397094,
                "tmdate": 1700488397094,
                "mdate": 1700488397094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YDsyOMfoht",
            "forum": "1k4yZbbDqX",
            "replyto": "1k4yZbbDqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt"
            ],
            "content": {
                "summary": {
                    "value": "This paper successfully demonstrates the use of RECTIFIED FLOW to linearize the model's sampling trajectory, followed by distillation to enhance the sampling speed of the ODE model. It proposes a method for distilling Text-Conditioned flow models, showcasing a variety of ablation studies and results across multiple settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The results of this paper are truly captivating. It manages to generate images of impressive quality with just one or two steps. The quality showcased in the figures is highly satisfying. In addition, the paper provides a detailed account of various experiments and the corresponding performance metrics, which adds greatly to its value."
                },
                "weaknesses": {
                    "value": "While the paper demonstrates impressive results, it appears to be a straightforward application of RECTIFIED FLOW. I was unable to discern any clear novelty in the algorithms or methods presented. If I'm wrong please kindly let me know the difference.\n\nThe model that claims to operate in 1 step actually resembles a configuration of two UNets linked together, and thus feels closer to a 2-step process. Additionally, when the refined model from SDXL is not applied, the results show a noticeable degradation in high-frequency details."
                },
                "questions": {
                    "value": "Given that RECTIFIED FLOW is trained based on its own trajectory, are there any issues that arise from this approach?\n\nMethods like DDIM inversion also seem like they could be applicable in a flow-based context. I am curious about the results in cases where a small number of steps, close to 2, are used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Additionally, I would like to highlight the importance of discussing the ethical implications of the presented work in the paper."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt",
                        "ICLR.cc/2024/Conference/Submission3230/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837717689,
            "cdate": 1698837717689,
            "tmdate": 1700532324174,
            "mdate": 1700532324174,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kSgkUIttfr",
                "forum": "1k4yZbbDqX",
                "replyto": "YDsyOMfoht",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Thank you for the review, comments and valuable questions! We appreciate that the reviewer recognizes the excellent results of our paper. We will address your concerns below. Please kindly raise the rating if our response makes sense to you.\n\n**Q#1(Novelty)**: Besides the novelty/difference/interesting phenomenon mentioned in General Response #1, we would like to claim that scaling-up is never straightforward. Rather, it is a resource-consuming process with much back-and-forth. However, despite the difficulties, successful scaling-up is indispensable for the modern machine learning community to acknowledge the value of a specific algorithmic framework. The **new, large-scale, extensive empirical comparison and state-of-the-art generation results** in our work, using **the novel text-conditioned reflow framework**, sets a solid foundation for the value of the methodologies / theories and inspires practitioners / methodologists to improve the existing algorithms in the future. This, in our opinion, is of great interest to the ICLR community. We will open-source the codes and pre-trained models to further advance the research of generative models.\n\n**Q#2 (Network Architecture)**: Please refer to General response #2. Most of the results are generated by InstaFlow-0.9B using the original U-Net as SD. \n\n**Q#3 (Performance Loss)**: We observe the training of 2-Rectified Flow is not totally converged and most text-to-image models are trained using more than 1B text prompts (InstaFlow only uses 1.6M) and thousands of GPU days (InstaFlow only uses 199).  Therefore, we believe InstaFlow can be hugely improved by investing more computation and resources, though that is beyond our reach. The emphasis of our research is that text-conditioned reflow has a decisive impact on distilling Stable Diffusion, and we get state-of-the-art one-step Stable Diffusion from that insight. \n\n**Q #4 (Flow Trajectory)**: Thank you for the question! We will answer according to our understanding, but further elaboration on the question is welcomed! InstaFlow abandons the original DDPM trajectory of the Stable Diffusion, and re-trains a new linear interpolated trajectory. In our practice, we found the pre-trained Stable Diffusion neural networks have an amazing ability to adapt to the new trajectory without catastrophic forgetting of the learned contents. At the beginning of the project, just like you, we were expecting potential issues, but it turns out the pre-trained networks have superior adaptivity. We think this is a valuable observation to the community as well, since it makes researchers confident to fine-tune with many other alternative trajectories. We will add that to the paper. Thank you for asking again.\n\n**Q #5 (DDIM Inversion)**: Thank you for the suggestion! We put additional results in Figure 26 of the revised Appendix. 2-Rectified Flow can efficiently encode the image to the latent noise space with even 1 step.\n\n**Q #6 (Ethical Implications)**: Thank you for the suggestion! We have added a discussion of societal impact in the revised version of the paper. \n\nFurther discussion are welcomed!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216258491,
                "cdate": 1700216258491,
                "tmdate": 1700216258491,
                "mdate": 1700216258491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZJovzZhS5P",
                "forum": "1k4yZbbDqX",
                "replyto": "kSgkUIttfr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for authors.\n\nI have resolved most of my initial queries after reading the response and revisiting your paper. However, a new question has emerged regarding the classifier-free guidance (CFG) aspect. Authors applied the original CFG formula directly to the velocity field v in its text-guided version. Although this seems plausible to me and appears to not significantly conflict with the existing rectified approach, I'm curious about the underlying intuition. Was this an experimental decision that fortunately yielded positive results, or was there a specific rationale behind it?\n\nAdditionally, when employing CFG, the approach relies on conditional distributions of both null and context T. Here, I wonder if the marginal property always ensures successful outcomes, or if this is more of an empirical success lacking theoretical backing?\n\nIn the context of the rectified paper and its discussion on 2-rectified flow, there's an experimental analysis of the CFG parameter alpha. Do you have any additional experimental results or theoretical insights regarding the guidance scale alpha and the recursive rectified flow (k-step)?\n\nAs a reviewer, I am keen to understand these aspects in greater depth from the authors' perspective."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367671406,
                "cdate": 1700367671406,
                "tmdate": 1700367671406,
                "mdate": 1700367671406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QEWRMlkpHZ",
                "forum": "1k4yZbbDqX",
                "replyto": "I32rTe1dtB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3230/Reviewer_VVpt"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the effort and contributions of the authors. \n\nI agree with the author's argument that this paper has demonstrated various possibilities that arise when expanding existing techniques to a large scale through empirical experiments. While I'm not entirely certain about the novelty of the algorithm and the training approach, this paper has provided ample experimentation and is considered valuable.\n\nI have revised my rating from 5 to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532307838,
                "cdate": 1700532307838,
                "tmdate": 1700532307838,
                "mdate": 1700532307838,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]