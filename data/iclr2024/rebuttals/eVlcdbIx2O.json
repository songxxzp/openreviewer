[
    {
        "title": "A Generative Model for Game Theory with Flow Equilibrium"
    },
    {
        "review": {
            "id": "wG2vdAztd7",
            "forum": "eVlcdbIx2O",
            "replyto": "eVlcdbIx2O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7790/Reviewer_TMg4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7790/Reviewer_TMg4"
            ],
            "content": {
                "summary": {
                    "value": "This paper frames learning a Nash equilibrium of stochastic game as minimizing the KL divergence between a generative model of agent play and the reward distribution of all agents over all end states at equilibrium. The generative model of agent play naturally includes models of each agent $i$'s individual policy as well as each agent $i$'s model of the remaining players' policies. The authors propose a variational policy gradient (VPG) update to learn this generative model (as well as a variational actor-critic method for more complex settings)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes an interesting approach to equilibrium approximation in stochastic games. The idea of repurposing generative models for game theory is intriguing. Mapping the Nash equilibrium condition to a flow equilibrium and then attempting to minimize the distance between the visiting policy and the reward distribution is interesting as well."
                },
                "weaknesses": {
                    "value": "I had a very hard time parsing some of the notation and derivations in this paper. I want to like this paper, but it appears to have been hastily written. Note it's also over the page limit. I have included more detailed questions below."
                },
                "questions": {
                    "value": "- Theorem 4.2: You state that you define a mapping $\\Gamma$ in Appendix A.1, but don't *actually* define $\\Gamma$ anywhere. This is too sloppy given how central it is to your paper.\n- Theorem 4.3: $\\delta$ is defined with a $\\max$ in the second term. What exactly is the $\\max$ over? Trajectories? It's hard for me to understand the usefulness of this result if $\\delta$ can be arbitrarily small (implying $\\epsilon$ could be quite large). Note that $Ret_{\\max}$ appears in Theorem 4.3 but is missing from the final claim in Appendix A.2 (equation 16).\n- Equation 2: Have you defined $P$ in terms of $\\pi$ explicitly anywhere? What about $R^i$ in terms of a $\\gamma$ and $r^i$? Same with $Z$? It's difficult to follow the derivation here without knowing those terms. Why does a $\\gamma^t$ appear in front of the entropy term $H$? Did you define $H$?\n- Proposition 4.5: Please prove this result. I didn't see anything in the appendix to support it.\n- Above equation 9, you say you use a sample-based method to estimate the expectation of the exponential of the Q values. Wouldn't Jensen's inequality say that the expectation of the exponential of the Q-values are an overestimate of the true value? How do you deal with this?\n- What is equation 9 for? It seems to appear without any description in the text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676448408,
            "cdate": 1698676448408,
            "tmdate": 1699636952097,
            "mdate": 1699636952097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iYSsDHW96A",
            "forum": "eVlcdbIx2O",
            "replyto": "eVlcdbIx2O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7790/Reviewer_RfBQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7790/Reviewer_RfBQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach for addressing Markov decision processes (MDPs) involving multiple agents. The authors propose a graphical model representation of the game and define a concept called flow equilibrium. They establish a connection between flow equilibrium and the Nash equilibrium in Markov games and introduce a training criterion for solving the flow equilibrium. Leveraging this foundation, the authors propose variational policy gradient (VPG) and generative policy inference (GPI) methods for solving the Markov game and provide theoretical proofs of convergence for Markov potential games. The effectiveness of the approach is demonstrated through experiments on synthetic datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents an interesting link between the proposed flow equilibrium and the Nash equilibrium in Markov games."
                },
                "weaknesses": {
                    "value": "The paper is significantly hampered by its writing quality, which obscures the presentation and makes it challenging to assess the contributions. Specific issues include:\n\n1. In Definition 4.1, $\\pi^i$ is defined but is not utilized in subsequent equations, leading to confusion.\n2. The proportionality $P\\_T(x\\_T; \\pi^i, \\pi^{-i}) \\propto R^i(x\\_T; \\pi)$ is perplexing. $P\\_T(x\\_T; \\pi) \\propto R^i(x\\_T; \\pi)$ or $P\\_T(x\\_T; \\pi^i, \\pi^{-i}) \\propto R^i(x\\_T; \\pi^i, \\pi^{-i})$ would be better.\n3. Theorem 4.2 confusingly states $R(x) = \\{R^i(x\\_T; \\pi)\\}$ when $R(x)$ should only be defined for terminating vertices.\n4. The claim in Section 4.2 that \"the return function $R(x)$ depends solely on the current vertex $x$\" is misleading since $R(x)$ is previously defined only at terminating vertices.\n5. Theorem 4.3 lacks clarity on what $\\mathcal{X}^i$ represents, and the definition of GMG implies that all agents share the same vertex set.\n6. In Theorem 4.3, there should be an assessment of $\\epsilon$'s magnitude since a large $\\epsilon$-NE could significantly deviate from a true NE.\n7. Equation (2) introduces $\\mathbb{E}\\_{\\pi^i, \\pi^{-i}}[R^i(x\\_T)]$ which is inconsistent with the notation $R^i(x\\_T; \\pi^i, \\pi^{-i})$ used in Section 4.1.\n8. The second term of Equation (2) is confusing as it attempts to calculate the KL divergence between distributions over different domains (trajectories vs terminating vertices).\n9. The inclusion of $\\rho$ in the third term of Equation (2) without it appearing in the first two terms is puzzling, raising questions about the derivation of the equation.\n10. Definition 4.4 introduces $\\hat{\\pi}$ without an adequate explanation.\n11. The derivations of Equation (3) and the subsequent equation between Equations (3) and (4) from Equation (2) are unclear.\n\nAdditional concerns extend beyond the mathematical content:\n1. The paper does not elucidate the relationship between the VPG and GPI algorithms.\n2. While the authors draw parallels between generative models and game theory, the connection to generative models within the proposed method is not clear.\n3. The proposed methods are purported to be applicable to neural networks, yet no experimental validation is provided to substantiate their efficacy in such contexts.\n\nIn light of these points, the paper needs substantial revision to clarify its theoretical contributions and to provide a more robust empirical validation."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699435886207,
            "cdate": 1699435886207,
            "tmdate": 1699636951976,
            "mdate": 1699636951976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "8wQTqM9lVS",
            "forum": "eVlcdbIx2O",
            "replyto": "eVlcdbIx2O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7790/Reviewer_39sv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7790/Reviewer_39sv"
            ],
            "content": {
                "summary": {
                    "value": "The aim of this paper is to connect game theoretic views with generative models, by introducing a multi-agent decision framework that eventually generates the data. This is done by defining the flow equilibrium, which is a new solution concept where all $P_T$ values are in proportion to $R$. The manuscript further shows that such a solution concept must exist, and characterizes that they resemble logit quantal response equilibrium. Based on the concept, the manuscript proposes variational policy gradient, which finds the best response of the aforementioned opponent model. It shows that this variant of policy gradient will converge in Markov potential games. Some simple experiments follow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper connects game theory and generative models, and propose a new solution concept of flow equilibrium. This idea is relatively new.\n2. They show one use case of the solution concept, which is to model the opponent in multi-agent decision tasks. Such an opponent modeling is compatible with multi-agent learning methods such as policy gradient.\n3. Some statements and experiments are provided."
                },
                "weaknesses": {
                    "value": "I would expect such a solution concept to be a bit more \"useful\" to be more beneficial to the community. At the moment its use case is to improve the opponent modeling part, which plausibly introduces some marginal improvement to multi-agent decision algorithms (which agrees with the experiments)."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699525767039,
            "cdate": 1699525767039,
            "tmdate": 1699636951844,
            "mdate": 1699636951844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]