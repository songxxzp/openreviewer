[
    {
        "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks"
    },
    {
        "review": {
            "id": "sMTF8F76Sq",
            "forum": "GOt2kP383R",
            "replyto": "GOt2kP383R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_v9HR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_v9HR"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores how to overcome the distribution mismatch in quantizing SR. Specifically, the authors intuitively reduce the distribution mismatch by directly regularizing the variance of features when the gradients of variance regularization are cooperative with that of reconstruction. In addition, the authors introduce selective distribution offsets to layers with a significant mismatch, which selectively scales or shifts channel-wise features. The extensive experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tI enjoy the analyses of the distribution mismatch and conflict of the existing SR network. These observations are non-trivial and are critical for practical quantizing SR. \n2.\tThe proposed method is novel and reasonable. The idea is simple yet effective. The authors have comprehensively demonstrated the proposed methods from the perspective of optimization.\n3.\tThe paper is well-written and is easy to follow."
                },
                "weaknesses": {
                    "value": "1.\tIt would be better if the authors can provide more details about Gradient conflict ratio.\n\n2.\tIn addition to the variance regularization, the selective distribution offsets also employ a learnable parameter about the standard deviation of the features. It would be better if the author could provide more discussions about the relation of these two terms. \n\n3.\tDoes the selective distribution offsets are learned on the features not processed in cooperative variance regularization?"
                },
                "questions": {
                    "value": "See the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739724517,
            "cdate": 1698739724517,
            "tmdate": 1699636289637,
            "mdate": 1699636289637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0S8tVtbOcA",
                "forum": "GOt2kP383R",
                "replyto": "sMTF8F76Sq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v9HR"
                    },
                    "comment": {
                        "value": "We thank Reviewer v9HR for spending time reviewing and providing insightful feedback. We are encouraged by the positive comments on the novel method, the simple yet effective idea, and easy-to-follow writing. Below is the detailed response to each question.\n\n\n**Q1: More details about the gradient conflict ratio** \\\n**A1:** \nThanks for your helpful suggestion. Upon your advice, we updated the manuscript to add more details about the gradient conflict ratio.\nIn the manuscript, we detailed how the gradient conflict ratio is measured: the ratio of parameters that the sign of gradient from reconstruction loss and that of regularization is different.\nAlso, we additionally emphasized that, according to Figure 2 of the main paper, the conflict ratio decreases minimally during training, which indicates that variance regularization can hinder the reconstruction loss throughout training.\nMoreover, instead of disregarding the gradient of the regularization when the two gradient conflicts, we added analysis for weighting the regularization loss by the degree of conflict between two losses (measured by the cosine similarity).\nAccording to the results, it was slightly better to disregard the variance regularization loss when it is not cooperative.\n\nTable R3: **Analysis on the degree of gradient conflict** on EDSR x4 (2-bit). cos() measures cosine similarity.\n| Method    | Set5  | Set14 | B100  |Urban100|\n|:-------------|:-----:|:-----:|:-----:|:------:|\n| VR disregarded when $\\nabla_\\theta L_R \\cdot \\nabla_\\theta L_V <0$ | 31.49 | 28.12 | 27.26 | 25.15  |\n| VR weighted with cos($\\nabla_\\theta L_R, \\nabla_\\theta L_V$)    | 31.45 | 28.12 | 27.25 | 25.14  |\n\n\n\\\n**Q2: More discussion about the relation between variance regularization and selective distribution offsets** \\\n**A2:** \nThank you for your advice. As the reviewer mentioned, variance regularization and selective distribution offsets both function to reduce the distribution mismatch (feature-wise std).\nEach component largely reduces the mismatch and results in higher accuracy over the baseline.\nWhen the two components are jointly used, the accuracy increases compared to using each component separately, although the increasing amount is relatively minor.\nThis indicates that there is, to some extent, an overlapping effect of selective offsets and variance regularization. Nevertheless, both components contribute to reducing the mismatch, resulting in a further accurate quantized SR network.\nWe added the discussion to the manuscript.\n\n\n\n**Q3: Do the selective distribution offsets learned on the features not processed in cooperative variance regularization?** \\\n**A3:** \nWe would like to clarify that selective distribution offsets are also processed through cooperative variance regularization. The parameters of the quantized network (including the selective distribution offsets) are end-to-end trained. We updated the manuscript followingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971645060,
                "cdate": 1699971645060,
                "tmdate": 1699971645060,
                "mdate": 1699971645060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vT1jXdxqzC",
                "forum": "GOt2kP383R",
                "replyto": "0S8tVtbOcA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3388/Reviewer_v9HR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3388/Reviewer_v9HR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your replies. I have no further questions and my concerns have been addressed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715980880,
                "cdate": 1700715980880,
                "tmdate": 1700715980880,
                "mdate": 1700715980880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iwIynjvcBf",
            "forum": "GOt2kP383R",
            "replyto": "GOt2kP383R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_p2GR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_p2GR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new quantization-aware training technique that relieves the mismatch problem via distribution optimization. Specifically, the authors use variance regularization loss, cooperative variance regularization and selective distribution offsets to reduce such mismatch. Experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a quantization framework to address the distribution mismatch problem in SR networks without dynamic modules. The proposed method achieves state-of-the-art performance with similar or less computations."
                },
                "weaknesses": {
                    "value": "The experiment section can be improved. Please refer to the details below."
                },
                "questions": {
                    "value": "1. The main motivation is that feature distributions of SR networks are significantly divergent for each channel or input image. In Figure 1, does the distribution mismatch only occur in the SR network? Does such a distribution mismatch occur in other networks? In addition, could you show the distribution after quantization?\n\n2. In the experiments, the authors mainly use EDSR, EDN and SRResNet. However, these methods are very old. Could you compare the new SOTA SR networks, e.g., SwinIR?\n\n3. In Table 1, for Bit=2, the results of EDSR-DAQ do not correspond to the original results of the DAQ paper. Could you discuss these results?\n\n4. The experiments only address the scale of 4. It would be better to conduct more experiments on other scales and put the results in supplementary.\n\n5. In the ablation study, could you conduct an experiment with only the cooperative variance regularization? In addition, the network with only the selective distribution offsets is comparable with Coop.+Var. Reg.+Sel. Off. This result demonstrates that Coop.+Var. Reg. are not  important."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Reviewer_p2GR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752518876,
            "cdate": 1698752518876,
            "tmdate": 1699636289537,
            "mdate": 1699636289537,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jJGIO3gNuB",
                "forum": "GOt2kP383R",
                "replyto": "iwIynjvcBf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer p2GR"
                    },
                    "comment": {
                        "value": "We thank reviewer p2GR for spending time reviewing and providing detailed feedback. We are encouraged that the reviewer recognized our state-of-the-art performance with similar or less computations. We address the reviewer's concerns and questions below in detail:\n\n\n**Q1-1: Does the distribution mismatch occur only in SR or other networks?** \\\n**A1-1:** \nThe distribution mismatch problem is especially severe for SR networks. For example, a classification network (ResNet20) shows a much more minor image-wise and channel-wise distribution mismatch compared to SR networks (EDSR, RDN). We measure the average variance of features on DIV2K validation set for SR networks and ImageNet validation set for the classification network. Thanks for your comments, we added such analyses to the supplementary material.\n\nTable R2: **Average feature mismatch**.\n| Model | Image-wise Variance  | Chanel-wise Variance |\n| ---------|:-----:|:-----:|\n| EDSR (x4) | 15.08 | 40.29 |\n| RDN (x4) | 6.40 | 58.14  |\n| ResNet-20 | 0.04 | 0.09  |\n\n\\\n**Q1-2: The distribution after quantization** \\\n**A1-2:** \nThanks for your suggestion. Based on your comments, we added new figures to the supplementary material of distribution after quantization and the distribution after using our framework, ODM. The distribution of the ODM-applied network shows that ODM is effective in reducing the distribution mismatch.\n\n\n**Q2: Comparison with more recent models (e.g., SwinIR)** \\\n**A2:** \nPlease kindly refer to our supplementary material (Table S1), in which we reported results on more recent models SwinIR [A] and CARN [B]. According to the results, our method achieves consistent gain over existing methods also on recent models.\n\n\n**Q3: EDSR-DAQ does not correspond to the original results of the DAQ paper** \\\n**A3:** \nWe note that the result reported in the DAQ paper uses the EDSR backbone of 32 residual blocks (of 256 channel dimensions). In comparison, we use the EDSR-baseline backbone that consists of 16 residual blocks (of 64 channel dimensions), following PAMS and DDTB. We have reproduced EDSR-DAQ directly by the official codebase, which also matches the accuracy reported for EDSR-baseline in the official GitHub repository. \n\n\n\n**Q4: Experiments only on a scale of 4** \\\n**A4:** \nAlso, kindly refer to our supplementary material (Table S2) for the results on scale 2 SR models. The results show that our method is also beneficial for scale 2 SR models.\n\n\n**Q5-1: Ablation only using the cooperative variance regularization** \\\n**A5-1:** \nIf we understood correctly, Table 5 (b) presents the result of only using cooperative variance regularization.\n\n\n\n\n**Q5-2: Ablation indicates that the coop. var. reg is not important** \\\n**A5-2:** \nWe would like to emphasize that, compared to the baseline, using cooperative variance regularization (Coop. Var. Reg.) brings 0.8 dB gain in PSNR, which demonstrates that Coop. Var. Reg is important.\nThe two main components we propose (Coop. Var. Reg. and Sel. Off.) both serve to reduce the distribution mismatch in SR; using two components together gives a relatively minor gain compared to the gain from each component (0.09 dB / 0.18 dB).\nHowever, this does not mean that each component is useless; each brings 0.8 dB / 0.9 dB gain over the baseline. It rather hints that the two attributes can have overlapping effects for reducing distribution mismatch. \n\n\\\n[A] SwinIR: Image Restoration Using Swin Transformer, CVPR2021. \\\n[B] Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network, ECCV2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971554700,
                "cdate": 1699971554700,
                "tmdate": 1699971554700,
                "mdate": 1699971554700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ie8RcF8qwG",
            "forum": "GOt2kP383R",
            "replyto": "GOt2kP383R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_Yy9i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_Yy9i"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to deal with the inherent distribution mismatch of the features in quantizing image super-resolution. To this end, the paper introduces a variance regularization loss, which can cooperate well with the reconstruction loss by computing the signs of gradients. Furthermore, the paper proposes to apply shifting/scaling offsets to layers with a large mean/deviation. The proposed quantization framework ODM is evaluated on three representative SR models in the main paper. ODM exhibits better performance over competitors using a small storage size and low BitOPs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes the variance regularization loss, which can regularize the distribution diversity beforehand and cooperate well with the reconstruction loss. The selective distribution offsets further reduce the variance distribution. The proposed methods are based on analyses and observations. The experimental results are competitive by achieving high performance and reducing computation overhead. The writing is easy to follow."
                },
                "weaknesses": {
                    "value": "The comparisons seem not fair in terms of training epochs, and the proposed method does not reduce BitOPs compared to the previous method, i.e., DDTB, which contradicts the motivation of the method."
                },
                "questions": {
                    "value": "1. The authors reproduce the results of other methods using the same training epochs. Does the number of epochs influent the performance of other methods? Why not use their optimal training epochs for comparisons?\n2. Compared to competitors, the authors use seemingly complicated methods to address the quantizing problem beforehand. Will the proposed method increase the training time?\n3. We can observe from Tab. 4 that the proposed method achieves a better tradeoff between the storage size and BitOPs, to be precise. What makes the ODM need higher storage space than DAQ.\n4. The verb is missing in the sentence after Eq.4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Reviewer_Yy9i"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699497660460,
            "cdate": 1699497660460,
            "tmdate": 1699636289476,
            "mdate": 1699636289476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tZoKcwY0fh",
                "forum": "GOt2kP383R",
                "replyto": "Ie8RcF8qwG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yy9i"
                    },
                    "comment": {
                        "value": "We thank Reviewer Yy9i for the time spent reviewing and providing constructive feedback. We are encouraged by the positive comments on competitive performance and easy-to-follow writing. We address all raised comments and questions below in detail.\n\n\n**Q1-1: Fair comparison regarding training epochs** \\\n**A1-1:** \nWe follow DDTB [A] to use 60 epochs, and for a fair comparison, we reproduce PAMS [B] and DAQ [C] for 60 epochs using the official codebase. \n\n**Q1-2: Influence on the number of epochs** \\\n**A1-2:** \nIf we set the training epochs to 300 (the epochs reported in DAQ), the accuracies of the overall methods increase. However, we note that the order is preserved, and ours outperforms other methods. \n\nTable R1: **Different training epochs** on 2-bit EDSR for Urban100 PSNR.\n| Epochs | EDSR-PAMS  | EDSR-DAQ | EDSR-DDTB | EDSR-ODM (Ours) |\n| -------------------------------|:-----:|:-----:|:-----:|:------:|\n| 60   | 23.72 | 24.88 | 24.82 | 25.15  |\n| 300 | 24.09 | 24.90 | 25.01 | 25.51  |\n\n\\\n**Q2: Comparison of training time** \\\n**A2:** \nUsing a single RTX 2080Ti GPU, the calibration for the selective offsets takes $\\sim$30 seconds. The overall training time for ODM is $\\sim$4.0 hours for quantizing EDSR, which is no less than that of DDTB ($\\sim$4.0 hours). \n\n\n**Q3: What makes ODM need higher storage space than DAQ?** \\\n**A3:** \nThe additional storage space of ODM originates from the selective offset parameters. However, the storage overhead is relatively minimal compared to the significant bitOPs overhead of DAQ: as DAQ adaptively adjusts the quantization range parameters by calculating mean and variance at test-time, the bitOPs overhead is substantial.\n\n**Q4: Typo after Eq. (4)** \\\n**A4:** \nThank you for pointing out. We have revised our manuscript.\n\n**Q5: ODM does not reduce BitOPs compared to DDTB** \\\n**A5:** \nWe would like to note that, although the bitOPs of ODM is similar to that of DDTB, ODM occupies a smaller storage size, which is also an important computational cost benefit.\n\n\n\\\n[A] Dynamic Dual Trainable Bounds for Ultra-Low Precision Super-Resolution Networks, ECCV 2022. \\\n[B] PAMS: Quantized Super-Resolution via Parameterized Max Scale, ECCV 2020. \\\n[C] DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks, WACV2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971304583,
                "cdate": 1699971304583,
                "tmdate": 1699971304583,
                "mdate": 1699971304583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0N9alBvit4",
            "forum": "GOt2kP383R",
            "replyto": "GOt2kP383R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_Bh7W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3388/Reviewer_Bh7W"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript focuses on quantizing super-resolution (SR) networks. Authors discover that the difficulty of quantizing SR networks is because of the fluctuation in activation distribution, which is significantly different in each channel. They propose ODM, a QAT framework to overcome the distribution mismatch problem by regularizing the variance in features using a new loss term. It mainly includes two contributions. First, it regularizes the gradients to ensure the losses are not in conflict. Second, it introduces a channel-wise offset that reduces the distribution mismatch."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation is clear and strong. The authors design a new loss term and channel-wise quantization factors to regularize the activation to make the network easy to quantize. The plug-in module can be introduced and gain improvements in other networks and tasks that has variance feature distribution. \n\nThe experimental results are comprehensive. The proposed methods show consistent improvements on various SR networks and datasets (But the improvements are not that significant). \n\nThe figures and illustrations are easy to understand and targeted to the problem. And the overall writing is easy to follow."
                },
                "weaknesses": {
                    "value": "There are lots of quantization methods that adopt channel-wise scaling and offsets. Although the channel-wise feature variance seems to be more severe in super-resolution networks, the channel-wise quantization factor is not novel. \n\nThe paper mainly solves one problem with two strategies. I wonder if they are repeated. The regularization loss makes the activation variance smaller in each channel, which is easy to quantize. And the channel-wise quantization factor quantizes the features in a channel-wise manner that will not be affected by the value differences between channels. The experimental results in the ablation study also show that the two methods are not orthogonal. Combining the two methods together can only outperform a little compared with solely using one of them. \n\nThe proposed methods may be too simple and need more insightful analysis and discoveries."
                },
                "questions": {
                    "value": "x_i in Eq. (2) denotes the feature (activation). However, I wonder if it will lead to the homogenization of features since they are expected to have a low standard deviation. Did the authors try to minimize the difference in the mean of each channel?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3388/Reviewer_Bh7W"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3388/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700200036954,
            "cdate": 1700200036954,
            "tmdate": 1700200036954,
            "mdate": 1700200036954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iC90HM9JE5",
                "forum": "GOt2kP383R",
                "replyto": "0N9alBvit4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Bh7W"
                    },
                    "comment": {
                        "value": "We thank Reviewer Bh7W for contributing time reviewing and providing valuable feedback. We are encouraged that the reviewer found our work strongly motivated, easy to follow, and comprehensively experimented with. More importantly, we address the reviewer's concerns and questions below in detail:\n\n\n**Q1-1: Novelty in channel-wise quantization factor** \\\n**A1-1:**\nAs the reviewer also pointed out, there are works on SR quantization that adopt channel-wise quantization functions (e.g., DAQ [A]). However, we would like to note that simply adopting channel-wise scaling and offsets incur substantial computational overhead. Instead, our method adopts only scaling for particular layers or shifting for certain layers and even none for some layers, which effectively reduces the computation overhead (e.g., the bitOPs overhead is $\\sim\\times 100$ smaller compared to DAQ [A]). We find our *selective* approach for offsets to be helpful for achieving accurate quantization accuracy without incurring significant overhead.\n\n**Q1-2: Channel-wise quantization will not be affected by the value differences between channels** \\\n**A1-2:** \nWe would like to clarify that we utilize layer-wise quantization factor for each layer. For a few layers, we shift or scale the input of the convolutional layer with channel-wise offsets, which are then quantized with a layer-wise quantization factor. Thus, the quantization accuracy is affected by value differences in channels.\n\n\n**Q2: Repeated strategies for one problem** \\\n**A2:** \nAs the reviewer mentioned, both variance regularization and selective offset strategy alleviate the distribution mismatch problem in SR. Using two components together gives a relatively minor gain (0.09 dB / 0.18 dB) compared to the gain from each component (0.8 dB / 0.9 dB). This indicates that there is, to some extent, an overlapping effect of the two strategies. Nevertheless, both components reduce the mismatch, resulting in a further accurate quantized SR network. We will add further discussion of the overlap to our supplementary material. \n\n\n\n**Q3: More insightful analysis and discoveries** \\\n**A3:** \nThanks for your suggestion, we added deeper analyses on the distribution mismatch problem in Section S5.1 of the supplementary material: how solving such a problem is especially crucial for SR networks and how our method effectively reduces the mismatch. Also, as suggested by another reviewer, we added analysis on the gradient conflict problem in Section S5.2. Please kindly refer to the supplementary material for more details.\n\n\n**Q4: Will variance regularization lead to homogenization of features?** \\\n**A4:** \nDirectly using variance regularization can lead to the homogenization of features, which can trigger a PSNR loss, as shown in our ablation study. Since we use variance regularization loss cooperatively with the reconstruction loss, the features are not fully homogenized, as an example is visualized in Figure S2 of the supplementary material.\nMoreover, we tried regularizing the difference in the mean of each channel, which gave a slight increase in accuracy for a few settings. Thank you for your constructive suggestions, we will integrate such an analysis into our manuscript.\n\nTable R4: **Variance regularization** on EDSR x4 (2-bit).\n| Method    | Set5  | Set14 | B100  | Urban100 |\n|:-------------|:-----:|:-----:|:-----:|:------:|\n| Regularize std of feature | 31.49 | 28.12 | 27.26 | 25.15 |\n| Regularize std of ch-wise mean | 31.51 | 28.15 | 27.29 | 25.21 |\n\n\n\n\\\n[A] DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks, WACV2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479619344,
                "cdate": 1700479619344,
                "tmdate": 1700479619344,
                "mdate": 1700479619344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]