[
    {
        "title": "Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness"
    },
    {
        "review": {
            "id": "IMFX2onIVy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_pjci"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_pjci"
            ],
            "forum": "60e1hl06Ec",
            "replyto": "60e1hl06Ec",
            "content": {
                "summary": {
                    "value": "The paper presents a new method to avoid the \"simplicity bias\" of neural networks, and learn more diverse features. This leads to models with better OOD generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Simple method.\n\n- Includes a theoretical analysis of the method under 2 lenses, a Gaussian mixture model, and causal structural model (appendix F2). These appear sound, given that the starting assumptions are met (see related comments in W1 below).\n\n- Large set of experiments on various benchmarks (from toy to realistic data).  Good performance on some realistic ones (e.g. Camelyon17)."
                },
                "weaknesses": {
                    "value": "W1. The benefits of the proposed method are entirely based on the premise that simple features = bad features.\n\nI think this is clear to the authors and also made clear upfront, but the limitations should be stated more prominently. In the introduction:\n\"*we take the viewpoint that features that are usually regarded as bein spurious for the task are often simple and quite predictive*\"\nThis is not just a \"viewpoint\", the authors should clearly say that this is the critical premise on which the entire work depends.\n\nLater on:\n\"*Models trained with ERM tend to focus on simple features (such as background) that do not generalize*\"\nThis is not correct. Simple features can very well be features that do generalize. The fact that deep learning is successful more often than not, and that OOD generalization is only a subfield of research, is the proof that the features learned by default are usually \"good\" ones.\n\nIn appendix E:\n\"alleviating simplicity bias can be a useful inductive bias in such cases. This is consistent with our experimental evaluations on several datasets\"\nI'm not sure that the authors realize that there is a heavy selection bias in the datasets that they use (which are datasets specifically selected on even purposefully *builts* to contain the kind of simple-but-spurious features that they propose to avoid). The only dataset where the distinction between the relevant/spurious feature is more realistic and not as clear (CelebA) is also the dataset on which the proposed method is much less effective.\n\nIn the summary of contributions:\n\"*We empirically evaluate and validate the hypothesis that spurious features are simpler than invariant features, with respect to representative datasets from Table 1*\"\nI do not think this is a valid claim: most of these datasets are purposefully selected or built to showcase the benefits of methods that \"debias\" simple features (most prominently: colored MNIST, waterbirds). So by definition, this hypothesis will be verified. If the authors want to validate the hypothesis, they should sample a random selection of datasets (NOT only from the OOD/debiasing literature!).\n\nNote that these issues are not specific to this paper and plague the whole field of debiasing methods. The authors do already take steps to make explicit the meaning of \"spurious features\" so I'm hopeful that this paper can rise above the standard of the field with just a few more clarifications.\n\n---------\n\nW2. Novelty of the method. The general idea of training one standard (biased) model, then training another regularized by the first one, has been thoroughly explored in the literature. All the references below (some of which are cited in the literature review) are based on this idea of training one initial biased model, with various improvements on top.\n\nThe method proposed in this paper is an implementation this general idea with a conditional-MI regularizer. The technical novelty therefore seems very limited.\n\n[1] A too-good-to-be-true prior to reduce shortcut reliance\n\n[2] Learning from failure: Training debiased classifier from biased classifier\n\n[3] Towards Debiasing NLU Models from Unknown Biases\n\n[4] A Conservative Approach for Unbiased Learning on Unknown Biases\n\n[5] BiasEnsemble: Revisiting the Importance of Amplifying Bias for Debiasing\n\n[6] BoosterNet - Improving Domain Generalization of Deep Neural Nets using Culpability-Ranked Features\n\n[7] Chroma-VAE Mitigating Shortcut Learning with Generative Classifiers\n\n[8] Learning Debiased Classifier with Biased Committee\n\n[9] MaskTune Mitigating Spurious Correlations by Forcing to Explore\n\n[10] Measures of Information Reflect Memorization Patterns\n\n[11] Roadblocks for Temporarily Disabling Shortcuts and Learning New Knowledge\n\n[12] Self-supervised debiasing using low rank regularization\n\n[13] Unsupervised Learning of Unbiased Visual Representations\n\n[14] BiasAdv: Bias-Adversarial Augmentation for Model Debiasing\n\n---------\n\nW3. The results on subgroup robustness are not that great (Table 8). I suggest replacing the table (which is tedious to read) with a scatter plot showing average vs. worst-group accuracy. It then becomes clear that the proposed method (and others, with the exception of GroupDRO that can leverage the additional knowledge required to solve OOD generalization) \"simply\" shifts the balance of relevant/spurious features such that the gains in worst-group accuracy are almost matched by a decrease in average accuracy.\n\n---------\n\nW4. The \"future directions\" proposed in the conclusion are very generic. For example, proposing the following is pretty much useless as a statement:\n\"*further explore the capabilities and limitations of our approach and to also further understand its theoretical properties*\"\n\nWhat are you thinking about exactly? Why wasn't it done yet for this paper?\n\nI do not understand what this means: \"*auditing large models with respect to much simpler models*\".\n\nI also do not understand: \"*explore the power of similar approaches for other desiderata*\"."
                },
                "questions": {
                    "value": "Please comment on W1 and W2.\n\nW1 is a very important issue, but I think it could be addressed with some edits.\n\nW2 is the critical issue (lack of technical novelty).\n\nW3 is (I believe) a consequence of W1.\n\nW4 is a very minor comment/suggestion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Reviewer_pjci"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697278227815,
            "cdate": 1697278227815,
            "tmdate": 1700761497414,
            "mdate": 1700761497414,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5cGiXrdJwz",
                "forum": "60e1hl06Ec",
                "replyto": "IMFX2onIVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our work and providing helpful suggestions to improve the paper. \n\n**W1:** *\u201c...the limitations should be stated more prominently.\u201d*\n\nWe have made some minor edits to the introduction/Fig. 1 caption to clarify some of the points mentioned by the reviewer.\n\n*\u201c... there is a heavy selection bias in the datasets used\u2026 If the authors want to validate the hypothesis that spurious features are simpler, they should sample a random selection of datasets (NOT only from the OOD/debiasing literature!).\u201d*\n\nWe note that in our experiments we consider several real-world datasets, such as ImageNet-9, Camelyon17, MultiNLI, CivilComments, Bias-in-Bios, and our approach is effective across all of these datasets. These results present strong evidence that mitigating simplicity bias is a useful inductive bias for several applications and modalities. Even on CelebA, where CMID is not competitive with recent methods, it significantly improves in terms of the worst-group accuracy compared to ERM. \n\nWe also note that our approach is mainly designed to improve generalization under distribution shifts, and as discussed in Appendix E, mitigating simplicity bias might not be that useful for ID generalization. Therefore, we only consider representative datasets from Table 1 for OOD generalization and subgroup robustness to validate our hypothesis that spurious features are often simpler than invariant ones. \n\n**W2: Novelty**\n\nPlease see Comment 1 in the general response section. Appendix D includes discussion on the differences with [1-2] and we will update it to discuss differences with [3-14]. We note that all of these methods (except [10]) are designed (or evaluated) specifically for either vision [4-9, 11-14] or language [3] datasets, whereas our approach is more broadly applicable to several modalities including vision, language and tabular data. [3] is similar to JTT and can suffer from similar drawbacks. [4, 6] are designed specifically for CNN-based architectures. Several of these methods are computationally intensive: [5, 8] involve training several biased models before training the final debiased model, [7] involves training a VAE in the first stage, [9] involves using an explainability approach to create a masked version of the dataset based on the first stage model to train the final model, [10] proposes a model selection strategy that requires training multiple models before selecting the best one, [11] involves using three models: a pretrained model, and an autoencoder and the final model that are trained simultaneously, [13] involves a three-stage approach, [14] involves creating adversarial samples for a biased model which are used to train the final model.\n\n**W3: Scatter plots showing average vs. worst-group accuracy for the subgroup robustness datasets**\n\nAs suggested by the reviewer, we have added scatter plots showing average and worst-group accuracy for the four subgroup robustness datasets in the paper (Fig. 12 in the Appendix). Please see the related discussion in Appendix H.3.4. \n\n**W4: Conclusion**\n\nWe will revise the conclusion to clarify these points."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528586886,
                "cdate": 1700528586886,
                "tmdate": 1700528586886,
                "mdate": 1700528586886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "svRKr5WeQP",
            "forum": "60e1hl06Ec",
            "replyto": "60e1hl06Ec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4"
            ],
            "content": {
                "summary": {
                    "value": "In this work authors propose a debiasing method based on minimizing conditional mutual information between a biased model and an unbiased one. The proposed method is tested on standard debiasing vision benchmarks and shows promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method does not require bias group labels\n\n- The proposed method shows good results overall \n\n- It does not require substantial training complexity compared to other methods"
                },
                "weaknesses": {
                    "value": "- The approach is not completely novel; MI has been investigated in many works as a way to mitigate bias eg. [1,2,3] and most importantly  [4] which is very similar to the proposed method. \n\n- The experimental evaluation does not compare to many established debiasing techniques eg [5-10] just to name some of the most recurring techniques.\n\n\n[1] Zhu, Wei, et al. \"Learning bias-invariant representation by cross-sample mutual information minimization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[2] Han, R., Wang, W., Long, Y., & Peng, J. (2022). Deep Representation Debiasing via Mutual Information Minimization and Maximization (Student Abstract). Proceedings of the AAAI Conference on Artificial Intelligence, 36(11), 12965-12966. https://doi.org/10.1609/aaai.v36i11.21619\n\n[3] Ragonesi, Ruggero, et al. \"Learning unbiased representations via mutual information backpropagation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[4] Tartaglione, E. (2022, November). Information Removal at the bottleneck in Deep Neural Networks. In 33rd British Machine Vision Conference 2022,{BMVC} 2022, London, UK, November 21-24, 2022.\n\n[5] Lee, Jungsoo, et al. \"Learning debiased representation via disentangled feature augmentation.\" Advances in Neural Information Processing Systems 34 (2021): 25123-25133.\n\n[6] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn:\nTraining deep neural networks with biased data. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2019.\n\n[7] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased\nrepresentations with biased representations. In International Conference on Machine Learning\n(ICML), 2020.\n\n[8] Mohsan Alvi, Andrew Zisserman, and Christoffer Nellaker. Turning a blind eye: Explicit removal \u02da\nof biases and variation from deep neural network embeddings. In Proceedings of the European\nConference on Computer Vision (ECCV), pp. 0\u20130, 2018.\n\n[9] Tartaglione, E., Barbano, C. A., & Grangetto, M. (2021). End: Entangling and disentangling deep representations for bias correction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 13508-13517).\n\n[10] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal\nbiases for visual question answering. In Advances in neural information processing systems, pp.\n841\u2013852, 2019."
                },
                "questions": {
                    "value": "- See weaknesses; \n\n- How does MI minimization relate to adversarial debiasing setups, i.e. when a classifier is trained on the bias labels (on the same feature space) and the main model is trained to minimize its accuracy while minimizing CE on the target task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4",
                        "ICLR.cc/2024/Conference/Submission3092/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698916632379,
            "cdate": 1698916632379,
            "tmdate": 1700586050529,
            "mdate": 1700586050529,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OKgHrpWUiQ",
                "forum": "60e1hl06Ec",
                "replyto": "svRKr5WeQP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our paper. \n\n**Novelty; \"MI has been investigated in prior works on debiasing\"**\n\nPlease see Comment 1 in the general response section. While MI has been investigated before, all of these works [1-4] require access to spurious feature labels. This information is often difficult to obtain in practice and in contrast to these methods, our approach does not have such a requirement.\n\n**Comparison with recent debiasing methods**\n\nPlease see Comment 3 in the general response section. We compare with [7] and [10] on the ImageNet-A dataset and with [5], [7] and [9] on the colored-MNIST dataset. \n\n**How does MI minimization relate to adversarial debiasing setups?**\n\nGenerally, debiasing methods that involve MI minimization require access to group/sensitive attribute labels whereas adversarial debiasing setups involve using an adversarial model to predict the group/sensitive attribute labels, while encouraging the main model to learn features that don\u2019t correlate well with the sensitive attribute."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528446636,
                "cdate": 1700528446636,
                "tmdate": 1700528446636,
                "mdate": 1700528446636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DKbTgq5Ids",
                "forum": "60e1hl06Ec",
                "replyto": "OKgHrpWUiQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_h3b4"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and I appreciate the additional empirical comparison with other techniques. I still think that the difference wrt to some of other works employing MI is somehow limited; nonetheless, this work still represents a good methodological contribution. I am willing to increase my score, and I suggest the authors incorporate a discussion on the similarities with other methods in the final version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586031569,
                "cdate": 1700586031569,
                "tmdate": 1700586031569,
                "mdate": 1700586031569,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MlhaHYD0PM",
            "forum": "60e1hl06Ec",
            "replyto": "60e1hl06Ec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_5jvr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_5jvr"
            ],
            "content": {
                "summary": {
                    "value": "The paper focus on the issue of spurious correlation that affects the neural networks and make a connection between this phenomenon and simplicity bias. As models tend to favor learning simpler features over more complex ones, even if the latter are more informative, the paper links the similar phenomenon of learning spurious feature to learning simple features. To counteract this, the authors propose a framework that first trains a simple model and then regularizes it to encourage the use of a diverse set of features for predictions. The approach is demonstrated to be effective in various scenarios, enhancing OOD generalization and robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-structured and easy to follow. The authors made good efforts to motivate their study with empirical findings. The illustrative figures are helpful. \n2. The authors presented experiments across a wide range of datasets. The analysis is comprehensive and the evaluation of CMID is done on multiple aspects. CMID does achieve satisfactory results on a majority of these tasks."
                },
                "weaknesses": {
                    "value": "1. Definition 1 lacks formality. The terms \"high (in-distribution) accuracy\" and \"certain complexity\" require clearer specification. Additionally, the description of a \"simple model\" as one with significantly lower complexity than benchmark models is vague. I understand that there might not be a specific threshold on \"high accuracy\", but I think this definition can be formalized better. Can we maybe define models with \"certain complexity\" as those capable of learning the training data so that the training loss converges to near-zero? Conversely, \"simple models\" could be characterized as those lacking the capacity to adequately fit the training data.\n2. Definition 2 also lacks clarity and formality. Another concern is that, spurious feature has been an established concept. It would be more appropriate to treat Definition 2 as a hypothesis, which needs to be substantiated with sufficient empirical observations.\n3. In Assumption 1, which outlines the data model, there is a need to ensure that the spurious feature here is consistent with that in Definition 2. Currently, it does not seem evident that the spurious feature in Assumption 1 matches that in Definition 2.\n4. (minor) Several recent works [1-2] also present theoretical results on the study of spurious correlation. Especially, [1] seems to align well with the finding in this paper but through different perspectives. It would be better have a brief discussion on the alignments and differences of these studies.\n\n[1] \"Robust Learning with Progressive Data Expansion Against Spurious Correlation.\" Advances in neural information processing systems. 2023.\n\n[2] \"Understanding and Improving Feature Learning for Out-of-Distribution Generalization.\" Advances in neural information processing systems. 2023."
                },
                "questions": {
                    "value": "See in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698924810917,
            "cdate": 1698924810917,
            "tmdate": 1699636255248,
            "mdate": 1699636255248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aFebhduCgo",
                "forum": "60e1hl06Ec",
                "replyto": "MlhaHYD0PM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our paper and providing constructive feedback to help improve our work. \n\n**Better formalization of Definition 1**\n\nThank you for this suggestion, we will update Definition 1 as follows:\n\n*Consider a task on which benchmark models which achieve high (in-distribution) accuracy and can attain near-zero training loss have a certain complexity (in terms of number of parameters, layers etc.). We consider models that have significantly lower complexity than benchmark models and cannot attain arbitrarily small loss on the training data as simple models. Similarly, features that can be effectively learned using simple models are considered as simple features.*\n\n**\u201cIt would be more appropriate to treat Definition 2 as a hypothesis.\u201d**\n\nWe agree with the reviewer, and we do treat this as a hypothesis, e.g. as mentioned in the first point of the contributions, \u201cWe empirically evaluate and validate the hypothesis that spurious features are simpler than invariant features, with respect to representative datasets from Table 1 (Section 2).\u201d Definition 2 is based on the experimental results in Section 2, which validate this hypothesis on some datasets. We have made some edits to Section 2 to clarify this.\n\n**\u201cIt does not seem evident that the spurious feature in Assumption 1 matches that in Definition 2.\u201d**\n\nPlease see Comment 2 in the general response section.\n\n**Discussion on recent works [1-2]**\n\nWe will include these in the discussion in Appendix D."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528313352,
                "cdate": 1700528313352,
                "tmdate": 1700528313352,
                "mdate": 1700528313352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x9PB7fgNsd",
            "forum": "60e1hl06Ec",
            "replyto": "60e1hl06Ec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to mitigate simplicity bias by incentivizing complex feature learning via a new regularization term. Specifically, the conditional mutual information between the model\u2019s features, and those of a pretrained \u201csimple\u201d model is added to the training objective.\n\n**Edit** Raised score to 5 based on author responses and additional experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The basic proposal of using CMI for regularization makes intuitive sense, and the evaluation covers a number of datasets and metrics for robustness."
                },
                "weaknesses": {
                    "value": "The contributions can be considered among 3 axes: proposal, empirical evaluation, and theory. In this reviewer\u2019s assessment, each of these axes have weaknesses, and the sum total falls somewhat short of a solid contribution.\n\n* Proposal: Novelty w.r.t. other feature diversity methods e.g., ESB [1] which is closest in spirit, and uses a gradient-based diversity incentive instead of CMI. Also many other related papers cited by the authors in the appendix.\n* Evaluation: \n    * Weak empirical results, eg, a simple method like JTT[2] is beating CMID across almost all datasets for Group robustness. Refer to table 7.  \n    * In generalization, too (WILDS / Fairness), surprisingly CMID does not have the best OOD, something that would be expected of a method designed to avoid spurious features. \n    * Missing head-to-head comparisons against more recent methods (some of them cited but not compared), including LWBC [3], SIFER [4], ESB [1]. \n    * Not necessary, but authors may consider evaluating on other datasets such as BAR, NICO, Imagenet-A/C/R, other datasets from DomainBed, etc., for a more thorough comparison for the above and other methods.\n* Theoretical results appear weak / not directly applicable to the problem setting (please see below.)\n* Conceptual: A key, unaddressed, question (as far as I could tell) is what happens when the simple model also depends to some extent on complex features. It seems reasonable to worry that the regularization can cause more harm than help. \n    * relatedly, the idea that simple=bad/spurious, complex=good/invariant, for some mathematical definition of simple & complex, seems unlikely to hold water in real world applications.\n\n\n\nOther questions/comments.\n* As I understand it, Theorem 1 considers features {invariant, spurious} that are in the same complexity class (i.e., gaussian), and a linear predictor. The \u201csimple model\u201d is **guaranteed to only use** the spurious features. Under these circumstances the CMI biases the model towards the invariant features. This seems not very convincing:\n    * It would appear that given the strong assumptions above, the claim follows in a straightforward manner. In any case, guaranteeing the \u201csimple model\u201d only uses simple features seems difficult in practice.\n    * The central claim that simple and complex features are from different complexity classes (see e.g., Section 2 and figure 2) is not captured here. Even in that case, the interesting result / question would be when the \"simple\" model has nonzero weight on invariant features as well.\n\n\n\n [1] Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization by Teney et. al.\n\n [2] Just Train Twice: Improving Group Robustness without Training Group Information by Liu et. al.\n\n [3] Learning Debiased Classifier with Biased Committee by Kim et. al.\n\n [4] Overcoming Simplicity Bias in Deep Networks using a Feature Sieve by Tiwari et. al."
                },
                "questions": {
                    "value": "* Results: \n    * Texture vs shape: this is good, but a) does it materially improve accuracy on IN-9? Or generalization on IN challenge datasets? b) how does it compare against the other feature diversity approaches?\n    * Generalization: Why, in your understanding, do other methods have better generalization but potentially worse in-domain accuracy than CMI? Given the nature and intent of the regularization (learn features that are conditionally independent of \"simple features\" presumably also learned on indomain data), one would have expected the exact opposite finding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3092/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699008088430,
            "cdate": 1699008088430,
            "tmdate": 1700713907985,
            "mdate": 1700713907985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WyIuDYT15X",
                "forum": "60e1hl06Ec",
                "replyto": "x9PB7fgNsd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to provide the detailed review and constructive comments for our work.  \n\n**Novelty**\n\nPlease see Comment 1 in the general response section. We also discuss differences with prior work including [1] in detail in Appendix D.\n\n**Evaluation**\n\n- *\u201c...JTT is beating CMID across almost all datasets for Group robustness.\u201d*\n\nWe consider several applications in our work: mitigating simplicity bias, enhancing OOD generalization and improving subgroup robustness and fairness. Although JTT is more effective than CMID on three subgroup robustness datasets, the gap on two of these is not very large. On the fourth dataset (CivilComments), CMID outperforms JTT by a considerable margin. In addition, CMID is significantly better than JTT for mitigating simplicity bias, and leads to improved OOD generalization, without a significant drop in IID performance, as seen in the experiments in Section 5. In addition, in contrast to JTT, our approach comes with theoretical guarantees.\n\n- *\u201cCMID does not have the best OOD in WILDS / Fairness.\u201d*\n\nWe note that while CMID and EIIL have the same OOD accuracy on the Adult-confounded (fairness) dataset, CMID has a much smaller gap between in-distribution (ID) and OOD accuracy compared to all other methods. Similarly, while JTT has slightly better OOD accuracy (0.1) than CMID on the Camelyon-WILDS dataset, it has a much larger gap between ID and OOD accuracy compared to CMID. We will clarify this in the paper.\n\n- *Comparison with more recent methods/evaluation on ImageNet-A dataset*\n\nPlease see Comment 3 in the general response section. We compare our approach with [2-4] on the ImageNet-A dataset.\n\n**Conceptual**\n\n- *\"What happens when the simple model also depends to some extent on complex features? It seems reasonable to worry that the regularization can cause more harm than help.\"*\n\nWe note that in Fig. 2, we compare the effect of using various model architectures of different complexities as the simple model for our approach for the Waterbirds dataset. Models such as ResNet18 and ResNet50 can rely on complex features as well, and indeed we observe some drop in the worst-group accuracy when we regularize the CMI with respect to these models to train the final model. However, we note that these worst-group accuracy values are still significantly better compared to ERM. We will update the figure and include this discussion in the paper.\n\nFurther, we note that we don\u2019t see any adverse effects of our approach in any of our experiments. Even on CelebA, where CMID is not competitive with recent methods, it significantly improves in terms of the worst-group accuracy compared to ERM. \n\n- *\u201cThe idea that simple=bad/spurious, complex=good/invariant, for some mathematical definition of simple & complex, seems unlikely to hold water in real world applications.\u201d*\n\nWe respectfully disagree with this comment. We note that in our experiments we consider several real-world datasets, such as ImageNet-9, Camelyon17, MultiNLI, CivilComments, Bias-in-Bios, and our approach is effective across all of these datasets. These results present strong evidence that this assumption is true across several cases and that mitigating simplicity bias is a useful inductive bias for several applications. \n\n**Regarding Theorem 1:**\n\n- *\u201cTheorem 1 considers a setting where the simple model is guaranteed to only use the spurious feature.\u201d*\n\nWe consider this setting for our theoretical results because it allows us to isolate the effect of the CMI regularization. We agree that it is also important to consider the case where the simple model has some nonzero weight on the invariant feature, particularly in practice. Therefore, we empirically compare the effect of using various model architectures of different complexities as the simple model for our approach in Fig. 2. We believe that this addresses the concern that \u201cguaranteeing the \u201csimple model\u201d only uses simple features seems difficult in practice\u201d and offers some insights about our approach in that case. \n\n- *\u201cThe central claim that simple and complex features are from different complexity classes (see e.g., Section 2 and figure 2) is not captured here.\u201d*\n\nPlease see Comment 2 in the general response section."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528110294,
                "cdate": 1700528110294,
                "tmdate": 1700528145292,
                "mdate": 1700528145292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YLD3aB0pKF",
                "forum": "60e1hl06Ec",
                "replyto": "YrBBVJQNub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Reviewer_Q6kW"
                ],
                "content": {
                    "title": {
                        "value": "Response to author comments"
                    },
                    "comment": {
                        "value": "Dear authors, thank you for investing the time and energy in crafting a detailed, thoughtful response to the reviews.  I have carefully read through the response, the other reviews and discussion. \n\nI am raising my score to a 5 as a result. Unfortunately, I still believe the paper falls short of acceptance.\n\nThe primary reason for raising score is the shared additional comparisons, and the commendable honesty in sharing shortfalls compared to other baselines (SIFER, DFA). I'm ok with overlooking the large advantage of REBIAS as it appears to be hand-crafted for the specific synthetic scenario. (A related nit, for future rewrites, is that it is strongly preferable to compare all baselines together on all datasets, or at least on one of the most challenging datasets. Results on synthetic datasets very often do not extend to real-world).\n\nThe reason for not accepting is that I continue to find the theoretical result on the one hand completely expected (if the simple model is **guaranteed** to not use complex features, are we at all surprised that CMI works? shouldn't it work simply by definition?) and on the other hand irrelevant to real-world scenarios.  Unfortunately the results do not stand on their own  as a strong empirical findings paper either."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713738922,
                "cdate": 1700713738922,
                "tmdate": 1700713738922,
                "mdate": 1700713738922,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SsOUzrXDLv",
                "forum": "60e1hl06Ec",
                "replyto": "x9PB7fgNsd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3092/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response and for increasing the score. \n\nWe note that in our theoretical results for the Gaussian features, we also consider a case where there are multiple spurious features (Appendix F.1). Even in that case, we theoretically show that CMI regularization is effective in increasing the weight on the invariant feature, which is a bit surprising. \n\nWe also note that previous methods in the problem areas that we consider generally do not have theoretical guarantees (discussed in more detail in Appendix D), and although the setting we consider is relatively simpler, it does offer insights about the regularization and the effectiveness of our approach. We believe that this is a step towards developing more theoretically grounded methods."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3092/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714172666,
                "cdate": 1700714172666,
                "tmdate": 1700720146355,
                "mdate": 1700720146355,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]