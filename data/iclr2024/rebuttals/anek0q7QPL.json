[
    {
        "title": "Exploring the Combined Power of Covariance and Hessian Matrices Eigenanalysis for Binary Classification"
    },
    {
        "review": {
            "id": "hU4nzynJY5",
            "forum": "anek0q7QPL",
            "replyto": "anek0q7QPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_YZvz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_YZvz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data projection approach which combines the power of covariance and Hessian matrices in the binary classification task. Specifically, the method combines eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability. Benefiting from the linear discriminant analysis (LDA) criteria, the proposed method achieves better class separability in contrast to PCA and the Hessian method. Empirical results show its better performance in binary classification."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)Combining the power of the covariance matrix and the Hessian matrix is interesting and novel, to my knowledge. And this work gives a new learning perspective for binary classification problems. \n\n2)The writing style is good and the motivation is clear."
                },
                "weaknesses": {
                    "value": "1)The description of the key technique is unclear. How to integrate the covariance matrix and the Hessian matrix can be confusing. The authors only offer some description in the Section 2.1 and it lacks more detailed theoretical explanation.\n\n2)The comparison methods are insufficient. To clarify the superiority of the proposed method, the authors compare the method with four data projection techniques including PCA, Hessian, UMAP and LDA. However, some other dimensionality reduction and data projection techniques should be included. For example, kernel based methods including kernel PCA and kernel LDA and manifold based methods like locally linear embedding (LLE) and t-distributed Stochastic Neighbor Embedding (t-SNE) are also representative methods in this problem. \n\n3)The classification results in Figure 2 can be confusing. The performance of the proposed method is not obviously superior than other competing methods at times. For example, the performance of the proposed method is very close to the performance of Hessian in the WBCD database. Moreover, the performance of the proposed method is very close to the performance of UMAP in the Pima Indians diabetes database."
                },
                "questions": {
                    "value": "1)As mentioned above, the authors should offer more details of the key technique about how to integrate the covariance matrix and the Hessian matrix. For better  readability, an interpretative figure to illustrate the key technique is needed.\n\n2)The authors should provide a specific algorithm of the proposed method. The procedure of the algorithm remains highly unclear. \n\n3)The authors should offer more experimental results to validate the effectiveness of the proposed method, which are not limited to more comparison methods and more convincing classification results. As mentioned above, some other dimensionality reduction and data projection techniques should be included. Besides, the performance on the classification problem is not outstanding, \n\n4) This work is limited in the problem of binary classification. And it would be better if the authors could devise a multi-class classifier considering the real-world applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698453117572,
            "cdate": 1698453117572,
            "tmdate": 1699636885218,
            "mdate": 1699636885218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "attwkDmo8T",
                "forum": "anek0q7QPL",
                "replyto": "hU4nzynJY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YZvz (part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer YZvz,\n\nWe appreciate the time and effort you invested in reviewing our manuscript. We are pleased that you acknowledge the interesting and novel nature of our proposed method, and found the writing style clear and the motivation well-presented. Moreover, your insights have been invaluable in refining the quality of our work. \n\nWe would like to inform you that we have already submitted a revised version of the manuscript that addresses the comments and concerns you raised. \n\nBelow are our responses to your specific points:\n\n1. **Response to question 1 and 2:**\n   - > \"As mentioned above, the authors should offer more details of the key technique about how to integrate the covariance matrix and the Hessian matrix. For better readability, an interpretative figure to illustrate the key technique is needed.\"\n   - > \"The authors should provide a specific algorithm of the proposed method. The procedure of the algorithm remains highly unclear.\"\n\nWe acknowledge the need for a more explicit presentation of the algorithm for the proposed method. In response to this, we have added additional clarity and detail in Section 2.1 of the revised manuscript, providing a more thorough description of the key technique.\n\nIn the revised Section 2.1:\n\n1. **Covariance matrix eigenanalysis:**\n   We clarified the eigenanalysis of the covariance matrix ($Cov(\\boldsymbol{\\theta})$) and its role in capturing the principal directions with the highest variances. The leading eigenvector ($\\mathbf{v}_1$) associated with the largest eigenvalue ($\\lambda_1$) represents the principal direction with the highest variance.\n\n   The eigen-equation is given by $Cov(\\boldsymbol{\\theta}) \\cdot \\mathbf{v}_1 = \\lambda_1 \\cdot \\mathbf{v}_1$.\n\n2. **Hessian matrix eigenanalysis:**\n   We explained the eigenanalysis of the Hessian matrix ($H_{\\boldsymbol{\\theta}}$) in detail, including the utilization of a deep neural network with binary cross-entropy loss during training. The leading eigenvector ($\\mathbf{v}_1'$) associated with the largest eigenvalue ($\\lambda_1'$) represents the direction corresponding to the sharpest curvature.\n\n\n   The eigen-equation is given by $H_{\\boldsymbol{\\theta}} \\cdot \\mathbf{v}_1' = \\lambda_1' \\cdot \\mathbf{v}_1'$.\n\n3. **Integration of matrices and projection of data:**\n   We explicitly presented the integration of matrices and projection of data into a 2D space with mathematical formulations.\n\n   $$ \\mathbf{U} = [\\mathbf{v}_1, \\mathbf{v}_1'] $$\n\n   The 2D projection of the data is obtained by:\n\n   $$ \\mathbf{X}_{\\text{proj}} = \\mathbf{X} \\cdot \\mathbf{U} $$\n   \n   where $\\mathbf{X}$ is the original data matrix, and $\\mathbf{X}_{\\text{proj}}$ represents the final output of the proposed method.\n\nWe want to emphasize that this summary provides the core mathematical ideas, and the complete and detailed explanations, along with additional context, can be found in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235620120,
                "cdate": 1700235620120,
                "tmdate": 1700236083128,
                "mdate": 1700236083128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9jnimJBfKw",
                "forum": "anek0q7QPL",
                "replyto": "hU4nzynJY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YZvz (part 2)"
                    },
                    "comment": {
                        "value": "2. **Response to question 3:**\n   - > \"The comparison methods are insufficient. To clarify the superiority of the proposed method, the authors compare the method with four data projection techniques including PCA, Hessian, UMAP and LDA. However, some other dimensionality reduction and data projection techniques should be included. For example, kernel based methods including kernel PCA and kernel LDA and manifold based methods like locally linear embedding (LLE) and t-distributed Stochastic Neighbor Embedding (t-SNE) are also representative methods in this problem.\"\n   - > \"The authors should offer more experimental results to validate the effectiveness of the proposed method, which are not limited to more comparison methods and more convincing classification results. As mentioned above, some other dimensionality reduction and data projection techniques should be included. Besides, the performance on the classification problem is not outstanding.\"\n\n-\tTo address the concern regarding the comparison methods, we have expanded our evaluation to include a broader spectrum of contemporary techniques. In the revised manuscript, we compare our proposed method with nine distinct approaches, encompassing kernel-based methods like KPCA and KDA, manifold based methods like UMAP and LLE as well as LOL [1]. This extended comparison aims to provide a more comprehensive view of the proposed method's performance.\n-\tWe have introduced nonlinearity through distinct kernels determined by grid search in KPCA and KDA for each dataset.\n-\tThe results, discussed in the revised manuscript, demonstrate the consistent superiority of our approach across various datasets and metrics.\n-\tThese extended results are completely reproducible using the same Colab notebooks whose links are provided for transparency and validation.\n-\tThe relevant discussion for these additional comparisons has been appropriately incorporated into the Discussion section of the revised manuscript.\n-\tThe abstract has also been updated to reflect the implications of these extended comparisons.\n\n\n3. **Response to question 4:**\n   - > \"This work is limited in the problem of binary classification. And it would be better if the authors could devise a multi-class classifier considering the real-world applications.\"\n\nWhile our work focuses on binary classification, we appreciate your suggestion to extend our methodology to multiclass classification as a potential avenue for future research. We have explicitly mentioned in the Discussion section of the revised manuscript that the binary classification focus in this work stems from foundational aspects guiding our formal proof, which is designed around binary assumptions to facilitate a streamlined and elegant derivation process. In particular, the use of binary cross-entropy as the loss function and the utilization of a linear SVM for evaluation inherently adhere to binary classification. Moving forward, careful exploration is needed to adapt our approach to multiclass scenarios to ensure its applicability and effectiveness across a broader range of classification tasks.\n\nWe hope these revisions address your concerns, and we welcome any further feedback or suggestions you may have. Thank you once again for your thorough review and valuable input.\n\nReference:\n\n[1] Vogelstein, J. T., Bridgeford, E. W., Tang, M., Zheng, D., Douville, C., Burns, R., & Maggioni, M. (2021). Supervised dimensionality reduction for big data. Nature communications, 12(1), 2872."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235864669,
                "cdate": 1700235864669,
                "tmdate": 1700236120940,
                "mdate": 1700236120940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qQSl4atBDB",
            "forum": "anek0q7QPL",
            "replyto": "anek0q7QPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_LZ1V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_LZ1V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient binary classification method based on integrating the covariance and Hessian matrices in improving classification performance. This method combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Both theoretical proofs and experimental results are demonstrated to consolidate the theory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "proposes a method that combines covariance and Hessian matrices to perform classification analysis more effectively"
                },
                "weaknesses": {
                    "value": "The part 3 of Methodologies session (Section 2.1) can be written more clearly."
                },
                "questions": {
                    "value": "I wonder if the part 3 of Methodologies session (Section 2.1) can be written more clearly? In particular, why the result of the claimed process yields a 2D projection of the data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7389/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7389/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7389/Reviewer_LZ1V"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635506134,
            "cdate": 1698635506134,
            "tmdate": 1700752472410,
            "mdate": 1700752472410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e8ylYtlYra",
                "forum": "anek0q7QPL",
                "replyto": "qQSl4atBDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LZ1V"
                    },
                    "comment": {
                        "value": "Dear Reviewer LZ1V,\n\nThank you for your thoughtful review and valuable feedback on our manuscript, Submission7389. We appreciate the time and effort you invested in providing constructive comments.\n\nWe would like to inform you that the revised and clarified content of Section 2.1 has been incorporated into the updated manuscript that we have submitted. We hope that the revisions address your concerns and enhance the clarity of the proposed approach.\n\nIn the revised Section 2.1:\n\n1. **Covariance matrix eigenanalysis:**\n   We clarified the eigenanalysis of the covariance matrix ($Cov(\\boldsymbol{\\theta})$) and its role in capturing the principal directions with the highest variances. The leading eigenvector ($\\mathbf{v}_1$) associated with the largest eigenvalue ($\\lambda_1$) represents the principal direction with the highest variance.\n\n   The eigen-equation is given by $Cov(\\boldsymbol{\\theta}) \\cdot \\mathbf{v}_1 = \\lambda_1 \\cdot \\mathbf{v}_1$.\n\n2. **Hessian matrix eigenanalysis:**\n   We explained the eigenanalysis of the Hessian matrix ($H_{\\boldsymbol{\\theta}}$) in detail, including the utilization of a deep neural network with binary cross-entropy loss during training. The leading eigenvector ($\\mathbf{v}_1'$) associated with the largest eigenvalue ($\\lambda_1'$) represents the direction corresponding to the sharpest curvature.\n\n\n   The eigen-equation is given by $H_{\\boldsymbol{\\theta}} \\cdot \\mathbf{v}_1' = \\lambda_1' \\cdot \\mathbf{v}_1'$.\n\n3. **Integration of matrices and projection of data:**\n   We explicitly presented the integration of matrices and projection of data into a 2D space with mathematical formulations.\n\n   $$ \\mathbf{U} = [\\mathbf{v}_1, \\mathbf{v}_1'] $$\n\n   The 2D projection of the data is obtained by:\n\n   $$ \\mathbf{X}_{\\text{proj}} = \\mathbf{X} \\cdot \\mathbf{U} $$\n   \n   where $\\mathbf{X}$ is the original data matrix, and $\\mathbf{X}_{\\text{proj}}$ represents the final output of the proposed method.\n\nWe want to emphasize that this summary provides the core mathematical ideas, and the complete and detailed explanations, along with additional context, can be found in the revised manuscript.\n\nIf you have any specific points or further inquiries, we are more than willing to address them. We look forward to hearing your thoughts on the revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158341709,
                "cdate": 1700158341709,
                "tmdate": 1700158341709,
                "mdate": 1700158341709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GR17ksEP03",
                "forum": "anek0q7QPL",
                "replyto": "e8ylYtlYra",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_LZ1V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_LZ1V"
                ],
                "content": {
                    "title": {
                        "value": "increase score to 4"
                    },
                    "comment": {
                        "value": "Thanks for the response. I would like to increase my score to 4."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713132786,
                "cdate": 1700713132786,
                "tmdate": 1700713132786,
                "mdate": 1700713132786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gFIvjqQaol",
            "forum": "anek0q7QPL",
            "replyto": "anek0q7QPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript theoretically and numerically evaluates the projection matrices derived from both the Covariance and the Hessian, in terms  of how they impact classification performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Covariance and Hessian matrices of various kinds are indeed of critical importance in classification performance, and warrant further study."
                },
                "weaknesses": {
                    "value": "1. I found the theory to be a weak.  For me to believe the relevance of this theory, it must operate within a multivariate context.  The question is about whether the top eigenvalues of either of these matrices contain the relevant signal. Theory operating on unidimensional data I think has relatively little to offer on this topic. \n\n2. As I read the paper, the authors talk about *the* Hessian matrix.  However, the paper is about a Hessian estimated using a specific deep net, which is of course *a* Hessian, but not *the* Hessian.  The discussion implied (to me) much more general claims than were warranted, imho, given the actual theoretical and empirical results. I would have expected a specific mention of Fisher's Information Matrix, which is closely related to the Hessian, as it includes it, and is a known bound of the variance for any random variable.\n\n3. LDA is well known to find the projection that balances maximizing across-variance while minimizing within-variance.  It was never clear to me why we would want another method to do something like that?  What is missing in LDA that this method achieves? I can imagine a desire to embed in multiple dimensions, rather than just 1, but see my next point about that.\n\n4.  Under the Gaussian model, the direction that captures the variance across classes is simply the difference of means vector (after 'whitening'), and the direction that maximizes the variance within is the class-centered covariance.  Reduced Rank LDA essentially combines those two: it projects the data onto the matrix which is the product of the difference of means with the low-rank estimate of the pooled covariance.  So, we already have a standard/classical approach to embedding into these two dimensions.  How is your approach better than this?\n\n5. The defined Hessian is closely related to another standard thing called the Pointwise Mutual Information (https://en.wikipedia.org/wiki/Pointwise_mutual_information), which is very commonly used in language processing and embedding.  A discussion/comparison with this method would be desirable.\n\n6. The numerical results indicate that embedding using the proposed approach is slightly better than the unsupervised approaches, or LDA, which is purely linear.  But your approach is nonlinear.  So, unless the data are strongly linear, a supervised nonlinear approach is likely to win.  And your proposed approach must lose in simulations where the data truly are linear. I'd think any reasonable kernel LDA approach would improve relative to PCA or LDA, assuming a large enough sample size."
                },
                "questions": {
                    "value": "There are a few relevant papers that might be worthwhile reading for more background, including our paper, https://www.nature.com/articles/s41467-021-23102-2, and one ours built on, https://www.sciencedirect.com/science/article/pii/S0047259X14001201?via%3Dihub."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7389/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7389/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699244375126,
            "cdate": 1699244375126,
            "tmdate": 1700685619396,
            "mdate": 1700685619396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cp5FpclN1O",
                "forum": "anek0q7QPL",
                "replyto": "gFIvjqQaol",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7ZV (part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer E7ZV,\n\nThank you for dedicating your time to review our manuscript. We sincerely appreciate your thoughtful comments. \n\nWe want to inform you that we have already submitted a revised version of the manuscript that addresses the concerns and questions you raised. We have incorporated significant revisions into the manuscript, focusing on clarity, detailed explanations, and additional comparisons to enhance the overall quality of the paper. The updated version aims to provide a more comprehensive and satisfactory reading experience.\n\nBelow is our detailed response to each of your points:\n\n1. **Response to concern 1:**\n   - > \"I found the theory to be a weak. For me to believe the relevance of this theory, it must operate within a multivariate context. The question is about whether the top eigenvalues of either of these matrices contain the relevant signal. Theory operating on unidimensional data I think has relatively little to offer on this topic.\"\n\nWe understand your concern about the perceived weakness of the theory, particularly in a simple, univariate context. In response to this, we have added a paragraph at the beginning of the Discussion section in the revised manuscript. In our revised discussion, we emphasize the strength of simplicity in achieving remarkable results. In our work, we firstly provide a profound theoretical insight, revealing a subtle yet powerful relationship between covariance and Hessian matrices. Our formal proof seamlessly links covariance eigenanalysis with the first LDA criterion while Hessian eigenanalysis with the second one. This unification under LDA criteria offers a fresh and intuitive perspective on their interplay. Secondly, we highlight that simplicity, when harnessed effectively, can lead to powerful and practical solutions. Drawing inspiration from the elegant and simple theoretical relationship, we introduce a novel method that consistently outperforms established techniques across diverse datasets. This unexpected efficacy is rooted in the straightforward relationship between covariance, Hessian, and LDA, showcasing the effectiveness of simplicity in addressing complex challenges.\n\nAdditionally, we find resonance in the work on Linear Optimal Low-rank projection. The methodology, despite its simplicity, has demonstrated remarkable success in enhancing data representations for various classification tasks while maintaining computational efficiency and scalability [1]. This further supports our belief in the potential of simplicity to achieve significant advancements in the field.\n\n2. **Response to concern 2:**\n   - > \" As I read the paper, the authors talk about the Hessian matrix. However, the paper is about a Hessian estimated using a specific deep net, which is of course a Hessian, but not the Hessian. The discussion implied (to me) much more general claims than were warranted, imho, given the actual theoretical and empirical results. I would have expected a specific mention of Fisher's Information Matrix, which is closely related to the Hessian, as it includes it, and is a known bound of the variance for any random variable.\"\n\nYou rightly pointed out that in the original manuscript, there was a usage of \"the Hessian matrix,\" which has been corrected in the revised version. We acknowledge this oversight, and the updated manuscript now consistently refers to \"a Hessian matrix.\" We have also explicitly mentioned Fisher's information as an approximation of the Hessian in Section 2.2 of the revised manuscript. This modification ensures a clear acknowledgment of the connection between the Hessian matrix and Fisher's information matrix, providing a more accurate representation of our methodology.\n\n\nReference:\n\n[1]\tVogelstein, J. T., Bridgeford, E. W., Tang, M., Zheng, D., Douville, C., Burns, R., & Maggioni, M. (2021). Supervised dimensionality reduction for big data. Nature communications, 12(1), 2872."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500057319,
                "cdate": 1700500057319,
                "tmdate": 1700500057319,
                "mdate": 1700500057319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YOaJEPud5E",
                "forum": "anek0q7QPL",
                "replyto": "gFIvjqQaol",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7ZV (part 2)"
                    },
                    "comment": {
                        "value": "3. **Response to concern 3 and 4:**\n   - > \" LDA is well known to find the projection that balances maximizing across-variance while minimizing within-variance. It was never clear to me why we would want another method to do something like that? What is missing in LDA that this method achieves? I can imagine a desire to embed in multiple dimensions, rather than just 1, but see my next point about that.\"\n   - > \" Under the Gaussian model, the direction that captures the variance across classes is simply the difference of means vector (after 'whitening'), and the direction that maximizes the variance within is the class-centered covariance. Reduced Rank LDA essentially combines those two: it projects the data onto the matrix which is the product of the difference of means with the low-rank estimate of the pooled covariance. So, we already have a standard/classical approach to embedding into these two dimensions. How is your approach better than this?\"\n\nOur initial motivation was not driven by a desire to embed in multiple dimensions, as mentioned. Instead, it originated from the intention to explore the relationship between two familiar concepts: covariance and Hessian matrices. Unexpectedly, this exploration led to their unification under another familiar concept, namely the LDA concept. The elegance of this relationship, as highlighted in the revised manuscript, allows us to introduce a method that outperforms established methods, both linear and non-linear, including Linear DA and Kernel DA. The method's superiority is not based on a need for multiple dimensions but rather on the unique insights provided by this unification.\n\nWe appreciate the opportunity to clarify why our approach outperforms LDA. Experimental results unequivocally demonstrate the superior performance of our method across all cases, supported by a robust theoretical explanation. Our method surpasses LDA by leveraging higher-dimensional feature spaces, aligning with Cover\u2019s theorem, which favors linear separability in higher dimensions.\n\n4. **Response to concern 5:**\n   - > \" The defined Hessian is closely related to another standard thing called the Pointwise Mutual Information (https://en.wikipedia.org/wiki/Pointwise_mutual_information), which is very commonly used in language processing and embedding. A discussion/comparison with this method would be desirable.\"\n\nWe appreciate your suggestion regarding the potential connection between the defined Hessian and Pointwise Mutual Information (PMI). However, after careful consideration, we believe that PMI might not be directly relevant to the goals and scope of our work, which focuses on binary classification tasks and the interplay between covariance and Hessian matrices. While PMI is indeed a valuable tool in language processing and embedding, the objectives and underlying principles of our work differ significantly.\n\nIf you have further questions or if there are specific aspects of PMI that you believe could contribute meaningfully to our study, please provide additional details, and we will gladly consider them in our response or future work.\n\n5. **Response to concern 6:**\n   - > \" The numerical results indicate that embedding using the proposed approach is slightly better than the unsupervised approaches, or LDA, which is purely linear. But your approach is nonlinear. So, unless the data are strongly linear, a supervised nonlinear approach is likely to win. And your proposed approach must lose in simulations where the data truly are linear. I'd think any reasonable kernel LDA approach would improve relative to PCA or LDA, assuming a large enough sample size.\"\n\nWe appreciate your insight into the potential advantages of nonlinear approaches, especially in scenarios where the data exhibits strong nonlinearities. To address this consideration, we expanded our evaluation to include a diverse set of contemporary techniques, such as kernel-based methods like KPCA and KDA. The revised manuscript provides a detailed discussion of the results, highlighting the consistent superiority of our approach across various datasets and metrics. Interestingly, our method not only competes favorably with linear methods like LDA but also outperforms sophisticated nonlinear approaches like KPCA and KDA. This emphasizes the robustness and effectiveness of our proposed method, even in situations where non-linearities play a significant role."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500408676,
                "cdate": 1700500408676,
                "tmdate": 1700500480866,
                "mdate": 1700500480866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "83mnsTruib",
                "forum": "anek0q7QPL",
                "replyto": "gFIvjqQaol",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E7ZV (part 3)"
                    },
                    "comment": {
                        "value": "6. **Response to the question:**\n   - > \" There are a few relevant papers that might be worthwhile reading for more background, including our paper, https://www.nature.com/articles/s41467-021-23102-2, and one ours built on, https://www.sciencedirect.com/science/article/pii/S0047259X14001201?via%3Dihub.\"\n\nThank you for providing the references to your work and related papers [1][2]. We have carefully reviewed the suggested articles. Your contributions in the field are highly valuable, and we appreciate the opportunity to engage with relevant literature.\n\nIn our revised manuscript, we have expanded the evaluation section to include a more comprehensive set of contemporary techniques. Specifically, we compare our proposed method with nine distinct approaches, covering kernel-based methods like KPCA and KDA, manifold-based methods like UMAP and LLE, as well as LOL [1]. We also acknowledge your work in the Discussion section when discussing the pros and cons of PCA compared to our proposed method. We emphasize the computational efficiency associated with unsupervised dimension reduction methods, aligning with insights from your research [2].\n\nWe believe these revisions address your concerns and contribute to the overall improvement of the manuscript. We hope you find the updated version satisfactory, and we appreciate your continued engagement with our work.\n\nReference:\n\n[1]\tVogelstein, J. T., Bridgeford, E. W., Tang, M., Zheng, D., Douville, C., Burns, R., & Maggioni, M. (2021). Supervised dimensionality reduction for big data. Nature communications, 12(1), 2872.\n\n[2]\tShen, C., Sun, M., Tang, M., & Priebe, C. E. (2014). Generalized canonical correlation analy-sis for classification. Journal of Multivariate Analysis, 130, 310-322."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500756131,
                "cdate": 1700500756131,
                "tmdate": 1700500756131,
                "mdate": 1700500756131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lkWwp6ykEq",
                "forum": "anek0q7QPL",
                "replyto": "cp5FpclN1O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
                ],
                "content": {
                    "title": {
                        "value": "concern 1 and 2"
                    },
                    "comment": {
                        "value": "1. I am not convinced by the paragraph.  Showing a theoretical connection between univariate and multivariate theory would be required for me to be satisfied.\n\n2. Great!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685165508,
                "cdate": 1700685165508,
                "tmdate": 1700685165508,
                "mdate": 1700685165508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9HgiZWn2kW",
                "forum": "anek0q7QPL",
                "replyto": "YOaJEPud5E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_E7ZV"
                ],
                "content": {
                    "title": {
                        "value": "concerns 3-6"
                    },
                    "comment": {
                        "value": "3 & 4. Fig 2 is helpful.  However, i think the comment in the caption is not justified:\n\n> Notably, the proposed method consistently outperforms all other techniques, achieving the highest scores across all evaluation metrics.\n \nI cannot tell if this is strictly true.  It is not obviously worse.  My guess is that if you included errorbars, you'd conclude that it is never worse, and sometimes about the same as many of the other methods. \n\n5. I think PMI is related, it is often used in NLP, which is also tabular data and discrete classification choices (eg, 1 word).  But, I don't have anything more specific or meaningful to recommend.\n\n6. It seems like your method beats other nonlinear supervised dimensionality methods, possibly because it embeds in higher dimensions?  \n\nI like the revisions, and will increase my score accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685581905,
                "cdate": 1700685581905,
                "tmdate": 1700685581905,
                "mdate": 1700685581905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OLb2cbMgk5",
            "forum": "anek0q7QPL",
            "replyto": "anek0q7QPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_sPuV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7389/Reviewer_sPuV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach for improving binary classification. The authors propose integrating the eigenanalysis of the covariance and Hessian matrices to optimize class separability. The approach aims to maximize between-class mean distance and minimize within-class variances, following the principles of linear discriminant analysis (LDA). Empirical validation across various datasets supports the theoretical framework, demonstrating the method's superiority over traditional methods and LDA itself."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents compelling empirical evidence across various datasets, demonstrating the efficacy of the proposed method. The consistent positive results highlight the robustness of the approach in different contexts.\n\n2. The experimental results show that the proposed method outperforms traditional methods, including principal component analysis and the Hessian method. This indicates that the combined use of covariance and Hessian matrices can better capture the intricacies of data for binary classification."
                },
                "weaknesses": {
                    "value": "1. The paper does not clearly delineate its unique contributions. The authors should explicitly state what differentiates their work from existing literature, aiding readers in understanding the novelty and significance of the proposed method.\n\n2. The theoretical results presented in Section 2.2 need a more formal presentation. The authors should use mathematical statements and rigorous proofs to enhance the credibility and clarity of these results.\n\n3. The proposed approach, which integrates the eigenanalysis of the covariance and Hessian matrices based on binary cross-entropy loss, raises questions about its applicability to other loss functions. The authors should clarify this point or extend their methodology to include different loss functions.\n\n4. The paper compares the proposed method primarily with traditional methods. Including comparisons with more contemporary techniques would offer a more comprehensive view of the method's performance in light of recent advancements in the field."
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699276872534,
            "cdate": 1699276872534,
            "tmdate": 1699636884871,
            "mdate": 1699636884871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Td8HmA3KP7",
                "forum": "anek0q7QPL",
                "replyto": "OLb2cbMgk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sPuV (part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer sPuV,\n\nWe appreciate your thorough review of our manuscript, Submission7389, and thank you for providing valuable insights. \n\nWe have already provided a detailed response to your comments in the revised manuscript. Several modifications have been made to improve clarity, address concerns, and enhance the overall quality of the paper.\n\nBelow are our responses to your specific points:\n\n1. **Response to question 1:**\n   - > \"The paper does not clearly delineate its unique contributions. The authors should explicitly state what differentiates their work from existing literature, aiding readers in understanding the novelty and significance of the proposed method.\"\n\n- The contributions of our paper are explicitly stated in both the original and revised versions. In the Conclusion section, we highlight the multifaceted contributions, showcasing the theoretical insight and practical method presented in our work. \n\n- Additionally, we have added a paragraph at the beginning of the Discussion section in the revised manuscript to further underscore the uniqueness and significance of our contributions. In this paragraph we emphasize that our work provides a compelling theoretical insight and a powerful, practical method, demonstrating the strength of simplicity in achieving remarkable results. Firstly, we provide a profound theoretical insight, revealing a subtle yet powerful relationship between covariance and Hessian matrices. Our formal proof seamlessly links covariance eigenanalysis with the first LDA criterion while Hessian eigenanalysis with the second one. This unification under LDA criteria offers a fresh and intuitive perspective on their interplay. Secondly, capitalizing on this theoretical elegance and simplicity, we introduce a novel method that consistently outperforms established techniques across diverse datasets. The unexpected efficacy of our method, rooted in the straightforward relationship between covariance, Hessian, and LDA, showcases the effectiveness of simplicity in addressing complex challenges.\n\n2. **Response to question 2:**\n   - > \"The theoretical results presented in Section 2.2 need a more formal presentation. The authors should use mathematical statements and rigorous proofs to enhance the credibility and clarity of these results.\"\n\nWe understand your concern about the formal presentation of theoretical results in Section 2.2. The full proof, including mathematical statements and rigorous proofs, is provided in Appendix A due to space constraints. The proof sketches in Section 2.2 serve as concise summaries, with the complete and detailed proofs available in the appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165849660,
                "cdate": 1700165849660,
                "tmdate": 1700165849660,
                "mdate": 1700165849660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hN5N84ydln",
                "forum": "anek0q7QPL",
                "replyto": "OLb2cbMgk5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sPuV (part 2)"
                    },
                    "comment": {
                        "value": "3. **Response to question 3:**\n   - > \"The proposed approach, which integrates the eigenanalysis of the covariance and Hessian matrices based on binary cross-entropy loss, raises questions about its applicability to other loss functions. The authors should clarify this point or extend their methodology to include different loss functions.\"\n\nThe adaptability of our methodology to various loss functions is acknowledged as a potential avenue for future research. We have explicitly mentioned in the Discussion section of the revised manuscript that the mathematical derivation in our current work relies on the elegant relationship between (the Hessian of) binary cross-entropy loss and within-class variances. Exploring the adaptability of our method to different loss functions requires careful scrutiny in the future work to establish analogous connections.\n\n4. **Response to question 4:**\n   - > \"The paper compares the proposed method primarily with traditional methods. Including comparisons with more contemporary techniques would offer a more comprehensive view of the method's performance in light of recent advancements in the field.\"\n\n- Your suggestion to include comparisons with more contemporary techniques has been incorporated into the revised manuscript. In the evaluation setup, we expanded the comparison to include nine distinct methods, covering a broader spectrum of contemporary techniques, including kernel-based methods like KPCA and KDA, manifold based methods like UMAP and LLE as well as LOL [1]. \n\n- We have included a note about introducing nonlinearity through distinct kernels determined by grid search in KPCA and KDA for each dataset. \n\n- The results, discussed in the revised manuscript, demonstrate the consistent superiority of our approach across various datasets and metrics.\n\n- These extended results are completely reproducible using the same Colab notebooks whose links are provided for transparency and validation.\n\n- The relevant discussion for these additional comparisons has been appropriately incorporated into the Discussion section of the revised manuscript.\n\n- The abstract has also been updated to reflect the implications of these extended comparisons.\n\nWe believe these revisions address your concerns and contribute to the overall improvement of the manuscript. We hope you find the updated version satisfactory, and we appreciate your continued engagement with our work.\n\nReference:\n\n1.\tVogelstein, J. T., Bridgeford, E. W., Tang, M., Zheng, D., Douville, C., Burns, R., & Maggioni, M. (2021). Supervised dimensionality reduction for big data. Nature communications, 12(1), 2872."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166454692,
                "cdate": 1700166454692,
                "tmdate": 1700166815454,
                "mdate": 1700166815454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9jyq49PVWb",
                "forum": "anek0q7QPL",
                "replyto": "hN5N84ydln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_sPuV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_sPuV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. While the current version has its own merits, I believe it necessitates substantial improvements to be ready for publication. For instance, the theoretical results in Section 2.2 should be presented more mathematically, ideally structured as theorems. Furthermore, the Discussion section should be more concise and focused. Additionally, please note that the main part of the submission has exceeded the page limit."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634093102,
                "cdate": 1700634093102,
                "tmdate": 1700634093102,
                "mdate": 1700634093102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TiMOXe9Vlv",
                "forum": "anek0q7QPL",
                "replyto": "l7sVpjvjNc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_sPuV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7389/Reviewer_sPuV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\nUpon reviewing your revised manuscript, I still have several concerns regarding your theoretical results. For instance, in Theorem 2, the equation $H_\\theta = \\frac{1}{\\sigma_{post}^2}$ isn't rigorously established in the proof. The approximation of Fisher information is used. Additionally, there are multiple assumptions used in the proof that aren't explicitly reflected in Theorem 2.\n\nWhile I appreciate the efforts to revise the manuscript, I believe it still needs significant work before it's ready for publication. As such, I see no reason to change my initial rating at this point."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712710669,
                "cdate": 1700712710669,
                "tmdate": 1700712710669,
                "mdate": 1700712710669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]