[
    {
        "title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video"
    },
    {
        "review": {
            "id": "UryWVVBzLM",
            "forum": "Yen1lGns2o",
            "replyto": "Yen1lGns2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates self-supervised representation learning from video with a specific focus on the data distributions. In particular, the paper questions the needs of using large-internet scale image datasets and propose instead to learn representation by watching few long videos.\n\nThe paper makes two mains contributions:\n-\tThe WTtour datasets which composed by 10 long-videos \n-\tDORA, a self-supervised representation learning approach that learns to represent and track object in a video at the same time.\n\nThe paper evaluates the learned representations on various downstream tasks including ImageNet linear probing, Pascal VOC unsupervised object discovery, MS-COCO/ADE20K object detection/segmentation and DAVIS-2017 for video understanding. DORA pretrained on WTours demonstrates strong performances on ADE20K and MS-COCO"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper explores the pretraining of visual representation using few long videos which is an original and novel empirical setting.\n\n- They propose a new datasets WTours which could be of interest to the representation learning community.\n\n-  Dora obtains reasonable performance when fine-tuning the performances on ADE-20K and MS-COCO."
                },
                "weaknesses": {
                    "value": "- Performance on ImageNet linear probing seems low. While I understand that video-pretrained models have a disadvantage over image-pretrained model as they can\u2019t be pretrain \u2018in-distribution\u2019 with respect to imagenet. However, ImageNet is a standard vision task. It is important to understand why there is such a gap between image and video models on this evaluation.\n\n- The DINO baseline is trained for 100 epochs only which is not the default setting. Additionally, DINO paper reports a performance of 61.8 with a VIT.S/16 on Davis while the paper reports of 54.6 for the same method. I would encourage the authors to report what are the performances of the DINO released models as those models are available.\n\n- DORA shares some similarity with the VITTO. Both approaches learn from video and use an 'unsupervised' pooling mechanism. However, DORA seems to underperform VITTO on the ADE20K and MS-COCO tasks. \n\n- Missing comparison with more recent baselines. It would be nice to add comparison with DINOv2 and a weakly-supervised baseline  OpenCLIP, which are both state-of-art methods."
                },
                "questions": {
                    "value": "I like the motivation and the novel exploration of the paper. However, I think the experimental evaluation could be improved to better support the claims of the paper. \n\nFirst, I think comparing with state-of-art image-baseline such as DINOv2 and CLIP on the different tasks would really highlight the importance of video pretraining. Second, I think it would be useful to discuss in depth the relation with the VITTO approach. Finally, the current approach currently falls short of image-pretrained model on ImageNet. It would be nice if the author could discuss this limitation in the manuscript."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763183113,
            "cdate": 1698763183113,
            "tmdate": 1700508085799,
            "mdate": 1700508085799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PgLAKvpHmH",
                "forum": "Yen1lGns2o",
                "replyto": "UryWVVBzLM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GQ4C"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer GQ4C's valuable feedback. We address the concerns as follows:\n\n**1. Linear probing performance**\n\nPlease refer to *Common response to R-DfLs, R-CACu, R-GQ4C: Results on Imagenet linear probing*.\n\n**2. DAVIS numbers**  \n\nDINO (Caron et al.) reports a performance of 61.8 on DAVIS using ViT-S/16 when pretrained on *ImNet* for 300 epochs. In our work, we report that DINO achieves a performance of 54.6 on DAVIS when pretrained on *WT-Venice* for 100 epochs. The difference in performance is from the different pretraining dataset and higher number of pretraining epochs.\n\nIn Section C under ``Longer pretraining\" and Table 7 in the Appendix, we have added the comparison of DoRA vs DINO when pretrained for 300 epochs on WT-Venice and WT-all.\n\n**3. DoRA vs VITO**  \n\nVITO is pretrained for 300 epochs using Resnet-50 on VideoNet: a curated dataset of 1 million videos whose distribution is similar to that of ImNet. While DoRA is pretrained for 100 epochs using ViT-S/16 on 1 or 10 long uncurated WTour videos whose distribution is different from ImNet.\n\n**4. Comparison with Dino-V2 and OpenCLIP**\n\nThanks for this suggestion. We will add experiments with DINO-v2 and OpenCLIP in the camera ready version"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230067925,
                "cdate": 1700230067925,
                "tmdate": 1700230067925,
                "mdate": 1700230067925,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1dJ0J0AJOF",
                "forum": "Yen1lGns2o",
                "replyto": "PgLAKvpHmH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers. The rebuttal did address most of my concern and I will update the paper score accordingly. \n\nI would encourage the author to add DINO-v2, OpenCLIP baselines for camera ready.\n\nThanks!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507532542,
                "cdate": 1700507532542,
                "tmdate": 1700507532542,
                "mdate": 1700507532542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HgpHHR9IeZ",
            "forum": "Yen1lGns2o",
            "replyto": "Yen1lGns2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_krCr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_krCr"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a method for self-supervised learning of image representation models. It is based on a similar scheme to DINO (Caron et. al. 2021) that learns image representation by distilling a moving average teacher model's representation of the global image view to the representation of a student model's on multiple local views. The novel way of achieving this in this work is to use the feature correspondence provided in hours-long walking tour videos to provide tracking, which can identify the location of different objects in any video frame without annotation. This object-centric way of generating local views seems to lead to good learned representation, which is examined in the experimental section.\n\nIn total, the proposed method does not need curated image or video data for self-supervised learning and can learn effective representation from hours-long walking tour videos. The proposed multi-object masking approach is shown to be significant in learning effective visual representation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The fact that this method does not use curated data is a big plus for me. The way of collecting this type of data seems to be easily scalable, as it can be from walking tours or vehicle-mounted cameras. \n\n+ Using correspondence-based tracking to provide localization is sound and practical in representation learning. \n\n+ The experimental results show the model trained with the proposed method on the walking tour videos can outperform strong baselines trained on curated datasets such as ImageNet and Kinetics-400."
                },
                "weaknesses": {
                    "value": "- One minor issue I have about the presentation is the introduction of the tracking module. The tracker is not learned, and it is only used to provide object locations. I would like further discussion on the potential use of the corresponding information. Also, a comparison on the effect of using different types of unsupervised trackers would also help strengthen the work as the major idea seems to be not dependent on a certain type of tracker."
                },
                "questions": {
                    "value": "I have the following questions after reading the text:\n\n1. In Eq. (8) the learning is done on a single frame. Because the tracking has already provided corresponding between locations across multiple frames, is there a certain consideration to not use views from multiple frames in this loss function?\n\n2. The authors have presented 10 walking tour videos. The results in Table 5 suggest training on one video already achieves similar accuracy obtained by training on all videos. Does this mean one video is sufficient? Is there some point in further scaling up the training data? I would like to see a discussion on this topic."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814058417,
            "cdate": 1698814058417,
            "tmdate": 1699636163895,
            "mdate": 1699636163895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "50D0PgxMZI",
                "forum": "Yen1lGns2o",
                "replyto": "HgpHHR9IeZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer krCr"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer krCr's valuable feedback. We address the concerns as follows:\n\n**1. Using unsupervised trackers**. \n\nThanks for this interesting suggestion. We shall integrate UnSupTrack [1*] with DoRA. We shall first detect the objects using our proposed multi-object masking method, which we then use as input to UnSupTrack. We shall add this result in the camera-ready version.\n\n[1*] Karthik *et al.*, Simple Unsupervised Multi-Object Tracking, ECCV 2020\n\n**2. Views from multiple frames**. \n\nThanks for the interesting question. We will apply the loss function between the global crop of the teacher network (from the reference frame $t_0$ that is used in tracking) to multi-object crops of the student network from all other frames $t$ in the mini-batch. Due to time and compute constraints, we will add this experiment to the camera-ready version.\n\n**3. WT-all vs. WT-1vid**  \n\nPlease refer to *Common response to R-DfLs, R-CACu, R-krCr: WT-all vs. WT-venice (1 video)*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229792065,
                "cdate": 1700229792065,
                "tmdate": 1700229792065,
                "mdate": 1700229792065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mIC1yUut2W",
                "forum": "Yen1lGns2o",
                "replyto": "KrldOkyOyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_krCr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_krCr"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses to my questions. My question regarding the data scalability is addressed.  I want to maintain my initial rating. \n\nI would be curious to see the results of the multi-frame experiments in the revised version."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711803237,
                "cdate": 1700711803237,
                "tmdate": 1700711803237,
                "mdate": 1700711803237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vfRXQvOmOn",
            "forum": "Yen1lGns2o",
            "replyto": "Yen1lGns2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_CACu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_CACu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new perspective on self-supervised learning (SSL). Instead of pretraining models on ImageNet-like object-centric datasets, the paper pretrained the models on egocentric videos (\u201cWalking Tours\u201d dataset) which depict numbers of objects and are comparable with human learning. Compared with other video datasets for SSL, Walking Tour dataset had more objects and classes and more gradual shifts in lightness. To pretrain on Walking Tours, the paper proposed a novel SSL method, based on DINO, to first discover objects and then track objects, named DoRA. In every batch, DoRA randomly sampled 8 frames temporally separated by 1 second, discovered objects in the first frame, and tracked them over the following 7 frames. In the default setting, objects are tracked by cross-attention in the multi-object tracker, which leads to spatially overlapping. The paper then proposed to establish object-patch correspondences using the Sinkhorn-Knopp algorithm to deal with the problem. After finding separated objects, the input video clip in the student branch would be masked to contrast with the clip in the teacher branch. Experiments on dense prediction tasks show that DoRA on Walking tours shows comparable performance with other SSL methods pretrained on ImageNet."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper proposed a new pretraining method on egocentric videos which are uncurated and comparable with human learning. As these videos do not involve human annotation, they can be easily obtained, leading to a promising way of SSL.\n2.\tThe proposed method is simple and neat. DoRA provides an intuitive but effective way to learn from frames that contain multiple objects."
                },
                "weaknesses": {
                    "value": "1.\tThe paper mainly talks about how we can learn discriminative representations from egocentric videos, whereas what kind of egocentric videos are suitable for DoRA is not deeply discussed. It would contribute more to the community if we knew what properties a video should have to be worth learning.\n2.\tA good SSL method should be scalable, not only on the dataset but also on the model structure. It would be better for authors to show more results on larger ViTs.\n3.\tSome minor writing problems. (1) in Sec. 4 \u201cDiscovering objects with multi-head attention\u201d, $\\widetilde{Q}$, $\\widetilde{K_t}$ are only defined in Fig.3 and are not defined in text. (2) In Fig.3 (Left), the input should be $X_t^{o_i}$"
                },
                "questions": {
                    "value": "1. In Table 4, why does DoRA perform worse when using WT_all than when using WT_Venice?\n2. DoRA shows inferior performance on ImageNet linear probe (LP) but superior performance on dense prediction tasks, would it perform better than other contrastive methods on the ImageNet fine-tuning task? Like MAE [1], lower on LP but higher on fine-tuning.\n\n[1] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Reviewer_CACu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832001681,
            "cdate": 1698832001681,
            "tmdate": 1699636163814,
            "mdate": 1699636163814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O2uxRwkHkb",
                "forum": "Yen1lGns2o",
                "replyto": "vfRXQvOmOn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CACu"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer CACu's valuable feedback. We address the concerns as follows:\n\n**1. Type of egocentric videos**. \n\nIn Table 2(a), we show that the performance of DoRA is not specific to egocentric videos, *e.g.*, WTours and Epic-Kitchens (EK), but also achieves good performance on diverse pretraining datasets like Kinetics-400 (K-400) and Movie videos (Movie$_{\\text{rom}}$). Thus, we observe that DoRA is agnostic to type of video pretraining dataset. Our argument for videos like WTours is that they can collected or filmed very easily.  \n\n**2. Using larger ViT**. \n\nPlease refer to *Common response to R-DfLs, R-CACu: Using larger ViT*\n\n**3. WT-all vs. WT-Venice**  \n\nPlease refer to the response in *Common response to R-DfLs, R-CACu, R-krCr: WT-all vs. WT-venice (1 video)*.\n\n**4. Linear Probing**  \n\nPlease refer to the response in *Common response to R-DfLs, R-CACu, R-GQ4C: Results on Imagenet linear probing*.  \n\n**5. Minor corrections in Figure 3**\n\nThanks for pointing this out. We have corrected it in the revised version. \nWe define $\\tilde{Q} \\in \\mathbb{R}^{n \\times d}$ in the sentence above eq(2) and $\\tilde{K} \\in \\mathbb{R}^{n \\times d}$ in the sentence below eq (3)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229526679,
                "cdate": 1700229526679,
                "tmdate": 1700231416357,
                "mdate": 1700231416357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LfLYBJv4GP",
            "forum": "Yen1lGns2o",
            "replyto": "Yen1lGns2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a dataset *Walking Tours (WT)* consisting of high definition street-walk videos of 23 hours total length, and DoRA, a multi-object-tracking-inspired framework to learn visual representation from different views of the same objects in adjacent frames. The proposed method largely follows the DINO framework, with the local crops in DINO replaced with a tracking of objects in a video. The method is tested on several mainstream visual tasks, including image detection and segmentation, video object segmentation, object tracking, image classification and object discovery. Compared to its baseline DINO, the proposed method outperforms it clearly when using the same WT dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The motivation to learn visual representation using of the appearance variation of object along time is sound and is probably worth exploring in the future as an additional source of information in self-supervised visual learning.\n\n* The proposed architecture to utilize temporal information by tracking the same object in different frames is novel, and the visualization can justify that the proposed method is working as expected."
                },
                "weaknesses": {
                    "value": "* **Scalability concern.** Although the proposed method does outperform DINO on all reported experiments in controlled experiments (i.e., with the proposed WT dataset), the results with even WT_{all} is still not clearly better than DINO with ImageNet-1k. This leaves it unknown whether the WT dataset will eventually outperform ImageNet-1k (or the even larger ones like ImageNet-22k and LVD-142M) with the dataset at a reasonable scale. Also the experiments are mostly on ViT-S, which is relatively small compared to the well-known works in the field (which usually report at least ViT-B), so it is also hard to tell whether the proposed method scale well with the model size.\n\n* **Significantly worse results on image classification.** I have noticed that the image classification results of WT-pretrained models are lower than ImageNet-1k-pretrained by a fairly large margin (45LP / 36KNN on WT vs. 72LP / 70KNN on ImageNet). Although one can argue that this is because of the domain gap between WT and IN, I would consider the accuracy difference large enough to require some formal justification (e.g., run WT and IN pretrained models on a 3rd classification dataset like iNaturalist or Places) to conclude that the WT-pretrained models are not significantly weaker on image classification tasks.\n\n* **The potential privacy and safety issues of the dataset.** Also see *Details Of Ethics Concerns*. As the paper claims the dataset as a main contribution, I would expect more efforts in assessing the privacy and safety issues in the dataset (e.g., How many clear faces are detected and what are their resolutions? How many harmful scenes are detected to the best effort of the authors?) and clarifying the legal issues and usage restrictions of the dataset (e.g., Is it possible that some videos are taken down upon the request from people appearing in them? Is their usage in some jurisdictions not allowed / limited to non-commercial only? What are some possible negative effects if the models remember the private information in it? What are the possible effects of some common mitigations, like blurring the faces in Google street view?)"
                },
                "questions": {
                    "value": "* In addition to training epochs, it would be kind to also mention the actual training time as a more practical measurement of the training cost.\n\n* In Appendix D, are there any other differences between DoRA without tracking and DINO other than the crop generation method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper proposes a new dataset consisting of 23 hours of UHD (4k) videos filmed on the public streets which may contain personal information like high resolution faces of strangers and audio recordings of the nearby people talking (very likely) without their consent. Although the videos are not filmed by the authors themselves and are in CC-BY licenses on YouTube according to the paper, I'm concerned that it needs a careful discussion regarding the compliance issues or restrictions of using them for machine learning purposes (or even posting them on YouTube in the first place) in different jurisdictions.\n\nThere is also no assessment or mitigation about the potentially harmful scenes (e.g., violent, harassing, criminal) in the proposed dataset."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs",
                        "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839111545,
            "cdate": 1698839111545,
            "tmdate": 1700712460331,
            "mdate": 1700712460331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1VXJOx2fVG",
                "forum": "Yen1lGns2o",
                "replyto": "LfLYBJv4GP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DfLs"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer DfLs's valuable feedback. We address the concerns as follows:  \n\n**1. Stability**\n\nPlease refer to the response in *Common response to R-DfLs, R-CACu, R-krCr: WT-all vs. WT-venice (1 video)*.\n\n**2. Larger ViT.**  \n\nPlease refer to the response in *Common response to R-DfLs, R-CACu: Using larger ViT*.\n\n**3. Linear probing**  \n\nPlease refer to the response in *Common response to R-DfLs, R-CACu, R-GQ4C: Results on Imagenet linear probing*.  \n\n**4. Potential privacy and safety issues of the dataset**  \n\nThe reviewer makes a very good point. To address this concern, we use Deface (https://github.com/ORB-HD/deface) to automatically detect and blur faces in WT videos. Using these modified WT videos, we apply DoRA on WT-Venice. We shall report the results once pretraining is completed.\n\n**5. Training Cost**  \n\nPlease refer to the response in *Common response to R-YYAs, R-DfLs: Training costs*\n\n**6. Difference between DoRA w/o tracking and DoRA with tracking.**  \n\nIn Appendix D, we apply equation (6) on a single image rather than a set of frames, i.e., there is no tracking involved when DoRA is pretrained on ImNet. Thus, other than the crop generation method, there is no other difference between DoRA* (DoRA without tracking) and DINO."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228971597,
                "cdate": 1700228971597,
                "tmdate": 1700229409901,
                "mdate": 1700229409901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "THfCV6jtEe",
                "forum": "Yen1lGns2o",
                "replyto": "LfLYBJv4GP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses from the authors and the comments from the other reviewers.\n\nMy concerns regarding the technical issues are mostly addressed: Despite that not all experiments are finished in the short period of the rebuttal, I do believe the method looks more promising with the current information and would raise my rating based on the improvements.\n\nI also appreciate very much the authors' efforts to resolve the ethical issues. Given that it has received emphasis only in the recent one or two years, I actually do not expect some specific experiments, but consider it good enough to give some general discussions about the best practices or the potential risks as a reminder for the future users of the dataset.\n\nConsidering the factors above I have raised the score to a weak accept."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714136194,
                "cdate": 1700714136194,
                "tmdate": 1700714136194,
                "mdate": 1700714136194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ytDhdexGqR",
            "forum": "Yen1lGns2o",
            "replyto": "Yen1lGns2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider how both (a) data and (b) method can improve training an image encoder in a self-supervised manner. Regarding data, they introduce an open-source dataset containing long, first-person videos, and propose several advantages of this over curated image (and video) datasets. Given this dataset contribution, the authors propose a self-supervised method which tracks objects to act as a signal for a classical multi-view self-supervised learning (SSL) loss. This method leverages some key properties from the dataset, specifically that the videos have natural scene transitions and are of a person-person view. Instead of using off-the-shelf object-trackers or optical flow to establish correspondence, the authors use the attention-map between the [CLS] token from a selection of heads and the patch-embeddings. Optimal transport is used to establish unique object-patch correspondence (i.e. non overlapping patches) and then given this multi-view correspondence, a technique based on DINO is used.\n\nOverall the authors show that for many downstream tasks, their method (DORA) pre-trained on (even one) video where the number of frames is comparable to imagenet-1K (IN-1K) achieves better performance than DINO pretrained on IN-1k. In comparison when DINO is pretrained on the same data, the peformance is worse than IN-1K which suggests it is the unique coupling of dataset and training-method which helps the authors achieve SOTA results"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and the supplementary work provides a good level of detail and convincing ablations and visualisations\n- The contribution of the open-source dataset (10 videos) to replicate this work is very useful for the community\n- The statistical analysis of the contributed dataset also useful\n- This is a nice use-case of Sinkhorn\u2013Knopp to avoid having to use non end-to-end approaches like optical-flow or off-the-shelf object-detectors and the motivation which shows overlapping spatial regions when linearly projecting the attention-map (instead) is a good argument for its use\n- Overall, the results with DORA are very impressive"
                },
                "weaknesses": {
                    "value": "- I'm a bit confused about Table 4: Video Object Segmentation (DAVIS-2017). In the DINO paper they report ViT-S/16 with INet getting 61.8, 60.2, 63.4 respectively (their Table 5), however your Table 4 reports DINO as getting 59.4, 57.4, 61.4 what accounts for this difference?"
                },
                "questions": {
                    "value": "- What is the stability like when using SK, aside from the entropy regularisation is some annealing schedule needed that transforms the coupling matrix from soft to hard gradually?\n- Is there any intuition why the k-attention maps obtained by projecting the attention map are spatially overlapping? Is there any possibility to use a simple heuristic to avoid it that can be ablated with SK?\n- What is the training cost of using SK for every forward-pass like this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699009476598,
            "cdate": 1699009476598,
            "tmdate": 1700577071014,
            "mdate": 1700577071014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sXfSWSNBuL",
                "forum": "Yen1lGns2o",
                "replyto": "ytDhdexGqR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YYAs"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer YYAs's valuable feedback. We address the concerns as follows:\n\n**1. Difference in VOS numbers.** \n\nIn Table 5 (Caron et al.), the authors evaluate DINO (ImNet) for *300 epochs* for Video Object Segmentation on DAVIS 2017. In Table 4 of DoRA, we reproduce DINO (ImNet) for *100 epochs*. Due to the difference in training epochs, we observe a difference in performance 59.4 (100 epochs) vs. 61.8 (300 epochs).  \n\n**2. Stability of SK**  \n\nSK is an iterative optimization algorithm that indeed converges, with its cost function monotonically decreasing with the number of iterations.\n\nWe understand that the reviewer asks about training stability when using SK---if not, please clarify. We empirically find that, when using SK with the features from the last layer of the transformer to compute refined object prototypes $P'$, training is indeed stable.\n\nUsing annealing to transform the coupling matrix from soft to hard might be needed if we target hard assignment between object prototypes and patch features. Hard assignment can also be achieved by using a smaller value of $\\epsilon$ (coefficient of entropy regularizer), which improves one-to-one matching, although it makes optimization harder. We evaluate the performance of DoRA with a smaller value of $\\epsilon$ and increasing the number of iterations to 60 (default iterations is 30), observing sub-optimal results on downstream tasks. This indicates that downstream task performance does not benefit from hard assignment.  \n\n**3. Overlapping attention maps.**  \n\nThe overlap of the attention maps obtained from different heads is commonly observed as there is no supervisory signal during training to ensure diverse attention from different heads. For example, see Fig. 10 of DINO (Appendix), where the authors show overlap for the different heads in the last layer.  \n\nIn our previous experiments, we used a simple heuristic inspired from CutLer [2*]. This is an unsupervised object discovery method that iteratively uses normalized cuts on patch affinity matrix to find foreground objects. Similarly, we iteratively removed attended regions to find more, non-overlapping ones. However, on WT videos, we observed that this heuristic did not achieve consistent improvements on all downstream tasks. In particular, its results were sub-optimal on linear probing on Imagenet and unsupervised object discovery on Pascal VOC.\n\n[2*] Wang *et al.*, Cut and Learn for Unsupervised Object Detection and Instance Segmentation, CVPR 2023.  \n\n**4. Training Cost**\n\nPlease refer to the response in *Common response to R-YYAs, R-DfLs: Training costs*"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228555724,
                "cdate": 1700228555724,
                "tmdate": 1700228555724,
                "mdate": 1700228555724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lJx3xksntE",
                "forum": "Yen1lGns2o",
                "replyto": "kuvA7G6uN0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for the clarification regarding your reproduction of DINO and further comments about the stability of the SK algo. I have increased my rating from weak accept to accept."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2314/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577127766,
                "cdate": 1700577127766,
                "tmdate": 1700577127766,
                "mdate": 1700577127766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]