[
    {
        "title": "Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation"
    },
    {
        "review": {
            "id": "yOMnCTLUS8",
            "forum": "d3xKPQVjSc",
            "replyto": "d3xKPQVjSc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_FevT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_FevT"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses a setting where a representation function $\\phi(X)$, (a generalization of propensity score $\\pi(X)$), is available while part of X is unobservable. That is, instead of following the typical approach of choosing X from only observables (expecting $\\phi(X)$ to be a balancing score) and discussing the potential effects of unobservable covariates, they follow the approach of considering X as all covariates including even unobservable covariates. At the same time, they assume that $\\phi(X)$ value is available (while part of X is not observable)\n\nIn the typical approach, when we cannot assume that $\\phi(X)$ is a balancing score, we may suffer confounding bias. In the exact same way in their approach, when we cannot assume that $\\phi(X)$ (in their case it is called the representation function) includes enough information about unobservable covariates, we may suffer a bias. (in their case it is called the representation-induced confounding bias or RICB)\n\nThe authors identify RICB, and propose a technique to estimate the bound.\nComprehensive simulation studies were followed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Simulation studies are quite comprehensive.\nTheoretical bounds has been proposed. \nThe paper is very well written. It was pleasant to read."
                },
                "weaknesses": {
                    "value": "\\textbf{1. Motivation of their approach}\n\nAs discussed in the Summary part of this review, for me it was hard to understand why we need a new approach of choosing X. The concept of RICB is, in essence, equivalent to confounder bias but formulated in a different choice of X. For example, in the traditional way of choosing X as only unobservables and talking about $\\phi(X)$ not being a balancing score, potential effect of unobservable covariates not being included as X can be discussed. So I am not sure about the potential benefit of choosing X to include unobservable covariates.\n\n\\textbf{2. Bounds}\nTheoretical bounds provided should be appreciated, but I cannot be sure how strong this theoretical bound is only from current version of the manuscript."
                },
                "questions": {
                    "value": "In terms of Weakness 1: Could you please give a clear motivation of choosing X to include unobservable but considering $\\phi(X)$ as observable, and then reframing the confounder bias we know as RICB in the newly proposed setting? Does the fact that we are dealing with representation learning make some difference? I just want to try to understand.\n\nIn terms of Weakness 2: How tight is the bound for some popular special cases, especially for the settings you did your experiment? Are they practically good?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Reviewer_FevT"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I went to check out [4] Alicia Curth and Mihaela van der Schaar. On inductive biases for heterogeneous treatment effect estimation. Advances in Neural Information Processing Systems, 2021a. Here is the assumption 1 I found.\n\nAssumption 1. [Consistency, unconfoundedness and overlap] Consistency: If individual i is assigned treatment $w_i$, we observe the associated potential outcome $Y_i=Y_i\\left(w_i\\right)$. Unconfoundedness: there are no unobserved confounders, so that $Y(0), Y(1) \\Perp W \\mid X$. Overlap: treatment assignment is non-deterministic, i.e. $0<\\pi(x)<1, \\forall x \\in \\mathcal{X}$.\n\nI think they don't assume that all confounders are observed inside X.\n\nI believe that the authors not only are failing to answer the question, but they tried to support the motivation of their setting by providing a misinformation. I strongly believe that this paper should be rejected."
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437071859,
            "cdate": 1698437071859,
            "tmdate": 1701036460587,
            "mdate": 1701036460587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "exdNyAdj4w",
                "forum": "d3xKPQVjSc",
                "replyto": "yOMnCTLUS8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FevT [1 / 2]"
                    },
                    "comment": {
                        "value": "Thank you! We appreciate your valuable review and that you found our paper well-written, and experiments comprehensive. In the following, we will respond to your questions.\n\n**Response to Weaknesses and Questions.**\n\n\n\n1. **Motivation of our approach.** We are more than happy to clarify the motivation behind our approach.  \n\n  + _What is the motivation behind our approach?_ In our paper, we build upon the task of CATE estimation. That is, we follow previous literature [1-7] and assume that **all the confounders** are **observed** inside of $X$. Hence, when using representation learning methods, one maps an $X$ onto lower-dimensional representation $\\Phi(X)$. Formally, $X$ is $n$-dimensional and $\\Phi(X)$ is $p$-dimensional with $p &lt; n$. Then, by learning the representation $\\Phi(X)$, we face an information loss as not all information about $X$ can be stored in the representation. In simple words, $X$ is now split into (i) the main component $\\Phi(X)$ which is used for the downstream analysis and (ii) an implicit (complementary) component $\\Omega(X)$ that captures everything else. Importantly, not only $X$ is observed, but also both components $\\Phi(X)$ and $\\Omega(X)$ are observed or could be inferred. However, CATE estimation using representation learning is only based on component (i) and not component (ii), (which is discarded). In the language of our paper, **component (ii) is all the information, discarded by the representation, which could contain ground-truth confounders and, thus, lead to bias**. \n\n\n   +  _How is our approach related to traditional, hidden confounders?_ An alternative view of component (ii) is to build upon causal sensitivity analysis and simply say that these are essentially \u2018non-observable\u2019 confounders. The reason is that, even though they are observed, they are actually not used in existing CATE estimation methods. As a result, **CATE estimation with a lower-dimensional representation leads to a confounding bias**. We highlight the differences between (a) traditional, hidden confounding and (b) our setting in the following table.\n  | Setting / Application            | Observed variables | Unobserved variables | Ignorability            | Confounding induced by the usage of |\n  |----------------------------------|--------------------|----------------------|-------------------------|-------------------------------------|\n  | Hidden confounding               | $X$                | $U$                  | $Y[a] \\mathrel{\\unicode{x2AEB}} A \\mid X, U$ | $X$                                 |\n  | Representation learning for CATE | $X, \\Phi(X)$       | \u2014                    | $Y[a] \\mathrel{\\unicode{x2AEB}} A \\mid X$    | $\\Phi(X)$                           |\n\n   + _Why is our approach relevant and general?_ As we show in our paper, such bias is inherent to **almost all** existing CATE methods that build upon representation learning. This just happens naturally because learning a lower-dimensional representation leads to information loss and thus confounding bias. Oftentimes, such confounding bias is reinforced by the learning objective (e.g., balancing), which presents further sources of bias. Of note, there is no \u2018easy fix\u2019 (or \u2018no free lunch\u2019) to the confounding bias from representation learning. On the one hand, CATE based on representation learning is actively searching to learn a lower-dimensional representation as this has lower variance and generalizes better. So, we actively aim for $p &lt; n$ (and not $p \\approx n$). Yet, this leads to confounding bias. On the other hand, we could remove confounding bias by setting $p \\approx n$, yet this would preclude any gains in prediction performance from representation learning. \n\n       To avoid confusion, we would also like to clarify the following. The bidirectional edge in Fig. 1 between $Y$ and $X$ is not used to indicate hidden confounding wrt. to treatment $A$, but rather to denote the general causal diagram, implied by assumptions (1)-(3). \n   \n&nbsp; &nbsp; &nbsp;  **Action:** We added clarifications along the above lines to our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302227293,
                "cdate": 1700302227293,
                "tmdate": 1700304860928,
                "mdate": 1700304860928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oret3HGWl0",
                "forum": "d3xKPQVjSc",
                "replyto": "yOMnCTLUS8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. **Bounds.** Thank you for raising this important question. We added a **new Corrolary 1** in **Appendix B,** where we state that bounds are **valid and sharp** (in the sense of sensitivity models). This result was formally proved in [8]. This implies the following: \n   * Valid means that our bounds always contain the ground-truth representation CATE (given the ground-truth representation-conditional distribution of the outcome). \n   * Sharp means that the bounds only include the observational distributions complying with a sensitivity constraint from Eq. 10.  \n\n    We empirically evaluated validity and sharpness jointly. E.g., the reported results in Tables 2-4 and Figures 4-6 showed improvements over the baselines due to our framework. Therein, a drop in error rates implies validity, and the fact that we defer not too many observations/individuals leads to sharpness.\n\n\n    A further question is how tight our bounds are, yet tightness is an empirical quantity that depends on both the fitted CATE estimator and a chosen sensitivity model. Hence, we cannot assess tightness theoretically but only through empirical analysis. To do so, we added new **Figures 7 and 8,** where we plot the ground-truth decision boundaries and decision boundaries based on the estimated bounds for the synthetic benchmark. We see that estimated bounds are valid (they contain the ground-truth decision boundary of CATE wrt. representations) and sufficiently tight, especially, when the representation is of the same dimensionality as the covariates (so there is less chance of the presence of the RICB). This confirms the effectiveness of our bounds.  \n\n    **Action:** We added a new **Corrolary 1 **in** Appendix B** on the properties of the MSM bounds to the revised version of the paper. We added **Figures 7-8** where we visualize the bounds (ground-truth and estimated decision boundaries) to show that our bounds are valid and tight. \n\n\n \\\n**References:** \n\n[1] Ahmed M. Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment effects using multi-task Gaussian processes. In Advances in Neural Information Processing Systems, 2017.\n\n[2] Ahmed M. Alaa and Mihaela van der Schaar. Bayesian nonparametric causal inference: Information rates and learning algorithms. IEEE Journal of Selected Topics in Signal Processing, 12:1031\u20131046, 2018a.\n\n[3] Ahmed M. Alaa and Mihaela van der Schaar. Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design. In International Conference on Machine Learning, 2018b.\n\n[4] Alicia Curth and Mihaela van der Schaar. On inductive biases for heterogeneous treatment effect estimation. Advances in Neural Information Processing Systems, 2021a.\n\n[5] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In International Conference on Artificial Intelligence and Statistics, 2021b.\n\n[6] Uri Shalit, Fredrik D. Johansson, and David Sontag. Estimating individual treatment effect: Generaliization bounds and algorithms. In International Conference on Machine Learning, 2017.\n\n[7] Fredrik D. Johansson, Uri Shalit, Nathan Kallus, and David Sontag. Generalization bounds and representation learning for estimation of potential outcomes and causal effects. Journal of Machine Learning Research, 23:7489\u20137538, 2022.\n\n[8] Dennis Frauen, Valentyn Melnychuk, and Stefan Feuerriegel. Sharp bounds for generalized causal sensitivity analysis. In Advances in Neural Information Processing Systems, 2023."
                    },
                    "title": {
                        "value": "Response to Reviewer FevT [2 / 2]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302400418,
                "cdate": 1700302400418,
                "tmdate": 1700302452379,
                "mdate": 1700302452379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "64yud8sH9Y",
            "forum": "d3xKPQVjSc",
            "replyto": "d3xKPQVjSc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_8op8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_8op8"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of confounding bias induced by learning representation of confounder for CATE estimation. The authors proposed a framework for estimating bounds on the induced confounding bias. A neural framework is used to compute the bounds."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents a problem that is novel and related to the representation of learning for CATE, which is a prominent research direction.\n- A detailed analysis of representation-induced bias is provided.\n- Both real-world and synthetic experiments are performed with the proposed framework."
                },
                "weaknesses": {
                    "value": "- The motivation for employing CDAG is not quite clear. \n- No theoretical proof of the proposed bounds."
                },
                "questions": {
                    "value": "- Could you provide some intuition about learning the representation of all the covariates together instead of the confounder?\n- If learning representation of covariates inducing bias is unavoidable, how does the bias compare with bias due to finite-sample? e.g., How does it compare with the non-representation learning approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Reviewer_8op8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735322392,
            "cdate": 1698735322392,
            "tmdate": 1699636587057,
            "mdate": 1699636587057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2m2lac3Nqy",
                "forum": "d3xKPQVjSc",
                "replyto": "64yud8sH9Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8op8"
                    },
                    "comment": {
                        "value": "Thank you for your review. We are pleased that you found our framework novel and the research direction prominent. We would like to elaborate on the mentioned weaknesses and questions.\n\n**Answer to Weaknesses:**\n\n* Thank you for asking this question. We employ a clustered DAG [1] for **a better intuition about valid and invalid representations**. With the clustered DAG, we do not need to know the exact structure of all the sub-covariates and can instead provide a more abstract, clustered causal diagram. This is a helpful way to describe the valid adjustment set [2], i.e., all the covariates are split into four categories of (1) noise, (2) instruments, (3) outcome-predictive covariates, and (4) confounders. Importantly, the exact partitioning of covariates is unknown in practice, and our MSM framework does not assume any specific partitioning.\n\n    **Action:** We clarified in our paper that we adopt a clustered DAG only for intuition purposes.\n\n* Thank you for suggesting to make the derivation of the bounds more explicit. For our bounds, we adapted the theoretic results, provided in [3,4]. We followed your suggestion and added a more rigorous derivation (see new **Lemma 3** and the theoretical proof).\n\n    **Action:** We added a new **Lemma 3** to **Appendix B** where we detail the derivation of the bounds.\n\n\n**Answer to Questions:**\n\n\n\n* Thank you. In real-world applications, the ground-truth partitioning or ground-truth confounders are usually unknown. Therefore, representation learning methods for CATE work with datasets that typically contain as many potentially useful covariates as possible. \n\n    **Action:** We added to the revised version of the paper that representation learning methods for CATE do not assume a specific partitioning and consider all the covariates as input. This typically helps in the downstream performance. \n\n* The question about the trade-off between finite-sample bias and representation-induced confounding bias is an interesting direction for future theoretic research. We could envision a work where such tradeoffs are learned explicitly but there are several open challenges (e.g., how do you accurately estimate the finite-sample bias? While Bayesian methods are a natural approach, this typically precludes the use of neural methods, etc.).  \n\n   Nevertheless, our paper is, to the best of our knowledge, the **first** work to raise the question of induced confounding in the representations, which would be important for any future research. Hence, we hope that our work spurs further follow-up research.  \n\n   Regarding the relevance to non-representation learning CATE estimators: we followed your suggestion and added non-neural CATE  baselines (k-NN, BART, and Causal Forests) to our revised manuscript. These are in our revised **Tables 2-8 and Figures 4-6**. In the same way as for other baselines, we evaluated rPEHE and the error rate of policies based on estimated CATE. In the new results, we observed that, e.g., non-neural methods outperformed the representation learning CATE estimators on the simple synthetic benchmark with large sample sizes, but fell short in other more complex scenarios (e.g., benchmarks with high-dimensional covariates or low-sample sizes).\n\n    **Action:** We added three non-neural CATE baselines, i.e., k-NN, BART, and Causal Forests to the revised version of the paper. See our revised **Tables 2-8** and **Figures 4-6**.    \n\n\n**References:**\n\n[1] Tara V. Anand, Adele H. Ribeiro, Jin Tian, and Elias Bareinboim. Causal effect identification in cluster DAGs. In AAAI Conference on Artificial Intelligence, 2023.\n\n[2] Carlos Cinelli, Andrew Forney, and Judea Pearl. A crash course in good and bad controls. Sociological Methods & Research, 2022.\n\n[3] Miruna Oprescu, Jacob Dorn, Marah Ghoummaid, Andrew Jesson, Nathan Kallus, and Uri Shalit. B-learner: Quasi-oracle bounds on heterogeneous causal effects under hidden confounding. In International Conference on Machine Learning, 2023.\n\n[4] Dennis Frauen, Valentyn Melnychuk, and Stefan Feuerriegel. Sharp bounds for generalized causal sensitivity analysis. In Advances in Neural Information Processing Systems, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301843377,
                "cdate": 1700301843377,
                "tmdate": 1700301843377,
                "mdate": 1700301843377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6o4NvmhuzD",
            "forum": "d3xKPQVjSc",
            "replyto": "d3xKPQVjSc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_Pw4L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_Pw4L"
            ],
            "content": {
                "summary": {
                    "value": "Estimating conditional average treatment effect (CATE) estimation widely uses low-dimensional representation learning, which can lose information about the observed confounders and thus lead to bias.\nIn this paper, the authors propose a new framework for estimating bounds on the representation-induced confounding bias (RICB). To summarize, the contributions are three-fold:\n1.\tCATE from representation learning methods can be non-identifiable due to RICB.\n2.\tThe authors propose a representation-agnostic framework to perform partial identification of CATE.\n3.\tThe authors demonstrate the effectiveness of our bounds together with a wide range of state-of-the-art CATE methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is technically sound and well-organized."
                },
                "weaknesses": {
                    "value": "It seems that the notations/symbols are not defined correctly. For example, in the section of notations, the authors claim that $\\mu_a^x(x)=\\mathbb{E}(Y|A=1,X=x)$, but $\\mu_a^x(x)$ should be $\\mathbb{E}(Y|A=a,X=x)$. In the same paragraph, the authors claim that $\\mu_a^\\phi(\\phi)=\\mathbb{E}(Y|A=1,\\Phi(X)=\\phi)$, but $\\mu_a^\\phi(\\phi)$ should be $\\mathbb{E}(Y|A=a,\\Phi(X)=\\phi)$. In addition, the authors define $\\pi_a^x(x)= \\mathbb{P}(A=a|X=x)$. I wonder why the authors do not simply $\\pi_a^x$ or $\\pi_a(x)$. Problem arises when the authors introduce overlap assumption. The authors claim that $\\mathbb{P}(0<\\pi_a^x(X)<1)=1$, but I cannot obtain $\\pi_a^x(X)$ from the definition. Indeed, in the definition of $\\pi_a^x(x)= \\mathbb{P}(A=a|X=x)$, The two \u201cx\u201ds in $\\pi_a^x(x)$ should be mapped to \u201cx\u201d in $\\mathbb{P}(A=a|X=x)$. Nevertheless, when $\\pi_a^x(x)$ is changed to $\\pi_a^x(X)$ or $\\pi_a^X(x)$, the mapping procedure is not clear."
                },
                "questions": {
                    "value": "1. According to the definition of $X$, $X=\\{X^\\emptyset,X^a,X^y,X^\\bigtriangleup\\}$. At the same time, $X$ is independent of $X^\\emptyset$, $X^a$, $X^y$ ,$X^\\bigtriangleup$ conditioning to $\\Phi(X)$. It is strange to claim Eqn. (4).\n2. In the example \u201cRepresentations with removed noise and instruments\u201d, the authors claim that under Eqn. (5), the validity follows from the d-separation in clustered casual diagram and Appendix B. In appendix B, only investigations related to the example \u201cInvertible representations\u201d are presented.\n3. I suspect the equality of $\\mathbb{E}(Y[1]-Y[0]|X=x)=\\mathbb{E}(Y[1]-Y[0]|X^\\bigtriangleup=x^\\bigtriangleup, X^y=x^y)$ and $\\mathbb{E}(Y[1]-Y[0]|X^\\bigtriangleup=x^\\bigtriangleup, X^y=x^y)= \\mathbb{E}(Y[1]-Y[0]|\\Phi(X)=\\Phi(x) $ in Eqn. (6) under Eqn. (5). Could the authors provide more details?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5647/Reviewer_Pw4L"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740826348,
            "cdate": 1698740826348,
            "tmdate": 1699636586962,
            "mdate": 1699636586962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4mZ59NZGEl",
                "forum": "d3xKPQVjSc",
                "replyto": "6o4NvmhuzD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pw4L"
                    },
                    "comment": {
                        "value": "We are thankful for your review, and we are happy that you found our paper sound and well-organised. Below, we would like to address the mentioned weaknesses and questions.\n\n**Answer to Weaknesses.** We apologize for the small errors in notation (e.g., in the definitions of $\\mu$) and thank you for spotting them. Regarding $\\pi_a^x(\\cdot)$, **the upper index is not a variable or argument but an indicator** that this propensity relates to the original covariate space (in contrast to $\\pi_a^\\phi(\\cdot)$, which relates to the representation propensity). Therefore, the overlap assumption is defined properly, as only the $x$ inside parenthesis is considered as an argument, namely, $\\pi_a^x(X)$. Hence,  $\\pi_a^x(\\cdot)$ serves as a measurable function, and the probabilistic statement maps a random variable $X$ to the (random) propensity score. Notably, our overlap assumption is equivalent to the $0 < \\pi_a^x(x) < 1$ for all $x$ s.t. $\\mathbb{P}(X = x) > 0$.\n\n**Action:** We corrected the errors in the revised version of the paper (e.g., we corrected the definitions for $\\mu_a^x(x)$ and $\\mu_a^\\phi(\\phi)$). We also clarified the notation around $\\pi_a^x(\\cdot)$. \n\n**Answers to Questions.**\n\n\n\n1. Whenever $\\Phi(X)$ is invertible wrt. to some sub-covariate, the conditional independence statement contains (partially) random $X$ and deterministic (constant) sub-covariate (due to invertibility). $X$ and its deterministic sub-covariate are then independent by the definition ([https://en.wikipedia.org/wiki/Independence_(probability_theory)#For_real_valued_random_vectors](https://en.wikipedia.org/wiki/Independence_(probability_theory)#For_real_valued_random_vectors)). \n\n    Eq. (4) is a way to formalize that the full information about all four sub-covariates is preserved in the representation. This means that the representation is an invertible function wrt. sub-covariates. Conversely, if the information is partially or fully removed in the representation, the non-independence statement will hold (e.g., in Fig. 1). This notation is e.g. consistent with the literature on the prognostic scores [1].\n\n\n    **Action:** We clarified in the revised version of the paper why Eq. 4 holds.\n\n2. Thank you for spotting a missing reference. \n\n    **Action:** We added a missing reference to **Lemma 2 (Removal of noise and instruments)** in the revised version of our paper.\n\n3. Thank you for asking this question. Eq. 6 holds under the assumptions of the clustered causal diagram and invertibility of $\\Phi(\\cdot)$ wrt. $X^\\Delta$ and $X^y$.\n\n    **Action:** We provide the derivation of Eq. 6 in a newly added **Lemma 2 (Removal of noise and instruments)** in a revised version of the paper.\n\n\n**References:**\n\n[1] Ming-Yueh Huang and Kwun Chuen Gary Chan. Joint sufficient dimension reduction and estimation of conditional and average treatment effects. Biometrika, 104(3):583\u2013596, 2017."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301622584,
                "cdate": 1700301622584,
                "tmdate": 1700303391644,
                "mdate": 1700303391644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "58NqmYT85N",
            "forum": "d3xKPQVjSc",
            "replyto": "d3xKPQVjSc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_a9aZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5647/Reviewer_a9aZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of induced confounding that occurs in neural network based conditional average treatment effect estimation as a result of representation learning that operates over a lossy reduced dimension embedding. The authors propose to account for the confounding by leveraging sensitivity analysis. In particular the authors use the marginal sensitivity model and provide bounds on the CATE. A framework is then introduced to estimate the proposed bound within a neural network training flow. A set of experiments are provided which validate the efficacy of the proposed approach."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper addresses a very important, and often overlooked, aspect of representation learning for causal effect estimation. The authors do a commendable job of describing the circumstances under which we should expect to incur bias due to representation induced confounding, and clearly delineate them from existing approaches which don't suffer from the same issues. The proposed sensitivity analysis is intuitive and the authors do a nice job of describing it's integration into the neural network training process."
                },
                "weaknesses": {
                    "value": "The largest weakness I see is the same as what is commonly shared throughout the sensitivity analysis literature, namely that practitioners must place assumptions on the extent of confounding."
                },
                "questions": {
                    "value": "Given the relative difficulty of CATE estimation in small sample regimes, as the authors point to, it would seem that there are a number of settings where representation based CATE estimation is inappropriate. Given this it would be useful for the authors to compare the bounds provided here and contrast to non-NN based approaches (e.g., BART / causal forests) to give a sense of the relative loss in precision due to the representation induced confounding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699399082803,
            "cdate": 1699399082803,
            "tmdate": 1699636586871,
            "mdate": 1699636586871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TpxT4HqfHi",
                "forum": "d3xKPQVjSc",
                "replyto": "58NqmYT85N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5647/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a9aZ"
                    },
                    "comment": {
                        "value": "**Answer to Weaknesses.** Thank you. As you nicely point out, a common limitation in the use of MSMs is that the sensitivity parameter (which guides the amount of hidden confounding) must be chosen through expert knowledge. Upon reading your comment, we realized that we should explain more clearly that we do **not** have this limitation but that we can learn the sensitivity parameter from data. The reason is that our framework aims at partial identification of CATE wrt. representations but not sensitivity analysis in the classical sense. Therefore, we employ the marginal sensitivity model (MSM) in an unconventional way. Specifically, we do not require practitioners to pre-specificy the extent of confounding through expert knowledge. Instead, our framework works in a data-driven manner and can infer the sensitivity parameters from the data (see Sec. 4.2, Stage 1). This is a crucial difference between usual MSM applications to detect hidden confounding and our setting where all the confounders are observed.\n\n**Action:** We stress this key difference more clearly in the revised version of the paper. In particular, we highlighted that we do **not** place assumptions on the extent of confounding (see **Section 2**).\n\n**Answer to Questions.** Thank you. This is indeed a great idea, to add classical non-neural CATE estimators for reference. We followed your suggestion and added non-neural CATE  baselines (k-NN, BART, and Causal Forests) to our revised manuscript. These are in our revised **Tables 2-8 and Figures 4-6**. In the same way as for other baselines, we evaluated rPEHE and error rate of policies based on estimated CATE. In the new results, e.g., we observed that non-neural methods outperformed the representation learning CATE estimators on the simple synthetic benchmark with large sample sizes, but fell short in other more complex scenarios, e.g., benchmarks with high-dimensional covariates or low-sample sizes. \n\n**Action:** We added three non-neural CATE baselines, i.e., k-NN, BART, and Causal Forests to the revised version of the paper. See our revised **Tables 2-8 and Figures 4-6.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301392321,
                "cdate": 1700301392321,
                "tmdate": 1700301392321,
                "mdate": 1700301392321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]