[
    {
        "title": "The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks"
    },
    {
        "review": {
            "id": "TrlnGo96pB",
            "forum": "FJlIwGqPdL",
            "replyto": "FJlIwGqPdL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission163/Reviewer_XaQc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission163/Reviewer_XaQc"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly discusses the robustness of conformal prediction (CP) against adversarial examples. The authors first present their discovery of two weaknesses of the existing CP against adversarial examples. From experiments, the authors empirically demonstrate that the existing CP methods have much larger prediction set sizes to cover correct predictions over adversarial examples. Also, while existing adversarial training methods improved top-1 accuracy, they also increased prediction set sizes. Then, the authors propose a new training method (AT-UR) to improve the robustness of existing CP methods. This method consists of two components: *entropy minimization* and *beta importance weighting*. Additionally, the authors present a theoretical finding that justifies beta importance weighting."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper contains many different findings: empirical discovery about CP, experimental verification of the proposed method, and one theoretical statement on beta importance weighting.\n2. The paper writing is clear enough to understand those findings. The findings are supported with visualizations that make the findings easier to understand.\n3. The experiments use four different datasets and demonstrate that the findings generalize over different datasets."
                },
                "weaknesses": {
                    "value": "1. There is only one attack method used in this paper, i.e., PGD. This could be good enough to show the problem of the existing CP methods. However, when showing the robustness improvement, it would be better to include other powerful attack methods, e.g., CW, DeepFool, etc."
                },
                "questions": {
                    "value": "1. It looks like the improvements on the CIFAR datasets are smaller than the improvements on the Caltech256 and the CUB200 datasets. Is this specifically related to the number of classes in those datasets? If so, would it be better to normalize the improvements by the number of classes?\n2. Minor comments\n    - Section 4 and Section 5 are relatively short compared to other sections. Maybe you can merge those sections into one section (that summarizes the discovery regarding CP against adversarial examples) with two subsections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594506003,
            "cdate": 1698594506003,
            "tmdate": 1699635941879,
            "mdate": 1699635941879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BxjKRr2gmD",
                "forum": "FJlIwGqPdL",
                "replyto": "TrlnGo96pB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XaQc"
                    },
                    "comment": {
                        "value": "We thank the reviewer's positive evaluation and constructive comments. Here are our responses. \n\n**Q1: There is only one attack method used in this paper.**\n\nR1: We use the strong AutoAttack (AA) to evaluate our method with AT. See the table below (each cell has the format of coverage, PSS) and the revised paper (Table 5). Under AA, the performance of our approach is also substantially better than the AT baseline. Interestingly, the robust accuracy under AA drops while the prediction set size reduced (CP efficiency is improved). \n\n|            |         CIFAR10         | CIFAR100 | Caltech256 | CUB200 |\n|:----------:|:-----------------------:|:--------:|:----------:|:------:|\n|     AT     | 93.25(0.45), 2.54(0.04) |    91.99 (0.61),  14.29(0.59)    |    94.35(0.81), 23.73(1.68)        |    91.87(0.90), 17.75(0.71)    |\n|    AT-EM   |    92.36(0.53), 2.45(0.04)                     |   91.87(0.61), 13.29(0.49)       | 93.41(0.58), 21.19(1.46)               |  91.26(0.57), 16.47(0.61)   |\n| AT-Beta    |    91.96(0.39), 2.50(0.04)                     |    91.24(0.69), 11.61(0.40)      |            93.52(0.73), 18.54(1.32)|  91.37(0.75), 16.56(0.76)      |\n| AT-EM-Beta |  92.06(0.44), 2.50(0.04)                       |    91.13(0.63), 11.78(0.47)      |            93.50(0.69), 18.56(1.32)|   91.93(0.68), 16.67(0.58)     |\n\n**Q2: would it be better to normalize the improvements by the number of classes?**\n\nR2: Yes, we agree on this point. Thus, in the updated paper, we normalize the PSS by the number of classees and show the result in Table 3. \n\n**Q3: Maybe you can merge those sections into one section (that summarizes the discovery regarding CP against adversarial examples) with two subsections.**\n\nR3: We agree on this point. We have revised the paper accordingly, see Section 4 with the name NECESSITATE AT FOR ROBUST AND EFFICIENT COVERAGE. \n\nThanks again for your review. We are happy to answer any further questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606027205,
                "cdate": 1700606027205,
                "tmdate": 1700606027205,
                "mdate": 1700606027205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i6u9bJOAhx",
            "forum": "FJlIwGqPdL",
            "replyto": "FJlIwGqPdL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission163/Reviewer_k8ir"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission163/Reviewer_k8ir"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the uncertainty quantification provided by the conformal prediction framework under adversarial attacks. The conformal framework constructs prediction sets (sets of classes/labels) with high-probability coverage guarantees. One desires to obtain such high-probability prediction sets with small set sizes. Consequently, this paper contributes the following:\n1. It experimentally demonstrates that adversarial training is required to achieve small prediction sets.\n2. It experimentally demonstrates that the prediction sets constructed under newer adversarial training variants are larger than those for the vanilla version (even though they improve top-1 accuracies).\n3. It experimentally demonstrates that two factors correlate with the prediction set size: (i) the entropy of the predicted class probabilities and (ii) the predicted rank of the ground-truth class. This paper proposes to reduce the prediction entropy and the predicted rank of the ground-truth class to reduce the prediction set sizes; the former by adding an entropy regularization term and the latter via importance weighting w.r.t. a fixed beta distribution. This method empirically reduces the prediction set sizes. The paper also provides theoretical results showing that beta importance weighting improves the generalization of the trained model."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is motivated by improving the performance of conformal prediction under adversarial attacks, an important research area for practical deployment.\n2. The writing structure is good, with empirical findings driving the direction of the paper.\n3. The proposed method is simple to implement."
                },
                "weaknesses": {
                    "value": "[Details included in the Questions section]\n\n1. Novelty and contributions - The paper argues its contributions to be as highlighted in the Summary section. However, some of the empirical findings are not so novel.\n2. Related work - Comparisons with related works like Gendler et al. (2021) and Ghosh et al. (2023) are insufficient.\n3. The proof for Theorem 1 seems incorrect."
                },
                "questions": {
                    "value": "1. Novelty and contributions\n    1. The fact that a model trained without adversarial training does not produce small conformal prediction sets on adversarial examples is not surprising. Since such a model has poor performance (often random or worse), this trend is expected; the prediction set size depends on the quality of the underlying model [Vovk et al. (2005), Shafer and Vovk (2008)].\n    2. Similarly, the correlation between the prediction set size with the prediction entropy and the predicted rank of the ground-truth class is not surprising. It is a consequence of the non-conformity function (or the set function) used; the correlation is apparent from the function definition. Instead, for example, if one were to use the 0-1 loss that outputs 0 if the top-1 prediction is correct and 1 otherwise, I believe the prediction set size would correlate with the top-1 accuracy (which this paper argues is not true).\n\n2. Related work\n    1. Gendler et al. (2021) and Ghosh et al. (2023) are highly related to this paper; they propose conformal algorithms to do well on adversarial examples. While this paper highlights the comparisons with Gendler et al. (2021) in Section 1, the more recent work of Ghosh et al. (2023) is not. Did the authors compare against their proposed method?\n    2. Did the authors compare the proposed beta importance weighting method against that of Einbinder et al. (2022)?\n    3. I believe the subsection \"Adversarial Robustness\" (Section 2) contains an incorrect citation. Was it meant to be Gendler et al. (2021) instead of Salman et al. (2020)?\n\n3. Proof for Theorem 1 - I encourage the authors to revisit the following.\n    1. The proof expands the gamma function as $\\Gamma ( n ) = ( n - 1 ) !$, which is true when $n$ is a positive integer, not a real value.\n    2. When $A$ and $B$ are combined, the bound $c \\leq K^{- \\alpha}$ is used. This is not satisfied when setting $c = \\max \\\\{ K^{- \\alpha} , \\cdot \\\\}$.\n    3. How is the bound $A \\leq c K^{- c}$ obtained?\n    4. How is the last inequality $\\sum_{k = 1}^{K} p_{\\text{Beta}} ( k / K ; a - c , b ) / K^{2} \\leq 1$ obtained?\n    5. It seems that the inequality $a > c$ is used throughout, but is not assumed.\n    6. Typos\n        1. $\\hat{r} ( x , y )$ should be replaced with $r ( x , y )$.\n        2. The indicator function should be $r ( x , y ) = k$ instead of $\\hat{r} ( x , y ) = 1$.\n\n4. The proposed method\n    1. Theorem 1 is not empirically supported (cf. Table 3).\n    2. What is the evidence to show that the majority of data lies in the promising region (stated in Section 6.2)?\n    3. What is done to reduce the importance weight when $r_{i} = 1$ or $\\hat{r}_{i} = 1 / K$ (since this is not part of the promising region)?\n    4. The paper should explicitly mention how the predicted ranks are normalized, i.e., define $\\hat{r} ( x , y ) = r ( x , y ) / K$.\n    5. Fig. 5 - The histograms look indistinguishable. Can the entire x-axis be included to highlight the difference, if there is one?\n    6. Fig. 4\n        1. How is the promising region determined in this illustration?\n        2. Is the ratio reported not the fraction of points in that region? The caption and the text supporting this figure are confusing.\n\n5. Experiments\n    1. What is the reason for only looking at $l_{\\infty}$ deviation adversarial examples? RSCP handles $l_{2}$ deviations; how do the experimental results differ?\n    2. What is the pre-trained model used for? Are the adversarial examples not constructed based on the model at hand?\n    3. APS is claimed to be more stable than RAPS in Section 7. What is this stability concerning?\n    4. MART is used for the initial experiments but not the main ones. Is there a reason for that? How does the proposed method perform with MART?\n    5. The generalizations for which version of the proposed method to use in Section 7.2 are done for datasets. However, it seems to be dependent on the adversarial training method used.\n\n6. Preliminaries\n    1. Prediction set size and conformal prediction inefficiency are used synonymously. However, the latter is not defined.\n    2. The paper includes empirical risk minimization but does not discuss its assumptions. The generalization error is bound under the i.i.d. assumption.\n    3. What are adversarial examples? What is the PGD attack? What is adversarial training? The paper should provide these details.\n    4. The conformal prediction framework is not explained well. Additionally, the paper does not discuss its assumptions and statistical guarantees in detail.\n    5. $y_{i j}$ in Eq. 1 is not defined.\n\n7. Section 4 does not show that adversarial training is indispensable (as mentioned in the last paragraph). It shows that standard non-adversarial training methods lead to large conformal prediction sets. Additionally, saying that the non-adversarially trained models are \"completely broken\" is incorrect. What the paper might want to emphasize is that the prediction sets are large, making them less informative.\n\n8. It is worth mentioning in the main paper that the top-1 accuracy decreases when using the proposed method.\n\n9. It is also worth including details of the experimental setup in Sections 4-6 or pointing to where they are in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790268127,
            "cdate": 1698790268127,
            "tmdate": 1699635941781,
            "mdate": 1699635941781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NooKIew287",
                "forum": "FJlIwGqPdL",
                "replyto": "i6u9bJOAhx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k8ir"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments. Here are our responses. \n\n**Q1: The fact that a model trained without adversarial training does not produce small conformal prediction sets on adversarial examples is not surprising.**\n\nR1: Note that we never claim this effect is surprising in our paper. We show this effect to make the point that adversarial training is necessary for conformal prediction to work in an adversarial environment. \n\n**Q2: The correlation between the prediction set size with the prediction entropy and the predicted rank of the ground-truth class is not surprising. Using the 0-1 loss makes the argument in this paper not true.**\n\nR2: Our paper is an empirical study into the prediction set size of adversarially trained models and how to make the conformal prediction more efficient in such robustly trained models. We are glad to hear that our proposed method makes sense intuitively, as the reviewer admits, while our paper empirically proves that this intuitive design works quite well across several datasets and AT methods. \nHowever, it is unclear to us what is the difference between the mentioned 0-1 loss function and top-1 accuracy and how to compute the non-conformity score based on the mentioned 0-1 loss function. It is appreciated if the reviewer could elaborate on this point to make it a valid weakness.  \n\n**Q3: While this paper highlights the comparisons with Gendler et al. (2021) in Section 1, the more recent work of Ghosh et al. (2023) is not.**\n\nR3: The reason why we did not compare with Ghosh et al. (2023) is that Ghosh et al. (2023) only considers the l_2 norm instead of l_inf norm and it does not consider the training stage, same as Gendler et al. (2021). Thus, similar to what the reviewer claims in Q1, we believe that the coverage of  Ghosh et al. (2023) would be unsurprisingly bad at adversarially robust coverage.\n\n**Q4: Did the authors compare the proposed beta importance weighting method against that of Einbinder et al. (2022)?**\n\nR4: We are running this experiment now and will update the paper once it is ready. It is worth mentioning that Einbinder et al. is not explicitly for improving CP efficiency in adversarially trained models but regularizing the non-conformity scores so that they are uniformly distributed. We compare with the focal loss in our ablation study, which is shown to be comparable with focal loss in Einbinder et al., and find that the focal loss is worse than our method with a huge performance gap. \n\n**Q5: Was it meant to be Gendler et al. (2021) instead of Salman et al. (2020)?**\n\nR5: This is a typo, we have fixed it in the updated paper. \n\n\n**Q6: Theory issues.**\n\nR6: We have explained how to get the $A \\leq cK^c$ in the updated version, page 17. Other theoretical issues like using general Gamma function for real-value $\\alpha$ and $\\beta$ will be fixed in our future work. \n\n**Q7: Theorem 1 is not empirically supported (cf. Table 3).**\n\nR7: Note that Theorem 1 does not guarantee that the top-1 test accuracy will be improved. Theorem 1 indicates that the generalization error of the beta-weighting method is comparable to that of ERM, so we can train a classifier with the beta-weighting in practice.\n\n**Q8: What is the evidence to show that the majority of data lies in the promising region (stated in Section 6.2)?**\n\nR8: Fig. 5 shows that most training samples are in the promising region. We compute the ratio of test samples in the promising region (Fig. 6) to all test samples on CIFAR100. The ratio is 44.36%.\n\n**Q9: What is done to reduce the importance weight when $r_i$=1 (since this is not part of the promising region)?**\n\nR9: We use small importance weights for those samples. See Fig. 4 and Equation 6 for the importance weight for samples outside the promising region.\n\n**Q10: The paper should explicitly mention how the predicted ranks are normalized.**\n\nR10: We will make it explicit in the next version. \n\n**Q11: Fig. 5 - The histograms look indistinguishable. Can the entire x-axis be included to highlight the difference, if there is one?**\n\nR11: Including the whole x-axis reduces the visual difference. The quantitative difference is shown in the bottom of Fig. 4.\n\n**Q12: How is the promising region determined in this illustration?**\n\nR12: According to the Beta-distribution shown in Fig. 4, we choose top 25% as the promising region for visualization purpose. \n\n**Q13: Is the ratio reported not the fraction of points in that region? The caption and the text supporting this figure are confusing.**\n\nR13: It is the ratio of p(r) instead of the ratio of samples, which is explained in the paragraph following Equation (7). We will make it clear in the next version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694294454,
                "cdate": 1700694294454,
                "tmdate": 1700694294454,
                "mdate": 1700694294454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLeDbTeqOy",
                "forum": "FJlIwGqPdL",
                "replyto": "i6u9bJOAhx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k8ir -- Continue"
                    },
                    "comment": {
                        "value": "**Q14: Is the ratio reported not the fraction of points in that region? The caption and the text supporting this figure are confusing.**\n\nR14: It is the ratio of p(r) instead of the ratio of samples, which is explained in the paragraph following Equation (7). We will make it clear in the next version. \n\n**Q15: What is the reason for only looking at $l_{\\infty}$ deviation adversarial examples? RSCP handles $l_2$ deviations; how do the experimental results differ?**\n\nR15: We explicitly explained this at the first section in the first page. We only handle the l_inf as it is a more challenging case than l_2. \n\n**Q16: What is the pre-trained model used for? Are the adversarial examples not constructed based on the model at hand?**\n\nR16: As we have explained in Section 7.1, we use pre-trained model as the initialization instead of random weights following Liu et al., 2023. \n\n**Q17: APS is claimed to be more stable than RAPS in Section 7. What is this stability concerning?**\n\nR17: As the prediction set is a quantification of prediction uncertainty, we use the stable APS instead of RAPS to have a low-variance uncertainty quantification to have a better contrast between different AT methods. \n\n**Q18: MART is used for the initial experiments but not the main ones. Is there a reason for that? How does the proposed method perform with MART?**\n\nR18: The reason why we did not include MART is that MART is not quite competitive in terms of CP efficiency, see Table 1.\n\n**Q19: The generalizations for which version of the proposed method to use in Section 7.2 are done for datasets. However, it seems to be dependent on the adversarial training method used.**\n\nR19: We provide our observations in Section 7.2. It makes sense that the performance depends on the AT method, as different AT has different prediction set sizes as Table 1 shows.\n\n**Q20: Prediction set size and conformal prediction inefficiency are used synonymously. However, the latter is not defined.**\n\nR20: It is defined in the caption of Fig. 1 and the reference is cited in the introduction.\n\n**Q21: What is the assumption of empirical risk minimization? What are adversarial examples? What is the PGD attack? What is adversarial training?**\n\nR21: As in every machine learning paper uses empirical risk minimization, we assume the data are i.i.d.. We cite the corresponding papers (adversarial examples, PGD attack and adversarial training) in our paper as most adversarial robustness papers do, as we all assume a basic understanding into the research topic. We have explicitly defined the adversarial training in Equation (2).\n\n**Q22: The conformal prediction framework is not explained well.**\n\nR22: We pointed the readers to Romano et al. 2020 for more details about CP in our paper.\n\n\n**Q23: The non-adversarially trained models are \"completely broken\" is incorrect.**\n\nR23: When the prediction set size is almost as large as the class numbers, the conformal prediction provides no uncertainty information, thus we think it is broken. We will revise the paper to make it clear.  \n\n**Q24: It is worth mentioning in the main paper that the top-1 accuracy decreases when using the proposed method. It is also worth including details of the experimental setup in Sections 4-6 or pointing to where they are in the paper.**\n\nR24: We have revised the paper to make it clear."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694485878,
                "cdate": 1700694485878,
                "tmdate": 1700694485878,
                "mdate": 1700694485878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gOSGMgrKdI",
            "forum": "FJlIwGqPdL",
            "replyto": "FJlIwGqPdL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission163/Reviewer_GTvN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission163/Reviewer_GTvN"
            ],
            "content": {
                "summary": {
                    "value": "The authors considered the problem of adversarial training to output probability prediction (for a classifier) that leads to a more efficient conformal prediction interval using APS. To achieve this goal, the authors proposed to include two additional loss terms: (1) the entropy minimization loss which encourages outputting uncalibrated prediction with more certainty, and (2) beta-weighting loss that up weights samples that in the moderate difficulty regime of classification (e.g., top probability does not correspond to the true label but not so far away)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Training a (robust) classifier that aims for a small prediction interval is an interesting problem."
                },
                "weaknesses": {
                    "value": "I feel that the efficacy why including the two additional terms can help the prediction interval construction and their robustness are not fully explored."
                },
                "questions": {
                    "value": "1.  Is the gain related to the entropy minimization loss tied with the APS framework, which, due to the additional randomization, favors prediction with low entropy? Do you still observe the improvement of using the entropy minimization loss using other conformal prediction constructions? For example, using Sadinle et all with average-coverage/per-class coverage.\n\n2. How does the choice of beta distribution shape influence the results? Are the results sensitive to the beta-weight parameters?\n\n3. How do different attack budgets influence the results?\n\n4. How do different training budgets and step-sizes influence the results?\n\n5. The assumption on how pk distributed in Theorem 1 seems very stringent. In addition, even if I accept the assumption (which the authors certainly need to justify), the proof also needs to be discussed in more detail and I am not completely convinced currently. For example, the first term in Lemma 1 seems to be dropped in Theorem 1's proof, but isn't it the case that the first term will change as you change the weights and will be higher for the beta-weighted problem?\n\n6. Some minor issues:\n6.a: what is d(x,y) in the proof of Theorem 1?\n6.b: the summation in d2(P||P/w) should be r=k instead of r =1.\n6.c: I need to go to later sections in order to understand Table 1 in the preliminary results, some brief explanations about the evaluation metrics will be helpful.\n....\n\n[1]Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. Journal of the American Statistical Association, 114(525):223\u2013234, 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855456152,
            "cdate": 1698855456152,
            "tmdate": 1699635941695,
            "mdate": 1699635941695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6ojLItp9sY",
                "forum": "FJlIwGqPdL",
                "replyto": "gOSGMgrKdI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GTvN"
                    },
                    "comment": {
                        "value": "We thank the review for the insightful comments. Here are our responses. \n\n**Q1: Do you still observe the improvement of using the entropy minimization loss using other conformal prediction constructions?**\n\nR1: When we use the RAPS as the conformal prediction method, the entropy minimization loss is also better than the AT baseline on Caltech256, see the following table. We think that the mentioned method Sadinle et all is not comparable in our context since their target coverage is not for adversarial examples.\n\n| Caltech256 |    Coverage    |         PSS        |\n|:----------:|:--------------:|:------------------:|\n|     AT     | 90.533 (1.483) |   32.738 (6.618)   |\n|    AT-EM   | 90.322 (1.369) | **30.303 (5.133)** |\n\n\n\n**Q2: How does the choice of beta distribution shape influence the results? Are the results sensitive to the beta-weight parameters?**\n\nR2: We use different beta-weight hyperparameters on Caltech256. The performance is stable within a range of $\\beta$ (3.0, 4.0, 5.0), see the table below. The numbers in the parenthesis mean ($\\alpha$,$\\beta$). \n| Caltech256 |    Coverage    |         PSS        |\n|:----------:|:--------------:|:------------------:|\n| AT-Beta (1.1, 2.0)    |   90.588 (0.560) | 37.091 (2.245)  |\n| AT-Beta (1.1, 3.0)    |   90.185 (0.850) | 35.225 (2.758) |\n| AT-Beta (1.1, 4.0)    |  90.251 (0.759) | 35.554 (2.281)  |\n| AT-Beta (1.1, 5.0)    |  90.201 (0.845) | 35.391 (2.661)     |\n| AT-Beta (1.1, 6.0)    |  90.966 (1.076) | 38.541 (3.470)    |\n\n**Q3: How do different attack budgets influence the results?**\n\nR3: In addition to the $\\epsilon$=8.0, we test different attack budgets $\\epsilon$=4.0, 12.0, 16.0 and add the result on Caltech256 below. It shows that across the attack budgets, our method is consistently better than the AT baseline. \n\n| Caltech256,$\\epsilon$=4.0 |    Coverage    |         PSS        |\n|:----------:|:--------------:|:------------------:|\n|     AT     |  92.728 (0.814) | 21.913 (1.593)   |\n|    AT-EM   |  92.928 (0.672) | 21.639 (1.895)  |\n| AT-Beta    |  91.851 (1.062) | **17.172 (2.049)**  |\n| AT-EM-Beta | 91.826 (0.761) | 17.589 (1.213)  |\n\n| Caltech256,$\\epsilon$=12.0 |    Coverage    |         PSS        |\n|:----------:|:--------------:|:------------------:|\n|     AT     |  89.684 (0.893) | 78.800 (5.430)  |\n|    AT-EM   |  89.483 (0.754) | 76.386 (4.210) |\n| AT-Beta    |  89.779 (1.217) | 79.789 (7.094)  |\n| AT-EM-Beta | 89.968 (0.922) | **74.383 (4.507)**  |\n\n| Caltech256,$\\epsilon$=16.0 |    Coverage    |         PSS        |\n|:----------:|:--------------:|:------------------:|\n|     AT     |   90.223 (1.096) | 136.863 (7.198) |\n|    AT-EM   |  89.968 (0.666) | 132.146 (4.077) |\n| AT-Beta    |  90.125 (0.872) | 142.175 (6.892)  |\n| AT-EM-Beta |  90.053 (0.980) | **131.636 (5.591)** |\n\n\n**Q4: How do different training budgets and step-sizes influence the results?**\n\nR4: We use the default setting in this paper following existing papers like Liu et. al. We will do more experiments using different training budgets in the updated version.  \n\n\n**Q5: The assumption on how $p_k$ distributed in Theorem 1 seems very stringent.**\n\nR5: Assumption on $p_k$ does not necessarily require that the polynomial distribution exactly hold. Instead, in practice, it can be generalized by setting a worse scenario as an upper bound.\n\n\n**Q6: The first term in Lemma 1 seems to be dropped in Theorem 1's proof?**\n\nR6: In generalization analysis, the dominating term is the second error term $O(\\sqrt{ d_2(\\mathcal P || \\mathcal P / \\omega) / m })$, rather than the first one $O(1/m)$, since $1/m$ is in higher order compred with $1/\\sqrt{m}$. As a result, it is common to make effort to improve the second term. See the variance-based robust learning [R1] and [R2] as examples.\n\n[R1] Namkoong, Hongseok, and John C. Duchi. \"Variance-based regularization with convex objectives.\" Advances in neural information processing systems 30 (2017).\n\n[R2] Maurer, Andreas, and Massimiliano Pontil. \"Empirical bernstein bounds and sample variance penalization.\" arXiv preprint arXiv:0907.3740 (2009).\n\n\n**Q7: What is d(x,y) in the proof of Theorem 1?**\n\nR7: $d(x,y)$ means the differential of the variable in the calculus. Since we couple the input $x$ and output $y$ together as a joint random variable, we use $d(x,y)$ as the differential of the joint variable.\n\n**Q8: The summation in d2(P||P/w) should be r=k instead of r =1.**\n\nR8: Thanks for pointing out this typo. We will revise it.\n\n**Q9: Some brief explanations about the evaluation metrics will be helpful in Table 1**\n\nR9: We will revise the paper and add some brief introduction to the evaluation metrics.\n\n\nWe hope that our responses could address your concern and it is appreciated if the reviewer could raise the score accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611522783,
                "cdate": 1700611522783,
                "tmdate": 1700611522783,
                "mdate": 1700611522783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbi4AZQ0NT",
                "forum": "FJlIwGqPdL",
                "replyto": "gOSGMgrKdI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission163/Reviewer_GTvN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission163/Reviewer_GTvN"
                ],
                "content": {
                    "comment": {
                        "value": "Q1+Q2: There are quite a bit variability, both from varying beta and from comparing different methods in the original paper and the table in Q1. This makes me feel a very transparent and replicable experiement, including the parameter tuning, is really important. alpha = 1.1 is also a very interesting value, looks like a value from fine tuning. Currently, the parameter candidates are described as from pilot study, and details about the pilot study and how the parameters are determined should be provided for replicability and making sure there is no information leak.  \n\nQ3: no further question.\n\nQ5: The faster the tail decays, the better the classifier needs to be, right?  Does this mean that this Theorem only works for a very accurate classifier? (The current Theorem should also be stated using the tail bound version, it is better than the exact equality, although I still feel it is very stringent.)\n\nQ6: Should state it precisely in the Theorem and make it rigourous in the proof (show distance < 1 instead of <= 1). (Also, I need to mention that I did not check the proof carefully. )"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707864002,
                "cdate": 1700707864002,
                "tmdate": 1700707911265,
                "mdate": 1700707911265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]