[
    {
        "title": "Optimization Framework of Transfer Learning and its Feasibility"
    },
    {
        "review": {
            "id": "RiF1ypF7uw",
            "forum": "pUy4mGPmUy",
            "replyto": "pUy4mGPmUy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_QBcv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_QBcv"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a mathematical formulation of transfer learning. Firstly, a class of loss functions is defined. Then, the problem is formulated as the optimization problem of the expected loss function under the population distribution. Finally, the existence of the optimal solution is proved. Based on the proof of the existence theorem, some theoretical properties of the optimal solution are investigated. Finally, the authors reveal that the added feature independent of Y conditioned on X does not convey relevant information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors rigorously proved the existence of the optimal solution for the optimization problem of transfer learning."
                },
                "weaknesses": {
                    "value": "Most part of the paper is devoted to preparing mathematical descriptions of transfer learning. The authors revealed rigorous assumptions for the existence of the optimal solution. However, the insight clarified by the theorems is not very clear. From the mathematical viewpoint, proving the existence of the optimal solution seems to be the starting point of the argument for statistical learning. However, the impact of this work on the machine-learning community seems to be quite limited. Not only the existence of the optimal solution but also additional theoretical results that have a practical impact would be necessary as an ICLR paper. \n\nIn transfer learning, the \"negative transfer\" phenomenon is an important issue. The authors could pursue the relationship between the theoretical result in the paper and negative transfer."
                },
                "questions": {
                    "value": "- The authors revealed rigorous assumptions for the existence of the optimal solution. However, the insight clarified by the theorems is not very clear. Can the authors add usefulness to their analysis from a practical standpoint?\n\n- In transfer learning, the \"negative transfer\" phenomenon is an important issue. Is there any relationship between the discussion in Section 3.2 and the negative transfer? In other words, can we use the theoretical result in the paper to reduce the effect of the negative transfer? \n\n- Recently, numerous works for self-supervised learning have been published to elucidate the success of the large language model (LLM). Is it possible to apply the theoretical result in the paper to reveal the reason why the LLM works so efficiently?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698131619318,
            "cdate": 1698131619318,
            "tmdate": 1699636627363,
            "mdate": 1699636627363,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "pJP5Yxmp2a",
            "forum": "pUy4mGPmUy",
            "replyto": "pUy4mGPmUy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_Uz8e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_Uz8e"
            ],
            "content": {
                "summary": {
                    "value": "This paper establishes the mathematical concepts necessary for transfer learning and builds a mathematical framework for transfer learning. To this end, this paper describes a three-step transfer learning procedure: 1) Input transport, 2) Applying pretrained model, and 3) Output transport.  Then, this paper analyzes the feasibility of transfer learning as an optimization problem."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper integrates the problems of transfer learning into one mathematical framework and deals with them as optimization problems. Then, for each problem, what previous studies have solved was mathematically specified."
                },
                "weaknesses": {
                    "value": "1. This paper covered the examples of transfer learning but not one of the important transfer learning problems, inductive transfer learning.\n2. It is excellent to establish a mathematical framework for transfer learning, but this is not new. This paper seems to be a review paper on transfer learning and does not seem suitable for the ICLR conference."
                },
                "questions": {
                    "value": "1. What new insights does the framework presented in this paper give us that we haven't seen in other studies?\n2. Can inductive transfer learning be dealt with and integrated into this framework?\n3. Wouldn't it be better to refer to the input mapping function as heterogeneous transfer learning learning according to Pan & Yang's nomenclature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5906/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5906/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5906/Reviewer_Uz8e"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698567862203,
            "cdate": 1698567862203,
            "tmdate": 1699636627248,
            "mdate": 1699636627248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "IKLtwRhYT9",
            "forum": "pUy4mGPmUy",
            "replyto": "pUy4mGPmUy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_yVrx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_yVrx"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an optimized perspective on transfer learning, encompassing two pivotal functions that require optimization: the input transformation function and the output transformation function. It convincingly illustrates that, given certain suitable conditions, an optimal solution is indeed attainable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The comprehensive three-step procedure delineated for transfer learning encompasses all possible scenarios wherein leveraging the source model is beneficial for addressing the target task, ensuring alignment in both input and output dimensions.\n- The optimization perspective of transfer learning presented here is intriguing and offers valuable insights."
                },
                "weaknesses": {
                    "value": "- The writing could benefit from improvement. The explanation of certain critical concepts, such as the proper loss function, is not sufficiently clear. Additionally, the assumptions require further elaboration, especially the third assumption. How do these assumptions align with common scenarios? A more systematic discussion would be highly advantageous.\n- The primary theorem in this paper demonstrates that under specific conditions, the optimal input mapping and output mapping do exist. This theorem provides a theoretical foundation for the proposed optimization procedure in transfer learning. However, it still poses challenges in guiding us towards algorithm design.\n- While there are classical theorems in transfer learning and domain adaptation, such as H-divergence, a more detailed comparison with these theorems is lacking. Furthermore, the distinctions from other theoretical works have not been adequately elucidated."
                },
                "questions": {
                    "value": "- It would be advantageous to explicitly address the distinctions between the proposed framework and the corresponding theorem in comparison to other theoretical analyses in the field of transfer learning.\n- The third assumption requires more detailed explanation. Could you please provide further elaboration and discuss how easily it can be satisfied in common scenarios?\n- In many cases, even if we are aware that optimal input mapping and output mapping exist, obtaining such mappings remains challenging. Often, we can only attain near-optimal solutions. How do we quantify the performance gap between the optimal solution and the one achievable in real-world scenarios within the proposed framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5906/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5906/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5906/Reviewer_yVrx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762317362,
            "cdate": 1698762317362,
            "tmdate": 1699636627147,
            "mdate": 1699636627147,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ZZ6AVqg9mZ",
            "forum": "pUy4mGPmUy",
            "replyto": "pUy4mGPmUy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_f6nM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5906/Reviewer_f6nM"
            ],
            "content": {
                "summary": {
                    "value": "This paper establishes a mathematical framework for transfer learning, and resolves its feasibility issue. It then identifies and formulates the three-step transfer learning procedure as an optimization problem, allowing for the resolution of the feasibility issue. Importantly, it demonstrates that under certain technical conditions, such as appropriate choice of loss functions and data sets, an optimal procedure for transfer learning exists.\n\nHowever, I think the application of this framework is restrictive and the advantage of transfer learning (compared with learning from scratch) is omitted."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper proposed a unified framework for transfer learning.\n* They proposed several conditions, under which the transfer learning problem is feasible."
                },
                "weaknesses": {
                    "value": "* There are several important settings this framework can't include. For example, the shared representation setting [1] and covariate shift [2]. These settings are commonly studied in both theoretical and algorithmic studies. So I think the application of this paper is restrictive.\n* Besides, a more important problem is that I can't find evidence from this framework that transfer learning has some superiority compared with learning from scratch. Is learning a transformation between $X_S$ and $X_T$ or between $Y_S$ and $Y_T$ easier than directly learning the relationship between $X_T$ and $Y_T$? I think this is the central wisdom in transfer learning, which is omitted in this paper.\n\n\n[1] Tripuraneni, Nilesh, Michael Jordan, and Chi Jin. \"On the theory of transfer learning: The importance of task diversity.\" *Advances in neural information processing systems* 33 (2020): 7852-7862.\n\n[2] Pathak, Reese, Cong Ma, and Martin Wainwright. \"A new similarity measure for covariate shift with applications to nonparametric regression.\" *International Conference on Machine Learning*. PMLR, 2022."
                },
                "questions": {
                    "value": "Please see Cons."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698926324482,
            "cdate": 1698926324482,
            "tmdate": 1699636627022,
            "mdate": 1699636627022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]