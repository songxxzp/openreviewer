[
    {
        "title": "Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation"
    },
    {
        "review": {
            "id": "hk9U2r7f61",
            "forum": "NgtEafc8NZ",
            "replyto": "NgtEafc8NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_WLtt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_WLtt"
            ],
            "content": {
                "summary": {
                    "value": "Valued-based RL typically aims to learn the Q-function (\"action value function\"), which maps state-action pair to a Q-value. The complexity of this approach doesn't scale well with high-dimensional action space. Instead, this work proposes to learn the V-function (\"state value function\"). There's a some description of the mathematical foundation for the proposed loss function and algorithm. \n\nExperiments were conducted on continuous control tasks including gym (mujoco) and DeepMind control suite."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Strong empirical performance in the experiments, especially compared to V-trace, which shares a similar mathematical formulation as the proposed approach Vlearn."
                },
                "weaknesses": {
                    "value": "- Lack of mathematical rigor in the algorithm description (see questions below)\n- An important connection to a particular piece of prior work seems to be obscured in the paper as written"
                },
                "questions": {
                    "value": "- Issues with mathematical notation\n  - In eqn (2) and the eqn right above eqn (4): what is $j$, what is the sum over $j$, and what is $K$? Below eqn (2), the text says \"$a_{t,j}$ defines the $j$th action in state $s_t$\". Does this mean $j$ is enumerating the collected transitions (presumably in a buffer)? Or is this following some other distribution - what distribution would that be? \n  - In eqn (4), since you are no longer summing over $j$, do you still need $\\frac{1}{K}$? \n  - Right above eqn (3), could you clarify what \"upper bound\" and \"lower bound\" this sentence is referring to, and why \"truncated importance weights can provide an upper bound\"? \n- The introduction mentions \"upper bounds of actual Bellman error\" - I assume this is referring to eqn (4) - however it was not explicitly stated around eqn (4). \n  - How was Jensen's inequality used to derive eqn (4) from eqn (2)? Could walk through this explicitly. \n- Connection to Mahmood et al. 2014\n  - In that referenced paper, the authors considered linear function approx setting and presented two importance sampling approaches for off-policy learning. \n  - What they call OIS-LS and WIS-LS seems to correspond to the difference between eqn (2) vs eqn (4). Specifically, their WIS-LS objective is $J(\\theta) = \\frac{1}{n} \\sum_k \\rho_k (Y - \\theta^{\\intercal} \\phi_k)$, which basically corresponds to eqn (4) of this paper $L(\\theta) = \\frac{1}{K} \\sum_{t} \\rho_t (V_\\theta(s_t) - (r_t + \\gamma V_{\\bar{\\theta}}(s_{t+1}))^2$ except now $V_\\theta$ is used in place of a linear function. \n  - The core idea of the current proposed approach seems to be simply extending the linear case to general function approximation. Is there any other difference? Either way, this connection should be explicitly stated, perhaps citing the mathematical formulation as well. Of course, I understand the prior work did not consider any continuous control benchmarks, and the contribution of this work is still valuable. But I believe the connection should be made clear. \n- The conclusion states \"proposed a novel approach applying trust region methods in the off-policy setting\" -- is that the main contribution? Or is learning V-function instead of Q-function the contribution. \n\nMinor issues: \n- typo on page 4 paragraph 2 \"Bellmann\" -> \"Bellman\"\n- typo on page 6 \"populate]\"\n- typo on page 8 conclusion \"an novel approach\" -> \"a novel approach\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698422083596,
            "cdate": 1698422083596,
            "tmdate": 1699636936284,
            "mdate": 1699636936284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RIie2ohtBN",
                "forum": "NgtEafc8NZ",
                "replyto": "hk9U2r7f61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Answer for Reviewer WLtt"
                    },
                    "comment": {
                        "value": "Dear Reviewer WLtt,\n\nThank you for your thoughtful review of our paper, we appreciate the time and effort you dedicated to providing detailed feedback. We are pleased to note your recognition of the strong empirical performance demonstrated in our experiments, especially when compared to V-trace. Below, we address your open questions and concerns.\n\n- **Derivation Details and Theoretical Justification:** We acknowledge your concern about the ambiguity in the derivation of the upper bound for the value function. In the revised manuscript, we have improved the notation in the main text and provided a comprehensive derivation in the appendix. In addition, we have given more insight into the method, explaining its relationship to and differences from V-trace. We have also now included a full proof establishing the consistency of the upper bound.\nWe hope that these revisions not only answer your questions about the derivation, but also provide a more intuitive understanding of the method. Should you require further clarification, we are available to provide additional information.\n\n  To answer some of the questions already at this point. The $\\sum_j^K$ over actions in state $s_t$ is a Monte Carlo estimate of the expectation over actions from our policy. In practice computing this is hardly possible as we would have to be able to execute multiple actions from the same state. Our method now moves this expectation outside of the square and allows us to use the joint expectation over states and actions. But we agree this should be also reflected in the notation, we updated it accordingly.\n\n- **\"Upper Bound\" and \"Lower Bound\" Clarification Eq (3):** The upper bound here simply refers to the truncated importance weights that clip the importance weights at typically 1. While we do not leverage a specific lower bound, the importance ratio has a natural minimum at 0.\n\n- **Contribution and Connection to Mahmood et al. 2014:** Our work is indeed similar to this previous work and could be seen as an extension to the general case of function approximation. However, direct application of this solution is still challenging as seen in our added ablation study, hence we also propose a proper way on how to leverage these results in an efficient way as part of an off-policy trust region method. Although a direct application is not possible, we believe that this related work further supports our argument that using importance sampling for the full loss function is preferable. We have made this connection clearer in the revised version.\n\n- **Minor Issues:** Thank you for pointing out the typographical errors. In the revised manuscript, we corrected these inaccuracies.\n\nWe believe that the newly added changes strengthen the overall clarity and robustness of our paper. Your feedback has been very helpful, and we remain at your disposal for any further inquiries."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497858646,
                "cdate": 1700497858646,
                "tmdate": 1700497858646,
                "mdate": 1700497858646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PMn1m8Q57F",
            "forum": "NgtEafc8NZ",
            "replyto": "NgtEafc8NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_aLJf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_aLJf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new objective to estimate the state value function in off-policy setting. The new objective modifies the importance-weighted value iteration objective by a) moving the importance weights to the outside of the squared loss and b) subsuming the per-action weights to the summation over the time horizon. This state value estimation method is then combined with many other RL practice, including using the target network, applying trust region constraint, truncating the importance ratios etc, to form a new algorithm. The paper empirically evaluates this new algorithm in control tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well written, with a clear statement of the research question. The related work also summarizes well the on-policy and off-policy methods. The use of a trust-region method to stabilize the training and the policy update is also convincing and turns out be to working well in empirical experiments."
                },
                "weaknesses": {
                    "value": "### Clarity & quality\n**Important details on the value loss function are missing and unclear**: \n\nThe main contribution of this is the use of a surrogate objective for the value function learning. However, the deduction of this new surrogate objective lacks important explanations. \n1. \u201caccomplish this by relaxing the squared loss function from Equation 2 using Jensen\u2019s inequality\u201d. It is not straightforward to me how to use Jensen\u2019s inequality to obtain $L(\\theta)$. Why we can move the ratios just outside of the square? \n2.  In Equation (4), the paper just subsumes the summation over $j$ into the sum over $t$. I\u2019m not sure quite how this can be done, given that the summation over $j$ \u201cimplicitly assumes multiple action executions per state\u201d. But the $L(\\theta)$ in Equation (4) clearly does not have such implicit assumption. Further, in the subsequent sections, there is no discussion on this, and Algorithm 1 just presents a single-action estimate for the loss. \n\n### Significance\n**Some conclusions and statements may need to be further justified**\n1. **the conclusion from the cited work may not carry over straightaway to the value estimation in this paper**. Specifically, \u201cthe new loss function is an upper bound of the original loss function and shares the same optima\u201d. However, in the cited work (Neumann & Peters, 2008), the proof is for the value estimation of $\\pi$ with a given Q and samples generated by the same policy (if I understand correctly). This is different from objective function in Equation (4), which is estimated using target networks and the off-policy samples.  Can the authors provide a theoretical proof for this convergence to the same optima? Also, regarding \u201csolving the relaxed loss should yield results consistent with the original loss\u201d, does it mean that the final loss will be consistent? Or the final policy will be the same? \n\n2. **It is unclear how the proposed state-value-estimation can be useful for the algorithm**: Particularly, the proposed algorithm is a combination of the multiple independent ideas: state value estimation, target networks, trust region projection, Gaussian policy approximation, replay buffer and importance weight truncation. Any of these ideas can be crucial and contributive for the strong empirical performance. It would be hard to detach the state value estimation from the other components and claim that the proposed state value estimation is the key. \n  \n3. **Further empirical studies are needed to verify the proposed idea**: the main empirical results only show the evaluation reward/returns for the trained policies. They are insufficient to show if the proposed value estimation objective can be better in learning an accurate value estimate. Particular, those strong performance of Vlean can be (very likely) a result from other implementation tricks. The paper should consider using Monte Carlo estimate to compare the estimated values with Vlean and the groundtruth values. Moreover, the ablation studies on the replay buffer size are irrelevant to the main idea of the proposed value estimate and should be substituted with a more informative study, e.g., with/without the ratios when optimization $L(\\theta)$. The current ablation studies give no clue on the state-value-function estimation."
                },
                "questions": {
                    "value": "1. The paper states that \u201cwe aim to reduce the variance by replacing the standard importance sampling ratio with truncated importance sampling\u201d. But Algorithm 1 has no such ratio clipping \n\nSome language issues (those do not affect my assessment): \n1. \u201cThis loss from Equation 5 can be optimized\u201d Equation 5 is not a loss, but an objective to maximize."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830271135,
            "cdate": 1698830271135,
            "tmdate": 1699636936165,
            "mdate": 1699636936165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kNvc2a6EfJ",
                "forum": "NgtEafc8NZ",
                "replyto": "PMn1m8Q57F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Answer for Reviewer aLJf"
                    },
                    "comment": {
                        "value": "Dear Reviewer aLJf,\n\nThank you for your thorough review of our paper, we appreciate your time and the constructive feedback you provided. We are grateful for your positive feedback on the clarity of the research question, the well-written nature of the paper, and the comprehensive summary of related work. Additionally, we appreciate your acknowledgment of the convincing use of a trust-region method to stabilize training. We have carefully considered your questions and concerns and aim to address them in the revised version of our manuscript as well as in our answer. \n\n- **Derivation Details and Theoretical Justification:** We acknowledge your concern about the ambiguity in the derivation of the upper bound for the value function. In the revised manuscript, we have improved the notation in the main text and provided a comprehensive derivation in the appendix. In addition, we have given more insight into the method, explaining its relationship to and differences from V-trace. We have also now included a full proof establishing the consistency of the upper bound.\nWe hope that these revisions not only answer your questions about the derivation, but also provide a more intuitive understanding of the method. Should you require further clarification, we are available to provide additional information.\n\n  To already address the question regarding the summation at this point, we indeed use a one sample estimate (as does V-trace), the formulation was probably misleading there. By moving the importance sampling out of the square in Vlearn, it allows us to work with the joint expectation over states and actions (this is what we referred to as subsummation). This way having only one action sample per state is less of an issue and leads to lower variance than having a one sample Monte Carlo estimate just for the expectation over actions, such as in V-trace. This is similar to what SAC is doing by only generating one action sample for its expectation over the Q-function. V-trace effectively splits the expectations of states and actions and still uses a one sample estimate for the expectation over actions in Eq. (2), leading to potentially higher variance. \n\n- **Utility of State-Value Estimation:** We agree with the reviewer that the effects of the individual components are currently difficult to filter out. We have extended the ablation section of the paper by performing additional variations of our proposed method. However, it should already be noted here that in our experiments we already compare to V-trace in an equivalent setting, using the same techniques and only changing the way the value function is trained. Nevertheless, V-trace is not able to achieve the same performance. For this reason, we also think that a comparison with a Monte Carlo estimator is difficult, since V-trace is not able to learn a reasonable policy, which would also be reflected in a relatively poor estimate of V-trace in this comparison.\n\n- **Language & Clarity:** We acknowledge there are discrepancies between Algorithm 1 and the method as stated in the main text. In the revised manuscript, we improved the algorithm description to make it more self-explanatory and aligned. We also fixed the mentioned language issues.\n\nWe sincerely appreciate your thoughtful review and are committed to enhancing the clarity, soundness, and presentation of our work based on your suggestions. We look forward to your feedback on the revised version that aims to address the mentioned points."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497718033,
                "cdate": 1700497718033,
                "tmdate": 1700497718033,
                "mdate": 1700497718033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2qgLmdc8gC",
                "forum": "NgtEafc8NZ",
                "replyto": "kNvc2a6EfJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7690/Reviewer_aLJf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7690/Reviewer_aLJf"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal feedback"
                    },
                    "comment": {
                        "value": "Thank you for the response. I think the reponse has not answered my questions directly. I would expect the authors to give a bit more rigorous analysis on the upper bound. The new empirical results in the updated manuscript also did not meet my expectqtions. I thus keep my score unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606114837,
                "cdate": 1700606114837,
                "tmdate": 1700606114837,
                "mdate": 1700606114837,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v8bAqwh0bX",
            "forum": "NgtEafc8NZ",
            "replyto": "NgtEafc8NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_FMyp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_FMyp"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides an off-policy trust region method that using only V to learn the optimal policy. In details, this paper proposes some modification (an uppper bound learning objective) to V-trace, and learns the policy by TRPL. Vlearn is claimed to bring more efficient exploration and exploitation in complex tasks with high-dimensional action spaces. This paper tests Vlearn on both Gymnasium and DMC dog tasks, Vlearn achieves strong results across all tasks, especially on 38-dimensional dog tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Empirical results are strong, Vlearn achieves strong results on high-dimensional gym tasks and DMC dog tasks.\n- Vlearn extends ppo (or TRPL) to off-policy methods, which is of good significance."
                },
                "weaknesses": {
                    "value": "- Why does the learning objective in Vlearn work better than 1-step V-trace is still not clear to me. Despite the empirical results, it will be more sound if the author could give some (theoritical) analysis to it.\n- Although Vlearn only needs to learn V so as to bypass the problem in explicitly learning Q function representation, it seems will bring the  problem to the estimate of $\\frac{\\pi(a|s)}{\\pi_b(a|s)}$ if the action space is high-dimensional.\n- Empirical results on easy, low-dimension task such as hopper, walker, cheetah are low."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699347648051,
            "cdate": 1699347648051,
            "tmdate": 1699636936070,
            "mdate": 1699636936070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XpwP0quVOW",
                "forum": "NgtEafc8NZ",
                "replyto": "v8bAqwh0bX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Answer for Reviewer FMyp"
                    },
                    "comment": {
                        "value": "Dear Reviewer FMyp,\n\nThank you for your detailed review of our paper, we appreciate your time and thoughtful evaluation of our work. We are particularly encouraged by your positive feedback on the strong empirical results and the significance of our work. Below we aim to address the remaining questions and concerns. \n\n- **Comparison to V-trace:** By moving the importance sampling out of the square in Vlearn, it allows us to compute the joint expectation over states and actions. This way having only one action sample per state should generally lead to less variance. V-trace effectively keeps the split expectations of states and actions and still uses a single sample to estimate the expectation over actions in Eq. (2), leading to potentially higher variance. Furthermore, V-trace is interpolating between the current (target) value function and the 1-step Bellman target\n$$\n    J(\\theta) = E_{s_t \\sim \\mathcal{D}} \\Bigg[ \\Big( V_\\theta^\\pi(s_t) - \n    \\Big(\n    (1-\\rho_t) V_{\\bar{\\theta}}(s_t) + \\rho_t \\big(r_{t}+ \\gamma V_{\\bar{\\theta}}(s_{t+1}) \n    \\big)\n    \\Big)\n    \\Big)^2\n    \\Bigg].\n$$\nThe smaller the importance ratio, the more the value function optimization focuses on the current (target) value function. However, for these samples, the overall loss scale and consequently the gradient remain unchanged. In the case of Vlearn, it is different \u2013 samples with small importance ratios contribute less to the total loss but still work towards the Bellman target. V-trace was initially designed for correcting off-policy asynchronous processing, where policy differences (i.e. importance ratios) are typically limited, so this is not much of a constraint. However, in a complete off-policy scenario, the samples in the replay buffer and the current policy deviate more rapidly and significantly from each other. We tried to emphasize this insight more in the revised version to make the distinction clearer.\n\n- **Estimation of Importance Sampling Ratio:** First of all, we want to mention that we can compute the importance ratio directly from our predicted Gaussian distributions in closed form, hence, unlike the Q function, it does not need to be estimated. While in higher dimensional action spaces the two Gaussians will naturally overlap less and approach 0 faster, we found that the state-wise trust regions from TRPL help stabilize this as the policy cannot move arbitrarily fast from any previous policy distribution. This ensures there are always sufficiently many samples in the replay buffer for which the ratio is $>0$. \nHowever, we are not 100% sure, we understand the reviewer's question regarding this correctly. If our provided answer is not sufficient, please feel free to follow up on this.\n\n- **Low-Dimensional Tasks:** We agree that our performance does not provide significant improvements for the low-dimensional task, but we did not expect it to, since the added complexity of learning a Q-function is likely negligible in these cases. Therefore, similar performance is a desirable result for our method. SAC on the HalfCheetah-v4, we see as an outlier, as other off-policy methods, such as MPO, also do not perform as well on this task, and it has been shown in previous work [1] that this particular task exhibits extreme behavior.\n\nThank you once again for your review, and we look forward to your feedback of the revised version that incorporates your suggestions.\n\n### References\n[1] Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr \u0301e Biedenkapp, Kurtland Chua,\nFrank Hutter, and Roberto Calandra. On the importance of hyperparameter optimization for\nmodel-based reinforcement learning. In International Conference on Artificial Intelligence and\nStatistics, pp. 4015\u20134023. PMLR, 2021"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497511786,
                "cdate": 1700497511786,
                "tmdate": 1700497636534,
                "mdate": 1700497636534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KuwQKEHagm",
            "forum": "NgtEafc8NZ",
            "replyto": "NgtEafc8NZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_syZi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7690/Reviewer_syZi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel off-policy actor-critic reinforcement learning method called Vlearn that only learns a state-value function for advantage estimation to mitigate the curse of dimensionality in tasks with large action spaces. The algorithm is based on a state-value function loss upper bound of one that multiplies the importance sampling ratio with the Bellman target. Vlearn leverages an experience replay memory for data efficiency and employs delayed update, double-Q-learning-like training, importance sampling ratio truncation, and trust region optimization for stability.\n\nThe experiments done on the Gymnasium and DMC environments demonstrate a competitive performance of Vlearn on most of the continuous control tasks. Vlearn exhibits an impressive performance on the DMC dog tasks that previous methods struggled to learn. The empirical study concludes with an ablation study on the effect of the size of the replay buffer on the algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation for this work is easy to understand. The authors straightforwardly introduce the problem they are trying to solve and explain why it is important.\n- The background material is abundant and comprehensive, crediting the proper related works.\n- The experiments demonstrate the strength of VLearn in high-dimensional action space tasks compared to a fair selection of competitors."
                },
                "weaknesses": {
                    "value": "- The derivation skips directly to the final equation, not providing enough detail. It's also possibly erroneous. Please see the question section.\n- Some equations and parts of the pseudocode are difficult to understand due to poor explanation of the notations. I will point out some examples in the question section.\n- The ablation study only investigates the effect of the size of the replay buffer. However, Vlearn uses multiple techniques from previous works, e.g., importance sampling ratio truncation. Each of them may contribute to the reported final performance. A more comprehensive ablation study can definitely enhance the value of this work."
                },
                "questions": {
                    "value": "- I tried to justify how the authors arrived at equation 4 from equation 2 using Jensen's inequality but failed. I hope the authors can explain how Jensen's inequality is applied here. The importance sampling ratios do not necessarily sum to one. Thus, the inequality may not hold.\n- Some notations are used before definition. For example, I was unsure what $K$ represents until I got to the pseudocode to find out it was the mini-batch size.\n- The pseudocode is somewhat vague. For instance, does the for $ i \\in \\\\{1,2\\\\}$ indicate a for-loop or a random sample from the set in lines 9 and 13? What does $\\theta$ in line 11 represent? It's not in the input to the algorithm.\n- What is the motivation behind using two sets of parameters for the state-value function? The overestimation bias exists in Q-learning. I don't think it exists in the authors' formulation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7690/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7690/Reviewer_syZi",
                        "ICLR.cc/2024/Conference/Submission7690/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699477467647,
            "cdate": 1699477467647,
            "tmdate": 1700686946136,
            "mdate": 1700686946136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T8X1jZMP9X",
                "forum": "NgtEafc8NZ",
                "replyto": "KuwQKEHagm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Answer for Reviewer syZi"
                    },
                    "comment": {
                        "value": "Dear Reviewer syZi,\n\nThank you for taking the time to review our paper. We are pleased to note your positive remarks on the clarity of our motivation, the comprehensive background, and the strength demonstrated in our experiments. Further, we appreciate your valuable feedback and constructive comments. Below, we address each point to ensure clarity and address any outstanding issues.\n\n- **Derivation Details and Theoretical Justification:** We acknowledge your concern about the ambiguity in the derivation of the upper bound for the value function. In the revised manuscript, we have improved the notation in the main text and provided a comprehensive derivation in the appendix. In addition, we have given more insight into the method, explaining its relationship to and differences from V-trace. We have also now included a full proof establishing the consistency of the upper bound.\nWe hope that these revisions not only answer your questions about the derivation, but also provide a more intuitive understanding of the method. Should you require further clarification, we are available to provide additional information.\n\n- **Pseudocode:** Here, the \"for\" is just an indicator that we compute this loss for both critic networks and equivalently for the target critics in line 13, i.e. a for loop over critics. We decided on this notation since it is also commonly used in other works, e.g. SAC [1]. The $\\theta$ in line 11 is indeed inconsistent, and should be the minimum of the two critic estimates. Overall, for the updated version, we have aligned the pseudocode with the main text.\n\n- **Overestimation Bias:** We agree the overestimation bias is not a main concern in our setting as compared to the Q-function based setting. We found, however, in practice that using two networks is also beneficial in our case (see new ablation study). We assume the second network can be seen as a small ensemble that acts as a form of regularization and by selecting the minimum value, we avoid introducing any overestimation bias by training two networks. As the twin network approach is common practice for the baseline methods, we kept it also in our case.\n\n- **Effect of Individual Components:** We agree with the reviewer that an ablation study on the effects of the individual components can be of good value. We extended this section of the paper by running additional experiments. However it should already be noted here that in our experiments we compare to V-trace in an equivalent setting using the same techniques and only changing the way the value function is trained. Yet, V-trace is not able to achieve the same performance.\n\nWe sincerely appreciate your time and thoughtful consideration of our work. Should you have any additional questions or require further clarification, we are readily available to assist. Thank you once again for your valuable feedback and engagement.\n\n\n### References\n[1] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861\u20131870. PMLR, 2018."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497243453,
                "cdate": 1700497243453,
                "tmdate": 1700497650100,
                "mdate": 1700497650100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ztIoOfPiqP",
                "forum": "NgtEafc8NZ",
                "replyto": "KuwQKEHagm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7690/Reviewer_syZi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7690/Reviewer_syZi"
                ],
                "content": {
                    "title": {
                        "value": "Comment on the Revision and Follow-up Questions"
                    },
                    "comment": {
                        "value": "Thank you for the update. The current version is certainly more readable than the previous version. Though not as comprehensive as I expected, the ablation study revealed some contribution of each component to the algorithm's performance.\n\nI have some follow-up questions and comments on this version of the work.\n- Regarding equation (2), the authors claimed ``Directly optimizing this can be difficult as approximating the inner expectation with Monte Carlo samples requires multiple action executions per state, an unrealistic assumption for most RL problems.\" I wonder why multiple action execution is needed to estimate the inner expectation. The state $s_t$ is sampled from $\\mathcal{D}$. The action is also sampled from $\\mathcal{D}$. Do they come from the same transition $(s_t, a_t, r_t, s_{t+1})$? In addition, the next state $s_{t+1}$ is sampled from $\\mathcal{T}$. Does that mean we should sample $s_{t+1}$ by running the sampled $s_t, a_t$ in the environment? An analogous question goes to equation (4).\n- What is the difference between the loss in equation (4) and the loss for the off-policy semi-gradient TD(0) (Sutton and Barto, 2018, pp. 258), except for the use of a target network?\n- The derivation of the upper bound objective now includes more details but again skips Jensen's inequality part. It is not trivial to show how to move the importance sampling ratio out of the square. Let $\\delta (s, a)$ denote the TD error, $\\rho(s, a) = \\pi(a|s)/b(a|s)$ denote the importance sampling ratio. The inner expectation of $L^*(\\theta)$ is $E_b[(\\rho(s,a)\\delta(s,a))]^2$. If we directly apply Jensen's inequality, we get $E_b[(\\rho(s,a)\\delta(s,a))]^2 \\le E_b[(\\rho(s,a)\\delta(s,a))^2]$.  $\\rho(s,a)$ is trapped in the square. I assume the other reviewers are concerned with this step as well. Instead, I believe it's better to apply Jensen's inequality early on and add the importance sampling ratio afterwards: $E_\\pi [\\delta(s,a)]^2 \\le E_\\pi [\\delta^2(s,a)] = E_b[\\rho(s,a) \\delta^2(s,a)]$.\n\nGiven the current state of the work, I am going to raise my rating slightly to reflect it.\n\n---\n## Reference\n\nSutton, R. S., Bach, F., &amp; Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT Press Ltd."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686886704,
                "cdate": 1700686886704,
                "tmdate": 1700687316997,
                "mdate": 1700687316997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]