[
    {
        "title": "Transformers can optimally learn regression mixture models"
    },
    {
        "review": {
            "id": "0fuNvldMHA",
            "forum": "sLkj91HIZU",
            "replyto": "sLkj91HIZU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_sqwx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_sqwx"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the ability of transformer models to perform mixture of linear regressions. Specifically, the authors train transformer models in the task of mixture of linear regressions, in which $k$ possible weight vectors are sampled with equal probability. They show that  there exists a decoder-based transformer that can implement the Bayes optimal for this task. Furthermore, they compare the performance of the transformer models with previously proposed algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper performs extensive experimental comparisons on the transformers' performance with other algorithms. They also extend the setting of previous work from linear regression to mixture of linear regressions."
                },
                "weaknesses": {
                    "value": "It is unclear why this setting is of interest. I understand that transformers match the performance of proposed algorithms and that they could be used as an alternative for mixture of linear regressions. However, we should think of the cost training transformers and the time consumed, compared to performing any of the other algorithms.  The current models are used to perform language tasks and we are unaware on how optimization/numerical tasks could be merged with language tasks. \n\nFurthermore, I think that more details should be provided for the experimental set-up.\nLooking at the appendix of the paper, it is unclear to me how this proof is implemented. I think that the results of [1] require the design of specific encodings and entail some error, which was also not analyzed in [1], but the authors should at least show how it is controlled. I find in general the proof to be very high level and I had trouble verifying that it is correct. \n\n[1]: Aky\u00fcrek, Ekin, et al. \"What learning algorithm is in-context learning? investigations with linear models.\" arXiv preprint arXiv:2211.15661 (2022)."
                },
                "questions": {
                    "value": "1. Did the authors perform experiments in which the weight vectors $w_i^*$ are not sampled with equal probability?\n2. What models exactly the authors used? GPT2? \n3. How many samples did they use to train the models and how many to test them? Which is also the sequence length for in-context learning? Is the sequence length during training the same as the one during inference? \n4. How in the proof of lemma 2, the authors simply select $W_K H:,i = I_{2x2}$ since this matrix is required to have the second dimension equal to the sequence length.\n5. Do the authors propose the use of transformers for this task ? \n6. In the discussion section it is mentioned that \"The fact that transformers... quite useful for practical problems\". Could the authors mention some of those problems in which transformers would be useful ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Reviewer_sqwx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698942518391,
            "cdate": 1698942518391,
            "tmdate": 1700702807654,
            "mdate": 1700702807654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dnlsi2I2as",
                "forum": "sLkj91HIZU",
                "replyto": "0fuNvldMHA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review; our comments and responses are below."
                    },
                    "comment": {
                        "value": "We thank you for your detailed comments and review. \n\nWe would like to begin by pointing out that although transformers could be in the worst-case sense slower than other prediction methods, we found that in practice the transformers actually do perform significantly faster than the state of the art algorithms (such as the EM procedure and Subspace Algorithm used as comparisons in our work). This can potentially be explained by the fact that transformers can leverage GPU acceleration and parallelism that some of these other algorithms cannot. \n\nTo answer your question (\u201cdo the authors propose the use of transformers for this task?\u201d): indeed, we believe we are the first to propose transformers or mixtures of linear (or as in our updated revision, non-linear) regressions.  Moreover, we want to emphasize the novelty in our Section 3.2: we observe that not only can transformers adapt to unknown mixture models, they can also do so in a sample-efficient way. This is shown by keeping the sample size used to train the transformer as well as other state-of-the-art algorithms fixed and the same, and then showing that the transformer has essentially the same (or even better) performance at inference time. We also want to emphasize that in the non-linear setting, we are not aware of any algorithms which are able to guarantee good prediction of the labels, and surprisingly the transformer seems to be applicable to this setting with no additional changes to the underlying architecture. \n \nMoreover, we would like to point you to Appendix C which describes the answers to your questions about the models and training parameters. Our model is the same as that used in [2], and our training methodology only differs by the parameter choices described in Appendix C.1. If you have questions which are not addressed by that section, please let us know and we can easily update it in the revision. Finally, we have also provided our exact code along with the submission in case there is any concern regarding reproducibility.  \n\nFinally, we thank you for your detailed reading of our original submission; we addressed your concern regarding the size of the identity matrix (it was a typo in the original submission), and added commentary regarding the hidden approximation errors as addressed in [1].\n\n[1] Aky\u00fcrek, Ekin, et al. \"\u200b\u200b What learning algorithm is in-context learning? Investigations with linear models.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[2]  Garg, Shivam, et al. \"What can transformers learn in-context? a case study of simple function classes.\" Advances in Neural Information Processing Systems 35 (2022): 30583-30598."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532146731,
                "cdate": 1700532146731,
                "tmdate": 1700532274617,
                "mdate": 1700532274617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TDaFaWtOEj",
                "forum": "sLkj91HIZU",
                "replyto": "0fuNvldMHA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "One additional update, based on your questions."
                    },
                    "comment": {
                        "value": "Additionally, we want to thank you for the question regarding different weights for mixture components. In the updated revision, we have added a section (Appendix F) where we discuss the sensitivity of the models to different mixture proportions during inference. Since the MSE for any set of mixture proportions is upper (resp., lower) bounded by the maximum (resp., minimum) MSE for a component, we have plotted the performance of each algorithm for varying prompt lengths, on the worst and best mixture component. Any other mixture distribution with the same weights but different proportions must have MSE lying between these lines (see Figure 12). Additional discussion appears in Appendix F. The qualitative takeaway is that the models are generally not very sensitive to the choice of inference-time mixture proportions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598098921,
                "cdate": 1700598098921,
                "tmdate": 1700598098921,
                "mdate": 1700598098921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g8XdX60hDo",
                "forum": "sLkj91HIZU",
                "replyto": "TDaFaWtOEj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_sqwx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_sqwx"
                ],
                "content": {
                    "title": {
                        "value": "Further question"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response and extensive experiments. I would like to further ask about the implementation of the construction. It seems that the $w_i^*$ are known to the model to be able to represent the proposed estimator. I think that the real question is how the model extracts this optimal coefficients $w_i^*$, which they cannot be known a-priori. Do the authors have any intuition on this? \n\nI have updated my score to weak accept since the experiments are extensive, however I do not think that the provide theory is indicative of what the model actually learns."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702790217,
                "cdate": 1700702790217,
                "tmdate": 1700702790217,
                "mdate": 1700702790217,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mDYfbX8hbU",
                "forum": "sLkj91HIZU",
                "replyto": "0fuNvldMHA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responding to your question."
                    },
                    "comment": {
                        "value": "Thank you for your very interesting question. Before responding to your main question, we also want to clarify where the weights are being stored in our construction. Our weights are stored by leveraging the affine operation in [1] (see top of page 14 in our draft). Since we leverage their construction, the mixture component parameters are stored in the feedforward part of the attention layer as in [1] (please see their equations (26) and (67) with Lemma 2 to verify this). \n\nYour main question, regarding \u201chow the model extracts this optimal coefficients,\u201d is certainly very interesting. However, we want to start by acknowledging that this is still open even in the case of simple linear regression (i.e., without any mixture distribution). Though better theoretical understanding of how transformers actually perform parameter estimation is important, we note that this is not a claimed direction of our work. More importantly, we feel strongly that it is quite far outside of the scope of this paper. Indeed, mixtures of (even linear) regressions is actually a challenging statistical setting that has led to numerous papers (for instance see section 1.2 in [3] for a list of over 20 recent papers in this area) and the fact that transformers can seamlessly solve this problem is\u2014in our view\u2014impressive. Moreover, our observations carry over to nonlinear settings as shown by Appendix E and as we show in section 3.2, this is also occurring in a sample efficient fashion. \n\nTo further justify why we believe your question must be deferred to future work, we want to raise two important points. First, regarding the weights not being known \u201ca-priori\u201d: while it is certainly true that the weights are not known to the transformer a-priori, it is actually information theoretically possible to learn the weights consistently as the number of prompts tends to infinity while keeping the prompt length size fixed. (For instance, one can see this by applying Theorem 2 of the paper [2] in the case k = 1. This clearly also implies the existence of a consistent procedure when k > 1 as well, but one can also apply the main results in Section 2.4 of [3] when k > 1.) Therefore, although our construction requires storage of the exact weights, an interesting hypothesis to study in future work is that the transformer potentially stores an approximate version of the weights (up to possibly a linear transformation). To stress this point further, the papers we just cited also show that there is no theoretical barrier to this possibility. \n\nSecondly, your question is equally applicable to the construction in the paper [1]. Note that they also use their affine operation to guarantee that a transformer can implement 1-step of SGD (please see Appendix A, pg. 13 in [1]). There is no argument in [1] that such a mechanism is necessarily the one they observed occurring in practice, but rather they use the construction simplify to argue that transformers can carry out a sequence of computations. That is the same spirit in which we view our construction: it simply shows that there is a transformer which can carry out the sequence of operations needed to implement our optimal method. Of course, it is unclear that this is necessarily the one that is used in the final trained model (which is of course a very challenging question to address). \n\nWe have also updated our paper by adding additional discussion on the construction at the end of the introduction in Section B.2. Finally, we want to thank you again for raising your question. We agree it is very important, but on the other hand, we also feel that our paper highlights numerous important findings for transformers as applied to the statistical challenging mixtures of linear and nonlinear regressions settings. These findings we believe should influence further theoretical progress and empirical work in this area. \n\n[1] Aky\u00fcrek, Ekin, et al. \" What learning algorithm is in-context learning? Investigations with linear models.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Kwon, Jeongyeol, and Constantine Caramanis. \"EM converges for a mixture of many linear regressions.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\n[3] Jain, Ayush, et al. \"Linear Regression using Heterogeneous Data Batches.\" arXiv preprint arXiv:2309.01973 (2023)."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710379133,
                "cdate": 1700710379133,
                "tmdate": 1700710433660,
                "mdate": 1700710433660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iSA7dKZ6Lg",
            "forum": "sLkj91HIZU",
            "replyto": "sLkj91HIZU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_KcGJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_KcGJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors explore whether transformer architectures can be trained to learn mixtures of linear models from batched data. They find that Transformers are surprisingly effective at this empirically, and they support their empirical results with a constructive proof that the optimal solution is representable in a transformer architecture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I should note that this paper is out of area for me, and while I followed all the details, I may have missed some of the broader context.\n\nI thought this a was well executed paper:\n - They give a nice constructive argument that shows that transformers can implement mixtures of linear regressions. \n - The experiments are very interesting in that they not only show that mixtures of linear regressions can be learned (this is perhaps not surprising given Garg et al's recent results showing this for linear regression), but that they are also competitive in terms of sample complexity with state of the art algorithms for this problem. I would have expected that you pay a larger cost for generality."
                },
                "weaknesses": {
                    "value": "When I got to the end of the experimental section, I felt that there was a missed opportunity to look at whether Transformers allow one to easily go beyond the linear mixtures setting. While it is very interesting that Transformers are competitive with recent specialist algorithms for the linear mixture setting, I think the key advantage of a black-box method like a transformer is the ability to directly apply it to settings whether the linear mixture assumptions fail. Investigating how performance degrades (if at all) would have been interesting and would have potentially allowed you to show where Transformers outperform existing approaches.\n\nThis is brought up in the future work section of the discussion, but I think it would have been better to include it in this paper."
                },
                "questions": {
                    "value": "I am most curious about whether you have experimented with any of the questions raised in your next steps. For example,\n\n> in practice, the regression function within each component could potentially be nonlinear. To what extent do transformers perform well in these settings? \n\n> In general, the decision-theoretic optimal method could be more complicated to compute, as implementing the posterior mean would require computing a high-dimensional integral. Nonetheless, is it possible to approximate the optimal method with a trained transformer? \n\nI would be very happy to increase my score if you could show some experiments that show whether Transformers easily generalize beyond the linear Gaussian case (or not - a negative result could also be interesting if it is explained)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Reviewer_KcGJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698987088230,
            "cdate": 1698987088230,
            "tmdate": 1700579019214,
            "mdate": 1700579019214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OAko6wTJAW",
                "forum": "sLkj91HIZU",
                "replyto": "iSA7dKZ6Lg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review; our comments and response are below."
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review. \n\nWe wholeheartedly agree that the non-linear mixture setting is quite important in practice. Additionally this is indeed a key advantage to transformers: they can potentially flexibly extend to nonlinear data. To address your concern, we have taken your suggestion and presented in Appendix E extensive additional new experiments along these lines. There, we present new simulation results when the conditional mean of the label (given the covariate x) is either a multivariate polynomial or a function which itself is expressible via a multilayer perceptron. In both cases, our previous observations from linear mixture models do in fact carry over: the trained transformer is able to achieve nearly the Bayes-optimal risk (that is, the risk achieved by the oracle which knows the exact model used to generate the data). Please see Appendix E for further discussion and details. \n\nWe believe that these experiments do support the claim that transformers can adapt to unknown (even nonlinear) mixture models, which should be of considerable interest to practitioners who work with data that is reasonably modeled as coming from a mixture distribution. In your original review, you had mentioned that you would \u201cbe very happy to increase [your] score\u201d on the basis of such new experiments, so we hope that you will review the new experiments and let us know if you have additional concerns during the discussion period."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532091921,
                "cdate": 1700532091921,
                "tmdate": 1700532091921,
                "mdate": 1700532091921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xaY0DzD8fs",
                "forum": "sLkj91HIZU",
                "replyto": "OAko6wTJAW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_KcGJ"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_KcGJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the updated experiments, I have increased my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579002620,
                "cdate": 1700579002620,
                "tmdate": 1700579002620,
                "mdate": 1700579002620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BmnXfdxZuC",
            "forum": "sLkj91HIZU",
            "replyto": "sLkj91HIZU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_1RLx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_1RLx"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the performance of transformers on in-context learning problems consisting of mixtures of linear regression models, where the prompt consists of input-output pairs coming from a linear model with one of $m$ different target weight vectors.\nThe authors show that the posterior mean in this problem may be implemented with a well-chosen transformer, and that empirically the predictions behave similarly to this algorithm, in particular outperforming OLS and EM approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper makes an interesting contribution in the recently popular literature on using transformers for in-context learning regression models.\n\nThe posterior mean in eq.(4) is particularly interesting as a desired goal, as it requires a more complex transformer architecture than related papers, which combines both in-context algorithmic operations, and some \"knowledge\" from the data distribution, in the form of the $w_i^*$ vectors.\n\nThe experiments seem to give promising evidence that this target model may indeed resemble the one learned by transformers."
                },
                "weaknesses": {
                    "value": "Two points would significantly strengthen the paper:\n\n* while empirical results suggest the transformer might be related to the posterior mean, it would be good to have some interpretability results to assess whether this is true in practice, and if your construction in Theorem 1 is practically relevant: is there any evidence that the blocks shown in Figure 1 are actually being learned by the pre-trained transformer? Where are the $w_i^*$ being stored in the weights? Is it necessary to have at least 5 layers in practice, as in your construction? Is it sufficient?\n\n* a more extensive empirical analysis would be useful, particularly on how the results vary when changing problem parameters. For instance, how does the performance change as $m$ varies? In particular, it seems difficult to find all the hidden directions $w_j^*$ once $m$ is too large -- does it start resembling OLS at some point? Does increasing the width, number of heads, number of layers change this?\n\n* related work: is your setting covered by the general setting in [this paper](https://arxiv.org/abs/2306.04637)?"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Reviewer_1RLx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699136702784,
            "cdate": 1699136702784,
            "tmdate": 1700699522752,
            "mdate": 1700699522752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ny6OUXTVO6",
                "forum": "sLkj91HIZU",
                "replyto": "BmnXfdxZuC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed review; our comments and response are below."
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. \n\nWe would like to begin by pointing out that the methodology we use in Section 3.2 to assess the similarity between the transformer and the posterior mean algorithm is essentially the same approach that is used in the paper [1] to compare multiple algorithms\u2019 similarity. Additionally, similar to that paper, we construct a transformer which is potentially larger than necessary to implement the posterior mean algorithm (note that our mathematical construction requires more than 5 layers, as is shown in section B.2.1). Nonetheless, our models are much smaller than the theory suggests in the case of 20-component mixtures, which indicates that transformers can still perform very well even with smaller models than theory can potentially guarantee will succeed. This type of gap we leave open, as is done in [1], where they similarly have a gap between the size of the model implemented and the size of the model used in theory. \n\nAdditionally, thank you for the pointer to the linked paper, we have added that paper to our extended related work, please see Appendix A. The paper is very insightful and definitely related to the  current work, however we believe our paper is substantially different from this paper. In particular, that paper does not consider mixtures of regressions of the form considered in this paper. (While they do consider 1 mixture-of-linear-regression setting, in that model they do not change the conditional mean, only the noise level. This makes their setup arguably easier than ours: for instance, ERM over all the data is consistent in their setting while it is not in ours). Moreover, that paper does not do any empirical comparison of sample complexity (such as we carry out in Section 3.2); sample efficient learning is essential in problems where one cannot collect additional data for free. Our results highlight that transformers are very general purpose, but more importantly and more strikingly, also very sample-efficient, since they achieve nearly the same error as state-of-the-art model-specific methods, with the same sample size.  \n\n[1] Aky\u00fcrek, Ekin, et al. \"\u200b\u200b What learning algorithm is in-context learning? Investigations with linear models.\" The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532040169,
                "cdate": 1700532040169,
                "tmdate": 1700532040169,
                "mdate": 1700532040169,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YFg15YWdpI",
                "forum": "sLkj91HIZU",
                "replyto": "BmnXfdxZuC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "One additional experiment added to address your question."
                    },
                    "comment": {
                        "value": "Thank you for your suggestion to investigate the relationship between the number of components, m, and the gap between the transformer and linear regression methods. First, we want to point out that it is more natural under our setup (where we have noise and sample weights from a distribution on a norm-constrained set) to consider the convergence of the transformer to ridge regression (with appropriately set penalty), as this is approximately equal to the Bayes estimator for this setup; see the papers [1, Corollary 3] and [2, Section 3.1.1] for additional details to justify this claim. \n\nTherefore, following your suggestion, we have now revised the paper to compare the ratio of the risk of transformer and ridge regression as a function of the number of components; please see Appendix G and Figure 13. The qualitative takeaway is: as the number of components grow, it is indeed true that the ratio of the risk of ridge regression to transformers is getting smaller, as your question predicted. Note also that in [3] they already show that transformers are close to OLS when sampling the weights from a Gaussian distribution. Note that in their setting, there is no noise, and so the optimal ridge estimator is in fact with penalty taken to be zero, i.e., OLS itself. Therefore, their observations are also consistent with our experiments: in the large m limit OLS and ridge regression are more competitive, but impressively, the transformer is able to adapt to this structure with out any modification to the model architecture.\n\n[1] Dicker, Lee H. \u201cRidge Regression and Asymptotic Minimax Estimation over Spheres of Growing Dimension.\u201d Bernoulli, vol. 22, no. 1, Feb. 2016.\n\n[2] Reese Pathak, Martin J. Wainwright, Lin Xiao. \u201cNoisy recovery from random linear observations: Sharp minimax rates under elliptical constraints.\u201d arxiv, 2023\n\n[3] Garg, Shivam, et al. \"What can transformers learn in-context? a case study of simple function classes.\" Advances in Neural Information Processing Systems 35 (2022): 30583-30598."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697362980,
                "cdate": 1700697362980,
                "tmdate": 1700697374252,
                "mdate": 1700697374252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7C6WvNSWRW",
                "forum": "sLkj91HIZU",
                "replyto": "YFg15YWdpI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_1RLx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_1RLx"
                ],
                "content": {
                    "title": {
                        "value": "thank you"
                    },
                    "comment": {
                        "value": "Thank you for your response. The new experiments are interesting, thanks for including them! I still think it would be good to interpret whether the trained model does resemble the proposed construction, and where the $w_i^*$ are being stored, but I understand that it's not an easy question. I am raising my score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699672079,
                "cdate": 1700699672079,
                "tmdate": 1700699672079,
                "mdate": 1700699672079,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d2HNcVEnfZ",
            "forum": "sLkj91HIZU",
            "replyto": "sLkj91HIZU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_wAvt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_wAvt"
            ],
            "content": {
                "summary": {
                    "value": "The authors argue that transformers provide a simple mechanism to efficiently and accurately learn mixtures of linear regression models.  They prove that the optimal solution is representable by such transformers and then experimentally verify that sample complexity/accuracy is on par with existing methods for this task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is a simply presented, well-articulated problem and solution.  The presentation is clear and more or less self-contained."
                },
                "weaknesses": {
                    "value": "I'm not sure that the new work really address limitations in the existing literature.  The main motivation is that existing methods are potentially brittle and that their theoretical guarantees do not extend to the model misspecification setting.  It isn't clear that this work really demonstrates much of an improvement in that regard.  As a result, it isn't clear to me what this approach really offers (other than perhaps simplicity?).  It too does not come with any guarantees more generally -- or maybe I have misunderstood?\n\nAlso, it also feels like this result is a bit preliminary and could potentially encompass a wider range of mixture models and statistical settings."
                },
                "questions": {
                    "value": "Questions are in the weakness section above. \n\nMinor typos/suggestions:\n- \"gradient descent would naturally extends\"\n- \"definitions in display (4).\" -> \"definitions in (4).\"\n- \"better predictor to adapted to the mixtures of linear regressions setting\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699253184938,
            "cdate": 1699253184938,
            "tmdate": 1699636667112,
            "mdate": 1699636667112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7M5JWd6XX5",
                "forum": "sLkj91HIZU",
                "replyto": "d2HNcVEnfZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review; our comments and response are below."
                    },
                    "comment": {
                        "value": "Thank you for your review and your constructive comments. \n\nWe would like to begin by addressing your question regarding the limitations that this work addresses. Indeed, we would like to point out that in many settings, such as federated learning, one has to select the choice of the number of components in the mixture model by hand. For instance see papers [1, Section 2] and [2, Section 4.2]. This is quite tedious, and it is indeed of interest to have general purpose prediction methods which do not need to deal with the unknown parameter of the number of components in a mixture model. \n\nRegarding additional benefits of our work, we believe that transformers is potentially desirable due to its ability to adapt to the unknown mixture structure. In our original experiments, this was shown by employing the exact same transformer architecture on models with different numbers of components (5 or 20) and different noise levels (0 or 1). In our revision we also show transformers can extend to nonlinear models (see more discussion below). In all of these cases, we show that our trained transformer can achieve the same error as the posterior mean and argmin algorithms which are the decision-theoretically optimal methods and are very strong comparisons since they are oracle methods that have knowledge of the model. This, we believe, is quite striking: transformers have absolutely no model knowledge, yet nonetheless are performing as well as methods which have exact knowledge of the precise model. \n\nMoreover, we believe the sample complexity comparison done in Section 3.2 is very novel. Most other transformer papers do not consider the sample complexity of achieving a transformer which achieves low generalization error. Our experiment fixes the training sample size and shows that indeed transformers are nearly as sample-efficient as recently proposed, quite complicated methods which are model-specific. This is quite relevant for practice, where one may not have the luxury of seeking unlimited data and must cope with a finite, and perhaps only moderate-in-size sample. We view these results as quite surprising: transformers have absolutely no explicit knowledge of the model structure and yet nonetheless are achieving error on par with state-of-the-art methods, and with exactly the same sample complexity. \n\nFinally, we have further strengthened our results to address your concern about the limited scope of our original set of statistical settings. For instance, in appendix E, we have done extensive additional experiments, where we present new simulation results when the conditional mean of the label given the covariate x is either a multivariate polynomial or a function which itself is expressible via a multilayer perceptron. In both cases, our previous observations from linear mixture models do in fact carry over: the trained transformer is able to achieve nearly the Bayes-optimal risk (that is, the risk achieved by the oracle which knows the exact model used to generate the data). Please see Appendix E for further discussion and details. We believe that these experiments do support the claim that transformers can adapt to unknown (even nonlinear) mixture models, which should be of considerable interest to practitioners who work with data that is reasonably modeled as coming from a mixture distribution. \n\nFinally, we addressed the typos you pointed out. Thank you for your very close reading of our initial submission.\n\n[1] Sattler, Felix, et al. \u201cClustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints.\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 8, Aug. 2021, pp. 3710\u201322. \n\n[2] Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh. \u201cThree Approaches for Personalization with Applications to Federated Learning.\u201d arxiv, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531982192,
                "cdate": 1700531982192,
                "tmdate": 1700533321940,
                "mdate": 1700533321940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oOcSAjPv5K",
            "forum": "sLkj91HIZU",
            "replyto": "sLkj91HIZU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_Pvi3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6154/Reviewer_Pvi3"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows that transformer architectures can implement a mixture of linear regressions. Namely, it can implement the optimal solution that uses the true underline model parameters. The authors showcase their claim via a sequence of experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The idea of the paper is original and novel.\n* The paper is written well. The illustration of the proof adds to the clarity of it and gives a good intuition.\n* In general, the experiment section is good (although I think it misses some things; but more on that in the next section).\n* Code was provided and the results seem to be reproducible."
                },
                "weaknesses": {
                    "value": "* I am not convinced about the significance of this work. To clarify that, I would like the authors to address the following questions, how and when can one use the observation in the paper? In the introduction, federated learning is mentioned as a possible application. But, federated learning systems do not use linear models and if they do the number of components is known in advance (which is arguably the biggest advantage of the proposed viewpoint in the paper). I acknowledge though that this work can be a stepping stone towards a mixture of non-linear models, which potentially can have more impact.\n* I do not have experience with mixture models. But, to me, it seems that a clear advantage of using mixture models is the access to the underline model and the mixture components. This is somewhat lost here. Having an approximation for the posterior mean only is nice, but I assume that in many cases one wants to evaluate each mixture separately in order to make a choice.\n* Unless I didn't understand something, a potential issue for taking the viewpoint of this paper is that training transformers can be demanding in computation and data. I would expect that in most cases one would like to use these types of models in exactly the opposite cases, i.e., small training sets with limited computation. \n* In section 3.1, under the noisy case, it is not surprising that the OLS model does not work as well since it doesn't model the noise. In my opinion, a more appropriate comparison would be an OLS model with a hyper-parameter for the noise variable which is chosen based on a validation set. Especially in light of the fact that a grid search was done for the dropout rate of the proposed approach. Conversely, and perhaps even more appropriate, is to compare to a Bayesian model and either optimize the noise via the marginal likelihood (or ELBO) or give it a full Bayesian treatment.\n* To complement the question of \"What is the transformer actually learning?\" in section 3.3., I believe that some form of evaluation on out-of-distribution data be done. I suspect that the transformer learns an approximation for the posterior mean only in regions of in-distribution, but outside of it, it will behave in an arbitrary fashion. On the other hand, we know exactly how the posterior mean solution will behave in every region. Perhaps the authors can verify that using a similar experiment to the one in section 3.3 or on a simple 2D problem. If that is indeed the case, then how can you guarantee that indeed the solution found by the transformer matches that of the posterior mean?"
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6154/Reviewer_Pvi3"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6154/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699437969515,
            "cdate": 1699437969515,
            "tmdate": 1700661277693,
            "mdate": 1700661277693,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PwxTCAVRVA",
                "forum": "sLkj91HIZU",
                "replyto": "oOcSAjPv5K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed review; our comments are below."
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. \n\nWe would like to begin by addressing your comments on federated learning. We want to point out that in federated learning applications, it is indeed not always the case that \u201cthe number of components is known in advance.\u201d For instance, please see the papers [1, Section 3] and [2, Section 4.2]. In these works, the number of clusters is either learned or treated as a hyperparameter; it is not fixed ahead of time. In fact, as the latter paper says, it is often advisable to \u201cfind clusters for several values of [the number of clusters] and use the best one for each client separately using a hold-out set of samples.\u201d \n\nRegarding motivation, we also wanted to point out that yet another motivation for this work can be found in pretraining LLMs, where one wishes to use diverse data (such as mixture data) with the hope that the transformer can zero-in on the relevant aspect (such as the correct mixture component). We study the simplest linear analogue of this. We also have revised the introduction to include this motivation as well as reference recent literature sharing in this motivation (see the blue text in the introduction of the revision). \n\nRegarding other predictors, we disagree that the reason the OLS model does \u201cnot work as well\u201d is due to a lack of \u201cmodel[ing] the noise.\u201d In contrast, OLS is actually known to be decision-theoretically optimal in the scenario where the learner only has knowledge of the noise level (see for instance paper [3, Theorem 1] below). Instead, it is due to the norm constraint we employ on the linear regression model that improvements can be had (along with, of course the Bayesian structure of the generative process: a discrete mixture). Thus, we do agree with you that a better procedure would use the structure of the parameter (although, notably this is a somewhat unfair comparison, since the transformer does NOT have this knowledge). \n\nNonetheless, following your suggestion we implemented the optimal ridge regression model under the assumption that the parameter is norm constrained, which consists of regularizing the parameter at the level of the noise level (see for instance the papers [4, Corollary 3] and [5, Section 3.1.1] below for justification of this claim). Moreover, as you seemed to suggest, this can also be seen as a MAP estimator under the Gaussian prior which concentrates on the scaled sphere in R^d. These results are now included in the revised version of Figure 2. Importantly, transformers still significantly outperform ridge regression. \nNote that the only change in the comparison qualitatively is that ridge regression is better than OLS when the prompt length is similar to the ambient dimension. \n\nRegarding other comparisons, please also note that throughout our paper we already compare the transformer to the optimal method which is the running posterior mean: see for instance our Figure 3. Note that the posterior mean is an oracle method since it knows the exact mixture distribution; surprisingly, the transformer is nonetheless able to achieve nearly the same performance as the posterior mean. This should be interpreted as: the transformer is doing essentially as best as possible for the settings described in our paper. \n\nRegarding out of distribution data, we would like to point out that figures 5 and 6, when compared to figures 8 and 9 in the appendix actually already comprises the comparison you asked for. We compare the performance of the transformer on various out of distribution settings and show that it behaves quite similarly to the posterior mean algorithm on these settings. We also have some discussion of this point in the paragraph immediately before the \u201cDiscussion\u201d heading in the main text. To clarify one point, we also do agree that the \u201ctransformer learns an approximation for the posterior mean only in regions of in-distribution.\u201d Nonetheless, the similarity to the performance of the posterior mean algorithm on covariate scaling experiments shows that it does gracefully degenerate in the same manner as the optimal in-distribution algorithm when evaluated in OOD settings. We would also like to point out that our methodology to justify the similarity of the transformer and the posterior mean is precisely the same as the paper [6]. \n \n(continued in next comment)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531894968,
                "cdate": 1700531894968,
                "tmdate": 1700531894968,
                "mdate": 1700531894968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xw2pish5od",
                "forum": "sLkj91HIZU",
                "replyto": "oOcSAjPv5K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continuation of response to your original review."
                    },
                    "comment": {
                        "value": "(continuation from above) \n\nRegarding the practicality of our setting, we would also like to point out that in the revision, we have attempted to take yet a further step towards addressing more practical and nonlinear models. For instance, in Appendix E, we have done extensive additional experiments, where we present new simulation results when the conditional mean of the label (given the covariate x) is either a multivariate polynomial or a function which itself is expressible via a multilayer perceptron. In both cases, our previous observations from linear mixture models do in fact carry over: the trained transformer is able to achieve nearly the Bayes-optimal risk (that is, the risk achieved by the oracle which knows the exact model used to generate the data). Please see Appendix E for further discussion and details. We thank you for pointing out that nonlinear data \u201ccan have more impact\u201d and hope the strong performance exhibited in our experiments can address this aspect of your previous concerns. \n\n[1] Sattler, Felix, et al. \u201cClustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints.\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 8, Aug. 2021, pp. 3710\u201322. \n\n[2] Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh. \u201cThree Approaches for Personalization with Applications to Federated Learning.\u201d arxiv, 2020. \n\n[3] Mourtada, Jaouad. \u201cExact Minimax Risk for Linear Least Squares, and the Lower Tail of Sample Covariance Matrices.\u201d The Annals of Statistics, vol. 50, no. 4, Aug. 2022. \n\n[4] Dicker, Lee H. \u201cRidge Regression and Asymptotic Minimax Estimation over Spheres of Growing Dimension.\u201d Bernoulli, vol. 22, no. 1, Feb. 2016.\n\n[5] Reese Pathak, Martin J. Wainwright, Lin Xiao. \u201cNoisy recovery from random linear observations: Sharp minimax rates under elliptical constraints.\u201d arxiv, 2023 \n\n[6] Aky\u00fcrek, Ekin, et al. \"\u200b\u200b What learning algorithm is in-context learning? Investigations with linear models.\" The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531920268,
                "cdate": 1700531920268,
                "tmdate": 1700531941158,
                "mdate": 1700531941158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XX6nAvEMZS",
                "forum": "sLkj91HIZU",
                "replyto": "xw2pish5od",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_Pvi3"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission6154/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6154/Reviewer_Pvi3"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the answers and the additional experiments conducted following mine and other reviewers comments. In my opinion, the authors addressed most of the concerns raised adequately. Therefore, I decided to raise my score to by two levels to 8."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6154/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661257089,
                "cdate": 1700661257089,
                "tmdate": 1700661257089,
                "mdate": 1700661257089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]