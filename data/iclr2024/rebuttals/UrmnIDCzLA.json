[
    {
        "title": "Overcoming bias towards base sessions in few-shot class-incremental learning (FSCIL)"
    },
    {
        "review": {
            "id": "AlgAeBMfYU",
            "forum": "UrmnIDCzLA",
            "replyto": "UrmnIDCzLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2257/Reviewer_qGui"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2257/Reviewer_qGui"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of Few-Shot Class-Incremental Learning (FSCIL). FSCIL consists of two stages: a base session involving training on a large-scale base dataset and incremental sessions with a few-shot setting. The evaluation metric used in previous methods is the mean accuracy of all test samples, where performance is dominated by the base classes due to their larger number of test samples. The performance in the incremental sessions alone is much worse than in the base session. To mitigate this bias towards the base classes and balance the two learning stages, this paper proposes an investigation into the knowledge of pre-trained models and classifier types. A well-pre-trained model is utilized and kept frozen, along with an updating base model. Three types of classifiers are explored, and those demonstrating superior performance in both the base and incremental sessions are combined for further improvement. Additionally, new evaluation metrics are introduced to separately assess issues like forgetting and incremental performance. Two new benchmarks are also proposed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The dominance of base class accuracy as an evaluation metric for previous FSCIL methods is indeed unfair. The proposed evaluation metrics provide a more intuitive benchmark for evaluation.\n- A baseline method can be significantly improved by simply utilizing and aggregating a frozen pretrained model.\n- Combining various types of classifier heads has also been shown to be beneficial."
                },
                "weaknesses": {
                    "value": "- The overall method seems less novel: utilizing the frozen pre-trained backbone to improve the forgetting issue is not new (e.g., [1]). The runtime increases for inference, while [1] does not as the knowledge merging happens in the parameter space; aggregating the prediction from multiple classifiers is an ensemble operation.\n- Regarding lightweight network (LN), catastrophic forgetting is overcoming by replay the samples from previous classes? Such process violates the setting of FSCIL, where accessing the data from multiple incremental sessions is prohibited. \n- What is the rationale behind choosing ALICE as the baseline model over other options? Is this choice based on heuristics or specific reasons? \n- In the CUB200 benchmark, if the settings remain consistent with those in other papers, why do the previous methods listed in Table 3 exhibit significantly lower performance, even on the base classes (a_0), compared to their reported results in their original papers?\n- The comparison with prior methods may not be entirely fair, as all of them exclusively use ResNet-18 or ResNet-20, which have significantly fewer parameters compared to ResNet-50 or ViT. Is the observed performance boost primarily attributed to the larger number of parameters?\n- An essential baseline is missing: Would fine-tuning the pre-trained model on the base classes and using it as a new \"frozen pre-trained model\" lead to an overall performance improvement? In the current method pipeline, it assumes that the pre-trained model possesses knowledge highly correlated with each dataset. However, this assumption may be vulnerable. \n\nDataset:\n- The two proposed benchmarks, Flower102 and DCC, may not be entirely suitable for FSCIL. The fundamental concept of FSCIL involves a large-scale dataset in the base session, which is reasonable for offline data collection and training. However, Flower102 and DCC have relatively small scales, with only 20 and 80 images per class for the base sessions, totaling 1200 and 5360 images, respectively. This can pose challenges for algorithm development, particularly as models grow larger. Additionally, the limited number of test images can result in significant evaluation variation\n\n\nA missing prior work: Liu et al. \"Few-Shot Class-Incremental Learning via Entropy-Regularized Data-Free Replay.\" ECCV 2022.\n\n[1] Zhang et al. \u201cGrow and Merge: A Unified Framework for Continuous Categories Discovery.\u201d NeurIPS 2022."
                },
                "questions": {
                    "value": "- In Table 8, are the experiments conducted excluding the baseline model as in Fig. 2?\n- Should \u201cFlowers102\u201d be replaced by \u201cFlowers100\u201d as only the first 100 classes are used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2257/Reviewer_qGui",
                        "ICLR.cc/2024/Conference/Submission2257/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590357301,
            "cdate": 1698590357301,
            "tmdate": 1700516644473,
            "mdate": 1700516644473,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QVgUAUW8G5",
                "forum": "UrmnIDCzLA",
                "replyto": "AlgAeBMfYU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our sincere gratitude for the time and effort you dedicated to reviewing our paper. Our response to the weakness and questions is present below.\n\nWeakness_1. \n\nA: Although aggregating predictions falls under the category of ensemble operations, we claim that our method differs significantly from conventional ones. Unlike typical ensemble methods that do not specify particular types of classification modules, our pipeline employs two distinct types: the base-favored and incremental-favored modules (Sec 3.4). It is crucial to note that simply combining a large number of classification modules does not necessarily lead to improvement. This is because many current methods excel only in the base, and combining such methods does not address the persisting issue of performance bias. As we have proposed in Sec 5.1, incorporating a classification module favoring the incremental session is essential for achieving overall balanced performance.\n\nWeakness_2.\n\nA: When training LN, we do not replay samples from the previous sessions. As we have described in the supplementary material (A), we trained a Gaussian generator for each class in each session (which is inspired by the setting of FeSSSS) and subsequently sampled data from the generator for the respective class.\n\nWeakness_3.\n\nA: While SAVC and BiDistFSCIL are the latest approaches, the high IMB values in Table 3 indicate that their performance exhibits a pronounced bias toward the base session. This results in an inflated value of the biased conventional metric, misleadingly suggesting proficiency as FSCIL methods. Conversely, ALICE achieved the most balanced performance according to our proposed metrics. Therefore, we decided to use ALICE as our baseline.\n\nWeakness_4.\n\nA: The discrepancy arises from our evaluation using the newly proposed metrics. While previous approaches assess the performance for all classes encountered up to the current session after training at each session, we assess session-level adaptability by exclusively evaluating the classes relevant to a specific session only after training on the last session (Sec 4.2). Hence, our reported values differ from those provided in the original papers.\n\nWeakness_5.\n\nA: We acknowledge that the improvement in performance compared to existing methods can be attributed to the larger backbone. We would appreciate it if the focus could shift from such a perspective to \u2018exploring how to effectively utilize backbones with a larger number of parameters (especially in the size constraint of ResNet50) in FSCIL.\u2019 Tables 1 and 2 illustrate that when using a larger backbone, simply employing LN, GAT, or NCM individually results in a biased performance\u2014either exhibiting high accuracy only in the base session (LN and GAT) or relatively higher incremental performance with a notable decline in base performance (NCM). We emphasize that integrating the two specific types of classification modules we proposed can effectively address this issue by achieving an optimal balance in performance compared to the naive use of a large backbone.\n\nWeakness_6.\n\nA: We assumed that fine-tuning the backbone with base classes induces an inevitable bias towards the base classes, a notion supported by the biased performance of current methods observed in our experiments. Such biased features may not only be unhelpful but could potentially be detrimental in adapting to the incremental sessions. Consequently, we posited the frozen pretrained backbone would be better rather than fine-tuned ones even if the pretrained knowledge is not highly correlated with the incremental sessions. While there may be instances where fine-tuning with base classes can be highly beneficial for adaptation to incremental sessions if there is a significant similarity between the distributions of base and incremental sessions, we believe such instances are generally rare. Therefore, we did not fine-tune the pretrained backbone with the base classes. However, as you suggested, using a fine-tuned backbone as a baseline could provide a more in-depth understanding of the impact of fine-tuning with base classes. We will carefully consider this advice.\n\nWeakness_7.\n\nA: From the perspective of the number of images per base class, (as we have described in sec 4.1,) the proposed datasets have more images\u2014Flowers102 has 40, DCC has 80 images\u2014than CUB200 where there are around 30 training images per base class. Furthermore, they can serve as valuable tools for assessing the robustness against the target distribution shifts. However, we agree that more test images would have been beneficial.\n\nQ1.\n\nA: Table 8 indicates the results evaluated with our proposed metrics where both baseline and pretrained backbones are used. The setup is the same as that of Figure 4.\n\nQ2.\n\nA: As you mentioned, since only the first 100 classes of Flowers102 are utilized, renaming it to Flowers100 seems appropriate to distinguish it from the original Flowers102."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971854898,
                "cdate": 1699971854898,
                "tmdate": 1699971854898,
                "mdate": 1699971854898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RDtKdxcFpJ",
                "forum": "UrmnIDCzLA",
                "replyto": "QVgUAUW8G5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2257/Reviewer_qGui"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2257/Reviewer_qGui"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for addressing my concerns. Some of them are well-addressed, therefore, I increase my rating from 3 to 5. But I still lean to reject the paper, mainly due to the concerns on novelty:\n\nThe overall pipeline is to utilize an imagenet pre-trained model due to its better representation. Several classification methods on top of representation are examined per the proposed evaluation metric. The ones that perform better on each metric are then aggregated by ensemble the final logits. Although a better benchmark performance is achieved, the whole method is too heuristic and simple, which weighs more for making my review decision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516759608,
                "cdate": 1700516759608,
                "tmdate": 1700516759608,
                "mdate": 1700516759608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LDTJPsmaW9",
            "forum": "UrmnIDCzLA",
            "replyto": "UrmnIDCzLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2257/Reviewer_SuEP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2257/Reviewer_SuEP"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors find that conventional methods exhibit excessively low incremental  session performance compared to base session performance. To tackle this issue, authors propose a new pipeline and conduct comprehensive experiments explore various conditions, including pre-trained representations, classification modules, and prediction aggregation. This paper further introduces new evaluation metrics and benchmark datasets to mitigate bias towards the base session, allowing for a more accurate assessment of FSCIL model generalization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Authors reveal the fact that current approaches cannot generalize on novel classes well.\n2. Authors propose a novel pipeline, evaluation metrics and datasets for FSCIL."
                },
                "weaknesses": {
                    "value": "Overall, though the paper might be useful, the paper lacks a clear motivation of the proposed methods and details of important parts of the model. This paper lacks the consistency between the efforts in the paper and conclusion. Meanwhile, the empirical results are not compelling enough to validate the effectiveness of proposed pipeline and evaluation metrics. \n\nMy questions and concerns are as follows:\n\n1.\tOne important contribution of the paper is proposing the pipeline to mitigate the performance issue, however, there is no detailed analysis of the reason behind the phenomenon. Why the proposed pipeline is able to tackle this issue?\n\n2.\tThe propose pipeline utilizes a pre-trained model to provide representations to aid FSCIL baseline. However, if the pre-trained model has already learned novel classes or similar classes, does it violate the incremental learning setting of learning novel classes?\n\n3.\tIn section 3.1, the description of incremental sessions causes misunderstanding. N is used to represent both the number of incremental sessions and classes in each session.  \n\n4.\tIn section 3.4, details are missing on how to aggregate the predictions. If the model is able to obtain the classification results of base and novel classes, why further aggregates the two prediction results to get the final predicition?\n\n5.\tIn table 3, the performance of a_0 of compared methods are listed wrong.  SAVC achieves 81.85 at a_0; FACT reports 75.90 at a_0; ALICE reports 77.40, etc. Also, the comparison might not be fair since majority of current methods adopt backbones like ResNet-12, 18, 20.\n\n6.\tThough author constructs two additional benchmark datasets for investigation of the effectiveness of FSCIL methods, as one important contribution, there is no explanation on the choice of two dataset, or comparisons between the proposed datasets and  previous benchmarks.\n\n7.\tIn section 4.2, what does IMB mean? Why the performance margin between the base session and each incremental session can measure the bias towards the base session? Also, session average is commonly used metric in previous works, this cannot be included as an contribution.\n\n8.\t In conclusion, \u201cwe propose a simple pipeline and introduce a set of novel session-level    metrics as well as new benchmark datasets for meticulous analysis and evaluation of robustness to the target distribution shift \u201d. I fail to see any related efforts on the evaluation of robustness to the target distribution shift, it seems irrelevant with the work of this paper."
                },
                "questions": {
                    "value": "see the weakness box"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807399013,
            "cdate": 1698807399013,
            "tmdate": 1699636158817,
            "mdate": 1699636158817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Gjgx1Zx9o",
                "forum": "UrmnIDCzLA",
                "replyto": "LDTJPsmaW9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our sincere gratitude for the time and effort you dedicated to reviewing our paper. Our response to the weakness and questions is present below.\n\nWeakness_1,4.\n\nA: Using only LN or NCM individually does not yield optimal outcomes. LN excels in the base session but falters in the incremental sessions, while NCM performs well in the incremental sessions but poorly in the base. Given their contrasting tendencies, we hypothesized that the two classification modules can potentially complement each other. Indeed, we observed that aggregating their output distributions significantly improved overall performance (Section 5.1).\n\n- The detailed process of the aggregation\nThe aggregation process entails summing the output distributions from LN and NCM. However, we noticed LN's output distribution exhibits excessive sharpness compared to NCM, leading to LN\u2019s dominance when simply summed.\nTo address this, we smooth LN\u2019s distribution with an optimal temperature [ref.1] before adding it to NCM's. Through multiple experiments, we find the optimal temperature, observing that LN's smoothened distribution slightly dominates when evaluating test data for the base session while NCM's has a slightly stronger influence when evaluation for the incremental sessions. Consequently, this aggregation enhances overall performance balance.\n\nWeakness_2,6. \n\nA: As we have described in Sec 4.1, we addressed the concern of the violation by setting the target distribution that diverges significantly from the pretraining dataset (ImageNet). Our criterion is as follows: if a self-supervised pretrained backbone yields better transferability to a target dataset than the supervised pretrained one, the target dataset has a considerable distribution shift from the pretraining dataset. This is primarily grounded in the fact that the supervised training aligns more closely with the distribution it trained with, due to the availability of label information.\n\nAccording to [ref.2], when the pretraining dataset is ImageNet, and the target dataset is Flowers102, DTD, or CD-FSL benchmarks (including CropDiseases and ChestX), self-supervised pretrained backbones outperform. Based on this, we chose Flowers102, DTD, and the CD-FSL benchmarks for the new datasets. However, due to limited data in DTD and CD-FSL benchmarks, we decided to combine them. Among the four datasets in CD-FSL, since EuroSAT has a much smaller resolution than the others, and including ISIC makes training challenging, we propose the new dataset DCC, which consists of DTD, CropDiseases, and ChestX. Unlike previous benchmarks similar to ImageNet, these datasets can introduce greater diversity to the benchmarks.\n\nWeakness_3.\n\nA: We appreciate your feedback on our oversight.\n\nWeakness_5.\n\nA: The reported a_0 from the previous methods are measured after the training on the base session. However, as we have described in Sec 4.2, we evaluated performance after training on the last session.\n\nWe acknowledge that the improvement in performance can be attributed to the larger backbone. We would appreciate it if the focus could shift from such a perspective to \u2018exploring how to effectively utilize larger backbones (especially in the size constraint of ResNet50) in FSCIL\u2019. (Further details are provided in the 5th response to R3)\n\nWeakness_7.\n\nA: Unlike the conventional metric, we assess session-level accuracy by exclusively evaluating the classes in a specific session only after training on the last session, i.e., the evaluation occurs solely for the classes in the i-th session to obtain a_i. Therefore, the difference(IMB) captures the extent of the disproportionately high performance of the base session compared to the average incremental performance, effectively measuring the degree of bias. Additionally, given the different composition of test set classes, our proposed 'session average' represents the average session-level accuracy, differing fundamentally from the previous one.\n\nWeakness_8.\n\nA: Here, the robustness is related to the selection of the pretrained backbone in our pipeline. As seen in Figure 4, in the case of CNNs, we notice that the supervised pretrained backbone outperforms the self-supervised ones on CUB200, close to ImageNet's distribution (compared to the other two). However, the trend reverses for Flowers102 and DCC, indicating sensitivity to the target dataset. In contrast, for ViTs, DINO consistently outperforms others across all datasets. Without the proposed datasets, choosing CNN backbones might favor the supervised pretrained one. When selecting ViT backbones, doubts may arise about DINO's performance in distributions different from ImageNet. Therefore, with our proposed datasets, we can identify backbones robust against target distribution shifts.\n\n[ref.1] Zhirong Wu et al. \"Unsupervised feature learning via non-parametric instance discrimination.\" CVPR 2018.\n\n[ref.2] Linus Ericsson et al. \"How well do self-supervised models transfer?\" CVPR 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987113789,
                "cdate": 1699987113789,
                "tmdate": 1699987113789,
                "mdate": 1699987113789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F0PGuvBxDv",
            "forum": "UrmnIDCzLA",
            "replyto": "UrmnIDCzLA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2257/Reviewer_jpg9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2257/Reviewer_jpg9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a pipeline for the incremental sessions, and used  feature extraction, classification module and prediction aggregation for the whole learning. Additionally, the paper introduced new evaluation metrics and benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method proposed in this paper is simple and the experiments are sufficient."
                },
                "weaknesses": {
                    "value": "1. In the incremental sessions, whether there have two results for one sample from the NCM classifier and Lightweight Network, and how to deal with this situation.\n2. In the experiments, I'm concerned about the accuracy of each session and the accuracy of each session compared to other methods.\n3. Whether the paper makes the comparison without using a pre-trained model."
                },
                "questions": {
                    "value": "Whether the relevant datasets can public, and please see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851136019,
            "cdate": 1698851136019,
            "tmdate": 1699636158737,
            "mdate": 1699636158737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XR2zJBBHjt",
                "forum": "UrmnIDCzLA",
                "replyto": "F0PGuvBxDv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our sincere gratitude for the time and effort you dedicated to reviewing our paper. Our response to the weakness and questions is present below.\n\n1. In the incremental sessions, whether there have two results for one sample from the NCM classifier and Lightweight Network, and how to deal with this situation.\n\nA: We summarize the detailed process of prediction aggregation below.\nThe aggregation process primarily entails summing the two output distributions from LN and NCM. However, we noticed LN's output distribution exhibits excessive sharpness compared to NCM, leading to LN\u2019s dominance when simply summed, making the prediction identical to using only LN's output distribution.\nTo address the issue, we smooth the LN\u2019s output (softmax) distribution with an appropriate temperature [ref.1] before adding it to that of NCM. Through multiple experiments, we determine the optimal temperature value for this adjustment. With this modification, we observed that when evaluating test data for the base session, LN's smoothened output distribution has a slightly stronger influence than that of NCM. Conversely, when evaluating test data for the incremental sessions, the influence of NCM's output distribution is slightly stronger than that of LN. Consequently, this aggregation process allows for achieving a more balanced overall performance.\n[ref.1] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733\u20133742, 2018.\n\n2. In the experiments, I'm concerned about the accuracy of each session and the accuracy of each session compared to other methods.\n\nA: We summarize the details about our proposed \u2018accuracy of each session\u2019 (Section 4.2) below.\nAs we have described in Sec 4.2, unlike previous methods, we have conducted evaluations only after completing training on the last session. While previous approaches assess the performance for all classes encountered up to the current session after training at each session, we assess session-level adaptability by exclusively evaluating the classes relevant to a specific session only after training on the last session.\n\nIf we have misunderstood your question, please feel free to clarify, and we appreciate your understanding.\n\n3. Whether the paper makes the comparison without using a pre-trained model.\n\nA: We conducted experiments in our pipeline without utilizing a pretrained backbone (the baseline is still ALICE). The results of the experiments are as follows.\n\n| CUB200 |a_0 | Inc_avg | IMB | A_s |\n|----------|----------|----------|----------|----------|\n| w/o pretrained | 73.60 | 50.68 | 22.92 | 52.76 |\n| Ours | 79.54 | 63.77 | 15.77 | 65.20 |\n\n| Flowers102 |a_0 | Inc_avg | IMB | A_s |\n|----------|----------|----------|----------|----------|\n| w/o pretrained | 94.92 | 84.84 | 10.08 | 85.96 |\n| Ours | 96.42 | 94.11 | 2.31 | 94.37 |\n\n| DCC |a_0 | Inc_avg | IMB | A_s |\n|----------|----------|----------|----------|----------|\n| w/o pretrained | 76.94 | 44.10 | 32.84 | 49.57 |\n| Ours | 80.97 | 55.00 | 25.97 | 59.33 |\n\nThe results show that using only the baseline backbone exhibits biased performance towards the base session. This reaffirms the significant assistance that the pretrained backbone provides in adapting to incremental sessions.\n\n[ref.1] Zhirong Wu et al. \"Unsupervised feature learning via non-parametric instance discrimination.\" CVPR 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972855534,
                "cdate": 1699972855534,
                "tmdate": 1699987146704,
                "mdate": 1699987146704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]