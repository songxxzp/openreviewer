[
    {
        "title": "Prompting-based Efficient Temporal Domain Generalization"
    },
    {
        "review": {
            "id": "8d8jr7jmx4",
            "forum": "U3ROVRTKTa",
            "replyto": "U3ROVRTKTa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_ci1c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_ci1c"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes an interesting prompt-learning strategy to capture temporal drifts in data over time. By representing data as occurring from different domains over time, the algorithm proposed by the authors learn two types of prompts from data by predicting future domains: One prompt capturing generalization across domains over time, and another prompt that incorporates ordered domain-specific prompts over time to capture temporal qualities exhibited by the data generation process. The authors empirically evaluate the proposed method over synthetic and real-world datasets to show the efficacy of the solution over other competing methods. Furthermore, they also provide an insightful ablation study to justify how prompts learned in the solution is useful in capturing temporal dynamics in data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to follow. The algorithm description and figure depictions really help in understanding the concepts and contributions presented in the paper.\n2. Empirical evaluation shows good performance on both synthetic and real-world data, when compared to other existing mechanisms.\n3. The ablation study clearly shows the need for two prompt types proposed in the paper."
                },
                "weaknesses": {
                    "value": "The problem description seems to promise more than what the eventual solution delivers. Particularly, the paper is positioned by the authors as domain generalization and promises a solution in a space where learnings from different domains can be utilized to capture information useful for predicting unknown target domains. However, after reading the solution and dataset description, this falls short of expectation as the authors focus on concept drift within the same dataset. Data from the same data generation process is divided into multiple windows, where each window is called a domain. So, data drifts indicated by the authors are within data drift over time. In the literature, there has been multiple articles published on concept drift or data drifts in general over the past few decades. For example, please see Lu, Jie, et al. \"Learning under concept drift: A review.\" IEEE transactions on knowledge and data engineering 31.12 (2018): 2346-2363. With this context, it is not clear why data windows within a dataset is termed as \"domains\", where it is truly not from a different domain. For true domain generalizability and adaptability, it would be good for the authors to explore how domain adaptation is setup, and empirically evaluate in-domain and across-domain generalizability and adaptation."
                },
                "questions": {
                    "value": "A few elements of Algorithm 1 are not clear.\n\t1. Are the number of data points in each domain the same?\n\t2. In Step13, how exactly is PT(t) generated? Is is a concatenation of previous domain-specific prompts concatenated when provided as input to gw? Particularly, what is the difference between Line 12 and Line 13?\n\t3. Given my understanding of the problem setup, it is unclear what exactly is Y? Say in your housing price prediction example, is Y the house prices in the target domain (validation data) or house prices at domain t available in the training data?\n\nThe empirical evaluation in Table 2 and 3 shows that the proposed method has the least error across all datasets with greater than 2 variables. However, both the synthetic data generation and temporal drifts learned by the prompts seem to work for non-abrupt changes. Does this also work for abrupt drift?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724248399,
            "cdate": 1698724248399,
            "tmdate": 1699636133398,
            "mdate": 1699636133398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K6VqtjX5Cn",
                "forum": "U3ROVRTKTa",
                "replyto": "8d8jr7jmx4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for your positive, insightful and valuable comments and suggestions which are very crucial for improving the quality of our manuscript.\n\n**The paper's problem description promises a domain generalization solution, but the actual solution seems to focus only on concept drift within the same dataset, falling short of initial expectations.**\n>This paper focuses on temporal domain generalization, addressing data distribution shifts over time. We followed the experimental setup of existing state-of-the-art temporal domain generalization methods [Nasery et al. (2021), Bai et al. (2023), Wang et al. (2020a), Ortiz-Jimenez et al.( 2019)]. And we cited the state-of-the-art method results in Table 1. We are not the first to propose temporal DG, and we followed the standard practice.\n\n---\n\n**Clarification on the classification of different windows in a dataset as 'domains' when they represent data drift over time, suggesting a need for true domain generalizability and adaptability evaluation, as per existing literature on concept drift.**\n\n>We followed the experimental setup of existing state-of-the-art temporal domain generalization methods [Nasery et al. (2021), Bai et al. (2023), Wang et al. (2020a), Ortiz-Jimenez et al.( 2019)], and we split the dataset following the SOTA temporal DG methods. We are not the first to propose temporal DG, we didn't propose a new way to split the dataset. We followed the standard practice.\n\n---\n\n**Clarification on elements of Algorithm 1, including uniformity in data points per domain, the generation process of temporal prompts, differences between Lines 12 and 13, and the specific nature of Y in the context of the housing price prediction example.**\n\n>The number of data points in each domain can vary. We do not enforce a uniform number of data points across all domains, as this depends on the dataset and the nature of the temporal segments being considered.\n>\n>In Line 12 of Algorithm 1, we are referring to the input tokens (PS1, PS2, ..., PS(t\u22121)) for the prompt generation module. These tokens represent the domain-specific prompts from previous time periods. In contrast, Line 13 describes the generation of PT(t), which is the current temporal prompt. This prompt is produced by the module using the concatenation of previous domain-specific prompts as input. The key distinction here is between the input prompts from previous domains (Line 12) and the output prompt for the current domain (Line 13).\n>\n>Y represents the label for the input data in the same domain. To illustrate with the housing price prediction example: if the features of a house in 2021 (X) correspond to a value of 200K, then Y (the label) would also be 200K. In other words, both the input (X) and output (Y) are derived from the same domain, ensuring that the model learns to predict within the context of each specific temporal domain.\n\n---\n\n**Effectiveness of our Method in abrupt drift scenarios.**\n>Our model is optimized for gradual temporal drifts. If abrupt drifts are present in the training data, it's likely our model can adapt to similar abrupt changes during testing. However, without exposure to abrupt drifts in training, the model may not effectively handle sudden changes in the test data."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553032968,
                "cdate": 1700553032968,
                "tmdate": 1700695694957,
                "mdate": 1700695694957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "suZMHY1hEl",
            "forum": "U3ROVRTKTa",
            "replyto": "U3ROVRTKTa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_zvod"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_zvod"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new approach for adapting to these temporal changes without needing future data during training. Using a prompting-based method, it tweaks a pre-trained model to address time-related shifts by using different types of prompts that understand time-based patterns. This technique works for various tasks, like classification and forecasting, and achieves leading performance in adapting to time-based data changes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents the first prompt-based method to handle temporal domain generalization.\n2. The proposed method achieves better performance than existing methods in both accuracy and efficiency. \n3. The studied problem is interesting and timely."
                },
                "weaknesses": {
                    "value": "1. The motivation for using prompts still lacks proper motivation. Specifically, the motivation for using prompts is claimed as \"none of these prior works can generate time-sensitive prompts that capture temporal dynamics.\" There are tons of other ways to learn temporal dynamics, and we don't have to use prompts.\n2. The experiment setup also has some issues, e.g., the ablation study can be further improved, more baselines can be included, etc.\n3. The overall presentation is a little messy. There are some undefined notations. The authors seem to misuse \\citep{} and \\citet{}, and the presented references impact the overall readability."
                },
                "questions": {
                    "value": "1. What's the mathematical formulation of the prompt?\n2. What would be the intuition of training the backbone network on the aggregated dataset? For some datasets with manually altered data distribution on each domain (like two moons), the decision boundary would be really difficult to learn if you mix all the data together.\n3. Following the previous question, the ablation study can be further designed to remove the backbone model to verify if the backbone model is truly useful.\n4. Some sentences are quite hard to understand, e.g., \"For each domain $t$, we prepend the input $X$ with a prompt $PS(t)$, which are learnable parameters.\" Are both $X$ and $PS(t)$ learnable?\n5. Not sure why some baselines are excluded in Table 3's comparison.\n6. One of the major claims is also confusing: \"Our paper presents a novel prompting-based approach to temporal domain generalization that does not require access to the target domain data\". I feel like no access to the target domain data is a default rule of domain generalization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772742927,
            "cdate": 1698772742927,
            "tmdate": 1699636133316,
            "mdate": 1699636133316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bE7QvtU7Aj",
                "forum": "U3ROVRTKTa",
                "replyto": "suZMHY1hEl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for your positive, insightful and valuable comments and suggestions which are very crucial for improving the quality of our manuscript.\n\n**What's the mathematical formulation of the prompt?**\n\n>Our work is inspired by the success of prompt-tuning in NLP tasks [Lester et al. (2021); Vu et al. (2021); Gu et al. (2021); Li & Liang (2021); Asai et al. (2022); Wang et al. (2023)]. As for the mathematical formulation of prompt, [Oymak et al. (2023)] developed a statistical foundation for gradient-driven prompt-tuning, examined its optimization and generalization behaviors, and investigated how it facilitates attending to context-relevant information. Additionally, they demonstrated that one-layer softmax-prompt-tuning is provably more effective than other methods, such as one-layer self-attention.\n\n>[Oymak et al. (2023)]: Oymak, S., Rawat, A.S., Soltanolkotabi, M. &amp; Thrampoulidis, C.. On the Role of Attention in Prompt-tuning. Proceedings of the 40th International Conference on Machine Learning (ICML 2023).\n\n---\n\n**Doubts raised about the need for using prompts, considering the claim that other methods can't generate time-sensitive prompts for temporal dynamics, despite the existence of various alternatives to learn temporal dynamics.**\n\n>While alternative methods exist, our approach is well-supported and aligned with current research trends in the field. We have extended prompting from NLP to a lot more applications, which is truly valuable. Most previous prompting methods only demonstrated their efficacy in NLP tasks or NLP-related neural networks like GPT or CLIP. We proposed the first prompting-based temporal DG method, which is a generic model and is applied to various tasks. This shows great potential in using prompt learning for various applications other than NLP.\n\n---\n\n**Typos and increasing readability.**\n\n>Thank you for your feedback. We will address the noted issues by correcting typos, clarifying undefined notations, and revising our citation format to improve readability and presentation.\n\n\n---\n\n**The logic behind training the backbone network on an aggregated dataset, especially for datasets with manually altered distributions across domains, which could complicate learning the decision boundary.**\n\n>Through aggregating datasets, our network is designed to learn general/common features across the entire dataset, while domain-specific prompts target specific feature sets. \n\n---\n**The ablation study can be further designed to remove the backbone model to verify if the backbone model is truly useful.**\n\n>Sorry, We didn't understand the question. How would prompting work without a backbone model? We followed the standard practice of prompting [Lester et al. (2021); Vu et al. (2021); Gu et al. (2021)], and a pre-trained backbone model is required.\n\n---\n**Clarification on whether both and are learnable parameters in the context of pre-pending inputs with prompts for each domain.**\n\n>Prepending input with a prompt is a standard practice of prompting learning [Lester et al. (2021); Vu et al. (2021); Gu et al. (2021)]. In our model, the inputs are standard, non-learnable data from the dataset, similar to other classic models. Only the prompts are learnable parameters.\n\n---\n\n**Missing baselines in Table 3.**\n\n>Other baselines are not applicable on Time series datasets. We will clarify that in Camera Ready. \n\n---\n**Confusion about the claim stating that: \"Our paper presents a novel prompting-based approach to temporal domain generalization that does not require access to the target domain data\", as having no access to the target domain data is a default rule of domain generalization.**\n>Yes, we agree that no access to the target domain data is a default rule of domain generalization. We just want to emphasize it and highlight it, since it is the key difference between domain adaptation and domain generalization."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552451658,
                "cdate": 1700552451658,
                "tmdate": 1700553822410,
                "mdate": 1700553822410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qtQILgQkTz",
                "forum": "U3ROVRTKTa",
                "replyto": "bE7QvtU7Aj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_zvod"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_zvod"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback. Part of my concerns are addressed, but the major ones still remain.\n\n1. Given the little improvement achieved compared to existing state-of-the-arts, I still wonder what is the rationale for using prompting in DG tasks. You use a bunch of papers of using prompting in language modeling/inference tasks. However, the prompting works for language models because the prompt is a piece of text inserted in the input examples, so that the original task can be formulated as a (masked) language modeling problem. Throughout the paper, it feels like you only claim you propose the first work of adopting prompting in temporal DG, and the model \"is a generic model and is applied to various tasks\", which I don't see why.\n\n2. The design of your foundation model still does not make much sense to me. I still doubt training a model on the aggregated dataset can learn general features. Since the task is temporal DG, you can test whether there will be a performance drop of training your foundation model only on temporal domains that are close to the target domain. (ablation study)\n\n3. Lastly, I am still not sure why important baselines are excluded in Table 3, and you seem to refuse to answer the question directly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659839687,
                "cdate": 1700659839687,
                "tmdate": 1700659839687,
                "mdate": 1700659839687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4fTckgdOCy",
            "forum": "U3ROVRTKTa",
            "replyto": "U3ROVRTKTa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a training paradigm for domain generalization. This contains 1) pre-training of a backbone model on all source domains, 2) the learning of source-domain-specific \"prompts\" for each source domain, 3) the learning of a \"temporal prompt\" for each source-domain, and 4) the learning of a global \"prompt\". At testing time, global and domain-specific prompts from the past can be used to make predictions in the new domain. The method is applied to time series classification and forecasting data sets and on a synthetic data set."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easy to follow and straightforward to read. The synthetic data experiment visually and numerically shows the shortcomings of other methods that motivate this work. Predictive performances are reported (at least partially) with standard deviations."
                },
                "weaknesses": {
                    "value": "I am having difficulty thoroughly understanding how the architecture is trained in detail (more details in the question). I also question whether it is appropriate to split time series data artificially into different source domains. If a domain is not explicitly defined, one could pick an arbitrary period and define it as a domain, as was done on the crypto data set. There is no sound justification for why a one month period was chosen or why it would be better than a two week period. It furthermore seems that the presented method's performance largely lies in the confidence intervals of competing methods. I also miss some scientific curiosity about the learned prompt representations; there is much more potential in this work than reducing it to performance metrics."
                },
                "questions": {
                    "value": "* Can you please clarify what the stopping criterion is when pre-training the backbone initially? You say the domain-specific prompts are learnable parameters, can you specify how they are connected to the backbone/output? You write they are concatenated, does this mean in the initial pre-training, we need to know the size of the prompt and mask the input accordingly? Or is there a linear layer whose parameters are learned? You might want to formalize all this by introducing a second set of parameters (as $\\theta$ is always frozen). Furthermore, is a new temporal prompt generator trained for each temporal prompt, or is it reused? In Figure 1, you \"freeze\" $P_{T2}$ but not $P_{T3}$ in the next step, why? In Fig. 1's caption, you say, \"[...] finally, [...] $P_G$ is trained\". This implies a sequential training but from the figure, it seems like $P_G$ is also the output of the temporal prompt generator. \n* Can you provide experimental results on data sets that naturally come from different domains? \n    * If this is not the case, is it possible to use your method to **determine** the existence of different domains? I am thinking of comparing the learned domain-specific prompts, for example, in terms of their cosine similarity. \n* Your algorithm omits many necessary details and does not add information that can't be inferred from the text. I would propose to either make the algorithm more informative (i.e., clarify the questions from above) or, to save space, remove it and clarify the points by extending Section 3. \n* How do gradient-boosted trees perform on the regression tasks? \n* How do standard forecasting methods such as ARIMA/Gaussian Processes perform on the data sets? \n* Why are no standard deviations in Tables 3, 4, and 5 reported? \n* Given its origin in NLP, I am not sure if \"prompt\" is the best fitting wording in the context of time series.\nIf all points can be addressed satisfactorily (particularly, the investigation into the representation of the learned prompts), I may consider raising my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2020/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2020/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773347281,
            "cdate": 1698773347281,
            "tmdate": 1700638820000,
            "mdate": 1700638820000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lBLIdHZmh5",
                "forum": "U3ROVRTKTa",
                "replyto": "4fTckgdOCy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for your positive, insightful and valuable comments and suggestions which are very crucial for improving the quality of our manuscript.\n\n**Clarification on stopping criterion for pre-training the backbone.**\n>The pre-training process is halted when the validation loss fails to improve for a predefined number of epochs (3)\n\n---\n**Clarification on how learnable domain-specific prompts are connected to the backbone/output, including details on concatenation, masking during pre-training, and potential use of a linear layer.**\n>We follow the standard practice of prompt learning to learn the prompts[Lester et al. (2021); Vu et al. (2021); Gu et al. (2021)], namely domain-specific prompts are extra tokens, and we are not increasing the dimension of tokens. The concatenation is about increasing the length of the input sequence, and transformers can take variable-length inputs. \nTo answer your question, yes prompts are learnable parameters and they are part of the inputs inputting to the backbone network. No, we don't need to know the size of the prompt for pre-training.\n\n---\n**Clarification on whether a new temporal prompt generator is trained for each prompt or if a single one is reused.**\n>For our temporal prompt generator, we adopt a dynamic tuning approach rather than training a new generator for each temporal prompt. Specifically, the process involves loading the weights from the previous step and then fine-tuning them based on the domain using previous domains information.\n\n---\n\n**clarification on Figure one and and the sequential training versus simultaneous output from the temporal prompt generator.**\n>As explained in Section 3.3, it is sequential training. The temporal prompt generator is sequentially trained to generate prompts for the upcoming new domain, and previously learned prompts are fixed.\n\n---\n**Concerns about the appropriateness of artificially splitting time series data into different source domains, with specific reference to the arbitrary period selection in the crypto dataset, lacking justification for the chosen duration.**\n>We followed state-of-the-art papers [Nasery et al. (2021), Bai et al. (2023), Wang et al. (2020a), Ortiz-Jimenez et al.( 2019) ] to split the dataset, and we split the Crypto dataset in the same way. We proposed one reasonable way of splitting the dataset, and we did not aim to propose the best way to split the dataset. Brute-force all kinds of ways splitting the dataset is a NP-hard problem, which is impossible and is also trivial for a paper.\n\n---\n**Method's performance largely lies in the confidence intervals of competing methods.**\n>Yes, but our methods achieve the performances with a lot fewer parameters and much less training time. Normally machine learning models need to trade performance for efficiency, while our model shows great performance with better efficiency.\n\n---\n\n**There is much more potential in this work than reducing it to performance metrics.**\n\n>We agree, so we expanded our works to more tasks and datasets. Most previous prompting methods only demonstrated their efficacy in NLP tasks or NLP-related neural networks like GPT or CLIP. We proposed the first prompting-based temporal DG method, which is a generic model and is applied to various tasks. This shows great potential in using prompt learning for various applications other than NLP.\n---\n**Request for experimental results on datasets naturally from different domains, or if not available, whether your method can identify different domains, possibly by comparing learned domain-specific prompts through cosine similarity.**\n\n>For experimental results in Table 1, we followed state-of-the-art temporal DG methods [Nasery et al. (2021), Bai et al. (2023), Wang et al. (2020a), Ortiz-Jimenez et al.( 2019)] and adopted commonly used datasets. Crypto dataset is a time series dataset that naturally comes from different temporal domains. Domain-specific and/or temporal prompts may contain some information to determine the existence of different domains, while generic prompt cannot since it contains shared information across different domains. We will compare our domain-specific prompts in the camera-ready version.\n\n---\n\n**Inquiry about the performance of gradient-boosted trees on regression tasks and standard forecasting methods like ARIMA/Gaussian Processes on the datasets.**\n\n>We have compared our methods with the current state-of-the-art methods. It would be great if the reviewer could kindly point us to existing state-of-the-art papers that apply these methods.\n\n---\n\n**Standard deviations in Tables 3, 4, and 5.**\n>We will report that in Camera ready, they are similar to other tables. \n\n---\n\n**Uncertainty regarding the suitability of the term \"prompt\" in the context of time series, considering its origin in NLP.**\n>We believe it is the best-fitting wording. We followed the exact standard practice of prompting [Lester et al. (2021); Vu et al. (2021); Gu et al. (2021)].\n\n---"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551761794,
                "cdate": 1700551761794,
                "tmdate": 1700551960817,
                "mdate": 1700551960817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pkyVOASfUt",
                "forum": "U3ROVRTKTa",
                "replyto": "lBLIdHZmh5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_HfUr"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I thank the authors for clarifying my concerns. I will raise my score by one."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638786997,
                "cdate": 1700638786997,
                "tmdate": 1700638786997,
                "mdate": 1700638786997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RlKdZsK3GO",
            "forum": "U3ROVRTKTa",
            "replyto": "U3ROVRTKTa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method for temporal domain generalization using prompts on transformer-based networks. This method is efficient and does not need data from future time periods during training. It uses global, domain-specific, and drift-aware prompts to adapt to data changes over time. The paper claims that the proposed method is adaptive on various tasks, such as classification, regression, and forecasting, The effectiveness of the framework is demonstrated through extensive experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper discusses a vital question on temporal domain generalization by leveraging soft prompts with transformer-based networks. \n- The idea and motivation of this paper are easy to read. The idea and method proposed in this paper are clearly illustrated and introduced, making the reader easily understand."
                },
                "weaknesses": {
                    "value": "- Overclaim 1. The second contribution of this work is \"parameter-efficient and time-efficient\". But their proposed method requires to train a transformer (Temporal Prompt Generator in Figure 1), which includes way more trainable parameters than existing methods such as DRAIN.\n- Overclaim 2. As for the time-efficient aspect, there is no training time comparison analysis to demonstrate the claimed \"time-efficiency.\" Especially, either pre-training or fine-tuning a transformer-based model to adapt to the specific task (temporal soft-prompt generation) are inefficient.\n- Unfounded. The authors claim that \"Only a few methods studied temporal DG problem Nasery et al. (2021); Bai et al. (2023), which are inefficient and complex to be applied to large datasets and large models,\" which is unfounded, no evidence supported, and without any quantitative analysis for demonstrating this assumption.\n- The performance improvement is minor and not significant, especially since the proposed method achieves inferior performance than DRAIN (state-of-the-art of TDG) on the 2-moon dataset, a basic synthetic dataset on testing TDG. The performance of the proposed method is not convincing.\n- The proposed framework seems to be adaptive on multiple modalities of transformer-based networks as the model backbone. However, the paper only evaluates their framework on one transformer-based network. The authors are highly encouraged to test their framework incorporated with multiple transformer-based networks."
                },
                "questions": {
                    "value": "- ONP has been proven to obtain no domain shifting [1], which means most of the TDG-based methods are useless in ONP. However, the proposed methods, in contrast, obtain good performance on ONP. Is there any specific reason that can explain this phenomenon?\n\n[1] Nasery et. al \"Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time\n\" NeurIPS 2021"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2020/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q",
                        "ICLR.cc/2024/Conference/Submission2020/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2020/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828991648,
            "cdate": 1698828991648,
            "tmdate": 1700637618502,
            "mdate": 1700637618502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zok5438bUq",
                "forum": "U3ROVRTKTa",
                "replyto": "RlKdZsK3GO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for your positive, insightful and valuable comments and suggestions which are very crucial for improving the quality of our manuscript.\n\n\n**Overclaim 1. How is the method \"parameter-efficient and time-efficient\" when it requires training a transformer with more parameters than existing methods like DRAIN?**\n\n>We appreciate the opportunity to address the concerns regarding any perceived overclaims in our paper. We would like to assure you that our assertions are carefully calibrated to reflect the findings of our research. As shown in Table 3, our approach, despite involving the training of a transformer (Temporal Prompt Generator), is actually more parameter-efficient. In fact, there are structural differences in how each method processes weights. DRAIN uses hypernetworks to generate weights for the backbone networks, and the size of hypernetworks is significantly higher than the backbone network. Especially when the domain task is a time series application, DRAIN requires generating weights for transformers, which requires an extremely high number of parameters. While for our method, whatever the domain task is, the goal is to generate a short prompting vector, which is much more parameter-efficient. Another aspect to note, for each domain DRAIN generates a backbone network, namely if there are $n$ domains DRAIN requires $n$ backbone networks. While for our method, the backbone network is shared, only a short vector (at most with length 128) is required for each domain. In general, time and parameter efficiency are the most well-known characteristics of prompt-tuning methods.\n---\n**Overclaim 2. How is the method time-efficient when there's no comparative analysis of training time and both pre-training and fine-tuning a transformer for temporal soft-prompt generation are known to be inefficient?**\n\n>The assertion that our model lacks time-efficiency is addressed in Table 3, which presents a detailed comparison of training times between our method and existing approaches. This table clearly demonstrates that our model is not only efficient in terms of parameter usage, as previously discussed, but also in training time. \n---\n**Unfounded. The authors claim that \"Only a few methods studied temporal DG problem .... which are inefficient and complex to be applied to large datasets and large models,\" which is unfounded,  and without any quantitative analysis for demonstrating this assumption.**\n\n>The claim about the inefficiency and complexity of existing methods for temporal dynamic graph (DG) problems is backed by the data presented in Table 3. This table specifically illustrates how the number of parameters in the DRAIN method increases exponentially with time series datasets. Given that DRAIN relies on MLPs for encoding and decoding weights, this exponential increase in parameters makes it impractical for use with larger models.  Moreover, DRAIN requires generating weights for the backbone networks using hypernetworks, and generating weights for larger models is impractical.\n---\n**The method shows only minor, insignificant performance improvements and underperforms compared to DRAIN on the basic 2-moon dataset, casting doubt on its effectiveness.**\n\n>We evaluated our method on 4 synthetic datasets and 6 real datasets. Our model demonstrates a significant improvement in performance compared to **DRAIN, which is the current SOTA model**, with an average margin of 20% across various benchmarks, as detailed in Tables 1, 2, and 3 on 6 real datasets and 3 synthetic dataset, and achieved comparable result on only 1 synthetic dataset. This general superiority is indicative of the model's robustness and effectiveness in various applications including classification, regression, and time series forecasting. Let alone our method is more time-efficient and parameter-efficient. \n\n---\n**Applying proposed framework to various transformer-based networks.**\n\n>We appreciate the suggestion to evaluate our framework across multiple transformer-based networks. Indeed, our model is designed to be versatile and adaptable to a variety of models dealing with temporal data. While the current paper focuses on a single transformer-based network for a thorough and focused analysis, we recognize that using our model on other baselines can also be an interesting and Exploring this in future work is definitely something we find compelling and valuable.\n\n---"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550872302,
                "cdate": 1700550872302,
                "tmdate": 1700595008323,
                "mdate": 1700595008323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TdLg9kQkJy",
                "forum": "U3ROVRTKTa",
                "replyto": "E02ZpojFf6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2020/Reviewer_Bw2Q"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I appreciate the discussion and feedback from the authors, which addressed some of my questions and concerns. I will increase my score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2020/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637594536,
                "cdate": 1700637594536,
                "tmdate": 1700637594536,
                "mdate": 1700637594536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]