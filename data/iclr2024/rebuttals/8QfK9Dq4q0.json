[
    {
        "title": "Class Incremental Learning via Likelihood Ratio Based Task Prediction"
    },
    {
        "review": {
            "id": "VDBNe9Hjt1",
            "forum": "8QfK9Dq4q0",
            "replyto": "8QfK9Dq4q0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_aMSi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_aMSi"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author proposes the use of a Likelihood Ratio to identify the Task-id for Class Incremental Learning (CIL). Traditionally, out-of-distribution (OOD) detectors were used for task identification, but this paper introduces a new method called TPLR (Task-id Prediction based on Likelihood Ratio) that leverages additional information like replay data and learned tasks for more effective and principled task-id prediction. TPLR outperforms traditional CIL approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation and method sound solid to me. I agree with the author that task-id prediction is pivotal for CIL under specific circumstances.\n\n- Motivation is clear and straightforward:  The author argues that using a traditional OOD detector is not optimal for task predictions and here they leverage the information in the CIL process for task-id prediction.\n\n- The proof and methods in section 3 and 4 look good to me."
                },
                "weaknesses": {
                    "value": "1. The writing requires improvement. The author frequently used abbreviations and jargon, especially in the introduction, which occasionally left me puzzled. It would be beneficial if these terms were interpreted more straightforwardly. \n\n2. The related works are also unclear: \n- Although the author clarifies their focus on Class incremental learning, which doesn't provide the task-id during inference, it remains ambiguous whether they are using a memory buffer (rehearsal-based) or are memory-free (online CIL). I suggest the author address this in the introduction and related works.\n- Some recent benchmarks are missing: The author left memory-free (non-replay-based approaches) CIL in related works. The author also left balanced CIL works, e.g., SS-IL, TKIL.\n\n3. Experimental settings:\n- Table 1 is impressive, but the comparisons seem biased. The author claims they compared with 17 baselines, including 11 replay-based and 6 non-replay-based. From my understanding, the author requires a memory buffer, as indicated in the \"Overview of the Proposed TPLR\", equation 2.\n-  It would be more equitable if the author juxtaposed their method with replay-based CIL. Specifically, the author should draw a clear comparison with methods using task-id prediction, highlighting the advantages of their technique. \n- One import baseline is missing: AFC[3]\n  \n\n4. The inference setting remains unclear. Does the author predict both the task-id and class-id simultaneously? Is there any fine-tuning step involved? Typically, some fine-tuning follows the task-id prediction. e.g., iTAML. If the author's method circumvents this, it could be seen as a distinct advantage. Therefore, I recommend the author incorporate a discussion about the computational load when integrating likelihood ratio predictions, elucidating the benefits and drawbacks of this model.\n\n5. Lacks Visualizations: Could the author add a real visualization of data distribution, like the \"Feature-based likelihood ratio score\" in Figure 1. It will be strong evidence the TPLR works well.\n\n\n[1] Ss-il: Separated softmax for incremental learning. In Proceedings of the IEEE/CVF International conference on computer vision \n\n[2] TKIL: Tangent Kernel Optimization for Class Balanced Incremental Learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision \n\n[3] Class-incremental learning by knowledge distillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
                },
                "questions": {
                    "value": "Please refer the weakness;"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1104/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1104/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1104/Reviewer_aMSi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698186515137,
            "cdate": 1698186515137,
            "tmdate": 1699636036567,
            "mdate": 1699636036567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MG6utM1EQ2",
                "forum": "8QfK9Dq4q0",
                "replyto": "VDBNe9Hjt1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer aMSi,\n\nwe are glad to address your concerns as follows.\n\n> The writing requires improvement. The author frequently used abbreviations and jargon, especially in the introduction, which occasionally left me puzzled. It would be beneficial if these terms were interpreted more straightforwardly.\n\nThanks for your suggestion. We have significantly reduced the use of abbreviations and jargon to make the introduction section easier to read and to interpret. Since we edited in many places, we did not highlight the changes. You may check out the new introduction section in the revised paper. \n\n> Although the author clarifies their focus on Class incremental learning, which doesn't provide the task-id during inference, it remains ambiguous whether they are using a memory buffer (rehearsal-based) or are memory-free (online CIL). I suggest the author address this in the introduction and related works.\n\nYes, we are using a memory buffer to save some previous data as in replay-based methods but the saved data in our method are used to estimate the distribution of U_{CIL} in training each task and also in testing rather than to replay them in training a new task like replay-based methods. We are not doing online CIL. We have added footnote 3 to make these clearer. \n\n> Some recent benchmarks are missing: The author left memory-free (non-replay-based approaches) CIL in related works. The author also left balanced CIL works, e.g., SS-IL, TKIL.\n\nMany systems in the related work section are memory-free methods, e.g., those regularization-based methods, orthogonal-projection based methods, and some of those task-id prediction based methods. \n\nAs a large number of papers have been published in continual learning, there are inevitable casualties in literature review. Thanks for pointing out the balanced CIL work. We have added it and cited both SS-IL and TKIL papers in the related work section. Please see the last sentence (in blue) on page 2 of the revised paper. \n\n> Table 1 is impressive, but the comparisons seem biased. The author claims they compared with 17 baselines, including 11 replay-based and 6 non-replay-based. From my understanding, the author requires a memory buffer, as indicated in the \"Overview of the Proposed TPLR\", equation 2.\n\nWe also think that our results are very strong. Thanks. We have some difficulty understanding your comment \u201cthe comparisons seem biased\u201d. Could you explain more? Yes, we use a memory buffer to save some past data like replay based methods but we have compared with 11 strong replay-based baselines. We also include 6 non-replay based methods as some non-replay based methods are also strong, e.g., PASS and SLDA. \n\n> It would be more equitable if the author juxtaposed their method with replay-based CIL. Specifically, the author should draw a clear comparison with methods using task-id prediction, highlighting the advantages of their technique. \n\nYes, we agree with you to juxtapose our method with replay-based CIL, and we have included 11 strong replay-based baselines in Table 1. Besides, the best baselines MORE and ROW also use task-id prediction. The results show that our method TPLR outperforms them by a large margin. We did not include some earlier task-id prediction based CIL methods (but they have been reviewed in the related work section) as they are much weaker than MORE and ROW. Below, we report the results of two more task-id prediction based CIL methods, iTAML [1] and PR-Ent [2], based on the same pre-trained model. Notice that the vanilla iTAML assumes each test batch of many instances comes from a single task and uses the whole batch of data to predict the task-id. We set its test batch size as 1 to disable the assumption for a fair comparison as all other methods predict one test instance at a time, which is more realistic.\n\n|        | C10-5T    | C100-10T  | C100-20T  | T-5T      | T-10T     | Average   |\n| ------ | --------- | --------- | --------- | --------- | --------- | --------- |\n| iTAML  | 78.65     | 59.56     | 52.31     | 49.53     | 46.34     | 57.28     |\n| PR-Ent | 84.37     | 64.51     | 60.52     | 58.63     | 55.44     | 64.69     |\n| TPLR   | **92.33** | **76.53** | **76.34** | **68.64** | **67.20** | **76.21** |\n\n> One import baseline is missing: AFC[3] \n\nWe\u2019ve added AFC as a new baseline in Table 1 and cited the paper as well. A summary of the results (accuracy after learning the final task) is presented in the following table. It can be seen that our TPLR also outperforms AFC by a large margin.. \n\n|      | C10-5T    | C100-10T  | C100-20T  | T-5T      | T-10T     | Average   |\n| ---- | --------- | --------- | --------- | --------- | --------- | --------- |\n| AFC  | 84.59     | 69.85     | 69.44     | 56.40     | 55.35     | 67.13     |\n| TPLR | **92.33** | **76.53** | **76.34** | **68.64** | **67.20** | **76.21** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111460267,
                "cdate": 1700111460267,
                "tmdate": 1700145672723,
                "mdate": 1700145672723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rdnvZBxUgG",
                "forum": "8QfK9Dq4q0",
                "replyto": "YI71BR4EuR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Reviewer_aMSi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Reviewer_aMSi"
                ],
                "content": {
                    "title": {
                        "value": "I thank the authors for updating the manuscript with new results and visualizations to clarify my questions."
                    },
                    "comment": {
                        "value": "I thank the authors for updating the manuscript with new results and visualizations to clarify my questions. However, the following concerns still exist.\n\n1. 'Comparisons seem biased': The authors should organize their writing to reflect a clear and straightforward comparison with proper baseline methods. I suggest the authors first clarify the categories of CL/task-IL/Class-IL and corresponding subcategories (memory/non-memory). From our discussion and the authors' rebuttals, it seems that the proposed method is memory-based class incremental learning, i.e., it needs memory and doesn't need a task label during reference. If that is the case, the authors should only compare it with such settings in CIL. Therefore, adding the memory-free approach to compare with the proposed method is unclear and also unfair. I have also read the other reviewer's comments and I see other reviewer also has a similar question: \n\n'In Table 1, which of these results are using the task label at inference time? For example, HAT needs the task label. So, are the results of HAT comparable here with the other methods? Or is HAT having a forward pass with each task label and then using some heuristic to pick the class?'\"\n\nHowever, the author's reply also makes me more confused:\n\n\"In Table 1, our method using HAT needs the task label. But since we run all task models at test time to compute the task label probability for each task, effectively we do not need any task label.\"\n\nTherefore, it is unclear to me whether the proposed method requires task labels or not.\n\n2. I also checked Appendix G as the author suggested. Table 8 is unclear to me. If the author is seeking a fair comparison, they are supposed to use the same model for comparisons. Why are there differences in parameters for different methods?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173347411,
                "cdate": 1700173347411,
                "tmdate": 1700173347411,
                "mdate": 1700173347411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WlHVIqdG1S",
                "forum": "8QfK9Dq4q0",
                "replyto": "VDBNe9Hjt1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer aMSi,\n\nThank you very much for your feedback. We are sorry for the confusion. Please see our clarifications below.\n\n> Comment: I thank the authors for updating the manuscript with new results and visualizations to clarify my questions. However, the following concerns still exist.\n>\n> 'Comparisons seem biased': The authors should organize their writing to reflect a clear and straightforward comparison with proper baseline methods. I suggest the authors first clarify the categories of CL/task-IL/Class-IL and corresponding subcategories (memory/non-memory). From our discussion and the authors' rebuttals, it seems that the proposed method is memory-based class incremental learning, i.e., it needs memory and doesn't need a task label during reference. If that is the case, the authors should only compare it with such settings in CIL. Therefore, adding the memory-free approach to compare with the proposed method is unclear and also unfair. \n\n**Answer**: We are happy to remove memory-free methods from the table. \n\n> I have also read the other reviewer's comments and I see other reviewer also has a similar question: 'In Table 1, which of these results are using the task label at inference time?\u201d\n\n**Answer**: None of the methods in Table 1 uses the task label at inference time. \n\n> For example, HAT needs the task label. So, are the results of HAT comparable here with the other methods? Or is HAT having a forward pass with each task label and then using some heuristic to pick the class? \n\n> However, the author's reply also makes me more confused:\n\n> \"In Table 1, our method using HAT needs the task label. But since we run all task models at test time to compute the task label probability for each task, effectively we do not need any task label.\"\n\n**Answer**: Sorry for the confusion. Let us try to clarify this. \n\nThe original HAT needs the task label for each test instance for task-incremental learning (TIL). However, we are using HAT for class-incremental learning (CIL) and we have **no** task label for each test instance x, so we need to adapt HAT for CIL (as we explained in the footnote of page 7). \n\nSpecifically, we run x on the model of every task. For example, we have 4 tasks and we have learned 4 models for the 4 tasks based on HAT: Model1, Model2, Model3, Model4. HAT\u2019s results in Table 1 are obtained by running x on every model via a forward pass to compute the following (for example): \n\nFor Model1: class c5 in task 1 gives the highest softmax value of 0.7\n\nFor Model2: class c2 in task 2 gives the highest softmax value of 0.6 \n\nFor Model3: class c6 in task 3 gives the highest softmax value of 0.3\n\nFor Model4: class c1 in task 4 gives the highest softmax value of 0.9 \n\nThen the test instance x is assigned the final class of c1 in task 4 because its highest softmax value is the highest among the 4 task models. \n\nFor our method, we do similarly but we compute the task probability based on each model with a softmax-based normalization, i.e., P(task-1 | x), P(task-2 | x), P(task-3 | x), P(task-4 | x).\n\n> Therefore, it is unclear to me whether the proposed method requires task labels or not\n\n**Answer**: No, we do not need task labels. \n\n> I also checked Appendix G as the author suggested. Table 8 is unclear to me. If the author is seeking a fair comparison, they are supposed to use the same model for comparisons. Why are there differences in parameters for different methods?\n\n**Answer**: We cannot ensure each method has the same number of network parameters as they are designed with different components in their algorithm. For example, DER, FOSTER, and BEEF are network-expansion-based methods. They dynamically increase the model size during continual learning as more tasks are learned; L2P needs to maintain a prompt pool; OWM is based on orthogonal projection and it needs to save some matrices.\n\nTo seek a fair comparison, we can only ensure that they are using the same backbone, i.e., pre-trained DeiT or non-pre-trained ResNet18 in our two experimental settings. \n\nHope these clear all your doubts. If you have additional questions or comments, please let us know."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180383290,
                "cdate": 1700180383290,
                "tmdate": 1700180752416,
                "mdate": 1700180752416,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gRmat3j09v",
            "forum": "8QfK9Dq4q0",
            "replyto": "8QfK9Dq4q0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_h9ds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_h9ds"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors address the challenge of task identification (task-id prediction) in Class Incremental Learning (CIL). They propose a novel method named TPLR (Task-id Prediction based on Likelihood Ratio), which enhances task-id prediction by utilizing replay data to estimate the distribution of non-target tasks. This approach allows for a more principled solution compared to traditional Out-of-Distribution (OOD) detection methods that cannot estimate the vast universe of non-target classes due to lack of data.\n\nTPLR calculates the likelihood ratio between the data distribution of the current task and that of its complement, providing a robust mechanism for task-id prediction. The method is integrated into the Hard Attention to the Task (HAT) structure, which employs learned masks to prevent catastrophic forgetting, adapting the architecture to facilitate both task-id prediction and within-task classification.\n\nThe authors demonstrate through extensive experimentation that TPLR substantially outperforms existing baselines in CIL settings. This performance is consistent across different configurations, including scenarios with and without pre-trained feature extractors. The paper's contributions offer significant advancements for task-id prediction in CIL, proposing a method that leverages available data more effectively than prior approaches."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n\n- TPLR's innovation lies in its unique application of likelihood ratios for task-id prediction, an approach that distinctively diverges from traditional OOD detection methods.\n- The paper creatively leverages replay data to estimate the data distribution for non-target tasks, which is a novel use of available information in the CIL framework.\n- Integration of TPLR with the HAT method showcases an inventive combination of techniques to overcome catastrophic forgetting while facilitating task-id prediction.\n\nQuality:\n\n-The methodological execution of TPLR is of high quality. It is underpinned by a strong theoretical framework that is well-articulated and logically sound.\n- Extensive experiments validate the robustness and reliability of TPLR, demonstrating its superiority over state-of-the-art baselines.\n\nClarity:\n\nThe paper writing quality is satisfactory.\n\nSignificance:\n\nTPLR's ability to outperform existing baselines marks a significant advancement in the domain of CIL, potentially influencing future research directions and applications.\nThe paper's approach to using replay data for improving task-id prediction could have broader implications for continual learning paradigms beyond CIL."
                },
                "weaknesses": {
                    "value": "The key weakness of this work I would argue is its overly complex presentation. I find that the organization of the paper can easily distract and confuse the reader, often finding myself fishing for key details of the main method."
                },
                "questions": {
                    "value": "- While the writing quality is satisfactory, I would argue for a friendlier approach to outlining the proposed method. First, outline the key ingredients. Then explain how they interact. Finally cross-reference these with the existing figure. \n- The existing figure is a bit too 'noisy' in terms of the information it is showing and the order it is showing it in. Consider reorganizing it so it can be read from left to right, top to bottom and with more emphasis on the key ideas and less detail that can distract from that."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716679553,
            "cdate": 1698716679553,
            "tmdate": 1699636036470,
            "mdate": 1699636036470,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GrOdM3Qe9b",
                "forum": "8QfK9Dq4q0",
                "replyto": "gRmat3j09v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer h9ds,\n\nThank you very much for supporting our paper. Below, we address your comments. \n\n> While the writing quality is satisfactory, I would argue for a friendlier approach to outlining the proposed method. First, outline the key ingredients. Then explain how they interact. Finally cross-reference these with the existing figure.\n\nThank you for the suggestion. We tried to follow a top-down approach to present our method: \n\nSection 3 presents the motivation and overview of our TPLR method, which consists of training and testing. \n\n- For training, the key components are our proposed network architecture and HAT for protecting each task model, which corresponds to the left part of Figure 1.\n- For testing, there are two key ingredients: within-task prediction and task-id prediction probabilities, which correspond to the top right part and bottom right part of Figure 1.\n\nAs estimating task-id prediction probability is the most critical part, we use the whole of Section 4 to introduce the theorem and the design. Also, the design consists of three key ingredients: \n\n- (1) estimating the likelihood ratio between $P_t$ and $P_{t^c}$ (corresponds to the \u201cFeature-based likelihood ratio score\u201d in Figure 1). \n- (2) combining with a logit-based score (corresponds to \u201cMax logit score\u201d in Figure 1). \n- (3) converting task-id prediction scores to probabilities (corresponds to \u201csoftmax\u201d in Figure 1).\n\nFollowing your suggestions, we have revised the paper (especially Section 3 and Section 4) to improve the clarity and also make it friendlier by adding more cross-references (in blue fonts).\n\n> The existing figure is a bit too 'noisy' in terms of the information it is showing and the order it is showing it in. Consider reorganizing it so it can be read from left to right, top to bottom and with more emphasis on the key ideas and less detail that can distract from that.\n\nThank you for this suggestion. We revised Figure 1 to make it friendlier. We added the subtitles \u201cNetwork Architecture\u201d and \u201cInference Pipeline\u201d to divide the figure into two parts (left and right). We will keep thinking about how to make Figure 1 even easier to understand."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111293153,
                "cdate": 1700111293153,
                "tmdate": 1700111293153,
                "mdate": 1700111293153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pgwYli5f30",
            "forum": "8QfK9Dq4q0",
            "replyto": "8QfK9Dq4q0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_SU84"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_SU84"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for class incremental learning (CIL) by directly predicting the task identifiers to perform the task-wise prediction. Using the energy based models, the given model computes the scores for each task based on the Mahalanobis distance and KNN distance, and estimate the task label. Furthermore, the proposed model actively utilizes the pre-trained model by just training the adapter module to efficiently train the parameters. In the experiment, the given algorithm outperforms the baselines in both CIFAR and Tiny-ImageNet dataset. In addition, the authors show the effectiveness of using each component in the ablation study."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. By directly estimating the task identifier, the proposed algorithm outperforms other baselines in the benchmark dataset.\n\n2. Since the proposed model utilize the task-wise classifier, it can be robust to the class imbalance problem which can occur when the difference between the size  of replay buffer and training data are large."
                },
                "weaknesses": {
                    "value": "1. I wonder the proposed methods can achieve high task-prediction accuracy. Different from the ideal situation, the accuracy may be lower than we expected. if the semantics across different classes are similar, the task-prediction accuracy can be low, and the overall performance also can decrease. \n\n2. Can this method outperform other baselines when it does not use the pre-trained model in ImageNet-1K? Furthermore, if the dataset used for pre-training are randomly selected (i.e. Randomly extract 500 classes from ImageNet-1K), can this method outperform other baselines? Since ImageNet-1K or other large datasets contain similar classes, the task-prediction is much harder than CIFAR or Tiny-ImageNet"
                },
                "questions": {
                    "value": "Already mentioned in the Weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740036958,
            "cdate": 1698740036958,
            "tmdate": 1699636036360,
            "mdate": 1699636036360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oroNDA6XAQ",
                "forum": "8QfK9Dq4q0",
                "replyto": "pgwYli5f30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer SU84, \n\nwe are glad to address your concerns as follows.\n\n> I wonder the proposed methods can achieve high task-prediction accuracy. Different from the ideal situation, the accuracy may be lower than we expected. if the semantics across different classes are similar, the task-prediction accuracy can be low, and the overall performance also can decrease.\n\nWe are not sure about the first part of your question and \u201cthe accuracy we expected.\u201d Our method does not explicitly predict the task, but only estimates the task prediction probability for each test instance. Just for curiosity, we use the estimated task prediction probability to do task prediction by choosing the task with the highest probability. The task prediction accuracy results (after learning the final task) are shown in the table below (we also compare with the best-performing baselines MORE and ROW). Our TPLR consistently outperforms the two strong baselines on task prediction accuracy, which demonstrates the superiority of TPLR over OOD detection-based methods for task prediction in MORE and ROW.\n\n|      | C10-5T | C100-10T | C100-20T | T-5T   | T-10T  | Average |\n| ---- | ------ | -------- | -------- | ------ | ------ | ------- |\n| MORE | 0.8861 | 0.7076   | 0.6997   | 0.7368 | 0.6835 | 0.7427  |\n| ROW  | 0.9131 | 0.7531   | 0.7411   | 0.7382 | 0.6855 | 0.7662  |\n| TPLR | **0.9291** | **0.7819**   | **0.7681**   | **0.7891** | **0.7393** | **0.8015**  |\n\nAbout the second part of your comment, if the semantics across different classes are similar, the accuracy should be lower, which is the case for any supervised learning. In our case, the accuracy will be lower for all CIL baselines. \n\n> Can this method outperform other baselines when it does not use the pre-trained model in ImageNet-1K? Furthermore, if the dataset used for pre-training are randomly selected (i.e. Randomly extract 500 classes from ImageNet-1K), can this method outperform other baselines? Since ImageNet-1K or other large datasets contain similar classes, the task prediction is much harder than CIFAR or Tiny-ImageNet.\n\nWe have conducted the experiment on ImageNet-1K without a pre-trained model. Following the setting in BEEF [1], we split the ImageNet-1K into 10 tasks and compared our TPLR with the top-five baselines DER, FOSTER, BEEF, MORE, and ROW. The CIL accuracy results (after learning the final task) are shown in the table below. We can see that TPLR also outperforms the baselines on ImageNet-1K without a pre-trained model.\n\n| DER    | FOSTER | BEEF   | MORE   | ROW    | TPLR       |\n| ------ | ------ | ------ | ------ | ------ | ---------- |\n| 0.5883 | 0.5853 | 0.5867 | 0.5030 | 0.5214 | **0.6171** |\n\nTo address the second part of your comment, we would first like to stress that our pre-trained model did not use the full ImageNet data. Instead, our pre-training used only 611 classes of ImageNet after removing 389 classes that are similar or identical to the classes of the experiment data CIFAR and TinyImageNet to prevent information leak (see the last sentence in page 7). \n\nBased on your suggestion, we also conducted experiments by randomly selecting 500 classes from the remaining 611 classes after removing overlapping classes with CIFAR and TinyImageNet to prevent information leaks. We feel this is the right way to do it. We trained three different pre-trained models using three different sets of 500 classes. The CIL accuracy results (after learning the final task) for the top-five baselines and our TPLR are shown as follows. For simplicity, we average the results on the five datasets (C10-5T, C100-10T, C100-20T, T-5T, T-10T). We observed that TPLR has similar performance gains compared to those reported in the paper. \n\n|                     | DER    | FOSTER | BEEF   | MORE   | ROW    | TPLR       |\n| ------------------- | ------ | ------ | ------ | ------ | ------ | ---------- |\n| pre-trained model 1 | 0.6815 | 0.6817 | 0.7011 | 0.7132 | 0.7312 | **0.7615** |\n| pre-trained model 2 | 0.6801 | 0.6777 | 0.6935 | 0.7081 | 0.7253 | **0.7591** |\n| pre-trained model 3 | 0.6753 | 0.6823 | 0.7015 | 0.7077 | 0.7214 | **0.7585** |\n\n[1] Wang et al. Beef: Bi-compatible class-incremental learning via energy-based expansion and fusion. ICLR 2022.\n\nHope our additional experimental results address your concerns. We also hope that our strong results and novel and principled technique can change your mind to support our paper. If you have any further comments, we will be very happy to address them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111174294,
                "cdate": 1700111174294,
                "tmdate": 1700187655863,
                "mdate": 1700187655863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uOav00emsQ",
                "forum": "8QfK9Dq4q0",
                "replyto": "pgwYli5f30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer SU84,\n\nThe deadline of the discussion period is soon approaching. We wonder whether our answers to your questions and the new experimental results based on your suggestion have addressed your concerns. If there are any additional discussion points or questions, we are happy to discuss. Thank you.\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667746508,
                "cdate": 1700667746508,
                "tmdate": 1700667746508,
                "mdate": 1700667746508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UhAJntUbCv",
                "forum": "8QfK9Dq4q0",
                "replyto": "pgwYli5f30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer SU84,\n\nRegarding your comment that similar classes across tasks may decrease the accuracy, we have conducted a new experiment, described below. \n\nIn our paper, we used the CIFAR-100 dataset, which comprises 100 classes grouped into 20 superclasses, each containing 5 similar classes (e.g., the \"flowers\" superclass includes \"orchids\", \"poppies\", \"roses\", \"sunflowers\", \"tulips\"). We conducted the CIFAR100-20T (20 tasks) experiment using different task sequences: \n\n- seq0: each task contains classes from a distinct superclass, which means that within each task the classes are similar, but across tasks, the classes are dissimilar.\n\n- seq1: each task contains classes from different superclasses, which means that within each task, the classes are dissimilar, but across tasks, there are many similar classes. \n\n- seq2: seq3 and seq4 are randomly-mixed tasks, meaning that the classes of each task are randomly selected from random superclasses. \n\nThis design shows different combinations of similar or dissimilar classes in the same or different tasks. As we can see from the table below (the results are the averages of 5 random runs with 5 random seeds), the performances across these task sequences are very similar. The differences are not significant. \n\n| **Last. Acc**               | **seq0** | **seq1** | **seq2** | **seq3** | **seq4** |\n| --------------------------- | -------- | -------- | -------- | -------- | -------- |\n| mean (5 seeds)               | 76.40    | 76.25    | 76.29    | 76.32    | 76.42    |\n| standard deviation (5 seeds) | 0.28     | 0.31     | 0.38     | 0.24     | 0.33     |"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712254416,
                "cdate": 1700712254416,
                "tmdate": 1700712314049,
                "mdate": 1700712314049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uvrZ4G7nHt",
            "forum": "8QfK9Dq4q0",
            "replyto": "8QfK9Dq4q0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_fLUE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1104/Reviewer_fLUE"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to use out-of-distribution ideas to solve the gap between task incremental and class incremental learning, indirectly predicting the task label and using it to refine the class prediction. They pose that using a low forgetting method such as HAT and pairing it with a good task-prediction from the ood-inspired setting, allows better estimates of both the intra- and inter-task probabilities, which leads to better performance in CIL scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is simple and well explained, backed up with justification of why they choose the likelihood ratio strategy. The idea of using ood ideas to overcome the task-ID limitation of TIL is interesting and aligns with the continual learning community directions. The experimental results are compared with a large array of existing methods and state-of-the-art approaches."
                },
                "weaknesses": {
                    "value": "The proposed method is for the most part an extension of existing previous work, which requires a replay buffer, pretrained models and the need for a forward pass for each task learned. Therefore, the advantage of not needing the task label at inference is not well contrasted with the limitations (mostly mentioned at the end of the appendix only). I would expect further discussion and justification about how these benefits and limitations balance in the main part of the manuscript."
                },
                "questions": {
                    "value": "It is mentioned that HAT prevents CF, but it actually only mitigates it. It is discussed later in the appendix that a very large sigmoid is used in order to force an almost binary mask to promote more of that CF mitigation. However, how relevant is that the masks are binary and that the sigmoid is close to a step function? Would then a method that guarantees no forgetting such as PNN [Rusu et al. 2016], PackNet [Mallya et al. 2018] or Ternary Masks [Masana et al. 2020] be more suitable for the proposed strategy? How do you deal with HAT running out of capacity when the sequence gets longer?\n\nIn Table 1, which of these results are using the task label at inference time? For example, HAT needs the task label. So are the results of HAT comparable here with the other methods? Or is HAT having a forward pass with each task label and then using some heuristic to pick the class?\n\nFor the experiments on running time, in Table 9 of the appendix it is only shown the running times for the 4 methods that have the same base strategy. How do those compare with all the other methods, because I would assume that for large sequences of tasks, it might become quite a limiting factor to have to forward each sample/batch T times. I would argue that is a relevant discussion to have in the main manuscript.\n\nIn the introduction it is mentioned \"This means the universal set [...] includes all possible classes in the world [...], which is at least very large if not infinite in size...\". Is there some paper or relevant source to back this? One of the papers that comes to mind is [Biederman, 1987], which states that there are between 10k to 30k visual object categories that we can recognize in images. And that would hint towards learning an estimate of the distribution for objects in images would not be such unfeasible (specially now with foundational models).\n\nIn conclusion, I find the idea interesting and relevant. However, the small extension from existing related work, and the lack of a better discussion of the limitations and motivation/relevance for the community could be improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1104/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699014382372,
            "cdate": 1699014382372,
            "tmdate": 1699636036299,
            "mdate": 1699636036299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hBhXcQ6oNV",
                "forum": "8QfK9Dq4q0",
                "replyto": "uvrZ4G7nHt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fLUE, \n\nwe are glad to address your concerns as follows.\n\n>  The proposed method is for the most part an extension of existing previous work, which requires a replay buffer, pretrained models, and the need for a forward pass for each task learned. Therefore, the advantage of not needing the task label at inference is not well contrasted with the limitations (mostly mentioned at the end of the appendix only). I would expect further discussion and justification about how these benefits and limitations balance in the main part of the manuscript.\n\nWe think there may be a slight misunderstanding here. Our method works with or without a pre-trained model. Due to space limitations, we put only the summary results without using a pre-trained model in Table 2 (the full results are given in Table 5 of Appendix B.3). Regarding the forward pass for each task, it should not hurt the running time with parallel computing (also addressed in your third question). We have included a discussion about this in the paper on page 4 (blue font above Eq.3). Additionally, \u201cnot needing the task label at inference\u201d is a key feature and challenge of CIL.\n\n> It is mentioned that HAT prevents CF, but it actually only mitigates it. It is discussed later in the appendix that a very large sigmoid is used in order to force an almost binary mask to promote more of that CF mitigation. However, how relevant is that the masks are binary and that the sigmoid is close to a step function? Would then a method that guarantees no forgetting such as PNN [Rusu et al. 2016], PackNet [Mallya et al. 2018] or Ternary Masks [Masana et al. 2020] be more suitable for the proposed strategy? How do you deal with HAT running out of capacity when the sequence gets longer?\n\nIn our experiments, we found that the sigmoid with a low temperature is enough to prevent forgetting. We examined the task-incremental learning (TIL) accuracies and saw no forgetting. The sigmoid simulates a step function with an error less than 1e-7. The methods you suggested (PNN, PackNet, and Ternary Masks) are similar to HAT and can also be applied to our method. The main reason we chose HAT is to make it easy to compare with previous works based on the same setting. We have cited and discussed these three papers on page 4 (blue font in line 3) and Appendix E (last paragraph in blue font).\n\nThe issue that HAT may run out of capacity is a good question. Actually, we tried to decrease or increase the hidden sizes of the inserted adapters used by HAT on long task sequences. We found that HAT is quite efficient in utilizing the network capacity. We believe this is due to the fact that the pre-trained model already has good features and additional adaptations in the adapters make the new task require little feature learning. We agree that the capacity will be an issue with super-long task sequences, which is a problem for all fixed architecture methods. Fortunately, it is quite simple to expand the network of HAT dynamically when needed without interfering with the learned models because its masks isolate a subnetwork for each task in a shared network.\n\n> In Table 1, which of these results are using the task label at inference time? For example, HAT needs the task label. So are the results of HAT comparable here with the other methods? Or is HAT having a forward pass with each task label and then using some heuristic to pick the class?\n\nIn Table 1, our method using HAT needs the task label. But since we run all task models at test time to compute the task label probability for each task, effectively we do not need any task label. Specifically, for a test instance x, a forward pass goes through the model for each task t to compute the task label probability for the task, i.e., P(t | x) and the class probability of each class $y_j^{(t)}$ in task t, i.e., $P(y_j^{(t)} | x)$. These two probabilities are used in Eq. (1) to predict the final class for x. The baselines MORE and ROW also need task label prediction. \n\nThere are also some other existing systems that need task label prediction, which we have discussed in the last paragraph of the related work section. However, we did not include them as baselines as they perform poorly (much poorer than MORE and ROW), which we have explained in the last paragraph of the related work section. For example, the two recent systems iTAML and PR-Ent give an average accuracy of 57.28 and 64.69 on all our experiments (using the same pre-trained model as TPLR), respectively, but our method TPLR gives 76.21. Their detailed results can be found in the table in our response to the third comment of Reviewer aMSi."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111016532,
                "cdate": 1700111016532,
                "tmdate": 1700145624286,
                "mdate": 1700145624286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pkC5HvRBkK",
                "forum": "8QfK9Dq4q0",
                "replyto": "uvrZ4G7nHt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "About your following question, we would like to clarify further.\n\n> In Table 1, which of these results are using the task label at inference time? For example, HAT needs the task label. So are the results of HAT comparable here with the other methods? Or is HAT having a forward pass with each task label and then using some heuristic to pick the class?\n\n**Answer**: The original HAT needs the task label for each test instance for task-incremental learning (TIL). However, we are using HAT for class-incremental learning (CIL) and we have **no** task label for each test instance x, so we need to adapt HAT for CIL (as we explained in the footnote of page 7). \n\nSpecifically, we run x on the model of every task. For example, we have 4 tasks and we have learned 4 models for the 4 tasks based on HAT: Model1, Model2, Model3, Model4. HAT\u2019s results in Table 1 are obtained by running x on every model via a forward pass to compute the following (for example): \n\nFor Model1: class c5 in task 1 gives the highest softmax value of 0.7\n\nFor Model2: class c2 in task 2 gives the highest softmax value of 0.6 \n\nFor Model3: class c6 in task 3 gives the highest softmax value of 0.3\n\nFor Model4: class c1 in task 4 gives the highest softmax value of 0.9 \n\nThen the test instance x is assigned the final class of c1 in task 4 because its highest softmax value is the highest among the 4 task models. This is similar to the \"some heuristic\" you mentioned in your question."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180419535,
                "cdate": 1700180419535,
                "tmdate": 1700180441735,
                "mdate": 1700180441735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4tVEeEtrWH",
                "forum": "8QfK9Dq4q0",
                "replyto": "zQNpcrtKQC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Reviewer_fLUE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Reviewer_fLUE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their detailed responses. They have addressed most of my concerns, although I do not agree with the simplicity of solving the amount of computation by parallel computing (PC) not being further discussed. I do understand that that part can be solved with PC, but the main point missing in the discussion is that it might not hold well when running for very large sequence of tasks, or does not acknowledge that the computational cost is still way larger than other existing CIL methods which do not require PC. \n\nI find the suggestion by reviewer SU84 of an analysis of the influence of highly similar/dissimilar classes on same/different tasks would be interesting. In that sense, the new tables provided do not seem to tackle the issue (unless I misunderstood it). However, the response by the authors to the second question (pretrained models) is very convincing and does support the effectiveness of the proposed approach. The discussions from the other two reviewers are appreciated and I have no major concerns there.\n\nIn conclusion, I still believe that the proposed method is interesting and that the provided changes help improve the manuscript. I would be in favor of acceptance."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676853497,
                "cdate": 1700676853497,
                "tmdate": 1700676853497,
                "mdate": 1700676853497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2NMi3NQ5UJ",
                "forum": "8QfK9Dq4q0",
                "replyto": "uvrZ4G7nHt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1104/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fLUE,\n\nThank you for your insightful feedback. We greatly value your comments and wish to discuss further about the two points you raised.\n\n1. **Discussion on Computation by Parallel Computing:** In our previous discussion, we focused primarily on the running time and did not provide a comprehensive analysis of computational costs. To address this, we have included a new section **Appendix E.2**, analyzing computation through parallel computing. Our findings reveal that our method can attain time efficiency comparable to the standard model (without T times of forward passes), albeit with a trade-off of increased runtime memory for storing task-specific features. This trade-off enables a balance between memory usage and time consumption by adjusting the **level of parallelism**. For instance, the running time memory can be reduced by forwarding the input more frequently with a lower level of parallelism.\n2. **Influence of Similar/Dissimilar Classes on Same/Different Tasks:** We appreciate this suggestion. We have conducted a new experiment to study this. In our paper, we used the CIFAR-100 dataset, which comprises 100 classes grouped into 20 superclasses, each containing 5 similar classes (e.g., the \"flowers\" superclass includes \"orchids\", \"poppies\", \"roses\", \"sunflowers\", \"tulips\"). We conducted the CIFAR100-20T (20 tasks) experiment using different task sequences: \n\n- seq0: each task contains classes from a distinct superclass, which means that within each task the classes are similar, but across tasks, the classes are dissimilar.\n\n- seq1: each task contains classes from different superclasses, which means that within each task, the classes are dissimilar, but across tasks, there are many similar classes. \n\n- seq2: seq3 and seq4 are randomly-mixed tasks, meaning that the classes of each task are randomly selected from random superclasses. \n\nThis design should mirror your concept of \"highly similar/dissimilar classes on same/different tasks.\" As we can see from the table below (the results are the averages of 5 random runs with 5 random seeds), the performances across these task sequences are very similar. The differences are not significant. \n\n| **Last. Acc**               | **seq0** | **seq1** | **seq2** | **seq3** | **seq4** |\n| --------------------------- | -------- | -------- | -------- | -------- | -------- |\n| mean (5 seeds)               | 76.40    | 76.25    | 76.29    | 76.32    | 76.42    |\n| standard deviation (5 seeds) | 0.28     | 0.31     | 0.38     | 0.24     | 0.33     |\n\nWe thank you again for your valuable comments which help us improve our paper significantly. \n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1104/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712196224,
                "cdate": 1700712196224,
                "tmdate": 1700712306033,
                "mdate": 1700712306033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]