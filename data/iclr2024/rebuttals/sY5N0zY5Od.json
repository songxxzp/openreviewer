[
    {
        "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines"
    },
    {
        "review": {
            "id": "qIhxcYk7zs",
            "forum": "sY5N0zY5Od",
            "replyto": "sY5N0zY5Od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5605/Reviewer_cQiR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5605/Reviewer_cQiR"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DSPy, an LM pipeline framework consisting of a programming model and compiler. The programming model provides composable and declarative modules for LM instruction. These modules function similarly to function calls, allowing users to define input/output behavior using natural language signatures. The compiler is capable of generating high-quality prompts automatically or fine-tuning LMs using a general optimization strategy (teleprompter). Through evaluations on GSM8K and HotpotQA, the authors demonstrate DSPy's ability to reduce the required number of handwritten prompt templates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Novel Approach. This paper introduces a novel approach to systematically build LM pipelines and compile modules into a set of prompts (or fine-tunes) optimized for specific tasks. The approach is promising, showcasing its ability to reduce the human effort required to develop prompt pipelines. \n2. Well-Written. The paper is clearly written and presents the core concepts of DSPy in a straightforward manner."
                },
                "weaknesses": {
                    "value": "1. Lack of details: DSPy stands out from other frameworks, like LangChain, due to its capacity for automatic prompt generation and optimization. However, the introduction of the DSPy Compiler lacks sufficient specificity. It would be beneficial to provide more comprehensive details on how DSPy generates candidate values for module parameters (instructions, field description, and example input/output) based on signatures."
                },
                "questions": {
                    "value": "## Approach:\n1. Can we easily implement common and advanced prompting techniques with signatures? I noticed that the signatures used in the Case Study are quite simple. When it comes to dealing with complex user intentions, do you think additional hand-written comments are necessary alongside signatures? For instance, in the ChainOfThought module, I found that the prompt \"Let's think step by step\" still requires handwriting within the module. How many prompts would need to be handwritten to achieve functional equivalence with a complex prompt (such as Appendix D prompt 6) using signatures?\n2. How can users debug their DSPy programs effectively? In Section 4 of the paper, it is mentioned that \"A well-decomposed program can typically find at least a few training examples...\" I suppose that the success of optimization depends on a well-structured decomposition by the user, which implies the need for frequent modification of DSPy programs. Consequently, it raises the question of how users can determine which modules are not functioning correctly. Is it necessary for users to repeatedly rewrite the entire DSPy program?\n3. How much does the optimization cost in terms of computing resources or API usage? (This question may exceed the paper's scope) \n## Evaluation\n1. In section 5, Hypothesis 2, the paper mentions the possibility of DSPy outperforming expert-written prompts. However, is there an evaluation comparing DSPy with handwritten prompts to support this claim?\n2. In Section 5 of the paper, it states, \"... \u2018how they compare on GSM8K with program P when compiled with strategy S\u2019, which is a well-defined and reproducible run.\u201d How can the reproducibility of these results be ensured?\n3. In Case Study 1 using the GSM8K dataset, GPT-3.5 with 5 few shots achieves a score of 57%, while vanilla+GPT-3.5+fewshot only scores 33.1%. What factors contribute to the decline in performance, considering the similarity between these two approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749087298,
            "cdate": 1698749087298,
            "tmdate": 1699636577748,
            "mdate": 1699636577748,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K5ckMLejSg",
                "forum": "sY5N0zY5Od",
                "replyto": "qIhxcYk7zs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed feedback on our work. We are delighted that you found our approach novel and promising and our paper well-written.\n\n\n> More comprehensive details on how DSPy generates candidate values for module parameters (instructions, field description, and example input/output) based on signatures.\n\nThank you for the suggestion. We now include a new appendix I (pages 28, 29, 30) which include snippets of our optimized prompts, when compiling with Llama2-13b-chat. This should help the reader concretely visualize the parts of a typical prompt constructed from a signature. In particular, it contains the instructions (which, if omitted, are inferred from the fields), the field organization, and the demonstrations.\n\nIn short, the module defines a parameter space. A teleprompter can then optimize these different pieces. For instance, we have an ongoing research project that optimizes the instructions of DSPy modules automatically. In this paper, we focus on optimization with random search over demonstrations, where we automatically created traces of each module, and then select different traces as few-shot examples (or finetuning training data), even if there are no hand-written labels for these modules.\n\n\n> Can we easily implement common and advanced prompting techniques with signatures? When it comes to dealing with complex user intentions, do you think additional hand-written comments are necessary alongside signatures?\n\nA prompting technique is best abstracted as a Module in DSPy. It is indeed possible that a specific approach to prompting requires adding new modules to DSPy, which should work for any signature (i.e., any input/output patterns).\n\nOur vision is that any minor hard-coded parts of the modules (e.g., \u201cLet\u2019s think step by step.\u201d) should eventually be treated as variables to optimize. Nonetheless, like in traditional DNNs, there are always hyperparameters that may be selected by hand, as long as the overall model at the end is optimized automatically.\n\nSince the submission of this work, we have built (or helped others build) a number of new sophisticated systems with DSPy. In some of them, we found that users like to include additional comments (or instructions) in the extended DSPy format that we present in Appendix B \u201cAdvanced Signatures\u201d. To make this more robust, we have been exploring techniques to automatically optimize these prompts and to allow for automatic backtracking logic to correct errors. Considering the scope, length, and complexity of this paper, we leave exploring these additional components to future publications related to DSPy.\n\n\n> How can users debug their DSPy programs effectively? [...] it raises the question of how users can determine which modules are not functioning correctly. Is it necessary for users to repeatedly rewrite the entire DSPy program?\n\nOne of the key reasons we built DSPy as a \u201cdefine-by-run\u201d abstraction (like PyTorch, or the \u201ceager mode\u201d of TensorFlow 2.0) is to facilitate debugging.\n\nIn particular, users of DSPy can simply insert checkpoints or use print statements between different steps of their DSPy programs. For example, in the multi-hop program, we can print or log every search query generated (or even manually change these queries) in order to understand which module is responsible for different mistakes.\n\nBecause it is possible to debug one module at a time, we do not anticipate any need for users to repeatedly rewrite their programs. Instead, they can focus on isolating individual types of errors and then improving the responsible module. Of course, DSPy is not a panacea and sometimes development or debugging may require more end-to-end tracing of the full program. To this end, we recently started adding support for tracing of the full program to visualize all the prompts during execution. \n\n[continued below]"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617696645,
                "cdate": 1700617696645,
                "tmdate": 1700617696645,
                "mdate": 1700617696645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbb1p0M16N",
                "forum": "sY5N0zY5Od",
                "replyto": "qIhxcYk7zs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[continue...]\n\n\n> How much does the optimization cost in terms of computing resources or API usage?\n\nThank you for emphasizing this element. To address this, we have added a new appendix A on Latency & Throughput of using DSPy. To illustrate that the cost of using DSPy is manageable, we consider a sophisticated program from our evaluation, i.e. the multi-hop search and QA program. We report on compiling it with OpenAI GPT-3.5 for $3.00 in just six minutes. Inference with this program takes approximately 3 seconds per example (single-threaded) and can easily exceed 150 questions per minute (with ten DSPy threads).\n\nFuture work should explore efficiency in a much broader evaluation to characterize this emerging space of LM programs. Just like the cost of training a new model, the cost of compiling a DSPy can vary greatly depending on the complexity of the program and the optimizer. However, we hope this small-scale evaluation sheds some light on the typical cost of compiling a program like this.\n\n\n> In section 5, Hypothesis 2, the paper mentions the possibility of DSPy outperforming expert-written prompts. However, is there an evaluation comparing DSPy with handwritten prompts to support this claim?\n\n\nThis is indeed an important aspect of DSPy. In our evaluations, the CoT program for GSM8K includes comparison between the \u201cbootstrap\u201d compilation approach and the \u201cfewshot +human_CoT\u201d setting (Table 1). The former generates its own optimized few-shot examples (including, in this program, the reasoning chains). The latter samples expert-written step-by-step reasoning chains, which are the typical way other papers build prompts for GSM8K (e.g., the original CoT paper by Wei et al). Another instance of this is the ReAct evaluations in Table 2, which compare handwritten prompting against our bootstrapping compilation approaches. Across these evaluations, we found that the compiled systems match or exceed the quality achieved by hand-written prompts and reasoning chains. These results can also be compared with systems from the literature (e.g., the original ReAct scores on HotPotQA), which generally use hand-written prompts (e.g., see Appendix E).\n\n\n>  In Section 5 of the paper, it states, \"... \u2018how they compare on GSM8K with program P when compiled with strategy S\u2019, which is a well-defined and reproducible run.\u201d How can the reproducibility of these results be ensured?\n\nTo reproduce a specific program, DSPy supports saving a dump of the compiled program. This is akin to saving a trained model, to draw an analogy with deep neural networks. As long as the LM can produce reproducible outputs, loading the saved program can be used to replicate old runs, if the LLMs are set for greedy decoding (i.e., temperature=0.0). DSPy also supports caching (and sharing the cache for) all invocations of the LM(s), which is another way to ensure reproducibility as needed.\n\nMore generally, it's also possible to re-run each setup several times (with higher temperature) and average the runs. Either way, like DNNs, DSPy offers a more self-contained setup than hand-drafting prompts (or hand-tuning classifier weights), which aids reproducibility of the key ideas in the the more general sense that the optimization process itself is automated and hence can be reapplied to new settings. In contrast, hand-tuning prompts is more of an art than a science and is hard to recreate.\n\n\n> In Case Study 1 using the GSM8K dataset, GPT-3.5 with 5 few shots achieves a score of 57%, while vanilla+GPT-3.5+fewshot only scores 33.1%. What factors contribute to the decline in performance, considering the similarity between these two approaches?\n\nThank you for the remark. Let us clarify how the different approaches for few-shot prompting work on GSM8K.\n\nIn Table 1, we report the following results with `gpt-3.5-turbo-instruct` (released in September 2023).\n\n1. Using the `fewshot` (LabeledFewShot) teleprompter in DSPy, on the `vanilla` program.\n2. Using the `fewshot` teleprompter on the `CoT` program.\n3. Using the `fewshot` teleprompter on the `CoT` program, with the extended \u201c+human CoT\u201d training set.\n\nIn the first two, we prompt the LM with few-shot examples that do *not* contain reasoning chains. That is, the demonstrations in #1 and #2 only include the final numerical answer. In the first one, we also do not request the model to engage in a step-by-step reasoning chain, unlike #2 and #3. In the third one, the few-shot examples include human-written reasoning chains.\n\nThese three approaches score 33%, 65%, and 78% on our development set. We test the last one on the official test set, and it scores 72%. In this case, the most comparable run to the original 5-shot GPT-3.5 evaluation by OpenAI is the third option. As you can see, our implementation here scores more highly (72% > 57%), which may be a result of using a newer release of GPT-3.5 (September vs March 2023), using extra demonstrations (5-shot vs 8-shot), and variance across samples (we average 5 runs)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617771342,
                "cdate": 1700617771342,
                "tmdate": 1700680958920,
                "mdate": 1700680958920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uzjRMISkL7",
            "forum": "sY5N0zY5Od",
            "replyto": "sY5N0zY5Od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5605/Reviewer_zPoD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5605/Reviewer_zPoD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a programming model DSPy that allows defining parameterized LLM-based text transformations. Under this model, multi-step LLM strategies like chain-of-thought, ensembles of chains-of-thoughts, ReAct, and reflection are straightforward to implement. The parameters in these strategies can include the few-shot demonstrations and the specific prompting wording, and DSPy admits using LLM-powered search strategies for optimizing over these strategy parameters. With DSPy, the paper authors demonstrate using a large language model to optimize prompting strategies for use with comparatively smaller language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper introduces a novel programming paradigm for defining and optimizing parameterized LLM-based text transformations.\n* Though I have not used DSPy myself, my impression is that there is a focus on expressiveness and usability in the programming model, which enhances its usefulness for the ICLR community. One piece of evidence for this expressiveness is the ability showcased to write CoT, reflection, ReAct, and other prompting strategies concisely within the DSPy model. Another is the composibility e.g. of different programs with different optimization (aka \"compilation\") approaches.\n* DSPy reduces the reliance of creating LLM-based text transformations on manual prompting. This can be seen as a strength, reducing the need for expertise in prompt writing. (It can also on some occasions be a weakness if it increases the amount of examples required e.g. from none.)"
                },
                "weaknesses": {
                    "value": "* My impression is that the value-add of DSPy for any of the listed strategies is somewhat small. E.g. implementing any of CoT, Reflection, or ReAct without DSPy does not require much code. The same is true, I think, for the optimization approaches / compilers. I expect this value-add grows when working with many such strategies at once, and additionally that DSPy provides organizational value both as the programs and compilers grow in complexity, and as the set of people using them grows. Being able to compose different strategies and compiler techniques is also one of the key beneficial properties of the system.\n* The evaluations performed do not provide a measure of compute usage or time (both for compilation as well as inference of the compiled programs), which makes comparisons across programs and compilers less meaningful.\n* There are no examples of compiled programs provided in the paper or appendices. I think an analysis of compiled programs would benefit the paper meaningfully. In particular, some unanswered questions about the compiled programs include: how do they differ from the types of prompt programs that people write by hand or using other frameworks? how do programs compiled for small LMs differ from those compiled for large LMs? are there any obvious patterns in the compiled programs? how about obvious shortcomings or irregularities, where additional hand-optimization would be easy? any evidence that the optimization techniques overfit to the validation set?"
                },
                "questions": {
                    "value": "# Questions and Suggestions:\n\nTerminology: In the abstract you say DSPy programs are \"compiled to\" a language model. I think this wording is a bit wrong or misleading, and that it would be better to say the program is \"optimized for\" or (if you insist on the language of compilation) \"compiled for\".\n\nThe paper has a repeated metaphor with neural network abstractions and deep learning frameworks (Section 1 paragraph 3, and Section 2 paragraph 1, and Section 3.2 paragraph 4). The metaphor rests on two properties of DSPy: its programs are modular and admit optimization. However, the main feature of deep learning systems is that they admit *gradient based* optimization, which is absent from DSPy, weakening the metaphor.\n\nTeleprompters are described as general-purpose optimization strategies, though if I understand correctly they are limited to gradient-free optimization techniques. This detail warrants mentioning.\n\nStrong claim: The paper claims DSPy is the first programming model that translates prompting techniques into parameterized declarative modules that can be optimized. In evaluating this claim, I considered the following relevant works that I thought I would note here.\nDohan 2022 Language Model Cascades https://arxiv.org/abs/2207.10342 also casts several prompting techniques like scratchpads / chain of thought, verifiers, STaR, selection-inference under a unified framework.\nDecomposed Prompting: A Modular Approach for Solving Complex Tasks https://arxiv.org/abs/2210.02406 splits tasks into subtasks that are solved a recombined to solve the overall task.\nANPL: Compiling Natural Programs with Interactive Decomposition https://arxiv.org/abs/2305.18498 admits the user performing the decomposition of tasks into subtasks.\nLarge Language Models as Optimizers https://arxiv.org/abs/2309.03409 like a DSPy compiler produces text-transformation using an LLM to perform prompt optimization.\nAmong these, Dohan 2022 is closest to the claim made in the paper.\n\n-- In your treatment of Predict, I don't think the inclusion of the \"None\" implementation detail aids the discussion.\n\nIn the list of parameters (Section 3.2, paragraph 3), field descriptions are omitted. \nHow do you think about safety during SQL generation, since SQL queries can modify or delete tables. How do you approach using tools with side effects during compilation?\nIn the RAG example at the end of Section 3.2, is it complete without a definition of Retrieve provided?\nHow can a DSPy user introduce a new Retrieve implementation?\n\nIn my view, one of the key results is that expensive LLMs can be used during the optimization process of a prompt pipeline targeting smaller LMs. I will copy here the text from the \"weaknesses section\" above pertaining to this point: There are no examples of compiled programs provided in the paper or appendices. I think an analysis of compiled programs would benefit the paper meaningfully. In particular, some unanswered questions about the compiled programs include: how do they differ from the types of prompt programs that people write by hand or using other frameworks? how do programs compiled for small LMs differ from those compiled for large LMs? are there any obvious patterns in the compiled programs? how about obvious shortcomings or irregularities, where additional hand-optimization would be easy? any evidence that the optimization techniques overfit to the validation set?\n\nRegarding how the compiled programs differ when targeting smaller LMs, a qualitative investigation with examples would be welcome.\n\nIn Table 1, reporting some measure of compute usage or time both for compilation and inference would be valuable, and would make the various strategies (both programs and compilation approaches) more readily comparable.\n\nNote: MultiChainComparison implementation is not provided.\nNote: Do the compilation approaches account for overfitting the validation set?\n\nAnother idea for further study: Can you compile a DSPy program for a small LM and then run that program using a more powerful LLM different from the one targeted during compilation in order to save on compilation costs? What sort of performance degradation does this incur?\n\nI also include here some typographic issues I identified in the paper. These did not meaningfully hinder readability of the work.\n\nTypo: GMS08K -> GSM8K (page 2, paragraph 4)\nTypo: format -> formats (page 4, Section 3.2)\nTypo: grammar: but -> or (page 5, Section 3.3)\nTypo: \"As we assumes\" in the example at the end of Section 3.3\nTypo: these -> this, ensembles -> ensembling (page 6, stage 3)\nTypo: \"specific math problems or particular LM.\" -> \"specific to math problems or a particular LM.\" (page 7)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795059222,
            "cdate": 1698795059222,
            "tmdate": 1699636577649,
            "mdate": 1699636577649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jZi366Opqd",
                "forum": "sY5N0zY5Od",
                "replyto": "uzjRMISkL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful review and the in-depth feedback on our work. We are glad to see that you found our programming paradigm 'novel' and our 'focus on expressiveness and usability', as evidenced in the 'composability' of programs and optimizations, promising for the ICLR community.\n\n\n> My impression is that the value-add of DSPy for any of the listed strategies is somewhat small. E.g. implementing any of CoT, Reflection, or ReAct without DSPy does not require much code. [...] I expect this value-add grows when working with many such strategies at once, and additionally that DSPy provides organizational value both as the programs and compilers grow in complexity [...] Being able to compose different strategies and compiler techniques is also one of the key beneficial properties of the system.\n\nThank you for the nuanced discussion. We agree that much of the power of DSPy comes from enabling rich compositions of modules and compilation strategies. However, we wish to emphasize that even simple programs (e.g., those that rely on a single module, like CoT or ReAct) stand to benefit from DSPy. To illustrate, it\u2019s indeed true that expressing CoT (or, say, ReAct) does not require much code without DSPy. However, expressing them *well* for a particular task generally requires writing a good prompt, often with high-quality few-shot examples for reasoning or tool use. Adapting this prompt across LMs may also pose a challenge. While these challenges for a single, simple module are not very large (and indeed more research is needed to make general claims about them), we believe the DSPy paradigm offers a consistent way to optimize both simple and complex programs alike.\n\n\n> The evaluations performed do not provide a measure of compute usage or time (both for compilation as well as inference of the compiled programs)\n\nThank you for emphasizing this element. To address this, we have added a new appendix A on Latency & Throughput of using DSPy. To illustrate that the cost of using DSPy is manageable, we consider a sophisticated program from our evaluation, i.e. the multi-hop search and QA program. We report on compiling it with OpenAI GPT-3.5 for $3.00 in just six minutes. Inference with this program takes approximately 3 seconds per example (single-threaded) and can easily exceed 150 questions per minute (with ten DSPy threads).\n\nFuture work should explore efficiency in a much broader evaluation to characterize this emerging space of LM programs. Just like the cost of training a new model, the cost of compiling a DSPy can vary greatly depending on the complexity of the program and the optimizer. However, we hope this small-scale evaluation sheds some light on the typical cost of compiling a program like this.\n\n\n>  There are no examples of compiled programs provided in the paper or appendices. [...] how do they differ from the types of prompt programs that people write by hand or using other frameworks? how do programs compiled for small LMs differ from those compiled for large LMs? are there any obvious patterns in the compiled programs?\n\nWe now include a new appendix I (pages 28, 29, 30) which include snippets of our optimized prompts, when compiling with Llama2-13b-chat. We believe an extensive analysis of these prompts, especially when varying LMs and varying optimization strategies, is a highly exciting future direction.\n\nNonetheless, it may be useful to consider Figure 11 (page 30) which reports on the Llama2-13b prompt for the second hop of query generation for HotPotQA. We see that it teaches the model two different strategies for generating search queries. The first bootstrapped few-shot example is a well-formed question \"When was the first of the vampire-themed fantasy romance novels published?\". The second is a keyword-based search query, \"Jeremy Paxman birth year\". It is possible to hypothesize this duality contributes to the selection of this prompt during optimization. Future work may try to characterize these patterns on a principled basis.\n\n\n> 'how about obvious shortcomings or irregularities, where additional hand-optimization would be easy?'\n\nOne of the limitations of the BoostrapFewshot set of teleprompters we discuss in this work is that they focus on generating and selecting demonstrations of the input/output behavior of each module. To expand this toward more open-ended optimization, we have started multiple research directions that explore optimizing the instructions and execution logic (e.g., backtracking on failure) for the modules.\n\n[continue below...]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617981563,
                "cdate": 1700617981563,
                "tmdate": 1700617994721,
                "mdate": 1700617994721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8S8hyYTHDS",
                "forum": "sY5N0zY5Od",
                "replyto": "uzjRMISkL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[continued...]\n\n\n> 1. \u201cIn the abstract you say DSPy programs are \"compiled to\" a language model.\u201d\n> 2. \u201cIf I understand correctly [teleprompters] are limited to gradient-free optimization techniques. This detail warrants mentioning.\u201d\n> 3. \u201cTypo: GMS08K -> GSM8K (page 2, paragraph 4) Typo: format -> formats (page 4, Section 3.2) Typo: grammar: but -> or (page 5, Section 3.3) Typo: \"As we assumes\" in the example at the end of Section 3.3 Typo: these -> this, ensembles -> ensembling (page 6, stage 3) Typo: \"specific math problems or particular LM.\" -> \"specific to math problems or a particular LM.\" (page 7)\u201d\n\n\nThank you for this extensive list of notes and typos. We believe we have now addressed all of these in the PDF. (We will update the submission abstract outside the PDF too, if the work is accepted. The system does not permit this change currently.)\n\n\n\n> Strong claim: The paper claims DSPy is the first programming model that translates prompting techniques into parameterized declarative modules that can be optimized. [...] Dohan 2022 is closest to the claim made in the paper.\n\n\nWe agree to soften our statement in the introduction, in light of this note. In short, Dohan et al. (whose work we cite as early as our first paragraph) present a rich self-described \u201cposition paper\u201d on the conceptual analogy between what they call \u201cLM cascades\u201d and probabilistic graphical models. While we find that work highly valuable, we are unable to immediately extrapolate from this conceptual analogy to a programming model in which prompting techniques can be optimized directly, as we do with DSPy. Nonetheless, to avoid any unintended implications, we have softened our statement in the introduction (marked in blue in page 2) in line with your comment.\n\n\n> Do the compilation approaches account for overfitting the validation set? [...] How do you think about safety during SQL generation, since SQL queries can modify or delete tables. How do you approach using tools with side effects during compilation?\n\nThese are both good examples where using DSPy does not replace the value of good, thoughtful experimental design. In particular, care must be taken if the program has access to tools that can modify (or destroy) records or execute code, like running it in a sandbox. We have a small Python execution sandbox in DSPy to aid with this, but it is not an alternative for thoughtful design.\n\nSimilarly, as with standard ML techniques, overfitting is an important consideration, especially when the training and validation sets are small. In our experience, we found limited signs of detrimental overfitting (i.e., selecting low-quality systems) but found evidence of sets of different difficulty such that the absolute scores of the systems are different across validation or evaluation sets.\n\nFor good experimental design, the teleprompters allow us to distinguish between the training, validation, development, and test sets. For instance, when compiling the multi-hop program on HotPotQA, we use the first 50 examples of the training set for training, and examples 50 through 200 for validation. We compare different approaches on a separate development set (300 examples). And we finally report the quality on the test set (1000 separate examples). While the teleprompters enable such good experimental design, this cannot be enforced. It is always possible for misuse of the optimization process to overfit.\n\n\n\n> In the RAG example at the end of Section 3.2, is it complete without a definition of Retrieve provided? How can a DSPy user introduce a new Retrieve implementation?\n\nAt the start of a script that uses DSPy, the user declares the \u201cdefault\u201d language model and retrieval model. These choices can also be changed for specific code blocks. In our experiments, we set the default retriever to be a ColBERTv2 (Santhanam et al., 2022) index over a Wikipedia corpus. However, this is very easy to extend. Our users have introduced many integrations for retrieval, from Pyserini to Pinecone to Weaviate, among other popular systems."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618154820,
                "cdate": 1700618154820,
                "tmdate": 1700618221421,
                "mdate": 1700618221421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S8CIqPE4or",
                "forum": "sY5N0zY5Od",
                "replyto": "8S8hyYTHDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Reviewer_zPoD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Reviewer_zPoD"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "Thank you for your responses to my review, and in particular for the multiple additions to the text (Appendixes A and I) that address my comments.\n\nWhile the addition of latency and throughput information that you've added in Appendix A is valuable for understanding the utility of DSPy, it does not address the main motivation I had for requesting that information; I was asking for some measure of cost in order to make the comparisons across programs and compilers in Tables 1 and 2 more meaningful. I would still encourage you to provide this.\n\nThe addition of Appendix I does address my concern that no samples of generated programs were provided; however, I would still encourage you to provide some discussion of the qualitative aspects of these generated programs. There is no such discussion at present.\n\nScore-wise, your response has indeed increased my view of the paper (though not to the level of a \"10\", which would be required for me to adjust my score in OpenReview)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668801162,
                "cdate": 1700668801162,
                "tmdate": 1700668801162,
                "mdate": 1700668801162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V4kCowY19U",
                "forum": "sY5N0zY5Od",
                "replyto": "uzjRMISkL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful comments on our response. We are glad that you found our additions to the text valuable. We also appreciate that you considered raising the score (but couldn\u2019t, because the system only accepts 10/10 as the next higher score).\n\n\n> some measure of cost in order to make the comparisons across programs and compilers in Tables 1 and 2 more meaningful\n\nWe agree that readers will benefit from a comparison of the costs associated with different programs. In principle, measuring the compilation costs requires us to re-compile our programs (as we did for Appendix A, for one program) while disabling our caches. Considering that this is a significant experimental endeavor, if our work is accepted, we will include compiling and inference costs in the camera-ready version. This can potentially serve as a new column in the results tables or an expansion of Appendix A.\n\nWe hope that the addition of Appendix A conveys the observation that the cost of compiling (which is a one-time, offline step) is fairly low. (As a ballpark, compiling is generally faster than typical finetuning jobs for instance.) Because of this, we typically recommend that DSPy users focus primarily on the quality and cost of the resulting programs, since compiling is an offline operation with manageable costs.\n\nNonetheless, to provide a preliminary comparison of inference costs, we took four rows of increased complexity for HotPotQA (Table 2) and tested them with gpt-3.5-1106, while disabling the cache (which would otherwise skip any repeated computations, e.g. retrieval queries or LM calls).\n\nWe ran 100 questions from HotPotQA with a single thread, and we report the average latency below. Each numbered bullet below represents a program (with compiler strategy in parentheses).\n\n**1. Vanilla (fewshot)**\n\nAverage Latency: 0.3 seconds per question\n\nAverage Cost: $0.0005 per question (1x)\n\n**2. CoT_RAG (fewshot)**\n\nAverage Latency: 1.1 seconds per question\n\nAverage Cost: $0.0013 per question (2.6x)\n\n**3. Multihop (fewshot)**\n\nAverage Latency: 2.6 seconds per question\n\nAverage Cost: $0.0018 per question (3.6x)\n\n**4. Multihop (bootstrap)**\n\nAverage Latency: 2.6 seconds per question\n\nAverage Cost: $0.0041 per question (8.2x)\n\nAs this shows, these programs are within an order of magnitude of the cost and latency of the simplest one, even though we see major quality improvements from vanilla to multihop. We hope this provides deeper insight into our evaluations, and will expand on this analysis if our work is accepted.\n\n\n> I would still encourage you to provide some discussion of the qualitative aspects of these generated programs.\n\nWe appreciate this suggestion. If the DSPy paper is accepted, we will use the time allocated for the camera ready (due in Feb) to expand Appendix I with a brief discussion of the prompts observed when compiling, and how that aids with debugging and analysis in our view, building off the feedback from (and discussion presented in our responses to) reviewers zPoD and cQiR."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678306190,
                "cdate": 1700678306190,
                "tmdate": 1700678560995,
                "mdate": 1700678560995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q00ZqwA9Bd",
            "forum": "sY5N0zY5Od",
            "replyto": "sY5N0zY5Od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5605/Reviewer_FCUg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5605/Reviewer_FCUg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DSPy, a framework for expressing LM pipelines in a higher level language with support for automatic pipeline optimization. The results demonstrate that 1) complex pipelines can be written quickly and concisely in DSPy and 2) their automatic pipeline optimizations yield substantial gains in performance for two multi-step reasoning tasks (math reasoning and multi-hop QA)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "DSPy is a major improvement over manually composing complex LM pipelines by hand. The paper also demonstrates the possibility of automatically optimizing parts of the pipeline once written in the DSPy framework. Finally, the experimental results are very impressive, particularly given the simplicity of user experience."
                },
                "weaknesses": {
                    "value": "There is a major missing related work [1], which takes a similar approach of expressing LM pipelines as programs, and also comes with built-in optimizations. I would be happy to increase my score if the paper is revised to include a discussion comparing the two approaches.\n\nA more minor concern is that the optimization techniques demonstrated here are relatively limited in scope. From a conceptual standpoint, I would have liked to see more than ensembling or bootstrapping few shot examples.\n\n[1] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2023. Prompting Is Programming: A Query Language for Large Language Models. Proc. ACM Program. Lang. 7, PLDI, Article 186 (June 2023), 24 pages. https://doi.org/10.1145/3591300\n\n====\n\nThe rebuttal has addressed my concerns and I have increased my score from a 6 to 8."
                },
                "questions": {
                    "value": "Do the authors have any ideas for future optimization strategies (or \"teleprompters\") that could be implemented within this framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5605/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5605/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5605/Reviewer_FCUg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807830527,
            "cdate": 1698807830527,
            "tmdate": 1700670096553,
            "mdate": 1700670096553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G2lGQharHT",
                "forum": "sY5N0zY5Od",
                "replyto": "Q00ZqwA9Bd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the helpful feedback. We are thrilled that you found our results 'very impressive' and that you appreciate the 'simplicity of the user experience' of the DSPy framework.\n\n\n> missing related work [1], which takes a similar approach of expressing LM pipelines as programs [...]  I would be happy to increase my score if the paper is revised to include a discussion comparing the two approaches.\n\nThank you for pointing out LMQL. We have revised the related work section to compare LMQL and DSPy. We highlight this new part in blue. In short, LMQL is query language for efficiently constraining the decoding algorithms of LMs to generate only strings that fulfill logical constrains (e.g., lists of bullets or values formatted correctly for a calculator). LMQL\u2019s lower-level interface for controlled decoding can be beneficial to implement specific advanced modules within DSPy.\n\nGiven this, we believe that LMQL has fundamentally distinct design goals from DSPy. DSPy focuses on optimizing the quality of a pipeline of LM calls toward a given metric (e.g., optimizing the prompts of a multi-hop program to achieve high answer exact match). In contrast, LMQL helps make sure LM outputs adhere to specific *formatting* characteristics. The DSPy optimizations have an arbitrary continuous metric in mind, whereas LMQL optimizations focus on the efficiency of decoding under logical constraints.\n\n> A more minor concern is that the optimization techniques demonstrated here are relatively limited [...] Do the authors have any ideas for future optimization strategies (or \"teleprompters\") that could be implemented within this framework?\n\nAbsolutely, we have a wide range of ideas for future optimization strategies. Our focus in this work is to show that the DSPy formulation of the problem (i.e., LM programs with optimizable modules, based on natural-language signatures) creates a very large space of potential optimizations. We then focus on showing that simple strategies for creating demonstrations (for prompting or finetuning), selecting between them, and ensembling them yield very large quality gains.\n\nSince we wrote this paper, we have also explored optimizing the instructions directly in a range of different ways, as well as strategies for automatic backtracking logic. We believe these extensions fall outside the scope of this paper, and we leave scientifically evaluating them for future work.\n\n---\n\nLastly, we appreciate your offer to raise the score given a discussion of LMQL vs DSPy. We believe this discussion of LMQL has helped us shed more light on the unique aspects of DSPy in the updated paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617323445,
                "cdate": 1700617323445,
                "tmdate": 1700618299513,
                "mdate": 1700618299513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "39KJzsbyqE",
                "forum": "sY5N0zY5Od",
                "replyto": "G2lGQharHT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5605/Reviewer_FCUg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5605/Reviewer_FCUg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I do think the paper would be slightly stronger if there were some section covering future work, rather than leaving the potential of the framework up to the imagination of the reader. \n\nAs my main concern regarding a comparison with LMQL is addressed, I have increased my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670029022,
                "cdate": 1700670029022,
                "tmdate": 1700670029022,
                "mdate": 1700670029022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]