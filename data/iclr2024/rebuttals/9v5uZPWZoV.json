[
    {
        "title": "Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators"
    },
    {
        "review": {
            "id": "huc1YzwQJD",
            "forum": "9v5uZPWZoV",
            "replyto": "9v5uZPWZoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_3cd6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_3cd6"
            ],
            "content": {
                "summary": {
                    "value": "The proposes to use text-to-image generative models like Stable Diffusion for interventional data augmentation to simulate interventions over the environmental factors that are likely to change across domains. The authors argue that this interventional data augmentation would improve the generalization behavior of models over out-of-distribution data and reduce the reliance on spurious features during training. The two metrics that the authors measure are Single-Domain Generalization (SDG), which tests the generalization behavior to a new domain for instance natural image to sketch when trained only on the natural image domain, and Reducing Reliance on Spurious Features (RRSF) which measures the reliance on spurious features for model training such as relying on background to classify foreground.\n\nFor SDG, the authors use SDEdit on top of text-to-image models to generate source images in the target domain and use these generated images also for training. Interestingly the authors also show that instead of generating images in the specific target domain, similar performance can also be achieved if images are generated in a different set of target domains.\n\nFor RRSF again, the authors explicitly design prompts that try to reduce the effect of specific spurious correlations. For instance, generate images in various backgrounds to reduce the bias towards the background.\n\nAcross different datasets the authors show improved performance compared to various baseline approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I like the idea of using SDEdit on top of text-to-image generative models to remove the model's biasness to spurious features and also generalizing to new domains.\n\n2. The ablation study showing that a similar performance can be achieved for SDG even if images are are not generated for a specific target domain is nice and very useful."
                },
                "weaknesses": {
                    "value": "1. In figure 3, it seems that Text2Image variant has the least biases across all three datasets. But the text on page 8 the authors suggest that \"Text2Image seems to be less effective than other techniques in reducing background and texture biases\". Can the authors clarify this?\n\n2. The results in Figure 2 where the Text2Image variant performs better/comparable to SDEdit variant undermine the idea proposed in the paper.  In the Text2Image variant, there is no image conditioning for generating images. Several other papers[1] have also shown that using synthetic data from generative models improves the generalization performance of the classifiers. Where is the novelty then coming for this paper?\n\n3. To further show that the generalization improves for unseen target domains, can the authors also show results for Cifar10-C and ImageNet-C datasets?\n\n[1] Synthetic Data from Diffusion Models Improves ImageNet Classification. Azizi et al. https://arxiv.org/abs/2304.08466"
                },
                "questions": {
                    "value": "I have already mentioned my questions in the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629728118,
            "cdate": 1698629728118,
            "tmdate": 1699636975933,
            "mdate": 1699636975933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tOl9c86a4I",
                "forum": "9v5uZPWZoV",
                "replyto": "huc1YzwQJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer 3cd6 for their detailed summary and feedback. In particular, we appreciate their efforts in finding an important typo in our analysis, which we have now corrected (see Q1). We address their concerns below.\n\n\n**Q1: Figure 3 shows Text2Image is the best at RRSF, but statements on page 8 seem to contradict this.**\n\n\nWe thank the reviewer for pointing out the typo. We fixed it in the revised draft (in red). Text2Image is indeed the most effective approach.\n\n\n**Q2.1: The effectiveness of Text2Image \u201cundermine[s] the idea proposed in the paper\u201d**\n\n\nPlease refer to **General Response (B)**.\n\n\n**Q2.2: \u201cWhere is the novelty\u201d relative to [1]?**\n\n\nThe goal of [1] is to show: \n*  it is possible to fine-tune stable diffusion to produce a class-conditional generative model whose synthetic samples mimic the training distribution (goal: reduce gap between synthetic and real)\n* they test the performance on i.i.d samples with respect to the training set\n\nOur goal is to:\n* provide an extensive analysis of how to best use zero-shot generative techniques to simulate non-i.i.d. test domains (with no form of fine-tuning!)\n* we test the performance on non-i.i.d. test domains, showing zero-shot generated samples can be used to reduce bias and improve generalisation across a wide variety of domains\n* draw conclusions about how to most effectively exploit zero-shot generative capabilities of SD\nThus, while [1] is an interesting related work (which is why we cite it in both Sections 1 and 2), the authors\u2019 purpose, experimental setting, and results are all distinct from ours. For additional context, please consult our list of contributions in **General Response (C).**\n\n\n\n\n**Q3: \u201cTo further show that the generalization improves for unseen target domains, can the authors also show results for Cifar10-C and ImageNet-C datasets?\u201d**\n\n\nWe would like to point out that CIFAR-10-C and ImageNet-C are synthetic datasets obtained by applying some well-known parametric and handcrafted transformations to the images. Several of these transformations try to approximate the observation of different weather conditions or lighting conditions. We have experimented on 7 benchmarks, among which we have  two large scale dataset (NICO++ and DomainNet), which represent some of these transformations and are made of naturalistic data \u2013 real photos captured in, e.g., dim light, autumn weather, etc. \n\n\nHowever, as requested, we have also performed a comparable SDEdit experiment (using handcrafted prompts) on CIFAR-10-C:\n\n\n**Cifar-10-C Result with ResNet-18**\n| | Blurring Avg. | Noise Avg. | Compression Avg. | Transformations Avg. | Overall Avg. |\n|-----------|---------------|------------|------------------|---------------------|--------------|\n| ERM       | 72.04        | 74.01    | 75.71            | 73.93              | 73.55       |\n| MixUp  | 73.79 | 76.22 | 77.46 | 75.72 | 75.48 |\n| PixMix | 75.84 | 79.41 | 81.39 | 79.32 | 78.63 |\n| SDEdit      | 74.28        | 75.71    | 77.10            | 75.02              | 75.11       |\n\n\nIt is important to note that Stable Diffusion (SD) is trained on web-scraped data, and as such we do not expect it to perform these kinds of artificial transformations as well as the naturalistic ones found in, e.g., NICO++. All the same, it appears that SD is still able to synthesize augmented data that can improve performance even on these unseen transformations. However, we note that, as these parametric transformations are artificially produced and mathematically well-understood (with implementations available in open-source libraries), it is obviously more efficient to target them with parametric augmentations at training time than with SD."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264661816,
                "cdate": 1700264661816,
                "tmdate": 1700270680780,
                "mdate": 1700270680780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fsh2uKqTbZ",
                "forum": "9v5uZPWZoV",
                "replyto": "tOl9c86a4I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_3cd6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_3cd6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "While I thank the authors for conducting experients over ImageNet-C and Cifar10-C datasets, I think the fact that the \"Text2Image\" baseline performs better/comparable to SDEdit variant undermines the novelty of the paper. While the authors mention in their rebuttal, that the experimental setting and results are different from [1], I feel the main idea is still the same. Thus I am unwilling to increase my ratings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628782786,
                "cdate": 1700628782786,
                "tmdate": 1700628782786,
                "mdate": 1700628782786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GoB6RtKxH3",
                "forum": "9v5uZPWZoV",
                "replyto": "huc1YzwQJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response, an important clarification"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer taking time to respond to our rebuttal.  \n\n**P1**: **the fact that the \"Text2Image\" baseline performs better/comparable to SDEdit variant undermines the novelty of the paper**\n\nWe respectfully disagree, because our novelty is *not* to establish SDEdit as the best performing method, it is to study a range of image synthesis and editing techniques (including both SDEdit and Text2Image) in the context of SDG and RRSF. We make a more detailed argument on this point in **General Response (B)** (Conditioning and Interventions); but in sum: it is a contribution, not a contradiction, that our investigation explores both ways to simulate interventions and finds Text2Image to often outperform SDEdit.\n\n**P2.1: [with respect to] [1], I feel the main idea is still the same**\n\nThe main similarity between [1] and our work is the use of foundational generative models to generate synthetic data to train classifiers. (We do not claim novelty on this point, and in Section 2, we provide a detailed discussion of additional works that are also similar to our work in this respect.) Below, we note several key differences between our work and [1]. \n\nIn [1], the authors show:\n- Assuming **access to all images of ImageNet**, Imagen can produce the best **approximation of ImageNet if fine-tuned** on it (according to FID, IS and CAS scores)\n- Adding the so-produced approximation of ImageNet to the ImageNet training set improves classification performance **on ImageNet itself**. \n- Although the authors talk of performing augmentations in the abstract, the generated data is explicitly generated to resemble ImageNet. **No deliberate manipulation of the image features is performed. The test set is i.i.d. with respect to the dataset**.\n\nIn our work: \n- Assuming **no access to non-i.i.d. test domains training data**, Stable Diffusion can approximate them **zero-shot** if some **meta-data** is accessible. No image is accessed from the non-i.i.d. test domains. This is a significantly more difficult problem that **cannot be solved with more available i.i.d. training data (like [1] does)**. If, similarly to [1], we assumed full access to the test domain (non-i.i.d., in our case), the problem would not even require a generative model to be solved with perfect accuracy on PACS/Office-Home. \n- Adding the zero-shot generated non-i.i.d. data to i.i.d. data, we improve the performance **on non-i.i.d. test data** (not accessible for any form of fine-tuning) and reduce the impact of **biases** on model's predictions.\n- Our experiments in the main paper and rebuttal (Table 2 in the paper,  Q2 response to reviewer MWyF) show **exact knowledge of the meta-data is not required**. \n- We ablate and analyse which conditioning, prompting and filtering techniques produce the best performance, and compare with a retrieval baseline pointing out weaknesses and strengths of all approaches in different settings, as already extensively discussed in **General Response (C)** (Novelty and Contribution).\n\n**P2.2: the experimental setting and results are different to [1], I feel the main idea is still the same\"** \n\nThe different experimental setting is not a minor difference. The availability of i.i.d. training data and testing only on i.i.d. training data vs. the non-availability of non-i.i.d. data and testing on non-i.i.d. test data is the fundamental difference that distinguishes in-domain generalization (which [1] addresses) and out-of-domain generalization and reducing the reliance on spurious features (which we address). The two fields are fundamentally different areas of research: solutions and conclusions that hold in one do not necessarily generalize to the others. Indeed, *under the assumptions of our setting, [1] would not be applicable (as there would be no non-i.i.d. data to fine-tune on).*\n\nHaving carefully read [1], it is unclear why the novelty of the results of our work are diminished by paper [1], as they operate in very different settings with different methodologies, goals, and contributions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644236947,
                "cdate": 1700644236947,
                "tmdate": 1700673644911,
                "mdate": 1700673644911,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dAbIuDzmn8",
            "forum": "9v5uZPWZoV",
            "replyto": "9v5uZPWZoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_nrcC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_nrcC"
            ],
            "content": {
                "summary": {
                    "value": "This article carried out the first investigation of T2I generators as general-purpose interventionaldata augmentation mechanisms, showing their potential across diverse target domains and potential downstream applications.\n\u2022 Authors perform extensive analyses over key dimensions of T2I generation, finding the conditioning mechanism to be the most important factor in IDA.\n\u2022 Authors show that interventional prompts are also important to IDA performance; but in in contrast with previous works, we find that post-hoc filtering is not consistently beneficial.\n\nGenerally, this article describe why text to image generator from stable diffusion outperforms others methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This article carried out the first investigation of T2I generators as general-purpose interventionaldata augmentation mechanisms, showing their potential across diverse target domains and potential downstream applications.\n\u2022 Authors perform extensive analyses over key dimensions of T2I generation, finding the conditioning mechanism to be the most important factor in IDA.\n\u2022 Authors show that interventional prompts are also important to IDA performance; but in in contrast with previous works, we find that post-hoc filtering is not consistently beneficial.\n\nThis work generally makes efforts on data shift problem. A good mind in solving data augmentation problem."
                },
                "weaknesses": {
                    "value": "Not very soundness from technical side. No novelty in the model is presented."
                },
                "questions": {
                    "value": "Have you ever compared results with some other generative methods like GAN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641880546,
            "cdate": 1698641880546,
            "tmdate": 1699636975795,
            "mdate": 1699636975795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X5lytde7UB",
                "forum": "9v5uZPWZoV",
                "replyto": "dAbIuDzmn8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer nrcC for acknowledging the clear strengths and contribution of our work. We respond to their remaining concerns below.\n\n\n**Q1.1: \u201cNot very soundness from technical side.\u201d**\n\n\nOur experiments use several well-known and widely-utilized methodologies (e.g., Stable Diffusion-based image generation techniques including Text2Image, SDEdit, ControlNet, etc.). We are confident in the soundness of our implementation, which we have anonymized and made available at https://tinyurl.com/49sb675n. We are happy to engage in further discussion or experimentation to address any specific technical concerns the reviewer might have.\n\n\n**Q1.2: \u201cNo novelty in the model is presented.\u201d**\n\n\nOur goal in this work is not to develop a novel methodology or model for performing interventional data augmentation (IDA). Rather, it is to:\n1. Analyse the extent to which generative diffusion models, when used in a zero-shot setting, can serve as a general-purpose IDA mechanism. \n2. For each application case (SDG, RRSF), discern the factors that contribute to their superior or inferior effectiveness (conditioning, filtering, prompting). \nNo previous work has conducted the necessary study to address these questions, and our extensive findings, as described in **General Response (C)**, are indeed novel.\n\n\n**Q2: Have you compared with GANs?**\n\n\nWe thank the reviewer for this suggestion. In response, we carry out our SDG PACS experiment using VQGAN-CLIP [1] as a comparison and include the result in the selected table below and **Table 3**. We observe it underperforms with respect to Stable Diffusion. While this should not discourage future work from considering newly developed GANs, this reflects the superior generative capabilities of diffusion models established in the current literature [2].\n\n\n**SDG PACS result with ResNet-18 for VQGAN-CLIP comparison**\n\n\n|              | Art    | Photo  | Sketch | Cartoon | Average |\n|--------------|--------|--------|--------|---------|---------|\n| ERM          | 74.8   | 39.67  | 48.12  | 72.37   | 58.74   |\n| VQGAN-CLIP(M)| 78.09  | 54.38  | 53.78  | 77.76   | 66.00   |\n| SDEdit(M)    | 82.27  | 58.87  | 72.76  | 81.93   | 73.96   |\n\n\n\n\n[1] Crowson, Katherine, et al. \"Vqgan-clip: Open domain image generation and editing with natural language guidance.\" *European Conference on Computer Vision.* Cham: Springer Nature Switzerland, 2022.\n\n[2] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.* 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242350313,
                "cdate": 1700242350313,
                "tmdate": 1700242350313,
                "mdate": 1700242350313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K4jFSOGS0r",
            "forum": "9v5uZPWZoV",
            "replyto": "9v5uZPWZoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_MWyF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_MWyF"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses text-to-image generators and editing techniques to generate training data. Experiments were performed for domain generalization benchmarks with supportive results. There were extensive ablations and analysis over types of prompts, conditioning mechanisms, post-hoc filtering and editing techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper has extensive experiments and ablations.\n- The analysis comparing different editing methods is insightful."
                },
                "weaknesses": {
                    "value": "1. Generalizability of the method\n    - Almost all the results seem to assume that the target domain can be easily described and that the number of domains are known. However, this does not always hold. E.g., in iwildcam, where the target domain consists of images from different camera traps resulting in different locations, viewpoints, etc., it may not be obvious how to describe the target domain. \n    - Furthermore, the proposed method on \u201cBreaking spurious correlations\u201d (Fig 3) requires a human to hand craft prompts which may be expensive to attain.\n    - E.g., [1] uses a captioning model to describe the data, then gpt to summarize into domain descriptions. These descriptions are then used in the prompts. Thus, it doesnt require knowledge of the domains.\n2. \"Describing the Target Domain Is Not Necessary\u201d. Table 2.\n    - It is not clear to me what the message of table 2 is. As it is without target domain information, it seems say that SD is biased towards generating certain domains and those domains happen to be aligned with the target for this dataset, but this may not be the case for other datasets.\n3. From A.1, it seems like there was different number of additional data for generated images and baseline augmentation techniques. Can the authors explain this choice? It may be interesting to see how the performance changes with amount of data similar to (He et al., 2023;  Sariyildiz et al., 2023).\n4. The conclusions of the paper seems similar to that of (Bansal & Grover, 2023) who also used pre-defined text prompts to generate data. They also showed that a combination of real and generated data results in better performance, although on IN-Sketch and IN-R. The evaluation setup may be slightly different but the conclusions from SDG seems to be similar. \n\n\n\n[1] Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation. NeurIPS\u201923"
                },
                "questions": {
                    "value": "Other than the questions raised above:\n- What is the performance of Retrieval on DomainNet in Fig 5?\n- I would suggest moving some technical details, e.g. the setup, how many images are generated for each original image, a brief description of how is retrieval done, to the main paper.\n- It may be useful to have an additional column in the table of results for the runtimes. The baseline augmentations should be much cheaper to attain than generating data with SD."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716143750,
            "cdate": 1698716143750,
            "tmdate": 1699636975692,
            "mdate": 1699636975692,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SKExenNqHP",
                "forum": "9v5uZPWZoV",
                "replyto": "K4jFSOGS0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer MWyF for their detailed feedback. The potential weaknesses they outline are important to address, and we believe that our responses below are sufficient to address each concern.\n\n\n**Q1.1a: Assumption that \u201cthe target domains can be easily described.\u201d**\n\n\nWe agree that, in principle, not all domain shifts can be easily described via language; however our work clearly targets those that can. These represent a wide variety of problems of interest (we use 7 of the most popular benchmarks in the domain-shift literature). Indeed, reference [1] suggested by the reviewer provides further evidence that even when they are not easily describable, language-driven generation can achieve impressive performance on 3 additional benchmarks. In contrast, none of the non-textual baselines are able to outperform ERM on more than a small subset of the 7 benchmarks considered by our work, clearly indicating they are less general.\n\n\n**Q1.1b: Assumption that \u201cthe number of domains [is] known.\u201d**\n\n\nWhile it is true that our primary SGD experiments assume that the number of domains is known, we have several key experiments (experiments in Table 2, the additional NICO++ we include in response to Q2 for SGD, and all RRSF experiments) that do not.\n\n\n**Q1.2: Human handcrafted prompts required? Expensive to attain?**\n\n\nFor SDG, we experiment extensively with two prompting strategies that do not require human input: language enhancement (LE) and minimal (M). The same is very likely possible for RRSF, but for this task, we elected to focus on ablating the conditioning mechanism instead of the prompting strategy, as our results from SDG showed this to be more important. Regarding expense, we note that the handcrafted prompts took only a single participant 2-5 minutes per benchmark.  (across all 3 RRSF benchmarks).\n\n\n**Q1.3: [1] does not \u201crequire knowledge of the domains.\u201d**\n\n\nWe thank the reviewer for mentioning [1]. However, we kindly note that : [1] assumes access to images of several domains to create the captions. Indeed, this is much more information than we access for, e.g., Minimal (M) prompts, which are produced with meta-data only. However, this requires explicit access to the target domain images, which is unavailable in our domain generalization setting. We will discuss the work in the revised draft. \n\n\n[1] Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation. NeurIPS\u201923\n\n\n**Q2: Table 2 seems to say that SD is biased to generate some domains that happen to be aligned with the target for this dataset, but may not be the case for other datasets.**\n\n\nWe perform the experiment of Table 2 for NICO++, which has domains that are significantly not aligned (see the column names in the Table below). This clearly indicates the intervention performed aids generalization per-se as the improvement is substantially maintained when the target domain is not included.\n\n\n**SDG NICO++ Result with ResNet-50 for comparing the effect of accessing synthetic target domain or not**\n\n\n|              | autumn | dim    | grass  | outdoor| rock   | water  | Average |\n|--------------|--------|--------|--------|--------|--------|--------|---------|\n| ERM          | 66.74  | 70.37  | 72.05  | 71.3   | 66.58  | 72.64  | 69.95   |\n| SDEdit(M) \u00d7  | 67.90  | 71.70  | 72.61  | 72.32  | 67.10  | 73.79  | 70.90   |\n| SDEdit(M) \u2713  | 68.28  | 71.42  | 72.68  | 72.31  | 67.95  | 74.07  | 71.12   |\n\n\n**Q3.1: Why \u201cdifferent number of additional data for generated images and baseline[s]\u201d?**\n\n\nThe difference is caused by the current cost of generating samples with Stable Diffusion (SD). Since the process is expensive, we perform it offline before training:\n* the augmentation baselines produce $D*E$ augmentations where $E= 50$ (the number of epochs), and $D$ is the size of the training set.\n* the SD-based approaches generate $D*(C-1)$ samples, where $C$ is the number of domains. $C=4$ or $C=6$ across the SDG experiments.\n* Since $(C-1)$ is much lower than 50, SD produces much less augmentations\nThe superior performance of using fewer SD-based augmentations indicates interventions targeting the right environmental variables are significantly more effective than generic   augmentations whose contribution to generalization is unclear. \n\n\n**Q3.2: How much does performance change with amounts of data similar to cited works?**\n\n\nIn **Appendix I.3** we already report this result for PACS. The conclusion is that a few additional data samples can help, but too much data can degrade the performance. The fact that few additional samples are enough to outperform all SOTA baselines indicates the slow generation is counterbalanced by the effectiveness of using just a few generated samples."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241749249,
                "cdate": 1700241749249,
                "tmdate": 1700241749249,
                "mdate": 1700241749249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xHdeQcvzmZ",
                "forum": "9v5uZPWZoV",
                "replyto": "K4jFSOGS0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_MWyF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_MWyF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications and additional experiments!\n\nAbout Table 2, my point was more general, if the target domain were from another dataset or with substantial shifts, it is unlikely that the transformed training data would help. Thus, it seems to be with some assumptions that \"Describing the Target Domain Is Not Necessary\". \n\nI have read the rebuttal and I appreciate the discussions. However, I am inclined to keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646865916,
                "cdate": 1700646865916,
                "tmdate": 1700646913748,
                "mdate": 1700646913748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Swk3QuqZlz",
            "forum": "9v5uZPWZoV",
            "replyto": "9v5uZPWZoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
            ],
            "content": {
                "summary": {
                    "value": "Utility of Text-to-Image generators for interventional data augmentation (IDA) toward improving single domain generalization (SDG) and reducing reliance on spurious features (RRSF) is the subject of this work.\nPrevious works studied using generators for generating training data; the contribution of this work is a deeper study of the same for SDG and RRSF tasks.\n\nThe work is a thorough study with many tasks and ablation. Although I believe the paper do not have any surprising finding and ranks low on novelty, it could be of interest to the research community.\nHowever, I have many questions regarding their setup, which muddled their contributions quite a bit. My assessment therefore is a placeholder at the moment, and would likely change."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Thorough study with four SDG and three RRSF tasks. Comprehensive evaluation with previous augmentation procedures and various image generators.\n- Writing and presentation of results is easy to follow. I enjoyed comparisons made using simple baselines."
                },
                "weaknesses": {
                    "value": "- Novelty of the paper is somewhat limited.\n  \nPlease see questions."
                },
                "questions": {
                    "value": "**Premise compromised?** The paper started with the premise that IDA is known to be useful for SDG and RRSF, and proceeded with two-fold objective of evaluating T2I generators and establishing a new state-of-art on SDG and RRSF.\nHowever, as observed from Fig. 3 and 5, text2Image and retrieval baselines performed the best, which are both non-interventional augmentations. What then is the role of IDA and conditional generators?\nClearly stating the contributions can help. Is the paper suggesting to only evaluate unconditioned image generators using the task? \n\n**Table 2** results are very interesting. Few questions. \n1. The performance on the sketch domain is better without simulated target domain, why is that?    \n2. For comparison, could you please include the baselines: (a) ERM trained on all but target domain, (b) ERM trained on all the domains, (c) ERM trained only on the target domain.    \n3. It is intriguing that the performance is comparable even without simulated target domain for SDEdit, but none of the other target-agnostic augmentation are even close, why is that?   \n4. I suspect if there are any implementation differences between SDEdit and others (MixUp, CutMix etc.) causing the massive improvement (a common and annoying problem with PACS and other datasets), releasing your implementation can help. Also, can you add to the table (2) the performance of SDEdit with even more irrelevant prompts? How about if we use the prompts from OfficeHome on PACS dataset? How does the performance compare then? \n\n**More information on prompts.** Could you please provide more information on the prompts used for generatring images on the three RRSF tasks? They are more nontrivial than for SDG, and yet their description is rushed in the main paper.\nOverall, how much effort was spent on engineering the prompts and how were they tuned?\n\n**Conclusion and contributions**. I am somewhat lost on the takeaways. Please spell them out. As I see it, conditioning of generators (since text2Image and retrieval work just as well) is not so important but the conclusion says otherwise.\nWhat are the implications for evaluation of generators and SDG/RRSF research?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7944/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7944/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766928457,
            "cdate": 1698766928457,
            "tmdate": 1700674900853,
            "mdate": 1700674900853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UBP0iUURgL",
                "forum": "9v5uZPWZoV",
                "replyto": "Swk3QuqZlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer bsQg for their comprehensive feedback, openness to improving their rating, and for clearly articulating their questions in detail. We believe the feedback helps us to further clarify the purpose of our work, and hope our response will stimulate an engaging and fruitful discussion.  \n\n\n**Q1.1: \u201cPremise Compromised?\u201d Core premises and goals are unclear.**\n\n\nPlease refer to **General Response (A).**\n\n\n**Q1.2 Is Text2Image interventional? Is it conditional?**\n\n\nPlease refer to **General Response (B).**\n\n\n**Q1.3: Clearly state the contributions.**\n\n\nPlease refer to **General Response (C).**\n\n\n**Q2.1 Why is sketch-domain performance \u201cbetter without simulated target domain\u201d?**\n\n\nThe distribution of sketches produced by Stable Diffusion is substantially different from those represented in PACS (see **Appendix I.5, Figure 16** of our updated draft). Since tuning the prompts to align the Stable Diffusion and PACS distributions is out of the scope of our analysis (see our response to Q3 below), this behavior is not unexpected.\n\n\n**Q2.2 \u201c[I]nclude the baselines: (a) ERM trained on all but target domain, (b) ERM trained on all the domains, (c) ERM trained only on the target domain.\u201d**\n\n\nWe perform the requested experiments and report the results in the Table below (and in our updated manuscript, **Table 14**). Before consulting the results, please note the following:\n1. Training on the target domains (b,c) is nonstandard in the context of SDG, as they are i.i.d. to the test sets. Thus, (b,c) do not constitute a baseline but an empirical \u201cupper bound\u201d on the best possible interventional augmentation strategy (i.e., where the generator can directly sample from a perfect approximation of the target distribution). \n2. Experiment (a) constitutes a typical Multi-Domain Generalization (MDG) setting that is not comparable to the SDG we focus on, as the reported metrics for the two kinds of experiments are different:\n   * In MDG, ERM is trained on all but the target domain and the performance on the unseen target domain is reported (column name is unseen target domain)\n   * In SDG, the average of the performance across 3 unseen target domains is reported for each known source domain (column name is the only known training domain).\nTherefore the MDG results cannot be compared with the ones in Table 2 of the paper, so for a fair comparison with (a), we use SDEdit in the context of MDG (i.e., SDEdit(MDG)) using minimal prompts.\n\n\nOur results are as follows:\n* Unsurprisingly, (b,c) achieve near-perfect performance, confirming that the train and test datasets are nearly perfectly i.i.d.\n* The performance improvement of SDEdit (MDG) with respect to (a) indicates the effectiveness of IDA also extends to the case where multiple training domains are available.\n\n\n|    | Art    | Photo  | Sketch | Cartoon | Average |\n|---------------|--------|--------|--------|---------|---------|\n| (b)  | 99.65  | 99.94  | 99.64  | 99.66   | 99.72   |\n| (c)    | 99.71  | 99.70  | 99.84  | 99.60   | 99.74   |\n| (a)    | 80.01  | 96.28  | 73.86  | 76.28   | 81.61   |\n| SDEdit (MDG)    | 87.5   | 95.75  | 79.21  | 85.2    | 86.91   |\n\n**Q2.3: Why is SDEdit so much better than target-agnostic augmentation[s]?**\n\nOur results indicate that the target-agnostic augmentations are:\n* Not as effective as commonly believed: most of them have been tested on benchmarks crafted from CIFAR and ImageNet but be less effective in other cases. On these datasets, a few of the methods we consider have already been shown not to be significantly more effective than ERM in the DomainBed benchmark (https://tinyurl.com/yeyjzvd9)  \n* Not as \u201ctarget-agnostic\u201d as commonly believed: our RRSF experiments clearly show they explicitly target specific types of invariances while neglecting others.\nOn the other hand, SDEdit can manipulate shape, texture and other factors in more sophisticated ways, as captured by the 4 SDG and 3 RRSF benchmarks we experiment with.\n \n**Q2.4.1 Concerns about implementation.**\n\nWe have integrated all baselines in the same codebase and ensured no method has an unfair edge over the others, using the same evaluation code across all methods. We have anonymized our codebase and made it available for review at https://tinyurl.com/49sb675n, and will release the de-anonymized codebase upon acceptance.\n\n**Q2.4.2 Can you experiment with using OfficeHome prompts on PACS?**\n\nWe appreciate the interesting suggestion for this experiment. We added the prompts for Officehome to the ones of PACS to augment the PACS dataset. We include the results in the table below (and in **Table 4** of the revised draft) indicating this method as SDEdit(PO-M). Note that adding these \u201cirrelevant\u201d prompts further improves performance. \n\n|            | Art   | Photo | Sketch | Cartoon | Average |\n|------------|-------|-------|--------|---------|---------|\n| SDEdit(PO-M)| 82.41 | 65.43 | 80.3   | 85.29   | 78.36   |\n| SDEdit(M)   | 82.67 | 62.94 | 73.78  | 86.33   | 76.43   |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241560005,
                "cdate": 1700241560005,
                "tmdate": 1700241560005,
                "mdate": 1700241560005,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZqOTZNe7q",
                "forum": "9v5uZPWZoV",
                "replyto": "Swk3QuqZlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: \u201c[P]lease provide more information on the prompts used for generating images on the three RRSF tasks [...] How much effort was spent on engineering them? How were they tuned?\u201d**\n\nWe asked a human participant to write up a set of prompts that would manipulate a specific feature of an image. We provided them only with information from metadata contained in the respective benchmark papers and the following high-level instructions:\n* For the background-bias prompts, we asked them to specify common image backgrounds randomly. \n* For the texture-bias prompts, we asked them to reason about what kind of transformations would change the texture of an object. Later, we asked the same participant to add a few additional prompts following the same instructions (see **Appendix F.1**).\n* For the demographic bias, using the meta-data of the task was enough to create \u201cMinimal\u201d prompts (i.e., considering all possible combinations of gender/color), so no participant was required.\n\nThe prompts were not engineered nor tuned on any metric: our goal was to evaluate how well these systems perform \u201coff-the-shelf\u201d (i.e., where no time was spent engineering or tuning them). This illustrates the usefulness, flexibility, and simplicity of using off-the-shelf T2I generators for RRSF. \n\n\n**Q4: \u201cPlease spell out [the takeaways \u2026] What are the implications for evaluation of generators and SDG/RRSF research?\u201d**\n\n\nPlease refer to our answers to **General Response (B)** (for context on our use of \u201cconditioning\u201d and \u201cIDA\u201d) and **General Response (C)** (for our response to this question)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241645113,
                "cdate": 1700241645113,
                "tmdate": 1700270754527,
                "mdate": 1700270754527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tXe3tWuj5z",
                "forum": "9v5uZPWZoV",
                "replyto": "vcY29G0JM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and patience. I am glad to find that the prompts used for RRSF task are not engineered. Also my implementation related concerns are answered well. Thanks for reporting teh performance of the requested baselines.\n\nFew concerns still remain.  \n\nThe abstract has the following sentence \"A general interventional data augmentation (IDA) mechanism that simulates arbitrary interventions over spurious variables has often been conjectured as a theoretical solution to this problem and approximated to varying degrees of success\". This statement is in contradiction to the 2nd contribution from the general response \"Text2Image (conditioning only on text) and Retrieval generally outperform SDEdit (conditioning on both image and text), indicating that augmenting the original image is not necessary.\"\n\nThe contribution lies in evaluating T2I models for SDG and RRSF. The takeaway from the paper is that we can suppress both the problems by simple methods like Retrieval or Text2Image (which is simple because it is only conditioned on the text). In that regard, the paper establishes a new state-of-the-art performance on many of the standard becnhmarks, which I appreciate. Especially since the new numbers are far greater than the previous best. But it seems like the improvement is coming from diverse datasets in the case of SDG (which is why target agnostic, i.e. SDEdit (M) $\\times$ from Table 2 or using irrelevat OfficeHome prompts on PACs performed well). In other words, SDG with the datasets considered are not challenging enough because all the datasets are responding to diversified but label or task relevant data \n\nThe experiments of the paper are sound, but the authors need to tone down unsupported claims made in the paper. SDEdit improvement over other target-agnostic baselines like MixUp is not because SDEdit made effective interventions instead it is likely because SDEdit generated novel images.  Similarly, the point about intervention as a solution for DG must be carefully addressed. Authors must also address the limitations of prompting (for instance, by inluding a failure case of SDG/RRSF) because it is not easy to descibe the the domain shift  that the T2I model also understands. Many of the distribution shift examples from the WILDS dataset are of that kind. \n\nFor the above stated reasons, I cannot recommend an eager accept of the paper. Nevertheless, the paper did a good job of evaluating many simple baselines and in establishing state-of-the-art on many benchmarks. I am changing my score to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674863943,
                "cdate": 1700674863943,
                "tmdate": 1700674863943,
                "mdate": 1700674863943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "btOvqosusX",
                "forum": "9v5uZPWZoV",
                "replyto": "Swk3QuqZlz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer bsQg,\n\nWe sincerely thank you for raising your rating. Thank you for recognizing our **contribution in evaluating T2I models for SDG and RRSF and experiments and acknowledging the experiments are sound**. We have indeed experimented with large-scale and challenging datasets with a wide variety of classes like NICO++ (61K imgs, 60 classes) and DomainNet (410K imgs, 345 classes; one of the most challenging benchmarks in the domain due to its outstanding **complexity and scale**) to an extent that is rarely seen in RRSF and SDG literature. \n\nRegarding the **tension between the following two statements**:\n1. \u201cA general IDA mechanism [\u2026] has often been conjectured as a theoretical solution\u201d (in the abstract)\n2. \u201cText2Image (conditioning only on text) and Retrieval generally outperform SDEdit [\u2026] indicating that augmenting the original image is not necessary.\u201d (in our General Response)\n\nSpecifically, the tension is this: our empirical findings contrast with earlier theoretical literature, in that we generally find that \u201caugmenting the original image is not necessary.\u201d In other words, when we empirically test the strength of the conjectured theoretical solution (e.g., an approach like SDEdit), we find that there are alternatives (e.g., an approach like Text2Image) that perform even better without direct augmentation (i.e., not requiring sample-level alignment, per **General Response (B)**). This is a key contribution of our work: it unearths an empirical limitation of the theoretical literature, pointing to the need for additional theoretical analysis in the context of IDA.\n\n**We will make this key point of tension and the associated empirical contribution clearer in our abstract and introduction.** We will also strive for greater clarity on the remaining points you raised and will remove ambiguous statements. \n\nAbout the **limitations of prompting**, we had already included a failure case in Appendix (on 2 datasets from the WILDS dataset mentioned by the reviewer, please refer to Figure 14) but rest assured we will emphasise this aspect more in the main paper.  \n\nThank you again for your careful analysis of our response, we hope these lines **address the remaining concerns** about the framing of our work. Given the little time before the end of the discussion period, we will not be able to update the draft immediately to reflect these changes, but **all the points discussed above will be modified considering your suggestions in the final draft.**\n\nBest Regards,\n\nAll Authors"
                    },
                    "title": {
                        "value": "Thank you for the response, about the remaining concerns."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680011187,
                "cdate": 1700680011187,
                "tmdate": 1700682157950,
                "mdate": 1700682157950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]