[
    {
        "title": "Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations"
    },
    {
        "review": {
            "id": "uTxUeZTGIT",
            "forum": "HfXDrAzFvG",
            "replyto": "HfXDrAzFvG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7869/Reviewer_pJPd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7869/Reviewer_pJPd"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the method of estimating the Lipschitz constant of a neural network using semidefinite programming (SDP) to the networks with non-slope-restricted activations functions such as GroupSort, Maxmin, and Householder. The SDP formulations are proposed for estimating $l_2$ and $l_\\infty/l_1$ Lipschitz constants for various network architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. well-written, easy to follow even for a non-expert.\n2. The extension of LipSDP to GroupSort, Maxmin, and Householder activations is new."
                },
                "weaknesses": {
                    "value": "The main concern I have is that this paper seems to be an extension of two works Fazlyab'19 and Wang'22 to the case of having sum-preserving activations like GroupSort, Maxmin, and Householder, which seems incremental."
                },
                "questions": {
                    "value": "1. How frequently are GroupSort, Maxmin, and Householder being used in practice? If they are not so popular, why we are studying LipSDP for them?\n2. This is merely a comment. The results would be more interesting and valuable if using GroupSort, Maxmin, and Householder activations have some implicit bias towards having an NN with a smaller Lipschitz constant."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7869/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698605816616,
            "cdate": 1698605816616,
            "tmdate": 1699636964844,
            "mdate": 1699636964844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8qywEp2HY7",
                "forum": "HfXDrAzFvG",
                "replyto": "uTxUeZTGIT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7869/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7869/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pJPd"
                    },
                    "comment": {
                        "value": "Thank you for your evaluation and your feedback. In the following, we address your concerns and your question. Plase do let us know if you have follow-up questions, additional concerns or suggestions.\n\n> The main concern I have is that this paper seems to be an extension of two works Fazlyab'19 and Wang'22 to the case of having sum-preserving activations like GroupSort, Maxmin, and Householder, which seems incremental.\n\nWe think that our contribution in deriving new quadratic constraints for GroupSort, MaxMin, and Householder are novel and significant, complementing the existing works such as Fazlyab'19 and Wang'22. We provide a detailed response in our general response (Part 1). Let us restate some of our key points here. We also included Appendix B to discuss the issue in our revised paper. The underlying idea in the quadratic constraint approach is to abstract \"challenging\", e.g., nonlinear, uncertain or time-varying elements using quadratic constraints. This abstraction allows us to formulate an SDP that can be solved effectively. Since the introduction of the quadratic constraint framework in the 1960s, many papers appeared that are only devoted to deriving new quadratic constraints. For example, Kao'07 derives quadratic constraints for a delay-difference operator to describe varying time delays, Pfifer'15 uses a geometric interpretation to derive new quadratic constraints for delayed nonlinear and parameter-varying systems, and Carrasco'16 summarizes the development of Zames-Falb multipliers based on frequency domain arguments for slope-restricted nonlinearites. In the context of NNs, so far only quadratic constraints for slope-restricted nonlinearities were utilized. By formulating quadratic constraints for GroupSort/Householder activations, we are the first ones to study multivariate activations, to formulate quadratic constraints for NNs that go beyond slope-restriction or Lipschitz continuity and in general the first to present quadratic constraints for sum-preserving elements. Finding novel quadratic constraints is by no means trivial. It requires to study and understand the characteristics of the underlying nonlinearity and then to capture the identified property using a quadratic form. There is no recipe for such a derivation and finding a quadratic constraint does not guarantee that it improves the analysis over other methods. The tighter the description of the nonlinearity by quadratic constraints, the better the analysis in terms of conservatism. The idea we used to derive these quadratic constraints is creative in the sense that it is very different from all the existing quadratic constraint derivations, hence offering unique new insights and complementing the large body of existing arguments in deriving quadratic constraints.  To understand the technical details of our derivation, we refer to the proofs in Appendices C.2 and C.3.\n\n> How frequently are GroupSort, Maxmin, and Householder being used in practice?\n\nPlease see our general response (Part 2). GroupSort, MaxMin, and Householder activations are gradient norm preserving and have been of particular interest when fitting Lipschitz functions and for the design of certifiably robust neural networks (NNs). There are many works (e.g. Leino'21, Singla'21, Trockman'21, Huang'21, Singla'22, Prach'22, Hu'23) that use MaxMin activations and provide state-of-the-art certified robust accuracy (please see our general response, Part 2 for a detailed list of the references). To compute the certified robust accuracy, these works rely on information of Lipschitz bounds and prediction margins. Hence LipSDP for MaxMin networks is of great interests. Another interesting application of MaxMin NNs is in learning-based control (Yion'23), where the information of the Lipschitz constant of a neural network in the loop can be useful to verify safety and robustness. Please note that we included a section on applications of MaxMin NNs as Appendix A.\n\n> The results would be more interesting and valuable if using GroupSort, Maxmin, and Householder activations have some implicit bias towards having an NN with a smaller Lipschitz constant.\n\nThe implicit bias towards smaller Lipschitz constant of neural networks is typically governed by specific neural network structures (e.g. Singla'21, Trockman'21, Prach'22) or regularization terms in the training objective (e.g. Leino'21,Huang'21, Hu'23), but not directly through the choice of the activation function. The popularity of the MaxMin activation (or GroupSort and Householder in general) for certifiably robust networks is mainly due to their gradient norm preserving (GNP) properties, since the GNP properties can help the training of certifiably robust neural networks become more stable."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7869/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256903426,
                "cdate": 1700256903426,
                "tmdate": 1700320750554,
                "mdate": 1700320750554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V0l2lUxM5P",
            "forum": "HfXDrAzFvG",
            "replyto": "HfXDrAzFvG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7869/Reviewer_8tsi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7869/Reviewer_8tsi"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents new semi-definite programs for computing upper-bounds on\nthe Lipschitz constants of deep neural networks with gradient-preserving\nactivations.  The authors derive new quadratic constraints which extend the\nstate-of-the-art LipSDP framework to Lipschitz estimation with the GroupSort,\nMaxMin, and Householder activation functions; these activations were previously\nnot covered by LipSDP since they do not satisfy the slope-restricted property.\nThe authors then extend their approach to compute Lipschitz constants in the\n$\\ell_\\infty$ norm and show how to apply their results to neural networks with\nresidual connections. Experiments confirm the empirical performance of the\nproposed SDPs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is an interesting submission which extends the existing LipSDP framework\nfor estimating Lipschitz constants of neural networks to new activation\nfunctions. The authors use the quadratic constraint approach from control\ntheory to obtain polynomial-time algorithms for the GroupSort and Householder \nactivations (these generalize the MaxMin activation). As only naive estimation\napproaches previously existed for these activations, this contribution is fairly\nstrong and represents the major strength of this paper.\n\nOther notable strengths are the following:\n\n- The proposed SDPs yield upper-bounds for small networks which are close to\n    those obtained by brute-force search over the activation space, particularly\n    for the $\\ell_2$-norm. Moreover, the bounds are much tighter than those\n    obtained using operator norms.\n\n- The methodology is presented clearly and the manuscript is polished."
                },
                "weaknesses": {
                    "value": "The major limitation of this work is the restriction to Householder and\nGroupSort activations. The utility of extending the LipSDP framework to these\nactivation depends directly on the how interesting the problem of Lipschitz\nestimation is for neural networks using these architectures. While the authors\nstate that such activations are becoming popular for the design of Lipschitz\nneural networks, no concrete examples are provided. I am also concerned about\nthe following:\n\n- The basic idea of LipSDP was developed by Fazlyab et al. (2019) while the \n    extension to estimation in the $\\ell_\\infty$ norm is from Wang et al. (2022).\n    The main theoretical contribution of this work is to develop new quadratic\n    constraints which fit into those frameworks, rather than build significantly\n    on top of them. Thus, the paper may be somewhat incremental in nature.\n\n- The authors do not provide the computation time for the naive baseline method\n    for approximating Lipschitz constants based on operator norms (MP),\n    so it is not clear what the trade-off between computation and accuracy is\n    for the proposed method.\n\nI am hesitant to recommend this submission for acceptance without additional \nevidence that the Householder and GroupSort activations are of practical\ninterest for Lipschitz estimation (see \"Questions\").\nMoreover, this paper is outside of my research area so it is difficult for me\nto judge its theoretical novelty; I did not check the proofs for correctness\nfor the same reason. Given this, and the smaller issues raised above, I am\n on the fence regarding this submission."
                },
                "questions": {
                    "value": "As noted above, I am not a expert on Lipschitz constant estimation for neural\nnetworks nor have I made use of algorithms from this area. Given this, can the\nauthors please provide additional details on why the GroupSort and Householder\nactivations are of particular interest for Lipschitz constant estimation? Since\nthese activations are the exclusive focus of the paper, I feel there must be an\nimmediate desire from the community to solve this problem in practice for\nthe paper to have a significant impact.\n\nI would also appreciate it if the authors could provide running times for the\nnaive estimation strategies in Table 1; this well help contextualize the cost\nof LipSDP-NSR and clarify the trade-off between accuracy and compute time. \n\nFinally, perhaps the authors can comment on the difficulty of deriving\nthe quadratic constraints for the SDPs. This will help me understand the novelty\nof the theoretical contributions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7869/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7869/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7869/Reviewer_8tsi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7869/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727874100,
            "cdate": 1698727874100,
            "tmdate": 1700510766015,
            "mdate": 1700510766015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fwYnBKHx8C",
                "forum": "HfXDrAzFvG",
                "replyto": "V0l2lUxM5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7869/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7869/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8tsi"
                    },
                    "comment": {
                        "value": "Thank you for your thorough evaluation of our paper. We would like to address your comments as below. We are also available to address any potential follow-up questions/suggestions.\n\n> Additional details on why the GroupSort and Householder activations are of particular interest for Lipschitz constant estimation?\n\n Please see our general response (Part 2). GroupSort and Householder activations are gradient norm preserving and have been of particular interest when fitting Lipschitz functions and for the design of certifiably robust neural networks (NNs). There are many works published in top machine learning conferences (e.g. Leino'21, Singla'21, Trockman'21, Huang'21, Singla'22, Prach'22, Hu'23) that use MaxMin activations and provide state-of-the-art certified robust accuracy (we provide details in our general response, Part 2). To compute the certified robust accuracy, these works rely on information of Lipschitz bounds and prediction margins of the trained networks. Hence LipSDP for MaxMin networks is of great interests. Another interesting application of MaxMin NNs is in learning-based control (Yion'23), where the information of the Lipschitz constant of a neural network in the loop can be useful to verify safety and robustness. Please note that we included a section on applications of MaxMin NNs as Appendix A.\n\n> I would also appreciate it if the authors could provide running times for the naive estimation strategies.\n\nWe are happy to provide this additional information. We recalculated the computation times for all our naive bounds. The computation times of the matrix product bound are for all our networks below 10s, so they are basically instantaneous. This means there definitely is a price for the tighter bounds using our method. The computation times for the lower bounds from sampling based on 200k samples range between 12 and 23 s. The FGL bound takes 4.54s for the 2FC-16 NN and 81.92s for the 2FC-32 NN. For larger NNs it however becomes intractable (we stopped our code for 2FC-64 after 4 hours).\n\nFind the running times of all naive l2 bounds for our networks in Table 1 in the table below:\n| Architecture  | Sample   | FGL | MP |\n| -- |:--:| --:|:--:| \n| 2FC-16        | 12.02 | 4.54 | 6.16 |\n| 2FC-32        | 13.08 | 81.92| 5.33 |\n| 2FC-64        | 12.84 |      | 3.95 |\n| 2FC-128       | 12.61 |      | 3.6  |\n| 5FC-32        | 13.18 |      | 5.43 |\n| 5FC-64        | 14.83 |      | 5.68 |\n| 8FC-32        | 14.14 |      | 5.57 |\n| 8FC-64        | 14.27 |      | 5.6  |\n| 8FC-128       | 14.15 |      | 9.85 |\n| 8FC-256       | 13.74 |      | 9.3  |\n| 18FC-32       | 23.28 |      | 7.64 |\n| 18FC-64       | 21.09 |      | 9.22 |\n| 18FC-128      | 19.48 |      | 4.9  |\n\n> Finally, perhaps the authors can comment on the difficulty of deriving the quadratic constraints for the SDPs.\n\nPlease see our general response (Part 1) for a detailed response. We are also happy to restate some of our key points here. We also included Appendix B to discuss this issue in our revised paper. The underlying idea in the quadratic constraint approach is to abstract \"challenging\", e.g., nonlinear, uncertain or time-varying elements using quadratic constraints. This abstraction allows us to formulate an SDP that can be solved effectively. Since the introduction of the quadratic constraint framework in the 1960s, many papers appeared that are only devoted to deriving new quadratic constraints. For example, Kao'07 derives quadratic constraints for a delay-difference operator to describe varying time delays, Pfifer'15 uses a geometric interpretation to derive new quadratic constraints for delayed nonlinear and parameter-varying systems, and Carrasco'16 summarizes the development of Zames-Falb multipliers based on frequency domain arguments for slope-restricted nonlinearites. In the context of NNs, so far only quadratic constraints for slope-restricted nonlinearities were utilized. By formulating quadratic constraints for GroupSort/Householder activations, we are the first ones to study multivariate activations, to formulate quadratic constraints for NNs that go beyond slope-restriction or Lipschitz continuity and in general the first to present quadratic constraints for sum-preserving elements. Finding novel quadratic constraints is by no means trivial. It requires to study and understand the characteristics of the underlying nonlinearity and then to capture the identified property using a quadratic form. There is no recipe for such a derivation and finding a quadratic constraint does not guarantee that it improves your analysis over other methods. The tighter your description of the nonlinearity by quadratic constraints, the better the analysis in terms of conservatism. The idea we used to derive new quadratic constraints is creative in the sense that it is very different from all the existing quadratic constraint derivations, hence offering unique new insights on how to use properties such as sum-preservation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7869/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256347078,
                "cdate": 1700256347078,
                "tmdate": 1700319693551,
                "mdate": 1700319693551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R5KK7m1JXP",
                "forum": "HfXDrAzFvG",
                "replyto": "V0l2lUxM5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7869/Reviewer_8tsi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7869/Reviewer_8tsi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for responding to my concerns and for providing the timings of the naive baseline methods. I also appreciate the inclusion of LipSDP-RR along with runtimes. I think these results provide a more nuanced view of LipSDP-NSR and fit well with the paper's story, which was motivated by better bounds in the L2 norm. I am not particularly concerned that LipSDP-RR sometimes has better performance for the L-infinity norm since computing the constant in the L-infinity norm is most \"value added\" in my opinion. \n\nIt is surprising to me that FGL is faster than LipSDP-NSR for small networks despite being an exponential time algorithm. Of course, this time complexity catches up for larger models as you mention. I suppose the constant-factor overhead for the SDP solver dominates when running on small architectures. \n\nThe matrix-product approach (MP), while naive, is very fast and provides acceptable-looking bounds when the number of layers is eight or less. I think it's worth including the runtimes you provided here in the paper (appendix is fine) and including a comment on the trade-off between efficiency and tightness for MP. In many cases it may be worth obtaining a loose bound quickly, which MP does well.\n\nOverall, I am convinced that the paper has practical use and that the quadratic constraints are of theoretical interest, so I will increase my score to 6. However, I do feel this paper is closer to 7 (although this doesn't exist for ICLR) and lean towards accepting the submission."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7869/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510746229,
                "cdate": 1700510746229,
                "tmdate": 1700510809147,
                "mdate": 1700510809147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0JzgUCeEAP",
            "forum": "HfXDrAzFvG",
            "replyto": "HfXDrAzFvG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7869/Reviewer_ykPt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7869/Reviewer_ykPt"
            ],
            "content": {
                "summary": {
                    "value": "This paper considered the problem of estimating the Lipschitz constant of neural networks with different kinds of activations that are not slope-restricted. Particularly, the authors investigated multi-layer (residual) networks applied with the GroupSort and Householder activations. The paper followed the idea of thge LipSDP formulation of the Lipschitz parameter estimation problem, and the main contribution is that the authors devised a new quadratic constrained that can deal with GroupSort and Householder which are not slope-restricted. In addition, the authors conducted empirical experiments which showed that the new formulation with quadratic cosntraints outperforms traditional matrix-product algorithms in terms of the accuracy of the estimated Lipschitz parameter."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is written clearly and the authors provided useful intuiation.\n\nThe new quadratic constraints enabled us to estimate the Lipschitz constants of neural networks with GroupSort or Householder activations with higher accuracy compared to traditional algorithms.\n\nIn addition, the authors considered both $\\ell_2$ and $\\ell_\\infty\\to\\ell_1$ lipschitz constants."
                },
                "weaknesses": {
                    "value": "The result seems to be weak since the quadratic constraints only applied to 2 specific activations. It would be more interesting if it can be applied to a class of activaitons."
                },
                "questions": {
                    "value": "How does the new formaulation compare to the original LipSDP for MaxMin networks in terms of computational efficiency/runtime?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7869/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814677873,
            "cdate": 1698814677873,
            "tmdate": 1699636964597,
            "mdate": 1699636964597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l2CXGpFZPG",
                "forum": "HfXDrAzFvG",
                "replyto": "0JzgUCeEAP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7869/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7869/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ykPt"
                    },
                    "comment": {
                        "value": "We appreciate your comments and evaluation of our work. We will address your concerns in the following and are happy to answer any follow-up questions.\n\n>The result seems to be weak since the quadratic constraints only applied to 2 specific activations. It would be more interesting if it can be applied to a class of activaitons.\n\nWe would consider GroupSort and HouseHolder as two classes of activations. Both generalize MaxMin which is widely used for robust networks due to the ease of implementation. Our constructions of quadratic constraints are also general in the sense that our GroupSort quadratic constraint also holds for the class of all sum-preserving activations. We now clearly state this in introduction of the revised version of the paper.\n\n>How does the new formaulation compare to the original LipSDP for MaxMin networks in terms of computational efficiency/runtime?\n\nThank you for this question. To investigate this question, we used the description of the MaxMin acitvation using a residual ReLU network (4), as shown in our motivating example in Section 3. For this equivalent formulation we then implemented an SDP for the residual ReLU equivalent form of MaxMin (the multi-layer variants of (5) under either $\\ell_2$ or $\\ell_\\infty$ setting) using quadratic constraints for slope-restricted activations to compute Lipschitz bounds. We denote the SDP on the residual ReLU formulation by LipSDP-RR. We added our results in the paper and for convenience also state the new results in the table below, where we compare the bounds and the computation times. We observe that our LipSDP-NSR provides better bounds in most cases, with two exceptions for 2-layer networks under the $\\ell_\\infty$ setting, i.e. $\\ell_\\infty$ Lipschitz bounds for 2FC-64 and 2FC-128. For 5- and 8-layer networks, our LipSDP-NSR is always better than LipSDP-RR, and there even is a significant gap.  Notice that LipSDP-NSR outperforms LipSDP-RR in all cases under the $\\ell_2$ setting. This is consistent with the intuition provided by our motivating example in Section 3 of our paper. Specifically, our motivating example in Section 3 implies that  the quadratic constraints derived from slope-restricted properties of ReLU have severe fundamental limitations in addressing the $\\ell_2$ case of MaxMin Lipschitz analysis, i.e. it cannot recover the basic 1-Lipschitz property of the MaxMin activation under the $\\ell_2$ setting. Our new quadratic constraints derived for MaxMin fix that, and our numerical results support our claim. In the $\\ell_\\infty$ setting, the output is always a scalar and the 1-Lipschitzness of MaxMin activation is not explicitly needed for SDP-based analysis. Hence our motivating example does not indicate much for the $\\ell_\\infty$ setting. However, our LipSDP-NSR still outperforms LipSDP-RR in most cases under the $\\ell_\\infty$ setting. Finally, we want to comment on the computation efficiency. We want to point out that advantages in computational efficiency of the MaxMin quadratic constraint formulation become apparent only for larger networks. It is interesting to notice that for small neural networks, LipSDP-RR can sometimes be faster, and this can be explained by the different implementations of the two problems. The computation time depends on the number of decision variables and on the conditioning of the problem. While the number of decision variables are the same in both problems, the multiple constraints for LipSDP-NSR are much smaller than the one constraint in LipSDP-RR. For networks larger than 8FC-64 we even run into memory problems when solving the SDP for the residual ReLU network formulation.\n\n| Architecture  | L-l2 (Time) NSR | L-l2 (Time) RR  | L-linfty (Time) NSR | L-linfty (Time) RR | \n| ------------- |:------------:| ------------:|:------------:| -----------:|\n| 2FC-16        | 22.59 (54)   | 23.92 (18)   | 203 (94)     | 207.3 (34)  |\n| 2FC-32        | 38.55 (86)   | 40.28 (24)   | 368.7 (128)  | 404.8 (44)  |\n| 2FC-64        | 73.77 (160)  | 74.31 (23)   | 885.9 (147)  | 870.8 (38)  |\n| 2FC-128       | 70.86 (182)  | 72.18 (55)   | 1058.8 (236) | 1033.1 (77) |\n| 5FC-32        | 224.6 (53)   | 388.0 (69)   | 2969.5 (96)  | 4283.1 (137)|\n| 5FC-64        | 386.8 (111)  | 623.4 (228)  | 6379.8 (190) | 8321.3 (416)|\n| 8FC-32        | 401.0 (58)   | 1078.5 (178) | 4303.9 (120) | 8131.1 (374)|\n| 8FC-64        | 722.3 (116)  | 1683.7 (1426)| 9632.3 (188) | 1.886E+04 (2495)|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7869/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255718860,
                "cdate": 1700255718860,
                "tmdate": 1700318555441,
                "mdate": 1700318555441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O9ADfrgc94",
                "forum": "HfXDrAzFvG",
                "replyto": "l2CXGpFZPG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7869/Reviewer_ykPt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7869/Reviewer_ykPt"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their feedback and their new experiments that showcase the efficiency of the LipSDP-NSR. I think the authors responded to my questions properly and detailedly. I would like to keep my scores unchanged."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7869/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588498140,
                "cdate": 1700588498140,
                "tmdate": 1700588498140,
                "mdate": 1700588498140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]