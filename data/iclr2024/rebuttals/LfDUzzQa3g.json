[
    {
        "title": "RepCodec: A Speech Representation Codec for Speech Tokenization"
    },
    {
        "review": {
            "id": "zZTQwfoOTf",
            "forum": "LfDUzzQa3g",
            "replyto": "LfDUzzQa3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a new speech tokenization for semantic modeling in this paper. Previous methods usually use k-means to discrete semantic representation, leading to information loss. Inspired by the audio codecs which reconstruct the raw audio, RepCodec encodes speech semantic representation from Hubert or data2vec to a set of vector quantization codebooks and reconstructs them by a decoder. The experiments demonstrate the superior performance of RepCodec in speech understanding and generation. Many detailed experiments also evaluate the performance in different configurations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation and description of the proposed method are very clear and easy to understand. The experiment is sufficient and demonstrates the effectiveness of RepCodec."
                },
                "weaknesses": {
                    "value": "1. In the introduction, the term \"semantic quality\" is mentioned. For clarity and the benefit of readers, could you provide a definition on what \"semantic quality\" encompasses?\n2. In previous works such as VALLE, AuidoLM, and PolySpeech mentioned in this paper, they all use the k-means clustering method to obtain semantic tokens. For the speech generation task (TTS, VC), the semantic information is important in discrete tokens, but the other \nencoded information such as speaker timbre is harmful to the task. Given the importance of speaker timbre information for downstream speech generation tasks, it would be of great value if a verification for speaker information of the encoded features is incorporated."
                },
                "questions": {
                    "value": "1. Section II.Speech Tokenization. \"However, the discretization step of k-means discards plenty of information of the speech\u201c. Can you give some examples so that readers can better understand the limitations of k-means?\n2. The first paragraph in Sectiion III. \"In AudioLM (Borsos et al., 2023), the WER is dramatically increased from 2.5% to 6.0% by using the discrete tokens of k-means from w2v-BERT XL\". What does this number 2.5% refer to? \n3. In Equation 5, some symbols are not defined. \n4. One question not related to the proposed method.  Why does the performance gap change among different speech encoders after VQ and K-means quantization compared with the original speech representations, especially whisper.  In other words, what kind of representations are suitable for clustering?\n5. Can you provide some samples of the speech resynthesis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698142020247,
            "cdate": 1698142020247,
            "tmdate": 1699635938655,
            "mdate": 1699635938655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BLsGAMdgVW",
                "forum": "LfDUzzQa3g",
                "replyto": "zZTQwfoOTf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gJKh"
                    },
                    "comment": {
                        "value": "Thanks for the valuable suggestions from the reviewer. We respond to your questions as follows.\n- **\u201cMeaning of semantic quality\u201d**\n\n    We believe we do not directly mention  \"semantic quality\". Instead, we mention the \u201cquality of semantic tokens\u201d, which refers to suitability of the semantic tokens for downstream tasks like ASR or TTS. When integrating LLMs with semantic tokens, we need to discretize speech into tokens to perform different downstream tasks. Some discrete tokens may not be suitable for downstream tasks and leads to inferior downstream task performance.\n- **\u201cSpeaker information of RepCodec\u201d**\n\n    We agree with the reviewer that the semantic units should carry as much semantic information as possible and as less speaker information as possible. Otherwise they may affect the speaker effect brought by other components, e.g. speaker embedding, acoustic tokens. We verify the speaker information carried by our units and the baseline k-means units through the voice conversion task:\n    \n    | Methods                   | Speaker Similarity \u2191| MOS\u2191 |\n    |:---|:---:|:---:|\n    |k-means(baseline)     |0.771                    |2.82\u00b10.36|\n    |RepCodec                 |0.764                    |3.48\u00b10.74|\n\n    In our voice conversion experiment, the speaker characteristics of the generated speech is controlled by the speaker embedding. However, the speaker information carried by the semantic units (if any) may also affect the result. In the above table, we measure the similarity of the voice converted speech and the original speech from the same speaker. A higher similarity score implies less speaker information carried by the units. It can be seen that the similarity scores are very close but RepCodec units have a much higher MOS score due to the improved semantic part. This means that RepCodec does not carry extra speaker information which is harmful to the task.\n\n\n- **\u201cLimitation of k-means\u201d**\n\n    The limitation of k-means is reflected in the degraded performance of the ASR modeling. Even if we train the seq-to-seq model with large amounts of data, using k-means tokens as input always results in much worse WER. The explanation to this empirical observation is that k-means discards some information of speech and makes the speech recognition of some similar words impossible so that the WER is inferior to using raw speech as input.\n- **\u201cReferred number of 2.5%\u201d**\n\n    We refer to Table II of the AudioLM paper, where WER of the original speech is 2.5%. However, the reconstructed speech using k-means has a much worse WER of 6.0%.\n- **Equation (5)\u201d**\n\n    Thanks for your advice. We will add explanations to symbols $\\tilde{n}_k$ and $e_k$ in our revision. They represent the moving average of EMA. $\\tilde{n}_k$ represents the moving average of the number of clusters $k$ and $e_k$ represents the moving average of the code book of cluster $k$.\n- **\u201cWhat kind of representations are suitable for clustering?\u201d**\n    \n    From our experiments, we believe that encoders trained with SSL objective function may be more suitable for clustering. Whisper is trained by end-to-end encoder-decoder architecture so that its representation from the encoder may require complicated transformation to be decoded as text. Therefore, its representation may be less suitable for clustering. \n- **\u201cExamples of speech resynthesis\u201d**\n    \n    We have uploaded some resynthesized speech to supplementary material."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708386933,
                "cdate": 1700708386933,
                "tmdate": 1700708386933,
                "mdate": 1700708386933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "krnkVmLwgv",
                "forum": "LfDUzzQa3g",
                "replyto": "BLsGAMdgVW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply to my comments. I have no more questions. It is nice to see the optimization and analysis of clustering methods for semantic representation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709445108,
                "cdate": 1700709445108,
                "tmdate": 1700709445108,
                "mdate": 1700709445108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ddLmeEcp5y",
            "forum": "LfDUzzQa3g",
            "replyto": "LfDUzzQa3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a speech representation code called RepCodec is introduced for semantic speech tokenization. It is seamlessly integrated into an end-to-end framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. RepCodec demonstrates promising results in both ASR and unit-to-speech resynthesis compared to the clustering method.\n2. The discovery that PMNI can deviate from performance is intriguing."
                },
                "weaknesses": {
                    "value": "1. Overall, this paper lacks novelty, as compared to SouldStream, it simply replaces the input from raw waveform with SSL representations.\n2. Some parts of the details in this paper are confusing:\n    * The difference in bar height in the encoder and decoder parts in Figure 1 is confusing because neither sampling nor dimension reduction is applied.\n    * Equation (5) lacks sufficient explanation. I am unsure of its correctness as neither ${\\overset{\\sim}{n_k}}$ nor $\\mathbf{e}_{i}$ is adequately defined or explained. \n    * Shouldn't equation (7) be\n$ F^* = \\arg\\max_F p(\\mathbf{y}|\\mathbf{s}) = \\arg\\max_F \\prod_{i=1}^{m} p(y_i|y_{<i}, \\mathbf{s}) $?\n\n3. It would be better to include a more in-depth analysis of the weights ($\\lambda_{r}$, $\\lambda_{q}$) of reconstruction loss and quantization loss."
                },
                "questions": {
                    "value": "3. Is WER a common metric for speech resynthesis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit",
                        "ICLR.cc/2024/Conference/Submission131/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698140574,
            "cdate": 1698698140574,
            "tmdate": 1700710304341,
            "mdate": 1700710304341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QHa7uMmfTW",
                "forum": "LfDUzzQa3g",
                "replyto": "ddLmeEcp5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6oit"
                    },
                    "comment": {
                        "value": "Thanks for the feedback from the reviewer. We have the following response to your comments.\n\n- **\u201cNovelty of the paper\u201d**\n  \n    We believe our paper proposes completely new methods to generate semantic units and provides new understanding of the quality of the semantic units. Before RepCodec, semantic units can only be produced by traditional k-means methods following HuBERT [1], which was proposed about two years ago. Our paper, despite technical similarity to SoundStream, provides a novel method that largely outperforms long existing k-means clustering. \n\n    In addition, we also provide a new understanding about the semantic units. In previous papers such as HuBERT [1] and AudioLM [2], similarity to phonemes is used to measure the quality of the semantic units. Our paper, however, shows that PNMI is not the only metric for evaluating the semantic tokens. Instead, the reconstruction loss provides more meaningful indication for the performance of downstream tasks. It brings new insight for future study of semantic tokens.\n- **\u201cAblation Study of $(\\lambda_q, \\lambda_r)$\u201d**\n\n    As shown in the following Table, we change the weights of reconstruction loss $\\lambda_r$ to train the encoder of RepCodec and report its WER on the downstream task of ASR modeling with representation from the 18th layer of data2vec large model. The results show that RepCodec is robust to the changes of the weights of $(\\lambda_q, \\lambda_r)$.\n    | $\\lambda_q$ | $\\lambda_r$ | PNMI |Reconstruction Loss| WER of ASR Modeling |\n    |:---|:---:|:---:|:---:|:---:|\n    |1.0|30.0| 0.3689 | 0.2091 | 2.91 |\n    |1.0|45.0| 0.3714 | 0.2100 | 2.87 |\n    |1.0|60.0| 0.3697 | 0.2090 | 2.83 |\n\n- **\u201cConfusing details of the paper: Figure 1\u201d**\n\n    Our method is compatible with different architectures of the encoder and decoder in Figure 1. To allow variants of the network architectures, we show in Figure 1 that dimension reduction can be applied. We will add more detailed illustrations in Figure 1.\n- **\u201cConfusing details of the paper: Equation (5)\u201d**\n\n    Thanks for your advice. We will add explanations to symbols $\\tilde{n}_k$ and $e_k$ in our revision. They represent the moving average of EMA. $\\tilde{n}_k$ represents the moving average of the number of clusters $k$ and $e_k$ represents the moving average of the code book of cluster $k$.\n- **\u201cConfusing details of the paper: Equation (7)\u201d**\n \n    Thanks for pointing out our typo. We will fix it in our revision.\n- **\u201cUsing WER to evaluate speech resynthesis\u201d**\n \n    We follow previous papers and use WER to evaluate the quality of semantic units such as mHubert [3] and AudioLM [2]. As the role of semantic units is to provide the semantic information of the speech, the best way to evaluate the semantic information is to measure the content, i.e. ASR-WER, of the output speech.  \n\n[1] Hsu, Wei-Ning, et al. \"Hubert: Self-supervised speech representation learning by masked prediction of hidden units.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 29 (2021): 3451-3460.\n\n[2] Borsos, Zal\u00e1n, et al. \"Audiolm: a language modeling approach to audio generation.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing (2023).\n\n[3] Lee, Ann, et al. \"Textless Speech-to-Speech Translation on Real Data.\" In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708122489,
                "cdate": 1700708122489,
                "tmdate": 1700708122489,
                "mdate": 1700708122489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oovtDOOrnu",
                "forum": "LfDUzzQa3g",
                "replyto": "QHa7uMmfTW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit"
                ],
                "content": {
                    "title": {
                        "value": "Increasing score from 3 to 5"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response. After considering their feedback and taking into account comments from other reviewers, I would like to revise my rating from 3 to 5."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710279878,
                "cdate": 1700710279878,
                "tmdate": 1700710279878,
                "mdate": 1700710279878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wNNyGJg74u",
            "forum": "LfDUzzQa3g",
            "replyto": "LfDUzzQa3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces RepCodec, a speech representation codec designed for semantic speech tokenization. It applies VQVAE to the representations from pretrained speech encoders to learn audio semantic tokens. The authors demonstrate the superiority of their proposed method over other discrete speech representation techniques, as evidenced by improved WER scores on ASR and speech resynthesis tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors demonstrate the superiority of their proposed method over other discrete speech representation techniques in terms of the WER scores on both ASR and speech resynthesis tasks.\n* The authors analyze the issue with the quality measure of semantic tokens based on their similarity to ground truth phonemes, while illustrating that the reconstruction loss of their proposed method exhibits a higher correlation."
                },
                "weaknesses": {
                    "value": "* Insufficient evaluation metrics. The research predominantly relies on WER as the principal evaluation metric for the performance of semantic speech tokens. To make a compelling case for the proposed method's superiority, it's essential to include other the evaluation metrics such as speaker similarity, F0 error, or mean-opinion score in the speech resynthesis experiments.\n* Limited exploration of core downstream tasks. While semantic tokens are integral to token-based language modeling of speech, the paper's experiments are primarily focused on ASR and speech resynthesis. It lacks empirical investigations into other vital application tasks such as language modeling of audio, text-to-speech, speech-to-speech translation, or conditional modeling of acoustic tokens given the semantic tokens."
                },
                "questions": {
                    "value": "I have concerns regarding the lack of evaluation results, as mentioned in the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826007327,
            "cdate": 1698826007327,
            "tmdate": 1699635938486,
            "mdate": 1699635938486,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IRCXeP5kim",
                "forum": "LfDUzzQa3g",
                "replyto": "wNNyGJg74u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wBo4"
                    },
                    "comment": {
                        "value": "Thanks for the constructive comments from the reviewer. We address your concern as follows.\n\n- **\u201cInsufficient evaluation metrics.\u201d**\n  \n    First, our evaluation metrics follow previous papers on evaluating the quality of semantic units such as mHubert [1] and AudioLM [2]. They all use word error rate (WER) to measure the quality of semantic units. As the role of semantic units is to provide the semantic information of the speech, the best way to evaluate the semantic information is to measure the content, i.e. ASR-WER, of the output speech.  Other metrics such as speaker similarity or mean-opinion score (MOS) measure the acoustic quality of the speech, so they do not reflect the quality of the semantic units.\n\n    While we do not think speaker similarity and MOS can measure the quality of RepCodec units, we still perform the experiments to calculate the speaker similarity, F0 error, and mean-opinion score on speech resynthesis task using LJSpeech. As shown in the following Table, RepCodec largely outperforms k-means on all these metrics, proving the superiority of the acoustic quality of RepCodec units in the speech generation tasks.\n\n    | Methods|F0 VDE\u2193| F0 FFE \u2193| Speaker Similarity \u2191| MOS\u2191 |\n    |:---|:---:|:---:|:---:|:---:|\n    |k-means|0.199|0.248|0.771|2.82\u00b10.36|\n    |RepCodec|0.174|0.185|0.764|3.48\u00b10.74|\n    |Original Speech| - | - | - |4.32\u00b10.92|\n\n\n- **\u201cInsufficient downstream tasks.\u201d**\n\n    First, we would like to point out that our experiments do focus on practical downstream tasks. In addition to ASR and speech resynthesis, we also perform voice conversion in Table 3(b). ASR is a critical task for speech understanding, and RepCodec can be corporated into frameworks like PolyVoice for speech-speech translation.  Voice conversion is also a practical task that has wide applications. \n\n    Completely performing the experiments like language modeling of audio, text-to-speech, speech-to-speech translation needs a lot of computation resources and a large amount of time to train the model, which makes it infeasible to complete the experiments in such a short time of the rebuttal. However, our experiments have verified both the input stage and output stage in the framework of PolyVoice for speech-to-speech translation, we believe it is sufficient to show that RepCodec is superior to k-means in these downstream tasks. \n\n[1] Lee, Ann, et al. \"Textless Speech-to-Speech Translation on Real Data.\" In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (2022).\n\n[2] Borsos, Zal\u00e1n, et al. \"Audiolm: a language modeling approach to audio generation.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707967791,
                "cdate": 1700707967791,
                "tmdate": 1700707967791,
                "mdate": 1700707967791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "02JRucYOLZ",
                "forum": "LfDUzzQa3g",
                "replyto": "wNNyGJg74u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer wBo4"
                    },
                    "comment": {
                        "value": "I appreciate the authors for addressing my comments. However, since the focus of this study is not on more powerful self-supervised representation learning, but rather on producing effective discrete units from representations of self-supervised pretrained models, I believe it should have further validated its effectiveness in language modeling on speech data. Considering both the strengths and limitations of this research, I will maintain my current rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731060236,
                "cdate": 1700731060236,
                "tmdate": 1700731374128,
                "mdate": 1700731374128,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1gV47tjQkU",
            "forum": "LfDUzzQa3g",
            "replyto": "LfDUzzQa3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission131/Reviewer_XznZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission131/Reviewer_XznZ"
            ],
            "content": {
                "summary": {
                    "value": "A novel RepCodec, a speech representation codec for semantic speech tokenization, has been introduced. RepCodec utilizes a vector quantization codebook to reconstruct speech representations from speech encoders like HuBERT or data2vec. RepCodec significantly outperforms the widely used k-means clustering approach in both speech understanding and generation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is great in its clarity and well-structured organization. Its proposed approach is lauded for its simplicity and effectiveness. The comprehensive nature of the experiments conducted further strengthens the paper's credibility. Based on these positive aspects, it is recommended for publication at the conference."
                },
                "weaknesses": {
                    "value": "The simplicity and effectiveness of the proposed approach are commendable. While there are no significant weaknesses to highlight, it would be intriguing to see the application of RepCodec in the context of zero-shot Text-to-Speech (TTS) systems, such as Vall-E. Exploring its potential in this domain could provide valuable insights and possibly further advancements in speech-processing technology.\n\nThe idea of SpeechTokenizer (SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models) has some similarities, could you please elaborate more regarding the difference? If possible, adding some baseline numbers using https://github.com/ZhangXInFD/SpeechTokenizer would add value to this paper."
                },
                "questions": {
                    "value": "See comment in the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829783650,
            "cdate": 1698829783650,
            "tmdate": 1699635938411,
            "mdate": 1699635938411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZRZIItLfZL",
                "forum": "LfDUzzQa3g",
                "replyto": "1gV47tjQkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XznZ"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your positive review and the valuable feedback you have provided. We hope our response completely addresses any concerns you may have.\n\n- **\u201cZero-Shot TTS such as VALL-E\u201d**\n   \n    We agree with the reviewer that applying semantic units of RepCodec to zero-shot TTS is an interesting experiment. However, VALL-E does not use semantic units as the inputs for semantic information. Instead, they use phoneme sequence to control the semantic content of the output speech. Our experiments include both speech resynthesis and voice conversion tasks, which demonstrate superior performance to the traditional k-means clustering. We believe current experiments are enough to demonstrate the superiority of RepCodec, and we leave the experiments of zero-shot TTS to further work.\n\n- **\"Comparison to SpeechTokenzier\"**\n\n    Thanks for your advice. We will add a citation of SpeechTokenizer in our paper. However, we would like to point out that SpeechTokenizer and RepCodec focus on different categories of the discrete units. RepCodec aims to improve the quality of semantic units, trying to preserve more information for downstream tasks. On the other hand, SpeechTokenzier targets at improving the quality of acoustic units. Therefore, we do not think directly comparing numbers of these two methods is meaningful."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707870926,
                "cdate": 1700707870926,
                "tmdate": 1700707870926,
                "mdate": 1700707870926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "spPokOMWN7",
                "forum": "LfDUzzQa3g",
                "replyto": "ZRZIItLfZL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_XznZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_XznZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for replying my comments and adding missing the reference, I keep my rating of the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission131/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726423009,
                "cdate": 1700726423009,
                "tmdate": 1700726423009,
                "mdate": 1700726423009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]