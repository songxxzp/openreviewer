[
    {
        "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks"
    },
    {
        "review": {
            "id": "wxPurMCUKL",
            "forum": "vlQ56aWJhl",
            "replyto": "vlQ56aWJhl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_mZie"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_mZie"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new learning rule to train spiking neural networks. The idea is based on a three-factor structure, but using BPTT and STDP as its components. The STDP-based eligibility trace function scales with $n$ and is temporally local (does not scale with $T$), which is an improvement over existing methods. This method, referred to by the authors as S-TLLR, has an additional non-causal component which scales with $n$, just like the causal component, and therefore does not affect its scaling with space and time. Experiments and benchmarks on numerous datasets reveal the advantage of this non-causal learning component.\n\nI am generally positive about this work in regards to the new proposed method and how it improves training of spiking networks. I hope that authors can clarify any misunderstandings I may have in the weaknesses section and I am very willing to adjust my score in the rebuttal."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The theoretical scaling advantage is highly relevant and important to the spiking neural network community. Using an STDP-based eligibility trace function also lends to biological plausibility, which has relevance to neuroscience audiences."
                },
                "weaknesses": {
                    "value": "I am doubtful of both main claims, on (1) temporal locality and (2) improvements from non-causal terms.\n\n(1) The temporal locality property of this method is unconvincing. In Figure 1, my naive understanding is that it is possible to simply truncate both BPTT and STDP methods in the same way S-TLLR is truncated using equation (11). In other words, all methods can have temporal locality. The only way to truly claim that the proposed method does not scale with time, is by using both BPTT and STDP (and perhaps even other existing methods) with this truncation and see if S-TLLR learns faster or if other methods fail to learn the objective. \n\n(2) The improvement from non-causal terms is similarly highly confounded by the secondary activation functions in equations (14-17). Suggestions for fair experiments could be:\n- universally use the same secondary activation function across all tasks, or use all 4 activation functions for all tasks\n- apply the same activation functions to other methods  \n\nTo be very clear, I understand that S-TLLR is compared across different values of $\\alpha$ within the same secondary activation functions, but it is not clear if this behavior is task and function specific. For example, dataset A and secondary function X could give better results with non-zero $\\alpha$, while dataset B with secondary function X or dataset A with secondary function Y has better results with $\\alpha = 0$. \n\n(3) It is also not clear how the method works in the recurrent neural network task. If I were to incorporate causal recurrent gradients in Figure 1, that would correspond to red lines being drawn from $u[t]$ to $y[t-1]$ (and others), which means most terms with have red and blue lines in parallel.  \n\n(4) The recurrent term in equation (1), while true and makes the equation general, simply disappears and lacks coherence and continuity with all future equations where the narrative centers around a feedforward network. For example, equation (4) has no recurrent term. This should be stated in the text somewhere or removed."
                },
                "questions": {
                    "value": "Should blue terms in Figure 1 also extend beyond $t-2$ (with three dots) just like the red terms?\n\nWhile theoretical scaling arguments are convincing, there are many factors underlying number of computations. How are the 1.1x, 4x and 10x claims actually made? Was it done by recording the number of floating point operations? More information is needed to substantiate these claims. The actual amount of time taken to train the networks is also an important metric to include as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Reviewer_mZie",
                        "ICLR.cc/2024/Conference/Submission7662/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614422396,
            "cdate": 1698614422396,
            "tmdate": 1700807954061,
            "mdate": 1700807954061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "00UDOyRbkU",
                "forum": "vlQ56aWJhl",
                "replyto": "wxPurMCUKL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer W1:** We would like to address this question by stating that temporal locality means using only information available at the present. For example, in a sequence with multiple time steps, a temporal local operation at time step ($t$)\u00a0must use only information available at that time step. By this definition, STDP expressed in equation (6) is a temporal local learning rule at any time step as it uses only information available at the present, and therefore its memory requirements are independent of the length of the sequence. In contrast, as we discuss in appendix C, BPTT does not present a temporal locality feature due to the backpropagation of errors trough time. This implies that the weight updates ($\\Delta w$) at any time step depend on future information as shown in equation (23) and Figure 2a. Note that this happens due to the implicit recurrent connections (trough membrane potential accumulation) of hidden layers and not the loss function itself, so even if the loss for BPTT is computed as described in equation (11) and just for the last time step ($T_l=T$), propagation of errors is going to be similar to the one shown in Figure 2a with the small difference that the connections between the last layer and the loss function disappear, but still the weight updates at time step $t$ ($\\Delta w [t]$) are going to depend on information at future time steps ($t+1, t+2, ..., T$). Therefore, BPTT is not a temporal local learning rule and requires the information of every single time step to compute the synaptic updates. In the case of S-TLLR, similar to STDP we can use trace variables to propagate the required information forward in time and use a learning signal obtained by propagation of errors trough the layers (not in time). Therefore, S-TLLR memory requirements are independent of the sequence length. An analysis on memory requirements is shown in Appendix C. Additionally, we would like to point out that equation (11) represent the instantaneous error signal obtained at the current time step ($t$), and the term $T_l$\u00a0only control at which point the targets are available and therefore the number of weight updates. So even if the learning signal is available for all time steps ($T_l=0$) the memory requirements of S-TLLR are the same.\n\n**Answer W2:** In response to the ablation study suggestion, we conducted experiments, presented in Table 5 (Appendix E3), using three activation functions with varied $\\alpha_{post}$ values due to resource constraints. The findings consistently demonstrate superior performance when using non-zero $\\alpha_{post}$ compared to $\\alpha_{post}=0$.\n\n**Answer W3:** We have included a new appendix D elucidating the application of our method in recurrent models. Additionally, we updated the Figure 1 to ensure the discussion's generality.\n\n**Answer W4:** We appreciate the feedback and have clarified that the primary text focuses on feed-forward networks, with discussions regarding recurrent models available in the supplementary material (Appendix D). This distinction aims to provide a comprehensive understanding while maintaining clarity within the main text.\n\n**Answer Q1:** Yes, the blue terms in Figure 1 should be extend beyond $t-2$. We have updated Figure 1 to ensure the discussion's generality.\n\n**Answer Q2:** In Appendix C, we presented the analysis to estimate the improvements and updated the reported improvements accordingly. Notably, we deliberately omitted specific details regarding the training duration of the models in GPU. This decision stems from our deliberate emphasis on explicitly illustrating the workings of S-TLLR within our implementation (code), prioritizing a comprehensive demonstration over runtime optimization. The variability in GPU training times is contingent upon specific code implementations, a facet that diverges from the primary focus of our discussion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696989374,
                "cdate": 1700696989374,
                "tmdate": 1700696989374,
                "mdate": 1700696989374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ik2A66ns5H",
            "forum": "vlQ56aWJhl",
            "replyto": "vlQ56aWJhl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_FGQ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_FGQ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an STDP-based learning algorithm that focuses on SNN training from the memory efficient perspective. The proposed algorithm has shown reduced complexity on the event-based dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method shows reduced time complexity, it is natural that the STDP-based learning requires less memory compared to BPTT with gradient surrogation. The proposed algorithm seems hardware friendly with discrete operations."
                },
                "weaknesses": {
                    "value": "**W1:** Insufficient experiments: I understand that the event-based computer vision tasks are suitable for spiking neural networks, but I think the dataset reported in this paper is not comprehensive enough. In addition to the popular DVS-CIFAR10 and DVS-Gesture, N-CalTech101, and NCARs are also adopted in prior works [R1] as benchmarks. However, these results are missing in the paper. \n\n[R1] AEGNN: Asynchronous Event-Based Graph Neural Networks, CVPR, 2022.\n\n\n**W2:** Since the proposed method claims that the conventional BNTT is memory expensive, it is important to demonstrate the memory-accuracy comparison between the proposed method and BNTT (e.g., GPU Memory) \n\n**W3:** Some recent papers and SoTA methods are not cited in this paper: \n\n[R2]: Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting\n\n[R2]: Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks, NeurIPS'21\n\n[R4]: Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation CVPR 2022\n\n**W4:** The methodology section should be elaborated more. Based on Figure 1, S-TLLR introduces the incoming gradient $\\partial L / \\partial y$ on top of discrete STDP. What is the theoretical advantage (or intuition) of doing that?"
                },
                "questions": {
                    "value": "Please refer to Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Reviewer_FGQ4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641223483,
            "cdate": 1698641223483,
            "tmdate": 1699636932127,
            "mdate": 1699636932127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VFG9KHeuic",
                "forum": "vlQ56aWJhl",
                "replyto": "ik2A66ns5H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer 1:** Thank you for your suggestion. In our revised version, we have incorporated the results for the N-Caltech101 dataset. Regrettably, integrating the Ncars dataset into our current framework wasn't feasible at this moment. Please, note that in our initial version, we already included evaluations beyond event-based vision classification tasks. Specifically, we incorporated assessments in event-based audio recognition and optical flow domains. These evaluations were deliberately included to explore longer temporal dependencies, such as in audio, and to tackle more intricate spatio-temporal regression through self-supervision, as observed in optical flow tasks.\n\n**Answer 2:** We have taken the reviewers' feedback into careful consideration and have supplemented our paper with a more extensive analysis of the computational and memory-intensive nature of BPTT in Appendix C. Although the analysis primarily discusses fully connected models, it can be extrapolated to encompass convolutional models as well. Additionally, we have included a visual representation in Figure 3 within Appendix C4 to illustrate a simple example that delineates how GPU memory scales linearly with the number of time steps, contrasting with the consistent of S-TLLR memory consumption. These additions aim to provide a comprehensive understanding of the computational and memory complexities associated with our proposed framework.\n\n**Answer 3:** We sincerely appreciate the reviewers' valuable input regarding the additional papers. To ensure completeness and accuracy in our comparative analysis, we have incorporated the suggested papers in Table 3. It's essential to note that while these prior works focused on achieving high performance in image classification on static datasets, our emphasis lies in addressing temporal locality and memory-efficient training for Spiking Neural Networks (SNNs). The highlighted papers, while significant, do not encompass the pivotal aspects of our proposed approach, which emphasizes temporal dynamics and memory efficiency in SNNs\n\n**Answer 4:** STDP, an unsupervised learning algorithm, operates by adjusting synaptic connections based on the relative timing of spiking activity but lacks a global supervisory signal to guide learning toward specific goals. So, we designed S-TLLR to incorporate a mechanism akin to STDP to identify neurons for update based on spiking activity. Then, we use a third factor\u2014the learning signal\u2014to attribute credit or assign blame to the neurons identified for update. This additional supervisory mechanism augments the synaptic plasticity process, enabling goal-oriented learning within the SNN framework."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696796420,
                "cdate": 1700696796420,
                "tmdate": 1700696796420,
                "mdate": 1700696796420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "67EJtdaPHR",
            "forum": "vlQ56aWJhl",
            "replyto": "vlQ56aWJhl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_H5rp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_H5rp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces S-TLLR, a novel learning rule for Spiking Neural Networks (SNNs) aimed at efficient online learning on resource-constrained edge devices. S-TLLR draws inspiration from Spike-Timing Dependent Plasticity (STDP) and utilizes both causal and non-causal relationships for synaptic weight updates, maintaining constant memory and time complexity. Through extensive experimentation, the authors demonstrate that S-TLLR achieves comparable accuracy to traditional methods like BPTT but with significantly lower computational demands. The paper's contributions are highlighted by the improved generalization and performance of SNNs on a variety of event-based tasks\u2014including image and gesture recognition, audio classification, and optical flow estimation\u2014and the validation of S-TLLR's efficacy across multiple network topologies, marking a step forward in deploying energy-efficient intelligence in real-world applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. S-TLLR is a groundbreaking approach that successfully trains SNNs with high efficiency, addressing the temporal and spatial credit assignment challenge that is inherent in such networks.\n2.  By incorporating principles from STDP, S-TLLR aligns closely with biological neural processes, potentially unlocking more natural learning patterns and efficiencies.\n3. S-TLLR successfully integrates both top-down modulation and the local algorithm.\n4. The proposed learning rule maintains constant time and memory complexity, which is a significant advancement for deploying SNNs on edge devices where resources are constrained."
                },
                "weaknesses": {
                    "value": "1. The complexity was estimated, but the real energy consumption/efficiency haven't been calculated/tested.\n2. While BPTT could work on much deeper SNNs, how about S-TLLR? Could it be extended to larger models/datasets?"
                },
                "questions": {
                    "value": "Please see the weaknesses:\n1. Could the energy consumption/efficiency be calculated/tested.\n2. While BPTT could work on much deeper SNNs, how about S-TLLR? Could it be extended to larger models/datasets, such as CIFAR100?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7662/Reviewer_H5rp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832170323,
            "cdate": 1698832170323,
            "tmdate": 1699636931996,
            "mdate": 1699636931996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wJx7fqhWiP",
                "forum": "vlQ56aWJhl",
                "replyto": "67EJtdaPHR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer 1:** We have expanded our discussion on the number of multiply-accumulate (MAC) operations required by both S-TLLR and BPTT in Appendix C. While the estimation of dynamic energy consumption is typically proportional to the number of MAC operations, during training, the energy consumption associated with memory read and write operations might significantly contribute, particularly for memory-intensive methods like BPTT. Accurately modeling such energy consumption is a complex task beyond the current scope of our research. It's worth noting that S-TLLR was specifically designed as a three-factor learning rule, tailored for potential utilization on neuromorphic hardware like Loihi, which is an avenue we plan to explore in our future research.\n\n**Answer 2:** Primarily, our focus lies on event-based tasks, as we believe these tasks inherently benefit from utilizing SNNs due to their temporal information, unlike static image classification tasks. While our exploration into SNNs hasn't delved extensively into deeper architectures, this limitation is largely due to the absence of neuromorphic datasets comparable in size to more mainstream vision tasks such as CIFAR100 or ImageNet. However, it's important to note our experiments on N-Caltech101, which contains a similar number of classes as CIFAR100, demonstrating that S-TLLR performs comparably to BPTT. Additionally, our experiments on optical flow using MVSEC, a challenging spatiotemporal regression task, show that S-TLLR can achieve performance similar to BPTT. These results instill confidence that S-TLLR can indeed be extended to deeper models and a broader spectrum of tasks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696740499,
                "cdate": 1700696740499,
                "tmdate": 1700696740499,
                "mdate": 1700696740499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u0iEpeEAOG",
            "forum": "vlQ56aWJhl",
            "replyto": "vlQ56aWJhl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_KFXk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7662/Reviewer_KFXk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new learning rule for Spiking Neural Networks. This rule has low linear memory complexity and quadratic time complexity in terms of number of neurons. Moreover, the proposed learning algorithm incorporates a non-causal learning term,  inspired by Spike-Timing-Dependent Plasticity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Evaluation is done on variety of tasks;\n2) Paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "My main concern is that the method considered in the paper (S-TLLR) is very similar to OTTT[1]: \n\n1) OTTT has the same learning rule as S-TLLR except that additionally S-TLLR leverages non-causality and few other minor differences. But this non-causal term doesn\u2019t help S-TLLR consistently based on Fig. 2;\n2) S-TLLR has the same time and memory complexity;\n3) S-TLLR doesn\u2019t outperform OTTT. \n\n[1] Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, and Zhouchen Lin. Online Training Through Time for Spiking Neural Networks, NeurIPS 2022"
                },
                "questions": {
                    "value": "1) Can the authors list all the differences between OTTT with S-TLLR methods?\n2) In the paper, it is mentioned that OTTT applies learning rules at each forward pass, whereas S-TLLR enforces the learning rule at every fourth forward step. Could the authors test the performance if S-TLLR's learning rule was applied at each forward pass, similar to OTTT?\n3) Can the authors do ablation study taking OTTT model as a reference starting point? The study would systematically integrate modifications that transition the model towards the S-TLLR approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "--"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877847564,
            "cdate": 1698877847564,
            "tmdate": 1699636931889,
            "mdate": 1699636931889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L1peCzN8Ut",
                "forum": "vlQ56aWJhl",
                "replyto": "u0iEpeEAOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7662/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer 1:** Thank you for pointing out the similarities with OTTT [1], which allows for a more in-depth discussion. It's essential to note that while both algorithms share certain aspects, their fundamental motivations differ significantly. OTTT strives to approximate BPTT and is derived from BPTT equations by detaching all recurrent connections from the auto-differentiation graph. In contrast, S-TLLR is based on the theory of three-factor learning rules, utilizing the STDP mechanism to compute eligibility traces and a learning signal derived from backpropagation through layers or random feedback connections (DFA).\n\nFurthermore, the selection of STDP in S-TLLR stems from the observation that the exponential decay of the causal term in STDP is akin to the computation of gradients over time with a decay factor equal to the leak factor of spiking neurons. However, while STDP considers both causal and non-causal terms for synaptic plasticity, focusing solely on causal information might result in a loss of valuable learning information. Hence, S-TLLR proposes a three-factor learning rule that models an STDP-like mechanism for neuron eligibility.\n\nAnother significant distinction lies in the target applications; OTTT, as stated by its authors, targets static image classification, while S-TLLR is specifically designed for temporal dependency tasks, deemed more suitable for SNNs. Notably, S-TLLR encompasses a pool of learning rules defined by the selection of hyperparameters (STDP parameters), where OTTT represents a specific case with hyperparameters $\\alpha_{post} = 0$ and $\\lambda_{pre} = \\gamma$.\n\nIn our evaluations (Table 2, 3, 4, 5), using non-causal terms consistently yields better performance than using only causal terms (S-TLLR with $\\alpha_{post}=0$ equivalent to OTTT). Even though we attempted to replicate OTTT's results for DVS CIFAR10 using S-TLLR ($\\alpha_{post}=0$) in Table 3, our batch size discrepancy\u2014using 48 compared to [1]'s 128 due to GPU memory limitations\u2014may have influenced the outcome. Our comparison across various datasets supports the superiority of S-TLLR using non-causal terms over OTTT.\n\n**Answer 2:** To clarify, we did not enforce the learning rule every fourth forward step but during the last $T-T_l$ timesteps of the sequence. For N-CALTECH101, DVS Gesture and DVS CIFAR10, it applies to the last 5 time steps, and for Optical Flow, it occurs at the final timestep. Following the reviewer's suggestion, we conducted simulations for DVS CIFAR20, presenting the learning signal during every timestep of the sequence, as indicated in Table 3. Consistently, S-TLLR with non-causal terms outperforms OTTT (S-TLLR with $\\alpha_{post} = 0$) for the same batch size 48.\n\n**Answer 3:** As previously discussed, OTTT can be viewed as a specific instance of S-TLLR when $\\alpha_{post}=0$ and $\\lambda_{pre}=\\gamma$ (the leak factor in LIF models). Consequently, the ablation studies comparing OTTT and S-TLLR are encompassed across the work in Table 2, 3, 4 and 5. Our results consistently demonstrate that S-TLLR outperforms OTTT (S-TLLR with $\\alpha_{post}=0$) under same conditions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696688891,
                "cdate": 1700696688891,
                "tmdate": 1700696688891,
                "mdate": 1700696688891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]