[
    {
        "title": "Improving Out-of-Domain Generalization with Domain Relations"
    },
    {
        "review": {
            "id": "NWahxoHWQj",
            "forum": "Dc4rXq3HIA",
            "replyto": "Dc4rXq3HIA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_u1oN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_u1oN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed D3G, which in contrast to previous approaches, utilizes domain metadata to create domain-specific models. During training, it learns domain-specific functions, and during testing, it reweights them based on domain relations, enhancing model adaptability using directly acquired domain metadata."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. The authors use appropriate notation and equations where necessary.\n- This paper utilizes domain meta-data to extract relations between domains.\n- The domain relations are learned rather than fixed."
                },
                "weaknesses": {
                    "value": "(1) In section 3.2, it is mentioned that both $m_i$ and $a_{ij}^{g}$ are derived from domain metadata, but the specific relationship or distinction between them is not clearly explained. The experimental results suggest that $a_{ij}^{g}$ is extracted from $m_i$, yet this section lacks explicit clarification.\n\n(2) It is unclear whether the two layer neural network $g$ should be trained or directly be used from other previous works. It is mentioned that weight vectors $w_r$ are learnable vectors. Are these weights and the neural network trained by the consistency loss?\n\n(3) The consistency loss implies that each head is trained on all the domains with different weights. This is similar to the case where each head is trained on all the domains with a weighted combination of the losses of all the domains. For the predictor $f^{(d)}$, the loss is in this form: $l(\\frac{\\sum_j^{N^{tr}} a_{dj} f^{(d)}(x^j)}{\\sum_j^{N_{tr}} a_{dj}})$. Would this loss also help the learning of domain-specific models?\n\n(4) Suppose that each head is trained on all the domains by the loss function (2). In this case, regardless of the values of $a_{ij}$, if both the supervised loss and consistency loss reach their minimum, then the learning of $a_{ij}$ would be useless. Does the proposed method face this issue?\n\n(5) In the experiment section, one baseline of the proposed method would be training each head on all the training domains by supervised loss and averaging the predictions of these heads as the test prediction."
                },
                "questions": {
                    "value": "Please refer to the Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Reviewer_u1oN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629285734,
            "cdate": 1698629285734,
            "tmdate": 1699636136357,
            "mdate": 1699636136357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GZGLmwMIYi",
                "forum": "Dc4rXq3HIA",
                "replyto": "NWahxoHWQj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u1oN (1/2)"
                    },
                    "comment": {
                        "value": "Thank you so much for the insightful and valuable comments! They are very helpful for further improving the clarity and quality of our paper. We'll revise our manuscript in the next version to address all of your concerns.\n\n**Q1**: Section 3.2 lacks explicit clarification of the specific relationship or distinction between domain meta-data and domain relations.\n\n**A1**: Thank you for pointing that out. The fixed domain relation $a_{ij}^g$ can be derived either from domain meta-data $m_i$ or directly captured by domain experts. In the former case, for instance, in spatial distribution shift scenarios like FMoW, we can use geographical information (e.g., longitude and latitude), which may be included in the meta-data $m_i$ or $m_j$, to establish geographical proximity relations. In the latter case, as an example, in drug-target interaction prediction, we employ a fixed relation represented by the protein-protein graph, while $m_i$ represents the protein structure. We have clarified these points in the updated paper in Section 3.2.\n\n---\n\n**Q2**: It is unclear whether the two layer neural network $g$ should be trained or directly be used from other previous works.\n\n**A2**: In this paper, the final domain relation $a_{ij}$ contains the learned relation $a_{ij}^l$, which is learned from the domain meta-data (Eqn. (5)) with the two layer neural network. The final domain relation $a_{ij}$ is reflected in the consistency training loss. Thus, the two-layer neural network can only be optimized during the training process only.\n\n---\n\n**Q3**: The similarity between the consistency loss and the case where each head is trained on all the domains with a weighted combination of the losses of all the domains. Would this loss also help the learning of domain-specific models?\n\n**A3**: Yes, this loss is similar to our consistency loss and would help the learning of domain-specific models. We evaluate the performance of D$^3$G with this loss, and present the results in Table R1.\n\n**Table R1**: Performance comparison between D$^3$G and its variant with losses from all the domains.\n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         D$^3$G with losses from all the domains        |   71.8 $\\pm$ 1.8%   |     0.367 $\\pm$ 0.041     |     0.249 $\\pm$ 0.037     |   27.99 $\\pm$ 0.67%   |   38.75 $\\pm$ 0.51%   |   78.40 $\\pm$ 0.11%   |   76.52 $\\pm$ 0.49%   |\n| **D$^3$G(Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nThe above results show that D$^3$G, utilizing weighted losses mentioned by Reviewer u1oN, performs well across all datasets since our original two-term loss (predictive loss + consistency loss) approximates a weighted loss form. Our original loss essentially upweights the corresponding domain of each head more significantly using a separate predictive loss term, leading to more improvement over the new variant."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551716098,
                "cdate": 1700551716098,
                "tmdate": 1700574262966,
                "mdate": 1700574262966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wLEmPApwA0",
                "forum": "Dc4rXq3HIA",
                "replyto": "NWahxoHWQj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u1oN (2/2)"
                    },
                    "comment": {
                        "value": "**Q4**: Subquestion 1: Suppose that each head is trained on all the domains by the loss function (2). In this case, regardless of the values of $a_{ij}$ if both the supervised loss and consistency loss reach their minimum, then the learning of $a_{ij}$would be useless. Does the proposed method face this issue?\n\nSubquestion 2: Results of a new baseline: training each head on all the training domains by supervised loss and averaging the predictions of these heads as the test prediction.\n\n**A4**: If each head is trained on all domains and results in minimal predictive loss and consistency loss, it implies that all domains are equally important in this problem, and it suggests that all $a_{ij}$ are identical. This can be considered a special case of our developed method. However, in real-world scenarios, encountering this situation is rare. This issue can be verified through the following experiment:\n\nAs suggested by the reviewer u1oN, we train each head on all the training domains using supervised loss and average the predictions of these heads as the test prediction. This experiment has been conducted in our original submission (see the first row in Table 3 in our paper, where no relations are used). We also present the corresponding results in Table R2.\n\n**Table R2**: Performance comparison between D$^3$G and its variant with no related domains.\n\n\n|      |   FMoW (Worst Acc. $\\uparrow$)    |                                     | ChEMBL-STRING (ROC-AUC $\\uparrow$) |                       |\n|:-----------------|:----------:|:----------:|:---------:|:---------:|\n|                  | FMoW-Asia | FMoW-WILDS | PPI>50 |        PPI>100        |\n| D3G with no related domains |   26.93 $\\pm$ 0.47%   |          35.32 $\\pm$ 0.66%          |        76.17 $\\pm$ 0.21%        |   73.38 $\\pm$ 0.13%   |\n| **D3G (Ours)** | **28.12 $\\pm$ 0.28%** |        **39.47 $\\pm$ 0.57%**        |      **78.67 $\\pm$ 0.16%**      | **77.24 $\\pm$ 0.30%** |\n\nThe results reveal that this baseline slightly outperforms ERM, yet significantly underperforms compared to our D$^3$G. This indicates the importance of using uneven domain relation to weight losses and predictions from different domains/heads."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554598472,
                "cdate": 1700554598472,
                "tmdate": 1700554651530,
                "mdate": 1700554651530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WNXMhKAWgA",
                "forum": "Dc4rXq3HIA",
                "replyto": "wLEmPApwA0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_u1oN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_u1oN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I keep the current score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716144796,
                "cdate": 1700716144796,
                "tmdate": 1700716144796,
                "mdate": 1700716144796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l3l6APVdfp",
            "forum": "Dc4rXq3HIA",
            "replyto": "Dc4rXq3HIA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_Htjr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_Htjr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new approach for out-of-distribution generalization named as D$^3$G. It leverages domain relations estimated from domain metadata to learn training domain specific models and the ensemble of training-domain-specific functions. Under some assumptions, for example the domain relations can accurately capture the model similarity between domains, the authors proved that the proposed method can generalize better to out-of-domain samples compared to the traditional averaging approach. Empirical results on both synthetic and real-world datasets show that D$^3$G surpassing the performance of traditional averaging methods and some other baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality: good. Though learning domain specific classification head has been proposed in exiting works, it is novel to ensemble domain specific classifiers by weights learned from domain meta-data.\n- Significance: the proposed method is simple but shown improved performance on several tasks.\n- The paper is overall well written and easy to follow.\n- Experiment setups are introduced in detail."
                },
                "weaknesses": {
                    "value": "- The robustness and generality of the proposed method is unclear. Practically, it is unclear how to design a good similarity definition for metadata of different tasks. The current way of constructing the fixed relations requires specific and expertise knowledge on each task. Nevertheless a learning approach is proposed, the ablation study results in Table 10 show that the learned relations can be less helpful on some tasks.\n- The limitations of this work are not fully discussed. For example, the generality of the assumptions in the theoretical analysis part."
                },
                "questions": {
                    "value": "Please refer to the concerns in the \"weaknesses\" part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836917774,
            "cdate": 1698836917774,
            "tmdate": 1699636136291,
            "mdate": 1699636136291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w0wSJHbGw5",
                "forum": "Dc4rXq3HIA",
                "replyto": "l3l6APVdfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Htjr"
                    },
                    "comment": {
                        "value": "Thank you so much for the insightful and valuable comments! They are very helpful for further improving the clarity and quality of our paper. We'll revise our manuscript in the next version to address all of your concerns.\n\n**Q1**: The robustness and generality of the proposed method is unclear. Practically, it is unclear how to design a good similarity definition for metadata of different tasks.\n\n**A1**: In our experiments, we use cosine similarity for DG-15, TPT-48, and FMoW, and apply PPI scores for biomedical data. Consequently, cosine similarity serves as a reliable backup for defining similarity when domain meta-data is featured.\n\nIn situations where domain meta-data is discrete (e.g., label or index) and cosine similarity cannot be applied, we can directly learn domain relations directly from the data. To do this, we compute the fixed relation across different domains by applying optimal transport on the corresponding data [1]. For the learnable part, we utilize the average of features from each domain as input to the two-layer neural network $g$. We report the results in Table R1. We\u2019ve added these results in Appendix H.3 in the updated paper.\n\n**Table R1**: Performance comparison between D$^3$G and its variant that inly utilize domain ids, along with reporting the best baseline performance for each dataset.\n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         Best baseline        |   66.2 $\\pm$ 2.0%   |     0.371 $\\pm$ 0.054     |     0.262 $\\pm$ 0.034     |   26.99 $\\pm$ 1.27%   |   37.64 $\\pm$ 0.92%   | 75.42 $\\pm$ 0.42% | 73.57 $\\pm$ 0.21% |\n|         D$^3$G w/o domain meta-data        |   70.2 $\\pm$ 3.7%   |     0.381 $\\pm$ 0.071     |     0.325 $\\pm$ 0.029     |   27.17 $\\pm$ 1.09%   |   37.82 $\\pm$ 0.31%   |   76.65 $\\pm$ 0.22%   |   75.01 $\\pm$ 0.36%   |\n| **D$^3$G (Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nThe findings indicate that this variant exhibits inferior performance compared to the one with domain meta-data, however, it still outperforms the best baselines on most datasets. These outcomes suggest that we can also design a relatively good similarity definition for meta-data of different tasks, even with only labels or indices to distinguish different domains. This further confirms the generality of D$^3$G.\n\n---\n\n**Q2**: The limitations of this work are not fully discussed. For example, the generality of the assumptions in the theoretical analysis part.\n\n**A2**: We\u2019ve discussed the limitations of our work in our original submission (see Appendix I). We\u2019ve added the reference in the conclusion (see \u201cWe discussed our limitations in Appendix I\u201d).\n\n---\n\n**References**:\n\n[1] David Alvarez-Melis and Nicol\u00f2 Fusi. Geometric dataset distances via optimal transport. ArXiv, abs/2002.02923, 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548492869,
                "cdate": 1700548492869,
                "tmdate": 1700549075359,
                "mdate": 1700549075359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kfK9S6DJPS",
            "forum": "Dc4rXq3HIA",
            "replyto": "Dc4rXq3HIA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel method called D3G for tackling the issue of domain shifts in real-world machine learning scenarios. The approach leverages the connections between different domains to enhance the model\u2019s robustness and employs a domain-relationship aware weighting system for each test domain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors provide a theoretical proof that using domain relations to reweight a specific function of the training domain can obtain stronger extraterritorial generalization."
                },
                "weaknesses": {
                    "value": "The novelty is limited, perhaps the authors have not sufficiently explained the differences between their work and existing methods.  \n\nThe authors state that \" Unlike prior works that rely on ensemble models to address the underspecification problem and improve out-of-distribution robustness, our proposed D3G takes a conceptually different approach by constructing domain-specific models.\" However, the  MoE[1] constructed domain-specific models and they adopted ensemble models to improve out-of-distribution robustness.\n\nThe meta-data is key of domain-relation in this work. However, there is no clear definition of meta-data, how to obtain meta-data, and why the meta-data work is lack of detailed elaboration and analysis.\n\nHow the consistency loss address the challenge of limited training data in certain domains?\n\nThe organizational of the method section needs to be adjusted. The description and calculation of the domain relationship \"a\" in advance make the method part more clear.\n\nIt is recommended to add the venue and year of the comparison method to the table in the experiment section.\n\n\n[1]Dai, Yongxing, et al. \"Generalizable person re-identification with relevance-aware mixture of experts.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW",
                        "ICLR.cc/2024/Conference/Submission2047/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843762258,
            "cdate": 1698843762258,
            "tmdate": 1700553397471,
            "mdate": 1700553397471,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pGItRRhEnO",
                "forum": "Dc4rXq3HIA",
                "replyto": "kfK9S6DJPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 79WW (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. We have carefully revised our paper based on your comments. Our responses to your questions are detailed below. We would greatly appreciate your input on whether our revisions address your concerns.\n\n**Q1**: The novelty is limited, perhaps the authors have not sufficiently explained the differences between their work and existing methods.\n\n**A1**: We appreciate the reviewer's concern regarding the novelty of our method in comparison with existing methods. While it is true that training specific models for each domain and combining them is similar to the mixture-of-experts approach, the major contribution of our work is the use of domain meta-data to generate more accurate domain relations. This is crucial since it is challenging to directly extract domain specific information and learn domain relational information from training samples in real-world applications. We also introduce a consistency loss to mitigate the impact of data size imbalance across domains. Both theoretical and empirical results demonstrate the effectiveness of our approach. A detailed comparison between D$^3$G and MoE is presented in the response (A2) of your question 2 (Q2).\n\nIn general, our D$^3$G pioneers the importance of accurate domain relations for improved generalization performance. Instead, though mentioning domain relation/distance/similarity, these previous methods focus on learning domain-specific functions and devote less effort to acquiring accurate relations. As a result, their learned weight does not reflect accurate domain relations, leading to a worse performance. Unlike these, we tackle this challenge by incorporating domain meta-data, which is a fresh perspective that sets our work apart from existing literature.\n\n---\n\n**Q2**: Comparison with MoE[1]\n\n**A2**: Our proposed method shares some similarities with multi-head network structures in MoE [1], but our D$^3$G can generate more accurate domain relations by using domain meta-data. To evaluate the performance of our method, we compared it with MoE on all datasets. The result is presented in Table R1, and we\u2019ve added these results in Figure 2 and Table 1 in the updated paper.. According to the results, we observe that D$^3$G still outperforms MoE and shows its effectiveness. We've included these results in our main paper in the updated version. Besides MoE, in our paper, we also compare several multi-head models without using domain meta-data such as DRM [2], LLE [3], and DDN [4]. Please refer to the paper for these results.\n\n**Table R1**: Performance comparison between D$^3$G and MoE on all datasets. \n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n|:-----------------|:----------:|:----------:|:---------:|:---------:|:----------------:|:----------------:|:----------------:|\n|                  |                       | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |    FMoW-Asia     |  FMoW-WILDS  |         PPI>50         |   PPI>100    |\n| MoE              |   53.7 $\\pm$ 1.9%   |     0.372 $\\pm$ 0.035     |     0.311 $\\pm$ 0.060     |   26.65 $\\pm$ 0.46%   |   36.51 $\\pm$ 0.71%   |   74.99 $\\pm$ 0.22%   |   71.48 $\\pm$ 0.49%   |\n| **D$^3$G (Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\n---\n\n**Q3**: The meta-data is the key of domain-relation in this work. However, there is no clear definition of meta-data, how to obtain meta-data, and why the meta-data work is lacking detailed elaboration and analysis.\n\n**A3**: The domain meta-data can be collected from various sources, depending on the application. For example, in applications related to spatial distribution shift (e.g., land usage prediction in FMoW), geographical information such as longitude and latitude can be used as domain relations. In the context of drug discovery, where different proteins are treated as different domains, we can use protein structures as domain meta-data. Since the domain meta-data includes abundant domain information, it can be used to accurately measure domain relations, thereby enabling more precise domain generalization and knowledge transfer from training domains to test domains."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547957472,
                "cdate": 1700547957472,
                "tmdate": 1700552858919,
                "mdate": 1700552858919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tniYqCTC1Q",
                "forum": "Dc4rXq3HIA",
                "replyto": "kfK9S6DJPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 79WW (2/2)"
                    },
                    "comment": {
                        "value": "**Q4**: How does consistency loss address the challenge of limited training data in certain domains?\n\n\n**A4**: The consistency loss is utilized to establish connections between domains and facilitate training on data insufficient domains. Throughout the training process, for all examples from domain $d$, we use these examples and employ the overall loss not only to update its domain-specific function $f^{(d)}$ but also to influence other domain-specific functions with the consistency loss. Consequently, the utilization of data from one domain effectively updates all parameters, implying that domain-specific models in data-scarce domains are optimized not only by their respective domain data but also by other domains, particularly those that are similar. For example, if training domain $d$ is more similar to training domain $d-1$ than to domain $d+1$, it will have a stronger influence on the optimization of the function $f^{(d-1)}$.\n\n---\n\n**References**:\n\n[1] Dai, Yongxing, et al. \"Generalizable person re-identification with relevance-aware mixture of experts.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[2] Yi-Fan Zhang, Han Zhang, Jindong Wang, Zhang Zhang, B. Yu, Liangdao Wang, Dacheng Tao, and Xingxu Xie. Domain-specific risk minimization. ArXiv, abs/2208.08661, 2022b.\n\n[3] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim. A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others. June 2023. URL https://arxiv.org/abs/2212.04825.\n\n[4] Daoan Zhang, Mingkai Chen, Chenming Li, Lingyun Huang, and Jianguo Zhang. Aggregation of disentanglement: Reconsidering domain variations in domain generalization. ArXiv, abs/2302.02350, 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548227979,
                "cdate": 1700548227979,
                "tmdate": 1700548414902,
                "mdate": 1700548414902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VrHzeSZIxP",
                "forum": "Dc4rXq3HIA",
                "replyto": "pGItRRhEnO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have addressed some of my concerns in their responses. Additionally, I would like to inquire whether this metadata is akin to the introduction of prior knowledge. In the context of various tasks, is the metadata extracted automatically by the algorithm, or does it require manual intervention?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549416698,
                "cdate": 1700549416698,
                "tmdate": 1700549416698,
                "mdate": 1700549416698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A6x0ZONGBR",
                "forum": "Dc4rXq3HIA",
                "replyto": "kfK9S6DJPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Response to Reviewer 79WW"
                    },
                    "comment": {
                        "value": "Dear Reviewer 79WW,\n\nThank you very much for your prompt response.\n\nYes, the metadata is akin to the prior knowledge, which is not extracted by the algorithm. Most often, domain meta-data is obtained as a by-product during the collection of test data, which then serves as the primary criteria for classifying domains. This meta-data typically manifests as an inherent property of the testing domain, such as latitude and longitude in geographic data or PPI scores in biomedical data. In most scenarios, obtaining meta-data for each testing domain is not overly expensive since the number of testing domains is relatively small.\n\nNevertheless, In situations where domain metadata and the domain similarity function are unavailable, we can derive domain distances directly from the data. To do this, we compute the fixed relation using optimal transport [1]. For the learnable part, we utilize the average of features from each domain as input to the two-layer neural network $g$. We report the results in Table R2. \n\n**Table R2**: Performance comparison between D$^3$G and its variant that does not utilize domain meta-data, along with reporting the best baseline performance for each dataset.\n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         Best baseline        |   66.2 $\\pm$ 2.0%   |     0.371 $\\pm$ 0.054     |     0.262 $\\pm$ 0.034     |   26.99 $\\pm$ 1.27%   |   37.64 $\\pm$ 0.92%   | 75.42 $\\pm$ 0.42% | 73.57 $\\pm$ 0.21% |\n|         D$^3$G w/o domain meta-data        |   70.2 $\\pm$ 3.7%   |     0.381 $\\pm$ 0.071     |     0.325 $\\pm$ 0.029     |   27.17 $\\pm$ 1.09%   |   37.82 $\\pm$ 0.31%   |   76.65 $\\pm$ 0.22%   |   75.01 $\\pm$ 0.36%   |\n| **D$^3$G (Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nThe findings indicate that this variant exhibits inferior performance compared to the one with domain meta-data, however, it still outperforms the best baselines on most datasets. These outcomes suggest that D$^3$G's model design inherently enhances generalization capabilities, which are further augmented by the use of domain meta-data. We\u2019ve added these results in Appendix H.4 in the updated paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552814555,
                "cdate": 1700552814555,
                "tmdate": 1700552878608,
                "mdate": 1700552878608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B4y0dDyfBg",
                "forum": "Dc4rXq3HIA",
                "replyto": "A6x0ZONGBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_79WW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply, I will raise my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553379081,
                "cdate": 1700553379081,
                "tmdate": 1700553379081,
                "mdate": 1700553379081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O0oANCPFmC",
            "forum": "Dc4rXq3HIA",
            "replyto": "Dc4rXq3HIA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_bK1N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_bK1N"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the domain shift problem and proposes a new approach called D3G to this problem. Instead of learning a single model from multiple source domains, the proposed method learns domain-specific models and infer a test domain specific model by exploiting the domain relations. The test domain model is a weighted combination of multiple source domain models whilst the weights are learned from the domain metadata. Theoretic and empirical analyses have been made and experimental results demonstrate the superiority of the proposed approach to many state-of-the-art counterparts."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "-- The approach distincts from most existing ones in that it learns domain-specific models instead of a unified model. \n\n-- The method is clearly presented and theoretic analysis has been made to clarify why it should work for domain generalization.\n\n-- The proposed approach is applicable to many real-world applications as demonstrated in the experiments. The superior performance makes a significant difference to practical problems of this type.\n\n-- Ablation studies have been conducted to demonstrate the effectiveness of each component of the proposed approach."
                },
                "weaknesses": {
                    "value": "-- The comparison of the proposed method with ensemble models of existing approaches should have been given.\n\n-- There exist some typos/language issues, e.g., \"Eqn. equation x...\"; \"We using weighted...\";"
                },
                "questions": {
                    "value": "1. Does it work when there is no domain metadata available? Can such domain relations be learned from the data themselves?\n2. How the domain metadata are used in the comparative methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936121273,
            "cdate": 1698936121273,
            "tmdate": 1699636136140,
            "mdate": 1699636136140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmnfCe9YDH",
                "forum": "Dc4rXq3HIA",
                "replyto": "O0oANCPFmC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bK1N (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions. We have revised our paper according to your comments. We respond to your questions below and would appreciate it if you could let us know if our response addresses your concerns.\n\n**Q1**: The comparison of the proposed method with ensemble models of existing approaches should have been given.\n\n**A1**: Our proposed method shares some similarities with ensemble methods, yet it is more closely aligned with multi-head network structures. As illustrated in Figure 1 in the paper, our approach employs a shared feature extractor and learns a collection of domain-specific heads. Consequently, in our paper, we compare several multi-head models without using domain meta-data, such as DRM [1], LLE [2], and DDN [3]. Our method consistently outperforms these baselines, highlighting the significant role of domain meta-data in generating accurate domain relations and enhancing generalization performance. \n\nFurthermore, we have also conducted an additional comparison between D$^3$G and one variant, which employs separate feature extractors for each training domain. The results are reported in Table R1 here.\n\n**Table R1**: Performance comparison between D$^3$G and using separate feature extractors for each training domain.\n\n|                  |  DG-15 (Avg. Acc. $\\uparrow$)   |        TPT-48 (MSE $\\downarrow$)        |                           |   FMoW (Worst Acc. $\\uparrow$)    |                       | ChEMBL-STRING (ROC-AUC $\\uparrow$) |                       |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         Using separate feature extractors        |   47.3 $\\pm$ 3.7%   |     0.563 $\\pm$ 0.071     |     0.519 $\\pm$ 0.076     |   23.62 $\\pm$ 0.47%   |   35.44 $\\pm$ 0.58%   |   76.11 $\\pm$ 0.31%    |   74.19 $\\pm$ 0.74%   |\n| **D$^3$G(Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nAccording to the results, D$^3$G shows superior performance compared to using separate feature extractors for each training domain. This underscores the importance of employing a shared feature extractor to learn a universal representation, while domain-specific heads can further identify domain-specific features. Additionally, our D$^3$G model demonstrates superior efficiency compared to this variant since it utilizes a shared feature extractor while the variant needs $N^{tr}$ feature extractors. We\u2019ve added these results in Appendix H.3 in the updated paper.\n\n\n---\n\n**Q2**: There exist some typos/language issues, e.g., \"Eqn. equation x...\"; \"We using weighted...\"\n\n**A2**: Thanks for pointing that out. We\u2019ve fixed these issues in the updated paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547633049,
                "cdate": 1700547633049,
                "tmdate": 1700548682008,
                "mdate": 1700548682008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n9W1xCmhwP",
                "forum": "Dc4rXq3HIA",
                "replyto": "O0oANCPFmC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bK1N (2/2)"
                    },
                    "comment": {
                        "value": "**Q3**: Does it work when there is no domain metadata available? Can such domain relations be learned from the data themselves?\n\n**A3**: Yes, we do an ablation study for this question by learning domain relations directly from the data. To do this, we compute the fixed relation using optimal transport [4]. For the learnable part, we utilize the average of features from each domain as input to the two-layer neural network $g$. We report the results in Table R2. \n\n**Table R2**: Performance comparison between D$^3$G and its variant that does not utilize domain meta-data, along with reporting the best baseline performance for each dataset.\n\n|                  |  DG-15 (Avg. Acc. $\\uparrow$)   |        TPT-48 (MSE $\\downarrow$)        |                           |   FMoW (Worst Acc. $\\uparrow$)    |                       | ChEMBL-STRING (ROC-AUC $\\uparrow$) |                       |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         Best baseline        |   66.2 $\\pm$ 2.0%   |     0.371 $\\pm$ 0.054     |     0.262 $\\pm$ 0.034     |   26.99 $\\pm$ 1.27%   |   37.64 $\\pm$ 0.92%   | 75.42 $\\pm$ 0.42% | 73.57 $\\pm$ 0.21% |\n|         D$^3$G w/o domain meta-data        |   70.2 $\\pm$ 3.7%   |     0.381 $\\pm$ 0.071     |     0.325 $\\pm$ 0.029     |   27.17 $\\pm$ 1.09%   |   37.82 $\\pm$ 0.31%   |   76.65 $\\pm$ 0.22%   |   75.01 $\\pm$ 0.36%   |\n| **D$^3$G(Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nThe findings indicate that this variant exhibits inferior performance compared to the one with domain meta-data, however, it still outperforms the best baselines on most datasets. These outcomes suggest that D$^3$G's model design inherently enhances generalization capabilities, which are further augmented by the use of domain meta-data. We\u2019ve added these results in Appendix H.4 in the updated paper.\n\n---\n\n**Q4**: How are the domain metadata used in the comparative methods?\n\n**A4**: As we discussed in the caption of Table 1 (page 8) and in the first paragraph of experiment section, for all baselines, we incorporate domain meta-data as\nfeatures during the training and test stages for fair comparison. In detail, we begin by converting domain meta-data into input embeddings, which are then passed through the two-layer neural network. Subsequently, we concatenate this intermediate domain feature with other features and pass this concat feature through another feature extractor. This procedure is consistently applied to all baselines.\n\n---\n\n**References**:\n\n[1] Yi-Fan Zhang, Han Zhang, Jindong Wang, Zhang Zhang, B. Yu, Liangdao Wang, Dacheng Tao, and Xingxu Xie. Domain-specific risk minimization. ArXiv, abs/2208.08661, 2022b.\n\n[2] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim. A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others. June 2023. URL https://arxiv.org/abs/2212.04825.\n\n[3] Daoan Zhang, Mingkai Chen, Chenming Li, Lingyun Huang, and Jianguo Zhang. Aggregation of disentanglement: Reconsidering domain variations in domain generalization. ArXiv, abs/2302.02350, 2023.\n\n[4] David Alvarez-Melis and Nicol\u00f2 Fusi. Geometric dataset distances via optimal transport. ArXiv, abs/2002.02923, 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547764647,
                "cdate": 1700547764647,
                "tmdate": 1700548805934,
                "mdate": 1700548805934,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7rjNHAyVhY",
            "forum": "Dc4rXq3HIA",
            "replyto": "Dc4rXq3HIA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_2jGa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_2jGa"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of out-of-domain generalization of discriminative models. The presented approach leverages domain relations and domain-specific meta-data to adapt to new domains at test time. Domain similarities are determined from a combination of a user-defined function and a learned function of the domain meta-data. The domain similarities are used to weight the different neural network heads in a mixture-of-experts fashion. At test time, the domain similarities are used to weight the output of all of the training set domain-specific heads to produce a prediction. Theoretical results provide justification for the proposed approach. Empirical results are shown on open source toy and real-world datasets, including several useful ablations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The motivation for the paper is strong, in a relevant area.\n* The paper is well-written.\n* The empirical results are very strong and well executed, including a large number of baselines. The ablation studies are beneficial, answering fundamental questions about the method."
                },
                "weaknesses": {
                    "value": "* The method requires a user-defined domain similarity function based on meta-data. This might not always be available. Additionally, for specific applications, a user might not be able to credibly define such a function.\n* The theoretical work makes (understandably) quite a few strong assumptions that are unlikely to be true in practice. These results could be improved to show how performance is affected by, for example, noisy domain relations."
                },
                "questions": {
                    "value": "* Is it possible for the proposed method to utilize negative relationships between domains?\n* How will the proposed method behave in a situation where there are no related domains according to the meta-data? Will the method perform as well as a domain-invariant approach? Is this what the consistency loss is for?\n* If domain information was unavailable, could a domain discovery method (e.g. clustering) be used to learn domains and domain relations from unlabeled data? In the limit, could the model architecture and inference algorithm be adapted to learn domains and domain relations end-to-end?\n* Could this method be used in data impoverished applications to improve performance? \n\nMinor typos:\n* Equations are referenced with \"Eqn. equation #\", maybe just \"(#)\" or \"Equation (#)\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699105739723,
            "cdate": 1699105739723,
            "tmdate": 1699636136061,
            "mdate": 1699636136061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rF3T1HkGuV",
                "forum": "Dc4rXq3HIA",
                "replyto": "7rjNHAyVhY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2jGa (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and for your valuable feedback. Below, we address your concerns point by point and we\u2019ve revised our paper according to your suggestions. We would appreciate it if you could let us know whether your concerns are addressed by our response.\n\n**Q1**: Availability of domain meta-data and domain similarity function.\n\n**A1**: Most often, domain meta-data is obtained as a by-product during the collection of data, which then serves as the primary criteria for classifying domains. This meta-data typically manifests as an inherent property of the domain, such as latitude and longitude in geographic data or PPI scores in biomedical data.  Furthermore, obtaining meta-data for each testing domain is also not overly expensive since the number of testing domains is relatively small in most scenarios. As for the domain similarity function, we employ cosine similarity across all datasets, except in the case of biomedical data with PPI scores. Therefore, when we do not have specific metrics in certain fields, the alternative is to choose from a range of general distance metrics.\n\nIn situations where domain metadata and the domain similarity function are unavailable, we can derive domain distances directly from the data. To do this, we compute the fixed relation using optimal transport [1]. For the learnable part, we utilize the average of features from each domain as input to the two-layer neural network $g$. We report the results in Table R1. \n\n**Table R1**: Performance comparison between D$^3$G and its variant that does not utilize domain meta-data, along with reporting the best baseline performance for each dataset.\n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         Best baseline        |   66.2 $\\pm$ 2.0%   |     0.371 $\\pm$ 0.054     |     0.262 $\\pm$ 0.034     |   26.99 $\\pm$ 1.27%   |   37.64 $\\pm$ 0.92%   | 75.42 $\\pm$ 0.42% | 73.57 $\\pm$ 0.21% |\n|         D$^3$G w/o domain meta-data        |   70.2 $\\pm$ 3.7%   |     0.381 $\\pm$ 0.071     |     0.325 $\\pm$ 0.029     |   27.17 $\\pm$ 1.09%   |   37.82 $\\pm$ 0.31%   |   76.65 $\\pm$ 0.22%   |   75.01 $\\pm$ 0.36%   |\n| **D$^3$G(Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nThe findings indicate that this variant exhibits inferior performance compared to the one with domain meta-data, however, it still outperforms the best baselines on most datasets. These outcomes suggest that D$^3$G's model design inherently enhances generalization capabilities, which are further augmented by the use of domain meta-data. We\u2019ve added these results in Appendix H.4 in the updated paper.\n\n---\n\n**Q2**: The theoretical work makes (understandably) quite a few strong assumptions that are unlikely to be true in practice. These results could be improved to show how performance is affected by, for example, noisy domain relations.\n\n**A2**:  Thank you for your suggestion. Our results can be extended to noisy domain relations by incorporating technical tools from the error-in-variable literature. Specifically, in this situation, our assumption will be extended to $\\|h^{(i)}-h^{(j)}\\|_\\infty$ $\\le G\\cdot$ $\\|Z^{(i)}-Z^{(j)}\\|$, while our estimated representation $\\hat{Z}^{(i)}=Z^{(i)}+\\epsilon_i$ includes a zero-mean sub-Gaussian noise term $\\epsilon_i$. Then, we can use technical tools from error-in-variable literature to denoise it, we will leave it as a future work.\n\nWe acknowledge that our theoretical analysis relies on some assumptions. Nevertheless, we still believe that our current theory provides valuable insights supporting the idea that utilizing good domain relations leads to better generalization. In fact, our proof suggests that using domain relations can help reduce variance while sacrificing a slight amount of bias, resulting in a more favorable bias-variance trade-off."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547254416,
                "cdate": 1700547254416,
                "tmdate": 1700549999914,
                "mdate": 1700549999914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NjNCSkk9Wc",
                "forum": "Dc4rXq3HIA",
                "replyto": "7rjNHAyVhY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2jGa (2/3)"
                    },
                    "comment": {
                        "value": "**Q3**: Is it possible for the proposed method to utilize negative relationships between domains?\n\n**A3**: Yes, the model can handle and utilize negative domain relations. If our understanding is correct, for a pair of domains $i$ and $j$, the existence of negative domain relations implies that updating domain $j$'s corresponding parameters with data from domain $i$ would negatively impact the performance of domain $j$. In such cases, if the fixed relation indicates these negative relationships or if the learned relation does so, we will set the domain relation $a_{ij}=0$ to prevent information propagation between these two domains. \n\n---\n\n**Q4**: How will the proposed method behave in a situation where there are no related domains according to the meta-data? Will the method perform as well as a domain-invariant approach? Is this what the consistency loss is for?\n\n**A4**: In situations where there are no related domains according to the meta-data, we will use a uniform domain relation, implying that all $a_ij$ are identical. During the training process, we emphasize the data from each head's corresponding domain with predictive loss, while applying uniform weights to other training domains in the calculation of consistency loss. During inference, our method performs comparably to a domain-invariant approach by averaging the outputs from each head. We analyze this situation in the first row of Table 3 in our paper, with the corresponding results also detailed in Table R2.\n\n**Table R2**: Performance comparison between D$^3$G and its variant with no related domains.\n\n\n|      |   FMoW (Worst Acc. $\\uparrow$)    |                                     | ChEMBL-STRING (ROC-AUC $\\uparrow$) |                       |\n|:-----------------|:----------:|:----------:|:---------:|:---------:|\n|                  | FMoW-Asia | FMoW-WILDS | PPI>50 |        PPI>100        |\n| D3G with no related domains |   26.93 $\\pm$ 0.47%   |          35.32 $\\pm$ 0.66%          |        76.17 $\\pm$ 0.21%        |   73.38 $\\pm$ 0.13%   |\n| **D3G (Ours)** | **28.12 $\\pm$ 0.28%** |        **39.47 $\\pm$ 0.57%**        |      **78.67 $\\pm$ 0.16%**      | **77.24 $\\pm$ 0.30%** |\n\nThe results reveal that without using domain relation, we can slightly outperform ERM, yet significantly underperforms compared to our D$^3$G. This indicates the importance of using accurate domain relation to weight losses and predictions from different domains/heads.\n\n---\n\n**Q5**: If domain information was unavailable, could a domain discovery method (e.g. clustering) be used to learn domains and domain relations from unlabeled data? In the limit, could the model architecture and inference algorithm be adapted to learn domains and domain relations end-to-end?\n\n**A5**: Yes, we do an ablation study for this question by clustering all training data into several groups, with the number of groups matching the actual number of domains. Each group is treated as a separate domain, and domain relations are established based on the distances between the clustering centers. The results, as presented in Table R3, show that by clustering data to learn domains, our method's variant surpasses ERM in 5 out of 7 settings, although still lags behind D$^3$G. This indicates that the model architecture and inference algorithm can somehow be adapted to learn domains and domain relations simultaneously in an end-to-end manner. Moreover, the appropriate use of domain meta-data and domain relation will then significantly improve the performance. We\u2019ve added these results in Appendix H.4 in the updated paper.\n\n**Table R3**: Performance comparison between ERM, D$^3$G and its variant that does not utilize domain information.\n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n| :------------------------- | :-----------------: | :-----------------------: | :----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                            |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n| ERM                        |   44.0 $\\pm$ 4.6%   |     0.445 $\\pm$ 0.029     |    0.328 $\\pm$ 0.033     |   26.05 $\\pm$ 3.84%   |   34.87 $\\pm$ 0.41%   |   74.11 $\\pm$ 0.35%    |   71.91 $\\pm$ 0.24%   |\n| D$^3$G w/o domain split information |   58.1 $\\pm$ 4.1%   |     0.429 $\\pm$ 0.055     |    0.340 $\\pm$ 0.095     |   25.46 $\\pm$ 1.35%   |   36.06 $\\pm$ 0.39%   |   74.94 $\\pm$ 0.79%    |   73.38 $\\pm$ 0.91%   |\n| **D$^3$G(Ours)**              | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |  **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550496213,
                "cdate": 1700550496213,
                "tmdate": 1700550804038,
                "mdate": 1700550804038,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SPDqJZ3YEB",
                "forum": "Dc4rXq3HIA",
                "replyto": "kGSgOkLpqC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_2jGa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_2jGa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I will be keeping my score as it stands."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593968655,
                "cdate": 1700593968655,
                "tmdate": 1700593968655,
                "mdate": 1700593968655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SO7ufX8SDm",
            "forum": "Dc4rXq3HIA",
            "replyto": "Dc4rXq3HIA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_nUD5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2047/Reviewer_nUD5"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to solve the multi-source multi-target domain generalization problem using domain relations. The authors claim that existing single domain-invariant or multiple domain-specific models that leverage equal weights on all domains fail to capture appropriate domain-specific correlations. To tackle this problem, the authors extract domain relations from domain meta-data and design a relation-aware consistency regularizer to weight the training domain-specific functions. The weighted functions on source domains are then transferred to predictions on target domains. A theoretical analysis is provided to prove that the proposed approach can perform better than the models using equal domain weights."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is clearly written and organized. The underlying idea is straightforward and easy to follow.\n\n2. The authors provide a theoretical analysis that verifies that the proposed relation-aware consistency loss can achieve superior generalization performance compared to the approach of treating all training domains\nequally. \n\n3. The experimental evaluation is comprehensive and thorough. An illustrative toy task is designed to examine the proposed methods and many tables and figures are presented to analyze the results."
                },
                "weaknesses": {
                    "value": "1. The novelty of the proposed method may be somewhat limited as it bears similarities to ensemble methods, which also involve assigning varying weights to different domains. The essential distinctions between the proposed methods and ensemble methods should be clearly elucidated, and it is important to consider baseline models that incorporate ensemble methods for comparison.\n\n2. The evaluation metrics used in this paper are different from previous works conducted on the TPT-48, FMoW, and ChEMBL-STRING datasets. It is unclear whether the metrics used, such as MSE, are suitable and fair to be used to evaluate domain generalization models.\n\n3. As stated by the authors, the theoretical analysis of this paper relies on certain assumptions. It is questioning whether these assumptions reflect the real-world datasets. For example, the assumptions that the domain relations accurately capture the similarity between domains, and that they are determined solely by the distance between domain representations, may not always hold true in real-world datasets. In the proofs of the theorems, the authors only prove that the proposed estimator outperforms the equally weighted estimator in the minimax sense, but not a theoretically global superiority. Moreover, the theorems only reveal that uneven weights can be better than equal weights, which does not entirely support the key ideas of acquiring appropriate weights for different domains."
                },
                "questions": {
                    "value": "Please see the weakness points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2047/Reviewer_nUD5"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2047/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699234783601,
            "cdate": 1699234783601,
            "tmdate": 1699636135996,
            "mdate": 1699636135996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QmS4RXYPk9",
                "forum": "Dc4rXq3HIA",
                "replyto": "SO7ufX8SDm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nUD5 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you so much for the insightful and valuable comments! They are very helpful for further improving the clarity and quality of our paper. We'll revise our manuscript in the next version to address all of your concerns.\n\n**Q1**: Comparison between D$^3$G and ensemble methods.\n\n**A1**: We acknowledge that our proposed method shares some similarities with ensemble methods, yet it is more closely aligned with multi-head network structures. As illustrated in Figure 1 in the paper, our approach employs a shared feature extractor and learns a collection of domain-specific heads. Consequently, in our paper, we compare several multi-head models without using domain meta-data, such as DRM [1], LLE [2], and DDN [3]. Our method consistently outperforms these baselines, highlighting the significant role of domain meta-data in generating accurate domain relations and enhancing generalization performance. \n\nTo further address your concern, we have also conducted an additional comparison between D$^3$G and one variant, which employs separate feature extractors for each training domain. The results are reported in Table R1 here.\n\n**Table R1**: Performance comparison between D$^3$G and using separate feature extractors for each training domain.\n\n|      |   DG-15 (Avg. Acc. $\\uparrow$)    |        TPT-48 (MSE $\\downarrow$)        |                           | FMoW (Worst Acc. $\\uparrow$) |              | ChEMBL-STRING (ROC-AUC $\\uparrow$) |              |\n| :--------------- | :-----------------: | :-----------------------: | :-----------------------: | :-------------------: | :-------------------: | :--------------------: | :-------------------: |\n|                  |                     | N(24) $\\rightarrow$ S(24) | E(24) $\\rightarrow$ W(24) |       FMoW-Asia       |      FMoW-WILDS       |         PPI>50         |        PPI>100        |\n|         Using separate feature extractors        |   47.3 $\\pm$ 3.7%   |     0.563 $\\pm$ 0.071     |     0.519 $\\pm$ 0.076     |   23.62 $\\pm$ 0.47%   |   35.44 $\\pm$ 0.58%   |   76.11 $\\pm$ 0.31%    |   74.19 $\\pm$ 0.74%   |\n| **D$^3$G (Ours)** | **77.5 $\\pm$ 2.5%** |   **0.342 $\\pm$ 0.019**   |   **0.236 $\\pm$ 0.063**   | **28.12 $\\pm$ 0.28%** | **39.47 $\\pm$ 0.57%** | **78.67 $\\pm$ 0.16%**  | **77.24 $\\pm$ 0.30%** |\n\nAccording to the results, D$^3$G shows superior performance compared to using separate feature extractors for each training domain. This underscores the importance of employing a shared feature extractor to learn a universal representation, while domain-specific heads can further identify domain-specific features. Additionally, our D$^3$G model demonstrates superior efficiency compared to this variant since it utilizes a shared feature extractor while the variant needs $N^{tr}$ feature extractors. We\u2019ve added these results in Appendix H.3 in the updated paper.\n\n---\n\n**Q2**: Evaluation metrics on the TPT-48, FMoW, and ChEMBL-STRING datasets\n\n**A2**: In this paper, the evaluation metrics we use are those originally selected by the creators of the respective datasets: \n- TPT-48: we follow GRDA [4] and use MSE as our evaluation metric.\n- FMoW: we use worse accuracy as our evaluation metric following Wilds [5].\n- ChEMBL-STRING: we use ROC-AUC as our evaluation metric following SGNN-EBM [6].\n\n---\n \n**Q3**: As stated by the authors, the theoretical analysis of this paper relies on certain assumptions. It is questioning whether these assumptions reflect the real-world datasets. In the proofs of the theorems, the authors only prove that the proposed estimator outperforms the equally weighted estimator in the minimax sense, but not a theoretically global superiority. Moreover, the theorems only reveal that uneven weights can be better than equal weights, which does not entirely support the key ideas of acquiring appropriate weights for different domains.\n\n**A3**: Thank you for your comment. We acknowledge that our theoretical analysis relies on some assumptions. The aim of our theoretical analysis is to explain some phenomena distilled from our empirical study and to help us understand why D$^3$G can work. In the future, we plan to relax these assumptions.\n\nFurthermore, we would like to note that the comparison in the minimax sense is a common approach in machine learning and statistics literature. For example, you can find it in papers, such as meta-learning [7], reinforcement learning [8], and private learning [9, 10]. Regarding your second point about uneven weights, we want to address it with care. It appears there might be a potential misunderstanding. In our theory, we propose that weighting according to similarity (controlled by the parameter $B$) can yield better results than using equal weights, as opposed to using uneven weights."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546370345,
                "cdate": 1700546370345,
                "tmdate": 1700548571993,
                "mdate": 1700548571993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RA848BaJV1",
                "forum": "Dc4rXq3HIA",
                "replyto": "SO7ufX8SDm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nUD5 (2/2)"
                    },
                    "comment": {
                        "value": "**References**:\n\n[1] Zhang, Yi-Fan, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, Liang Wang, Dacheng Tao, and Xing Xie. \"Domain-Specific Risk Minimization for Domain Generalization.\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3409-3421. 2023.\n\n[2] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim. A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others. June 2023. URL https://arxiv.org/abs/2212.04825.\n\n[3] Daoan Zhang, Mingkai Chen, Chenming Li, Lingyun Huang, and Jianguo Zhang. Aggregation of disentanglement: Reconsidering domain variations in domain generalization. ArXiv, abs/2302.02350, 2023.\n\n[4] Zihao Xu, Hao He, Guang-He Lee, Yuyang Wang, and Hao Wang. Graph-relational domain adaptation. arXiv preprint:2202.03628, 2022.\n\n[5] Pang Wei Koh, Shiori Sagawa, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021b.\n\n[6] Shengchao Liu, Meng Qu, Zuobai Zhang, Huiyu Cai, and Jian Tang. Structured multi-task learning for molecular property prediction. In International Conference on Artificial Intelligence and Statistics, pages 8906\u20138920. PMLR, 2022.\n\n[7] Tripuraneni, Nilesh, Chi Jin, and Michael Jordan. \"Provable meta-learning of linear representations.\" In International Conference on Machine Learning, pp. 10434-10443. PMLR, 2021.\n\n[8] Li, Gen, Yuling Yan, Yuxin Chen, and Jianqing Fan. \"Minimax-optimal reward-agnostic exploration in reinforcement learning.\" arXiv preprint arXiv:2304.07278 (2023).\n\n[9] Duchi, John C., Michael I. Jordan, and Martin J. Wainwright. \"Minimax optimal procedures for locally private estimation.\" Journal of the American Statistical Association 113, no. 521 (2018): 182-201.\n\n[10] Asi, Hilal, and John C. Duchi. \"Near instance-optimality in differential privacy.\" arXiv preprint arXiv:2005.10630 (2020)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546396178,
                "cdate": 1700546396178,
                "tmdate": 1700546425983,
                "mdate": 1700546425983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mJNjoiXcjy",
                "forum": "Dc4rXq3HIA",
                "replyto": "SO7ufX8SDm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_nUD5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2047/Reviewer_nUD5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. I will be keeping my rating."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2047/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670879099,
                "cdate": 1700670879099,
                "tmdate": 1700670879099,
                "mdate": 1700670879099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]