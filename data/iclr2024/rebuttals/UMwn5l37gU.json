[
    {
        "title": "Non-uniform Noise Injection For Enhancing DNN Adversarial Robustness And Efficiency"
    },
    {
        "review": {
            "id": "jIoR9iqyBV",
            "forum": "UMwn5l37gU",
            "replyto": "UMwn5l37gU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_4ReP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_4ReP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel method to address the challenges of adversarial attacks and computational costs in Deep Neural Networks (DNNs). Unlike previous studies that uniformly inject noise for robustness, this method strategically applies non-uniform noise at each DNN layer to disrupt adversarial attacks while preserving essential neurons. Experimental results show that this approach effectively enhances both robustness and efficiency in various attack scenarios, model architectures, and datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method of finding core and non-core neurons while simultaneously using those values to introduce noise is a good approach, in my opinion."
                },
                "weaknesses": {
                    "value": "* It is stated that \"A neuron is regarded as essential if its approximation from \u02dcz is larger than the predefined threshold.\" However, I am unsure why this makes it an important neuron. \u02dcz is used to reduce the value error through the MSE of z, meaning that the larger the z value, the more it will affect the size of \u02dcz. Therefore, an explanation is needed on why neurons with larger \u02dcz values are considered important.\nThe experiment tested both top-K injection and N:M injection methods, but there was no experiment that simply added noise values to a random N% of neurons. Therefore, there is a lack of validity in claiming that this method is effective.\n\n* During the experimental process, in the experiment where noise was added to non-core neurons, I think there needs to be a control group that adds different noise to those neurons as a comparison to the approximated values used for detection. It is necessary to verify whether the method of injecting this noise actually contributes to performance improvement or simply enhances computational efficiency.\n\n* Too much limited and insufficient experiments: There are no state-of-the-art attack such AutoAttack [1] and no state-of-the-art defense baselines such as AWP [2], SCORE [3], and ADML [4]. In addition, based on ADML, not only CNN structure and Transformer structures should be needed to validate. \n\n---\nReferences\n\n[1] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.\n\n[2] Wu, Dongxian, Shu-Tao Xia, and Yisen Wang. \"Adversarial weight perturbation helps robust generalization.\" Advances in Neural Information Processing Systems 33 (2020): 2958-2969.\n\n[3] Pang, Tianyu, et al. \"Robustness and accuracy could be reconcilable by (proper) definition.\" International Conference on Machine Learning. PMLR, 2022.\n\n[4] Lee, Byung-Kwan, Junho Kim, and Yong Man Ro. \"Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                },
                "questions": {
                    "value": "Refer to Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698212042802,
            "cdate": 1698212042802,
            "tmdate": 1699636574147,
            "mdate": 1699636574147,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ND0Z0zE6VL",
                "forum": "UMwn5l37gU",
                "replyto": "jIoR9iqyBV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable suggestions and comments. Details can be found below.\n\n**(1) Why neurons with larger \u02dcz values are considered important.** \n\nIntuitively, activations with a larger magnitude have more impact on overall model inference; hence important. We use the approximate values as proxy to choose neurons with large magnitude.\n\n**(2) Add noise values to a random N\\% of neurons. And control group that adds different noise to those neurons as a comparison to the approximated values used for detection.**\n\nThis is part of the ablation experiment that we missed to support our claim more rigorously, and we will include this experiment in future enhancements.\n\n**(3) Insufficient experiments.**\n\nYour constructive comments are much appreciated. We will include a comparison of these robustness-related methods in future experiments."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624182665,
                "cdate": 1700624182665,
                "tmdate": 1700624182665,
                "mdate": 1700624182665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mniyIYZYwi",
            "forum": "UMwn5l37gU",
            "replyto": "UMwn5l37gU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_xfb1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_xfb1"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the use of a non-uniform noise injection method to improve adversarial robustness and efficiency. The proposed method first trains a linear layer-wise approximation of the layer's output and uses the approximation to determine which neurons are essential and which are not. A binary mask is used to replace the outputs of non-essential neurons with their approximations. The paper later proves that this process is unlikely to decrease clean accuracy. The improvements in adversarial robustness and computational efficiency are also discussed. The experiments further validate these claims."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-organized and easy to understand.\n2. The proposed method that uses non-uniform noise injection is novel and produces promising results.\n3. The discussion provides theoretical support for the validity of the proposed methods."
                },
                "weaknesses": {
                    "value": "1. The threshold and the proportion of non-essential neurons should also be reported. It is also helpful to provide the distribution of the difference between $z$ and $\\tilde{z}$, and study the relationship between accuracies and the choice of threshold.\n\n2. The theorem does not seem to directly link to the results. Based on the theorem, the clean accuracy can be higher than that without the noise injection, and it does not explain the consistent minor drop in clean accuracy."
                },
                "questions": {
                    "value": "1. The learning of $\\tilde{W}$ and $\\tilde{b}$ minimizes the MSE. Is it possible that the optimal learned solution is $W=\\tilde{W}P$ and $b=\\tilde{b}$? \n2. Are there any specific reasons that $\\epsilon=4/255$ in Table 1 while $\\epsilon=8/255$ in Table 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5578/Reviewer_xfb1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753504106,
            "cdate": 1698753504106,
            "tmdate": 1699636574059,
            "mdate": 1699636574059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qo7rlZcPz5",
                "forum": "UMwn5l37gU",
                "replyto": "mniyIYZYwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate you for the positive feedback and your recognition of our contribution. Your questions are truly insightful which merit further investigation. Our answers are listed as follows:\n\n**(1) The threshold and the proportion of non-essential neurons.**\n\nIt indeed was a writing mistake. The threshold is not predefined, it is found during the top-k process. This threshold is the smallest number in the top-k, and the purpose of the threshold is to facilitate the generation of the mask. We will revise the manuscript to avoid ambiguity.\n\n**(2) The distribution of the difference between z and ~z.**\n\nThank you for your valuable advice. We will take this into consideration in our future improvements to explore the underlying reasons and to support our approach even more.\n\n**(3) Theorem explanation.**\n\nOur theoretical proof is divided into two parts, the first part is to show that this type of noise injection is naturally brought about by the dimensionality reduction method **random projection**, which has no effect on the distance between vectors and vectors in the high-dimensional space and therefore has no effect on the model's performance in clean accuracy. The second part shows that this kind of noise injection can be regarded as a kind of Gaussian noise, which improves the robustness, but too much noise injection affects the performance of the model in clean accuracy. Therefore, we chose the non-uniform noise injection that embodies a kind of trade-off.\n\n**(4) Optimal learned solution of approximation weight and bias.**\n\nThank you for your careful consideration. Essentially, this approach is slightly different from the optimization approach using MSE, mainly is that the difference in the optimization objective has an impact on the convergence of the model training, and the way MSE is chosen is a single objective optimization, which makes the model converge better. If choosing weights and biases, the optimization objective becomes four, and the hyperparameters are more difficult to choose. In addition, we want to make the activation having little influence the result, so we do the optimization in this way. We will try the method you mentioned.\n\n**(5) Adversarial attack epsilon choice.**\n\nIn Table 1, this validation of adversarial robustness does not use adversarial training, which means that the neural network is not pre-fitted on the adversarial samples, and thus in case of larger attack intensity epsilon, the robustness accuracy of both our method and the original model will be very low, which does not reflect the enhancement of our model in adversarial robustness. Instead, Table 3 is trained for confrontation and can withstand higher intensity attacks epsilon(e.g. 8/255)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624176049,
                "cdate": 1700624176049,
                "tmdate": 1700624176049,
                "mdate": 1700624176049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "79CdfQm3UK",
            "forum": "UMwn5l37gU",
            "replyto": "UMwn5l37gU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_A4cN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_A4cN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel method that inject non-uniform noise to non-essential neurons so that the adversarial robustness of the network is enhanced while the clean accuracy is not harmed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work deals with a significant and inspiring topic: enhancing adversarial robustness and efficiency simultaneously."
                },
                "weaknesses": {
                    "value": "- I find the paper a little hard to follow. Specifically, I cannot determine what \u201cirregular 50%\u201d and \u201cstructured/unstructured\u201d mean in the paper\u2019s context. It is very likely because I don\u2019t have sufficient background knowledge and I would appreciate it if the authors could refer me to 2-3 most related (and preferably recent) papers that can help me gain the basics. But for now, I tend to believe the authors fail to make the paper easy to follow.\n    \n- AutoAttack is not included in threat models, which I believe is necessary to show the adversarial robustness of a new method.\n    \n- The performance boost of the proposed method seems to be limited. In Table 3, clean accuracies decreased by a noticeable margin with limited improvement in robustness accuracies. So, it is hard to say that a better accuracy-robustness trade-off is achieved. Also, more baseline AT methods (e.g. TRADES) would be helpful to show the method\u2019s effectiveness.\n    \n- As it is claimed that the proposed method enhances adversarial robustness and execution efficiency simultaneously, it is important to show directly how much the efficiency is improved directly (e.g., throughput) compared to the baselines."
                },
                "questions": {
                    "value": "Can you intuitively explain why \u201ca neuron is regarded as essential if its approximation from $\\widetilde{z}$ is larger than the predefined threshold\u201d?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5578/Reviewer_A4cN",
                        "ICLR.cc/2024/Conference/Submission5578/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762054918,
            "cdate": 1698762054918,
            "tmdate": 1700700585774,
            "mdate": 1700700585774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wieY1CU8bj",
                "forum": "UMwn5l37gU",
                "replyto": "79CdfQm3UK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks so much for your feedback on our methods and constructive comments on the validation of robustness. We provide the answers below.\n\n**(1) \u201cirregular 50\\%\u201d and \u201cstructured/unstructured\u201d mean in the paper\u2019s context.**\n\nThank you for your advice. We realize that the lack of explanation of sparsity can be troubling to readers. So we need to explain the irregular as well as the structured/unstructured in the manuscript. `irregular/unstructured' and 'structured' both can be seen as a kind of sparsity. \u2018irregular/unstructured\u2019 means that the **activation pattern** after sparsification (top-k) is irregular[1]. 'structured' means for a fixed block, the sparsity is determediated[1] [2]. It's like the one shown in Figure 2 in the text. The difference between the two forms of sparsity is mainly in the efficiency of hardware execution; structured sparsity is more efficiently executed due to the fact that it can be chunked for the extraction of deterministic sparse values.\n\n**(2) Autoattack to show the adversarial robustness.**\n\nThe autoattack method is the state-of-the-art method in recent years, in our experiments we ignored this method because of insufficient research, in the future experiments we will add the validation of this method, thank you for your suggestion.\n\n**(3) The performance limitation.**\n\nWe noticed this limitation, the problem should be in the search of the optimal hyperparameters for adversarial training, for our model, the optimal hyperparameters may not be found, which leads to overfitting of our model during adversarial training. In our future work, we will use standard adversarial training[3] to prevent overfitting in order to further evaluate the robustness.\n\n**(4) Efficiency criteria.**\n\nWe will consider more measures that indicate model efficiency.\n\n**(5) A neuron is regarded as essential if its approximation is larger than the predefined threshold.**\n\nThis was a writing mistake that causes confusion. The threshold is not predefined, it is found during the top-k process. This threshold is the smallest number in the top-k, and the purpose of the threshold is to facilitate the generation of the mask. We will revise the manuscript to avoid ambiguity.\n\nReferences:\n\n[1] Ji Y, Liang L, Deng L, et al. TETRIS: Tile-matching the tremendous irregular sparsity[J]. Advances in neural information processing systems, 2018, 31.\n\n[2] Nvidia, \u201cNvidia a100 tensor core gpu architecture.\u201d 2020. [Online]. Available: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n[3] Leslie Rice, Eric Wong, J. Zico Kolter: Overfitting in adversarially robust deep learning. ICML 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624164734,
                "cdate": 1700624164734,
                "tmdate": 1700624164734,
                "mdate": 1700624164734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CEAU7cQpnv",
                "forum": "UMwn5l37gU",
                "replyto": "wieY1CU8bj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5578/Reviewer_A4cN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5578/Reviewer_A4cN"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for replying to my concerns. However, I think too much work is left for the future and the current version is not ready to publish in this venue. After I read other reviews and the authors' responses, I decided to lower my rating to 3."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700757056,
                "cdate": 1700700757056,
                "tmdate": 1700700757056,
                "mdate": 1700700757056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RBJraG7U5z",
            "forum": "UMwn5l37gU",
            "replyto": "UMwn5l37gU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_UxUE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5578/Reviewer_UxUE"
            ],
            "content": {
                "summary": {
                    "value": "This work suggests that only a subset of neurons is critical in representation learning and proposes a selection method to categorize neurons into essential neurons and non-essential neurons. Subsequently, the authors apply non-uniform noise injection to these two types of neurons in order to improve adversarial robustness and maintain clean accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The hypothesis that only a subset of neurons are critical in representation learning, and the rest can tolerate noise perturbations without affecting performance, is interesting.  \n2. Propose a method for distinguishing between essential neurons and non-essential neurons.  \n3. Propose a non-uniform noise injection method tailored for essential neurons and non-essential neurons."
                },
                "weaknesses": {
                    "value": "1. Limited novelty: The proposed method is a marginal improvement on existing methods.\n2. Over-claimed contribution: In the experiments, the proposed method still leads to a decrease in clean accuracy, rather than truly \"retaining clean accuracy\" as claimed.\n3. Robustness evaluation is inaccurate: It is suggested that the authors employ the experimental setup for the standard adversarial training [1] and use AutoAttack [2] for assessing the model's robustness. Compare the effectiveness of the proposed method on the model's best robustness and last robustness.\n\n[1] Leslie Rice, Eric Wong, J. Zico Kolter: Overfitting in adversarially robust deep learning. ICML 2020.  \nhttps://github.com/locuslab/robust_overfitting  \n[2] Francesco Croce, Matthias Hein: Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020.  \nhttps://github.com/fra31/auto-attack"
                },
                "questions": {
                    "value": "If the proposed method can substantially achieve higher robustness compared to standard adversarial training, I will increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699093892821,
            "cdate": 1699093892821,
            "tmdate": 1699636573842,
            "mdate": 1699636573842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w9cyaAzUKO",
                "forum": "UMwn5l37gU",
                "replyto": "RBJraG7U5z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5578/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your constructive comments. We will include the mentioned robustness validation methods in future experiments. We will incorporate your comment on the novelty and contribution with more rigours analysis."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624154863,
                "cdate": 1700624154863,
                "tmdate": 1700624154863,
                "mdate": 1700624154863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]