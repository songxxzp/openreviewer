[
    {
        "title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach"
    },
    {
        "review": {
            "id": "DR2C8HbrUg",
            "forum": "VzPGV19Bnp",
            "replyto": "VzPGV19Bnp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_qTeR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_qTeR"
            ],
            "content": {
                "summary": {
                    "value": "The authors observe that the commonly used regularization to avoid overfitting in customized text-to-image generative models may lead to the loss of detailed information on the customized objects. The authors propose to balance the influences of the prompt condition and the customization condition S* instead of applying regularization to avoid overfitting, ensuring the preservation of customized details and flexibility to work with diverse prompts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Motivated by interesting observations\n- Novel idea of removing regularization to preserve detailed information\n- Computational resource-efficient approach"
                },
                "weaknesses": {
                    "value": "- Encoder-based customized Text-to-Image generation is not new and missing comparisons with important previous works: [1][2][3][4]\n- The important term \"independent conditions\" is not clearly defined.\n- Figure 9 shows that before fine-tuning, fusion sampling even leads to worse identity-preserving performance than baseline sampling, implying that performance gain in identity similarity mainly comes from fine-tuning.\n- In Figure 4, the proposed method does not present visually better results compared to E4T.\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023."
                },
                "questions": {
                    "value": "- The authors aim at \"detail preservation\" but do not clearly explain what information these details include. What is the difference between detail preservation and identity preservation?\n- Minor issue of presentation. Figures 6 and 7 on page 8 are squeezed and Figure 7 has slightly occluded the caption of Figure 6. May rearrange for better visualization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Reviewer_qTeR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549070126,
            "cdate": 1698549070126,
            "tmdate": 1699636806020,
            "mdate": 1699636806020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L1kiv8EZy5",
                "forum": "VzPGV19Bnp",
                "replyto": "DR2C8HbrUg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qTeR"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful reviews, below we address some questions and concerns.\n\nQ: Some related works are missing:\n\nA: Actually, [3]  (E4T) is the most important baseline we have already discussed in our paper, our method obtained better results as presented in the paper. We will include [1] and [4], and add comparison. Although [2] is not an encoder-based method, we will add a discussion.\n\nQ: The important term \"independent conditions\" is not clearly defined.\n\nA: By \"independent conditions\", we simply mean the two conditions $S^*$ and $C$ are independent given the generation, i.e, $p(S^*,C|x) = p(S^*| x)p(C|x)$.\n\nQ: Question about identity preservation.\n\nA: We want to emphasize that we never claimed the better fine-grained details can be obtained by a sampling method. What we claimed is that **details can be better preserved if the model is trained/fine-tuned without regularization**. However, **baseline sampling fails to generate desired images with respect to text** in this case. Figure 9 shows that Fusion Sampling can generate images aligned with text, under the setting without regularization, no matter before or after fine-tuning. Better details are certainly obtained by fine-tuning. However, without Fusion Sampling, we can not enjoy the setting without regularization which has the best detail preservation. \n\nThat's why we propose to **fine-tune without regularization and perform Fusion Sampling after fine-tuning**. As we show in the paper, compared to related encoder-based method [3] which **fine-tunes the model with regularization and performs baseline sampling**, better results are obtained by our method.  \n\nFurthermore, we are able to perform more flexible generation with Fusion Sampling: we can choose to emphasize either better creativity or better details, which is why the results of our method in Figure 5 and Figure 7 are represented by line instead of point like other methods. And our line is above all the points, indicating better results are obtained by our method.\n\nQ: About experiment results.\n\nA: Customized generation is subjective and hard to evaluate. That's why we performed both quantitative evaluation and human evaluation. In quantitative evaluation, pre-trained models are used to compare extracted features from original and generated images. Better quantitative results are obtained by our method. In human evaluation, workers from Amazon Mechanical Turk are asked to compare results from different methods. All the workers have performed at least 10,000 approved assignments with an approval rate \u2265 98%, thus we believe the human evaluation results from the workers can objectively illustrate the effectiveness of our proposed method.\n\n\n\n[1] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. \n\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023. \n\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023). \n\n[4] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699643841482,
                "cdate": 1699643841482,
                "tmdate": 1699644781415,
                "mdate": 1699644781415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gOwOVq8ag6",
                "forum": "VzPGV19Bnp",
                "replyto": "L1kiv8EZy5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_qTeR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_qTeR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response. However, the response does not fully address my concerns. For example:\n\n- Fusion sampling is an important part of the proposed method, but its effectiveness is only demonstrated through one qualitative example in Fig.9. \n- I agree with other reviewers that the tradeoff between detail preservation and text alignment and its relation with regularization needs to be more carefully analyzed to prove the motivation of the regularization-free approach.\n- Why do the authors think suti [1] is not an encoder-based method?\n\n[1] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525907996,
                "cdate": 1700525907996,
                "tmdate": 1700525907996,
                "mdate": 1700525907996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "epbTU1ewlp",
                "forum": "VzPGV19Bnp",
                "replyto": "DR2C8HbrUg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qTeR"
                    },
                    "comment": {
                        "value": "We will add more experiment results to illustrate the effectiveness of the Fusion Sampling in the Appendix soon.\n\nThere might be some misunderstanding, we would like to clarify more on the \"encoder-based\" method.\nSuTI directly adopts the architecture from Re-Imagen [1], which is a retrieval augmented diffusion model. Different from E4T, ELITE or our method, it does not introduce an extra encoder network outside the diffusion model, but directly use the \"encoder part\" of the UNet. We did not call it encoder-based method, just like we would not call diffusion model itself as an encoder-based method.\nFurthermore, SuTI focuses on apprenticeship learning so that the final model can imitates the behaviors of different experts, rather than focusing on design encoder mapping to map input image into embeddings like E4T do. We are happy to add SuTI as related work for discussion, but want to emphasize the difference between it and other works like E4T and ELITE.\n\n[1]. Re-Imagen: Retrieval-Augmented Text-to-Image Generator. Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591869706,
                "cdate": 1700591869706,
                "tmdate": 1700592076543,
                "mdate": 1700592076543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PGIPVh0miQ",
            "forum": "VzPGV19Bnp",
            "replyto": "VzPGV19Bnp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_GgLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_GgLb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a regularization-free and detail-preservation approach for customized text-to-image generation.\nTo this end, this paper introduces PropFusion to tackle over-fitting problem without the widely used regularization.\nTherefore, PropFusion significantly reduces training time while achieve enhanced preservation of fine-grained details.\nMoreover, it also introduces a novel sampling method namely Fusion Sampling to meet the requirements of text prompts.\nExtensive experiments demonstrate the superiority of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "See summary."
                },
                "weaknesses": {
                    "value": "1. Can the authors elaborate more derivation details from Eq.10 to Eq.11?\n2. Comparisons with IP-Adapter[1]. IP-Adapter also projects the input image into the text embedding space, but requires no additional finetuning or well-designed sampling. The authors are encouraged to compare to this simple baseline.\n\n[1] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang. IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698580112087,
            "cdate": 1698580112087,
            "tmdate": 1699636805891,
            "mdate": 1699636805891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rACsrlDs5n",
                "forum": "VzPGV19Bnp",
                "replyto": "PGIPVh0miQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GgLb"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and suggestions, below we address some concerns and answer some questions.\n\nQ: From Equation 10 to Equation 11.\n\nA: Equation 11 is derived from Equation 8 and Equation 10. In Equation 8, we obtain an inaccurate prediction for timestep $t-1$, which is denoted as $\\widetilde{x_{t-1}}$. In equation 10, we inject the structure information in $\\widetilde{x_{t-1}}$ into the current noised sample $x_t$ to obtain an updated sample $x_t$. \nAs we discussed in the paper. Simply feeding the conditions independently into the model will lead to structure conflict. Thus we need to first obtain an \u201cinaccurate\u201d next-step sample, which is harmonized in terms of structure information . It is called \u201cinaccurate\u201d because which it is conditioned on $\\gamma S^*$. Using this $\\widetilde{x_{t-1}}$ as a prediction for $x_{t-1}$ leads to generation with some details lost, as shown in Figure 12. But this \u201cinaccurate\u201d sample  $\\widetilde{x_{t-1}}$ does not have structure conflict anymore. Thus we propose to use its structure to infer a corresponding updated sample  $\\widetilde{x_{t}}$ for current time step $t$, then perform sampling based on this  $\\widetilde{x_{t}}$ instead of  $x_t$.\n\nQ: Related works.\n\nA: Thanks for the suggestion, we will add the discussion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591133394,
                "cdate": 1700591133394,
                "tmdate": 1700591133394,
                "mdate": 1700591133394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tX9RNE3o6j",
                "forum": "VzPGV19Bnp",
                "replyto": "rACsrlDs5n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_GgLb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_GgLb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your elaborate response and I will keep my score 6 to accept this paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714761529,
                "cdate": 1700714761529,
                "tmdate": 1700714761529,
                "mdate": 1700714761529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ujDuxjlm5L",
            "forum": "VzPGV19Bnp",
            "replyto": "VzPGV19Bnp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_KZRC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_KZRC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework called ProFusion to tackle the over-fitting problem in customized text-to-image with an encoder network and a novel sampling method. The encoder network receives an image depicting a customized object, which makes the generation condition on that object. Then, a novel sampling method called Fusion sampling is proposed to enhance the generation to be conditioned on both the input image and the arbitrary user input text. The experiments conducted demonstrate the effectiveness and superiority of the proposed framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-organized, and the method is easy to follow. The core contribution Fusion sampling is novel and may have a broad impact on the conditional image generation community.\n2. The most important contribution of the paper is that it proposes a framework without regularization techniques to prevent over-fitting in customized text-to-image generation. The encoder network, which converts the image containing the customized object into an embedding $S^*$, makes the diffusion model condition on that object. Unlike previous methods adopting regularization techniques to prevent over-fitting, this paper proposes a novel Profusion sampling method. By assuming $S^*$ and the arbitrary user input text $C$ are independent, the sampling method decomposes the noise prediction into two terms that take both $S^*$ and $C$ into consideration.\n3. The experiments show that the proposed method achieves superior capability for preserving fine-grained details."
                },
                "weaknesses": {
                    "value": "1. Line#5 to Line#7 in Algorithm 1 is the standard diffusion denoising sampling step. However, the intuition behind #Line9 in the algorithm is unclear. \n2. The corresponding ablation study showing the difference between using or not using the two stages in the algorithm is not convincing enough."
                },
                "questions": {
                    "value": "1. Please clarify the intuition or motivation behind Line#9 in the fusion stage in the fusion sampling method.\n2. Please provide the corresponding ablation studies to support that using m=1 in practice works well for the fusion sampling method.\n3. Do the baseline methods all use the data augmentation method in the paper? If not, the comparison may be unfair.\n4. It is better to report the exact training time, fine-tuning time, and the GPU devices used for reproduction consideration.\n5. Is it possible to combine the proposed sampling method with other regularization techniques to further improve the performance? Or is it possible to adopt the sampling method to other methods to verify its effectiveness in improving detail preservation?\n6. One may trade off the detail preservation for more creative generations or the other way around. Can the authors provide such trade-offs in the proposed framework?\n6. Is the method capable of capturing the style of the input customized image? For instance, can the method perform style transfer like generating an image by the prompt \"a car in the style of $S^*$\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Reviewer_KZRC",
                        "ICLR.cc/2024/Conference/Submission6920/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759280003,
            "cdate": 1698759280003,
            "tmdate": 1700622114109,
            "mdate": 1700622114109,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CHUBnIgvXa",
                "forum": "VzPGV19Bnp",
                "replyto": "ujDuxjlm5L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KZRC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and suggestions, below we address some concerns and answer some questions.\n\nQ: About line#9 in the algorithm.\n\nA: As we discussed in the paper. Simply feeding the conditions independently into the model will lead to structure conflict. Thus we need to first obtain an \u201cinaccurate\u201d next-step sample, which is harmonized in terms of structure information . It is called \u201cinaccurate\u201d because which it is conditioned on $\\gamma S^*$. Using this $\\widetilde{x_{t-1}}$  as a prediction for $x_{t-1}$ leads to generation with some details lost, as shown in Figure 12. But this \u201cinaccurate\u201d sample  $\\widetilde{x_{t-1}}$  does not have structure conflict anymore. Thus in line #9 of the algorithm we propose to use its structure to infer a corresponding updated sample  $\\widetilde{x_{t}}$  for current time step $t$, then perform sampling based on this  $\\widetilde{x_t}$  instead of  $x_t$.\n\nQ: About m=1 in the algorithm.\n\nA: In all the experiments reported in the paper, we use m=1, which can already lead to better results than other methods. We believe that it is enough to show that m=1 works well in practice. We add ablation study on m>1, in the revised Supplementary Material. From which we can see that increasing m does not lead to noticeable improvement. Considering the fact that increasing m will lead to longer sampling time, thus we believe using m=1 in practice is a better option.\n\n\nQ: Do all the baseline methods use data augmentation?\n\nA: We didn\u2019t use data augmentation in the pre-training stage, which is the same as other method. Data augmentation during fine-tuning is part of our proposed method. We will provide ablation study of comparing baseline sampling and Fusion Sampling, under different settings including with data augmentation, without data augmentation, and different level of regularization during fine-tuning.\n\nQ: Implementation details.\n\nA: We conduct all the experiments on Nvidia A100 GPUs. The pre-training time for encoder on CC3M dataset cost around 1 week on 8 A100 GPUs. The fine-tuning time on a single A100 GPU is around 25 seconds for 50 steps.\n\nQ: Is it possible to combine the proposed method with other regularization methods?\n\nA: Good question. Yes, the proposed method can be combined with any regularization methods as the Fusion Sampling will not influence the training, it only changes the inference. However, whether it can lead to improvement over a model without regularization depends on the regularization method itself. Currently, applying Fusion Sampling on a model with L2 regularization does not lead to observable improvement. Because some fine-grained details will be lost by applying regularization, and can not be recovered by simply using Fusion Sampling.\n\nQ: Trade-off between creativity and detail preservation.\n\nA: Good question, we provide some results in the revised Supplementary Material. We can easily control the generation, obtaining different level of creativity and detail preservation, by tuning a single hyper-parameter $\\gamma$.\n\nQ: Can the method capture style?\n\nA: Good question, we provide some results in the updated Supplementary Material, from which we can see that the proposed method can successfully capture the style of the input image and combine it with user-input text to create new generations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591019378,
                "cdate": 1700591019378,
                "tmdate": 1700603066024,
                "mdate": 1700603066024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ne7fDymAgq",
                "forum": "VzPGV19Bnp",
                "replyto": "ujDuxjlm5L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_KZRC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_KZRC"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal by Reviewer KZRC"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal. The response addresses most of my concerns. However, I agree with other reviewers that the motivation of the need for \"regularization-free\" is not comprehensively analyzed. Perhaps the authors should emphasize more on the problem of overfitting to $S^*$ instead of detail preservation. Also, the effectiveness of the Fusion sampling method has not been demonstrated convincingly in the current version. Therefore, I lowered my rating accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622019631,
                "cdate": 1700622019631,
                "tmdate": 1700622235843,
                "mdate": 1700622235843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xi4b1FSvBV",
            "forum": "VzPGV19Bnp",
            "replyto": "VzPGV19Bnp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_Y2oZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6920/Reviewer_Y2oZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the task of customized text-to-image generation. It trains the PromptNet to map concepts in an input image into a text embedding for subsequent text-to-image generation. During training, the input image is augmented with affine transformation, and the UNet attention layers are fine-tuned. During sampling, the proposed Fusion Sampling method separates the obtained prompt S* and the arbitrary text prompt C, and feed them into UNet separately to avoid S* overriding C."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presentation is clear, and easy to follow.\n2. This paper presents some nice results of cutomized text-to-image generation."
                },
                "weaknesses": {
                    "value": "1. The motivation of the need for \"regularization-free\" is not comprehensively demonstrated. See Question No.1 for more details.\n2. The contribution of PromptNet is limited, where the main difference compared to other encoder-based approaches (e.g. ELITE) is that the affine transformation is introduced.\n3. Similar ideas towards Fusion Sampling has been seen in several works [1][2], where the input condition is decomposed into multiple segments, and fed into the diffusion model separately. \n[1] Compositional Visual Generation with Composable Diffusion Models (ECCV 2022)\n[2] Collaborative Diffusion for Multi-Modal Face Generation and Editing (CVPR 2023)"
                },
                "questions": {
                    "value": "1. The motivation is that \"regularization can result in detail lost\". However, the validity of this claim depends on the degree of regularization applied, and the degree of detail loss suffered. In figure 3, what is the regularization weighting (i.e. lambda)? Have you tried an even smaller regularization weighting, and what is the degree of detail lost then?\n2. In section 2.1, the observation is made that \"without regularization, the UNet has a tendency to overly prioritize the S* concept, and overshadowing C\". The question arises as to why feeding C independently into the UNet can address this issue, since the text-to-image mapping is not altered by the FusionSampling strategy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6920/Reviewer_Y2oZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699109434431,
            "cdate": 1699109434431,
            "tmdate": 1699636805651,
            "mdate": 1699636805651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "87rpsaQpQx",
                "forum": "VzPGV19Bnp",
                "replyto": "xi4b1FSvBV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y2oZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and suggestions, below we address some concerns and answer some questions.\n\nQ: About regularization used in verifying the claim \"regularization can result in detail lost\".\n\nA: In experiments, we tried regularization $\\lambda \\Vert S \\Vert^2$ with $\\lambda$ selected from [1e-5, 1e-4, 1e-3, 1e-2, 1]. Figure 3 is presented to illustrate the influence of regularization on generation, small regularization like 1e-5 will result in failure in generating creative contents, while larger regularization will lead to identity loss. In customization, we are looking for a point where the model can preserve the details while having the capability to generate creative contents. However, simply tuning regularization hyper-parameter can not solve this problem.\n\nQ: Why feeding C independently into the UNet can address this issue.\n\nA: When we feed both $S^*$ and $C$ into the model, the model will ignore $C$ because $S^*$ is too strong. But if we feed $C$ along to the model, for sure the model will notice $C$, as there is no other condition here. What\u2019s more, feeding conditions independently gives us the flexibility to control the influence of each condition, through a hyper-parameter $w_i$ in classifier-free guidance. We can control how much should the results be conditioned on $S^*$ and $C$. \nAs we show in the paper. Feeding conditions independently can not fully solve the problem, that\u2019s why we need another fusion stage, to avoid some structure conflict in the generated results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590558007,
                "cdate": 1700590558007,
                "tmdate": 1700590558007,
                "mdate": 1700590558007,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sFNyTzTxuR",
                "forum": "VzPGV19Bnp",
                "replyto": "87rpsaQpQx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_Y2oZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6920/Reviewer_Y2oZ"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their response. \n\nI can understand your claim that \"it's difficult to find a regularisation weighting to ensure both detail preservation and content diversity\". I just don't find the experimental proof convincing.\nFigure 3 only shows 3 different degrees of regularisation, and how do they match to the 4 weightings [1e-5, 1e-4, 1e-3, 1e-2, 1]? Furthermore, in the two rows of Figure 3, the smallest regularisation image (leftmost) in both rows look almost identical to me, but their text conditions are different. I am not sure whether this is the result caused by 1e-5 regularisation?\n\nIt would be good if there are further explanations on why S* can be too strong. However, most my concerns in the second question have been addressed."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644298555,
                "cdate": 1700644298555,
                "tmdate": 1700644298555,
                "mdate": 1700644298555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]