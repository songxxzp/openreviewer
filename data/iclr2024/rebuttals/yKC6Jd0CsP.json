[
    {
        "title": "Vision ELECTRA: Adversarial Masked Image Modeling with Hierarchical Discriminator"
    },
    {
        "review": {
            "id": "X98iqn1GL8",
            "forum": "yKC6Jd0CsP",
            "replyto": "yKC6Jd0CsP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1187/Reviewer_XJEn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1187/Reviewer_XJEn"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces VE, a MIM framework for computer vision (CV), inspired by ELECTRA. VE uses a generator-discriminator setup for pre-training, with the discriminator's encoder as the outcome. It adds innovations like adversarial training, Gaussian noise for diversity, and hierarchical discrimination for capturing macro and micro-level features. Experiments show VE outperforms mainstream MIM methods like SimMIM and MAE, especially in continual pre-training. VE excels in various CV tasks like classification, segmentation, and object detection. The paper validates each VE component empirically and highlights its cross-task transfer learning capabilities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a Masked Image Modeling (MIM) framework, VE, which adapts ELECTRA's principles to the field of computer vision.\n    \n2. The framework incorporates some elements, including adversarial training for image quality, Gaussian noise for diversity, and hierarchical discrimination for capturing features at both macro and micro levels.\n    \n3. VE leverages a generator-discriminator setup and the encoder of the discriminator as the pre-trained model, demonstrating its effectiveness in representation learning.\n    \n4. The paper conducts empirical validations, confirming the effectiveness of each component within the VE framework.\n    \n5. VE exhibits cross-task transfer learning capabilities, showing its applicability in different CV applications."
                },
                "weaknesses": {
                    "value": "1. As indicated in Table 1, VE lags noticeably behind SimMIM and MAE when trained from scratch. It's only in the Continual Pre-training setting that VE outperforms SimMIM and MAE. However, it's worth noting that Continual Pre-training still relies on SimMIM's model, which means that before using VE, SimMIM pre-training is still necessary, limiting the practicality of VE.\n    \n2. On page 6, the paper mentions, 'Note that, in this setting, the discriminator of VE was derived from ViT-B pre-trained by SiMIM.' However, the paper doesn't provide Continual Pre-training results based on pre-trained MAE models. This leaves us uncertain about VE's generalizability when based on MAE.\n    \n3. Comparing VE to SimMIM and MAE in a Continual Pre-training setup might seem somewhat unfair. This is because during the continued training phase, SimMIM and MAE are still engaged in generative pre-training, while VE follows a discriminative training approach. Therefore, it's not surprising that VE outperforms SimMIM and MAE in this context. An alternative and more equitable baseline would involve subjecting SimMIM and MAE to discriminative pre-training (e.g., contrastive learning training) during the continued training phase, and then comparing their performance to VE.\n    \n4. One important experiment that is missing is the result of using VE's MIM model as the derived pre-trained model.\n    \n5. The approach of jointly training the generator and discriminator as depicted in Equation 5 typically does not work for GANs. GANs usually employ an alternating training strategy, where the discriminator is trained first, followed by training the generator. Typically, the discriminator undergoes more updates than the generator to ensure stable training. I remain skeptical about the effectiveness of the joint training strategy for GANs outlined in Equation 5."
                },
                "questions": {
                    "value": "My main concern is that VE outperforms SimMIM and MAE only in the Continual Pre-training setting, implying that VE still relies on other pre-training methods (the pre-training model used in the article is based on SimMIM). VE is not an independent pre-training solution like SimMIM and MAE."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800898282,
            "cdate": 1698800898282,
            "tmdate": 1699636045269,
            "mdate": 1699636045269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "DeBtqXCnnl",
            "forum": "yKC6Jd0CsP",
            "replyto": "yKC6Jd0CsP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1187/Reviewer_rgaC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1187/Reviewer_rgaC"
            ],
            "content": {
                "summary": {
                    "value": "This paper adapts a pretraining framework widely used in NLP, ELECTRA, to the vision domain and shows empirically that the Vision ELECTRA performs better than the existing popular MIM methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work adapts a significant pretraining framework to the vision domain, which would likely promote the advancement of research in the multimodal domain."
                },
                "weaknesses": {
                    "value": "Given the simplicity of the proposed method, I expect much more extensive empirical studies to be conducted, so that we can learn how effective the ELECTRA framework is in the vision domain. For example, In Table 1, VE is only pretrained for a short period from scratch in addition to continual pretraining, neither of which is a common setting for self-supervised pretraining. How would VE perform when pretraining from scratch for 300/800/1600 epochs? Also, more datasets (e.g., ImageNet-21k) and more backbones should be included to show the scalability of the framework."
                },
                "questions": {
                    "value": "Can you discuss more about how your work may influence a larger community than self-supervised visual representation learning, such as foundation models or multimodal learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1187/Reviewer_rgaC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809477713,
            "cdate": 1698809477713,
            "tmdate": 1699636045201,
            "mdate": 1699636045201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "H6F40NrclZ",
            "forum": "yKC6Jd0CsP",
            "replyto": "yKC6Jd0CsP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1187/Reviewer_jdSs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1187/Reviewer_jdSs"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ELECTRA from NLP to the Vision domain.  To enhance the diversity of generated patches, the authors inject random noise into the generator. To train the discriminator, the authors employ both patch-level and image-level discrimination. Different from previous work like MAE, the proposed method uses the discriminator rather than the generator as the pre-training outcome. Experimental results verify the proposed method achieves comparable performance with MAE, and SimMIM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The writing is clear and easy to follow.\n* The injection of noise makes sense since there exists the one-to-many mapping for the generator.\n* Many visualizations are provided to make a better understanding."
                },
                "weaknesses": {
                    "value": "* The essential contribution is to introduce ELECTRA to the vision domain, which is quite limited. Although the authors propose the noise injection and the patch-level & image-level discriminator, however, 1) the noise injection brings marginal performance gain as demonstrated in Table 3, 2) the patch-level & image-level discriminator has been well-studied in image GAN, which is also missed in the related works.\n* Experimental comparison is not convincing, the baselines (i.e., MAE and SimMIM) are behind the times (CVPR 2022)."
                },
                "questions": {
                    "value": "* What are the advantages of introducing ELECTRA to the vision domain? Is there any unique superiority compared to the existing pre-training scheme like SimMIM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1187/Reviewer_jdSs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823626519,
            "cdate": 1698823626519,
            "tmdate": 1699636045119,
            "mdate": 1699636045119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]