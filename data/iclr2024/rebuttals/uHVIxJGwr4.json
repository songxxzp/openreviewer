[
    {
        "title": "Learning to Branch with Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "objje4zSAB",
            "forum": "uHVIxJGwr4",
            "replyto": "uHVIxJGwr4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_pc5v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_pc5v"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Ranking-Constrained Actor-Critic algorithm, an offline reinforcement learning approach for optimizing Mixed Integer Linear Programs (MILPs). Traditional MILP solvers depend on hand-crafted heuristics for branching, limiting their efficiency and generalizability. Recent deep learning methods rely on high-quality training data, which can be scarce, particularly for large problems. The key contributions of the paper are the development of the new RL algorithm and its ability to efficiently learn branching strategies even from sub-optimal training data. The algorithm outperforms previous methods in terms of prediction accuracy and computational efficiency across various MILP problems, addressing the limitations of traditional solvers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper claims to be innovative by being the first to apply offline reinforcement learning algorithms in branch-and-bound methods. Furthermore, the essence of the proposed method lies in further refining the dataset, specifically selecting the top-k actions in the set G\u03c9 for Bellman operator operations. This can effectively enhance the performance of the branching strategy. I believe this perspective can also be inspiring for similar problems in other domains."
                },
                "weaknesses": {
                    "value": "This paper proposes training branch-and-bound strategies using offline reinforcement learning. However, in practice, interacting with solvers is relatively straightforward, and under these circumstances, using online reinforcement learning may yield better performance. The authors need to clarify the necessity of utilizing offline reinforcement learning."
                },
                "questions": {
                    "value": "\u2022\tConsidering that interacting with solvers online is convenient, is there a necessity to use offline reinforcement learning to train branch-and-bound strategies?\n\u2022\tIn Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -\u03b4, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario?\n\u2022\tI believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in G\u03c9 for Bellman operator operations. I am curious to know if, after obtaining the top-k actions in G\u03c9, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4761/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4761/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4761/Reviewer_pc5v"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698547649041,
            "cdate": 1698547649041,
            "tmdate": 1699636458715,
            "mdate": 1699636458715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l50OKdQDdQ",
                "forum": "uHVIxJGwr4",
                "replyto": "objje4zSAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We want to thank reviewer pc5v for your questions and constructive suggestions. Here are our responses to all of your comments.\n\n > The authors need to clarify the necessity of utilizing offline reinforcement learning.\n\nWe have briefly discussed the comparison in the third paragraph in our introduction part, here we will try to further explain why using offline reinforcement learning is meaningful.\n\n1. Training reinforcement learning for branching is super time-consuming [1,2]. It is because interacting with the solver could also be slow (long interaction time) and the BnB tree could be super large when a RL policy is very bad at the beginning. Here we cite the training time from two recent RL for branching papers, `We set a maximum of 15,000 epochs and a time limit of six days for training` and `we left our retro branching agent to train for \u2248 13 days (\u2248 500k epochs)`. In comparison, the offline models can all be trained within hours.\n\n2. Bad performance of current RL methods. The results from the current two papers demonstrate that their performance is still way worse the imitation learning methods. In Table 2 and 3, we have also shown that the RL agent (tMDP) is even worse than RCAC which is trained over sub-optimal or small datasets.\n\n3. There are lots of known problems in training online RL agents for branching, as summarized in [2]. For example, existing RL methods choose the size of the search tree as the reward function, which leads to a sparse reward environment and that is why we propose to use the improvement in the dual-bound as the reward function. A subsequent problem is the credit assignment problem (how to tell which action leads to the good/bad outcomes), which is studied by [1], but the the model performance shows that this problem is still far from being solved.\n\n[1] Lara Scavuzzo, Feng Yang Chen, Didier Chetelat, Maxime Gasse, Andrea Lodi, Neil Yorke-Smith, and Karen Aardal. Learning to branch with tree MDPs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\n\n[2] Christopher W. F. Parsonson, Alexandre Laterre, and Thomas D. Barrett. Reinforcement learning for branch-and-bound optimisation using retrospective trajectories. Proceedings of the AAAI Conference on Artificial Intelligence, 37(4):4061\u20134069, Jun. 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709535759,
                "cdate": 1700709535759,
                "tmdate": 1700709535759,
                "mdate": 1700709535759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IgE83vz22c",
                "forum": "uHVIxJGwr4",
                "replyto": "objje4zSAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2)"
                    },
                    "comment": {
                        "value": "> In Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -\u03b4, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario? \n\nThanks for this great question. Actually, we do not directly fit the output of critic network $Q$ to $-\\delta$. Note in Equation 8, we only change the output of the target network to $-\\delta$. We realize this by first letting the target network output its value, then replace some of those outputs with $-\\delta$. The target network needs no training, so it will not be fit to a distribution with values centered around $-\\delta$. Then the fitting target of $Q$ in Equation 7 is also not a distribution with values centered around $-\\delta$.\n\n> I believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in G\u03c9 for Bellman operator operations. \n\nWe think there could be some misconceptions here. $G_{\\omega}$ is also a neural network rather than a set, so we actually do not do any dataset refining, i.e., filtering out good transitions from the dataset for training. The ranking is conducted on the scores of the $G_{\\omega}$, so the top-k actions do not necessarily have to be in the collected dataset, that is, we do not observe the reward for all these top-k actions. But we can still evaluate these transitions with our learned Q-network.\n\n> I am curious to know if, after obtaining the top-k actions in G\u03c9, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea.\n\nWe believe the most relevant results are in Table 5, since $G_{\\omega}$ has already been a neural network, simply imitating $G_{\\omega}$ is just a distillation of the model. The better performance of RCAC over $G_{\\omega}$ has already demonstrated that the source of improvement is from offline reinforcement learning."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709552179,
                "cdate": 1700709552179,
                "tmdate": 1700709693481,
                "mdate": 1700709693481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WNKeukMyQ0",
            "forum": "uHVIxJGwr4",
            "replyto": "uHVIxJGwr4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_s5Ux"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_s5Ux"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes the usage of offline reinforcement learning for variable selection in the branch-and-bound algorithm. To do so, they introduce a novel offline algorithm that uses a classifier to determine whether a state-action pair is in the offline dataset. Their offline Q-values are now restricted towards picking only the top-k most likely actions for each state."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The usage of offline reinforcement learning seems more fitting than current imitation learning algorithms due to its lack of reliance on high quality demonstrations."
                },
                "weaknesses": {
                    "value": "- The paper is a little unclear at some points. For instance, in the last paragraph of Section 2.2: Which variables are the selected ones? Just from the node chosen by the node selection policy, or all variables across the entire tree? In general, the distinction between node selection and variable selection doesn\u2019t become clear: Does the method also do node selection (by picking variables from the entire tree), or just variable selection?\n- Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.\n- One missing benchmark is the utilization of an off-the-shelf offline RL algorithm, such as conservative Q-learning as a baseline for the specific utility of RCAC over more established offline-RL algorithms (I.e. is the improvement in performance due to offline-RL or RCAC specifically?).\n- The testing set is also rather small: 10k training instances, 2k validation instances and, 20 test instances is a strange ratio.\n- The reward function is also a little bit strange: Why consider the dual bound, but ignore the primal one completely? Further, these bounds are not scale-invariant, meaning that the same problem, modulo a constant scalar, could have different dual bound improvements. Even if one takes care to normalize the objective vector c beforehand, most solvers like SCIP rescale this vector for increased numerical stability. Depending on which problems are chosen, the range of rewards across different instances might also be massive depending on the duality gap. However, we agree with the authors that this metric is still better than tree-size or number of nodes.\n\nSome minor points:\n- Abstract: hand-craft[ed]\n- Intro: The sentence \u201cAll of these models are trained\u2026\u201d needs a re-write\n- Intro: \u201cTo our knowledge, \u2026 to apply offline RL to MILP solving\u201d (re-write)\n- Sec. 2: typo pseudocsot\n- Sec. 2.2. A[n] MDP\n- Equation 4: one closing brace is too much (after $Q_\\theta$)\n- Sec. 3.1: when a[n] MILP instance\n- Sec 3.1: discounted factor $\\rightarrow$ discount factor\n- Sec 3.3: citation of Gasse et al.: use cite instead of citep; same again happened in Sec. 4.1\n- Sec. 4.1: please use cite and citep depending on how you add these citations into the text\n- Sec. 5.2 does not add any benefit to the paper and can be omitted in its current state"
                },
                "questions": {
                    "value": "- Which set of variables if being selected from?\n- What is the performance of other offline-RL algorithms?\n- Can you evaluate on a larger testset?\n- Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?\n- In Sec 3.2. \u201cIn fact, a good action does no harm to policy optimization even if it is an OOD action\u201d \u2013 can you please elaborate on this a bit more?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571655333,
            "cdate": 1698571655333,
            "tmdate": 1699636458619,
            "mdate": 1699636458619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mlOw9YHLHc",
                "forum": "uHVIxJGwr4",
                "replyto": "WNKeukMyQ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We want to thank Reviewer s5Ux\u2019s careful review and valuable advice. Some of the questions are actually related to the common practice in the existing work, but we will try our best to improve the experimental setting and clarify some descriptions based on your suggestions.\n\n> Which variables are the selected ones?\n\nOnly the node chosen by the node selection policy is used. The node selection policy is fixed. Thanks for the question and we will make this part clear.\n\n> Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.\n\nWe have multiple independent models trained on and evaluated on individual datasets. Actually, the description of the experimental settings exactly follows the descriptions in the previous works, but we will add more clarifications in our final version.\n\n> One missing benchmark is the utilization of an off-the-shelf offline RL algorithm\n\nSee Table 13 and 14 for the comparison with CQL.\n\n> The testing set is also rather small: 10k training instances, 2k validation instances, and 20 test instances is a strange ratio.\n\nThis ratio is actually a common practice utilized in existing works. 20 test instances x 5 seeds leading to 100 measurements. We follow your suggestion to expand the number of testing instances to 100 in Table 9 and 10, which leads to 500 measurements. The reason for such a small testing set is due to its time measurement, which could be inaccurate under parallel computing, and we believe this is a common problem in the existing research.\n\n> Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?\n\nFirst, this is not a weird choice since the dual-integral is also a common metric to evaluate the quality of the branching policy, where only the dual bound is used. Second, dual-bound improvement is used to compute the score in the strong branching, which is empirically proven to be a powerful branching policy. It can somewhat directly reflect the quality of the instant branching decision leading to it while primal-dual gap cannot. Finally, the improvement from the primal bound is highly related to the primal heuristics, and is sometimes hard to fit with the node features. \n\n\n> In Sec 3.2. \u201cIn fact, a good action does no harm to policy optimization even if it is an OOD action\u201d \u2013 can you please elaborate on this a bit more?\n\nOffline RL treats those actions with a low frequency in the dataset as OOD actions and avoids the query on these actions since the value estimation on these actions is inaccurate due to the lack of samples. However, if we can verify an action is good, for example, it brings a high dual-bound improvement, then we do not have to worry about its inaccurate value estimation. If its value is underestimated, this action will not be chosen by our actor, then there is no difference. If its value is over-estimated, our actor will choose this action, but since we have verified through the dual-bound improvement that it is highly possible to be a good action, it still brings no harm."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709380603,
                "cdate": 1700709380603,
                "tmdate": 1700709380603,
                "mdate": 1700709380603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9W43cphHAQ",
            "forum": "uHVIxJGwr4",
            "replyto": "uHVIxJGwr4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_3oNR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_3oNR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of learning variable selection policies for mixed-integer linear programming (MILP). The authors propose an offline reinforcement learning (RL) approach to learn branching strategies from sub-optimal or inadequate training signals. Experiments demonstrate the proposed method outperforms baselines on various benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is easy to follow.\n2.\tExperiments demonstrate the proposed method outperforms baselines on various benchmarks."
                },
                "weaknesses": {
                    "value": "1.\tThe novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.\n2.\tThe authors claim that the proposed method is the first attempt to apply the offline RL algorithms to MILP solving. However, I found one previous work [1] applies offline RL methods to branching strategies learning as well. \n3.\tThe authors may want to explain the novelty of their method over the work [1] in detail.  \n4.\tThe experiments are insufficient. First, the authors may want to evaluate their method on the load balancing dataset from the ML4CO competition as well. Second, the baselines are insufficient. The authors may want to compare their method to the work [1]. Third, the authors may want to evaluate the generalization ability of the learned models.\n\n[1] Huang, Zeren, et al. \"Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-Based Policy Learning.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022."
                },
                "questions": {
                    "value": "Please refer to Weaknesses for my questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766364331,
            "cdate": 1698766364331,
            "tmdate": 1699636458528,
            "mdate": 1699636458528,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s6f8SXIhYu",
                "forum": "uHVIxJGwr4",
                "replyto": "9W43cphHAQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1)"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer\u2019s questions, we have actually covered some of them in the paper, here we give more discussions about these points.\n\n> The novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.\n\nRCAC is not just a simple application of one offline RL algorithm to learning to branch. We actually start with the CQL algorithm and gradually modify the offline RL algorithm to the current framework, here we summarize some core technical contributions in our work.\n\nUsing a hard constraint (ranking) rather than the soft constraint adopted in many previous offline RL algorithms, such as CQL. This is because branching performance is somewhat sensitive to the branching decisions, a very bad branching decision would cause a huge subsequent BnB tree. Therefore, the offline RL agent should be trained more conservatively and that is why we use ranking as the constraint.\nUsing a weighted cross entropy loss $G_{\\omega}$ rather than a standard behavior cloning cross entropy loss. To overcome the over-conservativeness of RCAC, we give more preference to those promising actions when we rank the actions. This is realized through the dual-bound improvement, which is a heuristic adopted in strong branching. \n\nUsing the dual-bound improvement as the reward function. As we discuss in section 3.1, compared with the BnB tree size, dual-bound improvement is denser and more flexible to compute, which serves as a better reward function.\n\nWe also add the comparison with CQL in Table 13 and 14."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709208886,
                "cdate": 1700709208886,
                "tmdate": 1700709208886,
                "mdate": 1700709208886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v1bnyAe2k5",
            "forum": "uHVIxJGwr4",
            "replyto": "uHVIxJGwr4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_nNZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_nNZN"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of learning to select branching strategies while solving mixed integer programs via branch and bound algorithm. The key idea is to collect offline training dataset using full strong branching as behavior policy and learn an offline RL algorithm to generate the learned branching policy. Improvement of the dual bound is chosen as the reward function. Experiments are performed on four synthetic and two real world problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Using offline RL for branching policies seems like a natural idea that should do better than pure imitation learning. I am surprised that this wasn't tried earlier and commend the paper for making this simple but natural idea work well. \n\n- The description of the problem and solution is written clearly and easy to understand.\n\n- The proposed approach performs well on multiple benchmarks."
                },
                "weaknesses": {
                    "value": "- A large part of the paper talks about sub-optimality of the FSB policy. For example, this statement \"Although FSB generally achieves high-quality branching, it could still become sub-optimal when the linear programming relaxation is uninformative or there exists dual degeneracy\" Is there more justified argument for this backed by some evidence?\n\n- why choose the proposed algorithm over any existing offline RL algorithm like CQL[1], IQL etc.?\n\n[1] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 1179-1191."
                },
                "questions": {
                    "value": "- What are connections of equation 6 to reward weighed regression?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699130159488,
            "cdate": 1699130159488,
            "tmdate": 1699636458444,
            "mdate": 1699636458444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9KAfbekUpJ",
                "forum": "uHVIxJGwr4",
                "replyto": "v1bnyAe2k5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We want to thank Reviewer nNZN for your positive feedback. Please see the responses below for your questions.\n\n> A large part of the paper talks about sub-optimality of the FSB policy. For example, this statement \"Although FSB generally achieves high-quality branching, it could still become sub-optimal when the linear programming relaxation is uninformative or there exists dual degeneracy\" Is there more justified argument for this backed by some evidence?\n\nThe potential sub-optimality of Strong Branching has been pointed out in previous RL papers [1]. For example, in the multi-knapsack dataset, Strong Branching behaves clearly worse than RL methods. Even in Table 1, it can be observed that SB is worse than RPB.\n\n> why choose the proposed algorithm over any existing offline RL algorithm like CQL[1], IQL etc.?\n\nUsing a hard constraint (ranking) rather than the soft constraint adopted in many previous offline RL algorithms, such as CQL. This is because branching performance is somewhat sensitive to the branching decisions, a very bad branching decision would cause a huge subsequent BnB tree. Therefore, the offline RL agent should be trained more conservatively and that is why we use ranking as the constraint.\n\nPlease also see Table 13 and 14 for the comparison with CQL in our updated version.\n\n> What are connections of equation 6 to reward weighed regression?\n\nEquation 6 can be treated as a coarse-version of the reward weighed regression, since we only use the instant reward rather than the cumulative reward. Using instant rewards does not always make sense in the general RL formulation, but it works here since dual-bound improvement itself can reflect the quality of the branching decisions.\n\n\n[1] Lara Scavuzzo, Feng Yang Chen, Didier Chetelat, Maxime Gasse, Andrea Lodi, Neil Yorke-Smith, and Karen Aardal. Learning to branch with tree MDPs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708974963,
                "cdate": 1700708974963,
                "tmdate": 1700708974963,
                "mdate": 1700708974963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mRM6EGwpZa",
            "forum": "uHVIxJGwr4",
            "replyto": "uHVIxJGwr4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_9gri"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4761/Reviewer_9gri"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an offline Reinforcement Learning (RL) framework for learning to branch (L2B) which reportedly exhibits superior performance with a sub-optimal dataset compared to existing methods that require extensive, high-quality datasets. This advantage is particularly notable in reducing the time to collect datasets for training the models. The reported performance on the MIP instances also indicates the effectiveness of the framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Innovative Formulation:** The novel formulation of L2B as an Offline RL approach using a sub-optimal dataset is a significant departure from traditional methods.\n2. **Efficiency in Data Collection:** The framework requires significantly less time to collect its dataset, enhancing its practicality.\n3. **Performance:** The proposed framework improved performance compared to the GGCN framework on smaller dataset sizes, which is commendable."
                },
                "weaknesses": {
                    "value": "Despite the novelty of the work, I have reservations about the robustness of its results. These concerns are expanded upon in this section and further detailed in the questions that follow. \n\n1. **Lack of Scaling-Generalization Results:** A key aim of collecting datasets on smaller instances is to develop policies that excel on larger, more complex instances. It would be beneficial to see how various models perform on scaled-up versions of instances in various problem categories like SC, MIS, CA, or CFL. How do these policies perform on Medium or Hard instances (scaled-up versions) in SC, MIS, CA, or CFL? Does RCAC retain its performance advantage on scaling up to larger instances?\n\n2. **Insufficient Comparison with Existing Methods:** \n- The paper lacks a thorough comparison with recent advancements in the GGCN framework, particularly the augmented loss function introduced in \"Lookback for Learning to Branch\" (Gupta et al. 2022, https://arxiv.org/abs/2206.14987). It would be insightful to see how RCAC compares to this improved GGCN variant. \n - If I understand correctly, RCAC (S) and GGCN (S) primarily differ in their approach to training despite similarities in other aspects, such as dataset collection. Specifically, GGCN (S) employs a Cross-Entropy loss function, while RCAC (S) is focused on learning a Q-function (and a corresponding policy). The distinctiveness of the RCAC framework lies in its utilization of rewards instead of directly using FSB selections, as is the case with GGCN. However, an alternative comparison could involve integrating rewards into the GGCN framework as an additional signal. This could be achieved, for instance, by employing rewards to modulate the Cross-Entropy loss at each node, similar to how node depth might be used. Demonstrating RCAC's superior performance in this modified context would further reinforce the effectiveness of its RL-based approach as formulated in the study. \n    - It would be valuable to have the values of \\( k \\) specified for each model. I am particularly curious to know whether \\( k > 1 \\) for RCAC(S).\n- Comparisons with other RL methods, especially in terms of dataset size and time efficiency, would also be valuable."
                },
                "questions": {
                    "value": "Clarifications:\n\n1. **Section 3.3:** Should \"representation of the B&B tree\" be replaced with \"representation of the B&B node\" for accuracy? \n2. **Training Dataset for GGCN (H) and RCAC (H):** Are these models trained on the same dataset? Is GGCN (H) trained on a separate dataset collected as specified in the Appendix?\n3. **VHB Dataset Transitions:** Could the authors clarify what constitutes a 'transition' in this context? Does the transition include (s,a,s\u2019) even when FSB is not employed in VHB, which is 0.05 times? Do you discard any transition? How is it ensured that you explore a wide array of instances before 100K transitions are collected?\n4. **S Method Training:** Is the S method trained with only 5K transitions? \n5. **Reward Distribution:** Could the authors provide details on the distribution of reward values in the dataset, perhaps in the Appendix? Information on how this varies with tree depth and how normalization is handled would be valuable.\n6. **Figure 3 Clarity:** What is the specific problem family represented in Figure 3?\n7. **Practicality of H dataset collection:** Given that VHB takes longer than FSB (as indicated in column 2), is it still a practical choice since the performance is worse than S?\n8. **GGCN Expansion:** Could the authors clarify the abbreviation GGCN? It seems to be a variation of GCNN (Graph Convolutional Neural Networks) as used in Gasse et al. 2019.\n9. **Inference Procedure in RCAC:** Are there two forward passes $G_\\omega\\$ and $\\pi_\\phi$ during inference in RCAC? How does this differ from the inference process in GGCN?\n10. **Hyperparameter \\(k\\):** Figure 3 suggests that \\(k\\) has a significant impact on RCAC's performance. Could the authors provide the \\(k\\) values used for each model and dataset?\n\n11. **Aggregation in Table 4:** How are scores aggregated across 20 instances in Table 4? Assuming this is a cumulative sum, RCAC appears to outperform in WA but not against RPB in AP. Can the authors speculate on which problem types might be more amenable to improvement by RCAC?\n\n12. **Reward Ablation:** Could the authors discuss the rationale behind choosing dual bound improvement over primal-dual gap improvement? Understanding the preference for one metric over the other would be enlightening.\n\n\nSuggestions:\n1. **Dataset Comparison:** I think it will be pretty helpful to have a section or a figure demonstrating the difference (transition vs. individual nodes) between the dataset collected using the standard IL methods and the one proposed in this work. \n2. **Statistical Significance:** Please include p-values to indicate the statistical significance of differences in Tables 2 and 3.\n3. **Evaluation Methodology:** Given that 20 seems a relatively small sample size for testing, it's common practice to evaluate each instance with multiple seeds, as demonstrated in Gasse et al. 2019. Could the authors clarify whether a similar approach can be employed in their study?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699190885255,
            "cdate": 1699190885255,
            "tmdate": 1699636458378,
            "mdate": 1699636458378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UEtC49M7QL",
                "forum": "uHVIxJGwr4",
                "replyto": "mRM6EGwpZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4761/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank Reviewer 9gri for your questions and constructive suggestions on our empirical evaluations. Please see the following responses for your questions. \n\n> Lack of Scaling-Generalization Results: \n\nSee Table 11 and 12 in our updated version \n\n\n> The paper lacks a thorough comparison with recent advancements in the GGCN framework, particularly the augmented loss function introduced in \"Lookback for Learning to Branch\".\n\nThis paper focuses more on a better imitation of the strong branching, which is a bit different from what we study. Besides, the code for this paper is not public, we will try to ask the authors for the code or reproduce it in the final version.\n\n> However, an alternative comparison could involve integrating rewards into the GGCN framework as an additional signal.\n\nActually, $G_{\\omega}$ is trained in the way you suggest. Our result in Table 5 indicates that $G_{\\omega}$ has already brought some improvements, but RCAC can further boost the performance. Also, to ensure the reward if informative, we choose the dual-bound rather than the tree size used in previous RL for branching methods.\n\n> Comparisons with other RL methods, especially in terms of dataset size and time efficiency, would also be valuable.\n\nWe actually discuss it in Section 4.2. Here we cite the training time from two recent RL for branching papers, `We set a maximum of 15,000 epochs and a time limit of six days for training` [1] and `we left our retro branching agent to train for \u2248 13 days (\u2248 500k epochs)` [2] . In comparison, the offline models can all be trained within hours.\n\n\n[1] Lara Scavuzzo, Feng Yang Chen, Didier Chetelat, Maxime Gasse, Andrea Lodi, Neil Yorke-Smith, and Karen Aardal. Learning to branch with tree MDPs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\n\n[2] Christopher W. F. Parsonson, Alexandre Laterre, and Thomas D. Barrett. Reinforcement learning for branch-and-bound optimisation using retrospective trajectories. Proceedings of the AAAI Conference on Artificial Intelligence, 37(4):4061\u20134069, Jun. 2023.\n\n\n\n> Section 3.3: Should \"representation of the B&B tree\" be replaced with \"representation of the B&B node\" for accuracy?\n\nYes, thanks for the suggestion and we have updated it.\n\n> Training Dataset for GGCN (H) and RCAC (H): Are these models trained on the same dataset? Is GGCN (H) trained on a separate dataset collected as specified in the Appendix?\n\nYes. Please see Section 4.1, the paragraph above Table 1, for more details about the two datasets. \n\n> VHB Dataset Transitions: Could the authors clarify what constitutes a 'transition' in this context? Does the transition include (s,a,s\u2019) even when FSB is not employed in VHB, which is 0.05 times? Do you discard any transition? How is it ensured that you explore a wide array of instances before 100K transitions are collected?\n\nEach transition, as defined in section 2.3, includes (s,a,s\u2019, r(s,a)), which corresponds to the current state, current action, next state and the instant reward. It is always the same no matter VHB or SB is used. We do not discard any transition or make any sub-sequence sampling due to our basic assumption that collecting the samples is expensive. So the exploration of diverse instances is not considered in our case.\n\n> S Method Training: Is the S method trained with only 5K transitions?\n\nYes. \n\n> Reward Distribution: Could the authors provide details on the distribution of reward values in the dataset, perhaps in the Appendix?\n\nSee Figure 4 in our updated version.\n\n> Figure 3 Clarity: What is the specific problem family represented in Figure 3?\n\nAs the title suggests, it is the CA (combinatorial auction) problem. We change the ranking constraint $k$ and report of performance change of RCAC.\n\n> Practicality of H dataset collection: Given that VHB takes longer than FSB (as indicated in column 2), is it still a practical choice since the performance is worse than S?\n\nThe answer could also be found in the paragraph we refer to in the response to clarification 2. In short, VHB is used to simulate the scenario where the best available heuristics are sub-optimal. Since on these 4 commonly used datasets, SB has already been strong enough, we use VHB to simulate a sub-optimal policy and evaluate the performance of RCAC. Besides, models also have a better performance when trained on the dataset collected by VHB on the CFL problem, so it could also sometimes become a better choice.\n\n> GGCN Expansion: Could the authors clarify the abbreviation GGCN? It seems to be a variation of GCNN (Graph Convolutional Neural Networks) as used in Gasse et al. 2019.\n\nWe actually use GGCN to refer to the method used in Gasse et al. 2019. Since GCN or GCNN is more like a model name, which is also adopted in our learning method, to discriminate the learning method name from the model name, we try to use the author name G(asse)GCN to name the method."
                    },
                    "title": {
                        "value": "Response (1)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708526150,
                "cdate": 1700708526150,
                "tmdate": 1700708539645,
                "mdate": 1700708539645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]