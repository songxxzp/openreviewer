[
    {
        "title": "Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning"
    },
    {
        "review": {
            "id": "SIGSqs7RbQ",
            "forum": "TilcG5C8bN",
            "replyto": "TilcG5C8bN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_wGaZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_wGaZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient SSL approach called SimWnW. Through studying the impact of similar and dissimilar image regions on SSL performance, the authors find that similar regions are less important and removing them in augmented images (and in feature maps) can significantly reduce the computation cost and improve model convergence. To remove similar regions, the authors propose a new method under the ResNet/ConvNet settings. Specifically, a waxing-and-waning process is proposed for region removal while mitigating the region shrinking problem in convolutional layers. Experiments show that SimWnW can reduce the computation cost of SSL without compromising accuracy -- SimWnW yields up to 54% and 51% computation savings in training from scratch and transfer learning tasks, respectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper offers a comprehensive exploration of the impact of similar/dissimilar regions on SSL accuracy, which lays a good foundation for a region removal-based method to improve SSL efficiency.\n- Strong results in efficiency boost are achieved for two representative SSL frameworks.\n- Decent analysis is provided for region removal-related hyper-parameters like similarity threshold and block size."
                },
                "weaknesses": {
                    "value": "- The key hyper-parameter of block removing portion is unspecified, and convincing explanations are missing (see questions below).\n- The comparisons with recent related works seem insufficient, e.g. (Addepalli et al., 2022) and (Koc\u00b8yigit et al., 2023).\n- The proposed waxing-and-waning method is customed too much to ConvNets. It seems hard to translate to transformers and hence transformer-based SOTA SSL methods (this makes the paper title a bit overclaim)."
                },
                "questions": {
                    "value": "Key question around the portion of block removal:\n- Intuitively, comparing similar blocks won't generate too much useful signal for SSL. This is validated by Fig. 2 where the performance of \"Similar Blocks (x\\%)\" is consistently worse than \"Dissimilar Blocks (x\\%)\". On the other hand, comparing dissimilar blocks (after removing similar ones), despite being more useful, has a key hyper-parameter of the removing portion (1-x)\\% which can significantly affect the learning quality. Specifically, if we remove too much, comparing those top dissimilar blocks either makes learning too hard or the dissimilar blocks may not even be semantically related (which hurts SSL quality). If we gradually increase x\\%, the retained blocks would include both dissimilar and relatively similar blocks, which makes the learning signals more balanced for SSL.\n- Fig. 2 shows that SSL performance peaks at \"Dissimilar Blocks (75\\%)\". What's the actually used x\\% after region removal in SimWnW? If it's 75\\% or higher, then it shouldn't lead to that much of computation saving. Fig. 7(a) shows some hint about x in terms of similarity threshold. 1) When the default threshold is set to 20, what's the corresponding x\\%? 2) With the default similarity threshold 20, the SSL performance remains about the same but the training cost is increasing. So again, the computation saving is still concerning. Any comments?\n- One side question, why the compute saving on ImageNet is much smaller than CIFAR 10/100? This suggests the amount of removed blocks from high-resolution ImageNet images is smaller than that of low-resolution CIFAR images, given the same similarity threshold (if that's how it works). Any intuitions about why this is the case?\n\nOther minor questions:\n- To find similar blocks, what's the neighborhood size for searching? Does it depend on augmentation parameters? - since how we crop/rotate/flip images will impact the block locations a lot.\n- For \"block matching\" in pixel space, is PSNR an accurate enough metric? What if the found correspondence is wrong and how well can SimWnW tolerate such errors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Reviewer_wGaZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698482854083,
            "cdate": 1698482854083,
            "tmdate": 1700700572520,
            "mdate": 1700700572520,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "APDxtyh9Rn",
                "forum": "TilcG5C8bN",
                "replyto": "SIGSqs7RbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wGaZ (Part 1/4)"
                    },
                    "comment": {
                        "value": "**We appreciate the valuable comments from the reviewer. We carefully address all the reviewer\u2019s questions and revise the paper accordingly. We hope our response can help alleviate the reviewer's concern.**\n\n---\n\n### **Q1. Comparisons with recent related works, e.g. (Addepalli et al., 2022) and (Koc\u00b8yigit et al., 2023).**\n \nThanks for your suggestion. \n\nWe have added the experiments to compare SimWnW with (Addepalli et al., 2022) and (Koc\u00b8yigit et al., 2023). The experimental result is shown in Table R.9. We can observe that SimWnW consistently provides higher training time reduction with similar accuracy.\n\nSpecifically, (Addepalli et al., 2022) identify the noise present in the training objective as a primary factor contributing to the slow convergence in self-supervised learning. To mitigate this, they introduce rotation prediction as an additional, noise-free training objective, aimed at expediting the model convergence. On the other hand, (Koc\u00b8yigit et al., 2023) propose a strategy to accelerate model convergence through a combination of an innovative learning rate schedule and an input image resolution schedule. They also introduce a new image augmentation technique, designed to improve the quality of augmented images, thereby further accelerating the convergence of the model.\n \nFor these two methods, we modulate their training epochs to ensure a similar accuracy to SimWnW for a fair comparison. The reason why SimWnW outperforms them is that some computation reduction operations applied by these two methods cannot effectively translate into time reduction. For example, using low-resolution images to reduce the training computation might degrade model performance, resulting in the need for more training epochs. We have added the results in Section 5.2 and Table 3 in the revised paper.\n\nWe would also like to point out that our approach and these two methods optimize SSL in different dimension and are complementary. (Addepalli et al., 2022) accelerates the learning process by adding additional rotation prediction tasks; and (Koc\u00b8yigit et al., 2023) accelerates the convergence by adopting a novel learning rate schedule, using smaller input image resolution and novel image augmentation methods. On the other hand, our approach focuses on removing less important regions of the image, which indeed operates in the intra-image level. We also want to mention that we have provided the experimental results on combining our approach with two other SLL methods [1][2] in Section 5.4.\n \n>**Table R.9: Comparison with SOTA efficient SSL methods.**\n| **Method**              \t| **ImageNet** |       \t| **CIFAR-10** |       \t|\n|----------------------------------------------|----------------------------------|---------------|----------------------------------|---------------|\n|                                          \t| Acc.                         \t| Training Time | Acc.                             | Training Time |\n| SimCLR                                   \t| 66.39                        \t| 100\\%     \t| 90.16                        \t| 100\\%     \t|\n| SimCLR + (Addepalli et al., 2022)    \t| 66.21                        \t| 91\\%      \t| 90.04                        \t| 80\\%      \t|\n| SimCLR + (Koc\u00b8yigit et al., 2023)  | 66.30                        \t| 96\\%      \t| 90.12                        \t| 78\\%      \t|\n| **SimCLR + SimWnW (Ours)**   \t| **66.25**                        \t| **89\\%**      \t| **90.10**                        \t| **68\\%**      \t|\n| SimSiam                                  \t| 71.12                        \t| 100\\%     \t| 90.80                        \t| 100\\%     \t|\n| SimSiam + (Addepalli et al., 2022)   \t| 71.20                        \t| 94\\%      \t| 91.19                        \t| 75\\%      \t|\n| SimSiam + (Koc\u00b8yigit et al., 2023) | 71.19                        \t| 95\\%      \t| 91.11                        \t| 81\\%      \t|\n| **SimSiam + SimWnW (Ours)**  \t| **71.28**                        \t| **90\\%**      \t| **91.17**                        \t| **65\\%**      \t|\n\n[1] Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches. ECCV2022.\n\n[2] Rethinking Self-Supervised Learning: Small is Beautiful. arXiv 2103.13559."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553583062,
                "cdate": 1700553583062,
                "tmdate": 1700686478418,
                "mdate": 1700686478418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ITbg4p6wXb",
                "forum": "TilcG5C8bN",
                "replyto": "SIGSqs7RbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wGaZ (Part 2/4)"
                    },
                    "comment": {
                        "value": "### **Q2. Adapting SimWnW to vision transformers.**\n\n \nThank you for your valuable suggestion.\n\nWe would like to clarify that it is applicable to apply SimWnW to ViTs. First, there is indeed no removed region shrinking problem in ViTs, which makes it easier to apply SimWnW to the ViTs. Second, we also do not need to consider the block size since ViTs naturally divide the image into many tokens, typically in the size of 16x16. \n\nTherefore, removing similar blocks can be directly achieved by removing similar input tokens, resulting in a reduced input sequence length.\n\nWe use a well-recognized self-supervised vision transformer learning framework DINO [1] as our baseline. As shown in Table R.10, SimWnW significantly reduces the training cost by 38\\% without accuracy loss compared to the DINO baseline. \n\nWe also compare our approach to an efficient vision transformer training framework EViT [2]. For a fair comparison, we apply EViT to DINO and let it consume the same FLOPs as our approach. As shown in Table R.10, SimWnW achieves 0.82\\% higher accuracy than EViT with the same training computation.\n\n\n>**Table R.10: Evaluation on Vision Transformers. The encoder is DeiT-S and the dataset is ImageNet.**\n| **Method**                 \t| **Acc.** | **Training FLOPs** | **Training Time** |\n|--------------------------------|----------|--------------------|-------------------|\n| DINO                       \t| 58.95\\%  | 100\\%          \t| 100\\%         \t|\n| DINO + EViT                \t| 58.01\\%  | 57\\%           \t| 72\\%          \t|\n| **DINO + SimWnW**  | **58.83\\%**  | **57\\%**           \t| **72\\%**          \t|\n\n\nWe have added the experiments on ViTs in Appendix A and Table 5 in the revised paper.\n \n[1] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[2] Liang, Youwei, et al. \"Not all patches are what you need: Expediting Vision Transformers via Token Reorganizations.\" International Conference on Learning Representations. 2022.\n\n \n---\n\n \n#### **Q3. Intuitively, comparing similar blocks won't generate too much useful signal for SSL. This is validated by Fig. 2 where the performance of \"Similar Blocks (x%)\" is consistently worse than \"Dissimilar Blocks (x%)\". On the other hand, comparing dissimilar blocks (after removing similar ones), despite being more useful, has a key hyper-parameter of the removing portion (1-x)% which can significantly affect the learning quality. Specifically, if we remove too much, comparing those top dissimilar blocks either makes learning too hard or the dissimilar blocks may not even be semantically related (which hurts SSL quality). If we gradually increase x%, the retained blocks would include both dissimilar and relatively similar blocks, which makes the learning signals more balanced for SSL.**\n\nThank you for the feedback.\n\nYes, we agree with you that removing too many similar block pairs will affect the performance of the model. The removing ratio is a hyper-parameter, determined by the similarity threshold. We have provided a sensitivity study on the threshold in the paper in Section 5.3.\n\nRemoving fewer image blocks does help to maintain the model's accuracy. However, on the other hand, for applications that are not particularly sensitive to accuracy, opting for an aggressive threshold can save more computation.\n\nWe added more discussion in Section 5.3 in our revised paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553736037,
                "cdate": 1700553736037,
                "tmdate": 1700608409784,
                "mdate": 1700608409784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KwHzmZXdun",
                "forum": "TilcG5C8bN",
                "replyto": "SIGSqs7RbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wGaZ (Part 3/4)"
                    },
                    "comment": {
                        "value": "### **Q4. Fig. 2 shows that SSL performance peaks at \"Dissimilar Blocks (75%)\". What's the actually used x% after region removal in SimWnW? If it's 75% or higher, then it shouldn't lead to that much of computation saving. Fig. 7(a) shows some hint about x in terms of similarity threshold. 1) When the default threshold is set to 20, what's the corresponding x%?**\n\nThank you for your insightful question.\n\nWe have added the block removal ratio with the default similarity threshold 20 in our experiments, shown in Table R.11. We could see that in most experiments the block removal ratio is larger than 25%. In other words, the x% is lower than 75%.\n\nIn practice, the block removal ratio varies across datasets. This is because different datasets have different image complexity and different resolutions, so the number of similar blocks that can be found under the same threshold varies.\nWe have added these results to Appendix B and Table 6.\n \nIn terms of computation savings, we also want to clarify that the computation saving not only comes from the skipped computation of the removed region but also comes from the faster model convergence since SimWnW removes unnecessary noise or irrelevant features that might slow down the learning process, as stated in Section 5.1 in the paper.\n \n \n>**Table R.11: Image block removal ratio.**\n| **Dataset**   | **ImageNet** | **CIFAR-10** | **CIFAR-100** | **Stanford Cars** | **FGCV Aircraft** | **CUB-200** |\n|---------------|--------------|--------------|---------------|-------------------|-------------------|-------------|\n| **Removal Ratio** | 20\\%     \t| 30\\%     \t| 30\\%      \t| 33\\%          \t| 31\\%          \t| 35\\%    \t|\n \n\n---\n\n\n### **Q5. Fig. 7(a) shows some hint about x in terms of similarity threshold. With the default similarity threshold 20, the SSL performance remains about the same but the training cost is increasing. So again, the computation saving is still concerning. Any comments?**\n \nThanks for your valuable feedback.\n\nI respectfully suppose you are referring to Figure 7(a), which shows that when the similarity threshold is higher than 20, the model performance remains the same but the training cost keeps increasing.\n\nWhen we increase the similarity threshold, it will result in a fewer number of blocks can meet the similarity requirements, indicating fewer blocks can be removed (since we only remove the blocks that are highly similar). This will reduce the computation saving. Basically, the similarity shrehold controls the trade-off between computation saving and accuracy.  \nOn the other hand, the accuracy will not increase as it reaches the upper bound model accuracy. It means that in the benchmark in Figure 7(a), setting the threshold as 20 is the choice that can maximize the training cost reduction without affecting the model accuracy (let\u2019s call it the sweet point of the similarity threshold).\n\nWe would like to point out that we may not be able to find the optimal sweet point of similarity threshold in all applications. However, despite this, SimWnW still can effectively reduce a large amount of computation without affecting the accuracy, as long as the threshold is within a reasonable range.\n\nIn summary, our proposed method does significantly reduce the training FLOPs by skipping the computation on the removed region and accelerating model convergence as removing similar blocks can reduce the complexity of data and let the model focus on more important signals. We also add the results of training time to the results in Tables 1, 2, 3, 4, and 5 in the revised paper to further justify our proposed method.\n \n\n---\n \n### **Q6. Why the compute saving on ImageNet is much smaller than CIFAR 10/100? This suggests the number of removed blocks from high-resolution ImageNet images is smaller than that of low-resolution CIFAR images, given the same similarity threshold (if that's how it works). Any intuitions about why this is the case?**\n \nThanks for your insightful question. \n\nThe reasons why the number of removed blocks from high-resolution ImageNet images is smaller than that of low-resolution CIFAR images are two-fold. \n\nFirst, we apply the block size of 30x30 to the 224x224 images (ImageNet) when performing block-matching. For CIFAR, whose image size is 32x32, we proportionally scale down the block size to 5x5 when doing block-matching. In this case, a larger block used in ImageNet captures more features and details. As the two augmented images have different transformations applied, the aggregated difference over a larger block will be more pronounced than in a smaller block. Therefore, fewer block pairs will be considered similar given the same threshold. \n\nSecond, the images of ImageNet are more complex and contain more details compared to CIFAR, and therefore it is more difficult to find similarities."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553821151,
                "cdate": 1700553821151,
                "tmdate": 1700553821151,
                "mdate": 1700553821151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zu1vIyvaSM",
                "forum": "TilcG5C8bN",
                "replyto": "SIGSqs7RbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wGaZ (Part 4/4)"
                    },
                    "comment": {
                        "value": "### **Q7. To find similar blocks, what's the neighborhood size for searching? Does it depend on augmentation parameters? - since how we crop/rotate/flip images will impact the block locations a lot.**\n \nThank you for the questions. \n\nWe empirically set the neighborhood size to four times the block size. For example, the search space is 60x60 for a 30x30 block. And your understanding is correct, the neighborhood search region size that we pick does take the augmentation method into consideration.\n\nIntuitively, the most similar block could be at the corresponding location after augmentation such as flip or rotate. However, if we take the random scaling and cropping augmentation (zoom-in/zoom-out) into account (which is very critical in contrastive learning), it will cause some trouble. We may no longer be able to find an exact one-to-one correspondence block pair between the online branch and the target branch. This is one of the reasons that we want to search for the most similar block in a small region.  \nAnother reason that we search in a small region (instead of exhaustive search) is to reduce the computation overhead and ensure the semantic consistency of the paired blocks.\n\nIn practice, the image scaling ratio is generally not more than twice, so the length and width of the search region are set to twice the length and width of the block size (e.g., 60x60 search space for 30x30 blocks) to ensure that the most similar blocks are included. \n\nFor better illustration, we also include Table R.12  here, to show the impact of the search region size on accuracy. We have added the explanation and results to Appendix B Table 7 in the revised paper.\n \n>**Table R.12: Accuracy when using different search region sizes. The block size is 30x30 and the results are obtained from the transfer learning experiment on the Stanford Cars dataset. The base framework is SimSiam.**\n| **Size of Search Region** | 45x45 | 60x60 | 75x75 |\n|---------------------------|-------|-------|-------|\n| **Accuracy**          \t| 50.72 | 50.95 | 50.98 |\n\n\n---\n \n### **Q8. For \"block matching\" in pixel space, is PSNR an accurate enough metric? What if the found correspondence is wrong and how well can SimWnW tolerate such errors?**\n\nThanks for your valuable question.\n\nWe agree that PSNR might not be the optimal choice and always pinpoint accurate correspondences. However, based on our extensive experimental results, including: different datasets (e.g., CIFAR10, CIFAR100, ImageNet, Cars, Aircraft, CUB-200), different network structures (e.g., ResNet18, ResNet50, Deit), and different tasks (SSL training from scratch, SSL transfer learning), it suggests that our proposed approach can effectively reduce computation without sacrificing accuracy using PSNR as the metric.\n\nThere is an interesting thing that we would like to mention. As we claimed in section 5, we choose PSNR=20 in our work as the threshold to consider two image blocks are similar. Because, for the common practice in wireless image transmission, using a PSNR value of 20 between a compressed image and its original version is generally considered to have acceptable similarity for many applications. And as proved by our experiments (Fig.7), using a PSNR threshold of 20 for block similarity is also a desirable choice for self-supervised learning, which does not degrade the accuracy while achieving decent computation reduction. This could be considered as a guide for threshold selection for future research and exploring other possible metrics is also a direction that is worth further study.\n\nWe added more discussion in Appendix E in our revised paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553962369,
                "cdate": 1700553962369,
                "tmdate": 1700553962369,
                "mdate": 1700553962369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rxjKpek1Ic",
                "forum": "TilcG5C8bN",
                "replyto": "SIGSqs7RbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wGaZ"
                    },
                    "comment": {
                        "value": "Dear Reviewer wGaZ,\n\nThanks for your time and reviewing efforts! We appreciate your thorough comments.\n\nWe provide suggested results in the authors' response, including more experiments and explanations on the block removal details, experimental results on vision transformers, and comparisons with SOTA efficient self-supervised learning framework. \n\nWe hope our responses have answered your questions. It would be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654265896,
                "cdate": 1700654265896,
                "tmdate": 1700654265896,
                "mdate": 1700654265896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pO18FsIwOl",
                "forum": "TilcG5C8bN",
                "replyto": "rxjKpek1Ic",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Reviewer_wGaZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Reviewer_wGaZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Feedback"
                    },
                    "comment": {
                        "value": "Thanks for your detailed feedback. Most of my concerns have been addressed, and I think it's important to integrate the new results and discussions into the manuscript. I've raised score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700756847,
                "cdate": 1700700756847,
                "tmdate": 1700700756847,
                "mdate": 1700700756847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1NEKQKZ6ap",
            "forum": "TilcG5C8bN",
            "replyto": "TilcG5C8bN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_5JXS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_5JXS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to enhance the efficiency of self-supervised learning (SSL). Based on contrastive SSL methods, such as SimCLR and SimSiam, this paper proposes to reuse and remove the similar regions so as to save computation. To achieve this, this paper first identifies the similarities between regions. However, directly operating on regions would face the region shrinking problem caused by convolution layers, this paper proposes to expand the size of removed region. Compute savings in FLOPs in observed in ImageNet benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Self-supervised learning is computation expensive, this paper proposes to reduce the pretraining cost while preserving the accuracy, which is important topic for the community. \n\n+ The idea of reusing and replacing similar regions is intuitive. Also I am not sure if there are other similar works proposing similar ideas, it is good to see these simple yet effective training techniques."
                },
                "weaknesses": {
                    "value": "- This paper claims that the proposed method is efficient regrading the FLOPs. However, reduced FLOPs may not directly lead to time saving given that the proposed method requires dedicated sparse computation of convolutional kernel. It is important to report the real run time saving to claim efficiency.\n\n- In the title, authors claim the proposed method is generic. It is worth to apply SimWnW to self-supervised vision transformers as well. Moreover, the reuse and replace strategies are expected to be applicable to ViTs since there would be no region shrinking problem in ViTs."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Reviewer_5JXS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608816415,
            "cdate": 1698608816415,
            "tmdate": 1700689741451,
            "mdate": 1700689741451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yEk7kJQPXJ",
                "forum": "TilcG5C8bN",
                "replyto": "1NEKQKZ6ap",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5JXS (Part 1/2)"
                    },
                    "comment": {
                        "value": "**We would like to thank the reviewer for the valuable feedback. We also expect this simple yet effective approach could contribute to the community. We have added more results as suggested by the reviewer, including the experimental results on the training time reduction and the experimental results on vision transformers. We hope our response can help clarify the reviewer's questions.**\n\n---\n\n### **Q1. Experimental results on training time reduction.**\n\nThank you for your insightful suggestion. \n\nWe added more discussion (Appendix D) and results regarding the actual training time reduction in the revised paper. We have added the actual training time reduction to Tables 1, 2, 3, 4, and 5 in the revised paper. Our SimWnW reduces training time by an average of 24% (up to 35%) for CNN models and 28% for ViT. We measure the actual end-to-end training time under the PyTorch framework using Nvidia Tesla P100 GPU. \n\nSpecifically, the FLOPs reduction of our SimWnW mainly comes from two aspects.\n\nFor the first aspect, our SimWnW can improve the model convergency speed, indicating a fewer number of training iterations/epochs required to achieve a target accuracy. **This can directly lead to FLOPs reduction and training time saving, which does NOT require any dedicated sparse computation support.** Specifically, SimWnW removes the less important regions, resulting in removing irrelevant features that slow down the learning process, thereby improving model convergence speed. \n\nThe second aspect of FLOPs reduction is achieved by removing similar blocks.  \n\nFor the ViT-based models, removing similar blocks can be directly achieved by removing similar input tokens, resulting in a reduced input sequence length. **This can also directly achieve acceleration, while does NOT require any dedicated sparse computation support.** For the case of using CNN models, SimWnW indeed requires some support for sparse computation. This is a similar problem faced by the designs in other fields, such as sparse training or weight pruning. This usually can be solved in different ways.\n\nFor general-purpose devices such as GPUs or mobile devices, SimWnW can be supported by using sparse computation libraries and compiler optimizations.\nFor FPGA platforms, the convolution kernels need to be divided into tiles and computed separately. So, the tiling size used in FPGAs can be aligned with the block size used in the SimWnW. In this way, we can easily skip the computation clock cycle for the corresponding block, leading to direct time-saving.\n\nIt is worth mentioning that, in our SimWnW, we remove the entire similar blocks during the computation, which creates a coarse-grained sparsity. Compared to the unstructured or irregular sparsity which is usually used in sparse training or weight pruning works, the coarse-grained sparsity created in our SimWnW is much more friendly for sparse computation acceleration on both general-purpose devices and FPGA platforms.\n\nWe put Tables R.6 and R.7 (corresponding to Tables 1 and 2 in the revised paper) here for your reference for the training time reduction results. Please also refer to Table R.8 in the answer for Question 2 for more results.\n\n\n>**Table R.6: Results of training from scratch experiments. The encoder for ImageNet is ResNet50 and the encoder for CIFAR is ResNet18. The model is trained for 800 epochs.**\n| **Method**            \t| **ImageNet** |  \t|  \t| **CIFAR-10** |\t|  \t| **CIFAR-100** |   |  \t|\n|-------------|----------|--------|---------------|--------------|------------|----------------|--------------|------------|---------|\n|                                     \t| Acc.                     \t| Training FLOPs           \t| Training Time      \t| Acc.     \t| Training FLOPs | Training Time | Acc.     \t| Training FLOPs | Training Time |\n| SimCLR                              \t| 66.39                    \t| 100\\%                    \t| 100\\%              \t| 90.16    \t| 100\\%      \t| 100\\%     \t| 57.34    \t| 100\\%      \t| 100\\%     \t|\n| SimCLR (Same FLOPs as Ours)\t| 64.76             \t| 80\\%              \t| -                 \t| 86.07 | 48\\%\t| -        \t| 50.46 | 52\\%\t| -        \t|\n|  **SimCLR + SimWnW (Ours)**  | **66.25**                    \t| **80\\%**                     \t| **89\\%**               \t| **90.10**    \t| **48\\%**       \t| **68\\%**      \t| **57.69**    \t| **52\\%**       \t| **73\\%**      \t|\n| SimSiam                             \t| 71.12                    \t| 100\\%                    \t| 100\\%              \t| 90.80    \t| 100\\%      \t| 100\\%     \t| 57.21    \t| 100\\%      \t| 100\\%     \t|\n| SimSiam (Same FLOPs as Ours)   | 69.10             \t| 81\\%              \t| -                 \t| 87.28 | 46\\%\t| -        \t| 52.18 | 53\\%\t| -        \t|\n|  **SimSiam + SimWnW (Ours)** | **71.28**                    \t| **81\\%**                     \t| **90\\%**               \t| **91.17**    \t| **46\\%**       \t| **65\\%**     \t| **57.94**    \t| **53\\%**       \t| **71\\%**      \t|"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553402577,
                "cdate": 1700553402577,
                "tmdate": 1700608256309,
                "mdate": 1700608256309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GHRQPkmYuR",
                "forum": "TilcG5C8bN",
                "replyto": "1NEKQKZ6ap",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5JXS (Part 2/2)"
                    },
                    "comment": {
                        "value": "### **Q1. Experimental results on training time reduction. (cont.)**\n \n>**Table R.7: Results of transfer learning experiments. The encoder is ResNet50, which is pre-trained on ImageNet and then trained on the three datasets for 100 epochs using self-supervised learning methods. After that, we apply linear evaluation protocol to get the accuracy results.**\n| **Method**                          \t| **Stanford Cars** |        \t|              \t| **FGCV Aircraft** |        \t|              \t| **CUB-200**  |            |              \t|\n|-----------------------------------------|-------------------|----------------|----------------------|-------------------|----------------|----------------------|--------------|----------------|----------------------|\n|                                     \t| Acc.          \t| Training FLOPs | Training Time | Acc.          \t| Training FLOPs | Training Time | Acc.         | Training FLOPs | Training Time |\n| SimCLR                              \t| 46.12         \t| 100\\%      \t| 100\\%     \t| 48.43         \t| 100\\%      \t| 100\\%     \t| 35.78    \t| 100\\%      \t| 100\\%     \t|\n| SimCLR (Same FLOPs as Ours)\t| 43.03      | 53\\%\t| -        \t| 46.74  \t| 51\\%    | -        \t| 33.07 | 55\\%\t| -        \t|\n| **SimCLR + SimWnW (Ours)**  | **46.38**         \t| **53\\%**       \t| **79\\%**      \t| **48.26**         \t| **51\\%**       \t| **75\\%**      \t| **36.15**    \t| **55\\%**       \t| **84\\%**      \t|\n| SimSiam                             \t| 50.87         \t| 100\\%      \t| 100\\%     \t| 51.82         \t| 100\\%      \t| 100\\%     \t| 38.40    \t| 100\\%      \t| 100\\%     \t|\n| SimSiam (Same FLOPs as Ours)   | 46.22      | 50\\%\t| -        \t| 48.17  \t| 55\\%    | -        \t| 36.80 | 49\\%\t| -        \t|\n| **SimSiam + SimWnW (Ours)** | **50.95**         \t| **50\\%**       \t| **78\\%**      \t| **51.76**         \t| **55\\%**       \t| **73\\%**      \t| **38.22**    \t| **49\\%**       \t| **74\\%**      \t|\n \n\n \n\n---\n\n\n### **Q2. In the title, authors claim the proposed method is generic. It is worth to apply SimWnW to self-supervised vision transformers as well. Moreover, the reuse and replace strategies are expected to be applicable to ViTs since there would be no region shrinking problem in ViTs.**\n\n\nThank you for your valuable suggestion.\n\nThere is indeed no removed region shrinking problem in ViTs, which makes it easier to apply SimWnW to the ViTs. We also do not need to consider the block size since ViTs naturally divide the image into many tokens, typically in the size of 16x16. Therefore, removing similar blocks can be directly achieved by removing similar input tokens, resulting in a reduced input sequence length.\n\nWe use a well-recognized self-supervised vision transformer learning framework DINO [1] as our baseline. As shown in Table R.8, SimWnW significantly reduces the training cost by 38\\% without accuracy loss compared to the DINO baseline. \n\nWe also compare our approach to an efficient vision transformer training framework EViT [2]. For a fair comparison, we apply EViT to DINO and let it consume the same FLOPs as our approach. As shown in Table R.8, SimWnW achieves 0.82\\% higher accuracy than EViT with the same training computation.\n\n>**Table R.8: Evaluation on Vision Transformers. The encoder is DeiT-S and the dataset is ImageNet.**\n| **Method**                 \t| **Acc.** | **Training FLOPs** | **Training Time** |\n|--------------------------------|----------|--------------------|-------------------|\n| DINO                       \t| 58.95\\%  | 100\\%          \t| 100\\%         \t|\n| DINO + EViT                \t| 58.01\\%  | 57\\%           \t| 72\\%          \t|\n| **DINO + SimWnW**  | **58.83\\%**  | **57\\%**           \t| **72\\%**          \t|\n\n\n\nWe have added the experiments on ViTs in Appendix A and Table 5 in the revised paper.\n\n\n[1] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[2] Liang, Youwei, et al. \"Not all patches are what you need: Expediting Vision Transformers via Token Reorganizations.\" International Conference on Learning Representations. 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553465274,
                "cdate": 1700553465274,
                "tmdate": 1700608350342,
                "mdate": 1700608350342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TF0VTGdi9Z",
                "forum": "TilcG5C8bN",
                "replyto": "1NEKQKZ6ap",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5JXS"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5JXS,\n\nThanks for your time and reviewing efforts! We appreciate your valuable comments.\n\nWe provide suggested results in the authors' response, including the data and discussion on training time reduction, and experimental results on vision transformers. We hope our responses have answered your questions. It would be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653883602,
                "cdate": 1700653883602,
                "tmdate": 1700653883602,
                "mdate": 1700653883602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKO794SMuN",
                "forum": "TilcG5C8bN",
                "replyto": "TF0VTGdi9Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Reviewer_5JXS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Reviewer_5JXS"
                ],
                "content": {
                    "title": {
                        "value": "Good rebuttal, raise my score from 5 to 6"
                    },
                    "comment": {
                        "value": "Hi, Thanks for your additional results. They've solved my questions and I've raised my score from 5 to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689712078,
                "cdate": 1700689712078,
                "tmdate": 1700689712078,
                "mdate": 1700689712078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P70hwP3rgo",
                "forum": "TilcG5C8bN",
                "replyto": "1NEKQKZ6ap",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5JXS"
                    },
                    "comment": {
                        "value": "Thank you for raising the score and your time spent reviewing our paper. This is a great affirmation of our work. Your comments are very constructive (e.g., providing actual training time reduction results and applying our method to vision transformers to show generalizability), which makes our paper stronger. We have addressed all your comments in our revision. Thank you again for your valuable time.\n\nBest,\n\nAuthor"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696217860,
                "cdate": 1700696217860,
                "tmdate": 1700696217860,
                "mdate": 1700696217860,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EyI4OL18ga",
            "forum": "TilcG5C8bN",
            "replyto": "TilcG5C8bN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_Aoaq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_Aoaq"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to improve the training efficiency of self-supervised learning (SSL) and they propose a similarity-based SSL framework called SIMWNW. SIMWNW removes less important regions (remove most similar regions in two views) in augmented images and feature maps and saves the training cost. Experimental results show that SIMWNW reduces the amount of computation costs in SSL."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow.\n2. The authors analyze the importance of different regions on augmented images by removing and reusing similar blocks for the two branches.\n3. The authors show that the removed region will shrink after convolution operation and they propose to expand the size of removed region in the feature map.\n4. Experimental results show that the proposed method can achieve comparable accuracy using fewer training FLOPs."
                },
                "weaknesses": {
                    "value": "1. Compared with the training FLOPs, the actual time used for training is more important, and the authors did not report it. How much the proposed method can reduce the training time is what we are concerned about. Steps such as matching in the method cannot actually be reflected intuitively through FLOPs.\n2. In Table1 and Table2, the authors should list the accuracy of the baseline methods using the same training overhead. For example, how much lower will simclr be than the proposed method when using 80% overhead?\n3. Do the training FLOPs in Table2 refer to pre-training or downstream fine-tuning? If it is the former, why is it different from Table1\uff1fIf it is the latter, how is the proposed method used in single-branch supervised learning?\n4. From Figure 6, I cannot see the obvious advantages of the proposed method. I suggest the author change the horizontal axis to training hours.\n5. Some related works [1], [2].\n\n[1] Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches. ECCV2022.\n\n[2] Rethinking Self-Supervised Learning: Small is Beautiful. arXiv 2103.13559."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Reviewer_Aoaq",
                        "ICLR.cc/2024/Conference/Submission6644/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632401671,
            "cdate": 1698632401671,
            "tmdate": 1700798970703,
            "mdate": 1700798970703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qeoW03VSdd",
                "forum": "TilcG5C8bN",
                "replyto": "EyI4OL18ga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Aoaq (Part 1/3)"
                    },
                    "comment": {
                        "value": "**We appreciate the valuable comments from the reviewer. We carefully address all the reviewer\u2019s questions and revise the paper accordingly. We hope our response can help alleviate the reviewer's concern.**\n\n---\n\n### **Q1. Experimental results on training time reduction.**\n \nThanks for your insightful review. \n\nWe have added the actual training time reduction to Tables 1, 2, 3, 4, and 5 in the revised paper. We measure the end-to-end training time under the PyTorch framework using Nvidia Tesla P100 GPU. Specifically, our SimWnW reduces training time by an average of 24% (up to 35%) for CNN models and 28% for ViT. \n\nPlease also refer to Table R.2 and Table R.3 in the answer to Question 2 for details.\n\nWe have also provided a detailed discussion of the training time reduction and FLOPs reduction brought by our approach in the latest response \"Author Response on discussions of training time and FLOPs reduction\" below. \n\nWe have added this discussion to Appendix D in the revised paper.\n\n---\n\n### **Q2. Experimental results when SimCLR and SimSiam baselines consume the same training overhead as our proposed method.**\n\nThank you for the suggestion.\n\nWe have added the results when SimCLR and SimSiam baselines consume the same training overhead with our proposed method in Table 1 and Table 2 in the revised paper. As shown in Table R.2 and Table R.3 (Tables 1 and 2 in the paper), SimWnW consistently provides significantly higher accuracy, 3.5\\% on average (up to 7.23\\%) compared to the baseline consuming the same training overhead. This further strengthens the effectiveness of our proposed method.\n \n>**Table R.2: Results of training from scratch experiments. The encoder for ImageNet is ResNet50 and the encoder for CIFAR is ResNet18. The model is trained for 800 epochs.**\n| **Method**            \t| **ImageNet** |  \t|  \t| **CIFAR-10** |\t|  \t| **CIFAR-100** |   |  \t|\n|----|-----|-----|----|-----|-----|-----|---|-----|---|\n|                                     \t| Acc.                     \t| Training FLOPs           \t| Training Time      \t| Acc.     \t| Training FLOPs | Training Time | Acc.     \t| Training FLOPs | Training Time |\n| SimCLR                              \t| 66.39                    \t| 100\\%                    \t| 100\\%              \t| 90.16    \t| 100\\%      \t| 100\\%     \t| 57.34    \t| 100\\%      \t| 100\\%     \t|\n| SimCLR (Same FLOPs as Ours)\t| 64.76             \t| 80\\%              \t| -                 \t| 86.07 | 48\\%\t| -        \t| 50.46 | 52\\%\t| -        \t|\n|  **SimCLR + SimWnW (Ours)**  | **66.25**                    \t| **80\\%**                     \t| **89\\%**               \t| **90.10**    \t| **48\\%**       \t| **68\\%**      \t| **57.69**    \t| **52\\%**       \t| **73\\%**      \t|\n| SimSiam                             \t| 71.12                    \t| 100\\%                    \t| 100\\%              \t| 90.80    \t| 100\\%      \t| 100\\%     \t| 57.21    \t| 100\\%      \t| 100\\%     \t|\n| SimSiam (Same FLOPs as Ours)   | 69.10             \t| 81\\%              \t| -                 \t| 87.28 | 46\\%\t| -        \t| 52.18 | 53\\%\t| -        \t|\n|  **SimSiam + SimWnW (Ours)** | **71.28**                    \t| **81\\%**                     \t| **90\\%**               \t| **91.17**    \t| **46\\%**       \t| **65\\%**     \t| **57.94**    \t| **53\\%**       \t| **71\\%**      \t|\n \n\n\n>**Table R.3: Results of transfer learning experiments. The encoder is ResNet50, which is pre-trained on ImageNet and then trained on the three datasets for 100 epochs using self-supervised learning methods. After that, we apply linear evaluation protocol to get the accuracy results.**\n| **Method**                          \t| **Stanford Cars** |        \t|              \t| **FGCV Aircraft** |        \t|              \t| **CUB-200**  |            |              \t|\n|-----|------|-----|-----|-----|-----|------|-----|-----|-------|\n|                                     \t| Acc.          \t| Training FLOPs | Training Time | Acc.          \t| Training FLOPs | Training Time | Acc.         | Training FLOPs | Training Time |\n| SimCLR                              \t| 46.12         \t| 100\\%      \t| 100\\%     \t| 48.43         \t| 100\\%      \t| 100\\%     \t| 35.78    \t| 100\\%      \t| 100\\%     \t|\n| SimCLR (Same FLOPs as Ours)\t| 43.03      | 53\\%\t| -        \t| 46.74  \t| 51\\%    | -        \t| 33.07 | 55\\%\t| -        \t|\n| **SimCLR + SimWnW (Ours)**  | **46.38**         \t| **53\\%**       \t| **79\\%**      \t| **48.26**         \t| **51\\%**       \t| **75\\%**      \t| **36.15**    \t| **55\\%**       \t| **84\\%**      \t|\n| SimSiam                             \t| 50.87         \t| 100\\%      \t| 100\\%     \t| 51.82         \t| 100\\%      \t| 100\\%     \t| 38.40    \t| 100\\%      \t| 100\\%     \t|\n| SimSiam (Same FLOPs as Ours)   | 46.22      | 50\\%\t| -        \t| 48.17  \t| 55\\%    | -        \t| 36.80 | 49\\%\t| -        \t|\n| **SimSiam + SimWnW (Ours)** | **50.95**         \t| **50\\%**       \t| **78\\%**      \t| **51.76**         \t| **55\\%**       \t| **73\\%**      \t| **38.22**    \t| **49\\%**       \t| **74\\%**      \t|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553131467,
                "cdate": 1700553131467,
                "tmdate": 1700714101919,
                "mdate": 1700714101919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q0aezsOjY7",
                "forum": "TilcG5C8bN",
                "replyto": "EyI4OL18ga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Aoaq (Part 2/3)"
                    },
                    "comment": {
                        "value": "### **Q3. Do the training FLOPs in Table2 refer to pre-training or downstream fine-tuning? If it is the former, why is it different from Table1\uff1fIf it is the latter, how is the proposed method used in single-branch supervised learning?**\n \nThanks for your valuable question and we apologize for the confusion.\n\nIn the transfer learning settings, we follow the setup used in prior works [1].\nThe backbone network ResNet50 is initialized using ImageNet-pretrained weights. Then the ResNet50 is trained on downstream datasets (Cars, Aircraft, and CUB200) using a **self-supervised learning (SSL) method**. Finally, the backbone network ResNet50 is evaluated using the linear evaluation protocol, which is supervised learning.\n\nIn summary, we do not directly transfer the ImageNet-trained ResNet50 to the downstream datasets using the supervised learning method. Instead, we conduct self-supervised learning on the downstream datasets. So, the training FLOPs in Table 2 refer to downstream self-supervised fine-tuning.\nWe clarify it in the caption in Table 2 in the revised paper.\n \n**For a more comprehensive comparison, we also add the experimental results of directly fine-tuning the pre-trained model on the downstream task without self-supervised learning on them.** In this experiment, SimWnW is applied to SimCLR and SimSiam baselines during the self-supervised pre-training in ImageNet. After that, we directly transfer the pre-trained ResNet50 model to downstream datasets using supervised learning. As shown in Table R.4, applying SimWnW during the pre-training stage will not hurt the accuracy.\n\nWe have added this experiment in Appendix C and Table 8 in the revised paper.\n \n>**Table R.4: Accuracy results of transferability experiment.**\n| **Method**                          \t| **Stanford Cars** | **FGCV Aircraft** | **CUB-200** |\n|-----------------------------------------|-------------------|-------------------|-------------|\n| SimCLR                            \t| 38.49         \t| 40.46         \t| 30.10   \t|\n| **SimCLR + SimWnW (Ours)** | **39.90**         \t| **41.28**         \t| **30.19**   \t|\n| SimSiam                             \t| 39.78         \t| 38.37         \t| 32.89   \t|\n| **SimSiam + SimWnW (Ours)** | **40.46**         \t| **38.92**         \t| **33.66**   \t|\n \n[1] Shu, Yangyang, Anton van den Hengel, and Lingqiao Liu. \"Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n\n---\n\n \n### **Q4. From Figure 6, I cannot see the obvious advantages of the proposed method. I suggest the author change the horizontal axis to training hours.**\n \nThanks for your constructive suggestion. We have modified the horizontal axis of Figure 6 to training hours in the revised paper. Indeed, this can further demonstrate the advantages of our method."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553183111,
                "cdate": 1700553183111,
                "tmdate": 1700740359063,
                "mdate": 1700740359063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "scKtzqsFXI",
                "forum": "TilcG5C8bN",
                "replyto": "EyI4OL18ga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Aoaq (Part 3/3)"
                    },
                    "comment": {
                        "value": "### **Q5. Some related works [1], [2].**\n\n**[1] Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches. ECCV2022.**\n\n**[2] Rethinking Self-Supervised Learning: Small is Beautiful. arXiv 2103.13559.**\n\nThanks for the valuable review.\n\nThese two works are complementary to SimWnW. And SimWnW can further enhance the performance of these two works. As shown in Table R.5, integrating SimWnW to the Fast-MoCo [1] and S3L [2] can further reduce the training cost by 23\\% and 30\\% without accuracy loss, respectively, demonstrating the compatibility of SimWnW.\n\n>**Table R.5: Compatibility of SimWnW with SOTA efficient SSL framework. The encoder is ResNet50 and the dataset is ImageNet.**\n| **Method**                      \t| **Acc.** | **Training Time** |\n|-------------------------------------|----------|-------------------|\n| MoCo v2                         \t| 71.12\t| 100\\%         \t|\n| S3L (MoCo v2 based)                            \t| 69.96\t| 65\\%          \t|\n| **S3L + SimWnW**   \t| **70.06**\t| **46\\%**          \t|\n| MoCo v3                         \t| 72.28\t| 100\\%         \t|\n| Fast-MoCo (MoCo v3 based)                   \t| 72.46\t| 30\\%          \t|\n| **Fast-MoCo + SimWnW** | **72.30**\t| **23\\%**          \t|\n\n\nSpecifically, Fast-MoCo accelerates the training process by adding more positive pairs to regulate the training process, thereby accelerating convergence. S3L accelerates the training process by using smaller-resolution images and a partial backbone network. On the other hand, SimWnW accelerates the SSL in a different dimension, which removes less important image blocks during training.\n\nWe integrate our proposed method SimWnW into these two frameworks. We follow the setting in their paper and use the MoCo v2 framework as the baseline for S3L and MoCo v3 as the baseline for Fast-MoCo. Specifically, Fast-MoCo divides the input image in the online branch into four patches and then combines their four output embeddings to form six new embeddings, each of which involves two patches. In this case, the number of positive pairs is six times as normal training. Thus, it can get more supervision signals in each iteration and thus achieves promising performance with fewer iterations. For S3L, we follow their original setting for the ImageNet experiment in their paper, which uses 52x52 input images to train the model for 800 epochs and then uses 224x224 input images to train the model for 200 epochs.\n\nWe have added these experimental results in Section 5.4 and Table 4 in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553270781,
                "cdate": 1700553270781,
                "tmdate": 1700608145350,
                "mdate": 1700608145350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4b8vD9m5Ly",
                "forum": "TilcG5C8bN",
                "replyto": "EyI4OL18ga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Aoaq"
                    },
                    "comment": {
                        "value": "Dear Reviewer Aoaq,\n\nThanks for your time and reviewing efforts! We appreciate your constructive comments.\n\nWe provide suggested results in the authors' response, such as the training time reduction, more comparison with baselines, updates on figures, clarification and more experiments on transfer learning experiments, and experiments concerning other related works.\n\nWe hope our responses have answered your questions. It would be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653621733,
                "cdate": 1700653621733,
                "tmdate": 1700653621733,
                "mdate": 1700653621733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "exOKBjNVMQ",
                "forum": "TilcG5C8bN",
                "replyto": "EyI4OL18ga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response on discussions of training time and FLOPs reduction"
                    },
                    "comment": {
                        "value": "### **Q.1 Experimental results on training time reduction. (Cont.)**\n\n**Here we provide more discussions about the FLOPs and training time reduction brought by our approach.**\n\nThe FLOPs reduction of our SimWnW mainly comes from two aspects.\n\nFor the first aspect, our SimWnW can improve the model convergency speed, indicating a fewer number of training iterations/epochs required to achieve a target accuracy. **This can directly lead to FLOPs reduction and training time saving, which does NOT require any dedicated sparse computation support.** Specifically, SimWnW removes the less important regions, resulting in removing irrelevant features that slow down the learning process, thereby improving model convergence speed. \n\nThe second aspect of FLOPs reduction is achieved by removing similar blocks.  \n\nFor the ViT-based models, removing similar blocks can be directly achieved by removing similar input tokens, resulting in a reduced input sequence length. **This can also directly achieve acceleration, while does NOT require any dedicated sparse computation support.** For the case of using CNN models, SimWnW indeed requires some support for sparse computation. This is a similar problem faced by the designs in other fields, such as sparse training or weight pruning. This usually can be solved in different ways.\n\nFor general-purpose devices such as GPUs or mobile devices, SimWnW can be supported by using sparse computation libraries and compiler optimizations.\nFor FPGA platforms, the convolution kernels need to be divided into tiles and computed separately. So, the tiling size used in FPGAs can be aligned with the block size used in the SimWnW. In this way, we can easily skip the computation clock cycle for the corresponding block, leading to direct time-saving.\n\nIt is worth mentioning that, in our SimWnW, we remove the entire similar blocks during the computation, which creates a coarse-grained sparsity. Compared to the unstructured or irregular sparsity which is usually used in sparse training or weight pruning works, the coarse-grained sparsity created in our SimWnW is much more friendly for sparse computation acceleration on both general-purpose devices and FPGA platforms."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714485680,
                "cdate": 1700714485680,
                "tmdate": 1700714485680,
                "mdate": 1700714485680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IRNjctjFRK",
            "forum": "TilcG5C8bN",
            "replyto": "TilcG5C8bN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_vumm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6644/Reviewer_vumm"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for improving the efficiency of SSL methods by discarding features in augmented images and feature maps that are deemed less important, saving computation and reducing the risk of slowing the learning process by providing irrelevant features. The authors propose to remove blocks from pairs of augmented images that share high semantic similarity, in order to prevent unnecessary processing of irrelevant information such as image backgrounds. To this end, they provide a method for semantic matching of block pairs in images, their removal, and the treatment of the resulting feature maps throughout the network. Authors show results for training from scratch and transfer learning compared to a number of other SSL methods, in most cases showing barely degraded performance - or even improved performance - at a significantly reduced computational cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors provide a sensible method for improving computational efficiency of SSL methods, one of their main challenges currently. The authors are very thorough in motivating and describing their method, using illustrative examples throughout the paper. Experimental results are impressive, the proposed method shows good performance in its ability to reduce computational cost while retaining model performance.  A very sound paper overall, with good experimental design. Given that the authors spend some time sculpting the manuscript to improve its readability for the rebuttal, I think it represents an interesting and valuable addition to the CVPR proceedings."
                },
                "weaknesses": {
                    "value": "Overall readability of the paper could be improved, I\u2019m having a bit of a hard time understanding some of the specifics of the approach as outlined in 3.1 and 3.2. Specifically, the block matching as outlined in paragraphs 1 and 2 under 3.1 seem to overlap; from my understanding you first search for most similar block pairs (paragraph 1) after which you calculate similarity for all block pairs (paragraph 2)? Why not calculate similarity for all block pairs directly?\n\nUnder 4.1, you indicate that, for a given pair of original and augmented image, you divide the first into blocks and loop for a similar block in the paired image. However, instead of performing an exhaustive search over all possible blocks in the augmented image, you narrow the search to \u201ca specific region surrounding a block\u2019s counterpart in the paired augmented image\u201d to ensure semantic consistency. Where does this block\u2019s counterpart come from? Is it simply the same augmentation applied to the block in the original image, i.e. the location of the original block under a flip? In this case, why would the same block in the augmented image not be the most similar block? Semantically, their content is identical is it not? Could you give an intuition as to why you would want to pair image blocks in the same region in the online and target images but not simply pair exact matches under augmentation?"
                },
                "questions": {
                    "value": "Could you give a little more explanation for figure 1. In my opinion, the first two paragraphs of 3.1 read a bit confusingly. What is the distinction between the block matching described in the first paragraph and the similarity calculation after the creation of block pairs in the second paragraph? Aren\u2019t they overlapping?\n\nHow does computational complexity of the block-matching factor into the overall training complexity? I.e. do the FLOPs listed in tables 1 and 2 contain the overhead for your method? I think this should definitely be taken into account."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6644/Reviewer_vumm"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832766304,
            "cdate": 1698832766304,
            "tmdate": 1700920272440,
            "mdate": 1700920272440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sTf8PrOrd8",
                "forum": "TilcG5C8bN",
                "replyto": "IRNjctjFRK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer vumm (Part 1/2)"
                    },
                    "comment": {
                        "value": "**We would like to thank the reviewer for the positive feedback and valuable questions. We appreciate the reviewer's acknowledgment that our proposed work is sensible and the experimental results are impressive. We carefully address all the reviewer\u2019s questions and revise the paper accordingly. We hope our response can help clarify the reviewer's questions.**\n\n---\n\n#### **Q1. The block matching as outlined in paragraphs 1 and 2 under 3.1 seem to overlap; from my understanding you first search for most similar block pairs (paragraph 1) after which you calculate similarity for all block pairs (paragraph 2)? Why not calculate similarity for all block pairs directly? Could you give a little more explanation for figure 1. In my opinion, the first two paragraphs of 3.1 read a bit confusingly. What is the distinction between the block matching described in the first paragraph and the similarity calculation after the creation of block pairs in the second paragraph? Aren\u2019t they overlapping?**\n\nThanks for your thorough review and we apologize for the confusion. \n\nThe process we described In Section 3.1 consists of two steps: \n\n* Step 1 (Paragraph 1): For each image block in the online branch, we do directly calculate the similarity between this image block and all the blocks from the target branch, and find the most similar block on the target branch according to the similarity to form a block pair. If we assume there are 64 image blocks in the online branch, then we do the same thing for all 64 image blocks and obtain 64 image pairs. \n\n* Step 2 (Paragraph 2): Since we cannot remove the entire image (i.e., all 64 blocks), we need to figure out which blocks should be removed. Therefore, we rank/sort them according to the similarity score (obtained in step 1). And we tried to remove the most similar or most dissimilar p% block pairs respectively to see the impact on accuracy.\n\nSo, your understanding is correct. There is no need to recalculate the similarity in the second step. We have modified the text of paragraph 2 in Section 3.1 to clarify it."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552786707,
                "cdate": 1700552786707,
                "tmdate": 1700554005772,
                "mdate": 1700554005772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nJqJYvACVS",
                "forum": "TilcG5C8bN",
                "replyto": "IRNjctjFRK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer vumm (Part 2/2)"
                    },
                    "comment": {
                        "value": "#### **Q2. Under 4.1, you indicate that, for a given pair of original and augmented image, you divide the first into blocks and loop for a similar block in the paired image. However, instead of performing an exhaustive search over all possible blocks in the augmented image, you narrow the search to \u201ca specific region surrounding a block\u2019s counterpart in the paired augmented image\u201d to ensure semantic consistency. Where does this block\u2019s counterpart come from? Is it simply the same augmentation applied to the block in the original image, i.e. the location of the original block under a flip? In this case, why would the same block in the augmented image not be the most similar block? Semantically, their content is identical is it not? Could you give an intuition as to why you would want to pair image blocks in the same region in the online and target images but not simply pair exact matches under augmentation?**\n\nThank you for your questions. \n\nIntuitively, the most similar block could be at the corresponding location after augmentation such as flip or rotate. However, if we take the **random scaling and cropping augmentation (zoom-in/zoom-out)** into account (which is very critical in contrastive learning), it will cause some trouble. We may no longer be able to find an exact one-to-one correspondence block pair between the online branch and the target branch. This is one of the reasons that we want to search for the most similar block in a small region.\n\nAnother reason that we search in a small region (instead of exhaustive search) is to reduce the computation overhead and ensure the semantic consistency of the paired blocks.\n\nFor better illustration, we also include Table R.1 here, to show the impact of the search region size on accuracy. We have added the detailed explanation and results to Appendix B and Table 7 in the revised paper.\n\n>**Table R.1: Accuracy when using different search region sizes. The block size is 30x30 and the results are obtained from the transfer learning experiment on the Stanford Cars dataset. The base framework is SimSiam.**\n| **Size of Search Region** | 45x45 | 60x60 | 75x75 |\n|---------------------------|-------|-------|-------|\n| **Accuracy**          \t| 50.72 | 50.95 | 50.98 |\n\n\n\n---\n\n### **Q3. Do the FLOPs listed in Tables 1 and 2 contain the overhead for your method?**\n\nYes, all the training FLOPs listed in this paper include the overhead of block-matching. The detail of the overhead calculation for block-matching is presented in Appendix F in the revised paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552833216,
                "cdate": 1700552833216,
                "tmdate": 1700554027958,
                "mdate": 1700554027958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "moBKMIu7PA",
                "forum": "TilcG5C8bN",
                "replyto": "IRNjctjFRK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6644/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer vumm"
                    },
                    "comment": {
                        "value": "Dear Reviewer vumm,\n\nThanks for your time and reviewing efforts! We appreciate your constructive comments.\n\nWe provide explanations and clarifications of the questions in the review in the author's response. It would be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6644/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654540755,
                "cdate": 1700654540755,
                "tmdate": 1700654540755,
                "mdate": 1700654540755,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]