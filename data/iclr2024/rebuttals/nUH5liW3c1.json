[
    {
        "title": "When Hard Negative Sampling Meets Supervised Contrastive Learning"
    },
    {
        "review": {
            "id": "VbwZ4jgc8I",
            "forum": "nUH5liW3c1",
            "replyto": "nUH5liW3c1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1484/Reviewer_owdS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1484/Reviewer_owdS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new supervised contrastive learning objective function called SCHaNe, which addresses the limitations of the cross-entropy objective function used in pre-trained image models. SCHaNe incorporates hard negative sampling during fine-tuning to enhance the efficacy of contrastive learning. Experimental results demonstrate that SCHaNe outperforms the baseline model BEiT-3 in Top-1 accuracy across twelve benchmarks, with significant gains in few-shot learning settings and full-dataset fine-tuning. The proposed objective function sets a new state-of-the-art for base models on ImageNet-1k, achieving an accuracy of 86.14%. Additionally, the paper shows that SCHaNe produces better embeddings and explains the improved effectiveness observed in the experiments. Overall, the contributions of this work include the introduction of SCHaNe and its superior performance in few-shot learning and full dataset fine-tuning, establishing new state-of-the-art results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method has been validated on multiple datasets, and comprehensive experiments have been conducted on downstream datasets.\n\n2. The work appears to be relatively comprehensive, with a clear motivation, detailed method description, and important parameter ablation experiments. Overall, it seems well-executed and promising."
                },
                "weaknesses": {
                    "value": "1. To enhance the credibility of our research, you should consider using the same base models (such as ViT or Swin) as other studies for our baseline in Table 1 and Table 2.\n\n2. In order to provide a more comprehensive analysis, the results in Table 1 and Table 2 should include the performance of contrastive learning without the hard negative mining method.\n\n3. The representation of Formula 3 needs to be clarified to ensure better understanding, as it is currently not very clear.\n\n4. To provide a more complete comparison, Figure 3 and Figure 4 should include the results of BEiT-3-CE + contrastive learning, in addition to the results of our proposed method.\n\n5. In each comparative experiment, it is important to clearly indicate the improvement achieved by the hard negative mining method on top of the performance obtained with CE + contrastive learning. This will help demonstrate the added value of our approach."
                },
                "questions": {
                    "value": "1. Why did you choose BEiT-3 as your base model? There are other base models like DINOv2, CLIP, etc.\n\n2. Cross-entropy and contrastive learning have similar forms, so why is the hard negative sampling only applied to the contrastive learning part and not to the cross-entropy part?\n\n3. In general, fine-tuning with cross-entropy (CE) loss is not strongly coupled with the batch size, while contrastive learning can be affected by the batch size. This can limit the flexibility of fine-tuning with CE when combined with contrastive learning. What are your thoughts on this issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698558312184,
            "cdate": 1698558312184,
            "tmdate": 1699636077562,
            "mdate": 1699636077562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gRmFBF2Hec",
                "forum": "nUH5liW3c1",
                "replyto": "VbwZ4jgc8I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer owdS -- question 1 (part1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. Please find detailed responses below. Moreover, should you require more evidence or data to consider a higher rating for our paper, we would be grateful if you could specify these needs. We are eager to comply to the best of our abilities.\n>Question 1. Why did you choose BEiT-3 as your base model? There are other base models like DINOv2, CLIP, etc.\n\nWe conducted a thorough evaluation of different models, including CLIP, before selecting BEiT-3 as the base model for our study. Our decision was mainly based on BEiT-3's status as a Multimodal Large Language Model and its outstanding performance in image classification tasks. Nevertheless, I attached the results of other models, MAE [2] and ResNet50 [3]. The result demonstrates the observed improvements are not limited to BEiT-3 and show the wide applicability of our proposed loss objective. Furthermore, our selection of BEiT-3 was driven by a combination of the model's alignment with our research goals, the feasibility of conducting extensive experiments within our resource constraints, and its state-of-the-art performance. BEiT-3 was the most suitable platform for our study, guaranteeing robust and significant results, while DINOv2 and CLIP are also commendable but outside the scope of the paper. We further describe our rationale below.\n\n| Model    | FT method | Alpha | CIFAR 100 | CUB-200 | Caltech256 | Oxford-Flowers | Pet    |\n|----------|-----------|-------|-----------|---------|-------------|----------------|--------|\n| MAE      | CE        | 0     | 87.67     | 78.46   | 91.82       | 91.67          | 94.05  |\n| MAE      | SCHaNe    | 0.9   | 90.29     | 81.30   | 93.11       | 92.82          | 94.88  |\n| ResNet50 | CE        | 0     | 96.27     | 84.62   | 81.38       | 95.71          | 93.24  |\n| ResNet50 | SCHaNe    | 0.9   | 96.92     | 84.98   | 83.05       | 96.33          | 93.51  |\n| BEiT-3    | CE        | 0     | 92.96     | 98      | 98.53       | 94.94          | 94.49  |\n| BEiT-3    | SCHaNe| 0.9   | **93.56**    | **98.93**  | **99.41**      | **95.43**         | **95.62** |\n\n**a**. Model Suitability and Performance:\nThe implementation of a multimodal model like BEiT-3 has enabled us to delve into the possibilities of contrastive learning in diverse domains, paving the way for expanding the impact of our research. In this paper, we also offer valuable insights to the academic community regarding the performance of Multimodal Large Language Models in image-only tasks. This is achieved through comprehensive experimentation on few-shot learning and full-dataset evaluations, addressing a gap in the existing literature, and these evaluations are notably absent in the BEiT-3 paper. Lastly, BEiT-3 base model, at the time of its release, achieved a Top-1 accuracy of 85.4\\% on the ImageNet-1k dataset [1], setting a new benchmark in image classification.\n\n**b**. Experimental Workload Considerations: \nThe scope of our experiments significantly influenced our model selection. Our study involved evaluating four few-shot datasets in both 1-shot and 5-shot settings with two different loss functions, amounting to 16 unique cases. Additionally, we conducted evaluations across eight image benchmark datasets with four different models or methods, leading to 24 cases. Each case required at least three runs. Considering these requirements, our total experimental workload comprised over 120 runs, including several large and computationally intensive datasets such as ImageNet-1k, iNaturalist 2017, and Places365. This extensive experimental requirement necessitated a choice that was feasible within our academic budget and timeline constraints.\n\n**c**. Temporal and Computational Constraints:\nThe release of DINOv2 [4] in April 2023, subsequent to the initiation of our project in January 2023, presented a significant timing challenge. Starting experiments with DINOv2 in late April would have been impractical, given our academic budget and the sheer volume of experiments required. Thus, BEiT-3 was a more viable option due to its earlier release date, proven performance and fewer parameters with respect to DINOv2 (1100M for DINOv2 vs 86M, BEiT3).\n\n[1] Wang, Wen et al. \u201cImage as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks.\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023): 19175-19186.\n\n[2] He, Kaiming et al. \u201cMasked Autoencoders Are Scalable Vision Learners.\u201d 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 15979-15988.\n\n[3] He, Kaiming et al. \u201cDeep Residual Learning for Image Recognition.\u201d 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015): 770-778.\n\n[4] Oquab, Maxime et al. \u201cDINOv2: Learning Robust Visual Features without Supervision.\u201d ArXiv abs/2304.07193 (2023): n. pag."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070467315,
                "cdate": 1700070467315,
                "tmdate": 1700071711085,
                "mdate": 1700071711085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F7ABwXxIhf",
            "forum": "nUH5liW3c1",
            "replyto": "nUH5liW3c1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a novel supervised contrastive learning objective function called SCHaNe. SCHaNe enhances model performance without requiring specialized architectures or additional resources. The proposed approach combines supervised contrastive learning with hard negative sampling to optimize the selection of positive and negative samples, thereby achieving state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper proposes a novel supervised contrastive learning objective function, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. \n2.\tThe proposed method achieves state-of-the-art performance on ImageNet-1k and outperforms the strong baseline BEiT-3 in Top-1 accuracy across twelve benchmarks, with significant gains in few-shot learning settings and full-dataset fine-tuning."
                },
                "weaknesses": {
                    "value": "Strengths* \n1.\tThis paper proposes a novel supervised contrastive learning objective function, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. \n2.\tThe proposed method achieves state-of-the-art performance on ImageNet-1k and outperforms the strong baseline BEiT-3 in Top-1 accuracy across twelve benchmarks, with significant gains in few-shot learning settings and full-dataset fine-tuning. \nWeaknesses* \n1.\tThe paper could benefit from a more detailed comparison with existing methods. While the authors compare the proposed method with the strong baseline BEiT-3, they do not compare it with other similar state-of-the-art methods [1][2] in the field.\n2.\tThe starting point of the work [2] is very similar to this article. I hope the author can further clarify the relationship with it so that readers can further understand the core starting point of the article.\n[1]: Robinson J, Chuang C Y, Sra S, et al. Contrastive learning with hard negative samples[J]. arXiv preprint arXiv:2010.04592, 2020.\n[2]: Jiang R, Nguyen T, Ishwar P, et al. Supervised contrastive learning with hard negative samples[J]. arXiv preprint arXiv:2209.00078, 2022."
                },
                "questions": {
                    "value": "1.\tCan the authors provide a more detailed comparison and analysis with existing methods? See the Weaknesses section for details.\n2.\tIn Table 3, the performance of the CE + SimCLR method is much lower than that of the CE method alone. At the same time, according to the description in the table, Label is not used in this part. How is this part of CE implemented, and why does the performance drop after adding SimCLR?\n3.\tIf possible, can the method proposed in this article be easily integrated into other models (such as other pre-trained models or other few-shot learning methods) like BEiT-3? I hope the article can give relevant explanations and results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1484/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1484/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680545710,
            "cdate": 1698680545710,
            "tmdate": 1699636077472,
            "mdate": 1699636077472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yzPR3OLGFy",
                "forum": "nUH5liW3c1",
                "replyto": "F7ABwXxIhf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7hRi -- questions (1-2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. Please find detailed responses below. Moreover, should you require more evidence or data to consider a higher rating for our paper, we would be grateful if you could specify these needs. We are eager to comply to the best of our abilities.\n\n> Question 1. Can the authors provide a more detailed comparison and analysis with existing methods? See the Weaknesses section for details.\n\nWe appreciate your insight into providing a more comprehensive comparison, and we are happy to do so. Please see the attached result. It is worth noting that [1] is not a fair comparison with their proposed approach, mainly because their method is fully unsupervised. The original reason for us not comparing with [2] is that our aim is to provide an alternative of cross-entropy during the fine-tuning phase instead of solely boosting the performance of supervised contrastive learning. More analysis of [2] is provided in the answer of weakness 2. Nevertheless, we evaluate the methods from [1] and [2] in five image benchmarks and compare them with our proposed objective, as shown in the table below.\n\n| Model | FT method         | CIFAR 100 | CUB-200 | Caltech256 | Oxford-Flowers | Pet   |\n|-------|-------------------|-----------|---------|-------------|----------------|-------|\n| BEIT-3 | CE                | 92.96     | 98      | 98.53       | 94.94          | 94.49 |\n| BEIT-3 | [1]               | 84.19     | 87.69   | 89.12       | 85.94          | 85.54 |\n| BEIT-3 | [2]               | 91.08     | 95.41   | 97.15       | 92.03          | 91.62 |\n| BEIT-3 | SCHaNe  | **93.56**  | **98.93**| **99.41**  | **95.43**    | **95.62** |\n\nThe SCHaNe fine-tuning method consistently outperforms other methods across all datasets. Method [1] consistently shows the lowest performance across all datasets.\nMethod [2] perform slightly worse than BEIT3 with Cross-Entropy (CE) across five tested dataset.\n\n> Question 2. In Table 3, the performance of the CE + SimCLR method is much lower than that of the CE method alone. At the same time, according to the description in the table, Label is not used in this part. How is this part of CE implemented, and why does the performance drop after adding SimCLR?\n\nSorry for the ambiguity in implementation. For the result of CE+SimCLR, we provide label information for CE loss but not for SimCLR loss. We believe the main reason for this performance drop after adding SimCLR is because there is no label information (SimCLR is an unsupervised approach). Without guidance from a label, contrastive learning may produce wrong learning signals, which could be contradictory to the cross-entropy signal and eventually harm the overall performance.\n\n[1]: Robinson J, Chuang C Y, Sra S, et al. Contrastive learning with hard negative samples[J]. arXiv preprint arXiv:2010.04592, 2020.\n\n[2]: Jiang R, Nguyen T, Ishwar P, et al. Supervised contrastive learning with hard negative samples[J]. arXiv preprint arXiv:2209.00078, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072018294,
                "cdate": 1700072018294,
                "tmdate": 1700072018294,
                "mdate": 1700072018294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fH1JfjLp4r",
                "forum": "nUH5liW3c1",
                "replyto": "F7ABwXxIhf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1484/Reviewer_7hRi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the author for additional experiments and replies. Considering that the backbone network of the comparison method in Table 1 of the original paper is different, the author still needs a more fair comparison in the revised version. As the author mentioned in other replies, if the motivation of this article is \"...to address the issue of how cross-entropy can result in sub-optimal generalization rather than building a dedicated method for few-shot learning....\" , the author should focus on more analysis and experiments instead of comparing with methods such as few-shot learning. The structure and presentation of the article need to be modified accordingly. Given these considerations, I maintain the rating in its current state."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629374262,
                "cdate": 1700629374262,
                "tmdate": 1700629374262,
                "mdate": 1700629374262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "17IiR4XPdz",
            "forum": "nUH5liW3c1",
            "replyto": "nUH5liW3c1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1484/Reviewer_SnpE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1484/Reviewer_SnpE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new loss, SCHaNe, for supervised contrastive learning. The main idea of this novel loss is that introducing importance weights for negative samples based on their dissimilarity plays a significant role in improving performance. Experiments show that SCHaNe is an effective method for enhancing performance on various datasets, particularly in few-shot tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The assumption that this paper aims to validate is both simple and easy to understand. Furthermore, the proposed objective is straightforward and intuitive. \n2. There is a clear improvement in performance when compared with the conventional cross-entropy loss. \n3. The paper is generally well-organized and presents its content logically."
                },
                "weaknesses": {
                    "value": "1. One of the main weaknesses I've identified is that the primary baseline used in this paper is Cross-Entropy (CE) loss, not Supervised Contrastive Learning (SupCon). If the paper's central claim is that 'introducing importance weights for negative samples based on their dissimilarity plays an important role,' then I believe SupCon should be the main baseline for comparison. Although SCHaNe outperforms SupCon in the few-shot setting as shown in Table 3, the inclusion of SupCon results in other settings\u2014such as in Table 1, Table 2, and Figure 4\u2014could strengthen the paper.\n2. BEiT-3 is primarily utilized as the main architecture. The Future Work section suggests that extending this method to various architectures may be promising, but I believe that evaluating the proposed method across different architectures should be included in this paper.\n3. The proposed method appears to be limited in its applicability to various tasks, such as dense prediction tasks. While this may not be a significant drawback, explicitly stating this limitation could enhance the paper. Moreover, the paper claims that \u201cOur SCHaNe objective function can be applied using a wide range of encoders, such as BERT for natural language processing tasks,\u201d yet there are no experiments provided to substantiate this claim.\n\nMinor: The presentation of the paper could be improved. For instance, the notation in Equations 2-4 is confusing. If I understand correctly, $\\beta$ should vary with the index, but it might be misunderstood as a constant since it lacks an index. In Equation 2, $z$ denotes the label, which is not the case in Equations 3 and 4. Regarding Figure 4, while the trend is important, we cannot directly compare the accuracies across various downstream tasks in an 'apple-to-apple' manner.\n\n\n---\n\n**Post rebuttal**\n\nI appreciate the authors' response and the additional experimental results provided. Despite the manuscript's weaknesses, I believe its strengths outweigh them. Consequently, I maintain my rating of 'Weak Accept'."
                },
                "questions": {
                    "value": "Please see the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1484/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1484/Reviewer_SnpE",
                        "ICLR.cc/2024/Conference/Submission1484/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1484/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718933632,
            "cdate": 1698718933632,
            "tmdate": 1700812917536,
            "mdate": 1700812917536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FhCsXEgtAE",
                "forum": "nUH5liW3c1",
                "replyto": "17IiR4XPdz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1484/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer SnpE -- Weakness 1"
                    },
                    "comment": {
                        "value": "> Weakness 1: One of the main weaknesses I've identified is that the primary baseline used in this paper is Cross-Entropy (CE) loss, not Supervised Contrastive Learning (SupCon). If the paper's central claim is that 'introducing importance weights for negative samples based on their dissimilarity plays an important role,' then I believe SupCon should be the main baseline for comparison. Although SCHaNe outperforms SupCon in the few-shot setting, as shown in Table 3, the inclusion of SupCon results in other settings\u2014such as in Table 1, Table 2, and Figure 4\u2014could strengthen the paper.\n\nThank you for your insightful feedback regarding the baseline selection in our paper. You rightly point out that Supervised Contrastive Learning (SupCon) should be a central baseline for comparison, given our paper's focus on the importance of weighting negative samples based on dissimilarity. However, our aim is to provide an alternative to cross-entropy during the fine-tuning phase instead of solely boosting the performance of supervised contrastive learning. I attached the current result we have for you to review:\n\n| Model | FT method       | Alpha | CIFAR 100 | CUB-200 | Caltech256 | Oxford-Flowers | Pet   | iNat2017 | Places365 | ImageNet-1k |\n|-------|-----------------|-------|-----------|---------|-------------|----------------|-------|----------|-----------|-------------|\n| ViT-B | CE              | 0     | 87.13     | 76.93   | 90.92       | 90.86          | 93.81 | 65.26    | 54.06     | 77.91       |\n| MAE   | CE              | 0     | 87.67     | 78.46   | 91.82       | 91.67          | 94.05 | 70.5     | 57.9      | 83.60       |\n| BEIT-3 | CE              | 0     | 92.96     | 98      | 98.53       | 94.94          | 94.49 | 72.31    | 59.81     | 85.4        |\n| BEIT-3 | SCL             | 0     | 93.04     | 98.07   | 98.61       | 94.98          | 94.41 | 72.59    | 59.94     | 85.63       |\n| BEIT-3 | CE + SCL        | 0.9   | 93.15     | 98.29   | 98.93       | 95.13          | 94.78 | 73.62    | 60.92     | 85.8        |\n| BEIT-3 | CE + CL(SCHaNe) | 0.9   | **93.56**   | **98.93**   | **99.41**   | **95.43**   | **95.62** | **75.72**    | **62.22**     | **86.14**       |\n\nThe data presented in the table illustrates that the application of supervised contrastive learning generally results in a modest enhancement in performance, with the notable exception of the Oxford-Pet dataset. Our proposed method, SCHaNe, on the other hand, shows a significant and more consistent improvement across all evaluated benchmarks. We will update these results in the new version of our paper and provide more comparisons in the cameral-ready version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1484/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072224735,
                "cdate": 1700072224735,
                "tmdate": 1700072224735,
                "mdate": 1700072224735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]