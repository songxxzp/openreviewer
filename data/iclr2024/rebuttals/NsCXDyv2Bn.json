[
    {
        "title": "VoiceGen: Describing and Generating Voices with Text Prompt"
    },
    {
        "review": {
            "id": "b4nCkuImiu",
            "forum": "NsCXDyv2Bn",
            "replyto": "NsCXDyv2Bn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a small architecture improvement upon PromptTTS and also proposes to use SLU and LLM to generate text prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. There is some novelty in using SLU and LLM to generate text prompts for voice, as I believe this is the first work investigating this.\n2. A good mount of ablation and analysis.\n3. The paper is in general written clearly and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The authors claim that the proposed variational method solves the one-to-many problem, but I doubt if this is the case. The proposed module basically learns to predict a melspec encoder's output from text prompts, in addition to conditoning the TTS backbone. I don't see how this solves the one-to-many problem as in the end we are still predicting from text (one) to different variability (many) in the voice. The only difference is that there is an additional auxiliary loss from the melspec encoder. This seems to be in principle similar to InstructTTS's approach to regularize melspec encoder and text encoder output to close in the embedding space.\n2. The proposed prompt generation method, while the first of its kind, seems to be still quite limited by the very limited SLU output set, and the template used for LLM generation. \n3. The proposed method performs only marginally better than PromptTTS and InstructTTS."
                },
                "questions": {
                    "value": "Will the author open source the generated dataset or source code? I would consider the open sourcing effort as a contribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission907/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission907/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498961866,
            "cdate": 1698498961866,
            "tmdate": 1700689926716,
            "mdate": 1700689926716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QxKHRAzbnk",
                "forum": "NsCXDyv2Bn",
                "replyto": "b4nCkuImiu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dNEb"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your efforts in reviewing our paper and providing us with valuable and constructive feedback. Your comments have greatly benefited our work. We have addressed your questions below.\n\n**Q1: About one-to-many problem**\n\nAs mentioned in your review, previous work such as InstructTTS has attempted to address the one-to-many problem by regularizing the melspec encoder and text encoder outputs to converge in the embedding space using L2 loss. However, this approach is limited in its ability to resolve the one-to-many problem, as the output of the melspec encoder contains much more information than that of the text encoder.\n\nIn contrast, we approach the one-to-many problem as a generation task and utilize generative models (specifically, diffusion models) to generate the output of the melspec encoder based on the output of the text encoder. We believe that VoiceGen differs from previous frameworks, and is the first framework capable of generating and fixing virtual speakers. This is a significant contribution to both future research and the real-world application of prompt-based TTS systems.\n\n**Q2: About the limitation of SLU output set and template**\n\nThe template is written by LLM and can be various by changing the prompt. However, the current limitations of the SLU output set can restrict the capabilities of VoiceGen. We anticipate that more robust SLU models will become available in the future, and we plan to continue our efforts to improve the performance of VoiceGen in this regard.\n\n\n**Q3: About the performance**\n\nFirstly, our results demonstrate that VoiceGen outperforms previous methods such as PromptTTS and VoiceGen by a significant margin. Further analyses (see Q1 in [Response to Reviewer g23M](https://openreview.net/forum?id=NsCXDyv2Bn&noteId=j7nT1dF7U1) and Q7 in [Response to Reviewer Ghqv (2/2)](https://openreview.net/forum?id=NsCXDyv2Bn&noteId=w6K4vz3QGP)) have confirmed that this improvement is consistent across different datasets and TTS backbones.\n\nSecondly, the main contribution of our paper lies in the development of a general framework on modeling voice variability (first work to support virtual speaker creation) and text prompt writting (to save labeling cost while achieving good quality). We sincerely hope that the significance of our contribution is recognized.\n\n**Q4: About the code and dataset**\n\nWe have updated the supplementary material to include the code for VoiceGen and the prompt generation pipeline, along with the generated dataset. It is worth noting that this pipeline can be utilized to generate prompts for other attributes by providing the necessary attributes, classes, and ChatGPT keys (further details can be found in the README file located in the data_pipeline folder).\n\n**Finally, we would like to express our gratitude again to the reviewer for their time and effort in reviewing our paper. Please do not hesitate to let us know if you have any further concerns or comments. We would be happy to address them.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404774778,
                "cdate": 1700404774778,
                "tmdate": 1700404774778,
                "mdate": 1700404774778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CpPdg9fCGm",
                "forum": "NsCXDyv2Bn",
                "replyto": "Mq1GbOcFx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I am still not convinced by the reply to Q1. I don't think it solves the one-to-many problem. I can only see that adding an intermediate representation which the text encoder learns to predict helps with the performance. However since the authors addressed other issues and are open-sourcing their efforts. I would like to raise my score slightly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689911270,
                "cdate": 1700689911270,
                "tmdate": 1700689911270,
                "mdate": 1700689911270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bu1yZKDcrz",
            "forum": "NsCXDyv2Bn",
            "replyto": "NsCXDyv2Bn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission907/Reviewer_2wDM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission907/Reviewer_2wDM"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed VoiceGen, a text-to-speech framework that uses a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts.\n\nThe variation network predicts the representation extracted from the reference speech based on the text prompt. And the LLM formulates text prompts based on the recognition results from speech language understanding model. Compared to the previous works, VoiceGen generates voices more consistent with text prompts, offering users more choices on voice generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed modeling and data labeling pipelines for text-prompt based TTS systems can generate higher-quality speech with more consistent and noticeable control compared to previous systems. The variation network predicts speech representations that are more closely corresponding to the text prompt and more diversity by sampling from Gaussian noise. On the other hand, the LLM-based prompt generation pipeline can produce high-quality text prompts at scale and can easily incorporate new attributes. Overall, the proposed system provides a framework that is beneficial for future text-prompt based TTS research."
                },
                "weaknesses": {
                    "value": "After listening to the generated voices on the demo page, audio quality is still an issue and further improvements are required, especially for certain text prompts such as \"Please speak at a fast speed, gentleman\". The reason could be missing or few audio samples for corresponding prompts in training datasets."
                },
                "questions": {
                    "value": "1. What's the necessity of concatenating both text prompt representation and speech prompt representation for TTS backbone? Will the speech prompt representation itself be enough to guide the TTS backbone through cross attention?\n\n2. What's the impact of making use of a fixed-length representation for text and speech prompt representations? Fixed-length representations may be fine for global attributes such as age and gender, but is that enough for fine-grained or local control in guided speech generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705441951,
            "cdate": 1698705441951,
            "tmdate": 1699636017528,
            "mdate": 1699636017528,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zSp7T4FPeK",
                "forum": "NsCXDyv2Bn",
                "replyto": "Bu1yZKDcrz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2wDM"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your efforts in reviewing our paper and providing us with valuable and constructive feedback. Your comments have greatly benefited our work. We have addressed your questions below.\n\n**Q1: About imperfect sample**\n\nYes, we keep this imperfect sample to showcase that the sampling result can be occasionally imperfect (although this can be alleviated by sampling for another time). We will keep working on further improving the stability and quality of model in future work.\n\n**Q2: About the concatenating of both text prompt representation and speech prompt representation**\n\nAs you noted in your review, using only speech prompt representation is sufficient in terms of information completeness. However, concatenating both text prompt representation and speech prompt representation can enhance the model's awareness of the information contained within the text prompt, thereby leading to a slight improvement in model accuracy.\n\n**Q3: About the fixed-length representation for text and speech prompt representations**\n\nWe conduct experiments on the this hyper-parameter in VoiceGen, the length of prompt and reference representations. The results are shown as follows:\n\n|  Representation Length   | Gender | Speed | Volume | Pitch | Mean |\n|  ----  | ----  |  ----  | ----  |  ----  | ----  |\n| 4  | 98.54 | 92.49 | 93.26 | 88.66 | 93.24 |\n| 8 (Default Choice)  | 98.23 | 92.64 | 92.56 | 89.89 | 93.33 |\n| 16  | 98.77 | 92.18 | 93.95 | 90.27 | 93.79 |\n| 32  | 98.39 | 91.80 | 91.19 | 90.27 | 93.09 |\n\nThe results indicates that representation length is not a highly sensitive hyper-parameter in terms of performance, and increasing the length to 16 can lead to a slight improvement in model accuracy. \n\nWe believe that variable-length representation could enhance the model's fine-grained or local control, particularly as the text prompt becomes more complex. We plan to explore this area further in future research.\n\n**Finally, we would like to express our gratitude again to the reviewer for their time and effort in reviewing our paper. Please do not hesitate to let us know if you have any further concerns or comments. We would be happy to address them.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404542234,
                "cdate": 1700404542234,
                "tmdate": 1700404542234,
                "mdate": 1700404542234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bKVHKsjuN0",
            "forum": "NsCXDyv2Bn",
            "replyto": "NsCXDyv2Bn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission907/Reviewer_Ghqv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission907/Reviewer_Ghqv"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about text-to-speech (TTS). The TTS model is based on Naturalspeech 2. The TTS model is extended by a style module, which uses a text prompt to describe the style. During training, it additionally also uses the reference speech as input. However, for speech generation, a variation network instead generates the reference speech encoder outputs. This variation network is the core novelty proposed by the authors. It is supposed to add further variation in the speech style which cannot be covered by the text prompt alone. The variation network is a diffusion model using a Transformer encoder to iterate on the speech encoder outputs.\n\nThis proposed model is called VoiceGen.\n\nThe training data is based on the Multilingual LibriSpeech (MLS) dataset with 44K hours of transcribed speech. To generate the text prompt, needed to train the style model, a text prompt dataset generation pipeline is proposed, to extend the given transcribed speech: Based on a speech language understanding (SLU) model, the gender of the speech is identified. Additionally, using digital signal processing tools, pitch, volume, and speed is extracted and put into classes. Those attribute classes are then fed into a large language model (LLM) to generate a text prompt which conveys the style attributes.\n\nThe variability is measured using a WavLM-TDNN model to assess the similarity of two speeches, and it is shown that the introduction of the variation network leads to higher speech variability. At the same time, mean-opinion-score (MOS) on the quality of the proposed VoiceGen model is slightly better than other text-prompt-based TTS models, PromptTTS and InstructTTS specifically. It is also shown that the text prompt indeed works and can generate speech with the requested attributes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The claims are tested and it seems the proposed model adds quite a bit of variability, as it was intended."
                },
                "weaknesses": {
                    "value": "So many details are left out, as this would not really be possible to fit into the paper. So without the exact code + recipe to produce all the results, it will be almost impossible to reproduce the results. I think having code + recipe available here is very important.\n\nAll the experiments basically just show that the proposed model works well and solves the outlined problem. However, there is almost no analysis or ablation studies, etc. E.g. how important is it to use a diffusion model here? What about other model types? What about other smaller model details, and hyper parameters, etc?\n\nThere are a few things a bit unclear (see below)."
                },
                "questions": {
                    "value": "> Compared to traditional text-to- speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all.\n\nI don't exactly understand the difference between speech prompts and text prompts.\n\nA text prompt is really like a description of what should be generated, like \u201cPlease generate a voice of a boy shouting out\u201d.\n\nA speech prompt is the same but as speech? Or is this the text of the generated speech? Or sth else?\n\n\n> The input of variation network comprises the prompt representation (P1, ..., PM ), noised reference representation (R1t , ..., PMt ), and diffusion step t\n\nHow exactly? P and R are concatenated sequences? But the diffusion process only runs on R?\n\n\n\n\n\n\nClarification on text attributes for text prompt generalization: There is only gender (via the existing SLU model), pitch, volume, and speed, nothing else? I would expect some more attributes, e.g. different emotion categories, etc.\n\n\nSection 5.1, table 1: I don't exactly understand what is measured under what conditions on what data. So, each TTS model (VoiceGen vs the others) generates some speech, then some attributes are given to produce some prompt, and then, for the given fixed SLU models and digital signal processing tools, the accuracy is measured? That uses the generated prompts via LLM as mentioned before? All of them, so 20K text prompts? How many classes are there for each of the attributes?\n\n\nText prompt dataset: So this is released? Where? I did not find it.\n\n> WavLM-TDNN model (Chen et al., 2022a) to assess the similarity of two speech\n\nIs this a pretrained model which you just take? Or where is the model from?\n\nI don't exactly understand Section 5.1, table 3. This is always with the same phoneme sequence as input to the TTS model? But what is the text context here? Why are the results so different between PromptTTS, InstructTTS and VoiceGen? Are they really comparable? Do they have the same TTS backbone? Does the TTS backbone use diffusion in all cases?\n\nWhen you add variability through the variation network, how well are the properties of the text prompt for the speech style actually preserved? I guess this is table 1? But how much variance do you get when you add sampling results of the variation network?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707330195,
            "cdate": 1698707330195,
            "tmdate": 1699636017435,
            "mdate": 1699636017435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sb9RKh8wLk",
                "forum": "NsCXDyv2Bn",
                "replyto": "bKVHKsjuN0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ghqv (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your efforts in reviewing our paper and providing us with valuable and constructive feedback. Your comments have greatly benefited our work. We have addressed your questions below.\n\n**Q1: About the speech prompts and text prompts**\n\nSimilar to your comments, a text prompt is a description of the desired output voice, such as \u201cPlease generate a voice of a boy shouting out\u201d.\n\nFor the speech prompt, it is **not** the speech version of text prompt. The speech prompt can be a speech of target style (maybe a speech of a famous person). The speech prompt based TTS model can learn from the speech prompt to generate speech in the same style (or voice) as the speech prompt. However, identifying an appropriate speech prompt can be a time-consuming task and may raise ethical concerns. Therefore, text description can be a flexiable and safe way for voice generation.\n\n**Q2: About the input of variation network**\n\nTo provide a more precise analysis, we will discuss the problem with numerical details. The variation network we employed is a Transformer-based model, with the prompt representation, noised reference representation, and reference representation all set to a length of 8. The prompt representation and noised reference representation both have a shape of [batchsize, 8, hiddensize], which are concatenated to form a shape of [batchsize, 8+8, hiddensize]. This concatenated representation is then fed into the Transformer model, with the output also having a shape of [batchsize, 8+8, hiddensize]. The latter 8 outputs, corresponding to the noised reference representation, with a shape of [batchsize, 8, hiddensize], are used as the predicted results, along with the reference representation, for loss calculation. \n\nIt should be noted that although we only consider the latter 8 outputs, the prompt representation can also influence the output through self-attention. Our approach of using a single model for both prompt and noised reference representations was inspired by the implementation in DALLE-2 (https://github.com/lucidrains/DALLE2-pytorch).\n\n**Q3: About the clarification on text attribute and setting**\n\nIn our submission, all results presented in Table 1 are based on a fair comparison of text data, with all models utilizing LLM-written data. Additionally, the SLU and DSP models are fixed for metric calculation. We also conducted experiments on all models using text data from PromptTTS, which has lower quality. The results of these experiments can be found in Q1 in [Response to Reviewer g23M](https://openreview.net/forum?id=NsCXDyv2Bn&noteId=j7nT1dF7U1) and the conclusion is similar with that in Table 1 of the submission. \n\nRegarding the classes, we used two classes for gender (male and female) and three classes (high, normal, low) for other attributes such as pitch, speed, and volume. We plan to continue our work by adding more attributes, such as emotion, in our future research.\n\n**Q4: About the code and dataset**\n\nWe have updated the supplementary material to include the code for VoiceGen and the prompt generation pipeline, along with the generated dataset. It is worth noting that this pipeline can be utilized to generate prompts for other attributes by providing the necessary attributes, classes, and ChatGPT keys (further details can be found in the README file located in the data_pipeline folder).\n\n**Q5: About the WavLM-TDNN model**\n\nWe use the official model in https://github.com/microsoft/UniSpeech/blob/main/downstreams/speaker_verification/README.md."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403922425,
                "cdate": 1700403922425,
                "tmdate": 1700403922425,
                "mdate": 1700403922425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CYUl56DwHw",
            "forum": "NsCXDyv2Bn",
            "replyto": "NsCXDyv2Bn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of text-based voice creation for text-to-speech synthesis (TTS). \nPrior work on zero-shot TTS often relies on using reference voice samples of the target speaker (YourTTS) or target audio style (including both speaker and prosody, such as VALL-E) to prompt the model to generate the desired voice. \nHowever, the authors argue that such prompts may not always be available, and this paradigm is less user friendly. To address it, authors present a model to enable creation of voices through providing descriptions like \u201ca man with a normal voice\u201d, similar to the setup in InstructTTS and PromptTTS.\nThe contribution of the proposed method is two-fold. \nFirst, the authors tackle the one-to-many problem between text description and voice, where the same description, such as \u201ca low pitched female voice\u201d, can be mapped to many different voices. The authors adopt a variation network to sample the reference speech style embeddings given a text description prompt. \nSecond, the authors presented a pipeline to automatically create text prompts to address the data scarcity issue for descriptive texts for speech. The authors consider controlling four aspects of speech: gender, speed, volume, and pitch.\nIn addition, the authors present a face2voice application replacing text description with facial image."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies an interesting problem which enables creation of voices through text descriptions. This line of research has great potential of making speech generation more customizable.\n\n2. The authors present a systematic pipeline to produce text describing four aspects of speech, addressing the data scarcity problem. Ablation studying Table 7 shows the benefit of the step-by-step generation process.\n\n3. The variation model tackles the one-to-many problem. The author verified that when changing variation networks introduce speaker variation in Tabel 3."
                },
                "weaknesses": {
                    "value": "1. I am not certain if the proposed model and the baseline models are trained on the same data, and hence I cannot draw conclusions that whether the proposed model outperforms the baselines because of the additional LLM generated data or because of the introduction of the variational network to address the one-to-many problem. It would be good to show how well the baseline performs with and without LLM-augmented text prmopts\n\n2. Given that the number of attribute combinations is rather small (2 x 3 x 3 x 3 = 54), I am suspicious about how useful it is to increase the number of text prompts. The author could have conducted ablation studies comparing using only the PromptTTS prompts vs those + x LLM-augmented prompts\n\n3. The authors did not give sufficient background on InstructTTS. That model also deploys a diffusion model and in principle would be capable of modeling one-to-many mapping. Why does the VoiceGen model perform better than InstructTTS?\n\n4. Given that the conditional attributes are all categorical, it can be conditioned with a lookup table for each attribute straightforwardly. How does that method compare with the proposed method? The paper does not really showcase the strength of free-form text description for conditioning attributes that are hard to categorize.\n\n5. For step 1 when generating phrases, the authors show in Table 5 that male can be mapped to man/boy/guy/dude/gentlement. However, one would expect quite different voices between boy and man. This shows the weakness of such pipeline where text descriptions are created from underspecified labels."
                },
                "questions": {
                    "value": "See the questions in the Weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission907/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission907/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782711683,
            "cdate": 1698782711683,
            "tmdate": 1700725847011,
            "mdate": 1700725847011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j7nT1dF7U1",
                "forum": "NsCXDyv2Bn",
                "replyto": "CYUl56DwHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g23M"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your efforts in reviewing our paper and providing us with valuable and constructive feedback. Your comments have greatly benefited our work. We have addressed your questions below.\n\n**Q1: About the text data (Weakness 1 and 2)**\n\nSince the ablation study on text data shows the superiority of LLM-written data over the prompts in PromptTTS (as reported in Table 4 of the submission), the results in Table 1 of the submission are conducted when all the models using the LLM-written data. Thus, the results are a fair comparison in terms of text data. \n\nTo strengthen the conclusions of our paper, we conducted additional experiments on all models (i.e., PromptTTS, InstructTTS, and VoiceGen) using the text data in PromptTTS. The results are in the following table:\n\n|  Model   | Text Dataset  | Gender | Speed | Volume | Pitch | Mean |\n|  ----  | ----  |  ----  | ----  |  ----  | ----  |  ----  |\n| PromptTTS  | Prompts in PromptTTS | 98.93 | 87.43 | 89.35 | 85.44 | 90.29 |\n| PromptTTS  | LLM-written | 98.01 | 89.66 | 92.49 | 85.95 | 91.54 |\n|  ----  | ----  |  ----  | ----  |  ----  | ----  |  ----  |\n| InstructTTS  | Prompts in PromptTTS | 96.55 | 86.13 | 88.43 | 85.52 | 89.16 |\n| InstructTTS  | LLM-written | 97.24 | 90.57 | 91.26 | 86.82 | 91.47 |\n|  ----  | ----  |  ----  | ----  |  ----  | ----  |  ----  |\n| VoiceGen  | Prompts in PromptTTS | 98.77 | 90.80 | 90.57 | 89.58 | 92.43 |\n| VoiceGen  | LLM-written | 98.23 | 92.64 | 92.56 | 89.89 | 93.33 |\n\nFrom the Table, we observe that 1) VoiceGen outperforms baseline methods on both text prompt datasets; 2) using LLM-written data consistently improves the quality of prompt-based TTS on all models, compared to using the prompts in PromptTTS.\n\n**Q2: About the diffusion in InstructTTS**\n\nWe apologize for not discussing InstructTTS in our paper. InstructTTS addresses the one-to-many challenge by leveraging speaker ID (once the speaker id is provided, the variability on voice becomes smaller). However, this approach has two limitations. First, it restricts the application of InstructTTS to the speakers in the training dataset. Second, it prevents the voice generation model from modeling timbre, which is an important attribute of voice generation. In contrast, VoiceGen can create different voices, including variability in timbre, for the same text prompt.\n\nIn addition, InstructTTS has also attempted to address the one-to-many problem by regularizing the reference encoder and text encoder outputs to converge in the embedding space using L2 loss. However, this approach is limited in its ability to resolve the one-to-many problem, as the output of the reference encoder contains much more information than that of the text encoder.\n\nIn contrast, we approach the one-to-many problem as a generation task and utilize generative models (specifically, diffusion models) to generate the output of the reference encoder based on the output of the text encoder. VoiceGen is the first framework capable of generating and fixing virtual speakers. This is a significant contribution to both future research and the real-world application of prompt-based TTS systems.\n\n**Q3: About the categorical attributes (Weakness 4 and Weakness 5)**\n\nFirstly, regarding the effectiveness of text descriptions, we found that it outperforms the categorical lookup table method when controlling the voice with text description, as reported in PromptTTS[1], and we also confirmed this in our preliminary study.\n\nSecondly, for conditioning attributes that are difficult to categorize, we believe that the extension of Face2Voice to our model partially demonstrates its ability to generate voice based on unspecific information.\n\nThirdly, we acknowledge that using more specific labels and descriptions for hard-to-categorize attributes would be beneficial. We plan to address this in our future work. We sincerely hope that our paper's contribution in presenting a general framework for modeling voice variability and text prompt writing can be recognized.\n\n\n\n**Finally, we would like to express our gratitude again to the reviewer for their time and effort in reviewing our paper. Please do not hesitate to let us know if you have any further concerns or comments. We would be happy to address them.**\n\n**Reference**\n\n[1] PromptTTS: Controllable text-to-speech with text descriptions, Zhifang Guo, et al, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403444219,
                "cdate": 1700403444219,
                "tmdate": 1700403444219,
                "mdate": 1700403444219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DgQh8kvuqy",
                "forum": "NsCXDyv2Bn",
                "replyto": "0kXHalfPQU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "1. The author addressed my question. I would caution the authors against stating \u201cusing LLM-written data consistently improves the quality\u201d. Gender accuracy degrades for PromptTTS and VoiceGen when using LLM-written.\n2. I thank the author for the explanation. I see that the L2 loss in the embedding space do encourage reference encoder and text encoder to produce same embeddings. However, conditioned on the embedding (and other embeddings like content and speaker, as illustrated in Fig 5), InstructTTS still has the ability to generate diverse output, given the denoising Transformer models a stochastic process that could produce different output when initial noise (x^T) is different. Hence, in terms of modeling capability I do not see InstructTTS would have limitation of underfitting the one-to-many distribution. Nevertheless, I do agree with the authors that requiring speaker ID imposes another limitation on InstructTTS.\n3. Thank you for sharing the empirical finding that conditioning on text description outperforms conditioning on categorical lookup table. However, from information theoretical perspective, I do not see how this could happen, given all the captions are derived from a finite set of values for each category (e.g., male/female for gender) and hence text description does not provide more information than the categorical attributes (and is even a noisy version of that). Could the other provide hypothesis on why this would happen? I do see the value for Face2Voice which I believe highlight the strength of the approach (when attributes are hard to categorize)\n\nOverall I think the authors have addressed many of my questions, while I still have doubts on some of the results (which are minor and could be noise from empirical studies). I have raised my score to reflect this."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725826290,
                "cdate": 1700725826290,
                "tmdate": 1700725826290,
                "mdate": 1700725826290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QwKvNdMjU1",
                "forum": "NsCXDyv2Bn",
                "replyto": "CYUl56DwHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission907/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments"
                    },
                    "comment": {
                        "value": "Dear Reviewer g23M,\n\nWe are very delightful to see that most of the issues are addressed! Thanks for your further comments!\n\nFor the comment 1, thank you for your suggestion and we have changed the statement (in Appendix C of the revised paper) to avoid over-claiming.\n\nFor the comment 2, we agree that the L2 loss in InstructTTS is effective with more condition information such as speaker ID embedding or other embeddings. Considering the scenario of creating and fixing different virtual speakers or of limited conditional information, VoiceGen is a more general framework for voice generation compared with baseline methods since it can predict the missing information about voice variability.\n\nFor the comment 3, we understand your concern from information theoretical perspective. However, the input of the prompt-based TTS system is the text prompt instead of category label. So if we want to use the category-conditioned model, we need to recognize category from the text prompt, which is not perfect and can lead to cascaded error. If the category is (100%) accurately recognized, then the category based method can be quite competitive. Besides the above explanation, it is worth noted that prompt based TTS system has a potential on modeling hard-to-categorize attributes (which is shown in the Face2Voice and is our important future work).\n\nThanks again for your comments and the time for reviewing our paper!\n\nBest, Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission907/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729305130,
                "cdate": 1700729305130,
                "tmdate": 1700729343058,
                "mdate": 1700729343058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]