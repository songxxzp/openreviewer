[
    {
        "title": "GAIA: a benchmark for General AI Assistants"
    },
    {
        "review": {
            "id": "FN8gbACfcY",
            "forum": "fibxvahvs3",
            "replyto": "fibxvahvs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_GSBU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_GSBU"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel dataset, GAIA, intended for evaluating general AI assistants. The tasks within the dataset are designed to be easy for humans, but challenging for state-of-the-art models. These tasks necessitate a variety of skills including web browsing, tool use, and reasoning. Furthermore, the answers to these problems are simple and easy to evaluate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The design principles of GAIA are commendable and innovative, providing a comprehensive framework for creating similar datasets. This is the type of dataset that would ideally test all AI assistant models.\n2. The procedures for creating the dataset are reliable and well-structured, utilizing human input to generate questions and validate the quality of the dataset.\n3. The paper is well-written, providing clear and understandable content."
                },
                "weaknesses": {
                    "value": "1. The dataset's size is relatively small, which might limit its applicability.\n2. The paper does not provide sufficient assurance regarding the coverage or diversity of the problems within the dataset."
                },
                "questions": {
                    "value": "1. What are the main challenges that GAIA presents to current models/systems? Do these challenges stem from incorrect tool usage or mistakes made during step-by-step execution? Can you provide an error analysis of the current models?\n2. The paper mentions that \"level 3 are questions for a near-perfect general assistant.\" How do you define a perfect general assistant, and how do you ensure that good performance on your dataset (at level 3) implies a perfect general assistant? These questions could help determine if there are other abilities not covered by your dataset.\n3. What is the distribution of questions across the three levels?\n4. It would be helpful to have a detailed table showing the performance of each model, including the total score for all three levels."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762929877,
            "cdate": 1698762929877,
            "tmdate": 1699636554172,
            "mdate": 1699636554172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZUXDir8tnP",
                "forum": "fibxvahvs3",
                "replyto": "FN8gbACfcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n\n##\u00a0Detailed error analysis\n\nWe discuss this point, which is shared with other reviewers, in our general answer.\n\n\n##\u00a0On perfect general assistants\n\nWe think about Level 3 as questions requiring arbitrarily many tools and steps (albeit staying conceptually simple for humans). We dub an assistant solving this level as \u201cperfect\u201d since there is theoretically no upper bound for the number of tools and steps. In addition, while the number of steps and tools required grows linearly between levels, the associated difficulty grows exponentially since one more step means many new possible mistakes.\n\n\n##\u00a0Distribution of questions and detailed performance table\n\nThere are 146 Level 1 questions, 245 Level 2 questions, and 75 Level 3 questions. The aggregated scores of each model is therefore as follows:\n\n|               | Level 1 | Level 2 | Level 3 | Aggregated |\n|---------------|---------|---------|---------|------------|\n|Number of questions|146|245|75|466|\n|Model and scores |||||\n| Search Engine |    7.4     |     0    |     0    |     2.3       |\n| GPT4          |     6.4    |     2.5    |    0     |   3.3         |\n| AutoGPT4      |   14.4      |  6.5       |     0    |    7.9        |\n| GPT4 + plugins|  30.3       |    9.7     |     0    |     14.6       |\n| Human |  93.9       |    91.8     |     87.3    |   91.7        |\n\nWe will update Table 4 in the revised manuscript to include this information."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502722386,
                "cdate": 1700502722386,
                "tmdate": 1700502722386,
                "mdate": 1700502722386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P9YzZXxAFs",
                "forum": "fibxvahvs3",
                "replyto": "ZUXDir8tnP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_GSBU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_GSBU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I would maintain the original score of 8: accepted, as it is a quality paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534678051,
                "cdate": 1700534678051,
                "tmdate": 1700534678051,
                "mdate": 1700534678051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5cxaoUp0Vz",
            "forum": "fibxvahvs3",
            "replyto": "fibxvahvs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Gaia, a general purpose benchmark for image+text assistants. The questions generally require web searching and thus doing retrieval over a lot of documents with images. A few things:\n\n* There are 466 highly curated questions in the dataset \n* the questions are associated with 'levels' corresponding to the # of tools/steps required \n* at creation time, the answers can be found by looking at websites that don't have bots banned via robots.txt files\n* the creation process is crowdsourced and meant to focus on tool use\n\n\nThe paper evaluates GPT4, autogpt4, GPT4+plugins and includes human performance. Note that this is GPT4 without vision so it can't do any of the image questions, nor can it do retrieval on documents with images.\n\n----\n\nUpdate: I read the other reviews and the rebuttal, thanks (and thanks for promising to add GPT + vision results!). I still support this paper and recommend it be accepted; I can't find a compelling reason from the negative review (I think ICLR seems like a fine venue for this paper)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To this reviewer, creating harder benchmarks that aren't (as) gameable is an important research direction. The ideas proposed seem interesting and probably important for the community to discuss further. E.g. what _is_ actually important for a multimodal assistant to have? which tools should it be able to use? etc etc. The idea of making it hard by requiring open-ended internet use seems like a novel contribution to this reviewer. The open-endedness should hopefully make it harder versus older multihop datasets like MS-Marco, etc. that already provide a list of optional passages to reason over."
                },
                "weaknesses": {
                    "value": "I think the paper could use more details around how the benchmark works and was constructed, but that's stylistic preference. as a result I have some questions, will update my score if they can be resolved --"
                },
                "questions": {
                    "value": "My main concern is about evaluation (wasn't sure whether to put these under 'questions' or 'weaknesses' as there's significant ambiguity here on my end:\n\n* How are humans scored? The paper writes \"Human score corresponds to the portion of correct answers by validation annotators for valid questions.\" Are these the same validators involved for validating the data or a new fresh set of annotators? This is important to this reviewer as there are many datasets that claim suspiciously high human performance because they didn't run validation with a new set of annotators.\n* How do you grade open-ended answers? e.g. \"St. Petersburg\" vs \"Saint Petersburg\". I couldn't tell from the paper how the correctness is actually determined.\n\nI'd also be curious as to the portion of answers that GPT4 can't solve right now because it doesn't have access to the right tools (e.g. image understanding)?, versus using those tools well. Similarly I'd be curious if GPT4 with vision can answer those questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5446/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5446/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799312045,
            "cdate": 1698799312045,
            "tmdate": 1700715944680,
            "mdate": 1700715944680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tnk3gIN5Wa",
                "forum": "fibxvahvs3",
                "replyto": "5cxaoUp0Vz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n\n##\u00a0Details on human score\n\nThe set of annotators used to measure human performance is different from the set that created the questions and there is no second try, hence we believe this score is reliable. To clarify, our process is as follows: \n- First, an annotator A creates a question and reports its answer.\n- Then, the question is given to 2 new annotators B and C to answer. B does not see A and C\u2019s answer, and similarly for C. There are two cases:\n    - If the answers from annotators A, B, and C are similar the question is correct and the humans scored it well. We count B and C as correct answers for this question. \n    - If the answers are different, the annotators assess whether this was due to an ambiguity in the question or a human error, and there are again two cases:\n         - The question was ambiguous, e.g., \u201cHow many layers does BERT have?\u201d and one Reviewer answered 6 referring to BERT-small while the others said 12 referring to BERT-base. We do not use it to compute a human score.\n         - The question was clear and an annotator, B or C, made a clear mistake when answering it. We count it as a human error.\n\n\nWe hope this clarification solves the reviewer's legit concerns.\n\n\n##\u00a0Details on grading open ended answers\n\nThanks for raising this important point, we will add the following paragraph on evaluation in the manuscript:\n\n\u201cIn practice, each question is formulated so that the correct answer is unique and is formatted to be either a string (one or a few words), a number, or a comma separated list of strings or floats, unless specified otherwise. There is therefore only one correct answer. Hence, evaluation can be done via exact match between a model's answer and the ground truth (up to some simple normalization that is tied to the \u201ctype\u201d of the ground truth). A system (or prefix) prompt is used to inform the model about the required format. In practice, GPT4 level models easily follow our format.\u201d\n\nPrefix prompt: *You are a general AI assistant. I will ask you a question. Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\nYOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\nIf you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unles specified otherwise.\nIf you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\nIf you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.*\n\nWhen needed, the question also details the format. Importantly, some examples in our submission including the one mentioned by the reviewer mistakenly come from an older version of the dataset. In the current GAIA version, the prefix prompt and, when needed, the question asks to not use abbreviations. Hence, the question in the manuscript is actually:\n\n*Where were the Vietnamese specimens described by Kuznetzov in Nedoshivina's 2010 paper eventually deposited? Just give me the city name without abbreviations.*\n\nSt. Petersburg would actually be incorrect in the current version. We fixed this in the manuscript.\n\nFinally, we provide our scoring function in the supplementary material.\n\n\n##\u00a0Detailed error analysis for GPT-4 \n\nWe discuss this concern, that is shared with another reviewer, in the general comment.\n\n\n##\u00a0Results for GPT4-vision\n\nGPT4-vision was not available at the time of the writing, and the API is currently limited to 100 requests per day and organization. We will add GPT4-vision results by the end of the rebuttal if possible (it will require at least a few days to run because of OpenAI\u2019s limitation), and commit to add it in the final version of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502503683,
                "cdate": 1700502503683,
                "tmdate": 1700502503683,
                "mdate": 1700502503683,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hivgk3w9P9",
                "forum": "fibxvahvs3",
                "replyto": "Tnk3gIN5Wa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_FuKx"
                ],
                "content": {
                    "title": {
                        "value": "thanks!"
                    },
                    "comment": {
                        "value": "thanks! I think these clarifications + the GPT4-vision results will improve the paper. I still vote it be accepted."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716011235,
                "cdate": 1700716011235,
                "tmdate": 1700716011235,
                "mdate": 1700716011235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RFdAViPIX7",
            "forum": "fibxvahvs3",
            "replyto": "fibxvahvs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new benchmark for General AI assistants, called GAIA. The tasks require complex reasoning and tool usage skills (e.g. web browsing, integrating information from different modalities, and so on). The data is human-generated and human-validated. The resulting benchmark is highly challenging for current LLMs while humans perform quite well on it."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper clearly identifies its goals and places itself in the context of previous research. I especially appreciated how the authors broke away from the common thread of evaluating AIs on tasks that are more and more challenging for humans, as opposed to focusing on cases which are easy for humans but challenging for AIs.\n\nIn this context, however, it might be wise to more clearly acknowledge works that also break away from the abovementioned trend. E.g. it would benefit the paper to say a few more words about the Abstraction and Reasoning Corpus (https://arxiv.org/abs/1911.01547) which was cited, but, in my opinion, without fully clarifying why it was relevant.\n\n- The paper is very clearly written and is a pleasure to read.\n\n- I believe that the paper has the potential to be highly impactful and of broad interest and use to large portions of AI and ML community."
                },
                "weaknesses": {
                    "value": "Overall, the paper has a sound design and already acknowledges some of its limitations none of which I find to be a deal-breaker. That being said,\n\n- The biggest limitation not fully discussed is the dataset's modest size, which, unfortunately, restricts it solely to the role of a performance benchmark rather than a potential source of training data. This limits the paper's potential impact.\n\n- Another issue, discussed, but not fully acknowledged as a limitation, is the lack of \"partial success\" indicators. This is mitigated by the presence of questions with different difficulty levels, but might still be problematic. When solving a question requires a complex sequence of actions, it is highly desirable to have some measure of where the process breaks down. While the authors speculate that, potentially, in the future \"the paradigm of LLMs calling tools for every task other than text understanding might not stay\", the reality of today is such that having more nuanced/partial feedback would be helpful.\n\nNone of the issues listed above is a disqualifying weakness, however. I offer some suggestions on how, in my view, these issues could be mitigated in the next section."
                },
                "questions": {
                    "value": "1) The authors insist that a component of human labor is crucial to create high-quality questions (\"we believe human annotators are currently essential to have diverse and grounded questions, as opposed to programmatically generated ones\"). However, in my opinion, this might be underselling one key idea behind their own work.\n\nIt seems that, fundamentally, the questions in GAIA are challenging because they require reverse-engineering a fairly random process of question generation, which includes randomly choosing which tool to use, what to google, and so on. So the problems are challenging for AIs because they are under-defined inverse problems. \n\nAt the same time, problem generation is a (straight)forward process. The instructions given in appendix C could, quite possibly, be adapted to tool-assisted LLM prompts. Such LLM-driven automation might offer a good compromise of data price and quality, and such approaches seem to be getting traction (e.g. https://arxiv.org/abs/2303.15056, https://arxiv.org/abs/2305.13877).\n\nI would like to highly encourage the authors not to dismiss this direction, as adding such automation might dramatically increase the impact of their work. In the very least, it might be worth acknowledging this possibility in the discussion.\n\n2) As a potential way to mitigate the absence of \"partial success indicators\", it might be useful to release a version of the dataset where one possible ground truth solution trace is explored and, potentially, in which associated files are pre-processed (e.g. images are replaced by their detailed verbal descriptions). This simplified dataset version would allow to better diagnose existing models, and to disentangle a) conceptual difficulties of aggregating information from different sources and b) procedural challenges (choosing what tool to use, what to google, and, in general, what information to collect)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827832579,
            "cdate": 1698827832579,
            "tmdate": 1699636553960,
            "mdate": 1699636553960,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dGe0HnVgJY",
                "forum": "fibxvahvs3",
                "replyto": "RFdAViPIX7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. \n\n\n##\u00a0Size of the dataset \n\nWe discuss this concern, that is shared with other reviewers, in our general response.\n\n\n##\u00a0Further considering automation\n\nThis is a great point that we will discuss in the revised manuscript. \n- Current best models are not good enough for diverse generations given many strict constraints such as in our protocol (the most important and difficult being unambiguity in the question), which we believe is a different setup from https://arxiv.org/abs/2303.15056 and https://arxiv.org/abs/2305.13877. This is why human labor was instrumental to create valid questions. In fact, even human annotators had difficulties enforcing all the constraints in our protocol. Hence, we are pessimistic in the short term on automating generation of both varied and unambiguous questions.\n- However, the problem becomes much more tractable by removing one of the constraints. Removing the diversity constraint would allow to automatically generate unambiguous questions for a number of sources of truth limited by the human workforce needed to write associated code for each source. For example, given a source of truth such as wikipedia, one could come with an implementable set of rules leading to unambiguous questions and automatically fetch the answer. Removing the unambiguity constraint seems more scalable. In that case however, the answer, that would not be unique anymore, should probably be evaluated differently e.g. via a reward model (for the reasoning path), a judge model, or a human judge, with associated downsides in each setup.\n\nTo conclude, removing the ambiguity constraint is a promising path towards automated generation of GAIA-type data for training, at the expense of making factual evaluation of the final answer challenging again.  \n\n\n##\u00a0Adding indicators of partial success\n\nThis is also a great point. We already release the annotator's trace for the validation set, which can be used for this purpose since it contains which tools were used, how, and very often, intermediate information required to compose the final answer. Moving forward, it would be easy to modify the protocol to systematically include such intermediate information in a way that is easy to process. However, we believe it is not desirable to release such data for the test set, as this could lead to overfitting. We will add this discussion in the manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501878828,
                "cdate": 1700501878828,
                "tmdate": 1700501878828,
                "mdate": 1700501878828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "62MD8eJfLr",
                "forum": "fibxvahvs3",
                "replyto": "dGe0HnVgJY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your answer. I have read other reviews and the authors' responses.\n\nI understand the sources of difficulties when it comes to automating the process, I just thought that it's important to discuss this possibility in the paper. I am glad that the authors are ready to do so.\n\nRegarding dataset size - I think it's okay to admit that \"it would be great if GAIA was 10 or 100 times larger\". This is a limitation/area of improvement (in the sense of \"something that could have been better\"), but it's not a disqualifying limitation.\n\n As authors show, even at its present size, GAIA dataset is diagnostic of general model capacity. In other words, the dataset can serve its purpose \"as is\" and will likely be useful for the community.\n\nOverall, I still think it's a strong paper, and I still can confidently recommend its acceptance. Therefore I'm keeping my \"8: accept, good paper\" overall rating unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511189381,
                "cdate": 1700511189381,
                "tmdate": 1700511189381,
                "mdate": 1700511189381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "klNcXOAZWD",
                "forum": "fibxvahvs3",
                "replyto": "weblMtVt67",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Reviewer_CDvr"
                ],
                "content": {
                    "title": {
                        "value": "Second opinion"
                    },
                    "comment": {
                        "value": "I resonate with the authors' response on this matter. Benchmark/dataset papers are crucial for the ICLR community and for the advancement of the AI/ML fields in general. In this particular case, I am willing to actively advocate for the paper to be accepted and would like to encourage Reviewer nJWa to join in on the discussion (or, if their concerns were resolved, to adjust the score). It would be highly unfortunate if this high-quality contribution is rejected because of a potential misunderstanding."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722638842,
                "cdate": 1700722638842,
                "tmdate": 1700722638842,
                "mdate": 1700722638842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jqu7Syimy1",
            "forum": "fibxvahvs3",
            "replyto": "fibxvahvs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_nJWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5446/Reviewer_nJWa"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new benchmark for general AI assistants, GAIA. The benchmark consists of 466 questions across 3 levels of complexity, defined by the number of steps required to solve the task. The questions are designed with three principles in mind: (1) conceptually simple, but tedious for humans. (2) Interpretability, (3) Robustness against memorization. The process for generating the dataset started with authors designing initial questions, and sharing them with annotators alongside instructions to create more questions. Then, questions were validated by ensuring that additional annotators came up with the same response  to the questions. These validated questions formed the final set, which were then fed to GPT4 with and without plugins, and AutoGPT with the ChatGPT backend. Reported results show that GAIA is challenging for these agents, while being very easy for humans. Performance is also reported on a per-capability bases, with questions divided into 5 categories: Web Browsing, Coding, Multi-modality, Diverse File-type reading, and N/A."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Important problem: As LLMs advance and become part of everyday life, General AI assistant benchmarks are exceedingly important. \n\n2. Markedly different from other benchmarks: existing benchmarks focus on Narrow AI: expertise in a specific domain. GAIA focusses on more general purpose tasks that require multiple steps and are tedious rather than being hard for humans. Such simple but tedious tasks are optimally suited for AI assistants, making it a great guiding principle for the benchmark.\n\n3. Paper easy to follow: The paper does a great job of presenting the approach, design decisions, and related work. The figures explain the work well too."
                },
                "weaknesses": {
                    "value": "1. The venue is not a great fit. The majority of the contribution here is annotated dat, and the design decisions made in doing so. This would make the work a better fit for conferences focussing on these aspects, including CHI and UIST. There are no learned representations, or models, putting it out of the domain of the ICLR community.\n\n2. Size and composition of the dataset: While GAIA is a good start, 466 questions seems like a very small dataset for a general purpose AI agent. Furthermore, most of these questions come from web browsing, which makes the benchmark quite close to a narrow AI benchmark for web browsing.\n\n3. Experiments very thin: The utility of designing a benchmark could be justified if it taught reasonable insights about the behaviour of existing models. In its current form, the investigations presented in the paper do not offer any new insights about the behavior of these models."
                },
                "questions": {
                    "value": "I don't have any particular questions, as I see a serious misfit with the conference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699271537627,
            "cdate": 1699271537627,
            "tmdate": 1699636553852,
            "mdate": 1699636553852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "weblMtVt67",
                "forum": "fibxvahvs3",
                "replyto": "Jqu7Syimy1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5446/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n\n\n## Fit of the venue\n\nThe ICLR 2024 website lists benchmarks as a topic relevant to the conference. For example, the influential MMLU benchmark was published at ICLR 2021. Therefore, we cannot agree with the reviewer on the lack of fit for the venue. We would also like to emphasize that progress towards \u201clearning better representation\u201d would not be possible without clear ways to measure said progress, such as our proposed GAIA.\n\n\n##\u00a0Size and composition of the dataset\n\nWe discuss this concern, that is shared with other reviewers, in our general response.\n\n\n##\u00a0Experiments\n\nTo the best of our knowledge, our experiments do provide new insights into current models:  in spite of being the best available assistants equipped with relevant tools such as web browsing or image understanding (in the form of plugins), the models we evaluate do badly at our realistic yet simple tasks. In particular, Figure 4 displays the performance of LLMs on GAIA per capability. This suggests that, beyond giving LLMs access to tools, a lot more work is needed for LLMs to become viable assistants, which goes against a popular discourse in the AI community.\n\nWe discuss the possibility for more detailed analysis in the general comment. The lack of more detailed insights is due to the scarcity in (i) capable models to evaluate and (ii) detailed logs of OpenAI model\u2019s answers, including detailed tool calls. We are confident that going forward, such analysis will be possible thanks to submissions including detailed log and / or more open models. Does the reviewer have a particular experiment in mind?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501593624,
                "cdate": 1700501593624,
                "tmdate": 1700501593624,
                "mdate": 1700501593624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]