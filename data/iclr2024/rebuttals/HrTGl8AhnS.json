[
    {
        "title": "PACIA: Parameter-Efficient Adapter for Few-Shot Molecular Property Prediction"
    },
    {
        "review": {
            "id": "0UGyVcEtYg",
            "forum": "HrTGl8AhnS",
            "replyto": "HrTGl8AhnS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_mPeR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_mPeR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the few-shot molecular property prediction problem. Existing gradient-based few-shot methods generally need to update a large number of learnable parameters during the meta-test stage, which is prone to overfitting. To address this problem, this paper proposes the PACIA method the leverages a hypernet to generate adaptive parameters for each task and each molecule in a task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper studies an important research problem.\n2. The proposed method is clear in general and makes sense."
                },
                "weaknesses": {
                    "value": "1. Lack of comparison to SOTA. In Tab. 1 and Tab. 2, the only previous works on few-shot MMP included are ADKF-IFT and PAR. The authors should compare the proposed methods with more existing SOTAs such as [a].\n2. Lack of novelty. The core component is HyperNet. Based on HyperNet, the proposed PACIA makes no significant technical contribution.\n3. Poor writing. Many sentences are grammatically wrong. Some examples are:\n* [In Sec. 1] First is that ... difference.\n* [In Sec. 1] The chemical space is enormous that ... range.\n* [In Sec. 1] The molecule-level difference ... molecules.\n* [In Sec. 1] While others ... accurately.\nNote that four of the first 6 sentences of this paragraph are grammatically incorrect.\nThere are many errors in addition to the above examples. The authors are suggested to carefully proofread the paper to correct the errors.\n4. The design in modulating propagation depth seems not fully reasonable. According to Eqn. (8), $[p]_l$ measures the probability of the event that \"the $l$-th layer is in the model\". However, from Eqn. (7),  the only constraint on $[p]_l $ is $\\sum_l [p]_l = 1$. So it is likely that for some $1<i<j<L$, $[p]_i < [p]_j$. That is to say, a hidden layer (i.e., layer $j$) is more likely to be in the model than a layer before it (i.e. layer $i$), which is unreasonable."
                },
                "questions": {
                    "value": "Please see \"weakness\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767221799,
            "cdate": 1698767221799,
            "tmdate": 1699637111266,
            "mdate": 1699637111266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HsN7x3HkVV",
                "forum": "HrTGl8AhnS",
                "replyto": "0UGyVcEtYg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1. Lack of comparison to SOTA. In Tab. 1 and Tab. 2, the only previous works on few-shot MMP included are ADKF-IFT and PAR. The authors should compare the proposed methods with more existing SOTAs such as [a].**\n\n*Reply*. We compare PACIA with ADKF-IFT and PAR, which are recent baselines obtaining the best performance on MoleculeNet and FS-Mol respectively. We also mention MHNfs (Schimunek et al., 2023), which is not included as it uses additional reference molecules from external datasets. \nFor reference [a], could you please provide the paper title?\n\n\n**Q2. Lack of novelty. The core component is HyperNet. Based on HyperNet, the proposed PACIA makes no significant technical contribution.**\n\n*Reply*. Hypernetworks are a general concept involving neural networks that are trained to generate parameters for a main network. \nDesigning how a hypernetwork modulates the main network is a problem-specific and non-trivial challenge.\nOne cannot directly apply hypernetworks designed for other problems to a new problem. \nConsidering the few-shot MPP problem, existing works can be summarized within an encoder-predictor framework, where GNNs perform effectively, acting as both encoders and predictors. \nCorrespondingly, we have designed a unified GNN adapter that generates a few adaptive parameters. These parameters modulate the message passing process of the GNN in two key aspects: node embedding and propagation depth.\nAs a result, the encoder adapts at the property level, while the predictor adapts at the molecule level. \nOur proposed PACIA effectively addresses the few-shot MPP problem, achieving state-of-the-art performance on benchmark datasets.\nIn conclusion, PACIA contributes significantly to both the technological and empirical advancements in the field of few-shot MPP. \n\n**Q3. Poor writing. Many sentences are grammatically wrong.**\n\n*Reply*. We sincerely apologize for the grammatical issues. We have now carefully proofread the manuscript and marked the grammar corrections in the revised version.\n\n**Q4. The design in modulating propagation depth seems not fully reasonable. The design in modulating propagation depth seems not fully reasonable. According to Eqn. (8), $[p]_l$ measures the probability of the event that \"the $l$-th layer is in the model\". However, from Eqn. (7), the only constraint on $[p]_l$ is $\\sum_l [p]_l=1$. So it is likely that for some $1<i<j<L$, $[p]_i<[p]_j$. That is to say, a hidden layer (i.e., layer $j$) is more likely to be in the model than a layer before it (i.e. layer $i$), which is unreasonable.**\n\n*Reply*. This is a misunderstanding: the relative magnitude between $[p]_i$ and $[p]_j$ is independent of the order between i and j.\n\nFor any input sample $\\mathcal{X}_{\\tau,q}$, it will always go through a complete message passing procedure in a $L$-layer GNN. Afterwards, for each layer $l\\in1,2\\cdots,L$, hypernetwork takes the node embeddings after $l$-th layer (i.e., {$\\textbf{h}^l$}) as input, and generates a scalar $[p]_l$ representing how likely it can get good performance by using {$\\textbf{h}^l$} as the final node embeddings.  \nDuring meta-training, $\\{[p]_l\\}$ act as differentiable combination weights in Equation (8). In this way, the hypernetwork is trained to learn the optimal mapping from {$\\textbf{h}^l$} to $[p]_l$, through gradient flow among Equation (12)-(3)-(5)-(8)-(10) or Equation (12)-(3)-(5)-(8)-(11). \nDuring meta-testing, we directly select one optimal layer $l'$ by Equation (13). Then, only {$\\textbf{h}^{l'}$} of $l'$th layer are fed to next module."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700312696316,
                "cdate": 1700312696316,
                "tmdate": 1700313333420,
                "mdate": 1700313333420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H7hKwTkajG",
            "forum": "HrTGl8AhnS",
            "replyto": "HrTGl8AhnS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_smzw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_smzw"
            ],
            "content": {
                "summary": {
                    "value": "PACIA is a novel approach aimed at addressing the challenges in Molecular Property Prediction (MPP) when labeled data is scarce. The authors identify that existing methods, which typically rely on a gradient-based strategy for property-level adaptation, are prone to overfitting due to the large number of adaptive parameters required. To overcome this, they introduce PACIA, a parameter-efficient Graph Neural Network (GNN) adapter specifically designed for few-shot MPP scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Innovative Solution: PACIA introduces a novel parameter-efficient adapter for Few-Shot Molecular Property Prediction (MPP), addressing the challenge of overfitting in scenarios with scarce labeled data. This approach stands out due to its unique application of a hierarchical adaptation mechanism, modulating both the encoder and predictor in a GNN framework.\n\nWell-Defined Problem and Solution: The paper clearly defines the problem of few-shot MPP and presents PACIA as a well-justified solution. The hierarchical adaptation mechanism is meticulously designed, reflecting the high quality of the work.\n\nAdvancement in Methodology: The introduction of a parameter-efficient adapter and the application of hierarchical adaptation in GNNs for MPP represent a notable advancement in methodology, setting a precedent for future work in the domain."
                },
                "weaknesses": {
                    "value": "Lack of Ablation Studies on Hypernetworks:\nWhile the paper introduces the innovative use of hypernetworks for generating adaptive parameters, it lacks ablation studies or a deeper analysis of how different configurations of hypernetworks affect the performance of PACIA. Incorporating ablation studies or a detailed analysis focused on the hypernetworks component would provide valuable insights into its role and optimization, potentially leading to further improvements in PACIA\u2019s performance.\n\nNeed for Broader Applicability and Generalization:\nThe paper validates PACIA\u2019s performance in few-shot MPP problems, but it could strengthen its case by demonstrating the model\u2019s applicability and generalization across a wider range of molecular property prediction tasks. Conducting experiments or providing examples of PACIA\u2019s performance in diverse MPP tasks would showcase its versatility and generalization capabilities, further solidifying its contributions to the field."
                },
                "questions": {
                    "value": "Comprehensive Comparison with Baselines: Could the authors provide additional comparisons with a broader range of existing methods, especially those that have shown promising results in related domains, to strengthen the validation of PACIA\u2019s performance?\n\nAnalysis of Hypernetworks: How do different configurations or architectures of hypernetworks affect the performance of PACIA? Are there specific settings that are more optimal for this application?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8837/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8837/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8837/Reviewer_smzw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807226782,
            "cdate": 1698807226782,
            "tmdate": 1699637111159,
            "mdate": 1699637111159,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X9mTrBcZE6",
                "forum": "HrTGl8AhnS",
                "replyto": "H7hKwTkajG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1. Comprehensive Comparison with Baselines: Could the authors provide additional comparisons with a broader range of existing methods, especially those that have shown promising results in related domains, to strengthen the validation of PACIA\u2019s performance?** \n\n*Reply*. Please check Q2 in global response. \n\n\n**Q2. Analysis of Hypernetworks: How do different configurations or architectures of hypernetworks affect the performance of PACIA? Are there specific settings that are more optimal for this application?**\n\n*Reply*. \nFor specific settings, please refer to Table 4 and Table 5 for hyperparameters chosen on MoleculeNet and FS-Mol, where the configuration of hypernetwork is included. \n\nFor ablation studies, we now add results concerning three aspects in hypernetwork in Appendix C.4, which are reproduced below.  \n\n1.Effect of concatenating label $\\textbf{y}_{{\\tau},s}$ in Equation (9). Table below shows the testing ROC-AUC obtained on SIDER. As shown, \"w/ Label\" helps keep the label information in support set, which improves the performance.\n\n|     | 10-shot  | 1-shot    | \n| -------- | -------- | -------- | \n| w/ Label| $82.40_{(0.26)}$ | $77.72_{(0.34)}$ |\n| w/o Label | $76.91_{(0.17)}$ | $74.10_{(0.41)}$|\n\n\n2.Different ways of combining active prototype $\\textbf{r}^l_{\\tau,+}$ and inactive prototype $\\textbf{r}_{{\\tau},-}^l$ in Equation (10) and Equation (11). \nAs \"Concatenating\" active and inactive prototypes allows MLP to capture more complex patterns, it obtains better performance on SIDER as shown in the table below.\n\n|    | 10-shot  | 1-shot    | \n| -------- | -------- | -------- | \n| Concatenating| $82.40_{(0.26)}$ | $77.72_{(0.34)}$ |\n| Mean-Pooling | $79.67_{(0.23)}$ | $75.08_{(0.29)}$ |\n\n3.Effect of layers of MLP in hypernetwork. \nTable below shows the testing ROC-AUC obtained on SIDER. \nHere, we constrain that the MLPs in Equation (9-11) have the same layer number.\nAs shown, using 3 layers reaches the best performance. \nPlease note that although we can set different layer numbers for MLPs used in Equation (9-11) which further improves performance, setting the same layer number already obtains the state-of-the-art performance. Hence, we set layer number as 3 consistently. \n\n|     |  1 layer | 2 layer  | 3 layer  | 4 layer  | \n| -------- | -------- | -------- | -------- | -------- | \n| 10-shot| $79.98_{(0.35)}$ | $81.85_{(0.33)}$ |$82.40_{(0.26)}$ |$82.43_{(0.28)}$ |\n| 1-shot   | $75.02_{(0.40)}$ | $76.56_{(0.36)}$ |$77.72_{(0.34)}$ |$77.59_{(0.31)}$ |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304175078,
                "cdate": 1700304175078,
                "tmdate": 1700315655092,
                "mdate": 1700315655092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DjuY3adPvy",
            "forum": "HrTGl8AhnS",
            "replyto": "HrTGl8AhnS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_WZaL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_WZaL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a parameter-efficient approach for few-shot molecular property prediction (MPP) tasks by involving hypernetwork to modulate the GNN parameters. The proposed PACIA is built on top of a main encoder-decoder MPP network, and by learning from the training, a GNN adapter is trained to modulate the node embedding and GNN depth. Extensive experiments are conducted in two settings to evaluate the performance of PACIA. Several in-depth analysis is provided to further discuss the superiority of PACIA, such as running time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is interesting as it presents another direction for few-shot MPP. Unlike general gradient-based approaches, PACIA tends to learn certain key generalized parameters to minimize the training costs. \n+ The paper is well-written and easy to follow. \n+ The authors have conducted several in-depth analysis to comprehensively evaluate the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "- The approaches of modulating node embedding and GNN depth do not have sufficient theoretical support. It is more like experimental attempts. Can the authors provide more details about why the implementation is designed as such? How does such implementation ensure the adaptor learns sufficient information? \n- The main framework of PACIA is based on PAR, making the technical novelty incremental. \n- The figure font is small and hard to recognize. Fig.1 (b) is too abstract. The authors may consider plotting a more detailed overall framework to help understand their method.\n- In Table 2, why the baseline methods are different? PAR is the most similar baseline model, and should be compared."
                },
                "questions": {
                    "value": "See Weaknesses. \n\nThe proposed method is interesting and has certain merit. I am also curious that will it works on general MPP problems? Did the authors try to see the performance not under the few-shot setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8837/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8837/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8837/Reviewer_WZaL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699090549763,
            "cdate": 1699090549763,
            "tmdate": 1699637111047,
            "mdate": 1699637111047,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RhwCwZYM41",
                "forum": "HrTGl8AhnS",
                "replyto": "DjuY3adPvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1. The approaches of modulating node embedding and GNN depth do not have sufficient theoretical support. It is more like experimental attempts. Can the authors provide more details about why the implementation is designed as such? How does such implementation ensure the adaptor learns sufficient information?**\n\n*Reply*. Our PACIA is designed upon hypernetworks. It is proved that hypernetworks can be more expressive and can reach the same error with lower complexity (number of trainable parameters) than embedding-based methods in general [a1]. A line of amortization-based approaches (Requeima et al., 2019; Lin et al., 2021; Przewiezlikowski et al., 2022) have shown the effectiveness of designing hypernetworks to handle few-shot image classification and cold-start recommendation. We share the same spirit. While theoretically analyzing a particular hypernetwork is still an open question, we empirically analyze the contribution of each component in PACIA in Section 5.3, and provide evidence of property-level adaptation and molecule-level adaptation in Section 5.4.\n\n[a1] Galanti, T. and Wolf, L. (2020). On the modularity of hypernetworks. Advances in Neural Information Processing Systems, 33, 10409-10419.\n\n**Q2. The main framework of PACIA is based on PAR, making the technical novelty incremental.**\n\n*Reply*. Please note that we propose PACIA to enhance the encoder-predictor framework, which is adopted by existing works, with a parameter-efficient GNN adapter, rather than merely aiming to improve PAR. \nAs discussed in Section 3.2, \nexisting works take GNN as molecular encoder consistently, and predict by pair-wise similarity (Altae-Tran et al., 2017), multi-layer perceptron (Guo et al., 2021), Mahalanobis distance (Stanley et al., 2021), and GNN operates on relation graph (Wang et al., 2021). \nIn particular, PACIA learns to generate a few adaptive parameters to modulate the message passing process of GNN. \nIf it is only applied on GNN encoder, i.e., w/o M variant in Section 5.3, it obtains 83.62 and 82.01 for 10-shot and 1-shot tasks of Tox21. These results already outperform the best results obtained by existing methods, i.e.,  82.43 obtained by ADKF-IFT and 80.02 obtained by PAR for 10-shot and 1-shot tasks respectively. \nFurther, as the recent PAR obtains the state-of-the-art performance by learning a GNN on relation graphs, we can apply our adapter to modulate this GNN predictor on a molecular level. The benefit is that molecular-level difference can be captured. Empirical results in Figure 2(a) and case study in Section 5.4 validates that applying molecule-level adaptation is useful. \n\n\n**Q3. The figure font is small and hard to recognize. Fig.1 (b) is too abstract. The authors may consider plotting a more detailed overall framework to help understand their method.**\n\n*Reply*. We are sorry for the inconvenience. We have revised Fig.1 (b) and added a more detailed one in Appendix D.\n\n**Q4. In Table 2, why the baseline methods are different? PAR is the most similar baseline model, and should be compared.**\n\n*Reply*. Please check Q1 in global response. \n\n**Q5. I am also curious that will it works on general MPP problems? Did the authors try to see the performance not under the few-shot setting?**\n\n*Reply*. Please check Q2 in global response."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304113540,
                "cdate": 1700304113540,
                "tmdate": 1700316540516,
                "mdate": 1700316540516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BaBCV72SNO",
                "forum": "HrTGl8AhnS",
                "replyto": "RhwCwZYM41",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8837/Reviewer_WZaL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8837/Reviewer_WZaL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the rebuttal. I will consider updating my score after discussing with other reviewers."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644600451,
                "cdate": 1700644600451,
                "tmdate": 1700644600451,
                "mdate": 1700644600451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qZG649VMCG",
            "forum": "HrTGl8AhnS",
            "replyto": "HrTGl8AhnS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_tnxf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8837/Reviewer_tnxf"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the challenges of few-shot molecular property prediction, highlighting two major limitations in current approaches: the neglect of molecule-level differences and a predisposition to overfitting. In response, the authors introduce a parameter-efficient adapter complemented by a molecule-adaptive predictor. The experimental results on various benchmark datasets have demonstrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper focuses on an intriguing and pivotal issue. The scarcity of labeled datasets is a prevalent challenge in the realm of chemistry. \n2.\tThe proposed method is well-motivated. The introduction lucidly underscores the drawbacks of the existing works, and each module designed in this study directly addresses these shortcomings.\n3.\tThe experimental results shown in Table 1 and Table 2 clearly demonstrate the effectiveness of the proposed method compared with the various baselines. Additionally, the authors have undertaken an exhaustive ablation study that accentuates the significance of each individual component."
                },
                "weaknesses": {
                    "value": "1.\tThe exposition on the methodology appears somewhat nebulous, which hampers a clear comprehension of the distinct contributions of each module. Specifically, the average representation at the l-th GNN layer, as depicted in Equation (9), seems disconnected from subsequent steps. The property adaptation and molecule adaptation are not clear in the algorithm.\n2.\tThere is a noticeable discrepancy in the baselines used for comparison in Tables 1 and 2. The rationale behind this difference remains unexplained. For instance, while PAR[1] is conspicuously absent from Table 2, it seems like a plausible candidate for few-shot molecular property prediction in FS-Mol.\n3.\tThe presentation of results in Tables 1 and 2 would benefit from a consistent format, ensuring ease of interpretation for readers.\n\n\n[1] Property-aware relation networks for few-shot molecular property prediction. NeurIPS 2021."
                },
                "questions": {
                    "value": "Please refer to the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699245260232,
            "cdate": 1699245260232,
            "tmdate": 1699637110941,
            "mdate": 1699637110941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KFhgR8cP1Q",
                "forum": "HrTGl8AhnS",
                "replyto": "qZG649VMCG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1. The exposition on the methodology appears somewhat nebulous, which hampers a clear comprehension of the distinct contributions of each module. Specifically, the average representation at the l-th GNN layer, as depicted in Equation (9), seems disconnected from subsequent steps. The property adaptation and molecule adaptation are not clear in the algorithm.**\n\n*Reply*. We apologize for the confusion. \nWe have revised Generating Adaptive Parameters by Hypernetwork paragraph in Section 4.1 to make it clearer. \nProperty-level adaptation is generated by Equation (9) and Equation (10), and molecule-level adaptation is generated by Equation (9) and Equation (11).\nIn Equation (9), we obtain class prototypes $\\textbf{r}\\_{{\\tau},+}^l$ and $\\textbf{r}\\_{{\\tau},-}^l$ by mean-pooling over the embeddings of samples in active class (+) and inactive class (-) from $\\mathcal{S}\\_{\\tau}$. Consequently, the generated adaptive parameter are permutation-invariant to the order of input samples in $\\mathcal{S}_{\\tau}$.\n\n**Q2. There is a noticeable discrepancy in the baselines used for comparison in Tables 1 and 2. The rationale behind this difference remains unexplained. For instance, while PAR[1] is conspicuously absent from Table 2, it seems like a plausible candidate for few-shot molecular property prediction in FS-Mol.**\n\n*Reply*. Please check Q1 in global response. \n\n**Q3. The presentation of results in Tables 1 and 2 would benefit from a consistent format, ensuring ease of interpretation for readers.**\n\n*Reply*. Thanks for the advice. We now use a consistent format for Table 1 and Table 2 in the revised manuscript."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304696390,
                "cdate": 1700304696390,
                "tmdate": 1700304696390,
                "mdate": 1700304696390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]