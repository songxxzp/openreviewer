[
    {
        "title": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel"
    },
    {
        "review": {
            "id": "5cvMkVhyua",
            "forum": "YrXHEb2qMb",
            "replyto": "YrXHEb2qMb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_MCWB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_MCWB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces conditional flows of the Maximum Mean Discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. The joint distribution of the ground truth and the observations is approximated using discrete Wasserstein gradient flows, and an error bound for the posterior distributions is established. it is proven in the paper that the particle flow within our method indeed functions as a Wasserstein gradient flow of an appropriate functional. The paper's efficacy is demonstrated through various numerical examples, encompassing applications such as conditional image generation and the resolution of inverse problems, including superresolution, inpainting, and computed tomography in low-dose and limited-angle scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposal of MMD flows with a \"generalized\" kernel kernel which is also known as energy distance or Cramer distance is new. \n* The paper can prove that the particle flow with the generalized MMD is indeed a Wasserstein gradient flow of an appropriate function.\n* The paper uses the MMD flows in the setting of sampling from the posterior which is interesting and new.\n* Experiments are conducted on class-conditional image-generation (MNIST, FashionMNIST, and CIFAR10) and inverse problems with medical images."
                },
                "weaknesses": {
                    "value": "* There is no quantitative comparison in class-conditional image-generation with previous works e.g., score-based generative modeling (without using labels). Similarly, score-based generative models can also be used in medical image inverse-problem [1].\n* There is no comparison with Sliced Wasserstein Gradient flows e.g., with JKO scheme. [2]\n* Considering discrete flows is quite restricted. \n\n[1] Solving Inverse Problems in Medical Imaging with Score-Based Generative Models.\n[2] Efficient Gradient Flows in Sliced-Wasserstein Space"
                },
                "questions": {
                    "value": "* Standard Sliced Wasserstein is not optimal, there are other variants e.g., [3],[4]. Is standard SW preferred in this setting?\n* Can the proposed MMD flows be seen as a debiased version of Sliced Wasserstein gradient flow in the setting of discrete flows?\n\n[3] Generalized Sliced Wasserstein Distances\n[4] Energy-Based Sliced Wasserstein Distance"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Reviewer_MCWB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697766423086,
            "cdate": 1697766423086,
            "tmdate": 1700533493375,
            "mdate": 1700533493375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WTPO1S4Cui",
                "forum": "YrXHEb2qMb",
                "replyto": "5cvMkVhyua",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for the review. To address the first two points from the \"weaknesses\" part, we added a comparison for class-conditional image generation with the previous method of (Du et al. 2023) which is based on sliced Wasserstein gradient flows. For details, we refer to the general answer.\nWe address your other comments below.\n\n\n- Regarding other slicing variants: \nConsider other slicing procedures could be an interesting line of future research. \nIn this paper, we focused on the standard mean-slicing procedure, because of its theoretical tractability.\nWe added a sentence to the future-work section that we want to generalize our results for other slicing variants and transfer these results to (sliced) MMD functionals. \nNote that the focus of the paper is on conditional Wasserstein gradient flows of the MMD functional.\nClearly, we have for our special negative distance kernel that \"MMD = sliced MMD\" which can be used in the numerical part,\nbut plays no role in the rest of the paper.\n\n- Regarding gradient biases: (Bellemare et al. 2017) show that Wasserstein distances have biased sample gradients. Since in 1D Wasserstein and sliced Wasserstein coincide, this probably also holds true for the sliced Wasserstein distance, even though we are not aware of a formal proof of this statement.\nIn Appendix A and Proposition 3 of the same paper (Bellemare et al. 2017) it is shown that the energy distance (= MMD with negative distance kernel) has unbiased sample gradients.\nIn 1D, the energy distance coincides with the so-called Cramer distance, which is presented in Bellemare et al. 2017) as \"debiased Wasserstein distance\". In this sense, one could indeed view our MMD flows as \"debiased sliced Wasserstein gradient flows\". However, since we do not focus on gradient biases, we do not want to use this term within the paper.\n\n(Bellemare et al, 2017) The Cramer Distance as a Solution to Biased Wasserstein Gradients"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161579970,
                "cdate": 1700161579970,
                "tmdate": 1700161579970,
                "mdate": 1700161579970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KRkHlIikq1",
                "forum": "YrXHEb2qMb",
                "replyto": "WTPO1S4Cui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5222/Reviewer_MCWB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5222/Reviewer_MCWB"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your replies,\n\nMy concerns are partially addressed i.e., a baseline was added. I raised my score to 6. \n\nBest regards,"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533483299,
                "cdate": 1700533483299,
                "tmdate": 1700533483299,
                "mdate": 1700533483299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JdySIeobRZ",
            "forum": "YrXHEb2qMb",
            "replyto": "YrXHEb2qMb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_mNmx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_mNmx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes conditional MMD flows with the negative distance kernel for posterior sampling and conditional generative modelling. By controlling the MMD of the conditional distribution using the MMD of the joint distribution, the paper provides a pointwise convergence result. In addition, the paper shows that the proposed particle flow is a Wasserstein gradient flow of a modified MMD functional, and hence provides some theoretical guarantee for [1]. Finally, the paper experiments on several image generation problems and compares with other conditional flow methods.\n\n[1] C. Du, T. Li, T. Pang, S. Yan, and M. Lin. Nonparametric generative modeling with conditional slicedWasserstein flows. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett (eds.), Proceedings of the ICML \u201923, pp. 8565\u20138584. PMLR, 2023."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and clearly-organized.\n2. The paper proves that the proposed particle flow is a Wasserstein gradient flow of an appropriate functional, thus providing a theoretical justification for the empirical method presented by [1].\n3. Abundant generated image samples are shown in the experiments.\n\n[1] C. Du, T. Li, T. Pang, S. Yan, and M. Lin. Nonparametric generative modeling with conditional slicedWasserstein flows. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett (eds.), Proceedings of the ICML \u201923, pp. 8565\u20138584. PMLR, 2023."
                },
                "weaknesses": {
                    "value": "1. The novelty of the proposed method appears to be limited, since it is mainly the Generative Sliced MMD Flow [1] method applied to conditional generative modelling problems. Additionally, the proof of Theorem 3 partially follows [2].\n2. The theoretical comparison with different kernels (Gaussian, Inverse Multiquadric and Laplacian [1]) and discrepancies (KL divergence, W_1 [2] and W_2 [3] distance) in Theorem 2 is insufficient.\n3. The numerical results of image generation lack comparison with other methods like Generative Sliced MMD Flow in [1]. It would be better to compare the FID scores for different datasets and various methods like [1], since the proposed method adopts the computational scheme of Generative Sliced MMD Flow. It would be beneficial to compare with Conditional Normalizing Flow in the superresolution experiment and with WPPFlow, SRFlow in the computed tomography experiment.\n\n\n[1] J. Hertrich, C. Wald, F. Altekr\u00fcger, and P. Hagemann. Generative sliced MMD flows with Riesz kernels. arXiv preprint 2305.11463, 2023c\n\n[2] F. Altekr\u00fcger, P. Hagemann, and G. Steidl. Conditional generative models are provably robust: pointwise guarantees for Bayesian inverse problems. Transactions on Machine Learning Research, 2023b.\n\n[3] F. Altekr\u00fcger and J. Hertrich. WPPNets and WPPFlows: the power of Wasserstein patch priors for superresolution. SIAM Journal on Imaging Sciences, 16(3):1033\u20131067, 2023."
                },
                "questions": {
                    "value": "1. The paper states that MMD combining with the negative distance kernel results in many additional desirable properties, however it lacks convergence rate or discretization error analysis because \u201cthe general analysis of these flows is theoretically challenging\u201d. Regarding this problem, what is the advantage of MMD over other discrepancies like Kullback\u2013Leibler divergence or the Wasserstein distance especially for conditional generative modelling problems?\n2. Is it possible to provide a discretization error analysis between discrete MMD flow and the original continuous MMD flow?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698481481133,
            "cdate": 1698481481133,
            "tmdate": 1699636520287,
            "mdate": 1699636520287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n5m26S6fus",
                "forum": "YrXHEb2qMb",
                "replyto": "JdySIeobRZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Many thanks for your comments. We added several comparisons in the numerical part. In particular, we added a comparison with WPPFLows on CT and a FID comparison for class-conditional image generation.\n\n## Regarding the novelty\n\nNote this is **not only** \"a generative Sliced MMD Flow [1] method applied to conditional generative modelling problems\".\nOur paper is about **conditional Wasserstein gradient flows of the MMD functional**.\nThere is just a small hint that \"MMD = sliced MMD\" for the negative distance kernel, see [1], which is irrelevant for the theory part.\n\nThe consideration of the conditional flows brings indeed a lot of new theoretical challenges. \nTwo of them are addressed in our paper:\n\ni) Conditional generative models approximate the joint distribution  by learning a mapping $T$ such that\n$P_{X,Y} \\approx P_{T(Z,Y)),Y}$,\nbut in fact we are interested in the posterior distributions $P_{X|Y=y}$.\nIn this paper, we prove error bounds between posterior and joint distributions within the MMD metric.\nTo this end,\nwe use relations between measure spaces and RKHS as well as Lipschitz stability results \nunder pushforward measures which are quite involved.\nThese results are new and highly non-trivial to prove. Clearly they are based on preliminary work \nin particular on [2], but we just cited the necessary results and use them in completely new proofs.\n\nii) We want to characterize a **conditional** Wasserstein gradient flow of a functional $F$ as\n**usual** Wasserstein gradient flow of a modified functional to better understand the behaviour of the first one.\nTo establish a relation to Wasserstein gradient flows, we locally lift the $\\mathbb R^{N d}$ into the Wasserstein space using local isometries. As a byproduct we give a theoretical explanation for the results of (Du et al. 2023). \n\nWe have rewritten the contributions paragraph in the introduction to highlight the points i) and ii).\n\nIn the applications part, the conditional MMD flows for large-scale imaging inverse problems like computed tomography is new. \nOf course we use [1] for reducing the computational cost and we agree that this is crucial to obtain a tractable algorithm, but this is not the point of our contribution.\n\n## Other comments\n\n- Thanks for the suggestion to compare with a conditional normalizing flow for superresolution and with SRFlow for CT. We would like to point out that SRFlow is the same as a conditional normalizing flow, where the authors adapted the architecture for superresolution. We added a comment about that in the paper.\n\n- Different kernels: Since the negative distance kernel is the only kernel, where we can compute the gradient of MMD in $O(N\\log(N))$ instead of $O(N^2)$, it is the only kernel, where our method is applicable. From a theoretical viewpoint, generalizing Theorem 2 to a larger class of kernels would be nice and is a current line of our research. For the KL divergence, we have to restrict to absolutely continuous measures, but get a stronger version of Theorem 2 with an equality statement. We refer to Remark 7 in Appendix A.2 for a detailed discussion of Theorem 2 with KL or Wasserstein-1.\n\n- Advantage over KL and Wasserstein: Neither the KL divergence nor the Wasserstein distance admit a closed-form two-sample formulation. For the KL divergence, we have to estimate the density (or the score) of one of the measures, \nfor Wasserstein we have to solve an optimization problem. \nIn contrast, we can just compute the MMD and its derivative directly on two sets of samples. \nIn order to lower the complexity of the computations, one can apply the slicing procedure from [1]\nwhich admits a provable error bound.\n\n- Regarding error bounds between discrete and continuous MMD flows: In the one-dimensional case, (Carillo et al. 2020) proved that the discrete MMD flows with negative distance kernel converge in the mean-field limit to the continuous ones and we cited this in our paper.\nIn higher dimensions, the MMD functional with negative distance kernel is not $\\lambda$-convex along generalized geodesics such that most of the theory of Wasserstein gradient flows is no longer applicable. In (Hertrich et al. 2022), the authors derive analytic formulas for MMD flows with negative distance kernel in higher dimensions. \nDespite these examples, we are not aware of any theoretical results regarding existence, uniqueness or convergence results for such flows for $d\\geq 2$.\nTherefore, we feel that there is a long line of theoretical work which has to be done before a proof of such error bounds would be reachable. Therefore they are beyond the scope of our paper.\n\n## References:\n\n[1], [2] from the review\n\n(Carillo et al. 2020) Measure solutions to a system of continuity equations driven by newtonian nonlocal interactions.\n\n(Hertrich et al. 2022) Wasserstein steepest descent flows of discrepancies with Riesz kernels.\n\n(Hertrich et al. 2023) Generative sliced MMD flows with Riesz kernels."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161532458,
                "cdate": 1700161532458,
                "tmdate": 1700161787118,
                "mdate": 1700161787118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jtEYG2YqZS",
            "forum": "YrXHEb2qMb",
            "replyto": "YrXHEb2qMb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_eQGc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_eQGc"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, conditional MMD flow with negative distance kernel is introduced.\nThe model's stability is proven by bounding the expected approximation error of the posterior distribution.\n\nThrough theoretical justification, the authors obtain convincing results by neglecting the velocity in the y-component in sliced Wasserstein gradient flows.\nThen, the power of the method is also demonstrated by numerical examples including conditional image generation and inverse problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The theoretical justification of the proposed method is clear and detailed.\n2. Several experiments are conducted to prove the power of the method.\n3. Introducing negative distance kernel to MMD is a good idea and contributions are well-described."
                },
                "weaknesses": {
                    "value": "As mentioned by the authors, the proposed approach has some limitations:\n\n1. The model is sensitive to forward operator and noise type.\n2. Lack of meaningful quality metrics to evaluate the results.\n3. Realism of the computed tomography experiment results can not be guaranteed."
                },
                "questions": {
                    "value": "1. Except computed tomography experiment, only visulization results of other experiments are given in the paper, however, it is difficult to quantitatively evaluate the result and to compare with other method. Hence, evaluation metrics need to be introduced or self-defined.\n\n2. The related work: Neural Wasserstein gradient flows for maximum mean discrepancies with Riesz kernels, proposed similar method, what is the strength and advantage over it? and what about the performance difference?\n\n3. Why chosing UNet? Is there a significant difference in the effect of choosing other models such ResNet and transformer.\n\n4. As Fig.7c shows, inpainting results of CIFAR are not good enough, the generated images differ from each other greatly at the unobserved part, what is the reason? and are there any solutions to improve it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Reviewer_eQGc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652541495,
            "cdate": 1698652541495,
            "tmdate": 1699636520198,
            "mdate": 1699636520198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pLTznucWnN",
                "forum": "YrXHEb2qMb",
                "replyto": "jtEYG2YqZS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for your comments. We added quantitative evaluations of our method as outlined in the general answer. We address your other comments below.\n\n- Regarding the sensitivity to the forward operator/noise: We want to clarify that this issue is not specific for our method. Instead, any supervised learning method for inverse problems is sensitive to mismatches between the training and testing models.\nWe emphasize this issue in the limitations section, because it is particularly relevant for medical imaging applications, where such mismatches are unavoidable. However, in our paper we want to demonstrate the usability of our method for highly ill-posed and high-dimensional imaging inverse problems. In particular, calibrating it for clinical applications is not within the scope of our paper.\nThis is common-sense in the machine learning community and unfortunately most authors do not even mention that.\n\n- The paper (Altekr\u00fcger et al, 2023) **does not provide any \"proper\" generative model**. \nInstead the authors consider forward and backward schemes for non-discretized Wasserstein gradient flows. Consequently, they have to train a generative model **in each step of the gradient flow**. This allows an extensive analysis of such flows, but is computational costly and therefore not realizable for large problems. Indeed, their MNIST example does not consider the whole dataset but only a subset of a few hundred images.\n\n- The U-Net is a standard choice for many imaging applications as well as in diffusion models, see (Huang et al, 2021). \nWe did also some brief experiments with a lightweight transformer network and achieved slightly worse results.\nWe think that there is not a very large difference as long as the considered network is expressive enough.\n- Regarding the inpainting example: CIFAR10 is a highly diverse dataset such that a large variation of the generated samples for inpainting is natural. Consequently, the generated images have to differ from each other greatly. Note that for less diverse datasets like (Fashion)MNIST or CelebA, we obtain much less variation in the reconstructed images.\n\n\nReferences\n\n(Du et al, 2023) Nonparametric Generative Modeling with Conditional Sliced-Wasserstein Flows, Du, Li, Pang, Shuicheng, Lin, ICML 2023\n\n(Altekr\u00fcger, 2023) Neural Wasserstein Gradient Flows for Discrepancies with Riesz Kernels, Altekr\u00fcger, Hertrich, Steidl, ICML 2023\n\n(Huang et al, 2021): A variational perspective on diffusion-based generative models and score matching, Huang, Lim and Courville, NeurIPS 2021"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161490145,
                "cdate": 1700161490145,
                "tmdate": 1700161490145,
                "mdate": 1700161490145,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0cT1m6M8rx",
            "forum": "YrXHEb2qMb",
            "replyto": "YrXHEb2qMb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_PLVu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5222/Reviewer_PLVu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a conditional flow of the MMD with the negative distance kernel, which can be further implemented by conditional generative neural networks with application in image generation, inpainting, and super-resolution. The authors derive the convergence of the posterior under some certain stability conditions, and relate it to a Wasserstain gradient flow. Those results extend previous investigation for sliced Wasserstein flow. The work is relatively theoretical and lacks a thorough comparison with other generative models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents some interesting theories, and extends the analysis on sliced Wasserstein gradient flow."
                },
                "weaknesses": {
                    "value": "1. It would be better to elaborate on the pros and cons of using a negative distance kernel (efficiency, sample complexity, etc).\n\n2. The contribution is not entirely clear. Could the author comment on the effectiveness/efficiency/novelty/difficulty of the proposed method?\n\n3. A highlight of the proof techniques used by the authors to address gradient flows with respect to MMD with negative distance kernel without mean-field approximation would help to improve the importance of this work."
                },
                "questions": {
                    "value": "1. In Equation 4, $T$ is defined, however $T_\\sharp$ is not defined.\n\n\n2. Is it possible to validate the error bound via numerical experiments somehow?\n\n\n3. Could the author comment on the difference between the proposed analysis and sliced Wasserstein flow, as the implementation is still based on the sliced version of it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5222/Reviewer_PLVu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711823919,
            "cdate": 1698711823919,
            "tmdate": 1699636520090,
            "mdate": 1699636520090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cpvQdWPPpz",
                "forum": "YrXHEb2qMb",
                "replyto": "0cT1m6M8rx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for the review. We added comparisons with other generative models as outlined in the general answer. We address your other comments below.\n\n## Main proof techniques and contributions\n\nWe have two main theoretical contributions in addition to numerical applications:\n\ni) Conditional generative models approximate the joint distribution by learning a mapping such that\n$P_{X,Y} \\approx P_{T(Z,Y)),Y}$,\nbut in fact we are interested in the posterior distributions $P_{X|Y=y}$.\nIn this paper, we prove error bounds between posterior and joint distributions within the MMD metric.\nTo this end,\nwe use relations between measure spaces and RKHS as well as Lipschitz stability results \nunder pushforward measures which are quite involved.\n\nii) We want to characterize a **conditional** Wasserstein gradient flow of a functional $F$ as\n**usual** Wasserstein gradient flow of a modified functional to better understand the behaviour of the first one.\nTo establish a relation to Wasserstein gradient flows, we locally embed the $\\mathbb R^{N d}$ into the Wasserstein space using local isometries. As a byproduct we give a theoretical explanation for the results of (Du et al. 2023). \n\niii) Numerically, we approximate our conditional MMD flows by conditional generative neural networks and apply them in various settings like class conditional image generation, image restoration and CT.\n\nWe have rewritten the contributions paragraph in the introduction to highlight the points i), ii) and iii).\n\n## Difference to sliced Wassestein flows \n\nWe want to emphasize that our proposed method **does not use sliced distances**, also not the sliced Wasserstein distance.\nWe consider conditional gradient flows for the functional $F$=MMD within the $W_2$ geometry. \nIn contrast, conditional sliced Wasserstein gradient flows (Du et al. 2023) consider gradient flows for the functional $F$=sliced Wasserstein within the $W_2$ geometry. Consequently, both methods act in the $W_2$ geometry, but minimize **completely different functionals**.\nOur paper contains just a  hint that for the negative distance kernel (and only for this kernel) it holds\n``MMD = sliced MMD''.\nThis is an exceptional nice property proven in (Hertrich et al. 2023) which we clearly\nexploit in the numerical part for speeding up the computations.\n\n## Other comments:\n\n- Regarding the numerical verification of bounds we have to note that this is a worst case bound. \nTherefore it is hard to do numerical verification (as obtaining the ground truth posterior is hard/impossible in high dimensions) \nand it is likely that real world examples actually attain better rates. \n\n- We added some nice properties of the negative distance MMD (efficient calculation and good sample complexity) in the introduction.\n\n- $T$#$\\mu$: This denotes the pushforward measure of $\\mu$. We included the definition in the paper.\n\n## References\n\n(Hertrich et al. 2023) J. Hertrich, C. Wald, F. Altekr\u00fcger, and P. Hagemann. Generative sliced MMD flows with Riesz kernels. arXiv preprint 2305.11463"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161417558,
                "cdate": 1700161417558,
                "tmdate": 1700161738741,
                "mdate": 1700161738741,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]