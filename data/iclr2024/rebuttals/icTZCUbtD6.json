[
    {
        "title": "Dissecting sample hardness: Fine-grained analysis of Hardness Characterization Methods"
    },
    {
        "review": {
            "id": "U2n8lUlDOn",
            "forum": "icTZCUbtD6",
            "replyto": "icTZCUbtD6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_u8mR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_u8mR"
            ],
            "content": {
                "summary": {
                    "value": "The author claims that existing works on Hardness Characterization Methods (HCMs) fail to address comprehensive and quantitative evaluation of HCMs. Also, the author points out the absence of an integrated and practical software tool that can simulate and evaluate various types of HCMs. In this context, the author proposes a fine-grained taxonomy for categorizing multiple hardness types and introduces the Hardness Characterization Analysis Toolkit (H-CAT), which supports over 14K setups with 13 different HCMs across 8 hardness types. The author reports the performance of each HCM in various settings using H-CAT and provides invaluable takeaways and practical tips that were not seen in previous works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) This paper diligently considered all the recently proposed HCM algorithms within the past 3-4 years and developed H-CAT, reporting experimental results using it.\n2) Through graphical illustration (Figure 1) and table analysis (Table 1), the need for a systemic evaluation framework for HCMs was effectively underscored.\n3) Figures 3, 4, 5, and 6 analyzed the behavior of popular HCMs under various hardness settings from different perspectives. Additionally, the experiments conducted using the toolkit developed by the authors hold greater significance.\n4) The author provided detailed instructions for use of H-CAT and additional experimental results through a voluminous appendix."
                },
                "weaknesses": {
                    "value": "Hardness characterization is an important concept in various fields such as computer vision works like object detection and segmentation, as well as natural language processing and reinforcement learning. However, the proposed HCM evaluation framework in this paper can only handle image classification."
                },
                "questions": {
                    "value": "1) In the Introduction, it is stated that \"These 'hard' samples or data points can significantly hamper the performance of ML models.\" However, from an active learning perspective, can't we consider hard samples to be more informative and better samples with higher annotation efficiency?\n2) On page 3, in the section \"Data characterization,\" it is mentioned that difficult or easy samples are classified into separate easy or hard subsets. However, instead of categorizing sample difficulty into two binary sets, wouldn't it be better to dynamically control the learning process by assigning weights or using other methods based on continuous difficulty values? Additionally, excluding OOD/outlier and atypical samples from model training could potentially enhance the model's generalization capability. So, couldn't excluding them altogether from training be a suboptimal choice?\n3) The author mentioned that existing works have a disadvantage of conducting indirect evaluation in downstream tasks. In that case, how does the proposed framework perform direct evaluation?\n4) In Figure 6, Spearman rank correlation scores for HCMs are reported. As far as I know, Spearman rank correlation calculates the correlation between two variables. How was the correlation computed from multiple runs in this case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Reviewer_u8mR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697891727457,
            "cdate": 1697891727457,
            "tmdate": 1699636934900,
            "mdate": 1699636934900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ud1O2rAy9M",
                "forum": "icTZCUbtD6",
                "replyto": "U2n8lUlDOn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u8mR [Part 1/2]"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer u8mR``\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following in turn, along with corresponding updates to the revised manuscript. We have grouped answers for ease of readability as follows:\n\n(A) Hardness characterization in other settings __[Part 2/2]__\n\n(B) Comparison to hard samples in active learning __[Part 2/2]__\n\n(C) Usage of hardness scores __[Part 2/2]__\n\n(D) Direct evaluation clarification __[Part 2/2]__\n\n(E) Spearman rank correlation clarification __[Part 2/2]__"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124553985,
                "cdate": 1700124553985,
                "tmdate": 1700124553985,
                "mdate": 1700124553985,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "woMmg114Bh",
                "forum": "icTZCUbtD6",
                "replyto": "U2n8lUlDOn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u8mR [Part 2/2]"
                    },
                    "comment": {
                        "value": "### (A) Hardness characterization in other settings\n\nWe clarify that we primarily studied HCMs in the context of image classification, as most HCMs (10/13) have been developed in this context. While our current evaluation predominantly employs image datasets, we also incorporate tabular datasets (\"Covertype\" and \"Diabetes130US\") to demonstrate the generalizability of H-CAT across different data modalities. This inclusion of diverse datasets is a step towards extending the applicability of HCMs beyond image classification. We hope that the extensibility of our framework will encourage future work to cover even more domains as suggested by the reviewer.\n\n---\n\n### (B) Comparison to hard samples in active learning\n\nThank you for pointing out the important distinction between the concepts of \"hard\" samples in active learning and in data-centric AI/hardness characterization. We wish to clarify the different contexts in which samples are termed as \u201chard\u201d between the two areas of the literature. \n\n- **Active Learning**: \"hard\" samples refer to those instances where the model struggles to make accurate predictions, yet the samples are _correct_. These samples are valuable because they are informative. However, we note these samples are NOT inherently flawed or erroneous. \n- **Data-Centric AI and Hardness Characterization**: In contrast to active learning, in data-centric AI and hardness characterization, \"hard\" samples typically denote samples that have inherent issues or are erroneous, such as being mislabeled. Hence, the focus is on identifying and addressing these problematic samples to improve data quality. Consequently, we wish to detect and rectify the actual issues with the samples themselves, rather than to use them for informative purposes.\n\nGiven this distinction, our work aligns with the second context, focusing on identifying and characterizing samples that are inherently problematic or erroneous. We thank the reviewer for bringing up this point.\n\n---\n\n### (C) Usage of hardness scores\n\n\nWe agree with the reviewer that an important direction is how the types of samples could be used to dynamically control the learning process \u2014 for instance, a curriculum learning approach as suggested by the Reviewer in contrast to, for instance, pruning/sculpting them. These methods represent an interesting direction of how to leverage the hardness scores to enhance the performance of a downstream model.\n\nHowever, it is important to emphasize that these approaches, while important, are orthogonal to the primary focus of our work. Our research in H-CAT is centered on understanding, evaluating, and benchmarking the capability of HCMs to detect different types of hardness in samples. The core objective is to provide a comprehensive and systematic framework to evaluate the effectiveness of various HCMs in identifying hard samples across a range of hardness types.\n\nWhile we acknowledge the utility of the approaches suggested by the Reviewer in the context of model training, our research does not delve into the application of detected hard samples in training strategies. Instead, it focuses on the foundational aspect of accurately identifying these hard samples, which is a prerequisite for any subsequent application or strategy, including curriculum learning or sample pruning.\n\nWe thank the reviewer for this suggestion and have included it in **Sec 6**, wherein future work can build on H-CAT and explore this dimension of what is the best way to use the hardness scores for a downstream model.\n\n**UPDATE:** Sec.6 now outlines work to understand the usage of hardness scores to guide future model training.\n\n---\n\n### (D) Direct evaluation clarification\n\nWe clarify that we directly evaluate the capability of HCMs to detect specific hardness types. This approach allows us to understand which types of hardness each HCM can effectively identify, rather than merely assessing model improvement on a curated dataset, which might obscure the actual capabilities of different HCMs on different hardness types. offering valuable insights for the development and application of these methods. We hope that directly assessing which hardness types different HCMs are able to detect and showing their limitations will inspire researchers to develop new methodologies to address these limitations.\n\n---\n\n### (E) Spearman rank correlation\n\nWe clarify that when we compute the Spearman correlation of scores across runs (different seeds), this is done between the hardness scores obtained for all combinations. This mirrors the approach adopted by Maini et al., 2022 and Seedat et al., 2022."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124623344,
                "cdate": 1700124623344,
                "tmdate": 1700124623344,
                "mdate": 1700124623344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NbwlqfySBL",
                "forum": "icTZCUbtD6",
                "replyto": "woMmg114Bh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_u8mR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_u8mR"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your sincere response.\n\nMy concerns are all resolved and I acknowledge the contribution of this work.\n\nBest wishes,"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617188586,
                "cdate": 1700617188586,
                "tmdate": 1700617188586,
                "mdate": 1700617188586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eHLpYDgMXf",
            "forum": "icTZCUbtD6",
            "replyto": "icTZCUbtD6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_kMBG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_kMBG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new taxonomy for quantifying hardness of samples and then creates a framework on top of it (H-CAT) which uses existing hardness evaluation techniques to predict hardness categorized into different hardness types from their taxonomy. The paper then goes on to conduct hardness evaluation experiments on 2 image and 2 tabular datasets and highlights several key takeaways and recommendations for practitioners using their toolkit."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The taxonomy proposed by the paper is simple, clearly explicated, and practically useful. Relating it in detail to previous methods and frameworks and what's missing in existing HCMs based on this taxonomy helps readers create a good mental model of the space.\n2. The comprehensive benchmarking of more than 13 HCMs across multiple datasets, on robust evaluation criteria is useful for the community.\n3. Some takeaways like the efficacy of learning dynamics-based HCMs, lack of efficacy of computational-efficiency motivated methods, importance of significance testing and the variation in stability of the HCMs across seeds, backbones and parameterizations, all provide relevant, empirical information to practitioners using these methods."
                },
                "weaknesses": {
                    "value": "1. Sticking to just MNIST and CIFAR10 limits the usefulness of the takeways. The dataset sizes and complexity we currently operate with are very different from MNIST/CIFAR10 and I'm afraid these learnings might not generalize to larger, more complex datasets.\n2. The Near OOD, far OOD, and the atypical perturbations used in the experiments are oversimplified in my opinion. In the real world, defining near/far OOD and atypical have a complicated notion of support of the sample distribution, but the use of Gaussian noise, texture changes for near OOD and exchanging MNIST with CIFAR (which are drastically different) for far OOD, and using simple invariant functions like shift and zoom for atypical does not do justice to these concepts.\n3. [Minor] The bulk of paper is focussed on creating a taxonomy for hardness evaluation and then building a software toolkit to evaluate existing methods for its quantification. Some initial discussions during the taxonomy creation and some of the non-obvious takeaways are instructive for the researchers in this field, but most of this work is geared towards practitioners. This isn't necessarily a negative thing, but limits the research novelty/impact of the work."
                },
                "questions": {
                    "value": "1. As the severity of perturbations increases, do the methods used in HCMs also show an increase in the hardness score? Does this vary for learning-integrated methods vs others?\n2. What is the effect of using augmentations during training for these models? I'm guessing the hardness predictions will change for some methods but not others."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Reviewer_kMBG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678989798,
            "cdate": 1698678989798,
            "tmdate": 1699636934778,
            "mdate": 1699636934778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dh5hqQM9VY",
                "forum": "icTZCUbtD6",
                "replyto": "eHLpYDgMXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kMBG [Part 1/3]"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer kMBG``\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following in turn, along with corresponding updates to the revised manuscript. We have grouped answers for ease of readability as follows:\n\n(A) Beyond MNIST & CIFAR10 __[Part 2/3]__\n\n(B) Simplicity of perturbations __[Part 2/3]__\n\n(C) Nature of takeaways __[Part 2/3]__\n\n(D) Impact of perturbation severity __[Part 3/3]__\n\n(E) Effect of augmentations __[Part 3/3]__"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124334636,
                "cdate": 1700124334636,
                "tmdate": 1700124334636,
                "mdate": 1700124334636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "alRws7jqrJ",
                "forum": "icTZCUbtD6",
                "replyto": "eHLpYDgMXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kMBG [Part 2/3]"
                    },
                    "comment": {
                        "value": "### (A) Beyond MNIST & CIFAR10\n\nWe wish to clarify our choice of MNIST and CIFAR-10 was strategic as these datasets are well-established in the HCM literature and, as discussed in **Sec 5**, these datasets are \u201cclean\u201d without significant mislabeling. This is essential to create controlled experimental conditions for benchmarking where hardness is introduced. Furthermore, to address concerns about generalizability, we also included evaluations on more complex tabular datasets in Appendix D, such as \u201cCovertype\u201d and \u201cDiabetes130US\u201d from OpenML. This approach ensures our findings are relevant across different data modalities and sizes. \n\nThat said, we understand the Reviewer's potential concern and have conducted an additional experiment using a medical X-ray dataset from the NIH [R1], which features high-quality images curated by multiple medical experts. This additional experiment reinforces the applicability of our findings to more complex and real-world datasets. The results shown in the updated Appendix D.7 align with our initial findings, indicating that our observations regarding various hardness types are not limited to MNIST and CIFAR-10 datasets but are also relevant to more specialized datasets such as medical images. This further validates the generalizability and practical utility of our research across diverse data domains.\n\n**UPDATE:** Added a new Appendix D.7 with additional image experiment on a medical X-ray dataset\n\n---\n\n### (B) Simplicity of perturbations\n\nWe acknowledge the Reviewer's concerns about the perceived simplicity of the OOD and atypical perturbations. We highlight two aspects: (1) our methodology mathematically satisfies the definitions of these perturbations, and (2) **Figure 7 (now Figure 3 in the revised paper)** illustrates that the perturbations we used are representative of hardness types that could realistically occur in actual datasets. Moreover, while our work is indeed a first step, it represents a significant advancement from the existing state of HCM research which doesn\u2019t consider these issues (as shown in **Table 1**). We hope that our framework serves as a foundation for more complex and nuanced studies in the future. \n\nFinally, a key finding is that certain **HCMs even fail on these simple perturbations**, which uncovers a fundamental limitation of the HCM field that needs to be addressed.\n\n---\n\n### (C) Nature of takeaways [minor]\n\nThe paper indeed has a practical aspect of the benchmarking framework. We wish to clarify that this enables the HCM field to rigorously evaluate HCMs for the first time, which is essential for the field to progress. We believe this will help the HCM field to move from the ad hoc and qualitative state (as shown in Table 1) toward a more comprehensive and rigorous evaluation across multiple hardness types. \n\nSpecifically, our work responds to the recent calls for more rigorous benchmarking and understanding of existing ML approaches, as highlighted by Guyon (2022), Lipton & Steinhardt (2019), and Snoek et al. (2018). We then believe this benchmark offers critical insights into the HCMs which are distilled into (1) benchmarking takeaways: to guide researchers about future development of HCMs by highlighting their strengths and weaknesses and (2) practical takeaways: to guide selection and usage of HCMs. \n\nOverall, this work provides insights into what types of hardness different HCMs are able to detect and shows the limitations of HCMs in detecting certain hardness types, which we hope will inspire researchers to develop new methodologies to address these limitations."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124397026,
                "cdate": 1700124397026,
                "tmdate": 1700175204167,
                "mdate": 1700175204167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c60M7MtwMK",
                "forum": "icTZCUbtD6",
                "replyto": "eHLpYDgMXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kMBG [Part 3/3]"
                    },
                    "comment": {
                        "value": "### (D) Impact of perturbation severity\n\n\nWe thank the reviewer for this interesting suggestion! We have conducted an experiment for zoom shift, crop shift, and Near-OOD (covariate), where we have two severities of perturbation namely small and large. For zoom (2x vs 10x), crop shift (5 pixels vs 20 pixels), and Near-OOD (std deviation of 0.5 vs 2). \n\nWe report the results in Fig 99, in Appendix D.8. The results show the percentage changes in hardness scores (in the direction of hardness, e.g. increase or decrease) for different HCMs for the different hardness types considered. n all cases, we see an increase in hardness score$^1$ for increased severity, with some HCM scores showcasing greater sensitivity and changes than others. In particular, we see the gradient and loss-based approaches have the greatest changes as compared to the methods computing scores based on probabilities. We thank the reviewer for suggesting this interesting experiment which has helped to shed further insights into the sensitivities of different HCMs.\n\nFor ease of access the result can also be found at this Imgur link: **https://imgur.com/tTv6lO7**\n\n**UPDATE**: Included a new Appendix D.8 with the experiment.\n\n----\n\n### (F) Effect of augmentations\n\nRegarding the role of augmentations, we wish to clarify the placement of HCMs in the ML pipeline. HCMs are typically used to audit and curate the training set before the downstream model training begins. On the other hand, data augmentations are commonly employed during the model training phase to enhance robustness and generalizability. Given this difference, the HCMs we benchmark from the literature _do not_ use augmentations at all. Thus, since our study is about assessing existing HCMs, not proposing a new HCM, we believe it is outside the scope of our current study. However, we believe it is an interesting idea for future HCM algorithms to explore \u2014 potentially as a way to address the limitations uncovered by our benchmark. \n\n----\n\n### References\n\n[R1] Majkowska, Anna, et al. \"Chest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation.\" Radiology 294.2 (2020): 421-431.\n\n$^1$ For some HCMs increased hardness is an increased score, whereas for others is a decrease (see Appendix A). We report the direction of hardness."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124472660,
                "cdate": 1700124472660,
                "tmdate": 1700153484535,
                "mdate": 1700153484535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gH9z5ayIPi",
                "forum": "icTZCUbtD6",
                "replyto": "eHLpYDgMXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer kMBG"
                    },
                    "comment": {
                        "value": "Dear Reviewer kMBG\n\nThank you again for your time and expertise during the review process! Your suggestions have really helped us improve the paper.\n\nWe just wanted to check in if there's anything else we could do before the discussion period ends.\n\nRegards,\n\nPaper 7682 Authors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590370796,
                "cdate": 1700590370796,
                "tmdate": 1700590370796,
                "mdate": 1700590370796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cXVLn6kymZ",
            "forum": "icTZCUbtD6",
            "replyto": "icTZCUbtD6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_zsTV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_zsTV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework called H-CAT for evaluating HCM methods that measure how difficult it is for a classification model to learn from certain examples. The paper provides three possible reasons for an example to be 'hard':\n- The example is mislabeled\n- It is OOD or outlier\n- The example is valid but is atypical\n\nNow the methods that are used to estimate the any of hardness types above are called HCMs:\n- Learning dynamics-based\n- Gradient-based\n- Distance-based\n\nThe paper presents a codebase which allows to evaluate the hardness types as well as the HCMs. The main takeaways are:\n- Many HCMs disagree with each other.\n- Some types of hardness are characterize to measure than others.\n- Learning dynamics-based methods are generally more effective.\n- The best HCM to use depends on the specific problem."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a novel codebase for the systematic analysis of HCMs. The categorization done in section 2.1 is nice to read. It is great to define hardness more formally and identify different underlying reasons for the difficulty a classification model in learning from certain examples.\n\n- The paper have sufficient experiments. The authors evaluated 13 HCMs, providing a robust validation of their findings.\n\n- The takeaways are generally valuable. The authors offer practical advice on how to choose HCMs based on the type of data hardness, and they highlight the advantages of learning dynamics-based methods.\n\n- Overall, the paper is well-written. However, reorganizing the content and moving some parts to the appendix and placing back some key figures and explanations to the main text could improve its readability."
                },
                "weaknesses": {
                    "value": "- The paper could be structured better. Introducing and clearly defining \"hardness types\" and \"HCMs\" earlier would help readers understand the paper better. Some important explanations and figures that support the main concepts are in the appendix, which makes it difficult for readers to access them quickly and easily\n\n- The paper would be more technically rigorous if it included detailed equations and unified notation for the various HCMs in the appendix. Providing a more detailed explanation of methods, AUM for example, in the main text would also make the paper more instructive.\n\n- While the authors acknowledge certain limitations, two are particularly significant in my opinion:\n\n1. Before evaluating HCMs, you need to know what type of hardness is present in the data. This may not always be possible in real-world scenarios.\n2. The paper does not address the fact that hardness can change during the training process. A hard example at epoch 1 might not be hard at epoch 10."
                },
                "questions": {
                    "value": "I would like to know authors opinion regarding the limitations mentioned above, particularly 1) not knowing the type of hardness and 2) evolving hardness.\n\nTwo recommendations:\n- I would suggest to reorganize the content to ensure a logical flow of information, placing Figure 7, for example, and some details to the main body can help grasping key concepts.\n\n- Add detailed equations in a unified notation for the HCMs explored in section A.2.*. This would make your paper a go-to paper whenever someone wants to learn about hardness literature.\n\n--------------------\nPost rebuttal note:\nThank you very much for your response and applying the changes I suggested. I respectfully disagree with Reviewer 5UHg- the fact that the community's focus is now on LLMs doesn't make this type of works worthless. I believe that this paper has significant potential to advance our understanding of hardness in machine learning and hence I vote towards acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Reviewer_zsTV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698950321894,
            "cdate": 1698950321894,
            "tmdate": 1701051603183,
            "mdate": 1701051603183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sIalHzun0s",
                "forum": "icTZCUbtD6",
                "replyto": "cXVLn6kymZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zsTV [Part 1/2]"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer zsTV``\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following in turn, along with corresponding updates to the revised manuscript. We have grouped answers for ease of readability as follows:\n\n(A) Paper updates: Appendix content to main paper __[Part 2/2]__\n\n(B) Unified notation for HCMs __[Part 2/2]__\n\n(C) Clarification \u2014 knowing hardness a priori __[Part 2/2]__\n\n(D) Clarification \u2014 Hardness changing over training __[Part 2/2]__"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124677102,
                "cdate": 1700124677102,
                "tmdate": 1700124703524,
                "mdate": 1700124703524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rFm1iKAi54",
                "forum": "icTZCUbtD6",
                "replyto": "cXVLn6kymZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zsTV [Part 2/2]"
                    },
                    "comment": {
                        "value": "### (A) Paper updates: Appendix content to main paper\n\nWe thank the reviewer for the suggested paper updates of moving contents from the appendix to the main paper which we believe have improved the paper.\n\nWe have now moved Figure 7 (now Figure 3) into the main paper to readers' intuition of the differences between OOD and Atypical, as well as show that our perturbations are indeed realistic. \nIntroduce hardness types in the introduction further with intuitive examples\nBuilding on our introduction of HCMs in the intro, we move the grouping of the broad classes of HCMs (e.g. learning-based) from the Appendix into our formulation in Section 2.\n\n**UPDATE**: moved Figure 7 (now Figure 3) into the main paper\n\n---\n\n### (B) Unified notation for HCMs\n\n\nWe thank the reviewer for the suggestion of adding the equations and unified notation for the HCMs in the Appendix, to make it a go-to-paper on hardness literature. We describe this below and have **updated Appendix A** with the following changes.\n\n- Added the unified input-output formulation for any HCM. We have also formulated HCMs are defined by a scoring function $S$.\n- We mathematically define each scoring function under this unified formulation requiring a scoring function. \n- We have added a new Table 2 in the Appendix, formalizing the meaning of the scores per HCM.\n\n---\n\n### (C) Clarification \u2014 knowing hardness a priori\n\nWe wish to clarify the potential misunderstanding about the hardness type being known and make the distinction between the benchmark framework and HCMs. Our benchmark framework tests specific hardness types. This is vital for establishing a controlled benchmarking environment. Without such a ground truth, it would be challenging, if not impossible, to accurately assess and compare the capabilities of different HCMs. However, we emphasize that the HCM itself does not have knowledge of the hardness type when being evaluated. We apologize if this was unclear. \n\n---\n\n### (D) Clarification \u2014 Hardness changing over training\n\nThe inherent hardness of a sample remains consistent regardless of the training epoch. For instance, a sample that is hard due to mislabeling retains this characteristic throughout the training process. A mislabeled sample is indeed mislabeled at both epoch 1 and epoch 10. This constancy is crucial to our understanding and characterization of sample hardness. Our work on H-CAT then focuses on assessing the capability of the HCM scores to correctly identify these inherent hardnesses in the sample itself."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124748845,
                "cdate": 1700124748845,
                "tmdate": 1700175169446,
                "mdate": 1700175169446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ag71tp6RAk",
            "forum": "icTZCUbtD6",
            "replyto": "icTZCUbtD6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
            ],
            "content": {
                "summary": {
                    "value": "This paper did the following:\n- A taxonomy of hardness -- mislabel, OOD, atypical\n- Provide a library of modules for dataloading, hardness measurements across different types of hardness, unified interface and evaluator\n- Propose that hardness is ill-defined, indeed\n- Propose that hardness is usually being measured by downstream, which should not be\n- Give a list of takeaways at the end\n- Experimentd ran on cifar and mnist"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper dedicated a significant portion to describing why hardness is not well-defined anywhere. It also provides a taxonomy of hardness that are split into mislabeling, OOD and atypical. It also provides a unified interface for evaluation."
                },
                "weaknesses": {
                    "value": "The authors stated that hardness should not depend on downstream improvement. I find that to be very controversial. One model's hardness may be another model's easy. Maybe I have not understood this completely and would like clarification. Moreover, in the end you run classification on cifar and mnist to obtain a list of takeaways iiuc -- did the authors not just use downstream tasks (cifar and mnist classifications) to inform us of their takeaways? If I understood incorrectly, please provide clarification. Also the categorization of hardness into mislabel, ood and atypical is quite unsatisfying. Why mislabel? Also in this era of self-supervision, LLM autoregressive, etc., I find this paper a little not keeping up with time. What is hardness in this new day and age?\n\nExperiments on cifar and mnist only --- authors claimed that they did not select imagenet because imagenet has less than 5% mislabel. Can you not artificially mislabel yourself then?\n\nA large part of the paper reads like an engineering document. I appreciate that, but I think scientific insights are more important while the engineering information can be delegated to appendix."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959226262,
            "cdate": 1698959226262,
            "tmdate": 1699636934559,
            "mdate": 1699636934559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RZ679IHuCc",
                "forum": "icTZCUbtD6",
                "replyto": "Ag71tp6RAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5UHg [Part 1/3]"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer 5UHg``\n\nThank you for your review. We give answers to each of the following in turn, along with corresponding updates to the revised manuscript. We have grouped answers for ease of readability as follows:\n\n(A) Relevance of the paper to current ML trends __[Part 2/3]__\n\n(B) Clarifying the role of downstream improvement __[Part 2/3]__\n\n(C) Clarifying takeaways are not based on downstream improvement __[Part 2/3]__\n\n(D) Categorization of hardness types __[Part 3/3]__\n\n(E) Experiments on CIFAR and MNIST __[Part 3/3]__\n\n(F) Inclusion of technical details in the main manuscript __[Part 3/3]__"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123781836,
                "cdate": 1700123781836,
                "tmdate": 1700123781836,
                "mdate": 1700123781836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xESRW4RxMg",
                "forum": "icTZCUbtD6",
                "replyto": "Ag71tp6RAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5UHg [Part 2/3]"
                    },
                    "comment": {
                        "value": "### (A) Relevance of the paper to current ML trends\n\nWe respectfully disagree that a paper on hardness characterization is *\u201cnot keeping up with time\u201d*. To answer the Reviewer\u2019s question *\u201cWhat is hardness in this new day and age?\u201d*, as outlined in Section 1, data is a vital component of all machine learning methods and hence, methods for understanding data is critical. In our paper, we address data hardness, which is increasingly used to audit datasets. We examine hardness in both the input space $X$ and label space $Y$. Consequently, our work is relevant to many areas of ML, including multiple data modalities, learning paradigms, and model architectures. For instance, consider the following recent and high-profile LLM paper: Textbooks Are All You Need [R1], which shows the importance of identifying high-quality samples and filtering the dataset. This is the core premise of HCMs. In fact, hardness is even more relevant in current times as HCMs can help guide data filtering as datasets grow ever larger.\n\nAdditionally, the rise of data-centric AI (Liang et al., 2022; Seedat et al., 2022b) as a key ML research field highlights the importance of HCMs as a mechanism to enable data quality. The relevance and growing interest in this topic is shown by tutorials on data-centric AI at the following major conferences: KDD, IJCAI, and NeurIPS in 2023. Our work directly contributes to this increasingly important field, addressing fundamental challenges in understanding and improving data quality. \n\nMoreover, the HCMs we benchmark are from 2019-2023 underscoring their relevance to current times (see Table 1). Finally, we believe H-CAT also addresses recent calls for more rigorous benchmarking (Guyon, 2022) and for better understanding of existing ML methods (Lipton & Steinhardt, 2019; Snoek et al., 2018).\n\n---\n### (B) Clarifying the role of downstream improvement\n\nWe clarify that we did not state **\u201cthat hardness should not depend on downstream improvement\u201d**, rather we assert HCMs should be evaluated beyond __only__ their ability to curate a dataset to improve downstream performance. While downstream performance is undoubtedly significant, only looking at downstream performance \u201coverlooks the fundamental quantitative [hardness] identification task\u201d (see abstract and Sec 2.2). Hence, the key contribution of our work is to quantitatively evaluate the capabilities of different HCMs to identify different types of hardness in datasets. This represents the first work to assess HCMs on this fundamental task in a comprehensive manner. This is important as in order to advance the field of hardness characterization we need to understand the fundamental mechanisms behind HCMs and their strengths and weaknesses in handling different types of hardness.\n\n---\n### (C) Clarifying takeaways are not based on downstream improvement\n\nWe clarify that we do __not__ use downstream task performance to inform the takeaways. Rather we evaluate the fundamental hardness detection capability of HCMs on different hardness types, providing an objective and quantitative assessment of whether HCMs correctly detect/identify the hard samples they claim."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123890669,
                "cdate": 1700123890669,
                "tmdate": 1700123890669,
                "mdate": 1700123890669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yTOIW91tZh",
                "forum": "icTZCUbtD6",
                "replyto": "Ag71tp6RAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5UHg [Part 3/3]"
                    },
                    "comment": {
                        "value": "### (D) Categorization of hardness types\n\nWe clarify our categorization of hardness into mislabeling, OoD/outlier, and atypical is grounded in the existing body of ML literature which has highlighted these as prevalent and impactful types of data hardness in real-world data (see **Sec 2.1**). Mislabeling, in particular, is a pervasive issue in ML datasets (see Northcutt et al., 2021b) and poses a significant challenge to model performance in the real world. Please see the references used in our paper to motivate the categorization of hardness types in our taxonomy. \n\n - Mislabeling: (Paul et al., 2021; Swayamdipta et al., 2020; Pleiss et al., 2020; Toneva et al., 2019; Maini et al., 2022; Jiang et al., 2021; Mindermann et al., 2022; Baldock et al., 2021, Northcutt et al., 2021a; Sukhbaatar et al., 2015, Jia et al., 2022; Han et al., 2020; Hendrycks et al., 2018; Song et al., 2022)\n- Near OoD (Anirudh & Thiagarajan, 2023; Mu & Gilmer, 2019; Hendrycks & Dietterich, 2018; Graham et al., 2022; Yang et al., 2023; Tian et al., 2021)\n- Far OoD (Mukhoti et al., 2022; Winkens et al., 2020; Graham et al., 2022; Yang et al., 2023)\n- Atypical: (Yuksekgonul et al., 2023, Feldman, 2020; Hooker et al., 2019; Hooker, 2021; Agarwal et al., 2022)\n\nWe believe that our formal hardness taxonomy provides a structured framework for the ML community to engage with this critical but overlooked issue of the dimensions of hardness. Finally, while of course, other dimensions could be considered \u2014 our taxonomy is much broader than previously considered by HCMs and we hope the community will build upon it.\n\n---\n\n### (E) Experiments on CIFAR and MNIST\n\nWe first clarify the misunderstanding, we did not say **\u201cImageNet has less than 5% mislabel\u201d** but rather the opposite. On Page 7, we state **\u201cother common image datasets like ImageNet\u2026contain significant mislabeling (over 5%), hence we cannot perform controlled experiments\u201d** (see Sec 5 and Appendix B). The issue is not that we cannot artificially mislabel it, but that conducting controlled experiments is challenging due to substantial ground truth mislabeling. Consequently, this motivated our usage of MNIST and CIFAR-10; as mentioned in Sec 5, these datasets are clean without significant mislabeling (<0.5%). This allows us to create controlled experimental conditions for benchmarking where hardness, such as mislabeling, is introduced. \n\nAdditionally, we have conducted an additional assessment on a large-scale medical X-ray dataset from the NIH [R2], which has been audited and curated by multiple radiologists to ensure a high-quality and clean dataset for inclusion in our benchmark. We see that the major findings and takeaways are retained as shown in the updated Appendix D.7  for the X-ray dataset.\n\n**UPDATE:** Appendix D.7 with additional image experiment on a medical X-ray dataset\n\n---\n\n### (F) Inclusion of technical details in the main manuscript\n\nWithout the engineering aspects and technical details, such as the development of H-CAT, one cannot perform the rigorous evaluation of HCMs. We believe our takeaways and findings have led to both insights and understanding into HCMs and their strengths and weaknesses, which have never been evaluated before our framework. These insights are not secondary to the engineering details; rather they are interdependent. The technical and engineering details does not diminish the scientific rigor or the insights which we obtain through our work. On the contrary, it enhances its relevance and applicability. Moreover, we believe our formulation of the hardness taxonomy under a common framework is a key scientific contribution of the paper.\n\nAdditionally, the paper falls under the **ICLR call for papers of 'datasets and benchmarks'**. Hence, the benchmarking framework is a fundamental aspect of the paper. More specifically, the technical details of the benchmark are crucial to include in the paper to give the reader an understanding of how the insights in Section 5 are obtained.\n\n---\n### References\n[R1] Gunasekar, Suriya, et al. \"Textbooks Are All You Need.\" arXiv preprint arXiv:2306.11644 (2023).\n\n[R2] Majkowska, Anna, et al. \"Chest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation.\" Radiology 294.2 (2020): 421-431."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124024921,
                "cdate": 1700124024921,
                "tmdate": 1700124024921,
                "mdate": 1700124024921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ogX2rbIMWo",
                "forum": "icTZCUbtD6",
                "replyto": "Ag71tp6RAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
                ],
                "content": {
                    "comment": {
                        "value": "This is what you wrote verbatim \"Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task.\"\n\nI understand your point. I was asking more of this: it seems all your key takeaways that you listed are based off cifar/mnist dataset and downstream task. Hence I find it hard to draw conclusion supporting \"overlooking the fundamental quantitative identification task\" because your key takeaways are based mnist and cifar ...\n\nWhile I do appreciate the whole framework, but I am asking is it really useful in the fundamental sense which your paper is kind of claiming iiuc?\n\nRE: \"not keeping up with time\" -- let me put it this way, from your paper, I don't know how to characterize hardness for LLM, LMM. What is OOD in LMM/LLM?\n\nOn ImageNet: sorry, I misread. However, I am still concerned that the datasets used are very toy, very small. I would like to see some real world datasets."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626255790,
                "cdate": 1700626255790,
                "tmdate": 1700626427408,
                "mdate": 1700626427408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "66V3ZTuX21",
                "forum": "icTZCUbtD6",
                "replyto": "ogX2rbIMWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_5UHg"
                ],
                "content": {
                    "comment": {
                        "value": "Also, you did not answer my question (or at least I did not see it) on why categorize hardness into mislabel, ood and atypical. Do these three cover all the hardness in the world of data? \n\nYou also only show for images. For claiming being a fundamental hardness framework, what about other modalities? I am not trying to split hair, but my point is that it comes down to these three categories on images, and you create samples of them on mnist and cifar and run experiments on them, and finally make those takeaways. That's what it is. All the modules are engineering models (and I am not disputing their usefulness), but everything comes down to what I described. What is so fundamental about it?"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627582118,
                "cdate": 1700627582118,
                "tmdate": 1700627582118,
                "mdate": 1700627582118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zLTcipG4Dh",
            "forum": "icTZCUbtD6",
            "replyto": "icTZCUbtD6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_SFiV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7682/Reviewer_SFiV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a benchmark for evaluating methods that make supervised learning models robust to hard data examples. The paper introduces a taxonomy of hard cases and defines each category in terms of where the corruption happens and how the data distributions differ. The authors developed a toolkit to automatically evaluate models and include 13 algorithms in their study. The paper summarizes the results and presents the main highlights, together with an extensive supplementary material with more details."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The organization of the taxonomy is sound and formalizes observations from prior literature.\n* The systematic experimental design is strong and the evaluations are exhaustive.\n* The toolkit for evaluating and comparing performance across HCM seems well designed."
                },
                "weaknesses": {
                    "value": "* The conclusions of the study seem limited. Despite the extensive experimental work, it is unclear how the hardness characterization field can make progress. While the paper claims to have practical tips, there is the underlying assumption that the hardness type is known and coming from only one type. This is not realistic in practice and undermines the value of the recommendations for practitioners.\n* The paper insists that studying data hardness is critical for advancing data-centric AI, but it is unclear what data-centric AI is, and why HCMs are the key to advance it. A more fundamental question is, do we really need HCMs at all or we only need to create high quality datasets that eliminate corrupted examples?\n* Following the need for HCMs, a baseline of training models without HCMs is missing. There are two scenarios that should be considered as baselines: 1) training a model without HCM with the same level of corruption introduced as the other models. 2) Training a model without HCM and with only the easy examples, i.e. removing the portion of corrupted samples. This would be informative to evaluate what cases in the taxonomy really need to be investigated further with HCMs and which ones can be addressed by simply cleaning the dataset.\n* The taxonomy is manually defined based on the interpretation of previous literature, and the datasets used in the study are intentionally clean (MNIST and CIFAR) to simulate hard samples. Perhaps data hardness should be evaluated in a data-driven way, i.e., using a real world dataset and using data science, statistics, and machine learning to find out the nature of hard samples on the dataset. This reviewer finds the approach of defining the hardness of data artificial, and perhaps not reflecting the challenges of current problems and datasets.\n* The paper points to the use of indirect measures, such as improvement in downstream performance, as an issue (Sec. 2.2). However, it seems like the metric that they use to evaluate methods and create the benchmark is downstream performance in the corresponding classification task. It was not clear what the proposed solution to this issue is in the benchmark.\n* The writing style and paper formatting is too flashy and very distracting. Instead of having a natural flow, the paper has sentences in bold, boxes in colors, and other decorations that ask for too much attention, giving the impression that there is something more important to read than the argument the reader is focusing on. I found this style very hard to follow. In addition, the paper reads as if it was trying hard to sell a toolkit rather than presenting a study with solid conclusions."
                },
                "questions": {
                    "value": "Unfortunately, I don't find this study very compelling as a full research article. As it is currently, the paper seems to be more appropriate for a Datasets & Benchmarks track at NeurIPS, for instance, rather than bringing in novel insights to advance hardness characterization methodology. Maybe I'm missing what the most important conclusion is, which is what I'd like the authors to clarify."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7682/Reviewer_SFiV"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7682/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699033990176,
            "cdate": 1699033990176,
            "tmdate": 1700524865454,
            "mdate": 1700524865454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RFt5nnDc5E",
                "forum": "icTZCUbtD6",
                "replyto": "zLTcipG4Dh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SFiV [Part 1/3]"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer SFiV``\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following in turn, along with corresponding updates to the revised manuscript. We have grouped answers for ease of readability as follows:\n\n(A) Suitability for datasets & benchmarking __[Part 2/3]__\n\n(B) Conclusions of the paper & known hardness type __[Part 2/3]__\n\n(C) Clarification: What is data-centric AI & HCMs vs creating high quality datasets __[Part 3/3]__\n\n(D) Training models without HCMs __[Part 3/3]__\n\n(E) Evaluating data hardness __[Part 3/3]__\n\n(F) Indirect measures vs our use of direct measures __[Part 3/3]__\n\n(G) Writing updates __[Part 3/3]__"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124148288,
                "cdate": 1700124148288,
                "tmdate": 1700124148288,
                "mdate": 1700124148288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SizE16pvVq",
                "forum": "icTZCUbtD6",
                "replyto": "zLTcipG4Dh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SFiV [Part 2/3]"
                    },
                    "comment": {
                        "value": "### (A) Suitability for datasets & benchmarking\n\nWe thank the reviewer for acknowledging the suitability of our paper for a benchmarking track. We wish to highlight that this year the ICLR call for papers includes a track specifically for **datasets and benchmarks** \u2014 which we noted as the primary submission area for this work. With this in mind, our work responds to the recent calls for more rigorous benchmarking and understanding of existing ML approaches, as highlighted by Guyon (2022), Lipton & Steinhardt (2019), and Snoek et al. (2018). \n\nOur work provides four main contributions to the field: **(1) Hardness taxonomy:** we formalize a systematic taxonomy of sample-level hardness types; **(2) Benchmarking framework:** to evaluate the strengths of different HCMs across hardness types; **(3) Systematic and quantitative HCM evaluation:** prior to our study, no research had comprehensively evaluated HCMs across various hardness types nor directly evaluated HCMs' capabilities to correctly detect hardness \u2014 we do for 13 different HCMs across 8 different hardness types;** (4) Insights:** our benchmark provides novel insights into the capabilities of different HCMs when dealing with different hardness types, exposing gaps and opportunities in current HCMs and offering practical tips for researchers and practitioners. We hope the H-CAT framework will both add rigor to the HCM field and address the critical need for a structured and empirical approach to understanding and improving HCM methodologies.\n\nWe also highlight that similar analyses proposing a framework and benchmark have also previously been accepted by ICLR:\n - A Fine-Grained Analysis on Distribution Shift (Wiles et al., 2022)\n- BREEDS: Benchmarks for Subpopulation Shift (Santurkar et al., 2021)\n- MEDFAIR: Benchmarking Fairness for Medical Imaging (Zong et al., 2023)\n- Long Range Arena: A Benchmark for Efficient Transformers (Tay et al., 2021)\n- A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet (Galil et al., 2023)\n\n---\n### (B) Conclusions of the paper & known hardness type\n\nOur work enables the HCM field to rigorously evaluate for the first time, which is essential for the field to progress. We believe this will help the HCM field to move from the current ad hoc and qualitative state (as shown in **Table 1**) toward more comprehensive and rigorous evaluation across multiple hardness types. \n\nWe emphasize that our benchmark also offers critical insights into current HCMs, which we have distilled into **(1) benchmarking takeaways**: to guide the future development of HCMs by highlighting their strengths and weaknesses; and **(2) practical takeaways**: to guide selection and usage of HCMs. Overall, this work highlights what types of hardness different HCMs are able to detect and shows the limitations of HCMs in detecting certain hardness types, which we hope will inspire new methodologies to address these limitations.\n\nWe also wish to clarify the potential misunderstanding about the hardness type being known and make the distinction between the benchmark framework and HCMs. Our benchmark framework tests specific hardness types. This is vital for establishing a controlled benchmarking environment. Without such a ground truth, it would be challenging, if not impossible, to accurately assess and compare the capabilities of different HCMs. However, we emphasize that the HCM itself does not have knowledge of the hardness type when being evaluated. We apologize if this was unclear. \n\nFinally, regarding the concern of a single source of hardness. While in the main paper, we mainly focused on a single source of hardness at a time to disentangle effects, as mentioned in **Section 6**, we consider the case where multiple types of \u201chardness\u201d manifest simultaneously (e.g. Near-OoD and mislabeling together) and provide an example of simultaneous hardness in **Appendix D**. We hope this clarifies the Reviewer's concern."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124207914,
                "cdate": 1700124207914,
                "tmdate": 1700124207914,
                "mdate": 1700124207914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4uxvkVy6Ti",
                "forum": "icTZCUbtD6",
                "replyto": "zLTcipG4Dh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SFiV [Part 3/3]"
                    },
                    "comment": {
                        "value": "### (C) Clarification: What is data-centric AI & HCMs vs creating high-quality datasets\n\nWe thank the Reviewer for the opportunity to clarify the role of HCMs within data-centric AI. Data-centric AI focuses on systematic methods to enhance data quality \u2014 with hardness characterization being fundamental by identifying such hard examples in an algorithmic manner. We fully agree with the Reviewer we need to create high-quality datasets that eliminate corrupted examples \u2014 we emphasize that HCMs are one potential algorithmic tool to achieve this. Algorithmic identification is especially important when manual curation is impractical, for example at scale. Hence, we can think of HCMs being the algorithmic mechanism to achieve the data quality goals of data-centric AI. To illustrate the practical value and need for HCMs in creating high-quality datasets, we highlight that  (i) THE _Cleanlab HCM_ was used to audit ML benchmark datasets for mislabeling (Northcutt et al, 2021b), (ii) the _Data Maps HCM_ was used to audit and create high-quality RNA data in biology [R1], and (iii) the _Data-IQ HCM_ was used to audit and curate high-quality sleep staging data [R2].\n\n---\n\n### (D) Training models without HCMs\n\nWe wish to clarify the goal of this work is to evaluate the detection capabilities of various HCMs in identifying different types of data hardness, rather than assessing their curation impact on downstream model performance (e.g. suggested baseline without HCMs). The reason is previous research has already demonstrated the value of using HCMs to curate datasets to train downstream models. In contrast, our work complements the existing literature which already assesses the suggested baseline and extends previous work by providing a detailed analysis of how effectively different HCMs can identify various hardness types, which is a novel and necessary perspective for the field.\n\n---\n\n### (E) Evaluating data hardness.\n\nWe agree with the Reviewer that, in the real world, data hardness should be evaluated in a data-driven way (e.g. with data science and ML methods): this is precisely what an HCM does when used to identify and audit hard samples in a dataset. However, benchmarking and evaluating HCMs and their detection capabilities requires the creation of a controlled benchmarking environment. We note real datasets do not have ground truth of hardness; hence, to accurately assess the detection capabilities of HCMs, we need to establish a ground truth in our benchmark which we do via clean datasets (CIFAR and MNIST). We clarify this selection is a strategic decision to establish a clear baseline and understand the specific capabilities of each HCM under known conditions. Moreover, these datasets are widely adopted in the HCM literature. We contend that the use of such datasets doesn't detract from the study's relevance; instead, it ensures that the evaluation of HCMs is precise.\n\nWe also wish to clarify that indeed the hardness types defined in our framework are reflective of challenges and problems in real data. **Figure 7 (now moved to the main paper as Figure 3)** illustrates that the hardness types are representative of hardness types that could realistically occur in actual datasets. \n\n---\n\n### (F) Indirect measures vs our use of direct measures\n\nWe thank the reviewer for the opportunity to clear up this misunderstanding. By 'indirect,' we refer to the HCM literature only assessing changes in downstream performance rather than the actual hardness detection task \u2013- i.e. we don\u2019t know if HCMs do the task they claim to do. In contrast, we address the gap in the literature as shown in Table 1 and directly evaluate the capability of HCMs to detect specific hardness types. This approach allows us to understand which types of hardness each HCM can effectively identify, rather than merely assessing model improvement on a curated dataset, which might obscure the actual capabilities of different HCMs on different hardness types, offering valuable insights for the development and application of these methods.\n\n---\n\n### (G) Writing updates\n\nWe thank the reviewer for the suggestions to help improve the paper and its readability. We have made the following changes based on your suggestions.\n\n- Changed the citation color from blue to black to reduce the color on the page.\n- Reduced the in-text bolding, except to delineate sections.\n- Removed the color boxes in many places\n- We have also streamlined the description of the toolkit. This has allowed space for additional insights to be moved from the Appendix to the main paper\n\n\n\n---\n\n### References\n[R1] Nabi, Afshan, et al. \"Discovering misannotated lncRNAs using deep learning training dynamics.\" Bioinformatics 39.1 (2023): btac821.\n\n[R2] Heremans, Elisabeth RM, et al. \"U-PASS: an Uncertainty-guided deep learning Pipeline for Automated Sleep Staging.\" arXiv preprint arXiv:2306.04663 (2023)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124272148,
                "cdate": 1700124272148,
                "tmdate": 1700124272148,
                "mdate": 1700124272148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3sluLjbs5K",
                "forum": "icTZCUbtD6",
                "replyto": "RFt5nnDc5E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_SFiV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7682/Reviewer_SFiV"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications and revisions!"
                    },
                    "comment": {
                        "value": "I want to thank the authors for addressing my questions and recommendations. The paper is indeed valuable for advancing HCMs methodology.\n\nIn particular, thank you for bringing to my attention that \"datasets and benchmarks\" are part of the call for papers in ICLR. In addition, I appreciate that the authors have clarified that the performance metric is based on assessing the detection of hard samples, rather than downstream performance. Given that the metric is based on classification rates, it is easily confused with the main task on these datasets. I encourage the authors to give a distinctive name to the metric(s), to make it more clear and explicit for other readers and new comers to the HCM literature.\n\nOther aspects I appreciate is that the presentation style was updated, that the new experiments include a medical image dataset, and that the paper has been reorganized according to other feedback. The new version of the paper is great!"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7682/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524831328,
                "cdate": 1700524831328,
                "tmdate": 1700524831328,
                "mdate": 1700524831328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]