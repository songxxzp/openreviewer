[
    {
        "title": "ParFam - Symbolic Regression Based on Continuous Global Optimization"
    },
    {
        "review": {
            "id": "W8aO60lqQH",
            "forum": "5vXDQ65dzH",
            "replyto": "5vXDQ65dzH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_1Ff3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_1Ff3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a simple parametric method for symbolic regression, as well as a deep learning-based extension where the neural network is designed to constrain the set of learnable parameters."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Results on SRbench black-box problems: the Parfam method seems competitive\n- Clarity: the paper is well written and easy to follow"
                },
                "weaknesses": {
                    "value": "- Lack of novelty: the parametric approach is not particularly novel (it is used in existing methods such as EQLearner and FFX). The main trick enabling the competitive performance seems to rely a lot on manual crafting of the heuristics (Appendix A) and the extensive model parameter search (Appendix E). As for DL-parfam, it is not sufficiently validated, as detailed below.\n- Experimental validation: as acknowledged by the authors, the DL-parfam method is mainly in prototype stage right now. Are results of DL-parfam on Feynman problems not reported because they were not as good as the ones on synthetic data or because the authors did not have the time to test? In the first case, the authors should at least explain why the results aren\u2019t good (what is missing in the current state). In the second, it gives the paper an unfinished impression. In both cases, this section appears as a dealbreaker for a prestigious venue \u2014 results should be complete, otherwise the paper appears rushed."
                },
                "questions": {
                    "value": "\"Even though modern approaches are able to handle flexible data sets in high dimensions (Biggio et al., 2021; Kamienny et al., 2022), they fail to incorporate invariances in the function space, e.g., x + y and y + x are seen as different functions, as pointed out by Holt et al. (2023)\"\n\nI tend to disagree with the idea that this is the main limitation of modern approaches. In fact, modern methods easily learns these invariances, which can be seen by the fact that beam search typically reveals equivalent expressions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697635960181,
            "cdate": 1697635960181,
            "tmdate": 1699636590923,
            "mdate": 1699636590923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QLdYLEBr1p",
                "forum": "5vXDQ65dzH",
                "replyto": "W8aO60lqQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer 1Ff3 for their time and thoughtful comments.\n\n__Lack of novelty: the parametric approach is not particularly novel (it is used in existing methods such as EQLearner and FFX). The main trick enabling the competitive performance seems to rely a lot on manual crafting of the heuristics (Appendix A) and the extensive model parameter search (Appendix E).__\n\nThank you for raising this important topic. In our general response, we addressed the similarity to EQL and added a more in-depth discussion at the end of the introduction. \n\nFFX also follows the idea of translating SR to a continuous optimization problem using parametric functions. However, the main difference between FFX and ParFam is that FFX is limited to a parametric function which is linear in its parameters, to be able to use efficient regression techniques. This, however, limits the search space strongly, since they cannot model any coefficients inside of the base functions, i.e., functions like $\\sin(ax)$ are impossible. We decided to allow a more expressive parametric family for the cost of having a more complicated optimization problem. This difference in expressivity can also be seen in the results on the SRBench data sets. Since we agree that it is important to place ParFam in the field of SR and also show its differences with similar methods we added the following paragraph in the introduction regarding parametric SR models in general and FFX and SINDy specifically:\n\n> Most SR algorithms approach the problem by first searching for the analytic form of $f$ and then optimizing the resulting coefficients. In contrast, only a few algorithms follow the same idea as ParFam, to merge these steps into one by spanning the search space using an expressive parametric model and searching for sparse coefficients that simultaneously yield the analytical function and its coefficients. FFX (McConaghy, 2011) and SINDy (Brunton et al., 2016) utilize a model to span the search space which is linear in its parameters, to be able to apply efficient methods from linear regression to compute the coefficients. To increase the search space, they construct a large set of features by applying the base functions to the input variables. While these linear approaches enable fast processing in high dimensions, they are unable to model non-linear parameters within the base functions, restricting the search space to a predefined set of features.\n\nRegarding the question of what makes ParFam competitive: We do not believe that it is due to the manual crafting of heuristics and an extensive model parameter search. FFX is strongly limited by its expressivity since it only allows functions linear in the parameters, so manual crafting of heuristics and an extensive model parameter search would not be sufficient to reach competitive results on a complicated benchmark like SRBench. For our experiments with EQL on SRBench, we followed the general setting of Sahoo et al. [1] and performed also a model parameter search. However, due to the complicated optimization process, as discussed in our general response, the training time for each hyperparameter set is too high to cover multiple ones on SRBench, with their specified hardware settings (8h CPU times). We include experiments on a comparison between EQL and ParFam as soon as they are finished, see the request by Reviewer 5PDr. Furthermore, notice that the heuristics in Appendix A are mainly to avoid numerical issues, for instance, to avoid division by near zero terms and to avoid negative inputs to the square root.  \n\n__As for DL-parfam, it is not sufficiently validated__\n\nThank you for raising this important concern. We aimed to explain this in the general response and added more clarification in the paper. So, to answer the question of why the results on SRBench are currently not reported is because the method is at its current stage not able to handle the data sets given as inputs. This is due to the variability of the input dimension and the number of data points. We are thankful to Reviewer 1Ff3 for pointing this out and added a clarification of the prototype state of DL-ParFam at multiple points (see the main reply) and an explanation of why the SRBench data sets are out of reach for our prototype of DL-ParFam currently in Section 3.2:\n\n> Due to the prototype status of DL-ParFam, the ability to evaluate it on complex data sets, such as the Feynman dataset, is limited as the data to be processed is not sampled on the same grid. Therefore, we use synthetic data sets."
                    },
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514822803,
                "cdate": 1700514822803,
                "tmdate": 1700514870540,
                "mdate": 1700514870540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lbWUsZyJvT",
                "forum": "5vXDQ65dzH",
                "replyto": "KzfOxQpenG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5667/Reviewer_1Ff3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5667/Reviewer_1Ff3"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the response!\nI will maintain my score at the same level, because I believe that the contribution of Parfam is not sufficient to lead to publication alone, and the DL-Parfam method is still prototyped. But should the paper be rejected, I strongly encourage the authors to resubmit once DL-Parfam is improved!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563783979,
                "cdate": 1700563783979,
                "tmdate": 1700563783979,
                "mdate": 1700563783979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KGKvlQ9Mse",
            "forum": "5vXDQ65dzH",
            "replyto": "5vXDQ65dzH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_8mnU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_8mnU"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the task of symbolic regression that learns to discover the underlying expression from data. The authors make an important observation that the current expression is just a small fraction of the whole possible expression, so searching in this small family would be much easier than searching in the whole space. The author justifies the success of the proposed on several datasets and many baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is clearly written and the observation for the current symbolic regression dataset is interesting.\n- The experiment result is strong against a lot of baselines."
                },
                "weaknesses": {
                    "value": "- Figure 1 as well as the description of Equations 3 and 4 are very hard to understand. It is unclear how the observation in Equation 1 is actually implemented into an algorithm.\n- A deep understanding of the proposed family of symbolic expressions is needed. Since the observation is so strong, it eliminates a lot of \"impossible\" expressions and reduces the search space greatly. I hope the author could give some analysis on how much the reduction of the search space from all the possible expressions (of maximum length < 30) to the family of expressions described in Equation 1.\n- One important baseline is missing: Symbolic physics learner: Discovering governing equations via Monte Carlo tree search.\n- The basin-hopping algorithm is used to solve non-convex optimization problems. Is the structure of the symbolic family required to solve non-convex optimization instead of convex optimization? This is not justified. Also `scipy.optimize` has already offered the API for BFGS, basin-hopping, SHGO, Direct, dual annealing, and Differential evolution. The whole process of using these fancy optimizers is just changing these APIs in one line.\n\nHere is the link: https://docs.scipy.org/doc/scipy/reference/optimize.html"
                },
                "questions": {
                    "value": "1. A detailed description of the pipeline in Figure 1 is needed.\n2. Theoretically analysis of the observation of Equation 1 on the reduction of space of symbolic regression is needed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5667/Reviewer_8mnU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731322197,
            "cdate": 1698731322197,
            "tmdate": 1700908603379,
            "mdate": 1700908603379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZY38QPe08M",
                "forum": "5vXDQ65dzH",
                "replyto": "KGKvlQ9Mse",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer 8mnU for their time and thoughtful comments on our paper and a chance to address their concerns which we do in the following:\n\n__Figure 1 as well as the description of Equations 3 and 4 are very hard to understand. A detailed description of the pipeline in Figure 1 is needed__\n\nThank you for pointing this out. We added a more detailed description to Figure 1 in our paper.\n\nEquation 3 is the definition of the loss function of the neural network employed for DL-ParFam as the sum of the binary-cross entropy loss and Equation 4 is the standard definition of the [binary-cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html). Please let us know if there is still a problem regarding the clarity of these.\n\n__It is unclear how the observation in Equation 1 is actually implemented into an algorithm.__\n\nEquation 1 defines the structure of the parametric function $f_\\theta$ and the goal of ParFam is to find sparse parameters $\\theta$ such that $f_\\theta(x_i)\\approx y_i$. This is done by minimizing the loss function defined in Equation 2.\n\n__I hope the author could give some analysis on how much the reduction of the search space from all the possible expressions (of maximum length < 30) to the family of expressions described in Equation 1. Theoretically analysis of the observation of Equation 1 on the reduction of space of symbolic regression is needed.__\n\nThank you for raising this very interesting question. ParFam can represent any function that can be represented by an expression such that each path from the root to the leaf contains at most one unary base function (e.g., $\\sin$, $\\cos$, $\\exp$, $\\sqrt{}$, etc.). We are working on determining the exact ratio of functions covered by ParFam currently and will provide further updates on this the next days since we agree that this is an insightful theoretical analysis. However, we argue that this should not be taken solely as a measure of expressivity since this would give the same importance to the formula $\\sin(\\sin(...(\\sin(x_1))))$ as to $x_1+x_2$ where the latter one is more common in the real world and more interpretable. The focus of ParFam was to focus on the functions showing these properties: interpretable and common in the real world, see also our general response. Therefore, keeping this measure small restricts ParFam on the one hand, however, also strengthens it since the search space is strongly reduced, biased towards interpretable and common formulas. \n\nApart from the question how many formulas can be exactly represented by ParFam, one might also be interested in its approximative capabilities. ParFam can approximate any continuous function on a compact set which follows directly from the [Stone-Weierstrass Theorem](https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem). I hope we could address your concern reasonably well.\n\n__One important baseline is missing: Symbolic physics learner: Discovering governing equations via Monte Carlo tree search.__\n\nThank you for pointing us to this very interesting work. Unfortunately, they do neither report their performance on SRBench and nor make their code available such that we can perform these experiments on our own. To be able to compare ParFam to SPL we have to use the benchmarks used in their paper. The only standard benchmark they used is the Nguyen benchmark with and without constants, for which we will report our results as soon as they are finished in our supplementary material. \n\n__The basin-hopping algorithm is used to solve non-convex optimization problems. Is the structure of the symbolic family required to solve non-convex optimization instead of convex optimization? This is not justified.__\n\nSince some of the unary-base functions (e.g., $\\sin, \\cos, \\sqrt{}$) are non-convex and rational functions are not necessarily convex in their parameters, the loss function of ParFam can be expected to be non-convex for most model parameter choices and target functions. E.g., approximating $\\cos(ax)$ with $f_\\theta(x)=\\theta_1x+cos(\\theta_2x)+\\theta_3$ results in a highly non-convex loss function. This can also be seen by the difference in performance between the optimizers in Section B, since a convex problem would have been solved by all of those using gradient information.\n\n__Also `scipy.optimize` has already offered the API for BFGS, basin-hopping, SHGO, Direct, dual annealing, and Differential evolution. The whole process of using these fancy optimizers is just changing these APIs in one line.__\n\nYes, the goal of Appendix B was to give a comparison of different global optimizers, to show that the choice of basin hopping is optimal but not necessary for ParFam to work well and that simple heuristics like BFGS with multi-start work reasonably well themselves.\n\nWe thank Reviewer 8mnU again and hope we could clarify their concerns!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514774985,
                "cdate": 1700514774985,
                "tmdate": 1700514774985,
                "mdate": 1700514774985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xHjMh4Lffj",
            "forum": "5vXDQ65dzH",
            "replyto": "5vXDQ65dzH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_LMH9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_LMH9"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes ParFam, a simple regression method with a fixed and predefined structure, to tackle the symbolic regression problem. In ParFam, the function expression structure is directly specified by the user in advance, and then the coefficients are learned with a sparsity regularization from the observed data. In this way, the original symbolic regression problem can be reduced to a continuous optimization problem with respect to the coefficients.\n\nBased on the ground-truth problems from SRBench and the knowledge from the Cambridge Handbook of Physics Formulation, this work proposes a reasonable parametric expression structure to represent the physical formulas. Then, it uses a global continuous optimization method (basin-hopping algorithm) to find the optimal coefficients of the predefined expression. Experimental results show that ParFam can achieve promising performance on the symbolic regression problem for physics formulas."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Symbolic regression (SR) is an important but difficult problem that can be found in various domains. The proposed ParFam method can achieve promising performance on SR for physics formulas in a straightforward way."
                },
                "weaknesses": {
                    "value": "Although I enjoy reading this paper and appreciate the explicit discussion on the limitations, I have some major concerns about ParFam.\n\n**1. Is It still Symbolic Regression?**\n\nTo my understanding, symbolic regression is a learning-based approach to find the mathematical expression of a function from the observed data, which includes two important components:\n\n- Learn the analytical function structure;\n\n- Optimize the coefficients (parameters) of the structure;\n\nThe former is unique for symbolic regression, which distinguishes it from the other regression problems. Symbolic regression is difficult and is currently shown to be HP-hard with formal proof [1]. I think this is the reason why an efficient (approximate) SR algorithm will be \"usually quite complicated\" as described in this work.   \n\nIn ParFam, however, the analytical structure learning step is totally bypassed with a predefined function structure. The original problem is hence reduced to sparse regression with a fixed structure, and the only goal is to find the optimal coefficients. Is it still symbolic regression?  \n\n**2. Strong Prior Knowledge on Physics are Required**\n\nTo achieve promising performance on SR problems for physics formulas, ParFam requires prior knowledge of all possible physics formulas, as from SRBench and the Cambridge Handbook of Physics Formulation. I think this prior knowledge is very strong and only specific to physics formulas, and is hard to be generalized for other SR problems in real-world applications.  \n\n**3. DL-ParFam**\n\nThe idea of DL-ParFam, a deep learning-based pretrain model for ParFam, is interesting. But it is currently more like a toy prototype, and only tested on very simple synthetic problems. To truly show the advantage of DL-ParFam over other pre-training-based SR methods, a concrete model design on real-world SR applications is required. \n\nIn DL-ParFam, the model only takes the function value y as input to predict the mask c for all parameters, and all information of the function input x is completely ignored. It is hard to believe this approach can provide a reasonably good prediction for real-world SR applications, especially those with complicated structures. \n\nTo build the pre-trained model, DL-ParFam requires the input data x to have the same dimension $m$, and the data should be sampled on the same grid across all different data sets. Can this requirement be easily satisfied for physical SR problems and other SR problems?\n\n**4. Experiments**\n\nSince ParFam has a strong prior knowledge of the physical formulas, it is expected it can have promising performance on the physics SR problems. Indeed, according to the results, ParFam even discards part of the observed data, and only requires a subset of 500-1000 data points for coefficient optimization. It is hard to imagine this procedure could work well for real-world SR problems.\n\nDL-ParFam is only tested on very simple synthetic problems. It is hard to judge its potential for solving real-world application problems with complicated structures."
                },
                "questions": {
                    "value": "Please see the weaknesses section. I am willing to adjust my rating if the issues in weaknesses are well addressed.\n\n[1] Symbolic Regression is NP-hard. TMLR 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5667/Reviewer_LMH9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759558730,
            "cdate": 1698759558730,
            "tmdate": 1700567393536,
            "mdate": 1700567393536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QF0N62wLNX",
                "forum": "5vXDQ65dzH",
                "replyto": "xHjMh4Lffj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer LMH9 for their time and thoughtful comments on our paper and a chance to address their concerns. \nIn the general response, we already addressed your questions and concerns regarding the question of whether ParFam is still symbolic regression, the physical prior knowledge, and the limitations of DL-ParFam. Hence, we only comment shortly on these here. Please also note the related adaptions to our paper that we mentioned in the general response. \n\n__Is It still Symbolic Regression?__\n\nAs argued in the general response, from our point of view, ParFam is still an SR method and just combines the two steps of learning the function structure and optimizing the coefficients in the global search method. \n\n\n__Strong Prior Knowledge on Physics are Required__\n\nWe agree that the interest of SR is not limited to physics and, thus, the scope of SR algorithms should also cover other fields. As argued in the general response, the chosen parametric family covers also a lot of forumlas from other areas. Moreover, ParFam is highly expressive and can hence approximate many other formulas well as can be seen by the achieved high accuracy. \n\n__DL-ParFam__\n\nWe completely agree that it is not reasonable to omit the input data $x$ completely and to assume that all the data has the same size and dimension and is sampled on the same grid. This was a simplifying assumption for this paper to present the potential achievable by DL-ParFam. We hope to clarify the role of DL-ParFam in our general response. \n\n__Experiments__\n\nIt is an interesting question if the strong physics prior is the reason why we were able to discard such a large amount of the data for the Feynman problems.  \nHowever, note that also SRBench discards a lot of the data, since the original Feynman data set was extremely big ($10^5$) for these low dimensional problems (less than 10 dimensions), probably because the authors of the Feynman data set, Udrescu and Tegmark [1], had to train NNs on the data to test multiple symmetries. Furthermore, the data set size we used is closer to the usual data set size used in symbolic regression: Strogatz data set (400 data points), Nguyen (20 - 100 data points), Livermore (20-1000), R-Rationals (20). Note that interestingly we observed now when comparing ParFam to \"Symbolic physics learner: Discovering governing equations via Monte Carlo tree search\" [2], see the reply to Reviewer 8mnU, that ParFam has not enough prior to find the desired function on the small standard ranges as specified in the Nguyen data set. Instead, it finds other simple functions which are a near-perfect approximation on the data domain, indicating that the prior in ParFam might not be stronger than in other methods. An example for this is the target function $\\sin(x^2)\\cos(x)-1$ on the domain $[-1,1]$, where it instead finds the function $0.569x^2 - 0.742 \\sin(1.241x^2 - 2.059) - 1.655$ which is almost indistinguishable between $[-1,1]$.\n\n\nWe thank Reviewer 5PDr again and hope that we were able to address the concerns appropriately.\n\n[1] Silviu-Marian Udrescu and Max Tegmark. AI Feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020. doi: doi:10.1126/sciadv.aay2631.\n\n[2] Sun F, Liu Y, Wang JX, Sun H. Symbolic physics learner: Discovering governing equations via monte carlo tree search. ICLR. 2023. URL https://openreview.net/forum?id=ZTK3SefE8_Z."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514729041,
                "cdate": 1700514729041,
                "tmdate": 1700514729041,
                "mdate": 1700514729041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b3bovwyvcu",
                "forum": "5vXDQ65dzH",
                "replyto": "QF0N62wLNX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5667/Reviewer_LMH9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5667/Reviewer_LMH9"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the thorough response"
                    },
                    "comment": {
                        "value": "Thank you for the thorough response. I've also read other reviewers' comments and the corresponding responses. Since some of my concerns have been appropriately addressed, I raise my score to 5.\n\nHowever, I also believe the contribution of ParFam alone is not enough for a clear acceptance, while a well-developed DL-ParFam could significantly strengthen the quality of the current work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567372734,
                "cdate": 1700567372734,
                "tmdate": 1700567372734,
                "mdate": 1700567372734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "720C0pAxOs",
            "forum": "5vXDQ65dzH",
            "replyto": "5vXDQ65dzH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_5PDr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5667/Reviewer_5PDr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for symbolic regression in which the purpose is to search for a mathematical formula describing given data. The proposed method, termed ParFam, defines a structure of the target equations in advance and then optimizes the coefficients using gradient-based methods. Owing to this problem transformation, the symbolic regression problem becomes a contiguous problem from a discrete one. In addition, the technique that combines ParFam and the neural network-based structure prediction of the sparsity of coefficients is introduced. The authors experimentally evaluate the performance of ParFam using SRBench and show that it can achieve state-of-the-art performance compared to other symbolic regression methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A novel method for symbolic regression is presented, which translates the original discrete combinatorial optimization problem into the continuous optimization problem with a pre-defined structure of equations.\n- The proposed ParFam achieves state-of-the-art performance on SRBench.\n- This paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- When the number of the input variables of the equation increases, it seems hard for ParFam to handle the exponential growth of the number of parameters, as the authors describe in Section 4.\n- As the authors stated in the introduction, symbolic regression aims to find a symbolic model with as few assumptions as possible. However, in ParFam, the form (structure) of the target equations is pre-defined by users. If the structure of the equation is not suitable for a given data, it cannot represent an appropriate equation.\n- The experimental evaluation of DL-ParFam is limited to the synthetic datasets."
                },
                "questions": {
                    "value": "- The concept of the proposed ParFam is somewhat similar to equation learner (EQL). Given an appropriate network architecture that corresponds to the equation structure of ParFam, the search space of EQL could be almost the same as ParFam. Could you describe the main difference and advantage of ParFam against EQL? Also, is there any experimental comparison of EQL and ParFam?\n- Why is it difficult to apply and evaluate the DL-ParFam to SRBench?\n- How is the sensitivity of the performance of ParFam for the regularization hyperparameter $\\lambda$?\n- Could you report the exact number of parameters to be optimized in ParFam?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698955878628,
            "cdate": 1698955878628,
            "tmdate": 1699636590614,
            "mdate": 1699636590614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D9jxyJe2FE",
                "forum": "5vXDQ65dzH",
                "replyto": "720C0pAxOs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer 5PDr for their time and thoughtful comments on our paper. In the general response, we already addressed your questions and concerns regarding the expressivity of the parametric family, the limited experimental evaluation of DL-ParFam, and the similarity of EQL and ParFam. Please also note the related adaptions to our paper that we mentioned in the general response.\nTo answer the question shortly, why the prototype version of DL-ParFam cannot be used on the SRBench data set: These data sets require the pre-trained neural network to handle varying data set sizes and dimensions, which the simple prototype using a fully connected neural network is not able to. Thank you for pointing this out, we added this explanation at the beginning of Section 3.2: \n\n> Due to the prototype status of DL-ParFam, the ability to evaluate it on complex data sets, such as the Feynman dataset, is limited as the data to be processed is not sampled on the same grid. Therefore, we use synthetic data sets.\n\nRegarding the experiments with EQL, we will update our paper soon to show these results as well, thank you again for the idea to incorporate this. Regarding the exponential growth of the number of parameters, we agree that is a serious limitation, which is one of our main motivations to focus on DL-ParFam to reduce the number of parameters in the future. \n\n__How is the sensitivity of the performance of ParFam for the regularization hyperparameter $\\lambda$?__\n\nThank you for this very interesting question. Note that the role of $\\lambda$ can be well understood, if the MSE in the loss function in Equation (2) is scaled by the norm of $y$, i.e., if we consider the relative error, which is done in our implementation.\n\nIn this case, $\\lambda$ depicts a quantifiable relation between good accuracy and sparse/small coefficients. This explains why $\\lambda$ should probably be chosen to be less than 0.1 since otherwise there is a high probability that a model with very small coefficients exists, which has a lower loss than the true model. \nWe occasionally also observed this for 0.01 in toy experiments while debugging ParFam and found that $\\lambda = 0.001$ worked the best quite reliably. For this reason, we started the experiments on SRBench directly with that value. However, we think that a sensitivity analysis of this hyper-parameter would be a nice addition to our paper, which is why we ran additional experiments and added the results to the supplementary material, Appendix H. Interestingly, this shows that the performance of ParFam is not very sensitive to $\\lambda$.\n\n__Could you report the exact number of parameters to be optimized in ParFam?__\n\nWe only state the approximate number \"a few hundred\" in the numerical section, since the number of parameters depends strongly on the number of input variables and polynomial degrees. For example, if there are 3 input variables the maximal model parameter choices for the SRBench experiments yield a parametric family with 102 parameters, while if there are 5 input variables this results in a family with 296 parameters, and for 7 input variables 671 parameters. However, as described in Appendix E, there are multiple different model parameters, we iterate through, resulting in different parametric families with different numbers of parameters for each. We hope that this gives an idea of the number of parameters optimized for in ParFam.\n\n\nWe again thank Reviewer 5PDr for their time and hope that we could answer their questions and concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514668881,
                "cdate": 1700514668881,
                "tmdate": 1700514668881,
                "mdate": 1700514668881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]