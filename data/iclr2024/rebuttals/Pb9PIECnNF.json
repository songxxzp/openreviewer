[
    {
        "title": "Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?"
    },
    {
        "review": {
            "id": "x2jXLqO4Ne",
            "forum": "Pb9PIECnNF",
            "replyto": "Pb9PIECnNF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission884/Reviewer_cmE8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission884/Reviewer_cmE8"
            ],
            "content": {
                "summary": {
                    "value": "This work presents an empirical analysis of the detection performance of existing detectors and pre-training algorithms on a kind \nof out-of-distribution data entitled as pre-trained OOD (PT-OOD). PT-OOD are samples from the distribution used to pre-train \nthe model that is different from the distribution used to finetune the model. \nThey run experiments with supervised and self-supervised\nlearning algorithms to pre-train a backbone on a large dataset (ImageNet-1K) and finetune it with a linear layer head (linear probe)\nwith the backbone frozen on a smaller dataset. They found out that, even though feature-based methods achieve stellar \nperformance distinguishing PT-OOD data from in-distribution data (ID), methods based on the outputs of the finetuned model\nsuffer from detecting PT-OOD."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They tackle an original question of whether the source data used for pre-training a model that does not overlap with the\ntarget data the model was adapted to can be easily detected by showcasing empirical results with a couple of backbone architectures\nand a few learning algorithms.\n\nTo the best of my knowledge, they are the first to uncover that using simply the features of a model\nwith a simple method, such as kNN density estimation, one can almost perfectly distinguish PT-OOD from ID data. They also showcase \nlimitations on the MSP for such a task and other detectors that rely on the logits or the decision boundaries between ID classes\nto make a prediction."
                },
                "weaknesses": {
                    "value": "This paper tries to motivate the problem with web-scale models but conducts supervised pre-training on ImageNet-1K only. When actually using web-scale datasets, supervised pre-training becomes unfeasible.\n\nThis does not invalidate the results found but hinders the clarity and true contribution of the manuscript.\nExperiments on Dino v2 trained on a web-scale dataset would be appreciated, for instance. The model weights are easily available online.\n\nFigure 2 is a conceptual drawing. It would be nice to see a data-driven approach to support the claims of the authors\nby, for instance, comparing the decision boundaries of a linear probe for supervised and self-supervised pre-training on a \nbinary classification task.\n\nI disagree with the claim on page 3: \n\n> since a large amount of pre-training data is scraped from the web, it is more likely for PT-OOD to come as input.\n\nI reckon that the standard use case for classification-as-a-service applications is users inputting their own originally acquired data into the system, not necessarily scraped from the web. \n\nSome claims are too strong or are unsupported by sufficient evidence throughout the paper."
                },
                "questions": {
                    "value": "1. What can be changed on logits/softmax based approaches to improve their performance on PT-OOD detection? The authors explores\nmainly methods that do not have a reference from the ID dsitribution to perform density estimation similar to kNN. I suggest authors\ntry to perform experiemnts with kNN on the logits and investigate methods such as KL-Matching [2] or Igeood [3] that compares test samples to training prototypes.\n2. Would the same conclusions be observed by running on self-supervised experiments (e.g., DinoV2 [1]) trained on a web-scale dataset?\n3. Does a data-driven approach of Figure 2 show the same as what is conceptualized by the authors? \n\nI believe that suggesting improvements for logits/softmax-based methods (Question 1) or explaining their failure by backing the authors'\nintuition with data (Question 3) could improve the paper and change my opinion favorably.\n\nReferences:\n\n[1] Oquab et al. \"DINOv2: Learning Robust Visual Features Without Supervision.\" CVPR 2023. /abs/2304.07193.\\\n[2] Hendrycks et al. \"Scaling Out-of-Distribution Detection for Real-World Settings.\" ICML 2022. /abs/1911.11132\\\n[3] Gomes et al. \"Igeood: An Information Geometry Approach to Out-of-Distribution Detection.\" ICLR 2022. /abs/2203.07798"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Reviewer_cmE8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698398155145,
            "cdate": 1698398155145,
            "tmdate": 1699636015165,
            "mdate": 1699636015165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "H8xJUlamY6",
            "forum": "Pb9PIECnNF",
            "replyto": "Pb9PIECnNF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission884/Reviewer_PtvE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission884/Reviewer_PtvE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to study the OOD sample detection for pre-trained models where some OOD data may be in the pretraining dataset (PT-OOD). It is observed that the detection performance for self-supervised pretrained model is worse than supervised pretrained model. The paper propose to use k-NN to detect PT-OOD sample."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem setting of detecting PT-OOD samples is interesting and might be valuable for future application of large pre-trained models."
                },
                "weaknesses": {
                    "value": "1). The analysis lacks support. The paper report that OOD detection methods perform better on supervised pretrained model than on self-supervised pretrained model. The analysis in this paper says that it is because the features of models under supervised pretraining are linear seperable while the features of self-supervised trained models are not. Except the illustration in Fig.2, I have not found any theoretical or empirical evidence to support this analysis.\n\n2). The proposed method lacks novelty and contradicts to the analysis in section 4. This paper proposes to apply kNN on features to detect PT-OOD samples. While applying clustering method on features to classify data sample is a conventional way in feature laerning [1], the proposed method lacks novelty. Furthermore, it contradicts to the analysis in section 4, stating the PT-OOD features scatter among ID features.\n\n3). Some part of the paper is confusing. For example, in the last paragraph in the introduction, it first says \" We can utilize instance-by-instance discriminative features to separate ID and OOD, which require ID boundaries\" and in the next phrase, it says \"without using ID decision boundaries\", which is confusing. \n\n[1] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In ICLR 2020."
                },
                "questions": {
                    "value": "I have major concerns over the analysis in this paper. \n\n1). Why whould the features from self-supervised model be less linear seperable than supervised model? By tuning the last linear layer, self-supervised models achieve similar or better performance than supervised models, it is not obvious why less linear seperable is the reason for less robustness in detecting PT-OOD samples.\n\n2). If the analysis in this paper is true, why would kNN be an effective method to detect PT-OOD sample when \"PT-OOD can scatter in the feature space\"? \n\nTherefore, I think the paper requires a more in-depth analysis of the PT-OOD problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Reviewer_PtvE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571665789,
            "cdate": 1698571665789,
            "tmdate": 1699636015081,
            "mdate": 1699636015081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "bv3DDRLjjM",
            "forum": "Pb9PIECnNF",
            "replyto": "Pb9PIECnNF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission884/Reviewer_rG3x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission884/Reviewer_rG3x"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the impact of pretraining for OOD detection. Its main contributions are (1) Supervised pretraining is better than self-supervised pretraining for the downstream OOD detection task. (2) kNN is more effective\u00a0than maximum softmax probability for OOD detection when the model is a pretrained one."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. On CIFAR and ImageNet-1k datasets, the experiment scope is extensive.\n2. The paper is clearly written."
                },
                "weaknesses": {
                    "value": "1. The impact of post-processors (the scoring function) is huge, but the authors did not explore the state-of-the-art scoring functions such as ViM [1] and NNGuide [2]\n2. It is not clear what exactly is meant by 'instance-by-instance' discriminative representation.\n3. The experiments with models pretrained on ImageNet-21/22K are not available.\n4. One of the main observations (i.e., supervised > self-supervised) is too similar to the observation noted in [3]\n\n[1] Wang, Haoqi, et al. \"Vim: Out-of-distribution with virtual-logit matching.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n[2] Park, Jaewoo, Yoon Gyo Jung, and Andrew Beng Jin Teoh. \"Nearest Neighbor Guidance for Out-of-Distribution Detection.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[3] How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?"
                },
                "questions": {
                    "value": "Please address the weaknesses mentioned in the above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission884/Reviewer_rG3x"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852893518,
            "cdate": 1698852893518,
            "tmdate": 1699636015002,
            "mdate": 1699636015002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "a7irwGYgcr",
            "forum": "Pb9PIECnNF",
            "replyto": "Pb9PIECnNF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission884/Reviewer_wQPP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission884/Reviewer_wQPP"
            ],
            "content": {
                "summary": {
                    "value": "This work considers a class of OOD samples elicited by the use of pre-trained models in OOD Detection: pre-Trained OOD, referring to OOD samples memorized by pre-trained models. The authors explored the performance of PT-OOD detection under different pre-training algorithms through a number of experiments, and the results showed that the low linear separability of PT-OOD severely degraded the PT-OOD detection performance. They further proposed a solution based on a large-scale pre-trained feature extractor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The PT-OOD detection problem induced by the use of pre-trained models is interesting.\n2. Extensive experimental analyses were conducted.\n3. The influence of self-supervised and supervised pre-training are investigated."
                },
                "weaknesses": {
                    "value": "1. The definition of PT-OOD is very ambiguous.\n2. The article gives a weak motivation for the study and fails to see why targeted testing for PT-OOD is important.\n3.  In the experiments, the pre-training is not large-scale."
                },
                "questions": {
                    "value": "Q1. Dose the downstream task model have very poor prediction performance on PT-OOD samples? What is the importance of targeting PT-OOD as opposed to detecting OOD samples?\n\nQ2. What is the relationship between pre-training data and ID data for downstream detection tasks? When doing PT-OOD detection, is the groundtruth result AUROC=1 or FPR=0?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission884/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938103087,
            "cdate": 1698938103087,
            "tmdate": 1699636014926,
            "mdate": 1699636014926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]