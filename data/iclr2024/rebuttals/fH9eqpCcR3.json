[
    {
        "title": "Multiple Physics Pretraining for Physical Surrogate Models"
    },
    {
        "review": {
            "id": "NWdvtdtbwX",
            "forum": "fH9eqpCcR3",
            "replyto": "fH9eqpCcR3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_iLVr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_iLVr"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a pretraining strategy for autoregressive modeling of physical systems. It proposes a model architecture and training regime to handle the heterogeneity of training data. Experiments on the PDEBench dataset demonstrate the ability of the method to model different systems with state of the art accuracy. The authors further investigate the added value of the pretrained model in low-data settings and parameter-estimation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength 1: The authors experimentally validate the hypothesis that a single model can learn on diverse fluid mechanics systems, which is a key step for the development of general \u201cfoundation\u201d models for physics tasks. This line of experiments is well explained and rigorously reported. The use of the PDE-Bench dataset tasks and baseline models will ease all future work of the community to build and improve on the proposed approach. \n\nStrength 2: The architecture is novel and constitutes a first usage of transformer-based architectures for autoregressive modeling of physics systems. Given the exceptional scaling abilities of these models and their results in other scientific domains (chemical physics, biology), it is a promising direction for autoregressive tasks. Reporting the performance of the proposed architecture on PDEBench tasks, without multi-physics training, would already be valuable to the community. \n\nStrength 3 : The paper is easy to read and the questions it aims at answering are clearly formulated. Design choices and training regime are well motivated by the specificities of multi-systems learning, and clearly documented. This will ease the ramp-up of the community on pre-training tasks, and improve on the proposed architectures by proposing alternate design choices.\n\nStrength 4 : Figures and tables are clear, metrics and units are clearly reported and consistent with previous publications when taken from there"
                },
                "weaknesses": {
                    "value": "Weakness 1: The authors frame their question on the low-data regime (Section 5.2) as \u201cDoes MPP provide a fine-tuning advantage over existing spatiotemporal foundation models for new autoregressive prediction tasks?\u201d. Their experimental results indeed support the claim that MMP is superior. However I believe that in order to develop useful foundation models, such models should be compared to the \u201cbest one can do\u201d task-specific model in the low-data settings. Monitoring the progress on tasks that are too hard for task-specific baselines (like those implemented in the PDEBench paper) will provide rich insights to the community. The first line of experiments (Section 5.1) does not enable this, as all tasks are used at training time, including the test task. \nA possible experiment to add to section 5.2 would consist in training one of the PDEBench baseline models on the two low-data tasks\n\nWeakness 2: The model architecture is completely novel and more experimental validation should be done to maximize the value for the community. Specifically, to separate the added-value of the transformer architecture from the added value of the multi-systems training, it could have been interesting to train the MMP architecture on each of the PDEBench tasks individually, and include it in Table 1 as a new single-task baseline. It is also quite common to include ablation studies to validate intuitions or design choices, and the claim that positional encodings help to learn boundary conditions (Section 4.2, last paragraph) should be validated either by such experiment, or by a proof in the supplementary material. \n\nWeakness 3: The conclusion reminds the motivation behind multi-systems pretraining, but does not clearly recapitulate which questions have been answered and what is left to future studies. The two paragraphs are not well articulated and I feel would benefit from a rework. Ideally the reader should finish the paper with a clear view of the unanswered questions or limitations to be addressed."
                },
                "questions": {
                    "value": "The paper takes first relevant steps towards training general models to model physical systems. I believe the authors have shown that their method of pretraining is functional, but more thorough experiments on the model architecture and the application to low-data tasks would be beneficial to the community and greatly increase the interest of the paper :\n\n1) Put the low-data results (Section 5.2) in perspective with task-specific baseline performances : are these tasks way too hard for the PDEBench baselines ? Ideally, report the performance of one task-specific baseline in Figure 5. \n\n2) Rework the conclusion to simply recapitulate the questions answered and which questions should be explored in future studies.\n\n3) In section 4.2 last paragraph, clarify whether the claim on boundary conditions is justified theoretically, validated by an ablation study, or both. Ideally, add relevant information in the supplementary material.\n\nMinor comments to improve readability :\n\na) Code clarity : Papers that propose pretrained models should be as easy as possible to finetune, so that the community can make quick progress towards more efficient pretraining approaches on challenging downstream tasks. I would recommend rewriting the train.py file to remove a lot of boilerplate code and make it easier to reuse and tweak. \n\nb) Clarity of Figure 2 and its legend can be improved: ReVIN abbreviation for the normalization layer is not defined. \u201cphysics metadata\u201d is not defined neither in text nor in figure legend. \n\nc) Table 1 : abbreviations for task names are not defined in the manuscript and make it hard to find the correspondence in the initial PDEBench paper. Please define these abbreviations in the table legend.  \n\nd) There is a typo in section 5.1 (word \u201call\u201d written twice) : our models (denoted by MPP-AViT-*) must handle all all systems and regimes without finetuning"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2797/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2797/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2797/Reviewer_iLVr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697548887644,
            "cdate": 1697548887644,
            "tmdate": 1699636222434,
            "mdate": 1699636222434,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XPdI1kvl8L",
                "forum": "fH9eqpCcR3",
                "replyto": "NWdvtdtbwX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough reading and extremely useful suggestions for improvement. We have implemented most of these and we feel they have made the paper much stronger.\n\n\n#### W1\n> 1. The authors frame their question on the low-data regime... A possible experiment to add to section 5.2 would consist in training one of the PDEBench baseline models...\n\nThanks for the suggestion! We agree that this would be valuable. Since PDEBench does include results for these systems (albeit with access to the full training data), we plan on adding those results in 5.2 as dashed horizontal lines in Fig 5 for comparison so that reviewers can see at which data level our approach surpasses these widely used baselines. \n\n#### W2/Q1\n> ... train the MMP architecture on each of the PDEBench tasks... It is also quite common to include ablation studies to validate intuitions or design choices, and the claim that positional encodings help to learn boundary conditions (Section 4.2, last paragraph) should be validated either by such experiment, or by a proof in the supplementary material.\n\n> Put the low-data results (Section 5.2) in perspective with task-specific baseline performances : are these tasks way too hard for the PDEBench baselines ? Ideally, report the performance of one task-specific baseline in Figure 5.\n\nThis is also a very good suggestion. We have now broken out the \"B\" family of models into \"train from scratch\", \"pretraining only\", and \"finetuned\" models. Previously we were only showing \"pretraining only\" since our goal was to show that, since multi-task pretraining is new for physical dynamics, our approach was able to at least match modern baselines. On the whole though, the pretraining performance can be improved on most systems through finetuning. We note this in the paper, but did not include numerical evidence before.\n\n### W3 / Q2\n> 3. The conclusion reminds the motivation behind multi-systems pretraining, but does not clearly recapitulate which questions have been answered and what is left to future studies. The two paragraphs are not well articulated and I feel would benefit from a rework. Ideally the reader should finish the paper with a clear view of the unanswered questions or limitations to be addressed.\n\n> 2. Rework the conclusion to simply recapitulate the questions answered and which questions should be explored in future studies.\n\nThanks. We agree and appreciate the feedback here. We do discuss limitations, but these could certainly be less about problems faced by the entire field and be more specific to our approach. Improving this within the space constraints is something we need to think a bit more about, but we will update this when we post a revised copy prior to the discussion deadline. \n\n#### Q3/W2\n> 3. In section 4.2 last paragraph, clarify whether the claim on boundary conditions is justified theoretically, validated by an ablation study, or both. Ideally, add relevant information in the supplementary material.\n\nThis is a good point. The language is likely a bit strong in the current version and we should tone this down to suggest that it improves multi-task learning across BCs rather than enables zero-shot learning as we only have experimental support for the former. We have performed a small supplementary study on a pair of 1D advection system that differ only in boundary conditions (absorbing vs periodic) and show that our approach has a significant advantages when trained on both systems compared to traditional encodings. This will be referenced in the main text and added to the supplement.\n\n#### Minor comments\n> a) Code clarity : \n\nThanks. We are making several changes to the format of the repo (especially the README). These should come online in the anonymous repo by the end of discussion. We are still working some API changes to facilitate finetuning as some of the tensor shapes currently make that frustrating, but that is not something that will be completed by the deadlines. \n\n> b) Clarity of Figure 2\n\nThanks for this. We're still looking for a better word internally, but we will update this to mention \"field indices\" with the caption discussing what these are. On ReVIN - yes, that is also something we need to define. We mention the full word in the main text but didn't actually connect it to the acronym. We will do so both in the main text and in the caption of the figure. \n\n> c) Table 1 : abbreviations for task names are not defined in the manuscript and make it hard to find the correspondence in the initial PDEBench paper. Please define these abbreviations in the table legend.\n\nWill do!\n\n> d) There is a typo in section 5.1 \n\nThanks. We will fix this. \n\n.\n\nAgain, we want to emphasize how grateful we are for the suggestions. We believe that addressing your concerns will make this a much stronger paper. If you believe that we have addressed your concerns, we would ask that you consider raising your evaluation. If you have further concerns, please let us know! Thank you!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600882378,
                "cdate": 1700600882378,
                "tmdate": 1700600882378,
                "mdate": 1700600882378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JHnjBGbMGo",
            "forum": "fH9eqpCcR3",
            "replyto": "fH9eqpCcR3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_exE1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_exE1"
            ],
            "content": {
                "summary": {
                    "value": "The core of this paper is the proposal of a new transformer structure for pre-training on different neural operator datasets. Specifically, the paper employs several different PDE datasets from PDEBench for training, and the results can generalize to specific datasets in a zero-shot manner. Overall, the idea is novel, paving a new path for the application of neural operators in data-constrained scenarios. However, the learning of neural operators significantly differs from other data types like text and images, and whether a simple auto-regressive approach can be used for unified pre-training remains ambiguously addressed in the paper. Detailed concerns are noted in the drawbacks. All in all, I appreciate the idea presented in this paper, yet there is considerable room for improvement in problem definition, paper composition, experimental design, and comparisons. Thus I recommend the authors revise the paper and submit it to the next venue."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea presented in this paper is relatively novel.  To the best of my knowledge, no published work has utilized an auto-regressive approach for pre-training on different types of PDEs to enhance performance on downstream tasks.\n2. The network structure proposed in the paper is efficient, and capable of handling datasets or PDE problems of varying sizes, resolutions, and channel counts within an acceptable level of complexity.\n3. The experimental results validate that even with differences in equation parameters or properties, pre-training can significantly save data, which is a valuable point. Additionally, the authors found that transformers pre-trained on video datasets are also beneficial for tasks of operator learning. This is an interesting fact."
                },
                "weaknesses": {
                    "value": "1. Firstly, the paper forcibly combines different types of datasets for training without considering the potential conflicts in PDE solutions. For instance, one can construct two PDEs that have identical or minimally differing data within a certain frame count, yet due to the non-linearity of PDEs or inherent differences in the equations, they exhibit substantial differences in subsequent evolution. Unlike Ref [1] which unifies the form of PDEs, this paper's approach to mixed dataset training could lead to the model learning meaningless representations, especially in the presence of conflicting data. From the experiments and provided open-source code, it seems the paper selected equations with vastly different properties for pre-training, which doesn't address this challenge faced in PDE pre-training.\n\n2. Although the experimental section is logically well-structured, the descriptions of the experimental results and settings remain unclear. For example, PDEBench provides multiple datasets for CNS M1.0 and CNS M0.1, but the paper doesn't clearly state which dataset was used for training or testing. Besides, the source of the results for other baselines is not clarified, and the comparisons have too few baselines.\n\n3. I believe the paper's contribution of pre-training PDE representations is overstated. Upon careful examination of the appendices, I found that the paper nearly only utilized a few fluid dynamics datasets from PDEBench and a small diffusion-reaction equation dataset. Compared to the NLP or CV community, which employs almost all publicly available data on the internet for pre-training, the scale of pre-training in this paper is not large; it's more aptly termed as transfer learning.\n\nReferences\n1. Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior (https://arxiv.org/abs/2306.00258)"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575788731,
            "cdate": 1698575788731,
            "tmdate": 1699636222365,
            "mdate": 1699636222365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uec28Lcba3",
                "forum": "fH9eqpCcR3",
                "replyto": "JHnjBGbMGo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First off, we would like to thank the reviewer for the time  and effort they put into the review. It is greatly appreciated and will certainly help us improve our submission. \n\n> 1. ...two PDEs that have identical or minimally differing data within a certain frame count...\n\nThere is an important distinction that must be made here: while the benchmark systems are derived from PDEs, our goal is to develop methods that are not dependent on prior knowledge of the equations. In this setting, it is not always possible to restrict the functional form to avoid chaotic or bifurcative behavior because the physical parameters are often outside of our control. This is a challenge faced by all methods in this space that are not simply trying to learn to solve a single PDE.\n\nKnowing we are operating in spaces that can potentially be difficult to distinguish, we made three design choices to mitigate risk:\n1. We use a short trajectory of snapshots (16 in our experiments) rather than a single snapshot because of the need to differentiate between systems.\n2. Different sources of data are embedded into the shared space independently.\n3. We intended for the model to be finetuned for downstream tasks.\n\nPretraining is intended to learn to identify shared features that the model can refine for specific applications. For example, trajectories from two systems on either side of a bifurcation may follow a similar trajectory prior to diverging, but similar is not identical and the trajectories do evolve differently. (1) ensures that this gap is visible and the pretraining objective encourages the model to learn to differentiate between these small variations. It is not necessary for the model to be able to perform perfect system identification, as it is expected that it will be finetuned to the downstream task where system identification is no longer necessary. \n\nThe only case where the trajectories could realistically be identical is a system described by the weak solution of a PDE. In this case, due to discontinuity, certain terms may only be active in certain regions of phase space. If this region is never seen, like for any data-driven method, our approach will not be able to predict here. However, if this region is seen, since each data source is embedded separately, the model is given a priori knowledge to differentiate between systems even if the underlying equations are quite similar. \n\nIf the reviewer's concern is something that we have not discussed here, please let us know. \n\n> 2. ...CNS M1.0 and CNS M0.1...\n\nWith regards to the comparisons, our goals in the experiments are two-fold. First, we want to show that pretraining is learning useful behavior by showing that it can perform on par with dedicated modern baselines. The goal is not to show that pretraining alone results in state-of-the-art results. We have made this clearer by breaking out the B family of models into pretrained and finetuned variants to indicate that the pretrained models are not the upper ceiling on performance with these tasks.\n\nSecond, we want to show that given the same architecture, pretraining does improve performance on the downstream task. Since our method is the first to demonstrate the ability to learn from multiple physics, we also need to show that learning in this way outperforms learning from arbitrary spatiotemporal data (VideoMAE) as well. \n\nThank you for mentioning the CNS labeling. These are obtained by averaging results across the subsets with these parameters. We will add the results per file to the appendix. There is limited space in the main text, and the important distinction was the inclusion of compressible behavior which was not seen in the training corpus, so we aggregated by mach number. The pretrained models are still trained on the training partitions of all subsets and the baselines are still trained per-dataset. The split is performed per file as described in Appendix B.2. \n\n> 3. ...overstated. ...the scale of pre-training in this paper is not large; it's more aptly termed as transfer learning.\n\nWe are happy to address this concern. In our listed contributions on page 2, we do specifically mention we are exploring transfer capabilities so it seems we are in agreement there. If the contention is with the term large-scale, we would point out that the Pile, the dataset generally used for LLM training, is ~800GB. The subset of PDEBench we use is ~5TB. \n\nWe are cautious about our claims of scope - on pages 3 and 9, that we do not believe the dataset is sufficiently diverse to call our pretrained models \"foundation models\", but the scale of training is still quite large and the development of these methods are vital for us to develop true foundation models as the data environment matures.\n\n.\n\nWe sincerely appreciate the effort and careful reading by the reviewer. If we have been able to address your concerns, we would ask that you consider increasing your score. Also, please let us know of any other concerns. Thank you!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600706904,
                "cdate": 1700600706904,
                "tmdate": 1700600706904,
                "mdate": 1700600706904,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PX7rJYnBtu",
            "forum": "fH9eqpCcR3",
            "replyto": "fH9eqpCcR3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_ffib"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_ffib"
            ],
            "content": {
                "summary": {
                    "value": "The authors present Multiple Physics Pretraining (MPP), a methodology for task-agnostic pretraining of surrogate models in physics. This technique facilitates pretraining on a large scale, allowing for knowledge transfer across a variety of physical domains. To handle the problem that features of physical tasks are diverse, a shared embedding and normalization strategy are introduced to project the fields of multiple systems into a single shared embedding space. The experiments show that a single MPP-pretrained transformer can output competitive or outperform task-specific baselines ( fluid mechanics-oriented benchmark) on all pretraining sub-tasks without finetuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea to construct a large pre-trained base model for physical simulations is promising. The current learning based surrogate models usually have limited generalization ability, which require re-training from scratch given different governing equations. The pre-trained model has the potential to improve the generalization ability or make it possible only to fine-tune on specific tasks without training from scratch.\n- In the experiments part, the proposed model can achieve one order of magnitude smaller errors comparing to the existing methods."
                },
                "weaknesses": {
                    "value": "- The experiments for validating the proposed model are currently limited to 2-D cases (incompressible and compressiable Navier-stokes equations,  shallow-water equations, and a 2D DiffusionReaction equation). It is unclear whether the proposed model can achieve same level of performance and accuracy when applied to more realistic 3D physical simulations.\n- It seems that the current model mainly focus on predicting the solution one time step further. The video results (diffre, incompNS, mpp_swe) shown in the supplemental materials exhibits strong checkboard artifacts as the timesteps grow.\n- The proposed model can only handle simulations on structured mesh. However unstructured mesh (e.g., triangular, tetrahedra) is a more common choice in real world simulations. In addition, the multi-resolution i.e., mesh with adaptive resolution, which is also a common technique in large scale simulations, has not been taken into consideration."
                },
                "questions": {
                    "value": "- It is mentioned in the appendix that the model is trained for 500 epochs for one task. How long does the training process take?\n- Are there any intuitive explanation for the checkboard artifacts as the timesteps grow shown in the video? Does it mean the proposed method is numerical unstable to some extent?\n- Does the proposed method have the potential to be applied to 3D simulations while keeping the efficiency and accuracy? (as the max memory usage of current model is ~60GB)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685017546,
            "cdate": 1698685017546,
            "tmdate": 1699636222288,
            "mdate": 1699636222288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UES2HQIuSB",
                "forum": "fH9eqpCcR3",
                "replyto": "PX7rJYnBtu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We'd like to thank the reviewer for the time and energy they put into a close reading of our paper and greatly appreciate the reviewer's perspective on issues in the field. We have used your feedback to strengthen the paper and hope we've been able to address your concerns.\n\n> limited to 2-D cases... unclear whether the proposed model... more realistic 3D physical simulations.\n\n> ...applied to 3D simulations while keeping the efficiency and accuracy? \n\nFor introducing this method, we felt it was best to evaluate on a pre-existing benchmark within the community. 2D simulations are conventionally used for this as they elicit complex behavior without the extreme cost of 3D methods. We do choose our base architecture with scaling in mind, but since we're introducing fundamentally new capabilities to deep learning for physical systems (our approach is the first to demonstrate the ability to learn multiple nonlinear time-dependent dynamics simultaneously), it was important to place it in context.\n\nWe do agree that 3D fluids are a more realistic setting and did account for this future goal in our architectural choices. Using 3D patching, we are able to train on the (5x128x128x128) Compressible Navier-Stokes (CNS) AViT-B in mixed-precision at a memory cost of 43.90 GB per sample. Training from scatch is still enough to surpass the PDEBench baselines: \n\n| Model | NRMSE CNS (Turb) |  NRMSE CNS (Rand)|\n| -------- | -------- | -------- |\n| UNet     | 1.00      |  .23|\n| FNO     | .37    |  .24 | \n| AViT-B     | .32   | .18 | \n\nHowever, our goal here is exploring transfer and there are not yet accepted benchmarks that cover a diverse set of 3D problems. Transfer from 2D to 3D is non-trivial due to the downsampling assuming a fixed patch dimension, but dimension-agnostic \"patching\" is a promising future research direction for further expanding multiple physics pretraining and our design choices do enable this type of exploration. \n\n> one time step further...\n\n> ...checkboard artifacts... numerical unstable...\n\nWe're grateful that you examined our supplementary material! We will improve the labeling to make this clear, but the listed videos are actually all pretraining only rollouts. The CNS video is of a fine-tuned model. We've found these checkboard artifacts are addressed for many systems through finetuning as can be observed, though not all.\n\nWe suspect these are linked to well-studied stability issues [1, 2, 3, 4] that have been connected to aliasing behavior, though we would note that as you observed in the videos, the model does not seem to completely diverge while the linked works have established that most of the baselines do in similar settings. Long-run stability of these models is an important research direction, but is out of scope here. \n\nReferences:\n\n[1] Learned coarse models for efficient turbulence simulation. (https://arxiv.org/abs/2112.15275).\n\n[2] Towards Stability of Autoregressive Neural Operators. (https://arxiv.org/abs/2306.10619).\n\n[3] Pde-refiner: Achieving accurate long rollouts with neural pde solvers. (https://arxiv.org/abs/2308.05732). \n\n[4] Are neural operators really neural operators? Frame theory meets operator learning. (https://arxiv.org/abs/2305.19913). \n\n> ...unstructured mesh (e.g., triangular, tetrahedra) ... adaptive resolution...\n\nThank you for pointing this out! Natively handling arbitrary meshes is a very exciting problem both in machine learning and numerics and certainly is not solved in either space. Outside of GNNs, most deep learning methods, including all of the baselines discussed, assume uniform grids either natively or via interpolation. We fully agree that handling arbitrary meshes would be a valuable contribution, but our work is focused specifically on the pretraining task.\n\n> How long does the training process take?\n\nWe are planning on updating the language to switch from \"epoch\", which we arbitrarily define as 2000 optimization steps as we are sampling with replacement, to listing the number of steps for clarity. \n\nThe base model (AViT-B) is pretrained using ~960 GPU hours on H100s, though finetuning is comparable to the UNet and ORCA baselines. The inference time of each model on an A6000 GPU is listed below (we plan on adding this to an appendix since it is important information to convey)\n\n| Model | Time (ms) |\n| -------- | -------- |\n| UNet     | 67.7      | \n| FNO     | 7.2    | \n| AViT-Ti     | 83.5    | \n| AViT-B     | 105.6    | \n| ORCA | 98.5 |\n\n.\n\nAgain, we just want to express our gratitude to the reviewer for their valuable feedback. We expect that incorporating it will lead to a stronger paper. If we were able to address your concerns, please consider revising your score. Please let us know if you have further insights to share!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600426772,
                "cdate": 1700600426772,
                "tmdate": 1700600426772,
                "mdate": 1700600426772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TUST77H2jM",
            "forum": "fH9eqpCcR3",
            "replyto": "fH9eqpCcR3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_AmeY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_AmeY"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an autoregressive task-agnostic pretraining approach for physical surrogate modeling. As like foundation models, the proposed method proposes training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper builds on recent advances in deep learning for physical simulation to enable surrogate models to work for diverse physical systems with a few fine-tuning iterations. The exposition for the motivation is written crisply and convincing, and is generally easy to follow. The experiments are performed for diverse physical systems. The results show that large surrogate models outperform strong baselines, and even models with a relatively few parameters can learn such diverse physics evolutions and perform competitively."
                },
                "weaknesses": {
                    "value": "Although the generalizing capability is significant, the computational cost of the proposed models is still concerning. The followings are associated questions: \n* How do the authors expect the computational cost of the proposed models and how does it compare against other baselines in Table 1?\n* It is still a bit unclear if the comparison reported in Table 1 is fair, since the parameters of the first 4 models do not look comparable.\n* Is the model applicable to 3D simulations?\n\nMany details of the inverse problems seem missing, and it makes assessing the performance of the proposed model a bit difficult in this regard. The followings are some of the questions:\n* How do you define the inverse problem and what model was used for the tasks?\n* How does the performance compare to other baselines?\n* What is the running time of the proposed model and how is it comparable to the other baselines?\n* Could the authors provide insights or results on the inverse problem for boundary condition?\n* Providing qualitative results would be helpful.\n\u00a0\n**Minor comments** \n\nTypo in section 5.1:\u00a0must handle all all systems and regimes without finetuning"
                },
                "questions": {
                    "value": "Please have a look at the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Nothing particular."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701832676,
            "cdate": 1698701832676,
            "tmdate": 1699636222206,
            "mdate": 1699636222206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rbSKt88IRu",
                "forum": "fH9eqpCcR3",
                "replyto": "TUST77H2jM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We'd like to thank the reviewer for their time and efforts in helping us strengthen our paper. We've added additional detail in response to your concerns.\n\n> ...the computational cost of the proposed models is still concerning.\n\n> ...computational cost... compare...?\n\n> ...running time of the proposed model...\n\nThese are good points. While the purpose of pretraining methods is to develop multi-use models where the pretraining cost is borne by the pretrainers, this is important information and we have added an appendix section showing this. For reference, the pretraining for AViT-B currently takes ~960 GPU hours in H100s in single precision training (we plan on release the weights to amortize cost for users). During finetuning, the cost is more comparable to baselines. The time per forward pass on an A6000 GPU is listed below:\n\n| Model | Time (ms) |\n| -------- | -------- |\n| UNet     | 67.7      | \n| FNO     | 7.2    | \n| AViT-Ti     | 83.5    | \n| AViT-B     | 105.6    | \n| ORCA | 98.5 |\n\nOur AViT code is currently unoptimized - it is for instance possible to fuse the axial attentions into a single cuda kernel launch. Despite that, the runtime of B is comparable to ORCA while Ti is comparable to the UNet. The FNO from the benchmark is quite a bit faster, but the performance is competitive only on SWE.\n\n> ...unclear if the comparison reported in Table 1 is fair...\n\nOur goal in this experiment is to show that despite the difficulty of multi-task optimization, we are able to match established baselines. In vision and language, it is common to report only finetuned results, but due to the novelty of our approach within the domain, we felt showing that pretraining alone can reach currently acceptable accuracy was valuable. We are expanding our B family in the table to show the difference between pretraining and finetuning for emphasis.\n\nWhile we agree that more precise parity across all models would be valuable, Ti is about as small as people have had success training vision transformers and our research goal here is not development on that front. Ti is in line with the larger of the two comparable baselines (PINNs are included for completeness, but these are fit per example rather than trained for generalization). \n\n> Is the model applicable to 3D simulations?\n\nThis is something we are excited to explore in future work. The architecture as a whole is selected with scalability in mind and can be run on 3D data with minor changes, but established benchmarks for evaluating cross-system transfer in 3D  do not currently exist. Using mixed precision, we can fit an AViT-B on the largest 3D system in PDEBench (5x128x128x128) with ~43 GB of VRAM. Transfer from 2D to 3D has additional complications, like the dimension-specific downsampling (patching) process.\n\n> How do you define the inverse problem...\n\nThanks, we agree with these criticisms and will update the manuscript with a more detailed description.\n\nThere are two parameter inference problems discussed here. Both are tackled using the pretrained AViT-B model. The first is the forcing identification in INS. In equation 9 in the appendix, the INS equations are described with a forcing **f**. This **f** is randomly sampled per trajectory and constant through the simulation. In this task, we try to predict what this forcing is by finetuning (or training from scratch) the AViT-B model from Experiment 1. \n\nThe second is identifying the buoyancy parameter which is a single scalar serving a similar role in an outside simulation entirely separate from pretraining. We will update the appendix to include this precise equation as well for reference in the main text.\n\n> * How does the performance compare to other baselines?\n\nThis task only makes sense in the context of multi-task training where the model must perform implicit system identification. Most of the models from Exp 1 are developed and trained for use on only one system and therefore do not need to internally identify these parameters. Mialon et al., 2023 [1] is the only other \"pretraining\" paper that has tackled one of these problems (buoyancy). We will add these results to the table.\n\nThe scalar inverse problem was not a strength of our approach, but we felt this was important information to share. In NLP, autoregressive models typically perform worse than other approaches for non-generation tasks. The fact that the contrastive approach of [1] outperformed the mean prediction suggests that could be the case here as well, but these are early efforts so we cannot say anything absolute.\n\n[1] Gregoire Mialon, et al. Self-supervised learning with lie symmetries for partial differential equations, 2023 \n\n> Typo in section 5.1: must handle all all systems and regimes without finetuning\n\nThis has been fixed, thanks!\n\n.\n\nOnce again, we appreciate the time you put into reviewing our paper. If we were able to satisfy your concerns, please consider increasing your score. We'd love to hear any additional concerns as well."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599812434,
                "cdate": 1700599812434,
                "tmdate": 1700599812434,
                "mdate": 1700599812434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2gLOUHgBd8",
            "forum": "fH9eqpCcR3",
            "replyto": "fH9eqpCcR3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_2tFL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2797/Reviewer_2tFL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a multiple physics pretraining approach for surrogate modeling, which learns general useful features across diverse physical tasks with a shared embedding and normalization strategy. The experiment results show the proposed MPP-pretrained model outperforms task-specific baselines on all pretraining sub-tasks and also show superior finetuning results on new physics tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Constructing a physics-based foundational model and exploring multiple task pretraining for computational physics tasks is both interesting and beneficial.\n2. The experiments demonstrate impressive results in both pretraining sub-tasks as well as substantial transfer potential for new tasks with low-data system.\n3. The authors have conducted several training strategies to perform the pretraining effectively."
                },
                "weaknesses": {
                    "value": "1. More attempts can be made to address the transferability problems between different types of physics equations. For instance, when discussing the Navier-Stokes (NS) equations, whether at high or low Reynolds numbers, the equation forms are quite similar, which allows for the investigation into whether a single model still possesses robust merged learning capabilities for more different classes of equations, such as diffusion equations and wave equations.\n\n2. What are the advantages and disadvantages of this new approach compared to the Finite Element Method (FEM) for systems with explicit control equations or empirical formulas? One of my biggest problem is whether the physics task should be treated as a purely data-driven problem, or if it ought to incorporate certain explicit priors or equation-based guidelines.\n\n3. For models without PDEs, how can we determine the similarity of multiple physics fields and whether they can be learned simultaneously? I\u2019m concerned about the improved performances are due to the limited diversity of different physics tasks.\n\n4. In the appendix, experimental data from 1-step to 9-step show little change in the field, whether looking at ground truth or predicted solutions. If the selected time steps were longer, would the model still be able to accurately predict future changes?"
                },
                "questions": {
                    "value": "Overall, the purposes behind this work are valuable. However, there remain many questions that need to be addressed. Please consider to answer the above questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745067380,
            "cdate": 1698745067380,
            "tmdate": 1699636222132,
            "mdate": 1699636222132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s5rTCBz34Z",
                "forum": "fH9eqpCcR3",
                "replyto": "2gLOUHgBd8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2797/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We'd like to thank the reviewer for their close reading and valuable insights. These critiques have helped us strengthen the paper.\n\n> 1. ...the transferability problems between different types of physics equations....\n\nThese are interesting suggestions! While we agree that more diverse exploration would be valuable, multi-task training is new to this space, so in this initial paper, we wanted use an established benchmark that has been previously used for analysis in this space. We agree that exploring the limits of transfer is an exciting area going forward.\n\nAbout the Reynolds remark, the CNS data in PDEBench does actually include a range of Reynolds numbers - we do not report them as we believe the simulations have significant numerical diffusion due to insufficient resolution which we cannot account for in the computation of the effective Reynolds numbers. We do want to make this range clearer and we updated the appendix to reflect the results per nominal viscocity coefficient with the numerical diffusion caveat included in the text.\n\n> 2. ...advantages and disadvantages of this new approach compared to the Finite Element Method (FEM)...\n\nThis is an important distinction and one we will try to address in the discussion of neural PDE solvers vs operators in the related work. We do consider this to be a data-driven method. In this setting, PDEs are simply convenient testbeds rather than the true target. Numerical methods like FEM are clearly superior in both accuracy and predictability for tasks where the physics can be fully resolved by numerical methods. The advantage of data-driven methods is applicability to faster surrogates for difficult-to-resolve system or for learning directly from observational data in less controlled settings. As we note in our intro, these tend to also be areas where we have limited training data, which is why one of our focuses is transfer in the low-data regime.\n\nExplicit physical priors are another valuable approach in this regime, but recently vision and language have been finding success with less constrained models that are pretrained on large amounts of data, but it has not been clear how to do the same in a dynamics setting. Our work is the first that has seen some success for nonlinear dynamics. We believe both paths to be interesting research directions. \n\n> 3. For models without PDEs...\n\nThis is a very interesting question. It is true that the data in this benchmark are largely fluid transport, this is not a small area. Transport phenomena are ubiquitous in fluids, rheology, and plasma physics which show up in a broad range of applied fields. PDEBench is one of the most diverse datasets in this domain today (and ~5TB compared to the ~800 GB Pile used for LLM training) with data from multiple equations, parameters, solvers, and boundary condition types, but answering this question will likely require much more diverse data sets than currently exist. Curating such a data set would be an strong contribution to the field on its own.\n\nThe limits of transfer from multi-task pretraining is something that was not even possible to explore previously. The results with VideoMAE suggest that the limits to transfer benefits may extend further than we'd normally assume and this is something that would be interesting to explore as data in this space matures. We believe that the fact that these questions are now open to researchers is one of the most exciting aspects of our work.\n\n> 4. In the appendix, experimental data from 1-step to 9-step show little change in the field, whether looking at ground truth or predicted solutions. If the selected time steps were longer, would the model still be able to accurately predict future changes?\n\nThe answer to this is likely task-specific. For our method and most methods in this space there is likely going to be a trade-off between small steps that rapidly accumulate autoregressive error and large steps which render the problem too difficult. Our goal in experiment 1 was to show that our pretrained models learn the system at least as well as modern baselines, so we used the benchmark steps which vary considerably system-to-system. CNS, for example, has highly visible changes step-to-step and is one of the systems where we see some of the largest improvements by percentage. This does indicate that our method performs no worse in this regard than standard approaches.\n\n.\n\nWe greatly appreciate the reviewer's time and effort that went into helping us improve our paper. If we were able to address your concerns, please consider increasing your score. Thank you!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599158590,
                "cdate": 1700599158590,
                "tmdate": 1700599158590,
                "mdate": 1700599158590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]