[
    {
        "title": "Hierarchical Graph Latent Diffusion Model for Molecule Generation"
    },
    {
        "review": {
            "id": "gzXY1gdjFb",
            "forum": "RSincg5RBe",
            "replyto": "RSincg5RBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a novel hierarchical latent diffusion model for molecular graph generation. To be specific, this work introduces GLDM, a latent diffusion model for graphs using graph-level embeddings, and proposes HGLDM, a latent diffusion model that further incorporates structural information, for which these approaches enable efficient training and sampling while outperforming previous graph diffusion models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. \n\n- The motivation for using the latent approach, i.e., overcoming the mismatch between the continuous diffusion space and discrete data space and further reducing computational cost, is clear.\n\n- Using hierarchical embeddings of graphs for graph latent diffusion is novel and shows improvements in conditional molecule generation tasks compared to the naive latent diffusion model (GLDM) as well as previous diffusion models."
                },
                "weaknesses": {
                    "value": "- Although this work states that the (hierarchical) latent approach for graph generation provides a scalable solution for molecule generation, the provided experiments are limited to datasets (e.g., GuacaMol) in which previous diffusion models (e.g., GDSS and DiGress) are applicable. In order to justify the scalability of the proposed method, it should be evaluated in a larger dataset.\n\n- The experimental setting for evaluating the computational efficiency is not clear. Is the training and sampling time measured in the same condition, e.g., training conducted via DDP and using the same number of V100 GPUs? \n\n- Generation performance on unconditional molecule generation tasks should be evaluated with more descriptive metrics, for example, FCD, Scaffold similarity [1], and Fragment similarity [1]. Reported metrics, i.e., validity, uniqueness, and novelty fail to measure how similar (e.g., chemical aspects) are the generated molecules to the molecules from the test set. In particular, under the current setting, GDSS seems to be showing comparable results in large datasets (ZINC250K and GuacaMol) with significantly fewer parameters.\n\n- The quantitative results of Tables 2 and 3 show that the performances of GLDM and HGLDM on unconditional generation tasks are almost the same, whereas there is a significant improvement using the hierarchical approach for conditional generation tasks. What is the reason for the hierarchical approach only effective in conditional tasks?\n\n- As the continuous diffusion model (e.g., GDSS) outperforms the discrete diffusion model (e.g., DiGress) in Table 2, the continuous diffusion model should be compared as a baseline in Table 3 (i.e., conditional generation task). Although GDSS does not explicitly present a conditional framework, recent work [2] proposes a conditional molecule generation framework using classifier guidance based on GDSS, which could be used as a baseline.  \n\n- The performance of GLDM (and HGLDM) comes from the effectiveness of using a latent representation of graphs compared to previous graph diffusion models, not from the diffusion processes. Thereby, analysis of the latent representation, e.g., interpolation in the latent space or clustering of the latent points with respect to certain conditions, would greatly strengthen this work.\n\n- Missing references on related works:\n  - Qiang et al., Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D, ICML 2023\n  - Xu et al., Geometric Latent Diffusion Models for 3D Molecule Generation, ICML 2023\n\n- I would like to raise my score if the above concerns are sufficiently addressed.\n\n---\n\n[1] Polykovskiy et al., Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models, arXiv 2018\n[2] Lee et al., Exploring Chemical Space with Score-based Out-of-distribution Generation, ICML 2023"
                },
                "questions": {
                    "value": "- Please address the questions in the Weakness.\n\n- Is the results of Table 2 from a single run or an average of multiple runs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643531899,
            "cdate": 1698643531899,
            "tmdate": 1699636793631,
            "mdate": 1699636793631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WNdiU2IB0f",
                "forum": "RSincg5RBe",
                "replyto": "gzXY1gdjFb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y9cz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive and detailed comments. The reviewer has provided a nice suggestion by including the analysis of the latent representation. We agree that it will better emphasize the contribution of our work, and have included this experiment in the revised version. The response for each concern has been provided as follows. \n\n---\n### Weaknesses:\n**W1. Although this work states that the (hierarchical) latent approach for graph generation provides a scalable solution for molecule generation, the provided experiments are limited to datasets (e.g., GuacaMol) in which previous diffusion models (e.g., GDSS and DiGress) are applicable. In order to justify the scalability of the proposed method, it should be evaluated in a larger dataset.**\n\nR1: Thank you for your suggestion. During the rebuttal period, we have added experimental results on a larger molecule dataset, MOSES dataset. The results are shown in the Table below. Our proposed method achieved better Valid, Novelty, Diversity and comparable Unique scores compared to the baseline methods on the MOSES dataset.\n\n**Table 1: Unconditional Generation Results on MOSES Dataset**\n| Methods | Valid $\\uparrow$           | Unique$\\uparrow$           | Novelty$\\uparrow$          | Diversity$\\uparrow$        | FCD$\\downarrow$             | NSPDK MMD$\\downarrow$      |\n|-----------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|-------------------------------------|------------------------------------|\n | GDSS    | **1.000 $\\pm$ 0.000** | 0.994 $\\pm$ 0.003          | 0.999 $\\pm$ 0.000          | 0.899 $\\pm$ 0.000          | 21.265 $\\pm$ 0.249          | 0.037 $\\pm$ 0.005          |\n| DiGress | 0.858 $\\pm$ 0.005          | **1.000 $\\pm$ 0.000** | 0.996 $\\pm$ 0.001          | 0.886 $\\pm$ 0.000          | **9.228 $\\pm$ 0.081**  | **0.010 $\\pm$ 0.000** |\n | PS-VAE  | **1.000 $\\pm$ 0.000** | 0.999 $\\pm$ 0.000          | **1.000 $\\pm$ 0.000** | 0.905 $\\pm$ 0.000          | 26.401 $\\pm$ 0.078          | 0.079 $\\pm$ 0.000          |\n | GLDM    | **1.000 $\\pm$ 0.000** | 0.998 $\\pm$ 0.000          | **1.000 $\\pm$ 0.000** | 0.905 $\\pm$ 0.000          | 26.365 $\\pm$ 0.095          | 0.077 $\\pm$ 0.001          |\n | HGLDM   | **1.000 $\\pm$ 0.000** | 0.999 $\\pm$ 0.000          | **1.000 $\\pm$ 0.000** | **0.906 $\\pm$ 0.000** | 25.815 $\\pm$ 0.053          | 0.072 $\\pm$ 0.000          |\n\n**W2. The experimental setting for evaluating the computational efficiency is not clear. Is the training and sampling time measured in the same condition, e.g., training conducted via DDP and using the same number of V100 GPUs?**\n\nR2: Sorry for the confusion. The training and sampling time are measured under the same conditions. We have clarified this in the revised version.\n\n**W3. Generation performance on unconditional molecule generation tasks should be evaluated with more descriptive metrics, for example, FCD, Scaffold similarity [1], and Fragment similarity [1]. Reported metrics, i.e., validity, uniqueness, and novelty fail to measure how similar (e.g., chemical aspects) are the generated molecules to the molecules from the test set. In particular, under the current setting, GDSS seems to be showing comparable results in large datasets (ZINC250K and GuacaMol) with significantly fewer parameters.**\n\nR3: Thank you for your suggestion. In the revised version, we have added FCD and NSPDK to evaluate the graph distribution of the generated molecules. The detailed results are shown in the Tables 1-4 below. Although the proposed method does not achieve best results in FCD and NSPDK, this is mainly limited by the VAE backbone model, and we have improved upon PSVAE in terms of  FCD and NSPDK across all the datasets. Furthermore, unlike general graph generation that primarily focuses on learning graph distributions, we believe that comparing distribution similarity in molecular generation tasks without considering molecular properties lacks practical significance. Therefore, after verifying that the performance of the proposed method in terms of metrics such as validity and novelty are comparable, indicating that our model performs well in unconditional generation, we shift our focus to conditional molecular generation  that can incorporate more conditional information into the generation process."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571321712,
                "cdate": 1700571321712,
                "tmdate": 1700571321712,
                "mdate": 1700571321712,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VUAqyh14XG",
                "forum": "RSincg5RBe",
                "replyto": "DId5cKCPCu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response and additional experiments.\n\n- The results of MOSES do not show improvements over the baselines: Validity, Uniqueness, and Novelty are similar to GDSS with a small improvement in Diversity (0.7 pt) while a significant increase in FCD and NSPDK.\n\n- The FCD and NSPDK results on all the datasets are significantly worse than the baselines, indicating that the proposed method is not capable of learning the data distribution. Validity, Uniqueness, and Novelty are similar to that of GDSS with no clear improvement.\n\n- In Table 3, GDSS performs the best on multi-properties which is the most practical setting for real-world tasks, for example, drug discovery where it is required to satisfy many different objectives. In my opinion, performing better in a single property generation does not provide much merit.\n\nDue to these concerns, I maintain my score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720699061,
                "cdate": 1700720699061,
                "tmdate": 1700720699061,
                "mdate": 1700720699061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GL6u39Y8DN",
            "forum": "RSincg5RBe",
            "replyto": "RSincg5RBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_vEbk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_vEbk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a latent diffusion model that aims to generate hierarchical levels of latent variables such as node-level, subgraph-level, and graph-level, simultaneously. To construct the latent space, the authors leverage the PS-VAE where the decoder converts the graph-level latent variables to the molecule by sequentially predicting the fragments and then predicting the links between fragments. To generate the latent variables of node-, subgraph-, and graph-level, the authors leverage the generative process of DDPM. The authors propose an architecture that models the dependency between node-, subgraph-, and graph-level embeddings alleviating the burden of considering the edge features. The proposed method is evaluated on the molecule generation tasks in the conditional and unconditional settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The auxiliary generation of node and subgraph embeddings can enrich the forwarded information of the diffusion process. Specifically, even though PS-VAE requires only the graph embedding in the decoding stage, the authors propose to generate the node and subgraph embedding along with the graph embedding. Defining the correlated generative processes in this paper could convey more information to the model."
                },
                "weaknesses": {
                    "value": "* It is not clear why the authors select a way to generate graph embeddings and then decode them. Accessing the graph embedding could contain less information than accessing the subgraph- or node- and edge-level latent variables. The \n* The name of the proposed method is misleading. The goal of the proposed method is to generate the hierarchical latent variables. However, the name (Hierarchical Graph Latent Diffusion Model) can be misinterpreted as a sequence of the diffusion models to generate the latent variables.\n* The authors report the validity, uniqueness, and novelty as the main results. However, these metrics seem to be restricted to only measure the sample diversity.\n>- For the validity, it is unfair to compare with the denoising diffusion model such as GDSS and DiGress, as the decoding stage of the proposed method and PS-VAE intrinsically do the validity check while linking the fragments. Therefore, reporting and comparing the validity are not enough to demonstrate the effectiveness of the proposed method.\n>- For the uniqueness and novelty, they demonstrate that the generative model can guarantee sample diversity. However, to demonstrate whether the generative model precisely learns the data distribution, reporting the uniqueness and the novelty is not enough. Please note that recent works [1,2] leverage FCD, Scaffold similarity, SNN and NSPDK to measure the difference of the generated distribution and the data distribution. Therefore, I believe that measuring the uniqueness and novelty is important, but to demonstrate the effectiveness, it would be better to measure the distributions of the generated molecules.\n* The efficiency of the proposed methods seems to come from the light model architecture with smaller dimensions than the model architecture used in the DiGress.\n\n[1] Vignac, Clement, et al. \"Digress: Discrete denoising diffusion for graph generation.\" arXiv preprint arXiv:2209.14734 (2022).\n\n[2] Jo, Jaehyeong, Seul Lee, and Sung Ju Hwang. \"Score-based generative modeling of graphs via the system of stochastic differential equations.\" International Conference on Machine Learning. PMLR, 2022."
                },
                "questions": {
                    "value": "For clarification, I would appreciate if the authors provided an explanation of my questions.\n1. In Section 4.1.1, to my understanding, the number of subgraph embeddings should not be the number of nodes. If so, how do you sample the number of subgraph embeddings at the beginning of the sampling stage?\n2. How do you get the subgraph embeddings from the PS-VAE architecture?\n3. In Table 3, how did you measure the mean absolute error (MAE) on the unconditional setting? Does it mean training on the selected 100 molecules without the conditions?\n4. Why are some reported values different from the original papers? For example, the novelty and uniqueness of PS-VAE on the QM9 dataset and the validity of GDSS on the ZINC250k dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670806464,
            "cdate": 1698670806464,
            "tmdate": 1699636793430,
            "mdate": 1699636793430,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xu5oLqDbTO",
                "forum": "RSincg5RBe",
                "replyto": "GL6u39Y8DN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vEbk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the provided comments. Regarding Weakness 1, we hope that we have clarified the model design and main contributions of our work to address the misunderstandings. With regards to Weaknesses 2 and 3, we have made revisions in accordance with the reviewer's requests in the revised version. We sincerely hope that the reviewer will raise any further questions if our responses are still unclear. The response for each concern has been provided as follows. \n\n---\n### Weaknesses:\n**W1. It is not clear why the authors select a way to generate graph embeddings and then decode them. Accessing the graph embedding could contain less information than accessing the subgraph- or node- and edge-level latent variables.**\n\nR1: We agree with your viewpoint that \"Accessing the graph embedding could contain less information than accessing the subgraph- or node- and edge-level latent variables.\"  We need to clarify that **the main contribution of this paper is the HGLDM model, which includes graph embeddings, subgraph embeddings, and node embeddings.** The reason why edge-level embeddings are not included is that, as demonstrated by the baseline methods in the Section 5.1, the diffusion process on the edge matrix is computationally expensive. The graph embedding-based model GLDM proposed in this paper is just a straightforward attempt inspired by the latent diffusion model, as there exists the decoder that can generate molecules from graph embeddings. However, in this paper, we found that the hierarchical embeddings proposed in this paper are more effective than the graph embeddings, as the latter contain less information, as you mentioned.\n\n**W2. The name of the proposed method is misleading. The goal of the proposed method is to generate the hierarchical latent variables. However, the name (Hierarchical Graph Latent Diffusion Model) can be misinterpreted as a sequence of the diffusion models to generate the latent variables.**\n\nR2: Thank you for your suggestion. We have modified it to \"Latent Diffusion Model for Hierarchical Graph\" in the revised version to avoid potential misleading.\n\n**W3. The authors report the validity, uniqueness, and novelty as the main results. However, these metrics seem to be restricted to only measure the sample diversity.\nFor the validity, it is unfair to compare with the denoising diffusion model such as GDSS and DiGress, as the decoding stage of the proposed method and PS-VAE intrinsically do the validity check while linking the fragments. Therefore, reporting and comparing the validity are not enough to demonstrate the effectiveness of the proposed method.\nFor the uniqueness and novelty, they demonstrate that the generative model can guarantee sample diversity. However, to demonstrate whether the generative model precisely learns the data distribution, reporting the uniqueness and the novelty is not enough. Please note that recent works [1,2] leverage FCD, Scaffold similarity, SNN and NSPDK to measure the difference of the generated distribution and the data distribution. Therefore, I believe that measuring the uniqueness and novelty is important, but to demonstrate the effectiveness, it would be better to measure the distributions of the generated molecules.**\n\n\nR3: For the validity, we need to clarify that during the decoding stage, the PS-VAE only performs link prediction to connect fragments and does not perform validity check. In the revised version, we have added FCD and NSPDK to evaluate the graph distribution of the generated molecules. The new results are shown in Tables 1-4 below. Although the proposed method does not achieve best results in FCD and NSPDK, this is mainly limited by the VAE backbone model, and we have improved upon PSVAE in terms of  FCD and NSPDK across all the datasets. **Furthermore, unlike general graph generation that primarily focuses on learning graph distributions, we believe that comparing distribution similarity in molecular generation tasks without considering molecular properties lacks practical significance**. Therefore, after verifying that the performance of the prpoposed method in terms of metrics such as validity and novelty are comparable, indicating that our model performs well in unconditional generation, we shift our focus to conditional molecular generation  that can incorporate more conditional information into the generation process."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571026163,
                "cdate": 1700571026163,
                "tmdate": 1700571026163,
                "mdate": 1700571026163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FFnvdfWTNv",
                "forum": "RSincg5RBe",
                "replyto": "hqZxewkU1u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_vEbk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_vEbk"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the detailed explanation and additional experiments. However, I am still concerned about the experimental settings and the contribution of this work.\n\n1. Regarding the validity check : \nTo the best of my knowledge, PS-VAE actually performs the validity check while adding the bond between fragments as described in Algorithm 3 of the PS-VAE's paper. Did you change your decoding stage from the one of PS-VAE?\n\n2. Regarding the additional experiments with FCD and NSPDK metrics:\nI am afraid that the proposed method is not capable of learning the practical molecular properties because of their inferior performance on FCD and NSPDK metrics as shown in the additional experimental results. \nFCD and NSPDK are related to biological, chemical, and structural properties since FCD takes into account the chemically and biologically relevant information and NSPDK considers the structural connectivity based on their atom types, whereas QED and logP are easily optimizable [1]. I believe that the low FCD and NSPDK MMD values demonstrate the capability of capturing the molecular properties within the training dataset, which can lead to practical utilization. However, the proposed method does not show the better performance of FCD and NSPDK, and thus it seems not to contribute a lot to the graph generation field.\n\n3. Regarding the contribution:\nAlthough I agree that generating the node, subgraph, and graph embeddings can reduce the computational cost, the generation quality of the proposed method is not clearly demonstrated, which does not seem to contribute to the graph generation field. To clearly convince the readers of the contribution of this work, I suggest conducting another goal-directed benchmark that is closely related to the actual application such as binding affinity as in MOOD [2].\n\n\n[1] Nathan Brown et al., \"GuacaMol: Benchmarking Models for de Novo Molecular Design\", JCIM, 2019.\n\n[2] Seul Lee et al., \"Exploring Chemical Space with Score-based Out-of-distribution Generation\", ICML, 2023."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639390370,
                "cdate": 1700639390370,
                "tmdate": 1700639390370,
                "mdate": 1700639390370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nzzNHxdtnQ",
            "forum": "RSincg5RBe",
            "replyto": "RSincg5RBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a form of latent diffusion ala Rombach et al. for graphs, in particular, molecule generation. For this they combine a PS-VAE autoencoder with a DDIM style denoising diffusion model which leverages a hierarchy-aware GNN which uses a GAT style subgraph embedding update and PNA pooling for the graph embedding update at every layer.  The method is compared against it's constitutent components and two SotA Diffusion baselines (Digress/GDSS) as well as VAE and other methods on QM9,ZINC250K and Guacamol."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall a good \"Nothing to complain about\" paper.\n\nOriginality:\n\nLatent diffusion for graphs was a thing waiting to be done, but it is still worth doing. The hierarchical block is a nice construction, as is the continuous-discrete combo.\n\nQuality: The evaluation including Guacamol is good, the QM9 and ZINC250k benchmarks show impressive results.\nclarity: The paper is very clearly presented and the appendix, while sparse, gives most of the information required for presenting things.\nSignificance: Getting hierarchical graph modeling like this going is likely to have a very high impapct, iff the method generalizes."
                },
                "weaknesses": {
                    "value": "- I'd like to see error bars indicating variance accross multiple seeds  if possible\n- While QM9 and ZINC250k performance is imprressive, these graphs are kind of solved. What is the performance on MOSES or shapenet?"
                },
                "questions": {
                    "value": "1. To clarify, you are evaluating all datasets without hydrogens?\n2. The PS-VAE is not permutation equivariant right (or does it canonicalize things)? Did you do any experiments with a purely equivariant backbone?\n3. Purely because I found this [paper today](https://arxiv.org/abs/2210.02410) and found the idea exciting, if you manage to perform any diversity quantification using graph embedding similarity across the datasets, I'd be curious how the models differ. This is purely a nerd sharing a neat idea though, not a critique of the paper.\n4.  There is no limitations section which is always sus, are there really *no* downsides and limitations worth discussing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696084022,
            "cdate": 1698696084022,
            "tmdate": 1699636793264,
            "mdate": 1699636793264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FJ1pYVmWyW",
                "forum": "RSincg5RBe",
                "replyto": "nzzNHxdtnQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NAaX"
                    },
                    "comment": {
                        "value": "We greatly appreciate the reviewer's recognition of the soundness and contributions of our work. In response to the reviewer's request, we included the error bars indicating variance accross multiple seeds, as well as evaluation results on MOSES dataset. Additionally, we added a new metric, diversity, and the Limitation Section in the revised version.\n\n---\n### Weaknesses:\n**W1. I'd like to see error bars indicating variance accross multiple seeds if possible.**\n\n R1: Thank you for your suggestion. During the rebuttal period, we have conducted eight samplings with  different seeds and reported the mean and variance in the revised version. The new results are shown as below:\n\n **Table 1: Unconditional Generation Results on QM9 Dataset**\n Methods | Valid $\\uparrow$           | Unique$\\uparrow$           | Novelty$\\uparrow$          | Diversity$\\uparrow$        | FCD$\\downarrow$             | NSPDK MMD$\\downarrow$      |\n|-----------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|-------------------------------------|------------------------------------|\n| GDSS    | 0.957 $\\pm$ 0.000          | 0.982 $\\pm$ 0.003          | 0.988 $\\pm$ 0.001          | **0.925 $\\pm$ 0.000** | 2.959 $\\pm$ 0.040           | 0.003 $\\pm$ 0.000          |\n| DiGress | 0.992 $\\pm$ 0.003          | 0.960 $\\pm$ 0.001          | 0.391 $\\pm$ 0.001          | 0.920 $\\pm$ 0.000          | **2.123 $\\pm$ 0.033**  | **0.001 $\\pm$ 0.000** |\n| PS-VAE  | **1.000 $\\pm$ 0.000** | 0.981 $\\pm$ 0.002          | 0.996 $\\pm$ 0.000          | 0.881 $\\pm$ 0.000          | 16.877 $\\pm$ 0.059          | 0.059 $\\pm$ 0.000          |\n| GLDM    | **1.000 $\\pm$ 0.000** | 0.982 $\\pm$ 0.001          | 0.996 $\\pm$ 0.000          | 0.884 $\\pm$ 0.000          | 14.829 $\\pm$ 0.169          | 0.050 $\\pm$ 0.001          |\n| HGLDM   | **1.000 $\\pm$ 0.000** | **0.985 $\\pm$ 0.002** | **0.997 $\\pm$ 0.000** | 0.884 $\\pm$ 0.000          | 14.576 $\\pm$ 0.183          | 0.047 $\\pm$ 0.001          |\n\n**Table 2: Unconditional Generation Results on ZINC250K Dataset**\n| Methods | Valid $\\uparrow$           | Unique$\\uparrow$           | Novelty$\\uparrow$          | Diversity$\\uparrow$        | FCD$\\downarrow$             | NSPDK MMD$\\downarrow$      |\n|-----------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|-------------------------------------|------------------------------------|\n| GDSS    | **1.000 $\\pm$ 0.000** | 0.997 $\\pm$ 0.001          | **1.000 $\\pm$ 0.000** | 0.902 $\\pm$ 0.000          | 16.086 $\\pm$ 0.071          | **0.018 $\\pm$ 0.000** |\n| DiGress | 0.565 $\\pm$ 0.005          | **1.000 $\\pm$ 0.000** | **1.000 $\\pm$ 0.000** | 0.882 $\\pm$ 0.000          | **13.042 $\\pm$ 0.164** | 0.031 $\\pm$ 0.001          |\n | PS-VAE  | **1.000 $\\pm$ 0.000** | 0.993 $\\pm$ 0.001          | **1.000 $\\pm$ 0.000** | 0.912 $\\pm$ 0.000          | 20.386 $\\pm$ 0.061          | 0.085 $\\pm$ 0.001          |\n| GLDM    | **1.000 $\\pm$ 0.000** | 0.994 $\\pm$ 0.001          | **1.000 $\\pm$ 0.000** | 0.913 $\\pm$ 0.000          | 20.444 $\\pm$ 0.088          | 0.086 $\\pm$ 0.001          |\n | HGLDM   | **1.000 $\\pm$ 0.000** | 0.997 $\\pm$ 0.001          | **1.000 $\\pm$ 0.000** | **0.914 $\\pm$ 0.000** | 19.913 $\\pm$ 0.091          | 0.084 $\\pm$ 0.000          |\n\n**Table 3: Unconditional Generation Results on Guacamol Dataset**\n | Methods | Valid $\\uparrow$           | Unique$\\uparrow$           | Novelty$\\uparrow$          | Diversity$\\uparrow$        | FCD$\\downarrow$             | NSPDK MMD$\\downarrow$      |\n|-----------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|-------------------------------------|------------------------------------|\n| GDSS    | **1.000 $\\pm$ 0.000** | 0.986 $\\pm$ 0.001          | 0.996 $\\pm$ 0.001          | 0.892 $\\pm$ 0.000          | 40.291 $\\pm$ 0.072          | 0.058 $\\pm$ 0.000          |\n| DiGress | 0.875 $\\pm$ 0.005          | **1.000 $\\pm$ 0.000** | **0.999 $\\pm$ 0.001** | 0.904 $\\pm$ 0.000          | **12.069 $\\pm$ 0.051** | **0.018 $\\pm$ 0.000** |\n | PS-VAE  | **1.000 $\\pm$ 0.000** | 0.998 $\\pm$ 0.000          | 0.998 $\\pm$ 0.000          | **0.905 $\\pm$ 0.000** | 24.105 $\\pm$ 0.082          | 0.090 $\\pm$ 0.000          |\n | GLDM    | **1.000 $\\pm$ 0.000** | 0.998 $\\pm$ 0.000          | 0.998 $\\pm$ 0.000          | 0.904 $\\pm$ 0.000          | 23.879 $\\pm$ 0.041          | 0.095 $\\pm$ 0.000          |\n| HGLDM   | **1.000 $\\pm$ 0.000** | 0.999 $\\pm$ 0.001          | **0.999 $\\pm$ 0.000** | **0.905 $\\pm$ 0.000** | 23.845 $\\pm$ 0.098          | 0.095 $\\pm$ 0.001          |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570675129,
                "cdate": 1700570675129,
                "tmdate": 1700570675129,
                "mdate": 1700570675129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jkCNw2p37z",
                "forum": "RSincg5RBe",
                "replyto": "FJ1pYVmWyW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
                ],
                "content": {
                    "comment": {
                        "value": "Just to clarify: my comment was about training multiple models, not multiple samplings. All of these numbers come from a single model?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571081868,
                "cdate": 1700571081868,
                "tmdate": 1700571081868,
                "mdate": 1700571081868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O8HspEHwBh",
                "forum": "RSincg5RBe",
                "replyto": "oikGqJha9K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comments and additional evaluation. One slight pushback: The shapenet ablation would have been interesting for non-molecular data (and presumably, different latent factors). Molecules are also 3D objects."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571269351,
                "cdate": 1700571269351,
                "tmdate": 1700571269351,
                "mdate": 1700571269351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yVzW7hL4Go",
            "forum": "RSincg5RBe",
            "replyto": "RSincg5RBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_bWg3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6846/Reviewer_bWg3"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes a latent diffusion model for graph generation. The problem definition is very common and can be treated as a distribution-fitting problem. The framework utilizes PS-VAE as an encoding-decoding model. And apply a diffusion model over latent variables."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The presentation is good.\n2. The structure design for the diffusion model is reasonable."
                },
                "weaknesses": {
                    "value": "1. Equation (2) is not correct. The probability for each node is computed twice. I think the correct definition should be $\\prod p(x_i) \\prod \\prod (e_{ij})$.\n\n2. The submission claims that they first introduce the latent diffusion model into graph generation. This overclaims the contributions. [1] and many other previous works use latent diffusion models for graph generation tasks. I think the basic idea is the same: only diffuse node variables, and decode edge types from them. This is very common in the area.\n\n3. It is not reasonable to design a hierarchical diffusion model. There is no need to sample three variables $z^x, z^M, z^G$ at the same time. As a hierarchical model, the decoding process of PS-VAE is $ G \\sim q(G|z^M)q(z^M|z^G)$. That is, decoding a subgraph from a graph-level vector by a GRU, and then predicting the connection for the subgraphs. So actually, we only need to define a diffusion model over graph-level $z^G$. During the sampling process, we first sample $z^G$ from the diffusion model, and then use decoding of PS-VAE to get the graph. The current framework actually learns the decoding part twice, during the training of PS-VAE, the relationship between each level has been learned already. However, the diffusion model learns it one more time.\n\n4. The results lack MMD metrics. I think it is very important to check the distribution of the graphs.\n\n[1] https://arxiv.org/pdf/2211.10794.pdf"
                },
                "questions": {
                    "value": "1. \"However, these approaches sacrifice the random exploration ability to ensure that the final noisy data conforms to the appropriate discrete category distribution. \" Why do you make such claims? The definition for the distribution of the discrete variables is different. And people can also define a discrete diffusion process over them such as [1] and many other works. The performance is also very good and I think people should select models based on the specific problem. There is no any conclusion to support that continuous features is better than discrete process.\n\n[1]https://arxiv.org/pdf/2209.14734.pdf"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839796528,
            "cdate": 1698839796528,
            "tmdate": 1699636793167,
            "mdate": 1699636793167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BoYMk5wy53",
                "forum": "RSincg5RBe",
                "replyto": "yVzW7hL4Go",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bWg3"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the provided corrections and suggestions. After reading the comments in the weaknesses 1, 2, and 3 seriously, we are afraid that the reviewer has probably misunderstood the contributions and the model design of our paper. We will try our best to eliminate the misunderstandings via the following responses, and sincerely hope that the reviewer raises any further questions if our responses are still confused.\n\n---\n### Weaknesses:\n**W1. Equation (2) is not correct. The probability for each node is computed twice. I think the correct definition should be $\\prod p(x_i)\\prod\\prod(e_{ij})$.**\n\nR1: Thank you for your correction. We have revised it to the correct form in the revised version.\n\n**W2. The submission claims that they first introduce the latent diffusion model into graph generation. This overclaims the contributions. [1] and many other previous works use latent diffusion models for graph generation tasks. I think the basic idea is the same: only diffuse node variables, and decode edge types from them. This is very common in the area.**\n[1] https://arxiv.org/pdf/2211.10794.pdf\n\nR2: Sorry for the misunderstanding here. In the abstract, the word 'first' is meant to convey that we 'first' propose GLDM in this paper and then propose HGLDM. As proposed in the Introduction Section, **our main contribution is that we incorporate the latent diffusion framework and hierarchical structure to address the suboptimal diffusion results caused by using only graph-level embeddings**. Although latent diffusion has been adopted for graph generation in some related work, they only perform diffusion at the node level [1] and ignore the hierarchical information. In the revised version, we have modified the abstract to avoid the misunderstanding and included [1] in the Related Work section."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570339641,
                "cdate": 1700570339641,
                "tmdate": 1700570339641,
                "mdate": 1700570339641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]