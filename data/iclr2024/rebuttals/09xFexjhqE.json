[
    {
        "title": "AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework"
    },
    {
        "review": {
            "id": "iepKR6e3l5",
            "forum": "09xFexjhqE",
            "replyto": "09xFexjhqE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission538/Reviewer_qytY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission538/Reviewer_qytY"
            ],
            "content": {
                "summary": {
                    "value": "**Summary Of The Paper:**\n\nThis paper introduces AutoLoRa, a parameter-free automated robust fine-tuning framework to improve adversarial robustness in downstream tasks that disentangles the optimization process into two distinct components: (1) optimizing natural objectives via the LoRa branch, (2) and adversarial objectives via the FE. It addresses the issue of divergent gradient directions, when optimizing both adversarial and natural objectives through the feature extractor (FE), in existing robust fine-tuning methods and achieves state-of-the-art results across various tasks without the need for hyperparameter tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Strength:**\n\n-   The motivation that optimizing both adversarial and natural objectives through feature extractor yields divergent gradient directions make sense. The proposed disentangling of the training objective by introducing the LoRA branch is consistent with the motivation.\n-   Extensive empirical evaluation (including the P-test) demonstrates the improvements in Robust Fine-Tuning on various tasks."
                },
                "weaknesses": {
                    "value": "**Weakness:**\n\n-   The reason for automating scheduling hyper-parameters is not well illustrated, and the ablation study in Table 5 can not show its superiority, especially for the *RA* metric.\n-   In Formula 7, the constant factor $6$ is not well-explained, and it could be considered as a hyper-parameter with further ablation study.\n-   Table 4 is confusing. Specifically, when the adversarial budget is set to $8$ which is the default configuration, the resulting metric is supposed to be that in Table 2. However, this is not true.\n-   Typo in Section 5.2 Ablation Study, the end of adversarial budgets paragraph says \"consistently achieves consistently\".\n-   Diverse network backbone architectures are encouraged to be considered beyond ResNet."
                },
                "questions": {
                    "value": "Refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission538/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698535701854,
            "cdate": 1698535701854,
            "tmdate": 1699635981185,
            "mdate": 1699635981185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aBLYOBWwpz",
                "forum": "09xFexjhqE",
                "replyto": "iepKR6e3l5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qytY"
                    },
                    "comment": {
                        "value": "Many thanks for your comments! Please find our replies below.\n\n> [Reply to W1.1] TWINS requires expensive computation to grid-search the appropriate hyperparameters, which motivates us to automate the scheduler of hyper-parameters. \n\nTWINS [1] is sensitive to hyper-parameters, such as the initial learning rate (LR) as shown in the following table.\n\n|Initial LR for TWINS| SA | AA|\n|-|-|-|\n|0.001| 61.54 | 23.51 |\n|0.01| 63.03 | 25.45 |\n|0.03 | 1.00 | 1.00 |\n\nTherefore, the authors of TWINS [1] conducted a time-consuming grid-search for hyper-parameters including the initial LR $\\eta^{(0)} \\in \\\\{3e-4,1e-3,3e-3,1e-2,3e-2\\\\}$, the weight decay $\\mathrm{WD} \\in \\\\{1e-5,1e-4,1e-3,1e-2\\\\}$, and the scalar $\\gamma \\in \\\\{ 0.1,0.2,0.3,0.4,0.5,1.0\\\\}$, which aimed to achieve the state-of-the-art (SOTA) performance. It means that TWINS needs to repeat $120$ ($= 5 \\times 4 \\times 6$) runs of fine-tuning in order to achieve the SOTA performance for each backbone and each downstream task, which is extremely time-consuming and inconvenient for practical usage. \n\n\nBy leveraging our proposed automatic scheduler, AutoLoRa can automatically achieve the SOTA performance by *ONE* run of robust fine-tuning on various models and various downstream tasks.\n\n> [Reply to W1.2] We make the first effort to propose an automated LR scheduler to avoid expensive grid-searching of the hyper-parameters. \n\nIn Table 5, TWINS w/o LR scheduler carefully selects the appropriate hyper-parameters for each downstream task via expensive grid-search. Table 5 shows that TWINS w/ the LR scheduler can achieve comparable performance to TWINS w/o the LR scheduler, which validates the effectiveness of the LR scheduler in avoiding grid-search.\n\n> [Reply to W2] We provide extensive results under various factors in Eq. (7). \n\nWe denote the factor as $\\lambda_2^{\\max} \\geq 0$, i.e., $\\lambda_2^{(e)} = \\lambda_2^{\\max} \\cdot \\mathrm{SA}(\\mathcal{D}_{\\mathrm{train}}; \\\\{\\theta_1^{(e)}+B^{(e)}A^{(e)}, \\theta_2^{(e)}\\\\})^\\alpha$ (refer to Eq. (7)). \nThere is a trade-off between standard accuracy and robust accuracy under AutoAttack (AA) by adjusting the factor $\\lambda_2^{\\max} $. We set $\\lambda_2^{\\max}=6.0$ by default to achieve good adversarial robustness without sacrificing too much standard generalization.\n\n|$\\lambda_2^{\\max}$ | SA | AA |\n|-|-|-|\n| 3.0 | 63.43 | 26.91 | \n| 6.0 | 62.10 | 27.48 | \n| 9.0 | 61.38 | 27.51 | \n\nThe experiments are conducted on CIFAR-100 using ResNet-18.\n\n> [Reply to W3] In Table 4, we report performance using different adversarially pre-trained FEs under $\\epsilon_{\\mathrm{pt}} \\in \\\\{ 0, 1/255,2/255, 4/255, 8/255\\\\}$. In Table 2, we report performance using the adversarially pre-trained FE under $\\epsilon_{\\mathrm{pt}} = 4/255$, which is the same setting as TWINS.\n\n$\\epsilon_{\\mathrm{pt}}$ refers to the adversarial budget used during the *pre-training* phase. We kept the adversarial budget as $8/255$ during the fine-tuning and evaluation phase.\nYou can find that the results of $\\epsilon_{\\mathrm{pt}} = 4/255$ in Table 4 are consistent with the results in Table 2.\n\n> [Reply to W4] Thanks for pointing out this typo! We have revised it in our manuscript.\n\n> [Reply to W5] We provide extensive results on Wide ResNet of depth 28 and width 10 (WRN-28-10), which validate that AutoLoRa is compatible with various models.\n\n|WRN-28-10|SA|RA|\n|-|-|-|\n|Vanilla RFT| 61.56 | 35.64 |\n|AutoLoRa| **62.67** | **36.43**|\n\nWe could not provide the results of TWINS because the authors of TWINS [1] did not implement TWINS on other models except for ResNet. We used the pre-trained WRN-28-10 on ImageNet-1K provided by [2] and applied Vanilla RFT and AutoLoRa on the CIFAR-100 task. *RA* refers to the robust test accuracy under PGD-20.\n\n---\n\n> [Update **extensive results** for mitigating W5] Besides existing experiments on  *ResNet-18*, *ResNet-50*, and *WRN-28-10*, we provide results on extra pre-trained models, i.e., *Vision Transformer (ViT)* and *Data-Efficient Image Transformers (DeiT)* to validate the effectiveness of AutoLoRa on various model structures.\n \n1. [ViT (S/16)](https://github.com/google-research/vision_transformer)\n\n|ViT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 80.31 | 51.06 |\n|AutoLoRa| **80.97** | **51.51**|\n\n2. ViT (B/16)\n\n|ViT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 83.87 | 53.39 |\n|AutoLoRa| **84.79** | **54.10**|\n\n3. [DeiT (DeiT-Tiny)](https://github.com/facebookresearch/deit)\n   \n|DeiT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 78.85 | 49.72 |\n|AutoLoRa| **79.49** | **50.52**|\n\n4. DeiT (DeiT-small)\n   \n|DeiT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 81.73 | 51.92 |\n|AutoLoRa| **82.33** | **52.63**|\n\n---\n*References:*\n\n[1] TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization, CVPR 2023.\n\n[2] Using Pre-Training Can Improve Model Robustness and Uncertainty, ICML 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699877943374,
                "cdate": 1699877943374,
                "tmdate": 1700271978708,
                "mdate": 1700271978708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4pSdqfh9Tj",
            "forum": "09xFexjhqE",
            "replyto": "09xFexjhqE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
            ],
            "content": {
                "summary": {
                    "value": "Robust fine-tuning (RFT) is an efficient method to obtain a robust model on a downstream task by robustly fine-tuning a robust model that is adversarially trained on a large dataset. \nHowever, this paper shows that the existing methods (vanilla RFT, TWINS [Liu et al. 2023]) suffer from the same issue that optimizing both adversarial and natural objectives yields significantly divergent gradient directions.\nThis paper points out that this divergence can hinder obtaining high robustness and make the training unstable.\n\nTo resolve the issue of the divergent gradient directions, this paper proposes a robust fine-tuning framework called AutoLoRa using the low-rank adaptation (LoRA) technique. \nThe idea is to have separate branches for adversarial and natural objectives: the adversarially pre-trained encoder is trained by the adversarial objective, and the LoRa branch is trained by the natural objective.\n\nAdditionally, AutoLoRa introduces automatic scheduling of hyperparameters, in contrast to vanilla RFT and TWINS, which require expensive hyperparameter searches. The balance between adversarial and natural objectives is determined by natural accuracy on the train set: as the standard accuracy increases, the weight on the natural objective decreases. The learning rate decays automatically with a condition of the validation accuracy.\n\nIn this paper, the pre-trained models are adversarially trained on ImageNet-1k. \nOn the six downstream datasets (CIFAR10, CIFAR100, DTD-57, DOG-120, CUB-200, and Caltech-256), AutoLoRa achieves higher adversarial robustness compared to vanilla RFT and TWINS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper points out the common issue of vanilla RFT and TWINS [Liu et al. 2023] that optimizing both adversarial and natural objectives yields significantly divergent gradient directions, which is beneficial knowledge for the adversarial robustness research community.\n\nS2. The application of LoRA for the adversarial robustness problem is novel, and separating the branches for adversarial and natural objectives with the low-rank branch is interesting. The results show that their method can effectively improve adversarial robustness.\n\nS3. The proposed automatic strategy to determine hyperparameters is useful since adversarial training is time-consuming."
                },
                "weaknesses": {
                    "value": "W1. A more careful ablation study is needed.\n\n- (W1-1.) It remains unclear how much each of the two proposed components contributes to the performance: (1) the automatic hyperparameter scheduling and (2) the LoRa branch. To clarify, does AutoLoRa without automatic scheduling (utilizing grid search) yield a similar result to AutoLoRa with dynamic hyperparameter scheduling? In other words, does the dynamic hyperparameter scheduling contribute to the robustness improvement, or is it only for avoiding grid search? \n\n- (W1-2.) In line with (W1-1), it is also not evident how significantly the learning rate scheduling and the scalar parameter ($\\lambda$) scheduling impact performance. While it appears that dynamic learning rate scheduling might not have the benefit of improving robustness, as seen in Table 5, I assume that dynamic scalar ($\\lambda$) scheduling could contribute positively to performance, in addition to the benefit of avoiding grid search.\n\nW2. The paper's claim regarding divergent gradient directions and training stability needs clarification.\n\n- (W2-1.) The paper lacks evidence to support the claim that Vanilla RFT and TWINS are sensitive to hyperparameters. It would be helpful to specify which hyperparameters these methods are sensitive to and to what extent.\n- (W2-2.) Since AutoLoRa employs automatic hyperparameter scheduling, it remains unverified whether the use of the LoRa branch indeed contributes to training stability regarding hyperparameters. To discuss the hyperparameter sensitivity, I would expect experiments comparing the different magnitudes of a specific hyperparameter and the corresponding performances for compared training methods.\n\nW3: Natural accuracy trade-off in AutoLoRa compared to TWINS.\n- For all cases in ResNet-18 and the three cases in ResNet-50, AutoLoRa exhibits a slightly lower natural accuracy compared to TWINS, despite achieving higher robust accuracy. Further discussion or insights on this trade-off would be valuable."
                },
                "questions": {
                    "value": "Q1. Related to W1-2, it's worth considering the possibility of applying automated hyperparameter scheduling to Vanilla RFT and TWINS.   The scaler $\\beta$ in Vanilla RFT or $\\gamma$ in TWINS can be scheduled, by simply replacing $\\lambda_2$ with $\\beta$ or $\\gamma$ in Equation 7. It would be interesting to see whether \"Vanilla RFT + scaler scheduling\" or \"TWINS + scaler scheduling\" can be better than the original methods. Additionally, comparing \"Vanilla RFT + scaler scheduling\" or \"TWINS + scaler scheduling\" with AutoLoRa could provide insights into the benefits of the LoRa branch.\n\nQ2. How exactly is the gradient similarity calculated? A feature encoder has multiple layers to measure the gradient similarity. \n\nMinor comment:\n- It appears that TWINS in this paper corresponds to the TRADES version of TWINS, known as TWINS-TRADES [Liu et al. 2023]. It might help clarify the paper's context by explicitly mentioning this relationship.\n\n-------\n[Liu et al. 2023] Twins: A fine-tuning framework for improved transferability of adversarial robustness and generalization. CVPR2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission538/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698580151495,
            "cdate": 1698580151495,
            "tmdate": 1699635981101,
            "mdate": 1699635981101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B2dvjlAGd0",
                "forum": "09xFexjhqE",
                "replyto": "4pSdqfh9Tj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4DPH (Part 1)"
                    },
                    "comment": {
                        "value": "Many thanks for your comments! Please find our replies below.\n\n> [Reply to W1] We provide extensive results of AutoLoRa with or without the automated scheduler.\n\n|Automated LR scheduler|Automated scalar scheduler| SA | AA |\n|-|-|-|-|\n|&#10003;|&#10003;| 62.10 | 27.48 |\n|&#10003;|&#10005;| 61.96 | 27.21 |\n|&#10005;|&#10003;| 62.16 | 27.54 |\n|&#10005;|&#10005;| 61.64 | 27.29 |\n\nBy comparing Row 1 with Row 2 in the above table, we can find that the automated scalar scheduler is beneficial to performance improvement and avoid the expensive grid-search.\n\nBy comparing Row 1 with Row 3, we can find that the automated LR scheduler can help obtain the satisfied performance without expensive grid-searching for hyper-parameters.\n\nThe above table uses ResNet-18 on the CIFAR-100 dataset. Without the automated scalar scheduler, we set $\\lambda_1=0.5$ and $\\lambda_2=6.0$. Without the automated LR scheduler, we set the initial learning rate as 0.01 and the learning rate is divided by 10 and 100 at Epoch 30 and 50.\n\n> [Reply to W2.1] We detailed the issue of sensitivity to hyper-parameters as follows.\n\nAccording to TWINS, Vanilla RFT and TWINS are senstive to the initial learning rate (LR), the weight decay, and the scalars. Therefore, the authors of TWINS conducted an expensive grid search on the aforementioned three hyper-parameters for Vanilla RFT and TWINS to obtain the best performance.\n\nThe following results show that Vanilla RFT and TWINS are sensitive to the initial LR caused by poor training stability. For example, when the initial LR is sampled from \\{0.001, 0.1, 0.03\\}, the robust accuracy (AA) ranges from 18.29 to 24.07 obtained by Vanilla RFT and 1.00 to 25.45 obtained by TWINS. \n\n|Method|Initial LR| SA | AA|\n|-|-|-|-|\n|Vanilla RFT| 0.001| 58.55 | 24.07 |\n|Vanilla RFT| 0.01 | 52.19 | 20.96 | \n|Vanilla RFT| 0.03 | 49.10 | 18.29 |\n|TWINS| 0.001| 61.54 | 23.51 |\n|TWINS| 0.01| 63.03 | 25.45 |\n|TWINS| 0.03 | 1.00 | 1.00 |\n|AutoLoRa w/o automated LR scheduler| 0.001 | 61.97 | 27.51 |\n|AutoLoRa w/o automated LR scheduler| 0.01 | 62.88 | 27.21 |\n|AutoLoRa w/o automated LR scheduler| 0.03 | 62.21 | 27.33 |\n\nThe experiments are conducted on CIFAR-100 using ResNet-18. The LR is divided by 10 and 100 at Epoch 30 and 50, following TWINS. Note that, TWINS fails to converge when the initial LR is 0.03.\n\n> [Reply to W2.2] We empirically validate that AutoLoRa mitigates the issue of sensitivity to hyper-parameters (e.g., the initial LR).\n\nWe can find that AutoLoRa is less sensitive to hyper-parameters such as the initial LR since AutoLoRa's standard deviation of the accuracy among different initial LRs is very small compared to Vanilla RFT and TWINS.\n\n|Method|Standard deviation of SA|Standard deviation of AA|\n|-|-|-|\n|Vanilla RFT| 3.934 | 2.362 |\n|TWINS|28.896|11.097|\n|AutoLoRa|**0.385**|**0.123**|\n\nAccording to the table in **[Reply to W2.1]**, we show the standard deviation of the accuracy among different initial LRs in the above table.\n\n\n> [Reply to W3] The trade-off between robust and standard test accuracy could be mitigated by utilizing larger models. Further, we figure out a hyper-parameter (i.e., $\\lambda_2^{\\max}$ in Eq. (7)) that can adjust this trade-off.\n\nThe trade-off between accuracy and adversarial robustness ubiquitously exists [1,2]. By comparing the results on CIFAR-10 and CIFAR-100 in Tables 1 and 2, we can find that utilizing larger models (that contain more trainable parameters) could mitigate the trade-off since both standard and robust accuracy get improved using ResNet-50.\n\nFurther, we find that the hyper-parameter $\\lambda_2^{\\max}$ in Eq. (7), which is used for controlling the scale $\\lambda_2^{(e)}$, can adjust the trade-off. We set $\\lambda_2^{\\max}=6.0$ by default in our paper. The following table shows that an appropriate $\\lambda_2^{\\max}$ for AutoLoRa (e.g., $\\lambda_2^{\\max}=3.0$) can yield improvement on both the standard and robust accuracy compared to TWINS.\n\n| | SA | AA |\n|-|-|-|\n|TWINS| 63.03 | 25.45 |\n| AutoLoRa ($\\lambda_2^{\\max}=3.0$) | **63.43** | 26.91 | \n| AutoLoRa ($\\lambda_2^{\\max}=6.0$) | 62.10 | 27.48 | \n| AutoLoRa ($\\lambda_2^{\\max}=9.0$) | 61.38 | **27.51** | \n\nThe experiments are conducted on CIFAR-100 using ResNet-18."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699877642330,
                "cdate": 1699877642330,
                "tmdate": 1699878842276,
                "mdate": 1699878842276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wV2Xa3B2jM",
                "forum": "09xFexjhqE",
                "replyto": "4pSdqfh9Tj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4DPH (Part 2)"
                    },
                    "comment": {
                        "value": "> [Reply to Q1] We demonstrate the results of ''Vanilla RFT + scalar scheduling'' and \"TWINS + scalar scheduling\" as follows.\n\nThe results validate that the scalar scheduler is applicable to Vanilla RFT and TWINS. Besides, it justifies the effectiveness of the auxiliary LoRa branch in enhancing performance.\n\n\n| RFT Method | SA | AA |\n|-|-|-|\n| Vanilla RFT | 58.55 | 24.07 |\n| Vanilla RFT + scalar scheduling | **59.09** | **24.43** |\n| TWINS | 63.03 | 25.45 |\n| TWINS + scalar scheduling | **63.14** | **25.87** |\n| AutoLoRa | 62.10 | 27.48 |\n\nThe experiments are conducted on CIFAR-100 using ResNet-18. Following your suggestion, the scalar scheduler for $\\lambda_2$ is applied to $\\beta$.\n\n> [Reply to Q2] We provide a detailed illustration of calculating gradient similarity as follows.\n\nAt each epoch, for each convolutional layer of the FE, we obtain a gradient similarity value according to Eq. (3) and (4). Then, we demonstrate the average and the standard deviation of all the convolutional layers' gradient similarity values in the FE at each epoch in Figure 1(a). \n\n> [Reply to Minor Comments] Thanks for your suggestion! We mention the relationship in Appendix A.1.\n\n*References:*\n\n[1] Theoretically Principled Trade-off between Robustness and Accuracy, ICML 2019.\n\n[2] Understanding and Mitigating the Tradeoff Between Robustness and Accuracy, ICML 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699877721351,
                "cdate": 1699877721351,
                "tmdate": 1699878867779,
                "mdate": 1699878867779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iDj1XAXX8F",
                "forum": "09xFexjhqE",
                "replyto": "B2dvjlAGd0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Regarding W1:\nThank you so much.\nI observe that auto-scheduling mainly avoids the expensive grid search rather than improves accuracy.\n\nRegarding W2.1, W2.2:\nThe provided tables look good. \nI recommend the authors add in the paper to clarify their claim that \"Vanilla RFT and TWINS are hyperparameter-sensitive, and AutoLoRa is not.\"\nHowever, I believe that only considering the initial learning rate is insufficient to claim \"using LoRa branch reduces sensitivity to hyper-parameters.\"\n\nRegarding W3:\nThank you so much. I understand that AutoLoRa seems to require a capacity of network to mitigate clean-robust trade-off."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645169364,
                "cdate": 1700645169364,
                "tmdate": 1700645169364,
                "mdate": 1700645169364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FeSqSJcpCz",
                "forum": "09xFexjhqE",
                "replyto": "iDj1XAXX8F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_4DPH"
                ],
                "content": {
                    "title": {
                        "value": "Response (part 2)"
                    },
                    "comment": {
                        "value": "Regarding Q1:\nThis table looks nice to validate the effectiveness of the proposed automated hyperparameter scheduling.\nThank you so much.\n\nRegarding Q2. \nThank you for the clarification. Layer-wise analysis could be interesting for future research."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645437833,
                "cdate": 1700645437833,
                "tmdate": 1700645437833,
                "mdate": 1700645437833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MOU33ucSMy",
            "forum": "09xFexjhqE",
            "replyto": "09xFexjhqE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission538/Reviewer_kvxv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission538/Reviewer_kvxv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces AutoLoRa, an automated robust transfer learning framework. The authors present empirical evidence showing a significant divergence in the gradients of adversarial and natural objectives with respect to the feature extractor (FE), leading to unstable optimization. This observation motivates the authors to propose an auxiliary low-rank (LoRa) branch to disentangle the robust fine-tuning process, enabling the optimization of natural objectives through the LoRa branch and adversarial objectives through the FE.\n\nAdditionally, the authors introduce automatic schedulers for adjusting the learning rate and loss weights. The empirical results demonstrate that AutoLoRa achieves state-of-the-art robustness in downstream tasks without the need for hyperparameter search."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-organized and well-written, making it easy to follow most parts of the paper.\n\n2. The proposed method is well-motivated. The authors empirically discover that optimizing the natural and adversarial objectives leads to divergent optimization directions, which serves as the motivation for the LoRa branch.\n\n3. The comprehensive results across various datasets provide strong support for the effectiveness of AutoLoRa.\n\n4. AutoLoRa is parameter-free, offering practical utility. Additionally, the automated learning scheduler is adaptable to different methods."
                },
                "weaknesses": {
                    "value": "1. The backbone models are pre-trained through robust supervised learning. It would be beneficial to demonstrate the performance of various backbone models pre-trained using robust self-supervised learning with AutoLoRa.\n\n2. Robustness is currently assessed using AutoAttack. It would be more informative to assess the robustness under various attackers."
                },
                "questions": {
                    "value": "Refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Reviewer_kvxv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission538/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726224214,
            "cdate": 1698726224214,
            "tmdate": 1700584660896,
            "mdate": 1700584660896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cjGswaqZiA",
                "forum": "09xFexjhqE",
                "replyto": "MOU33ucSMy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kvxv"
                    },
                    "comment": {
                        "value": "Many thanks for your comments! Please find our replies below.\n\n> [Reply to W1] AutoLoRa is applicable to fine-tuning the pre-trained models via self-supervisions.\n\nWe applied AutoLoRa to finetune the pre-trained models via robust self-supervision including DynACL [1] and DynACL-AIR [2]. The pre-training and fine-tuning were conducted on CIFAR-10 and STL-10 datasets, respectively. We report the performance in the following table. Here, the robust accuracy is evaluated by AutoAttack.\n\n|Self-supervised method|Standard accuracy by Vanilla RFT|Robust accuracy by Vanilla RFT|Standard accuracy by AutoLoRa|Robust accuracy by AutoLoRa|\n|-|-|-|-|-|\n|DynACL-AIR [2]| 35.66 | 63.74 | **35.88** | **64.25** |\n|DynACL [1]| 35.25 | 63.53 | **35.51** | **64.16** |\n\nThe results validate that AutoLoRa can improve both standard and robust test accuracy of the pre-trained models via self-supervisions in the downstream task.\n\n> [Reply to W2] AutoLoRa achieves better robustness under various attacks than Vanilla RFT and TWINS.\n\nWe evaluated the robustness under three strong white-box attacks (APGD-CE [3], APGD-DLR [3] and FAB [4]) and one strong black-box attack (i.e., Square Attack [5]). We evaluate the robustness on CIFAR-10 of the pre-trained ResNet-18 after finetuning in the following table.\n\n|Method|APGD-CE [3]|APGD-DLR [3]|FAB [4]|Square Attack [5]|\n|-|-|-|-|-|\n|Vanilla RFT|51.91|49.05|47.94|55.81|\n|TWINS|51.76|49.46|48.40|56.08|\n|AutoLoRa|**52.06**|**49.91**|**48.87**|**56.38**|\n\nThe experiments are conducted on CIFAR-10 using ResNet-18.\n\n*References:*\n\n[1] Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning, ICLR 2023.\n\n[2] Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization, NeurIPS 2023.\n\n[3] Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, ICML 2020. \n\n[4] Minimally distorted adversarial examples with a fast adaptive boundary attack, ICML 2020. \n\n[5] Square attack: a query-efficient black-box adversarial attack via random search, ECCV 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699877508007,
                "cdate": 1699877508007,
                "tmdate": 1699877508007,
                "mdate": 1699877508007,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gmQBWnp2FH",
                "forum": "09xFexjhqE",
                "replyto": "cjGswaqZiA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_kvxv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_kvxv"
                ],
                "content": {
                    "comment": {
                        "value": "Hi Authors, \n\nThanks for your responses and additional empirical results. After reading your responses, my concerns have been well \naddressed. \n\nBesides, I have read other reviewers\u2019 comments, and I am convinced that AutoLoRa is a powerful and automatic robust fine-tuning method, which can be a practical tool and a good addition to the community.  \nFurthermore, to my best of knowledge, the application of low-rank adaptation to robust fine-tuning is novel. \n\nThus, I increase my score and confidence to support this paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584772156,
                "cdate": 1700584772156,
                "tmdate": 1700584772156,
                "mdate": 1700584772156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "deEv3TQoyY",
            "forum": "09xFexjhqE",
            "replyto": "09xFexjhqE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a decoupled fine-tuning framework for learning adversarially robust features. Specifically, a conventional robust finetuning pipeline consists of two losses: a natural objective and an adversarial objective. The paper shows that the divergence in gradients from the two objectives is correlated with downstream accuracy on robustness benchmarks. Therefore, it proposes to decouple a model into two branches where the second branch is constructed using Low-rank adaptation (LORA). In the decoupled training scheme, the main model (first branch) is only exposed to the adversarial objective and the LORA branch (second branch) is only exposed to the natural objective. The paper claims that the disentanglement avoids gradient divergence and leads to better downstream robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* **Clear presentation**: the paper uses clear and concise notations for equations. The method section is easy to follow.\n\n* **Good ablation study**: the paper conducts ablation study on important hyper-parameters such as the rank in LORA and the LR scheduler."
                },
                "weaknesses": {
                    "value": "* **Missing ablation on Equation 5**: Equation 5 is the main loss function of the proposed method, which has three terms. The second cross-entropy term is a new addition $L_{KL}(h_{\\theta}(\\tilde{x}),y)$ in this paper and is not motivated and ablated in the experiments.  \n* **Doubt on mitigating divergence**: a main motivation of the method is that it can avoid divergent gradient updates on the main model parameters. However, this is not validated through experiments explicitly. For example, even though the main model parameters are not directly trained on the natural objective, it is indirectly affected by the natural objective through the KL divergence. Moreover, the unexplained second cross-entropy term $L_{KL}(h_{\\theta}(\\tilde{x}),y)$ can be seen as a natural objective on perturbated input. \n* **Why parameter-free**: the claim on parameter-free can be confusing. The model not only fine-tunes the main model parameters but also additional LORA parameters. So, it is not parameter-free in the sense of fine-tuning. Even though the method reduces the need for extensive hyper-parameter tuning, the design choices of the automated scheduler and the rank selection for LORA are all hyper-parameters. It\u2019s not clear what aspect of the proposed method is parameter-free."
                },
                "questions": {
                    "value": "* Could the authors provide the cosine similarity between the two terms in the adversarial objective in Equation 5 and comment on the functionality of the second cross-entropy term $L_{KL}(h_{\\theta}(\\tilde{x}),y)$? \n\n* Could the authors clarify the parameter-free characteristic? It could be that I misunderstood the meaning here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK",
                        "ICLR.cc/2024/Conference/Submission538/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission538/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772887917,
            "cdate": 1698772887917,
            "tmdate": 1700492923885,
            "mdate": 1700492923885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fEuxq8dJUi",
                "forum": "09xFexjhqE",
                "replyto": "deEv3TQoyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for your comments! Please find our replies below.\n\n> [Reply to W1&Q1.2] The second cross-entropy (CE) term $\\ell_{\\mathrm{CE}}(h_{\\theta}(\\tilde{x},y))$ aims to provide *ground-truth labels* for the FE to learn the adversarial data, which can improve the adversarial robustness as validated in the following table.\n\nWithout the second CE term, the FE would be only regulated by the KL term that uses the natural soft labels generated by the LoRa branch. However, the natural soft labels could contain wrongly classified labels, which could degrade the performance. Empirically, we found that the natural training error evaluated on the LoRa branch keeps about $30\\\\%$ on CIFAR-100, which indicates that there are some noisy labels in natural soft labels generated by the LoRa branch. Thus, it necessitates the second CE term to provide the ground-truth labels for updating the FE together, which can further improve performance.\n\nThe following empirical results validate that *with* the second CE term, both adversarial robustness and standard generalization achieved by AutoLoRa get improved. The experiments are conducted on CIFAR-100 using ResNet-18.\n\n| | SA | AA |\n|-|-|-|\n| Without the second CE term (setting $\\lambda_1 = 1.0$) | 61.84 | 27.07 |\n| With the second CE term (setting $\\lambda_1 = 0.5$) | 61.96 | 27.21 |\n| With the second CE term (dynamic scheduling $\\lambda_1$) | **62.10** | **27.48** |\n\n> [Reply to W2&Q1.1] AutoLoRa mitigates the issue of divergent gradient directions, which is further empirically validated in Figure 3 (in Appendix A.4). \n\nFigure 3 shows that AutoLoRa achieves high cosine similarity between the two terms of the adversarial objective in Eq. (5), which empirically justifies that AutoLoRa mitigates divergence.\n\nNote that both the second CE term and the KL term aim to let the FE learn the adversarial data. In this way, the LoRa branch disentangles the model to learn the adversarial and natural data via the FE and the LoRa branch, respectively. Thus, AutoLoRa can mitigate the issue of divergent gradient directions.\n\n> [Reply to Q2] The term *parameter-free* refers to that you do not need to specify any hyper-parameters (e.g., the initial learning rate, the learning rate scheduler, the scalars) when applying AutoLoRa to various downstream tasks (e.g., CIFAR, DTD-57, CUB-200) and various models (e.g., ResNet-18, ResNet-50).\n\nThe term *parameter-free* is inspired by the title of AutoAttack [1], in which *parameter-free* refers to that you do not need to specify any hyper-parameters while utilizing the AutoAttack to attack various models on various datasets. \n\nAutoLoRa is *parameter-free* benefiting from our proposed automated scheduler for the learning rate and the scalars. We empirically validate that AutoLoRa is *parameter-free* since AutoLoRa only needs to specify the downstream dataset and the model, without specifying any other hyper-parameters, to automatically obtain superior performance in downstream tasks.\n\n*References:* \n\n[1] Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, ICML 2020."
                    },
                    "title": {
                        "value": "Response to Reviewer pJnK"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699877345395,
                "cdate": 1699877345395,
                "tmdate": 1699877402014,
                "mdate": 1699877402014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fk1akO0p7M",
                "forum": "09xFexjhqE",
                "replyto": "fEuxq8dJUi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. I appreciate the additional visualization on the cosine similarity between gradients of the AutoLORA method. \n \nNow I understand that the parameter-free property refers to the automatic hyper-parameter scheduling. However, I am still not completely convinced by the parameter-free claim in the paper. \n\nThe hyper-parameter scheduler is an empirical function of the robust accuracy, standard accuracy, a hyper-parameter alpha (Eq.6,7), and another hyper-parameter that balances between the standard accuracy and the robust accuracy (Eq.7). \n\nFirst, the empirical nature of this scheduler makes it less convincing unless more experiments are provided, e.g., experiments beyond ResNet as suggested by Reviewer qytY. Second, even though the hyper-parameters in the scheduler use default values in existing experiments, there is no guarantee that they will perform well on other datasets and architectures. In other words, the validity of the parameter-free claim depends on what experiments are demonstrated in the paper and its generalizability cannot be tested. \n\nIn my opinion, the scheduler is a good idea inspired by heuristics. It could be that with the proposed scheduler, the optimization becomes less sensitive to the choice of hyper-parameters and is easier to tune. While this weakens the claim in the paper, it is much more reasonable and still makes a good contribution."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974927868,
                "cdate": 1699974927868,
                "tmdate": 1699974927868,
                "mdate": 1699974927868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J6kmFy0rHO",
                "forum": "09xFexjhqE",
                "replyto": "deEv3TQoyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We provide extensive results to strengthen our parameter-free claim."
                    },
                    "comment": {
                        "value": "First, many thanks for your commending that our heuristic scheduler is good and makes a good contribution!\n\nSecond, following your suggestions, we provide results on extensive model structures and extensive downstream datasets to strengthen our parameter-free claim.\n\n> Besides existing experiments on  *ResNet-18* and *ResNet-50*, we provide results on three extra pre-trained models including *Wide ResNet (WRN), Vision Transformer (ViT)*, and *Data-Efficient Image Transformers (DeiT)* to strengthen our parameter-free claim. \n\n1. [WRN with depth 28 and width 10](https://github.com/hendrycks/pre-training/tree/master/robustness/adversarial)\n\n|WRN|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 61.56 | 35.64 |\n|AutoLoRa| **62.67** | **36.43**|\n\n2. [ViT (S/16)](https://github.com/google-research/vision_transformer)\n\n|ViT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 80.31 | 51.06 |\n|AutoLoRa| **80.97** | **51.51**|\n\n3. [DeiT (DeiT-tiny)](https://github.com/facebookresearch/deit)\n   \n|DeiT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 78.85 | 49.72 |\n|AutoLoRa| **79.49** | **50.52**|\n\n4. [DeiT (DeiT-small)](https://github.com/facebookresearch/deit)\n   \n|DeiT|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 81.73 | 51.92 |\n|AutoLoRa| **82.33** | **52.63**|\n\n---\n\n**[Update extensive results]**\n\n5.  [ViT (B/16)](https://github.com/google-research/vision_transformer)\n\n|ViT (B/16)|Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 83.87 | 53.39 |\n|AutoLoRa| **84.79** | **54.10**|\n\n---\n\nAs for WRN, pre-training and fine-tuning are conducted on ImageNet-1K and CIFAR-100, respecitve. As for ViT and DeiT,pre-training and fine-tuning are conducted on ImageNet and CIFAR-10, respecitve. We do not provide the results of TWINS since TWINS is only implemented on ResNet.\n\n> Besides existing experiments on *CIFAR-10, CIFAR-100, DTD-57, DOG-120, CUB-200*, and *Caltech-256*, we provide results on three extra downstream dataests including *STL-10, Caltech-101*, and *Cars-196* to strengthen our parameter-free claim. \n\n1. [STL-10](https://cs.stanford.edu/~acoates/stl10/)\n\n|STL-10| Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 63.74 | 35.66 | \n|AutoLoRa| **64.25** |**35.88** | \n\n2. [Caltech-101](https://data.caltech.edu/records/mzrjq-6wc02) \n\n|Caltech-101| Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT| 84.72| 60.46 | \n|TWINS| 87.83 | 61.59 | \n|AutoLoRa| **90.01** |**62.29** | \n\n3. [Cars-196](https://www.tensorflow.org/datasets/catalog/cars196) \n\n|Cars-196| Standard accuracy|Robust accuracy|\n|-|-|-|\n|Vanilla RFT | 65.89 | 37.21 | \n|TWINS| 68.48 | 39.69 | \n|AutoLoRa| **70.23** |**41.19** |  \n\nAs for STL-10, we used pre-trained ResNet-18 via adversarial contrastive learning on CIFAR-10 of $32 \\times 32$ resolution and then fine-tuned the model on STL-10 of $32 \\times 32$ resolution. We do not provide the results of TWINS since TWINS is only implemented on datasets of $224 \\times 224$ resolution. As for Caltech-101 and Cars-196, we used ImageNet-1k pre-trained ResNet-50 for fine-tuning."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100507156,
                "cdate": 1700100507156,
                "tmdate": 1700271833062,
                "mdate": 1700271833062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GBLy0ENnb1",
                "forum": "09xFexjhqE",
                "replyto": "5lRRSjhpdH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission538/Reviewer_pJnK"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal remark"
                    },
                    "comment": {
                        "value": "Hi Authors, \n\nI appreciate the additional empirical evidence and clarifications and have increased my score to reflect this.  However, I still feel that the claim on parameter-free is too strong given the empirical nature even though the rest of the paper is very good. As I said earlier, we can't be sure if the proposed heuristic, which has hyper-parameters of its own, will work well unless we test it. I wouldn't be surprised that if some tuning is needed to achieve the best performance in a slightly different setup. In other words, the proposed scheduler is not functionally parameter-free. It just means that the default hyper-parameters work well across the experiments in this paper, i.e., robust to hyper-parameters. \n\nIn fact, I would recommend softening this claim if accepted because this would only make the paper more rigorous."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission538/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493523220,
                "cdate": 1700493523220,
                "tmdate": 1700493523220,
                "mdate": 1700493523220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]