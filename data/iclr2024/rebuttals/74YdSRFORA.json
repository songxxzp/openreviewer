[
    {
        "title": "Out of Sight: A Framework for Egocentric Active Speaker Detection"
    },
    {
        "review": {
            "id": "45rRb7hNS3",
            "forum": "74YdSRFORA",
            "replyto": "74YdSRFORA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission902/Reviewer_1nZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission902/Reviewer_1nZN"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a transformer-based approach, Out of Sight, that can model all the speaking activities from three prediction targets (visible speakers, camera wearer, and global background speech). The proposed method consists of 3 building blocks: Encoder, Multimodal Mixer, and Decoder. The encoder is a series of convolutional networks that embeds the visual and audio features. The multimodal mixer performs cross-modal attention and aggregates relevant information from audio-visual modalities. The decoder maps all types of tokens (audio, visual, and camera wearer) into a common representation space and predicts the final prediction. To further improve the performance, it uses a technique of long-term feature modeling by incorporating an extended temporal sampling."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The proposed architecture is very simple and easy to understand.\n- It can be used to predict all the speaking targets (visible speakers, camera wearer, and global background speech) using a single architecture."
                },
                "weaknesses": {
                    "value": "First of all, it looks like the authors provide inconsistent comparisons.\n- In Table 1, the authors report vASD mAP scores on the Ego4D\u2019s validation set. The problem is that they report mAP\\@0.5 (which is different from mAP) for previous methods when they report mAP only for their method. When we compute the mAP score, we only use the ground-truth face bounding boxes. However, computing mAP\\@0.5 involves comparing IoU between the face bounding-box detections and the ground-truth, therefore mAP\\@0.5 is estimated much lower than mAP.\n- The authors didn\u2019t report the vASD mAP of Min et al. Min (2022) although they report eASD mAP of it. This kind of partial reporting might make their method look more powerful, but doesn\u2019t seem appropriate.\n- Reporting unfair and inconsistent comparisons confuses the readers and the whole computer vision community. I believe the authors should be more accurate in describing their validation scheme and the validation strategy of the Ego4D paper.\n\nSecond, the proposed method and the results are not state-of-the-art.\n- SPELL (2022) and STHG (2023) achieve 71.3% vASD mAP and 75.7% vASD mAP on Ego4D\u2019s validation set, respectively, which significantly outperform the proposed method. Furthermore, STHG (2023) achieves 85.6% eASD mAP, which also outperforms the proposed method in this paper. Please refer to the challenge reports and recognize them. It is recommended by the Ego4D organizers to properly acknowledge their technical reports.\n\nMoreover, the proposed method has some weaknesses in its form.\n- For the weighted visual loss, the user needs to pre-compute a weight factor. The weight factor needs to be fine-tuned for each dataset (because it is data-dependent), which is ineffective and seems ad hoc.\n- There are many other hyper-parameters that need to be fine-tuned for each dataset: $\\alpha$, $k$, $n$, $\\beta$, which makes the overall method complicated and hard to optimize and utilize.\n\n[SPELL (2022)] Intel Labs at Ego4D Challenge 2022: A Better Baseline for Audio-Visual Diarization\n\n[STHG (2023)] STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced Audio-Visual Diarization"
                },
                "questions": {
                    "value": "What are the FLOPS and memory requirements for the proposed method? What is the throughput? Most of the previous state-of-the-art approaches are very efficient in terms of FLOPS and memory, and I wonder if the proposed method is comparable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "- The authors falsely report the performance by treating mAP and mAP\\@0.5 as the same metrics. I am worried this can confuse the readers. In addition, the authors only report the scores of the lower-performing models than their proposed method.\n- Table 24 of the Ego4D paper (p.60, Ego4D: Around the World in 3,000 Hours of Egocentric Video) shows that all the methods report mAP\\@0.5 (not mAP). In addition, the official evaluation tool that all the challenge participants use computes mAP\\@0.5. When we compute the mAP score, we only use the ground-truth face bounding boxes. However, computing mAP\\@0.5 involves comparing IoU between the face bounding-box detections and the ground-truth, therefore mAP\\@0.5 is estimated much lower than mAP. mAP score of previous approaches already reached over 70%, which outperforms the proposed method in this paper."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698622136021,
            "cdate": 1698622136021,
            "tmdate": 1699636017153,
            "mdate": 1699636017153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iK4V9ibhct",
                "forum": "74YdSRFORA",
                "replyto": "45rRb7hNS3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Evaluation Metric and Comparison to ArXiv pre-prints"
                    },
                    "comment": {
                        "value": "Since the review bases his/her entire rejection argument in the use of the mAP metric and the comparison against SPELL and  STHG, we refer him/her to the global replies titled 'Comparison against ArXiv Submisions' and 'mAP Metric'"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733634836,
                "cdate": 1700733634836,
                "tmdate": 1700733634836,
                "mdate": 1700733634836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qlIGAp6WmR",
            "forum": "74YdSRFORA",
            "replyto": "74YdSRFORA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission902/Reviewer_2WYw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission902/Reviewer_2WYw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for active speaker detection of egocentric videos. Unlike existing works that focus on YouTube or broadcast videos, ASD for egocentric videos brings additional challenges such as head movement and camera wearer's speech. The paper proposes a 3-stage architecture consisting of (1) modality encoders, (2) multimodal mixer, (3) speech decoder. In particular, a learnable token helps to model speech from the camera wearer. Short- and long-term architectures enable effective multi-modal fusion and extended temporal modelling. The authors use additional tricks like weighted visual loss and position of face in order to improve performance. The method is evaluated on the recent Ego4D dataset which contains various egocentric videos including human speech."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is well-engineered and achieves a strong performance. \n- Techniques such as face position and weighted loss paper is well tailored to this dataset."
                },
                "weaknesses": {
                    "value": "- The performance exceeds existing works, but the gap between the proposed method and LoCoNet is not too significant, given that this paper is specifically tailored to this dataset.\n- The methods used such as \"face position\" and \"weighted loss paper\" are the sources of most of the improvement compared to LoCoNet, but these tricks might be overfitting to the biases that exist specifically in this dataset. Does the method still generalise to existing non-egocentric ASD datasets, such as AVA-ASD or ASW?\n- Most similar literature to this is (Jiang et al., 2022) but there is very little comparison to this work in the paper.\n- I am not sure if \"active speaker detection\" is an appropriate term for the overall task. vASD in this paper is usually called ASD in other literature, and the ASD usually does not encompass what is called eASD in this paper. A similar work (Jiang et al., 2022) does not refer to this task as ASD.\n- Regardless of the term used, I am not sure if the proposed combined problem (eASD+vASD) is useful, in between vASD and AV speaker diarisation. The camera wearer is a specific identity, whereas we do not consider the identity information of the visible speakers.\n\nRegarding clarity/writing:\n- The authors use abbreviations egoASD/egoVAD in page 6, but these are not explained or used anywhere else. \n- What is meant by \"visible\" and \"unseen\" exactly? Why not \"seen\" and \"unseen\" or \"visible\" and \"invisible\" for example?\n- Is \"at last 4.41%\" at the end of abstract is the intended expression?\n- \"fine grain...\" in the introduction should be \"fine-grained...\"\n- Sec 3.2 refers to \"Token Supervision\" without section number, but this only appears much later making it confusing."
                },
                "questions": {
                    "value": "- Please see questions in 'weaknesses'.\n- Does the proposed method work well when there are multiple off-screen speakers? For example, it is realistic to have off-screen speakers next to the camera wearer in a meeting.\n- Do the authors use the pre-trained weights for LoCoNet, or is it re-trained on the same dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736221203,
            "cdate": 1698736221203,
            "tmdate": 1699636017082,
            "mdate": 1699636017082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BojBcvwJEP",
                "forum": "74YdSRFORA",
                "replyto": "qlIGAp6WmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Performance vs LoCoNet."
                    },
                    "comment": {
                        "value": "While LoCoNet has a close performance in the vASD task we highlight that Out-of-Sight has a much simpler architecture (we dont model inter-speaker or long-term single speaker relationships), despite the simple architecture, direct and accurate estimates for the egoASD and vASD emerge from the direct multi-modal modeling without using the spatial and temporal attention schemes proposed in LoCoNet. In a single end-to-end trainable model Out-of-Sight can model both the egoASD and vASD. \nFor further comparison, LoCoNet requires an input tensor of up to 200 frames to achieve optimal performance, meanwhile O2S achieves improved results covering only 35 frames."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732338279,
                "cdate": 1700732338279,
                "tmdate": 1700732338279,
                "mdate": 1700732338279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T4AGKfJi0J",
                "forum": "74YdSRFORA",
                "replyto": "qlIGAp6WmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Face position and Weighted loss."
                    },
                    "comment": {
                        "value": "Certainly O2S learns from  global spatial patterns found in Egocentric video (as shown by the improved performance using the face location). As outlined in the paper this correlates to the camera wearer\u2019s  gaze. However,  using the face positions is not exclusive to O2S or Egocentric video. Zhang et al. have already demonstrated the usefulness of this information source in commercial movies (where being at the center of the scenes correlates with directing the attention of the audience to an important character). \nWe see this additional information as common prior in the ASD task, regardless of the setup, egocentric or commercial movies. In the egocentric domain, we are the first to show the usefulness of this spatial prior."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732369440,
                "cdate": 1700732369440,
                "tmdate": 1700732369440,
                "mdate": 1700732369440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tqlhsjFLrL",
                "forum": "74YdSRFORA",
                "replyto": "qlIGAp6WmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Terminology"
                    },
                    "comment": {
                        "value": "We diverted slightly from the standard terminology trying to emphasize that both tasks have a strong correlation even when part of the cross-modal modeling does not have a direct visual support. Our transformer-encoder decoder represents a straightforward way to model both. In the final version we will follow a more standard naming of  the task, but still emphasize on the connection between them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732392619,
                "cdate": 1700732392619,
                "tmdate": 1700732392619,
                "mdate": 1700732392619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xfsWLpikgM",
                "forum": "74YdSRFORA",
                "replyto": "qlIGAp6WmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Usefulness in AV speaker diarization."
                    },
                    "comment": {
                        "value": "We must note that ASD and speaker diarization can be two independent tasks, for the case of audio only diarization. Certainly the AudioVIsual diarization and improved ASD pipeline task benefits the diarization taks. We see O2S as a method that can bridge the gap in both domains. It can directly estimate if the speech event must be attributed to visible speakers (thus an AV diarization method could be used for improved performance) or if the event must be attributed to unseen speech (thus audio only diarization must be used)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732427344,
                "cdate": 1700732427344,
                "tmdate": 1700732427344,
                "mdate": 1700732427344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XOca4NvmlF",
                "forum": "74YdSRFORA",
                "replyto": "qlIGAp6WmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Writing Clarity"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading of our paper, we will fix the typos and reference errors in the final version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732454359,
                "cdate": 1700732454359,
                "tmdate": 1700732454359,
                "mdate": 1700732454359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "51GteHIabu",
                "forum": "74YdSRFORA",
                "replyto": "qlIGAp6WmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Questions"
                    },
                    "comment": {
                        "value": "We can not tell how many potential out-of-screen speakers there are in a given scene in Ego4D. To the best of our knowledge no method or dataset provides such insight. Without this data we can not provide the requested ablation or at least an insight in the requested direction.\nHowever, we consider it is completely realistic to have multiple off-screen speakers, simply put the Field of View of most capture devices is far less than 360 degrees, even some people inside the FOV could be occluded, for example behind a third person or a grocery store stand.\nWe are not aware of an official release of weights from LoCoNet for the Ego4D dataset. As outlined in the paper we train from weight initialized in Imagenet (2.5D) and Kinetics (3D)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733013812,
                "cdate": 1700733013812,
                "tmdate": 1700733013812,
                "mdate": 1700733013812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZIhop56Hzr",
            "forum": "74YdSRFORA",
            "replyto": "74YdSRFORA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission902/Reviewer_dygL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission902/Reviewer_dygL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes O2S, a framework for egocentric active speaker detection. Most active speaker detection literatures fall in commercial movies and social media videos, while egocentric videos are less investigated. O2S consists of three stages: 1) An audio encoder and a video encoder are employed to obtain audio features and visual face features. 2) A transformer serves as multimodal mixer to aggregate information from audio and video. 3) Another transformer serves as speech decoder to predict speech event for each face feature, audio feature, and an additional feature for the invisible camerawearer. There are some additional changes made for egocentric videos. First, face positions are added in the visual feature. Second, as egocentric videos may present many blurred faces due to fast motion, noisy faces are less contributed to the loss. Experiments are conducted on the Ego4D dataset for two tasks: Active Speaker Detection of visible targets\n(vASD) and egocentric Active Speaker Detection (eASD)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed O2S achieves the state-of-the-art performance on the Ego4D for both vASD and eASD.\n2. The proposed method is a reasonable solution for egocentric active speaker detection.\n3. The paper presentation and writing are very clear."
                },
                "weaknesses": {
                    "value": "1. The authors should highlight the main differences between the proposed method and the previous 3rd person view active speaker detection methods. This is important to show the contribution of this proposed method.\n2. I think Visual Token Representation and Weighted Visual Loss are the two unique contributions for the egocentric scenario. However, these two contributions are not significant. This brings back to the first concern: the authors should highlight the main differences compared to previous works especially on the main architecture.\n3. Although the whole pipeline is reasonable, it is complicated. Does it need to first use a face detector to detect faces? In the main architecture of O2S, there are CNNs for encoding video and audio, and then there are transformers for mixing video and audio and decoding them. Why not encode and mix video and audio in just a single transformer? In my view, all the three stages can be simplified in one single transformer in principle."
                },
                "questions": {
                    "value": "\"In other words, the active speaker is more likely to appear near the center of the frame\" I agree in most cases the active speaker appear around the center. But in some cases, the camerawearer may not look at the speaker or only turn eyeballs to look at the speaker. It may be more accurate to incorporate eye gaze location in the Visual Token Representation. After all, the XR device should already detected eye gaze."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission902/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission902/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission902/Reviewer_dygL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795111945,
            "cdate": 1698795111945,
            "tmdate": 1699636017008,
            "mdate": 1699636017008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QYiUr7MkKM",
                "forum": "74YdSRFORA",
                "replyto": "ZIhop56Hzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Full Transformer Architecture"
                    },
                    "comment": {
                        "value": "We politely direct the reviewert to the general discussion regarding the mAP metric to clarify the use of face detections in the validation set. \n\nWe also clarify that, while we welcome the reviewer's suggestion, we must also bring into consideration that we can not simply  plug a multimodal transformer into the proposed head, we explain in detail:\n\nFirst, Since vASD predictions are made over individual face-crops. We still have to figure out a way to model features for each individual face-crop. Since the number of faces per frame can vary, we must resort either to simultaneous transformer forward passes (as is the case in O2S, but with CNN architectures), or a series of special tokens  delimiting the different faces and the individual face crops on each.\n\nSecond,  it must be empirically determined if these facial features should be mixed prior to the encoder-decoder head proposed in O2S or earlier in the visual stream. This constitutes an additional hyper-parameter that should be explored. If it is determined that no attention is required between the individual face crops, we regress to the same scenario of O2S where  the CNN is replaced  with a visual transformer.\n \nThird, as we integrate the location data  of the face crops, an even more complex input stream for the visual transformer must be built. One might think about simply inputting the entire frame and encoding some ROIs corresponding to the faces. This will clearly incrates the memory allocation for the proposed network, and might be impractical as a portion of the visual objects on the frame have no direct relation with the task. For example the visual feature of a supermarket will not be truly useful in identifying the actual source of a speech event.\n\nFourth, some of the structure proposed in the encoder-decoder transformer must remain to generate predictions which are temporally consistent and directly correspond to the input faces, in other words we can not simply mix all the data without any strategy to generate individual predictions over the inputs. Part of the complexity introduced in O2S is due to the requirement of directly supervising each token in the output with its corresponding ground-truth, if these correspondence is lost, the network supervision becomes significantly more challenging \n\nCertainly we share the same opinion of the reviewer regarding the versatility of transformers architectures for ASD tasks, but we also consider that it also brings extra challenges in a problem where visual and audio streams must be carefully aligned. We think this suggestion aligns better with the future work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733060073,
                "cdate": 1700733060073,
                "tmdate": 1700733060073,
                "mdate": 1700733060073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GjVvkfqljP",
                "forum": "74YdSRFORA",
                "replyto": "ZIhop56Hzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty and Contribution"
                    },
                    "comment": {
                        "value": "We politely invite the reviewer to check the general general comment posted in this regard"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733080238,
                "cdate": 1700733080238,
                "tmdate": 1700734253660,
                "mdate": 1700734253660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pngTlo9oLo",
                "forum": "74YdSRFORA",
                "replyto": "ZIhop56Hzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Questions"
                    },
                    "comment": {
                        "value": "We fully agree that the camera Field of View is not a direct estimation of the person's gaze, but rather a correlated variable. As outlined in the paper this empirical improvement of explicitly locating the face crops has already been explored  in datasets composed of  commercial movies.\nWe see this additional information as common prior in the ASD task, regardless of the setup, egocentric or commercial movies. In the egocentric domain, we are the first to show the usefulness of this spatial prior."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733378386,
                "cdate": 1700733378386,
                "tmdate": 1700733378386,
                "mdate": 1700733378386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ylaXOcUnLU",
            "forum": "74YdSRFORA",
            "replyto": "74YdSRFORA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission902/Reviewer_kwFT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission902/Reviewer_kwFT"
            ],
            "content": {
                "summary": {
                    "value": "Paper proposed a novel approach for the Active Speaker Detection (ASD) problem in egocentric data (esp. First Person Video (FSD)). This problem is relatively unexplored. The challenges for ASD in FPV is mainly to the \"invisibility\" of the camera wearer in the video which the SOTA ASD algorithms cannot handle correctly.\n\nThe proposed method uses multimodality to overcome this challenge via 3 building blocks: (i) Modality Encoder; (ii) Mutlimodal Mixer; (iii) Speech Decoder.\n\nExperiments were performed to compare against SOTA ASD in FPV methods for the Ego4D dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Paper's position that ASD in FPV is less research and proposed a novel method to overcome the specific issue for this problem is well explained and motivated.\n\n2. Proposed method is somewhat novel and logical.\n\n3. Experimental results are quite strong."
                },
                "weaknesses": {
                    "value": "1. Problem statement is somewhat niche.\n2. Novelty of proposed solution is limited as it's a special case of multimodality matching. The unseen visual features are replaced with a special token (c)."
                },
                "questions": {
                    "value": "No question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837052189,
            "cdate": 1698837052189,
            "tmdate": 1699636016914,
            "mdate": 1699636016914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tn9n9ajUL7",
                "forum": "74YdSRFORA",
                "replyto": "ylaXOcUnLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Niche Area of Study"
                    },
                    "comment": {
                        "value": "We politely disagree with the reviewer's assertion, and instead bring his/her attention to the fact that there are multiple ASD papers with over 100 citations published in some of the important venues such as CVPR, ICCV and ECCV. Moreover, we must take into account that this area lacked a large-scale benchmark before the publication of the AVA-Active speaker dataset in 2019, and the Ego4D benchmark was only released last year. Overall this is an emerging line of work into the domain of multi-modal representation learning."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734079364,
                "cdate": 1700734079364,
                "tmdate": 1700734202464,
                "mdate": 1700734202464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2b0t2m83OY",
                "forum": "74YdSRFORA",
                "replyto": "ylaXOcUnLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty"
                    },
                    "comment": {
                        "value": "We politely invite the reviewer to check the general general comment posted in this regard"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734237679,
                "cdate": 1700734237679,
                "tmdate": 1700734237679,
                "mdate": 1700734237679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]