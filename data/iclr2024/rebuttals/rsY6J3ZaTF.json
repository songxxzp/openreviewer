[
    {
        "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation"
    },
    {
        "review": {
            "id": "9VAC54OMLv",
            "forum": "rsY6J3ZaTF",
            "replyto": "rsY6J3ZaTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4571/Reviewer_p4vK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4571/Reviewer_p4vK"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose DistillSpec that uses knowledge distillation to better align the draft model with the target model. The authors have a wide exploration of different dilation methods from SeqKD to on-plicy GKD. DistillSpec yields impressive 10 \u2212 45% speedups over standard speculative decoding on a range of standard benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The experiments are quite solid. It explores different distillation losses and different ways to collect data for distillation. All the tables and figures are well written, and it is easy to get the difference between different methods.\n2. The authors also explore the trade-off between accuracy and latency.\n3. The paper is well-written and easy to read."
                },
                "weaknesses": {
                    "value": "1. All the distillation methods are based on existing works. The work is more of a comprehensive study of knowledge distillation on LLM for speculative decoding.\n2. All the experiments are done in an in-domain setting, while the benefits of LLM are on zeroshot/fewshot setting on out-of-domain datasets. Thus the experiments can not show whether it can replace the general draft models."
                },
                "questions": {
                    "value": "Why do different distillation methods vary a lot in Figure 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4571/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827660872,
            "cdate": 1698827660872,
            "tmdate": 1699636435178,
            "mdate": 1699636435178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B2JHUVu8Zi",
                "forum": "rsY6J3ZaTF",
                "replyto": "9VAC54OMLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response: Clarify the novelty and contributions. Clarify the performance variance. Zero-shot CoT evaluation on BigBenchHard (23 tasks)."
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to provide your thoughtful and valuable feedback on our work. Below are responses to your concerns.\n\n> *All the distillation methods are based on existing works. The work is more of a comprehensive study of knowledge distillation on LLM for speculative decoding.*\n\nIndeed, **the goal of our paper is not to propose a new KD algorithm but to provide a way to improve SD using KD**. That said, applying KD to SD is not well-explored in the past literature. As shown in Figure 2, 3a, the previous attempt [1] using SeqKD turned out to be not very effective to improve SD. Therefore, we provide a systematic and comprehensive study on the most cost effective way to apply KD to SD.\n\nTo understand the key factors that affect the distillation performance, we categorize most commonly used KD techniques in terms of \u201cdivergence function\u201d and \u201ctraining data'' in Table 1 and investigate many other KD variants in Section 5.2 (Figure 4). Note that many combinations of the divergence function and training data have not been studied in the past literature. We believe that our findings that 1) using student generated data is most cost-effective; and 2) the best divergence function depends on tasks and decoding strategy; are meaningful and provide new insights on how we should apply distillation in practice to improve LLM latency. \n\nFurthermore, we wish to emphasize our contributions to the study of the quality and latency trade-off, a central challenge in LLM inference. Our research explores this trade-off through two innovative perspectives: 'lossy speculative decoding' and 'model garden'. The former offers fine-grained control over the trade-off curve, while the latter presents optimal strategies for combining SD and Distillation. These contributions are novel and highly pertinent for practitioners seeking to balance quality and latency in model deployment.\n \n[1] Stern, M., Shazeer, N., & Uszkoreit, J. (2018). Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31.\n\n> *All the experiments are done in an in-domain setting, while the benefits of LLM are on zeroshot/fewshot settings on out-of-domain datasets. Thus the experiments can not show whether it can replace the general draft models.*\n\nThanks for the suggestion! We have added a new experiment to study the transferability of the distilled draft model across different datasets. Specifically, we take the distilled draft model from GSM8K and evaluate it on all **BigBenchHard** tasks (23 tasks in total and all are out-of-domain) [1] with **zero-shot CoT**. We observe that the distilled draft model transfers well to other datasets. Compared to standard SD, it improves the average decoding speed from 1.93x and 1.78x to 2.21x and 2.02x using the greedy and non-greedy decoding, respectively. See Appendix Figure C.2 and C.3 for a detailed breakdown of the tasks in BBH.\n\n[1] Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., ... & Wei, J. (2022). Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\n\n> *Why do different distillation methods vary a lot in Figure 4?*\n\nIndeed, it is an interesting finding from our comprehensive study on divergence function in Section 5.2. Empirically, we observe that different divergence functions perform differently on different tasks. It may be due to certain specific characteristics of the tasks but we do not have a good explanation yet. While this performance variance is disappointing, we think it truthfully reflects the fact that distillation in the real world is challenging. Therefore, we recommend to view the divergence function as one of the hyperparameters in a distillation algorithm and we should tune it just like any other hyperparameters.\n\n-----\n*We hope that most of the reviewer\u2019s concerns have been addressed and, if so, we would appreciate it if they could reconsider their assessment. We\u2019d be happy to engage in further discussions.*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4571/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359884563,
                "cdate": 1700359884563,
                "tmdate": 1700359884563,
                "mdate": 1700359884563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q4ScIuaXbF",
            "forum": "rsY6J3ZaTF",
            "replyto": "rsY6J3ZaTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4571/Reviewer_GLMj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4571/Reviewer_GLMj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DistillSpec, a method that uses knowledge distillation to improve the speed and quality of speculative decoding. The paper explores various factors that affect the alignment between the draft and target models, such as the training data, the divergence function, and the decoding strategy. It also extends DistillSpec to lossy speculative decoding, which allows for sampling away from the target model distribution. The paper evaluates DistillSpec on several tasks and datasets to demonstrate its speedups."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It provides a comprehensive and systematic study of different aspects of knowledge distillation for speculative decoding, such as data generation, divergence functions, and lossy sampling. The conclusion of models generated data being important makes sense.\n- It demonstrates the effectiveness of DistillSpec on several tasks and datasets, using both greedy and non-greedy sampling, and compares it with representative baselines.\n- Its lossy speculative decoding results offer novel insights and recommendations for combining knowledge distillation and speculative decoding in different scenarios."
                },
                "weaknesses": {
                    "value": "- The presentation of the paper is a bit messy and unclear. For instance, it is difficult to find the formal definition of DistillSpec among the analysis of various existing distillation approaches; it is not clear how distillation data is generated in detail; the specific configurations of target and draft models in figure 6 are not given, especially for the size of the target model in the DistillSpec case. The clarity and structure of the paper should be improved.\n- While DistillSpec is effective on certain tasks, the experiments mainly focus on T5 models and simple tasks (except for GSM8K), compared with the recent advances in LLMs. This also results in the lack of discussion on the difficulty of LLM distillation. Is DistillSpec also effective for LLMs like LLaMA-7B? Can distilling LLMs for some specific tasks also be helpful for other tasks in general? The effectiveness of DistillSpec should be evaluated using more recent large models and some zero-shot benchmarks to prove its effectiveness.\n- The paper does not compare or discuss DistillSpec with other methods that combine large and small models at inference, especially under the lossy decoding cases [1].\n- The paper does not analyze the generated texts by DistillSpec in greater detail, like the diversity and coherence, which are important aspects of text quality and largely influenced by the sampling approaches. It would be helpful to provide some examples of the generated tasks.\n\n[1] Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney, M.W., Gholami, A., & Keutzer, K. (2023). Speculative Decoding with Big Little Decoder."
                },
                "questions": {
                    "value": "I am wondering how will DistillSpec be compared with recent work that combines speculative decoding and distillation [1]. It would be interesting to have this discussion.\n\n[1] Liu, X., Hu, L., Bailis, P.D., Stoica, I., Deng, Z., Cheung, A., & Zhang, H. (2023). Online Speculative Decoding. ArXiv, abs/2310.07177."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "na"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4571/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4571/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4571/Reviewer_GLMj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4571/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840054864,
            "cdate": 1698840054864,
            "tmdate": 1699636435104,
            "mdate": 1699636435104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5I4HxJ0N8C",
                "forum": "rsY6J3ZaTF",
                "replyto": "Q4ScIuaXbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response: Clarify the definition of DistillSpec, distillation data generation, model architecture choices and model sizes. Zero-shot CoT evaluation on BigBenchHard (23 tasks)."
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to provide your thoughtful and valuable feedback on our work. Below are responses to your concerns.\n\n> *For instance, it is difficult to find the formal definition of DistillSpec among the analysis of various existing distillation approaches*\n\nDistillSpec (Algorithm A.2) refers to our general framework that improves the SD by aligning the target model and draft model using the whitebox knowledge distillation. We do not introduce new divergence functions or new data generation procedures, but instead systematically investigate which combinations of divergence functions and data generation procedures can be best leveraged to improve upon vanilla SD. We have revised the paper to include this definition. \n\n> *it is not clear how distillation data is generated in detail.*\n\nTo generate new data, we closely follow the previous work [1]. Specifically, we prompt the model with a task input sampled from the training dataset and ask the model to generate an output. This data generation process works for both teacher and student models.\n\n[1] Agarwal, R., Vieillard, N., Stanczyk, P., Ramos, S., Geist, M., & Bachem, O. (2023). GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. arXiv preprint arXiv:2306.13649.\n\n> *the specific configurations of target and draft models in figure 6 are not given, especially for the size of the target model in the DistillSpec case.*\n\nWe have updated the figure and summarized the target and draft model size below.\n\n1) **Raw**: Deploying supervised fine-tuned (SFT) T5 models. It contains five model points: T5-Small (77M), T5-Base (250M), T5-Large (800M), T5-XL (3B), and T5-XXL (11B).\n\n2) **Distilled**: Applying KD to T5 models for optimizing downstream task performance. It contains four model points: Distilled-T5-Small (77M), Distilled-T5-Base (250M), Distilled-T5-Large (800M), Distilled-T5-XL (3B). All distilled models use Raw T5-XXL (11B) as the teacher.\n\n3) **Speculative**: Applying SD to T5 models. It contains four target models and all use Raw T5-Small (77M) as draft model for best performance. The target models are T5-Base (250M), T5-Large (800M), T5-XL (3B), and T5-XXL (11B). \n\n4) **DistillSpec**: Applying KD to T5 models and using distilled models as target and draft. It has three target model sizes: (1) Distilled-T5-Base (250M) (2) Distilled-T5-Large (800M), and (3) Distilled-T5-XL (3B). We use Distilled-T5-Small (77M) as the draft model for all target models. \n\n> *While DistillSpec is effective on certain tasks, the experiments mainly focus on T5 models and simple tasks (except for GSM8K) ... The effectiveness of DistillSpec should be evaluated using more recent large models and some zero-shot benchmarks to prove its effectiveness.*\n\nThanks for the suggestion! We have added a new experiment to study the transferability of the distilled draft model across different datasets. Specifically, we take the distilled draft model from GSM8K and evaluate it on all **BigBenchHard** tasks (23 tasks in total) [1] with **zero-shot CoT**. We observe that the distilled draft model transfers well to other datasets. Compared to standard SD, it improves the average decoding speed from 1.93x and 1.78x to 2.21x and 2.02x using the greedy and non-greedy decoding, respectively. See Appendix Figure C.2 and C.3 for a detailed breakdown of the tasks in BBH.\n\nAs for the model we use, we choose the T5 model series because they are a set of powerful and representative models, which are carefully studied in the original SD paper [2] and a follow up study in [3]. Further, to demonstrate the effectiveness of our method for the decoder-only model, we followed [2][4]  and had already included an experiment on LM1B in our original submission. As shown in Table C.1, DistillSpec consistently yields speed gain over the vanilla SD. We expect our findings from the LM1B experiment with GPT-like decoder-only models to extend to LLaMA sized models.\n\n[1] Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., ... & Wei, J. (2022). Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\n\n[2] Leviathan, Y., Kalman, M., & Matias, Y. (2023, July). Fast inference from transformers via speculative decoding. In International Conference on Machine Learning (pp. 19274-19286). PMLR.\n\n[3] Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney, M.W., Gholami, A., & Keutzer, K. (2023). Speculative Decoding with Big Little Decoder.\n\n[4] Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., & Yu, F. (2023). Spectr: Fast speculative decoding via optimal transport."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4571/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358730052,
                "cdate": 1700358730052,
                "tmdate": 1700359531897,
                "mdate": 1700359531897,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TcqbOjr3gH",
            "forum": "rsY6J3ZaTF",
            "replyto": "rsY6J3ZaTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4571/Reviewer_gZt5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4571/Reviewer_gZt5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DistillSpec, which improves the efficiency of speculative decoding (SD) by aligning the distributions (at the token and sequence level) between the draft model and the target model in advance. The paper further provides valuable insights regarding the recipe of distillation data, distillation objective and sampling strategy.\n\nExperiments show that DistillSpec speedup SD by 10-45% while preserving the model performance across four diverse datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. SD is an important direction in accelerating LM inference. The idea of leveraging distillation to speed up SD is novel and very effective.\n2. Using acceptance rate as the efficiency measure is well-motivated. Using total variation distance (TVD) objective is simple and straightforward in maximizing this efficiency measure.\n3. The authors provide a fast alternative of using student-generated data for training, with a theoretical justification and strong experimental results."
                },
                "weaknesses": {
                    "value": "1. The technical novelty is a bit limited. It is a direct application of existing KD techniques in SD.\n2. The performance of the method is task-dependent, posing concerns for using the method in practice. For example, the speedup on WMT En-De is marginal and TVD has varied performance on different tasks.\n3. Major experiment results are based on small target models. The results on larger models are not very clear -- in Figure 6, which data points correspond to which sizes of target and draft models?\n4. The method is target-model-dependent, meaning that we need to distill a new draft model for each new target model. Such a distillation cost can be quite expensive, especially when using online data generation."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4571/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877183262,
            "cdate": 1698877183262,
            "tmdate": 1699636435009,
            "mdate": 1699636435009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NlUfUas10j",
                "forum": "rsY6J3ZaTF",
                "replyto": "TcqbOjr3gH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response: Clarify the novelty and contributions. Clarify the performance variance of different tasks and divergence functions."
                    },
                    "comment": {
                        "value": "Thank you for taking the time to provide your thoughtful and valuable feedback on our work. Below are responses to your concerns.\n\n> *The technical novelty is a bit limited. It is a direct application of existing KD techniques in SD.*\n\nWe argue that the application of KD in SD is not as straightforward or well-explored. Our approach diverges from the standard KD, which primarily aims at enhancing the student model's performance. Our focus is on improving alignment to enhance decoding efficiency, a distinct objective as evidenced in Figure 5a.\n\nAdditionally, our work critically examines the effectiveness of applying KD to SD. Figures 2 and 3a demonstrate that previous attempts, such as the use of SeqKD [1], were inadequate in enhancing SD efficiency. Our research contributes to this field by providing a systematic and comprehensive study (Section 5.2) on the most cost-effective methods of applying KD in SD. The insights we offer, particularly regarding the cost-effectiveness of using student-generated data and the task and strategy-dependent nature of the best divergence function, are valuable for practical applications.\n\nFurthermore, we wish to emphasize our study of the quality and latency trade-off, a central challenge in LLM inference. Our research explores this trade-off through two perspectives: 'lossy speculative decoding' and 'model garden' (Section 5.3). The former offers fine-grained control over the trade-off curve, while the latter presents optimal strategies for combining SD and KD. These contributions are novel and highly pertinent for practitioners seeking to balance quality and latency in model deployment.\n\n[1] Stern, M., Shazeer, N., & Uszkoreit, J. (2018). Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31.\n\n\n> *The performance of the method is task-dependent, posing concerns for using the method in practice. For example, the speedup on WMT En-De is marginal.*\n\nIndeed, the additional speedup provided by DistillSpec is task-dependent. There are two reasons for this: \n\n1) First, **different tasks have different amounts of headroom**. As shown in Figure 1 and Table D.1, the original SD already shows a speedup of 2.36x on WMT, in contrast to a 1.44 speedup on XSum. It suggests that target model and draft model exhibit different levels of alignment when trained on different tasks and the model trained on WMT is already much more aligned than XSum. Thus, the headroom for improvement is smaller for WMT than XSum. The difference in task nature was also observed in previous SD literature [1][2]. \n\n2) Secondly, **we did not attempt to optimize the hyper-parameters for each task**: we use the same hyperparameters for all distillation experiments, so they may be suboptimal for each task. Figure C.8 and C.9 show the progression of the acceptance rate tested on the sequence generated by the target model. The unsaturated learning curves suggest that we can achieve much better performance if we train the model longer, especially in the WMT case. However, our current training strategy can already show the effectiveness of the DistillSpec and training longer to optimize the absolute performance gain might not provide any new insights. \n\n[1] Leviathan, Y., Kalman, M., & Matias, Y. (2023, July). Fast inference from transformers via speculative decoding. In International Conference on Machine Learning (pp. 19274-19286). PMLR.\n\n[2] Chen, C., Borgeaud, S., Irving, G., Lespiau, J. B., Sifre, L., & Jumper, J. (2023). Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318.\n\n\n> *TVD has varied performance on different tasks.*\n\n**We argue that \u201cTVD has varied performance on different tasks\u201d is itself a finding of our paper.** Although TVD is the theoretically motivated objective we should optimize, our systematic study in Section 5.2 suggests that it does not consistently yield the best empirical performance. While this gap between theory and practice is disappointing, we think it truthfully reflects the fact that distillation in the real world is challenging. \n\nJust like ML algorithms that have some hyperparameters to tune, we think the divergence function is one of the hyperparameters in a distillation algorithm and you should tune it for your task. Note that this process only needs to be done once and can be amortized across many serving queries. \n\nOverall, we view treating the divergence function as a hyperparameter as an interesting take-away from our study."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4571/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357749655,
                "cdate": 1700357749655,
                "tmdate": 1700359960966,
                "mdate": 1700359960966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B5DKJs6QUG",
                "forum": "rsY6J3ZaTF",
                "replyto": "TcqbOjr3gH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4571/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response: Clarify the model sizes and distillation cost. Add new experiments on the transferability of the distilled draft models (different target sizes, GSM8K ->BBH)."
                    },
                    "comment": {
                        "value": "> *Major experiment results are based on small target models.*\n\nOur main results (Figure 1) are based on the second large T5 model (T5-XL, 3B) as target and T5 (T5-small, 77M) as draft. Besides, we include additional results using the largest T5 (T5-XXL, 11B) as target in Appendix (Figure C.1). Compared to recent literature on SD, where BiLD [1] only considers T5-Large (770M) as the target model, SpecTr [2] considers a 97M decoder only model, Spector & Re [3] consider a 762M GPT2 model, our target model choices (3B, 11B) are not small, which closely follows the original SD paper [4].\n\n[1] Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney, M.W., Gholami, A., & Keutzer, K. (2023). Speculative Decoding with Big Little Decoder.\n\n[2] Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., & Yu, F. (2023). Spectr: Fast speculative decoding via optimal transport. \n\n[3] Spector, B., & Re, C. (2023). Accelerating LLM inference with staged speculative decoding. arXiv preprint arXiv:2308.04623.\n\n[4] Leviathan, Y., Kalman, M., & Matias, Y. (2023, July). Fast inference from transformers via speculative decoding. In International Conference on Machine Learning (pp. 19274-19286). PMLR.\n\n> *The results on larger models are not very clear -- in Figure 6, which data points correspond to which sizes of target and draft models?*\n\nWe agree and have updated the figure. The target and draft model size are detailed below.\n\n1) **Raw**: Deploying supervised fine-tuned (SFT) T5 models. It contains five model points: T5-Small (77M), T5-Base (250M), T5-Large (800M), T5-XL (3B), and T5-XXL (11B).\n\n2) **Distilled**: Applying KD to T5 models for optimizing downstream task performance. It contains four model points: Distilled-T5-Small (77M), Distilled-T5-Base (250M), Distilled-T5-Large (800M), Distilled-T5-XL (3B). All distilled models use Raw T5-XXL (11B) as the teacher.\n\n3) **Speculative**: Applying SD to T5 models. It contains four target models and all use Raw T5-Small (77M) as draft model for best performance. The target models are T5-Base (250M), T5-Large (800M), T5-XL (3B), and T5-XXL (11B). \n\n4) **DistillSpec**: Applying KD to T5 models and using distilled models as target and draft. It has three target model sizes: (1) Distilled-T5-Base (250M) (2) Distilled-T5-Large (800M), and (3) Distilled-T5-XL (3B). We use Distilled-T5-Small (77M) as the draft model for all target models. \n\n> *The method is target-model-dependent, meaning that we need to distill a new draft model for each new target model. Such a distillation cost can be quite expensive, especially when using online data generation.* \n\nSD\u2019s efficiency depends on the alignment between the target model and draft model, which is \u201cmodel dependent\u201d. When we have a new target model to deploy, if it is very different from the previous target model, then, it is reasonable to distill a new draft model tailored to the new target model. However, if the new target model is very similar to the old target model (e.g., they belong to the same model class, but are of different sizes), then it is sensible to reuse the existing distilled draft model. As we demonstrated in Appendix Figure C.1, the distilled draft model from T5-XL (3B) as target also works well for T5-XXL (11B) as target. Though it is a suboptimal choice, it still achieves a consistent improvement of 7%- 37% over original SD across four different datasets. \n\nIn contrast, if the new target model is very different, then the extra distillation cost is inevitable to realize significant gains via SD. Note that the distillation process only needs to be done once, which can be amortized across many serving queries and the real cost is serving to a large number of users. As for the online data generation, Figure 3a suggests that using the student model for online data generation is actually the most cost-effective way to do distillation. In contrast, when you use a static ground truth dataset (i.e., SupKD), your model performance will soon plateau. \n\n**Additionally, we have added a new experiment to study the transferability of the distilled draft model across different datasets.** Specifically, we take the distilled draft model from GSM8K and evaluate it on all BigBenchHard tasks (23 tasks in total) [1] with zero-shot CoT. We observe that the distilled draft model transfers well to other datasets. Compared to standard SD, it improves the average decoding speed from 1.93x and 1.78x to 2.21x and 2.02x using the greedy and non-greedy decoding, respectively. See Appendix Figure C.2 and C.3 for a detailed breakdown of the tasks in BBH.\n\n[1] Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., ... & Wei, J. (2022). Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\n\n-----\n*We hope that your concerns/questions have been addressed and, if so, we would appreciate it if you could reconsider your assessment. We\u2019d be happy to engage in further discussions.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4571/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358198245,
                "cdate": 1700358198245,
                "tmdate": 1700360712451,
                "mdate": 1700360712451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]