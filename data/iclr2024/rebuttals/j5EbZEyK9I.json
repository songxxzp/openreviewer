[
    {
        "title": "Mo' Data Mo' Problems: How Data Composition Compromises Scaling Properties"
    },
    {
        "review": {
            "id": "RUq9nSHJ6L",
            "forum": "j5EbZEyK9I",
            "replyto": "j5EbZEyK9I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_7keK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_7keK"
            ],
            "content": {
                "summary": {
                    "value": "The work is devoted to exploring the difference between two ways of sampling data from different sources facing shifts in them compared to the general distribution. The authors demonstrate that sequential sampling expectedly results in the shift of performance. The work contains large experimentations resulted in observational study of the effects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Intensive experimental evaluation and analysis"
                },
                "weaknesses": {
                    "value": "I see a notable weakness of this work in the novelty and the contribution. The paper seems to state claims that are more-or-less known in research community. So, even if there are no publications on the topic, e.g., the fact of having shift of distributions (and thus, lower performance) in the case you sample not from the target distribution is common knowledge and directly follows from ML grounds. So, such knowledges are also referred to as folklore. So, in my opinion, the paper contribution is narrowing down to experimental analysis, which is good but looks like an observational study without clear new insights (besides the ones expected by folklore knowledge). It is not enough for this venue. I would assume that such an analysis is a good illustration / help for students that study ML / statistics and might be published through some books on the topic.\n\n\nAnother related weakness is the problem setup. The authors try to explore some effects when sampling not from the underlying distribution D in different ways knowing that they sample with shifts. What is the problem to be solved? If the practitioner face a situation, knowing D, then they will sample from it. If they have no access to D, then they try to sample some distribution such that it is most close to D. If they face two sources and expect that they have representation of D (they believe that their union is close to D), then they will sample randomly from the union. And etc. In any way, the practitioner knowing basics of ML, will attempt to be close to the best knowledge of D for them. It is hard for me to imagine situations described in the work, where a practitioner is aware of ML grounds and is increasing samples without carrying about the general population. I assume, it might be the case when this practitioner is working with ML tools without knowing ML grounds (so, in this case, a book on ML grounds might help). Overall, the work lack of clear problem statement: what is known for a practitioner, what is not, which decisions they can take, what are limitations. \n\n\nSec.3.1 serves for me as support of absence of clear problem statement in the work. The authors write \u201cWhile in reality the test set cannot be accessed, we assume we can use some part of the training set (e.g., the D_s_1 ) that is similar to the test distribution).\u201d Seems that it means that the practitioner believes that D_s_1 IS the distribution D. So, when receiving D_s_2 they should take D_s_1 united with D_s_2 as D, or not considering D_s_2 at all."
                },
                "questions": {
                    "value": "In section 3.1: \u201cWhile in reality the test set cannot be accessed, we assume we can use some part of the training set (e.g., the D_s_1 ) that is similar to the test distribution).\u201d\n\n-\tI see two closing parentheses while having only one open one.\n\n-\tWhat do you mean saying \u201cthat is similar to the test distribution\u201d it is unclear\n\n\nIn Section 4: \u201cdata composition on model outcomes as measured on a test set sampled exclusively from the initial dataset Source A (e.g., SD) \u2013 which we call the reference test set.\u201d\n\n-\tI do not understand: at the beginning of the work it was stated that \u201ctest set\u201d is the union. Here, it is stated that it is just D_d_1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396209133,
            "cdate": 1698396209133,
            "tmdate": 1699636942574,
            "mdate": 1699636942574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3a6bz3tpWH",
                "forum": "j5EbZEyK9I",
                "replyto": "RUq9nSHJ6L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of Our Work and Contributions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising their concerns; however, our contributions may have been fundamentally misunderstood by the reviewer. \n\n**Main Contribution**\n> So, even if there are no publications on the topic, e.g., the fact of having shift of distributions (and thus, lower performance) in the case you sample not from the target distribution is common knowledge and directly follows from ML grounds. \n\nThe reviewer is correct that the fact that \u201cdistribution shift between train and test datasets results in reduced performance\u201d has been studied extensively in past work (See our references Sagawa et. al. in the main text and Appendix C.5 detailed related work on distribution shift). \n\nHowever, what is **not** known is (1) why distribution shift may arise as dataset size increases and (2) do the harms induced by distribution shift from scaling eclipse the benefits of scaling. This is an incredibly common scenario in the real world. For example, government resource allocation for social programs depends on projections from census data [1]. In smaller counties and states, the decision has to be made on whether adding more data is beneficial or harmful to achieve more accurate and equitable predictions. Moreover, formalizing **why** adding data does not yield better outcomes is important and it is a central contribution to our work. \n \n> What is the problem to be solved? If the practitioner face a situation, knowing D, then they will sample from it. If they have no access to D, then they try to sample some distribution such that it is most close to D.\n\nWithout access to D, how is it possible to know what distribution is closest to D? Our central question (Figure 1) is: \u201c**How** does data composition impact model performance under scaling?\u201d. We examine the situation where there is limited access to D (e.g. a very small sample). A problem arises when we have to decide whether adding data from some other distribution $D_{s_2}$, is beneficial or harmful with respect to overall accuracy, worst group accuracy, and group accuracy disparity. \n\n> Seems that it means that the practitioner believes that D_s_1 IS the distribution D. So, when receiving D_s_2 they should take D_s_1 united with D_s_2 as D, or not considering D_s_2 at all.\n\nSuppose we are policymakers in South Dakota, $D_{s_1}$ is the dataset from our state, but $D_{s_2}$ is a very different state like California. If $D_{s_2}$ is not considered at all, $D_{s_1}$ might be too small and uniform to give adequate accuracy/fairness (e,g, only 2.6% of the population is Black [2] ). If we sample from the union of $D_{s_2}$ and $D_{s_1}$, the result would depend on the relationship between $D_{s_2}$ and $D_{s_1}$ (this is captured in our Mixture model Sec. 3). \n\nCrucially, we know that California is a different state but we do not know **how** it is different. In fact, the Mixture model (see Section 3), would sample from the union of South Dakota and California and we see that it yields worse results than just $D_{s_1}$ in the sequential model with a smaller n (Figure 3a vs Figure 4a). We also know that Louisiana could be as different from South Dakota as California is. However, adding data from Louisiana improves outcomes (Figure 9 vs Figure 3a). We identify Louisiana as a useful state to add using the heuristic we propose (Equation 1).   \n\n\n**Answers to the reviewer\u2019s questions**: \n> I see two closing parentheses while having only one open one.\n\nThis is a typo where there is an extra closed parenthesis symbol that we will update the paper to exclude the second \u201c)\u201d. \n\n> What do you mean saying \u201cthat is similar to the test distribution\u201d it is unclear\n\nIn our problem setup, the first source $D_{s_1}$ is very small and limited. For example, if you are a very small state (e.g. South Dakota) or a local hospital with a limited number of ICU records. We assume that to evaluate the trade-offs between training on more data vs induced distribution difference, the deployment dataset may be different from the first source but is likely to be most similar to the first source (e.g. $D_{s_1}$). \n\n> I do not understand: at the beginning of the work it was stated that \u201ctest set\u201d is the union. Here, it is stated that it is just D_d_1.\n\nIn section 3, we state that in the data accumulation setting, the **training set** is a union of sources. Could the reviewer tell us specifically where we said the test set is the union? \n\nWe hope our explanations have clarified the novelty of our contributions. As far as we are aware, we agree with the reviewer that there is no prior work that models the mechanisms that induce distribution shift when dataset size is scaled up. Moreover, our theoretical contributions are validated by our empirical results in different settings, and our practical metric bridges our theoretical model with empirically observed divergences. \nWe are happy to continue answering any questions to help the reviewer better understand the contributions of our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166064598,
                "cdate": 1700166064598,
                "tmdate": 1700166064598,
                "mdate": 1700166064598,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RTDk9i4YwZ",
            "forum": "j5EbZEyK9I",
            "replyto": "j5EbZEyK9I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_4rSz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_4rSz"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces models for data composition changes in single-source and multi-source settings, analyzes the effects of data scaling on performance, and presents empirical findings for three real-world datasets. The authors also use a heuristic for determining when the addition of more data could be detrimental based on measuring excess KL divergence. The paper only focuses on tabular data scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- I appreciate the theoretical result provided by the authors as Lemma 3.2. \n- The authors evaluate on 3 real-world datasets and conduct multiple diverse experiments throughout the paper."
                },
                "weaknesses": {
                    "value": "In my opinion, there are multiple issues with the work originating from the simplicity of the findings, mismatch in motivation/setting, and a lack of consistent evaluation throughout. I provide more details on these below, but due to these reasons I am leaning towards rejection as I believe the paper does not meet the bar for acceptance at ICLR:\n\n- **Simple Empirical Findings and Lack of Generalizability**: The experiments conducted as well as the results obtained are quite simple. Both the Sequential and Mixture model settings are quite trivial, and the obtained results are unsurprising to me. For instance, it seems intuitive that multi-source dataset scaling could lead to worse outcomes on a reference test set, and that it might lead to better generalization (Section 5). The only results that look at data scaling more deeply are located within Section 5.1, but still by themselves those cannot motivate this work. Furthermore, the results obtained (as well as the approach undertaken) are highly dataset dependent (for e.g., what if I sample the reference test set differently for Folktables? Or use a different state altogether?) This issue is also showcased via the accuracy results for both the Yelp and MIMIC-IV datasets under the Sequential paradigm (refer to Figure 6a and Figure 7a, respectively). These figures (for obvious reasons) show very different trends across both datasets for the same model.\n- **Inconsistent Evaluation Across Datasets**: The experiments are mostly conducted on the Folktables dataset, with a few results for Yelp and MIMIC-IV. For consistency in evaluation, all the datasets should be used and conclusions can be drawn from the results more adequately. For instance, all the results for Section 5.1 (such as those on the generalized test set) consider only the Folktables dataset, and Yelp and MIMIC-IV are not considered. Furthermore, it is not always mentioned in the text when the other datasets are being used and what the motivation is to discard others, which can be confusing for readers.\n- **Limited Scope and Applicability**: The biggest drawback of the work is the mismatch in whether this work tackles a useful practical problem and its actual motivation. The original outlined motivations in the abstract and introduction imply that the paper will aim to provide more insights on data scaling in useful practical scenarios. However, the work only considers tabular data and very simple models (the most complex is the MLP). The focus on tabular datasets significantly narrows the paper's scope, especially considering that data scaling is a critical concern in large language models (LLMs) and other domains outside of tabular data (such as NLP and Vision). The paper fails to provide insights or implications for these broader and arguably more impactful areas (such as deep learning), limiting its relevance, scope and applicability. In its current form, I do not think the paper provides insights that are useful for a real-world practical application scenario."
                },
                "questions": {
                    "value": "- Could the authors provide an appropriate justification or real-world scenario as an example for concentrating exclusively on tabular datasets? In this scenario, if models are not deep learning based would they be prone to large data scaling issues?\n- In this simpler paradigm with models such as Logistic Regression etc, instead of continuously adding more data for generalizability, would it not make sense to just focus on approaches that curb distribution shift and retrain the model cyclically over certain time periods? \n- Please feel free to respond to any of the other weaknesses listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824811973,
            "cdate": 1698824811973,
            "tmdate": 1699636942467,
            "mdate": 1699636942467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Z5px19oWm",
                "forum": "j5EbZEyK9I",
                "replyto": "RTDk9i4YwZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification of our contribution and justification of our scope (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reading through our paper and raising interesting questions. We would like to reiterate the contribution and scope of our paper in our response and clarify some misconceptions. \n\n**Answers to the reviewers\u2019 questions**: \n> Could the authors provide an appropriate justification or real-world scenario as an example for concentrating exclusively on tabular datasets? In this scenario, if models are not deep learning based would they be prone to large data scaling issues?\n\nTabular datasets remain fundamental in many applications such as surveys (e.g. census, public health), loan decisions, recidivism prediction, portfolio optimization, and recommender systems. For fairness questions, in particular, Adult and COMPAS are, by far, the most studied datasets by far ([1] Figure 1). Given known issues with the Adult dataset, Folktables (Ding et al.) was developed to provide a wider sample of data for developing policy decisions based on census data. How this much larger sample of census data should be used is a question we seek to answer through our work; as we show, using all available data is not necessarily the best solution. \nWe point to Figure 10 in the appendix where we show the benefits of increasing dataset size. We see that accuracy and worst group accuracy both improve while group disparities decrease monotonically with more data in the tabular setting. This motivates our work in questioning whether these benefits of more data can be applied to all states, even the smaller ones like South Dakota. \n\n> In this simpler paradigm with models such as Logistic Regression etc, instead of continuously adding more data for generalizability, would it not make sense to just focus on approaches that curb distribution shift and retrain the model cyclically over certain time periods?\n\nWe want to emphasize that state-of-the-art models for tabular datasets are not deep learning models, but tree-based methods and simply MLPs (NN, GB, XBG in Section 5) [2, 3]. In the settings we study, extremely large general tabular datasets are available, but the benefits of incorporating massive tabular data for a specific task in a subset of the data are unknown. Our work follows a line of data-centric works that study the effect of data composition on different state-of-the-art models for the task at hand.\nIn fact, our excess KL metric is designed to evaluate distribution shifts induced by new data sources in order to select the best dataset source. Our technique is a version of curbing distribution shift. \n\n**Response to Weakness**\n\n**LLMs and scope of work**: \n> The focus on tabular datasets significantly narrows the paper's scope, especially considering that data scaling is a critical concern in large language models (LLMs) and other domains outside of tabular data (such as NLP and Vision).\n\nIndeed, we agree with the reviewer that running similar experiments on LLMs, and maybe also VLMs would increase the scope of our paper. However, due to resource constraints, pretraining LLMs from scratch with different data sources is currently outside of our capabilities and our scope. However, we emphasize that the folklore of more data being better is ubiquitous in machine learning even outside of the LLM setting. Furthermore, the question of how many EHR, loan approval, and recommendations datasets should be combined is important for practical decision-making. Our paper gives guidance in this common setting in terms of how data composition can be related to scaling matters. \n\n**Generalizability**\n> Furthermore, the results obtained (as well as the approach undertaken) are highly dataset dependent (for e.g., what if I sample the reference test set differently for Folktables? Or use a different state altogether?) \n\nWe agree that using a different state altogether might result in different results. However, the contribution of our work is not to claim that \u201cdata scaling benefits are **always** eclipsed by distribution shift\u201d (this conclusion cannot be true if $D_{s_2}$ is sufficiently close to $D_{s_1}$). What we are trying to answer is \u201c**How** does data composition impact model performance under scaling\u201d (Figure 1 of main text). Our contribution is to give a principled theoretical model for data accumulation that is flexible to describe effects regardless of the dataset. Our empirical results show a **there exists** claim where we want to urge caution when adding data to illustrate that there are splits of datasets that lead to undesirable effects."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166554463,
                "cdate": 1700166554463,
                "tmdate": 1700166554463,
                "mdate": 1700166554463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VsLZ5e3gla",
                "forum": "j5EbZEyK9I",
                "replyto": "l7FFGjEUlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7727/Reviewer_4rSz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7727/Reviewer_4rSz"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I would like to express my gratitude to the authors for their rebuttal. However, after going through the response my concerns largely remain. I have condensed some of these below, and would be happy to engage with the authors on them or if I have misunderstood something. If the authors can provide adequate justification, I am amenable to updating my score:\n\n* **Generalizability**: \n    * I am not convinced by the response provided. I understand the research question the authors seek to answer, but when accumulating data at scale, wouldn't practitioners automatically evaluate if the data being added is improving the predictive capability of a model (be measuring performance on a test set)? Also, if one were to employ k-fold cross validation over multiple test set strata, wouldn't this give a holistic picture of whether or not the data being added actually improves the model or if this performance is just specific to the test/validation set at hand? This is my primary concern with generalizability since the research question posed _cannot_ actually be answered in an effective or generalizable way (e.g. if states are different), and an evaluation via cross validation (which is already a part of most data science pipelines) needs to be undertaken irrespective. I also do not think any data science practitioner is just throwing large scale tabular data at these models without evaluating incremental performance gain using the added data. \n    * Regarding my previous question in the review, I am still not sure why a practitioner cannot just retrain their model cyclically and curb distribution shift. Basically, keep churning old training data out and adding recent data. If the current test set is sampled from the recent data distribution, this should curb distribution shift. This should ideally work and is also what is likely done in most production pipelines. In my opinion, this should have nothing to do with the models being classical or deep learning based as mentioned in the authors' response.\n\n* **LLMs and scope of work**: I agree with the authors about the importance of tabular data. But as pointed out, models for tabular data are simpler (and faster) to train, so scaling can simply be evaluated using cross-validation or evaluation on test sets. However, for LLMs (or deep learning models) this is not possible, due to the retraining time complexity. Therefore, where I think the work would be extremely useful is in fine-tuning LLMs or deep learning models (for e.g. for classification on the same datasets considered) and then undertaking the aforementioned scaling analysis. This is what I was suggesting in my review originally as pretraining LLMs or deep models is clearly not a viable option.\n\nGiven the concerns above, I will maintain my current score for now."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549339440,
                "cdate": 1700549339440,
                "tmdate": 1700549339440,
                "mdate": 1700549339440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dQQ4vszdeW",
            "forum": "j5EbZEyK9I",
            "replyto": "j5EbZEyK9I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_UNBZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_UNBZ"
            ],
            "content": {
                "summary": {
                    "value": "Describes how the way data is added to a model (data accumulation) affects performance against the reference test set. Two methods are presented. First, a data mixture method where data is added in the same subgroup mixture as the original dataset. Second is sequential - this is where datasets are added one after another with no guarantee that the mixture of subgroups is the same as the original data. The authors point out that sequential additions can harm model performance especially when there are distinct distribution differences between the datasets (i.e. high KL divergence)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Well written and useful analysis, and especially suitable for this track.  It guides researchers on what to expect when adding new datasets, what circumstances lead to good outcomes and which one might not. Also great caution to the assumption that more data (except perhaps noisy or corrupt data etc) is always good for the model."
                },
                "weaknesses": {
                    "value": "Overall this was an interesting paper to read. Most of these are about clarifications and how the authors have interpreted their results.\n\nIt is unclear how the target dataset is constructed. It should not matter in the mixture set-up but it would be consequential in the sequential set-up. The target set should be a sample from all n datasets, unless it is updated each time a new dataset is added.\nIt is also not clear how long the model is re-trained with the new examples. This can help us better understand if the examples can\u2019t be learned or if the model just did not have as many iterations to incorporate these new examples. \nThe implications of this work are not clear. In real-world settings, if there exists a datasets similar to one that we currently have but has high divergence does it mean it should not be included in the analysis? Doesn\u2019t not doing so restrict the model from better generalising? Eg. Yelp reviews in MN vs SD.\nThirdly, it looks like adding more data reduces performance disparity between groups and in general helps the least performing group. Reducing disparity is perhaps indicating that the model is generalising better and getting more robust and these should be good things."
                },
                "questions": {
                    "value": "1. How is the reference test set constructed? If it's in the appendix, it should included in the main paper because it is consequential.\n2. How long do you retrain after adding new datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7727/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7727/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7727/Reviewer_UNBZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826461145,
            "cdate": 1698826461145,
            "tmdate": 1699636942337,
            "mdate": 1699636942337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r1fARfi80h",
                "forum": "j5EbZEyK9I",
                "replyto": "dQQ4vszdeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to reviewer questions and clarifications"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read and understand our work. We agree with the reviewer that our work is useful in the datasets track \u2013 to help formalize practical questions raised by the distribution shift that arises from data scaling. \n\n**Answering Reviewer Questions**\n1. The reference test set is constructed in our experiments from a very small initial training source $D_{S_1}$ (e.g. South Dakota). We make this assumption to simulate a setting where there is some limited access to the initial dataset of interest but it is then unclear if adding more data would help with outcomes on this initial dataset. We assume that the eventual deployment set might be different from the first source but is likely most similar to the first source.\n2. We train from scratch at each dataset size presented in our plots. Thus, each training trajectory shows 5 models over 50 seeds (not 5 - typo in the main text) at for each size n. \n\n**Implications of our work**\n> The implications of this work are not clear. In real-world settings, if there exists a datasets similar to one that we currently have but has high divergence does it mean it should not be included in the analysis? Doesn\u2019t not doing so restrict the model from better generalising? Eg. Yelp reviews in MN vs SD. \n\nIn the real world, if we knew a dataset is vastly different (e.g. loan applications for credit cards vs loan applications for mortgages), it does make sense to not include it. However, the question that arises is, what level of divergence would harm my model, and what level is acceptable? In all of our datasets, we are looking at the differences that might arise **within** a dataset. We show that adding data from states (near and far) with reasonable divergences (e.g. there is SOME shift) improves outcomes. Especially when the starting dataset is small (and undiverse) excluding data in the same dataset might not be the best choice. This is a very nuanced point and we will better explain it in our paper. \n\n> Thirdly, it looks like adding more data reduces performance disparity between groups and in general helps the least performing group. Reducing disparity is perhaps indicating that the model is generalising better and getting more robust and these should be good things.\n\nReducing performance disparity by adding more data is indeed a desirable goal. We hope to show that by measuring not just overall accuracy but also disparity and worst group performance, understanding the consequences of data scaling is a multi-faceted pursuit and KL divergence as a lone metric is only just a first heuristic. We hope our work will inspire future work to develop further metrics for determining when to add data for properties like disparity and worst group robustness in particular.  \n\nWe hope our responses have clarified the questions raised. If we can further provide any explanations to improve the reviewers confidence in our work, please do not hesitate to raise further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338587062,
                "cdate": 1700338587062,
                "tmdate": 1700338587062,
                "mdate": 1700338587062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rfGyYojZTi",
            "forum": "j5EbZEyK9I",
            "replyto": "j5EbZEyK9I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_r5EZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7727/Reviewer_r5EZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors model data accumulation from multiple sources and present an analysis of two strategies that result in adding more data, degrading the overall model performance. They empirically demonstrate on three real-world datasets that adding training data can reduce overall accuracy and reduced worst-subgroup performance while introducing further accuracy disparities between subgroups."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the authors tackle the well-known issue that more data does not always lead to better machine learning outcomes: data quality (the whole dataset composition should mirror the data we will receive at inference time) is of primary importance.\n - the paper is of good quality: the authors propose several scenarios and work with real-world data to draw conclusions\n - the paper is well-structured and written"
                },
                "weaknesses": {
                    "value": "- the authors did not consider research on domain adaptation, which could be considered key in this particular setting\n - the authors did not check for data-based techniques used in active learning settings that can help identify data relevant to machine learning models"
                },
                "questions": {
                    "value": "We consider this research interesting and relevant. Nevertheless, we would like to point to the following improvement opportunities:\n1. \"training data is often considered to be set in stone and imposed as a pre-existing and static constraint.\" -> The authors should consider that while sometimes this is true, the fact that distribution shift exists and takes place should be, therefore, evaluated on training sets too. We encourage you to reframe the sentence stating such an evaluation as a best (and often forgotten) practice.\n2. *Criteria for rejecting more data*: The problem posed by the authors resembles active learning and some specific data-based strategies. Furthermore, some research has been performed on active learning and stopping criteria. The authors may be interested in researching these areas. Here, we list two works they may find useful: (a) Fu, Yifan, Xingquan Zhu, and Bin Li. \"A survey on instance selection for active learning.\" Knowledge and information systems 35 (2013): 249-283, and (b) Zhang, Yexun, et al. \"Stopping criterion for active learning with model stability.\" ACM Transactions on Intelligent Systems and Technology (TIST) 9.2 (2017): 1-26.\n3. *Experimental setup*: we consider the experiments valuable and valid. Nevertheless, the authors should consider enriching them with some scenarios where domain adaptation is used to mitigate distribution differences. The authors may be interested in the following work: Farahani, Abolfazl, et al. \"A brief review of domain adaptation.\" Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE 2020 (2021): 877-894.\n4. *Results and analysis*: Do the authors venture some hypothesis as to why the XGB model is robust to data from different distributions, suffering a lower accuracy loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7727/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7727/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7727/Reviewer_r5EZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840046476,
            "cdate": 1698840046476,
            "tmdate": 1699636942233,
            "mdate": 1699636942233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4CP3On5Amx",
                "forum": "j5EbZEyK9I",
                "replyto": "rfGyYojZTi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7727/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to reviewer questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful suggestions on our work and we value their positive comments on the quality and presentation of our work. \n\nPlease see our answers to reviewer questions below: \n1. This is a good point, we will indeed rephrase this sentence to better acknowledge prior work in the distribution shift (in addition to our discussion in C.5 of our supplementary materials). \n\n2. We thank the reviewer for pointing out related work in active learning. We agree that it is important and we will include this in our work. We do highlight that our KL metric is distributional rather than for specific instances as described in equation 15 of Fu et. al. While Zhang et. al. takes a, notably different, model change approach to stop data addition, we believe it is still very wonderful prior work we will include to frame our contribution.  \n\n3. In our experimental setup, we specifically wanted to focus on data interventions rather than model interventions suggested by domain adaptation strategies. In our supplementary materials C.5 we discuss in detail our work in relation to domain adaptation in particular.\n\n4. Excellent question. While in our results we show that XGBoost does not suffer from an **accuracy drop**, the model still performs much worse than it would if more data from a more similar distribution were to be added. In Figures 11b and 11c (supplementary materials), we show that XGB continues to improve substantially when other sources are added like Louisiana. \n\nWe value these helpful comments for the reviewer. We believe that taking this data composition framework to understand scaling under possible distribution shifts is an important direction that will inspire future work in richer domains. We are happy to answer any further questions or add experiments and hope that the reviewer will vouch for our work!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338461183,
                "cdate": 1700338461183,
                "tmdate": 1700338461183,
                "mdate": 1700338461183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5tBApNtOKN",
                "forum": "j5EbZEyK9I",
                "replyto": "4CP3On5Amx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7727/Reviewer_r5EZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7727/Reviewer_r5EZ"
                ],
                "content": {
                    "comment": {
                        "value": "We have reviewed the authors' responses and have no further observations."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727664468,
                "cdate": 1700727664468,
                "tmdate": 1700727664468,
                "mdate": 1700727664468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]