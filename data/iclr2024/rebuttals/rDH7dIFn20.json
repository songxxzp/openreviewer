[
    {
        "title": "Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits"
    },
    {
        "review": {
            "id": "GGOQHDsoFw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_83pH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_83pH"
            ],
            "forum": "rDH7dIFn20",
            "replyto": "rDH7dIFn20",
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of dueling bandits with a variance-aware regret bound. It introduces an algorithm for contextual dueling bandits, which accounts for uncertainty in pairwise comparisons between arms and provides a variance-aware regret bound. The paper highlights the importance of considering uncertainty in decision-making scenarios and proposes an efficient algorithm with a regret bound that depends on the variance of comparisons, dimensionality of context vectors, and the time horizon. The authors demonstrate the effectiveness of their method through empirical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and easy to follow.\n\nThe proposed algorithm is the first algorithm with a variance-aware regret bound. It has some novelty, and the symmetric arm selection is new.\n\nThe authors also support their theoretical claims with empirical experiments on synthetic data."
                },
                "weaknesses": {
                    "value": "I feel this paper is an extension to [1], with similar algorithm proposed and regret analysis. The main algorithm has identical structure with the SAVE proposed in [1] based on the SupLin methodology, and it is not new and novel for the variance-aware contextual bandit problem. Under the generalized linear contextual dueling bandit setting used in the paper, we can regard $x_t - y_t$ as the contextual information, which would then be degeneralized to the ordinary generalized linear contextual bandit problem. I think the high similarity with the existing literature is the major weakness of this work.\n\nTypos and minor issues: \n1. title of subsection 4.3 (sysmmetric -> symmetric)\n2. I may overlook. What is $\\kappa$ in line 20 Algorithm 1 and Eqn. (4.3)?\n\n[1] Variance-dependent regret bounds for linear bandits and reinforcement learning: adaptivity and computational efficiency. Zhao et al., COLT."
                },
                "questions": {
                    "value": "In addition to my concerns in the above Weaknesses section,\n\n1. Is the variance of the noise $\\epsilon_t$ equal to $p_t(1-p_t)$ under the Bernoulli setting? In that case I feel the variance would be fully dependent on the arms selected in each round."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Reviewer_83pH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697332549383,
            "cdate": 1697332549383,
            "tmdate": 1699637029243,
            "mdate": 1699637029243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NMDyDC5bFE",
                "forum": "rDH7dIFn20",
                "replyto": "GGOQHDsoFw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 83pH"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We address your questions point-by-point as follows.\n\n**Q1**: Comparison with SAVE in [1] and algorithms for generalized linear bandit problem.\n\n**A1**: Compared with SAVE, the main difference is that we consider the problem in the generalized linear model and dueling bandit setting, which has not been studied in the literature before. Note that SAVE is for linear bandits, rather than generalized linear (dueling) bandits. The nonlinearity of the problem requires us to consider MLE instead of linear regression. Since the linear regression problem has a closed-form solution, the analysis of MLE is more difficult. \n\nCompared with the generalized linear bandit, there is a key difference. You cannot simply regard $\\mathbf{x} _ t- \\mathbf y _ t$ as the feature vector to reduce dueling bandits to multi-armed bandits. Note that the regret in dueling bandits is defined as $\\text{Regret}$ $(T) = \\sum _ {t} $ $(\\mathbf x _ t+ \\mathbf y _ t)^\\top \\boldsymbol{\\theta}^*$ $- 2 \\mathbf x ^ {*\\top} \\boldsymbol{\\theta} ^ *$. In this definition, the regret depends on $\\mathbf x _ t+ \\mathbf y _ t$ rather than $\\mathbf x _ t- \\mathbf y _ t$, i.e., it aims at minimizing the regret caused by both arms. This distinction poses a challenge for dueling bandit algorithms, necessitating a design that differs from the generalized linear bandit algorithm.\n\n----\n\n**Q2**: Typos and minor issues:\ntitle of subsection 4.3 (sysmmetric -> symmetric)\nI may overlook. What is $\\kappa$ in line 20 Algorithm 1 and Eqn. (4.3)?\n\n**A2**: Thanks for pointing out the typos. $\\kappa$ is the same as $\\kappa_\\mu$. We have revised them. \n\n----\n\n**Q3**: Is the variance of the noise equal to under the Bernoulli setting? In that case I feel the variance would be fully dependent on the arms selected in each round.\n\n\n**A3**: Yes, the noise equals $p_t(1-p_t)$ under the Bernoulli setting, and it depends on the arm being selected in each round. We study the problem in a general case where the arm set is time-varying and therefore the variance of arms can vary with respect to time and arms. Therefore, the variance $p_t(1-p_t)$ in each round is influenced by both the historical data and the potentially adversarial arm set $\\mathcal{A}_t$ in round $t$. Our result indicates that the regret of the problem should depend on the variance of the selected arms, which aligns with our intuition that comparing high-variance dueling arms pose a greater challenge than comparing lower-variance dueling arms.\n\n----\n\n[1] Zhao et al., 2023, Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency, COLT"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540076854,
                "cdate": 1700540076854,
                "tmdate": 1700540076854,
                "mdate": 1700540076854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eiGu0Kg7N2",
                "forum": "rDH7dIFn20",
                "replyto": "NMDyDC5bFE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8277/Reviewer_83pH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8277/Reviewer_83pH"
                ],
                "content": {
                    "title": {
                        "value": "Thanks a lot for your response"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed responses. I acknowledge the problem setting in this work is new, and I really appreciate the presentation of this work. However, I still hold concerns on the novelty of this work, given the existing literature, especially SAVE. \n\n1. Yes, the analysis of MLE in generalized linear model is more difficult. However, I feel it is already a well-studied problem. As the authors mention in their work, the bound (Eqn. 4.2) can be deduced by using the results from Zhao et al. 2023a. Therefore, there is no new contribution here.\n\n2. Compared with SAVE, I feel the only difference of the algorithm lies in line 8 in the pseudocode, where the author uses $x_t + y_t$ instead of $x_t - y_t$ for the exploitation term since the regret bound depends on $x_t + y_t$. And reason can be easily observed in Appendix C method 3 where the deduction is also not technically difficult. Therefore, I feel the theory is a mild extension to SAVE and not very novel.\n\nAdditional question: For the variance-aware contextual dueling bandit, the variance completely depends on the pulled arm $x_t$, while for the general variance-aware contextual bandit the variance can be fully independent. Do author think this extra information will bring in some more benefits for analysis? It would be better to have a discussion on this issue."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631089688,
                "cdate": 1700631089688,
                "tmdate": 1700631089688,
                "mdate": 1700631089688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u5irDELm9y",
            "forum": "rDH7dIFn20",
            "replyto": "rDH7dIFn20",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_Kvxm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_Kvxm"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of contextual dueling bandits, where the feedback is based on pairwise comparisons between arms. The authors focus on scenarios where the binary comparisons are generated from a GLM. The authors proposed VACDB algorithm that adapts to the variance of the pairwise comparisons, leading to potentially better performance in scenarios with varying levels of uncertainty. The variance-aware regret bound of order $O\\left(\\sqrt{d\\sum_{t=1}^{T} \\sigma_t^2} + d\\right)$, which also aligns with intuitive expectations, reducing to $O(d)$ in deterministic scenarios. The authors validate their approach through experiments on synthetic data, demonstrating the advantages of VACDB over previous variance-agnostic algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The primary contribution of the paper is the introduction of a new algorithm VACDB (Variance-Aware Contextual Dueling Bandits), which incorporates a SupLinUCB-type approach to handle the contextual information and provide a variance-aware regret bound. \n- The regret bound $O\\left(\\sqrt{d\\sum_{t=1}^{T} \\sigma_t^2} + d\\right)$ proposed by the authors provides a more nuanced performance measure that reflects the difficulty of the decision-making problem.\n- Beyond the specific algorithm and regret bound, the paper also contributes to the theoretical understanding of generalized linear bandits, correcting an issue in the existing analysis of the MLE estimator.\n\nOverall, I think the paper enhances our understanding of decision-making in dueling bandits scenarios by explicitly accounting for the uncertainty in pairwise comparisons, providing both theoretical insights and practical algorithms to address the challenge on variance-awareness algorithms."
                },
                "weaknesses": {
                    "value": "- The paper primarily conducts experiments on synthetic data to validate the proposed algorithm. While this is a common practice, the performance of the algorithm in real-world scenarios might differ. To strengthen the paper, the authors could include experiments on real-world datasets, particularly those related to the applications mentioned like ranking, recommendation systems, or any human-interactive system, ensuring the practicality and robustness of the algorithm in diverse settings.\n- The concept of a layered design for bandit algorithms, which the authors adopt in the VACDB algorithm, has been previously explored in the literature. Layered or epoch-based approaches are widely used in bandit algorithms to balance exploration and exploitation in stochastic settings. The paper could benefit from a more thorough discussion on how the layered design in VACDB specifically contributes to or differs from existing approaches.\n- The VACDB algorithm relies on the computation of regularized MLE for parameter estimation. The computation of these estimators is known to grow in complexity with the number of iterations, potentially leading to scalability issues, especially in settings with a large number of arms, contexts, or iterations."
                },
                "questions": {
                    "value": "- There is a minor typo on the second line of Introduction: \"arm\" -> ``arm\"\n\n\n================================================\n\n**After Rebuttal:**\n\nI appreciate the response from the authors, and I am going to keep my original rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Reviewer_Kvxm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698420820938,
            "cdate": 1698420820938,
            "tmdate": 1700605383128,
            "mdate": 1700605383128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KzNSXO9QvF",
                "forum": "rDH7dIFn20",
                "replyto": "u5irDELm9y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kvxm"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback. We address your questions below.\n\n**Q1**: Performance of the algorithm on real-world datasets.\n\n**A1**: To showcase the performance of our algorithms in a real-world setting, we have added a new experiment in Appendix B. We use EventTime dataset [1]. In this dataset, $K = 100$ historical events are compared in a pairwise fashion by crowd-sourced workers. The data contains binary response indicating which one of the events the worker thinks precedes the other. There is no side information \n$\n    \\mathcal A = \\\\{ \\mathbf x_i, i \\in [K] \\\\},\n$\nor the true parameter $ \\theta^*$ readily available in the dataset.  Thus, we estimate them with pairwise comparison data. To achieve this, let $C_{ij}, i,j \\in [K]$ be the number of times event $j$ precedes event $i$ labeled by the workers. The following MLE estimator is used:\n$$\n\\mathop{\\mathrm{argmax}} _  { \\\\{\\mathbf{x} _ i \\\\}, \\theta} \\sum_{i \\in [K]} \\sum_{j \\in [K]} C_{ij}\n\\log\n(\n    \\sigma((\\mathbf x _ i - \\mathbf x _ j)^\\top\\theta)\n).\n$$\nWith the estimated $\\mathcal A$ and $\\theta^*$, it is then possible to simulate the interactive process. We can see that after about $2500$ rounds, our algorithm starts to outperform $\\texttt{MaxInP}$ in terms of cumulative regret. \n\n----\n\n**Q2**: The paper could benefit from a more thorough discussion on how the layered design in $\\texttt{VACDB}$ specifically contributes to or differs from existing approaches.\n\n**A2**: Thanks for your suggestion. We have provided a detailed discussion in Appendix A in our revision. Our approach is different in the elimination phase, where we construct our confidence set with a variance adaptive radius to determine the confidence radius (lines 10-12).  Therefore, besides the layer number, our algorithm also leverages the variance information in the exploration. Moreover, we construct a carefully chosen weight dependent on the layer  (line 15). It is used in the calculation of the regularized MLE estimator.\n\n----\n\n**Q3**: Computation of the MLE estimators grows in complexity with the number of iterations.\n\n**A3**:  The MLE estimator is widely used in the literature of generalized linear bandit and dueling bandits, such as [2] and [3]. Although the calculation can be expensive in some cases, there are some literally tractable solutions to that. The regularized MLE can be formulated as a finite-sum offline optimization problem. For many widely used models, such as the Bradley-Terry-Luce (BTL) model, the regularized MLE is a strongly convex and smooth optimization problem. We can solve it using accelerated gradient descent [4] and SVRG [5], both of which achieve a linear rate of convergence. This can mitigate the scalability issues caused by the increasing number of iterations. The regularized MLE estimator can also be solved by an online learning algorithm such as in [6,7].\n\n----\n\n**Q4**: There is a minor typo on the second line of Introduction: \"arm\" -> ``arm\" \n\n**A4**: Thanks for pointing that out. We have revised it.\n\n----\n\n[1] Zhang et al., 2016, Crowdsourced top-k algorithms: An experimental evaluation. VLDB.\n\n[2] Li et al., 2017, Provably optimal algorithms for generalized linear contextual bandits, ICML\n\n[3] Saha 2021, Optimal algorithms for stochastic contextual preference bandits, NeurIPS\n\n[4] Nestorov 2003, Introductory lectures on convex optimization: A basic course,\n\n[5] Johnson & Zhang, 2013, Accelerating Stochastic Gradient Descent using Predictive Variance Reduction, NeurIPS\n\n[6] Jun et al., 2017, Scalable generalized linear bandits: online computation and hashing. NeurIPS\n\n[7] Zhao et al., 2023, Optimal online generalized linear regression with stochastic noise and its application to heteroscedastic bandits, ICML"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539655263,
                "cdate": 1700539655263,
                "tmdate": 1700539655263,
                "mdate": 1700539655263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YgF4BafE9J",
            "forum": "rDH7dIFn20",
            "replyto": "rDH7dIFn20",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_W8bc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_W8bc"
            ],
            "content": {
                "summary": {
                    "value": "This work studied the stochastic contextual dueling bandits. It proposed the VACDB algorithm, which is a new SupLinUCB-type algorithm. It provided a variance-aware bound on the regret of the proposed algorithm. This work presented a detailed review on existing literature and clarified the novelty of the proposed algorithm. It also evaluated the performance of the algorithm with numerical experiments.\n\n\n\n==============\n\nI appreciate the response from the author(s). I may increase the score if they further resolve my concern."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is in general well organized and easy to follow.\n1. The variance is considered in the dueling bandit setting.\n1. I appreciate the detailed review on existing literature, and the clarification on the novelty of the proposed algorithm and the differences from existing algorithms (especially SupLinUCB-type ones)."
                },
                "weaknesses": {
                    "value": "1. The variance $\\sum_{t=1}^T \\sigma_t$ in the regret bound is a random variable. I think it would be much better to involve a term that indicates the variance of the instance in some sense but is not random in an expected bound.\n    1. The appearance of $\\sum_{t=1}^T \\sigma_t$ indicates that even if we know $X_t$ for all arms and $\\theta^*$, we may not know the value of the derived upper bound.\n1. I wonder if it is possible to derive a lower bound for the problem. If not, may the author(s) clarify the analytical challenge?"
                },
                "questions": {
                    "value": "Please refer to the **Weaknesses** section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Reviewer_W8bc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727855142,
            "cdate": 1698727855142,
            "tmdate": 1700622862921,
            "mdate": 1700622862921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7KRH3j3ooA",
                "forum": "rDH7dIFn20",
                "replyto": "YgF4BafE9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W8bc"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We address your questions as follows.\n\n**Q1**: The variance in the regret bound is a random variable. I think it would be much better to involve a term that indicates the variance of the instance in some sense but is not random in an expected bound. We may not know the value of the derived upper bound when knowing the exact model.\n\n**A1**: Yes, the randomness of the variance is from the arm being chosen in each round, which further depends on historical data. The setting we study is quite general where the arm set is time-varying and therefore the variance of arms can vary with respect to time and arms. That\u2019s why the regret depends on a random variable representing the variance of the chosen arms in each round. We view this as a strength rather than a weakness of our regret bound. \n\nWhen we restrict our setting to a special case where the variances for all the pairwise comparisons are identical, the randomness of $\\sigma_t^2$ can be removed in the regret bound. But this is similar to the worst-case regret implied by our variance-aware regret.\n\n----\n\n**Q2**: Is it possible to derive a lower bound for the problem?\n\n**A2**: We study this problem in a general setting where the variance of instances can vary over time and across different arms. In this context, a variance-dependent lower bound is difficult and not found even in the literature on stochastic bandit settings. Deriving that lower bound stands as an open problem. The proof is challenging especially when the variance can change over time. For example, consider a scenario where initially there is no variance, but later, substantial variance emerges. In such cases, accurate early exploration could make correct policies, effectively mitigating the impact of later variance. However, the variance-dependent regret bound in the literature does not inherently possess this characteristic. In general, we are not sure if it is possible, and leave it as an interesting future work.\n\nAlthough getting a lower bound in the general case is hard if not impossible, we can show our optimality in two special cases:\n\n (1) when we consider the worst case, the variance is upper bounded by $1/4$, our result matches the $d\\sqrt{T}$ lower bound proved in [1]; and (2) when the preference is deterministic, our result is $\\tilde O(d)$, which is also nearly optimal.\n\n----\n[1] Bengs et al., 2022, Stochastic contextual dueling bandits under linear stochastic transitivity models, ICML"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538576430,
                "cdate": 1700538576430,
                "tmdate": 1700538576430,
                "mdate": 1700538576430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eEAqGIj3X2",
                "forum": "rDH7dIFn20",
                "replyto": "7KRH3j3ooA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8277/Reviewer_W8bc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8277/Reviewer_W8bc"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "1. As the author(s) said that 'When we restrict our setting to a special case where the variances for all the pairwise comparisons are identical, the randomness of $\\sigma_t^2$ can be removed in the regret bound.', I am curious what is this bound.\n2. If we consider the worst case 'the variance is upper bounded by $1/4$', do the upper and lower bounds match regarding terms except for $d$?\n\nI appreciate that the author(s) include these two points in the main paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622797326,
                "cdate": 1700622797326,
                "tmdate": 1700622797326,
                "mdate": 1700622797326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Eo4o0R2juV",
            "forum": "rDH7dIFn20",
            "replyto": "rDH7dIFn20",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_2YSg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8277/Reviewer_2YSg"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies contextual dueling bandits, where binary comparison is generated from a generalized linear model. The work proposes a new framework to obtain variance aware regret guarantees. This work further provides empirical results comparing the algorithm against baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem setup is well laid out and easy to follow with the precisely required assumptions."
                },
                "weaknesses": {
                    "value": "The paper is not self contained and requires reader to go through multiple papers for example in section 4.2."
                },
                "questions": {
                    "value": "No questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8277/Reviewer_2YSg"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840409035,
            "cdate": 1698840409035,
            "tmdate": 1699637028917,
            "mdate": 1699637028917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "woOv1KRZvV",
                "forum": "rDH7dIFn20",
                "replyto": "Eo4o0R2juV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2YSg"
                    },
                    "comment": {
                        "value": "Thank you for your strong support. We address your question as follows.\n\n**Q1**: The paper is not self contained and requires the reader to go through multiple papers for example in section 4.2.\n\n**A1**: Thanks for your suggestion. In the main text, we have added Section 4.1 to give an overview of the algorithm and introduce the multi-layered structure in more detail. We have also added Appendix A to provide a detailed introduction to our algorithm and a comparison with previous works."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538240598,
                "cdate": 1700538240598,
                "tmdate": 1700538623378,
                "mdate": 1700538623378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]