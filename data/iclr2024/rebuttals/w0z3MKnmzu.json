[
    {
        "title": "ResPrompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models"
    },
    {
        "review": {
            "id": "EW2jZltFk1",
            "forum": "w0z3MKnmzu",
            "replyto": "w0z3MKnmzu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_z49G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_z49G"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to improve the multi-step reasoning ability of Chain-of-Thought (CoT) prompting. When coping with complex problems, CoT tends to only leverage the reasoning outcomes from one prior steps and ignores those from several prior steps. Thus, CoT only exploits the linear reasoning structure in problem-solving and struggles to look back on several steps earlier, whose reasoning scheme is represented as a graph. To address this issue, this work introduces a new prompting method, called RESPrompt, to explicitly add connections to previous reasoning steps in prompting exemplars. Experiments on the family of Llama models show the performance improvement of RESPrompt over the Vallina CoT method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Enhancing the complex reasoning ability of CoT-like prompting methods is a crucial problem in prompt-based reasoning of Large Language Models (LLMs). It is promising to construct the graph-like reasoning scheme and back-track prior reasoning steps to facilitate next-step inference for complex tasks."
                },
                "weaknesses": {
                    "value": "However, the reviewer has several concerns about the Methodology and Experiment.  \nMethodology:  \n1. First, the Vallina CoT seems to already leverage the reasoning results from previous steps for next-step inference, as shown in Figure 2 (b). In Step 4, the Vallina CoT uses the results obtained in Step 1,2,3 to infer the money Tobias earned from shoveling driveways. Thus, the reviewer is quite skeptical that Vallina CoT can intrinsically leverage the results from several prior steps for next-step inference, especially on GPT-3.5 and GPT-4 models with increased model capability.   \n2. Concerns about Novelty and Generality. Although RESPrompt is claimed to match the graph structure of reasoning in complex tasks, it also revokes a linearly step-by-step reasoning scheme using more crafted CoT-like prompting examples. Thus, the reviewer is concerned with the novelty of RESPrompt. Moreover, it may require extensive labor and expertise to 1. manually design the prompt examples for RESPrompt and 2. establish optimal residual connections within multi-step reasoning. These issues make RESPrompt less generalizable to new tasks.  \n3. What are the 'provided question conditions' in Line 11, Page 4?\n\n\nExperiments:  \n1. RESPrompt is only compared with the Vallina CoT method. Empirical comparison between RESPrompt with representative methods such as Least2Most, and CoT-SC is lacking.  \n2. Moreover, the reviewer is curious about whether the performance gain of RESPrompt over the Vallina CoT diminishes when testing on more powerful models such as GPT 3.5 and GPT 4.  \n3. The performance gain of RESPrompt over the Vallina CoT is marginal. As shown in Figure 6, RESPrompt only enjoys significant performance gains in specific few-shot prompting settings on some datasets. This raises concerns about the effectiveness of RESPrompt in general settings and tasks."
                },
                "questions": {
                    "value": "The authors are encouraged to address the concerns in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653810191,
            "cdate": 1698653810191,
            "tmdate": 1699636240868,
            "mdate": 1699636240868,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WbUOCxN9vU",
                "forum": "w0z3MKnmzu",
                "replyto": "EW2jZltFk1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer z49G - Part1"
                    },
                    "comment": {
                        "value": "Dear Reviewer z49G,\n\nWe sincerely appreciate your in-depth and insightful feedback and comments. In the following, we provide new experiments and explanations to your concerns one-by-one. We really appreciate your suggestions and hope our response could address your concerns. We also welcome further discussion if you have any additional questions.\n\n\n### **Methodology:**\n\n> **W1: Vanilla  CoT seems to already leverage the reasoning results from previous steps in Figure 2 (b). Vallina CoT can intrinsically leverage the results from several prior steps especially on GPT-3.5 and GPT-4 models with increased model capability.**\n\n\n**A1:** We greatly appreciate the reviewer for this valuable feedback. We would like to clarify that Figure 2 (b) **is one CoT Prompt, not the output of LLMs**. We showcase this CoT prompt in Figure 2 (b) to illustrate that its reasoning flow is linear (Figure 2 (c)), and therefore cannot recover the reasoning graph shown in Figure 2 (d). Instead, we present the output of LLMs with vanilla CoT prompts in Figure 8. In the step 3 of Figure 8, the LLaMA model fails to understand the meaning of the results from previous steps when using CoT prompts alone. In contrast, with ResPrompt, the model is able to establish correct connections with the previous results in step 3 and derive the correct final outcome. This case study in Figure 8 demonstrates that vanilla CoT prompting fails to effectively utilize the results from prior steps.\n\n\nTo assess whether ResPrompt continues to enhance multi-step reasoning compared to vanilla CoT on more capable LLMs, we conduct comparison experiments between ResPrompt and vanilla CoT using GPT3.5 and GPT4 on the GSM8K dataset. The results, presented in the following table, demonstrate that ResPrompt is effective in improving complex reasoning even for more capable OpenAI models.\n\n\n| | Vanilla CoT (8-shot)  | ResPrompt (8-shot) |\n|:-----------:|:----------:|:------------------------:|\n|GPT3.5| 73.0 | 76.0 |\n|GPT4|  91.0 | 93.0|\n\nSince the reviewer also asks a question for this experiment in the following, we will present more details of this experiment in the following.\n\n\n\n\n> **W2: Novelty concern: ResPrompt also revokes a linearly step-by-step reasoning scheme and Generality concern: ResPrompt requires extensive labor and expertise.**\n\n\n**A2:** We sincerely thank the reviewer for this in-depth and valuable comment. \n\nFor novelty, we would like to clarify using the prompt example in Figure 2. When comparing the reasoning flow of CoT (Figure 2 (c)) with the underlying reasoning graph of the question (Figure 2 (d)), it shows that the linear structure in Figure 2 (c) misses several links necessary to reconstruct the graph structure in Figure 2 (d). ResPrompt is designed to add these missing links. It is important to note that although ResPrompt builds upon the linear prompt of CoT, it enables the model to realize the necessary intermediate results in earlier steps by repeating them. This repeating operation effectively adds the missing links and helps the model become aware of the graph structure. In summary, the novelty of ResPrompt lies in its **ability to recover the underlying complex reasoning graph of multi-step questions, which CoT is unable to achieve**.\n\n\nFor generality, we would like to express our appreciation to the reviewer for raising this question. We would like to clarify that while ResPrompt indeed requires labor and expertise to design the prompts, this is **not a unique limitation of our approach**. To the best of our knowledge, almost all prompting-based reasoning approaches in LLMs require extensive labor and expertise [1-4]. For example, in the vanilla CoT paper [1], the authors manually design prompts for each benchmark. And in [4], the authors manually work principles to select examples to build prompts. However, we agree that the automatic generation of ideal prompts for reasoning is a promising research question for future study.\n\n\n> **W3: What are the 'provided question conditions' in Line 11, Page 4?.**\n\n\n**A3:** We really thank the reviewer for seeking clarification on this point. \"Provided question conditions\" refers to the conditions explicitly given in the question itself, rather than intermediate results derived from earlier steps during the problem-solving process. For example, in step 6 of Figure 2 (e), two conditions are required: 1) \"```$35``` earned from shoveling\" and 2) \"```$7``` to shovel\". Here the ```$35``` \u201cearned from shoveling\u201d is an intermediate result from earlier steps, while \u201che charges ```$7``` to shovel\u201d is a given condition from the question in Figure 2(a)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635580913,
                "cdate": 1700635580913,
                "tmdate": 1700635580913,
                "mdate": 1700635580913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gFoq2Hq3VR",
            "forum": "w0z3MKnmzu",
            "replyto": "w0z3MKnmzu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_BjpG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_BjpG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a residual connection prompting method for enhancing multi-step reasoning in large language models. The proposed ResPrompt aims at reconstructing the reasoning graph within prompts, which is implemented by adding missing links to transform the linearly structured CoT prompts into graph-like structures. Experiments are conducted via LLaMA and LLaMA2 on 6 benchmarks covering mathematical reasoning, sequential reasoning, and commonsense reasoning. The experimental results show the benefits of the proposed ResPrompt and some interesting findings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The proposed approach uses a simple method to recover complex reasoning graphs and reduce reasoning difficulty, which is easy to implement. According to the experimental results, the proposed approach shows significant improvements. The ablation study and analysis provide interesting findings."
                },
                "weaknesses": {
                    "value": "Please see the questions listed below."
                },
                "questions": {
                    "value": "Q1: For the performances according to the number of reasoning steps (Figure 3), it is shown that ResPrompt performances are fluctuant as the number of reasoning steps grows in the MathQA dataset, and in the questions with steps >= 5 it shows similar performance with the compared approaches. What are the possible reasons for this observation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2966/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2966/Reviewer_BjpG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681422691,
            "cdate": 1698681422691,
            "tmdate": 1699636240764,
            "mdate": 1699636240764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GpD0VjLm6z",
                "forum": "w0z3MKnmzu",
                "replyto": "gFoq2Hq3VR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BjpG"
                    },
                    "comment": {
                        "value": "Dear Reviewer BjpG,\n\nWe sincerely appreciate your supportive and positive feedback on our paper! Please find our response in the following .We hope this explanation could further address your comments.\n\n> **W1: ResPrompt performances are fluctuant as the number of reasoning steps grows in the MathQA.**\n\n**A1:** We sincerely appreciate the reviewer for this comment. We think the fluctuation in MathQA can be explained in the following two perspectives:\n- According to [1], the annotation in MathQA dataset is much noisier, so we use the examples in [1] to build Long-CoT and Short-CoT to avoid unintended noise for MathQA dataset. However, these examples from [1] are already after a careful example selection based on complexity, and therefore is already a very strong approach. We believe achieving better results with ResPrompt compared to this baseline actually further shows the effectiveness of ResPrompt.\n\n- The results are also model-dependent. Figure 3 shows the results on LLaMA2-70B, while results on LLaMA-65B are shown in Figure 9 in appendix. We notice that ResPrompt has a more clear improvement on >=5 step questions on LLaMA-65B. Therefore, one specific result of one dataset may be model-dependent, but generally, ResPrompt is able to improve the multi-step reasoning performance. \n\n\\\n\\\n[1] Fu et al, Complexity-Based Prompting for Multi-Step Reasoning, ICLR 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635519942,
                "cdate": 1700635519942,
                "tmdate": 1700635519942,
                "mdate": 1700635519942,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1gGUzVj36k",
            "forum": "w0z3MKnmzu",
            "replyto": "w0z3MKnmzu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_w7DW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_w7DW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces RESPROMPT, a new prompting strategy that enhances multi-step reasoning in large language models (LLMs). By incorporating residual connections into prompts, RESPROMPT captures the complex reasoning graphs inherent in multi-step problems. Experimental results demonstrate significant improvements in reasoning accuracy, particularly for complex multi-step tasks, which confirms its motivation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The proposed method is simple yet effective. By using the same tokens in intermediate steps to represent the connections between edges in the graph logic structure, it enhances the model's ability to solve complex multi-step reasoning problems.\n\n2.The experimental results partially demonstrate the effectiveness of the method, but it could be more persuasive to include a broader range of baselines and benchmarks to further support the findings (see Weakness)."
                },
                "weaknesses": {
                    "value": "1.The paper's baselines only include conventional CoT methods (no matter standard, long, or short), lacking comparisons with some other approaches specifically designed to enhance CoT multi-step reasoning ability (e.g. [1][2][3]). This limitation weakens the solidity of the experimental results. Additionally, while the last paragraph in Introduction briefly mentions the advantages of the proposed method over approaches that introduce tree or graph structures during the reasoning stage, the experiments lack a proper comparison and analysis in this regard.\n\n[1] Least-to-Most Prompting Enables Complex Reasoning in Large Language Models, ICLR 2023\n\n[2] Complexity-Based Prompting for Multi-Step Reasoning, ICLR 2023\n\n[3] Decomposed Prompting: A Modular Approach for Solving Complex Tasks, ICLR 2023\n\n2.The proposed method in this paper appears to be effective primarily for mathematical and sequential reasoning benchmarks. Although the introduction and experimental setup mention commonsense reasoning, the main experiments do not include evaluations on such tasks. Despite using StrategyQA in Section 3.5 to demonstrate that RESPROMPT is not essential for simple questions, it would be valuable to showcase the effectiveness of RESPROMPT on more complex commonsense reasoning tasks such as CommonsenseQA or HotpotQA. Including experiments on these tasks would provide a comprehensive assessment of the efficacy of RESPROMPT across different domains of reasoning.\n\n3.In section 3.3, it is mentioned that RESPROMPT demonstrates improved stability with variations in the number of exemplars in the prompt. However, it is difficult to observe from Figure 6 that RESPROMPT is more stable than other baselines and may even be less stable than the standard CoT. The results presented in the figure do not clearly indicate the claimed stability advantage of RESPROMPT."
                },
                "questions": {
                    "value": "1.In section 3.3, it is mentioned that using the same tokens instead of symbolic variables yields better results. How would you explain this phenomenon? Intuitively, it seems challenging for the language model to reference the same tokens in the intermediate steps when they are not explicitly shown in the prompt.\n\n2.This approach appears to somewhat decrease the readability of the reasoning process. What advantages does it offer in practical interactions with human?\n\n3.How does RESPROMPT make the model aware of the graph structure during reasoning? Or does this linear modification in the prompt truly enable the model to learn the graph structure?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681656063,
            "cdate": 1698681656063,
            "tmdate": 1699636240659,
            "mdate": 1699636240659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PesDZtVv3t",
                "forum": "w0z3MKnmzu",
                "replyto": "1gGUzVj36k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer w7DW - Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer w7DW,\n\nWe sincerely appreciate your positive comments and really thank you for the in-depth and insightful suggestions and questions! We provide a response one by one as follows. We hope these new results and explanation could address your concerns and we welcome further discussion. \n\n \n> **W1: Compare to more enhanced baselines for multi-step reasoning, and lack comparison with tree of thought and graph of thought.**\n\n**A1:** We greatly appreciate the reviewer for providing this feedback. We conduct new experiments on GSM8K to compare ResPrompt with advanced prompting methods for multi-step reasoning as described in [1], [2], and [3]. The results, presented in the following table, consistently demonstrate that ResPrompt outperforms these advanced baselines for multi-step reasoning. In the revised paper, we include a new section to incorporate these new comparison results and provide a detailed discussion.\n\n\n|| DECOMP (1-shot) [1] | ResPrompt (1-shot) | Complexity (8-shot) [2] |ResPrompt (8-shot) | Least_to_most (4-shot) [3] | ResPrompt (4-shot) |\n|:-----------:|:----------:|:------------------------:|:---------:|:---------:|:---------:|:---------:|\n|LLaMA-65B | 40.4       | 46.6                   |48.3   |59.2 | 53.6 |58.4|\n|LLaMA2-70B| 50.3      | 57.2                   | 64.2  |65.3 | 60.1 |67.5 |\n\nNote that to ensure a fair comparison, we directly use the prompts as originally specified in the respective papers or open-sourced repositories [4] from the authors [2] of each baseline. Accordingly, for ResPrompt, we employ the same number of exemplars (x-shot) as used in each of these sourced prompts.\n\nRegarding the comparison with Tree of Thoughts (ToT) [5] and Graph of Thoughts (GoT) [6], we would like to clarify that ToT and GoT are more relevant for tasks that require strategic reasoning, such as backtracking, traversal, sorting, etc. The demo applications in [5, 6] include examples like sorting, document merging, game of 24, etc. On the other hand, ResPrompt aims to capture the complex underlying structure in standard multi-step problems. In summary, although both ResPrompt and ToT/GoT are related to the complex \"structure\u201d, ResPrompt targets different purposes compared to ToT and GoT. We include a more detailed discussion on this topic in the revised paper.\n\n\n> **W2: Run experiments on CommonsenseQA (CSQA) or HotpotQA. Including experiments on these tasks would provide a comprehensive assessment.**\n\n**A2:** We sincerely appreciate the reviewer for providing this valuable suggestion. We agree that including experimental results on these two benchmarks, which assess the knowledge ability of LLMs, would make the experiment more comprehensive and provide further insights into when ResPrompt may not be essential. We have included the results on CSQA and HotpotQA in the following table.\n\n\n| | CSQA (4-shot) | HotpotQA (4-shot) EM/F1 |\n|:-----------:|:----------:|:------------------------:|\nStandard| 78.6       |       33.3/43.2             |\nCoT | 79.0      |      37.3/50.9                   | \nResPrompt| 78.0      | 36.3/49.9                  | \n\nThese results demonstrate that ResPrompt can just achieve comparable performance to the baselines on both CSQA and HotpotQA. This outcome is not surprising since both benchmarks primarily require extensive knowledge to answer the questions, rather than complex multi-step reasoning. Therefore, it is natural that ResPrompt may not be essential in these knowledge assessment benchmarks. Due to time and resource limitations, we randomly selected 300 examples from each benchmark for these experiments.  We will include experiments on the full benchmarks in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635445960,
                "cdate": 1700635445960,
                "tmdate": 1700635445960,
                "mdate": 1700635445960,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DqYHBhu9zZ",
            "forum": "w0z3MKnmzu",
            "replyto": "w0z3MKnmzu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_n8nq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2966/Reviewer_n8nq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new prompting pipeline named ResPrompt to boost the performance of the LLMs, particularly focusing on complex reasoning tasks. The authors claim that the complex interdependence of prior reasoning steps during reasoning makes the question more challenging for LLMs. To alleviate this question, the authors propose to explicitly connect different reasoning steps and recap the LLM with prior reasoning results by repeating them in chain-of-thoughts.  Empirical evaluations demonstrate the potential of the proposed method."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed method is neat and easy to follow. The authors provide relatively sufficient experiments to empirically demonstrate the performance improvement of the proposed method."
                },
                "weaknesses": {
                    "value": "1. **Lacks sufficient verification of the necessity of the proposed method.** The authors claim that the complex interdependence between the current reasoning step and prior steps requires explicit connections among different reasoning steps. However, this is only an intuitive hypothesis and the authors do not verify whether it hold or not. Instead, they directly propose \"repeating can help build the connections among reasoning steps and improve the performance\". A better way to formulate the method could be designing simple yet straightforward experiments to verify whether the claim holds or not. For example, the authors can examine whether LLMs can make wrong references during a specific intermediate step. Consider the following CoT:\n\n   ....So the money he earned from shoveling driveways is $110 - $15 - $60 = $35... \n\n   We can change the above sentence into `So the money he earned is $110 - $15 - $60 = $35.` and then query the LLM again asking where the `$35` is earned from. If the model cannot correctly answer `from shoveling driveways`, then that means the authors' claim might hold as the model does make mistakes to recovering the references from prior reasoning steps. \n\n2.  **Comparison with baselines might be unfair.** The authors claim that the performance can be achieved by more than 20% (relatively) on some datasets and models. However, if we look at the experiment setting and details of baselines, it seems the performance of baselines is suppressed, leading to a misunderstanding of the performance improvement. Specifically:\n\n   a. There is no need to include the performance using the prompt used in [1], as the performance is too low.\n\n   b. I do not understand why long prompts (`Long CoT`) lead to worse performance on the GSM8K dataset compared to short prompts (`Short CoT`), as the previous study [2] has already demonstrated that longer prompts provide better performance. The authors need clarifications and explanations for this phenomenon.\n\n   c. It does not make sense to include questions with different numbers of reasoning steps in the `Long CoT`. Previous work [2] has already shown that using the most complex CoTs leads to the best performance. The author should include the original prompts used by [2] as another baseline. Note that the proposed method, due to repeating previous steps, uses significantly long prompts compared to baselines. Without including baselines that use prompts with similar lengths, it cannot be answered whether the performance improvement comes from longer prompts or the proposed method (repeating to connect intermediate steps)\n\n   d. LLaMA performance on GSM8K could be much higher than the numbers in Table 1. See https://opencompass.org.cn/leaderboard-llm for a reference, where the LLaMA-2 achieves 63.5% accuracy and the LLaMA achieves 54.5% accuracy on GSM8K without any special techniques but CoTs. If we take this into consideration, the performance improvement is rather small.\n\n3. **Cost-performance tradeoff is not discussed.** Again, the proposed method uses much longer CoTs compared to baselines. The author should discuss the inference cost given the long prompts.\n\n4. **Additional ablation study.** I would like to emphasize this point again. The performance improvement could come from two aspects: longer prompts and proposed connections among different reasoning steps. The author should design an ablation study to demonstrate where the performance improvement comes from, rather than simply study which experiment designs can affect the final performance.\n\n[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022b.\n\n[2] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In ICLR 2023"
                },
                "questions": {
                    "value": "Please refer to the weakness above. Although I give a low rating to this paper, I would be delighted to increase my rating given my questions addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810934543,
            "cdate": 1698810934543,
            "tmdate": 1699636240576,
            "mdate": 1699636240576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2tal2qHZ3r",
                "forum": "w0z3MKnmzu",
                "replyto": "DqYHBhu9zZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n8nq -Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer n8nq,\n\nWe sincerely appreciate your detailed and insightful comments. We provide explanations to your concerns point-by-point in the following. We hope that these new results and explanations adequately address your concerns, and we welcome further discussion.\n\n\n> **W1: Lacks sufficient verification of the necessity of the proposed method.**\n\n**A1:** We greatly appreciate the reviewer for the valuable feedback and the insightful experiment suggestion. We conduct a quick verification experiment using LLaMA2-70B as per the suggestion. However, upon further consideration, we realize that the reviewer's suggested query, \"query the LLM again asking where the ``$35`` is earned from,\" may not align exactly with our original motivation. This is because the amount of ``$35`` is derived from the immediately previous step, whereas our main observation is that the model struggles to connect with \"results of several steps earlier\". Therefore, instead of querying the LLM about the source of ``$35``, we modify the query to ask where ``$110`` is earned from. Specifically, we used the following prompt to query LLaMA2-70B:\n\n\u201cQuestion: Tobias is buying a new pair of shoes that costs ``$95``. He has been saving up his money each month for the past three months. He gets an ``$5`` allowance a month. He also mows lawns and shovels driveways. He charges ``$15`` to mow a lawn and ``$7`` to shovel. After buying the shoes, he has ``$15`` in change. If he mows 4 lawns, how many driveways did he shovel? Before the purchase, he had ``$95 + $15 = $110`` in total. From his allowance, he earned ``3 * $5  = $15``. From mowing lawns, he earned ``4 * $15 = $60``. Where the ``$110`` is earned from?\u201d\n\nThe greedy decoding output of LLaMA2-70B base model is\n\u201c``$15 + $60 = $75``. From shoveling driveways, he earned ``$110 - $75 = $35``. How many driveways did he shovel? ``$35 / $7 = 5``. He shoveled 5 driveways.\u201d, which continues to solve the problem but does not answer the question. So we use a softer temperature and query LLaMA2-70B chat model for 5 times. The outputs are shown in following \n\n1. Tobias earned ``$110`` from mowing lawns and shoveling driveways combined. He mowed 4 lawns, so he shoveled ``($110 - $60) / $7 = 12`` driveways.  \n2. The ``$110`` is earned from the sum of the money earned from his allowance and the money earned from mowing lawns.  \n3. Tobias earned ``$110`` from his allowance and mowing lawns combined. He earned ``$15`` from his allowance, and ``$60`` from mowing lawns.  \n4. The ``$110`` Tobias earned came from his allowance and mowing lawns. He earned ``$15`` from his allowance and ``$60`` from mowing lawns, totaling ``$110``.  \n5. Tobias earned the ``$110`` from his allowance and mowing lawns. He earned ``$15`` from his allowance and ``$60`` from mowing lawns, which is a total of ``$75``. Where the remaining ``$35`` came from is not specified in the information provided.  \n\nFrom these results, we notice that LLaMA2-70B can not correctly answer the source of ``$110``. We believe this could be an empirical verification that supports our claim and motivation.\n\n\n\n> **W2:Comparison with baselines might be unfair**\n\n> **W2 (a) : There is no need to include the performance using the prompt used in [1], as the performance is too low.**\n\n**A2 (a):** We thank the reviewer for this detailed comment. We agree that the prompts used in [1] are relatively simple. However, considering that the LLaMA family of models, particularly LLaMA2, is relatively new, we believe that including the performance of LLaMA2 with these prompts could provide a reference for the community. Additionally, in certain experiments, such as AQUA-RAT, the original prompts can still serve as strong baselines. For these reasons, we have decided to include these prompts in our evaluation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635288353,
                "cdate": 1700635288353,
                "tmdate": 1700635308374,
                "mdate": 1700635308374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]