[
    {
        "title": "Investigating the Ability of PINNs To Solve Burgers' PDE Near Finite-Time BlowUp"
    },
    {
        "review": {
            "id": "deq8DfSzos",
            "forum": "XaqaitclOA",
            "replyto": "XaqaitclOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_aEZi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_aEZi"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization error of PINN for Burgers' equation. The theoretical framework is informative of the empirical evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper innovatively studies the PINN generalization error of the Burgers equation.\nEmpirical evaluation validates the effectiveness of the theoretical framework.\nThe bound does not depend heavily on the neural network.\nThe solution of the Burgers equation is stiff, which hinders PINN from learning this part of the mutation. Therefore, the topic studied in the paper is important."
                },
                "weaknesses": {
                    "value": "Although I recognize the theoretical contribution of this paper, the actual PINN experiment deviates from the theory to a certain extent.\nBecause the solution to the Burgers equation is very stiff, many PINN variants have been proposed to solve these problems, such as self-adaptive weight PINN, adaptive sampling, or adversarial training. Their core points are to focus the optimization of PINN on these stiff areas with relatively large losses to fit the stiff area of the Burgers equation well.\nSince the theory of this paper is mainly based on PINN's L2 loss to bound the final generalization error. Therefore, I suspect that the conclusions of this paper cannot fit well with these PINN variants, such as self-adaptive weight PINN, adaptive sampling, or adversarial training, because the loss function they use is no longer L2 loss. In other words, the most popular method to solve Burger is adaptive loss. Can the author's theoretical framework be applicable to these variants?"
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698471093124,
            "cdate": 1698471093124,
            "tmdate": 1699636857263,
            "mdate": 1699636857263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vlChY6YOuo",
                "forum": "XaqaitclOA",
                "replyto": "deq8DfSzos",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our bounds can be invoked on the approximate solution obtained from any kind of a PINN loss."
                    },
                    "comment": {
                        "value": "We thank you for your kind comments about our work. \nIn the following, we address the key concern you have raised. \n\n> Can the author's theoretical framework be applicable to PINN variants, such as self-adaptive weight PINN, adaptive sampling, or adversarial training?\n\nAlthough our bounds in the theorems are computing errors in the $L^2$ norm, the proof can be applied to any surrogate solution found irrespective of the method used to obtain it (like the choice of norms in the loss function used) - in other words, our bounds will hold as long as the initial and boundary conditions are kept similar to ours for any method solving the Burgers' PDE and returning a function as a candidate solution. \n\nFor eg. in self-adaptive weighting (https://arxiv.org/abs/2009.04544) they modify the loss function by adding specific weights to each residual and these weights are updated in each time step. In adaptive sampling (https://arxiv.org/abs/2210.00279), they choose a set of points in each step of training on the basis of ``failure probability'' at a few points in space. In both of these cases, the modifications do not affect the applicability of our bound since that only depends on being able to estimate the required boundary, initial conditions and bulk error computing integrals for the surrogate at the end of training - howsoever obtained."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700094084522,
                "cdate": 1700094084522,
                "tmdate": 1700094084522,
                "mdate": 1700094084522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UTSNeluOGK",
            "forum": "XaqaitclOA",
            "replyto": "XaqaitclOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_ndTQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_ndTQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper starts by highlighting a gap: current general rules for using neural networks to solve PDEs don't exist if the PDE has a known explosive solution. This pushes the authors to explore how PINNs tackle the Burgers' PDE, especially when it's close to exploding.\n\nFirst, the authors explain PINNs. These are neural networks trained to follow the rules of a physical system, including its boundary and starting points. They then test how well PINNs handle the challenging parts of Burgers' PDE and compare this to older, standard methods.\n\nAfter that, they work out general rules for errors in the Burgers' PDE. These rules estimate how much the neural network might get wrong on new data. The authors find a link between these rules and the solution the neural network comes up with. They suggest these rules can help shape how the neural network is built.\n\nThe paper then talks about the balance between getting the answer quickly and getting it right in PINNs. The authors suggest a new training method for PINNs that finds a good middle ground. They test this on Burgers' PDE and find it gives good answers much faster than older methods.\n\nTo sum up, this paper adds a lot to the world of using neural networks to solve PDEs. It shows how PINNs can handle tough PDE situations, gives rules for estimating errors, and introduces a faster training method. All these can shape how future neural networks are designed for this job, leading to quicker, more accurate results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Fresh Perspective: The paper delves into how Physics Informed Neural Networks (PINNs) handle particular solutions in PDEs, a topic not widely tackled before. Additionally, the authors outline error estimation rules for the Burgers' PDE when using neural networks, marking a pioneering step in neural network-based PDE solutions.\n\nThoroughness: The study dives deep into PINNs' stability, providing a well-rounded theoretical perspective. The authors craft error rules for the Burgers' PDE rooted in robust mathematical studies, bolstering the case for using PINNs to solve PDEs.\n\nPractical Tests: The team showcases how PINNs can manage the Burgers' PDE, especially when it's on the brink of a complex issue, and stack these results against established methods. They also suggest and test a fresh PINN training technique that strikes a balance between speed and precision. These hands-on results further confirm the potential of PINNs in this domain.\n\nClear Writing: The paper is neatly composed and straightforward. With lucid explanations and detailed accounts of their methods and findings, it caters to a broad audience, even those just venturing into neural network-based PDE solutions."
                },
                "weaknesses": {
                    "value": "One limitation of this paper is its narrow focus on addressing the Burgers' PDE near a specific complex scenario. Although this is a significant topic, it might not cover the spectrum of PDEs used in real-world situations. This could limit how much the findings in this paper can be applied to other PDEs.\n\nFurthermore, the study works under the assumption that we always know the main equations driving the physical system. However, in real situations, these equations might be unknown or hard to pinpoint. This could reduce the range of situations where PINNs can be effectively used for solving PDEs.\n\nLastly, the paper could have delved deeper into comparing its method with other leading neural network solutions for PDEs. While there's a comparison with classic numerical methods, a broader analysis including other neural network strategies would give readers a fuller understanding of where this method stands in the landscape of PDE-solving techniques."
                },
                "questions": {
                    "value": "What is the trade-off between accuracy and speed of inference in PINNs?\n\nHow do PINNs detect finite-time blow-ups in PDEs?\n\nWhat are the generalization bounds for Burgers' PDE and how are they correlated to the neurally found surrogate solution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709065212,
            "cdate": 1698709065212,
            "tmdate": 1699636857154,
            "mdate": 1699636857154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kONbTqS7to",
                "forum": "XaqaitclOA",
                "replyto": "UTSNeluOGK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added new experiment details and literature reviews in Appendices C, D and E"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed analysis of our paper and the kind comments. \n\nFirstly, we would like to point out that in the newly added Appendix C we have now given a side-by-side comparison between the true PDE solution and the neurally found approximate PDE solution for a sequence setups with 1D Burgers' PDE whose true solutions are increasingly getting close to being singular.  \n\nSecondly, please see Appendix D where we have demonstrated how the run-time to solve 1D Burgers' PDE by the PINN method is barely affected by the proximity to the singularity of the true solution.\n\nThirdly, kindly note the new Appendix E where we have reviewed that to the best of our knowledge existing results in the classical numerical analysis literature don't seem to be applicable to analyze PINN experiments with Burgers' PDE. \n\nIn the following we shall address the key concerns you have raised. \n\n>it might not cover the spectrum of PDEs used in real-world situations\n\nTo the best of our knowledge there is hardly any universal principle for how finite-time singularities appear in different PDEs. In our introduction, we have reviewed the evidence for such singularities in various famous models like the Frank-Kamenetskii PDE, 2D Boussinesq PDE, and the 3D Euler PDE. Each has its own peculiarities and mechanisms of blowing-up and we believe that each will require its own new analysis. \n\nWe hope that our work could be a starting point for various such analyses in the future. \n\n>The paper could have delved deeper into comparing its method with other leading neural network solutions for PDEs.\n\nThe theoretical framework proposed by us can be invoked on *any* surrogate solution found for the specified Burgers' PDE. The *applicability of our theoretical bounds does not depend on any specific method used for solving the PDEs* as long as the chosen numerical method gives a function approximation to the true PDE solution - as PINNs do, which is the primary method we focus on. \n\n\n>the study works under the assumption that we always know the main equations driving the physical system. \n\nIn this work we focus on PINNs, which is a method of using deep-learning to solve PDEs in an unsupervised way - and by definition, this method is applicable only when the exact PDE is known. We posit that this is a common use case in the real world - and why PINNs are getting increasingly deployed for various uses.\n\nIn cases where the underlying PDE is unknown but some input-output function samples (like samples of solution for different initial conditions) are available, operator methods such as DeepONets can be used. Formulating the problem of stability of operator learning in the face of finite-time blow-ups is an entirely different question (about learning in infinite dimensions) than what we focus on. \n\n>What are the generalization bounds for Burgers' PDE and how are they correlated to the neurally found surrogate solution?\n\nIn usual ML parlance when we want to prove bounds on the generalization error for a setup, we are looking to bound the difference between the population risk and the empirical risk of a loss function. As we have pointed out in the review given in our Section 2, such bounds for PDE solving losses is available only for special kinds of linear PDE. Burgers' PDE being non-linear is not in the ambit of such existing proofs - and that is one among the many motivations for our work. \n\nTowards such a goal we posit that it could be an interesting direction of future exploration to try to understand the Rademacher complexity of the loss class considered here."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093240426,
                "cdate": 1700093240426,
                "tmdate": 1700093240426,
                "mdate": 1700093240426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1dgi7xj8TI",
            "forum": "XaqaitclOA",
            "replyto": "XaqaitclOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the approximation ability of PINNs for the inviscid Burgers equation is theoretically estimated. This equation is known to have so-called blow-up solutions, which are solutions that diverge to infinity in finite time. In this paper, whether PINNs can find such a solution is investigated theoretically. Specifically, two theorems are presented in this paper; the former theorem gives an error estimate for the multi-dimensional Burgers equation, and the latter theorem gives an improved result for the 1-dimensional equation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is just my impression but theoretical error analysis of numerical methods for computing blow-up solutions is a difficult problem. Even for classical numerical methods, such as the finite difference method and the finite volume method, there are not so many papers on this topic. A strength of this paper is that the authors tackle such a challenging problem, and certain results are in fact given."
                },
                "weaknesses": {
                    "value": "I suppose that there are a few weaknesses in this paper.\n1) I believe that inequalities estimating numerical errors should show that the error bound converges to zero in some sense. If I understand the result correctly, the error bound in the first theorem does not converge to zero because $C_1$ and $C_2$ include the terms given by the norm of the solutions.  So, the inequality (5) does not appear to make sense as an error analysis.\n\n2) As for the results of the numerical experiments, although it is interesting that certain correlations between RHS and LHS of the inequalities are observed, the magnitudes of them are very different. So, I am not sure whether these results are meaningful or not.\n\n3) Perhaps this is not a weakness, but honestly, it is difficult for me to assess the value of this paper in the ML community. Although the analysis shown in this paper may be an important first step in this direction, I am not sure whether the results of this paper meet the criteria of a top ML conference. My concern is that, in my impression, papers on error analysis of classical numerical methods (e.g., the finite difference method) for the Burgers equation seem unlikely to be accepted by top journals of numerical analysis because the Burgers equation is the simplest partial differential equation with blow-up solutions."
                },
                "questions": {
                    "value": "My biggest concern is the first one of the above weaknesses. Does the error bound (5) converge to zero in certain situations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7212/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698715231372,
            "cdate": 1698715231372,
            "tmdate": 1699636857045,
            "mdate": 1699636857045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jujhn0AhoC",
                "forum": "XaqaitclOA",
                "replyto": "1dgi7xj8TI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added new experiment details and literature reviews in Appendices C, D and E"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their kind reading of our work and for appreciating the challenge that we have tried to solve. \n\nFirstly, we would like to draw your attention to the newly added Appendix E in the draft where we have reviewed multiple classical results in numerical analysis for Burgers' PDE to establish that to the best of our knowledge even for this smallest non-trivial case of finite-time blow-up, classical error bounds do not exist which can be invoked on PINN experiments with Burgers' PDE.\n\nThus, our bounds and experiments in this work can be seen to be a first-of-its-kind result to address a challenging interface between deep-learning and non-linear PDEs. \n\n>Does the error bound (5) converge to zero in certain situations?\n\nThanks for raising this interesting question! As you would note we have specified this exact question as an open question in our conclusion in the language of \"stability\" - which we have briefly defined in the footnote on page 6.\n\nFor PDEs for which the ML method of solving it has a \"stability\" guarantee in this precise sense are the very cases where we can get a data-driven sufficient condition for the error of the approximation to be $0$. \n\nAs we have pointed out in our discussion below Theorem 3.2, that this property of stability is not immediate for many of the ML losses used for PDE solving and one of our key observations (in Theorem 3.2) is that for Burger's PDE in one dimensions such a stable bound is possible despite allowing for the difficult solution of a finite-time blow-up. \n\n>Although it is interesting that certain correlations between RHS and LHS of the inequalities are observed, the magnitudes of them are very different. So, I am not sure whether these results are meaningful or not.\n\nAs we have emphasized in the literature review done at the beginning of Section $4$, getting non-vacuous generalization bounds for deep-learning is an extremely open problem. The objective of our research here is not to try to solve this question which is open for even much more basic setups with neural nets. \n\nThe crux of our work - in particular our experiments - is to establish that the theoretical bounds we prove are not only applicable to this involved setup of nets solving PDEs but that they also evaluate to values that are highly correlated to the functional distance of the found surrogate from the true PDE solution. \n\nWe posit that it is somewhat of a surprise that this correlation between the bound and the truth is strong (approaching nearly $1$ at times) despite the vacuity of the bound and despite the proximity to singularities - which is a unique feature of our PDE under consideration."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092707064,
                "cdate": 1700092707064,
                "tmdate": 1700092707064,
                "mdate": 1700092707064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ohKbQTHxeN",
                "forum": "XaqaitclOA",
                "replyto": "ViLlPjUwyC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for your reply. I am afraid to say, but, although my impression of this paper is not so bad, my biggest concern in Questions has not been fully addressed. I agree with the authors that the bounds provided in this paper contribute to the stability analysis of PINNs for the blow-up solutions of the Burgers equation and this paper proceeds an important step in this direction; however, I believe that \"generalization bounds\" should converge to zero under certain assumptions. I suppose that this paper would be more readable and of greater value if it could be rewritten as a paper on stability analysis."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615082265,
                "cdate": 1700615082265,
                "tmdate": 1700615082265,
                "mdate": 1700615082265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f29xmXCASn",
                "forum": "XaqaitclOA",
                "replyto": "1dgi7xj8TI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our use of the term \"generalization error\" is consistent with foundational papers in PINN theory"
                    },
                    "comment": {
                        "value": "Thanks for your comments. In usual ML, the word \"generalization bound\" typically refers to a bound on the gap between the population risk and the empirical risk and this can often be shown to have easy sufficient conditions of approaching $0$ by methods such as Rademacher complexity. \n\n*But in our theorems here, the LHS is not this notion of generalization gap* but a measure of the distance between the surrogate function and the true PDE solution. In the context of PINNs this quantity has been called as the \"generalization error\" right from the first theory papers in PINNs like the canonical result from 2022, Theorem 2.6 here,  https://doi.org/10.1093/imanum/drab093  - which laid the foundations of the subject. Please note that the authors there too called this gap from the true PDE solution as \"generalization error\" \n\nBut kindly note that the bounds like in the foundational papers use an unconventional notion of training error (which carefully weights each collacation point) which is not implemented in PINN experiments these days but our bounds don't have such a condition - and thus we take a step forward compared to them and build insights closer to experimental setups. \n\nSo, we posit that our use of the word generalization error for this quantity is consistent with the foundational papers in the theory of PINNs while we take a step forward in many other ways. Maybe we kindly request the reviewer to take cognizance of this fact."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647639776,
                "cdate": 1700647639776,
                "tmdate": 1700650275195,
                "mdate": 1700650275195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R8PEUraHYE",
            "forum": "XaqaitclOA",
            "replyto": "XaqaitclOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_QLQE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7212/Reviewer_QLQE"
            ],
            "content": {
                "summary": {
                    "value": "This work derives what the authors call a generalization bound for the PINN-based solution of Burgers' equation near the formation of singularities. They show empirically that their bound, while vacuous, is surprisingly correlated with the error vs the true solution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Introducing more analytical techniques to the study of PINN training is a worthwhile cause. I also appreciate the author's openness to admit the vacuousness of their bound and investigating the empirical correlation of their bound with the right hand side."
                },
                "weaknesses": {
                    "value": "Listed in decreasing order of gravity\n\n1. The bound derived by the authors depends on $L^\\infty$ norm of the gradient. As the equation approaches blow-up, this quantity approaches infinity. The bound thus does not provide meaningful information in the vicinity of the blowup, which is undercutting the main claimed contribution. \n\n2. The claim by the authors \n>Most importantly, Theorem 3.2 shows that despite the setting here being of proximity to finite-time\nblow-up, the naturally motivated PINN risk in this case 3\nis \u201c(L2, L2, L2, L2)-stable\u201d4 in the precise sense as defined in Wang et al. (2022a). This stability property being true implies that if the PINN\nrisk of the solution obtained is measured to be O(\u03f5) then it would directly imply that the L2-risk\nwith respect to the true solution (10) is also O(\u03f5). And this would be determinable without having\nto know the true solution at test time.\n\nis misleading. If the exact solution is unknown, neither is the $L^\\infty$ value of its gradient at a given time, preventing the bounding of the error vs the true solution.\n\n3. I do find the expression \"generalization bound\" for Theorem 3.1 somewhat misleading. These type of stability estimates (of the operator mapping right hand side and initial condition to the solution) are standard tools in the theory of partial differential equations, making this seem more like a rebranding. It would strengthen the paper if the authors would discuss related results in the PDE literature.\n\n4. The literature review on operator learning approaches misses the works on both neural operators and BCR-NET (the latter predates both neural operators and DeepONet). \n\n5. The referral to the works on the euler singularity of Wang should make more clear the differences between the two works. To my understanding, the work of Wang et al uses a rescaled coordinate system and therefore does not actually solve a PDE with singular solution. The blow-up studied by this community is also specific to incompressible problems as the blowup of the compressible Euler equation (of which the Burgers equation is the zero sound speed limit) arises from a different phenomenon."
                },
                "questions": {
                    "value": "I suggest the authors directly respond to my criticism in the last paragraph. I would gladly reconsider my recommendation if it turns out that I overlooked something."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796543314,
            "cdate": 1698796543314,
            "tmdate": 1699636856912,
            "mdate": 1699636856912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rFKdqxkkMa",
                "forum": "XaqaitclOA",
                "replyto": "R8PEUraHYE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added new experiment details and literature reviews in Appendices C, D and E"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their critical reading of our work. \n\nMay we request you to reconsider your scores in light of the explanations that we provide below for the $5$ weaknesses you have pointed out. \n\n> As the equation approaches blow-up, the $L^\\infty$-norm of the gradient approaches infinity. The bound thus does not provide meaningful information in the vicinity of the blowup. \n\nWe suggest reading our bounds not at the blow-up point but in its vicinity i.e when $\\delta \\in [0,1)$ for say Theorem $3.2$. At any such $\\delta$ the bound is finite and that's when we do our experiments. The meaning we extract from these bounds is as displayed in Figures $1$ and $2$ where we show that despite the bounds being computed arbitrarily close to a blow-up they maintain correlation with the distance of the neurally found surrogate from the true solution. \n\nTo the best of our knowledge, except this work, currently there are no other theorems available off-the-shelf which can be used to get any bound at all on the population risk of a neurally found surrogate solving for Burgers' PDE near a finite-time blow-up. \n\nWe would like our work to be seen as a critical first step towards understanding the training of neural nets to solve PDEs near a blow-up.\n\n>[The $(L_2,L_2,L_2,L_2)$-stability claim] is misleading\n\nWe would like to draw your attention to how stability has been defined in the footnote of page $6$. This definition is inspired from an earlier work, https://doi.org/10.48550/arXiv.2206.02016. As you would note that this definition is about an order estimate - that if the population risk of the PINN loss is ${\\cal O}(\\epsilon)$ then it should be a sufficient condition to ensure that the distance of the surrogate from the true PDE solution is also ${\\cal O}(\\epsilon)$. \n\nWe note that in general for arbitrary PDE, it is not guaranteed (and may even be provably impossible) for the PINN loss to have this property of stability.\n\nAnd this is the special phenomenon that happens in our Theorem $3.2$. \n\nThus even when the true solution is unknown, a PINN loss could be proven to be stable if the bound on the true risk can be shown to depend on the true solution such that the dependencies can be absorbed into the constants kept implicit in the ${\\cal O}$ estimate.\n\n>\"generalization error\" term for Theorem 3.1 is misleading. These type of stability estimates (of the operator mapping right hand side and initial condition to the solution) are standard tools in the theory of partial differential equations, making this seem more like a re-branding. It would strengthen the paper if the authors would discuss related results in the PDE literature.\n\nWe would like to emphasize that the key message of the theorems like what we have proven here is to able to estimate the proximity of a surrogate solution from the true PDE solution in terms of errors made by the solution in satisfying the PDE in the interior of the domain considered and the errors made in satisfying the boundary and the initial conditions. \n\nTo the best of our knowledge, there are scant results in conventional PDE literature which give such bounds. Kindly see the newly added Appendix E where we have reviewed various classical literature to establish that to the best of our knowledge existing results in numerical analysis cannot be deployed to understand PINN training - as is the target here. In particular, specifically for the $0$ viscosity Burgers' PDE we can only find results like this, https://doi.org/10.1090/S0025-5718-1987-0906180-5, where the theory does not give a bound on the distance from the true solution of the PDE to that found by the finite element method. \n\nMore generally, in works such as Corollary 3.5 in https://doi.org/10.1137/0728048 the authors consider a weak solution of the $\\varepsilon$-viscosity regularized Burgers' PDE and derive bounds on the local $L^p$-distance between the weak solution and the actual solution. There is no obvious way to apply these bounds for a PINN solution since the trained net has no guarantee to be satisfying the conditions required of the surrogate here. \n\nFor results like Theorem 2.1 in https://doi.org/10.1137/0729087, we observe that these too don't have an obvious way for the bounds to be applied for PINN experiments because they need stringent conditions (like satisfying the conservativeness property) to be true for the approximant, for the bounds to apply and there is no natural way to know if the neural surrogate satisfies these conditions. Also neither of the above cited classical bounds are tailored to any compact domain and hence there is no boundary condition error that is getting tracked there as in our Theorem $3.2$.\n\n\nIn contrast to such literature, even in the context of existing PINN guarantees, we posit that our theorems are a step towards novel and useful guarantees."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092377856,
                "cdate": 1700092377856,
                "tmdate": 1700095167640,
                "mdate": 1700095167640,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]