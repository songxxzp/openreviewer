[
    {
        "title": "TABLEYE: SEEING SMALL TABLES THROUGH THE LENS OF IMAGES"
    },
    {
        "review": {
            "id": "JXLd2JZndp",
            "forum": "K8Mbkn9c4Q",
            "replyto": "K8Mbkn9c4Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to transform tabular data into image formats and utilize pretrained vision models to help the learning of tabular few-shot learning. The experiment results show that the method can be better than a strong LLM-based method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Very simple and effective method.\n- Transforming tabular data into image format is intuitive and novel.\n- The proposed method has good performance even with a small visual encoder, better than an LLM-based method, which is very promising."
                },
                "weaknesses": {
                    "value": "- Only two papers are discussed in the related work section, which makes reader difficult to place the paper in an appropriate context.\n- The relationship between the quality of the visual encoder and the few-shot tabular performance is not shown. \n- Missing an important baseline (See questions).\n- Only the domain transformation module is proposed by the authors. Novelty is somewhat lacking."
                },
                "questions": {
                    "value": "- Can you discuss more related works in the paper? For example, a brief introduction to the tabular learning literature.\n- Can you give results using a more powerful visual encoder? In Luo et. al [1], it has been shown that better visual encoders can lead to better few-shot learning performance. Perhaps, you can try to use pretrained CLIP [2] or DINO-v2 [3] and report the results.\n- Another straight way of transforming the tabular data into images is to directly visualize the table on an image in its original form. This should be a baseline to illustrate the advantage of your proposed tabular data transformation.\n\n[1] A Closer Look at Few-shot Classification Again. ICML 2023.\n\n[2] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.\n\n[3] DINOv2: Learning Robust Visual Features without Supervision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe",
                        "ICLR.cc/2024/Conference/Submission3231/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698393794792,
            "cdate": 1698393794792,
            "tmdate": 1700706893452,
            "mdate": 1700706893452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mJUm8HYHkY",
                "forum": "K8Mbkn9c4Q",
                "replyto": "JXLd2JZndp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "**Q1. Can you discuss more related works in the paper? For example, a brief introduction to the tabular learning literature.**\n\n- Thank you for your question. I realize that I did not consider that readers might be unfamiliar with the concept of few-shot learning. For those who are completely new to few-shot learning, We have added a background section in Appendix A on page 12 of the paper to provide essential context and understanding.\n\n\n**Q2. Can you give results using a more powerful visual encoder? In Luo et. al [1], it has been shown that better visual encoders can lead to better few-shot learning performance. Perhaps, you can try to use pretrained CLIP [2] or DINO-v2 [3] and report the results.**\n\n*[1] A Closer Look at Few-shot Classification Again. ICML 2023.*\n\n*[2] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.*\n\n*[3] DINOv2: Learning Robust Visual Features without Supervision.*\n\n- We conducted experiments using CLIP's ViT-B/32 encoder as the backbone and the Proto-layer as the classifier. Although we anticipated the excellent few-shot performance of CLIP, the experimental results showed that CLIP did not learn effectively on any of the four datasets (CMC, Diabetes, Karhunen, Optdigits). We believe this is because CLIP's pre-trained visual encoder is designed to reduce the distance between the image and text domains, which is a different concept compared to backbones like conv2, conv3, which were trained specifically for natural image few-shot classification.\n\n- Therefore, we also conducted experiments using the ViT as the backbone and the Proto-layer as the classifier. The results showed that ViT performed reasonably well on all four datasets but did not surpass the performance of other methods used in TablEye. We attribute this to the fact that while the learning parameters of the current models have been optimized through numerous experiments, ViT has not undergone such optimization. However, this also indicates the potential applicability of the ViT structure to tabular images. We have added these experimental results to Appendix H on page 16 of the paper.\n\n\n**Q3. Another straight way of transforming the tabular data into images is to directly visualize the table on an image in its original form. This should be a baseline to illustrate the advantage of your proposed tabular data transformation.**\n\n- Thank you for mentioning an important baseline. We conducted additional experiments with two methods, including the baseline you mentioned, and compared them with domain transformation. The graph below experimentally demonstrates that domain transformation more effectively converts tabular data into a form suitable for CNNs compared to simply changing the order and size of tabular data to form a 2D image and representing it graphically. We have added a section in Appendix G on page 13, 15 of the paper to provide this experiment. Thank you for providing important comments that helped improve the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409216484,
                "cdate": 1700409216484,
                "tmdate": 1700409216484,
                "mdate": 1700409216484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SUxH612xO9",
                "forum": "K8Mbkn9c4Q",
                "replyto": "mJUm8HYHkY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. While I am satisfied with the response to Q3, I still have some concerns about Q1 and Q2.\n\n1. Note that what I mean by \"context\" is related work, not problem setup. The authors should discuss more related work relevant to few-shot learning, tabular learning, and few-shot tabular learning, instead of introducing the setup in the appendix.\n\n2. (minor) Since I have plenty of experience in visual few-shot learning, I deeply doubt the experiment with CLIP. As evidenced by Luo et. al [1], the visual encoder of CLIP performs extremely well on each of the diverse 10 datasets (including several natural image datasets), thus there is no reason that CLIP cannot perform better than a miniImageNet-trained model on table-based images. Besides, I do not see any value in replacing the backbones with ViT. What I mean is to use better pretrained models trained on massive datasets to see the relationship between the **quality** of the visual encoder and the few-shot tabular performance. Note that this is not a major problem to reject the paper, but doing this can better enhance the value of the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581699639,
                "cdate": 1700581699639,
                "tmdate": 1700581699639,
                "mdate": 1700581699639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FOHiysgrtJ",
                "forum": "K8Mbkn9c4Q",
                "replyto": "rF6STesAK9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I am satisfied with the additional related work which appropriately contextualizes the paper. I also understand the limited time for conducting more experiments which can be left to the near future. I thus increased the score to 6. \n\nMinor: I noticed that there is always no space between citations and texts, please check carefully and modify them."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631624438,
                "cdate": 1700631624438,
                "tmdate": 1700631624438,
                "mdate": 1700631624438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tnL3njwWZt",
                "forum": "K8Mbkn9c4Q",
                "replyto": "FOHiysgrtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_6PPe"
                ],
                "content": {
                    "comment": {
                        "value": "After another careful read of other reviewers' and Caspian's comments, I find that the proposed method lacks novelty and scalability, thus I decide to maintain the original score (5)."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706869779,
                "cdate": 1700706869779,
                "tmdate": 1700706869779,
                "mdate": 1700706869779,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BAyZrGaKEn",
            "forum": "K8Mbkn9c4Q",
            "replyto": "K8Mbkn9c4Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new few-shot learning method for tabular data. By transforming the tabular data into image representations (_tabular images_), they hope to transfer prior knowledge that is readily available in the image domain onto the tabular task to improve results and make up for the scarcity of otherwise shared/prior information in the tabular domain. The argument is that in this way, proven methods from the well-explored image-based few-shot learning area can be leveraged to advance the area of tabular few-shot learning. The authors test their approach using two popular few-shot methods and four vision backbones on a variety of datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Originality & Significance:   \nThe paper explores an interesting underlying idea to leverage information from a well-explored area (in this case the image domain) and transfer both prior knowledge and existing/proven algorithmic methods; \n\n### Quality: \n- Data: Authors experiment on different datasets and consider different important aspects: 1) feature diversity (categorical vs. numerical), 2) task diversity (n-way classification), 3) applicability to/relevance for \u2018real-world\u2019 applications, in this case medical data; \n - Architectures: experimentation with 4 different versions to gauge parameter and architectural influences;\n \nI do however see severe weaknesses in most other parts, see the following."
                },
                "weaknesses": {
                    "value": "While I do like the general underlying idea, there are several severe weaknesses present in this work \u2013 leading me to lean towards rejection of the manuscript in its current form. The two main areas of concern are briefly listed here, with details explained in the \u2018Questions\u2019 part:\n\n### 1) Lacking quality of the \u201cDomain Transformation\u201d part\nThis is arguably the KEY part of the paper, and needs significant improvement in two points: Underlying intuition/motivation/justification,   as well as technical correctness and clarity.  There are several fundamental points that are unclear to me and require significant improvement and clarification; This applies to both clarity in terms of writing but, more importantly, to the quality of the approach and justifications/underlying motivations. \nPlease see the \u201cQuestions\u201d part for details.\n\n### 2) Lacking detail in experiment description: \nDescription of experimental details would significantly benefit from increased clarity to allow the user to better judge the results, which is very difficult in the manuscript\u2019s current state; See \"Questions\" for further details."
                },
                "questions": {
                    "value": "### Main questions regarding Domain Transformation part:\n\nTechnical parts: \n\n-\tCreating the (N,N) feature matrix R via Euclidean distance between N features -> What is the intuition behind this? Euclidean distance is symmetric (as squared), so isn\u2019t the (N,N) matrix symmetric (if unranked) or has double-entries (if sorted/ranked)?\n-\tThe authors then go on to state: \u201cWe also measure the distance and rank between N elements [..] to generate an (N,N) pixel matrix, denoted as Q.\u201d -> What exactly is being compared/contrasted here? What \u2018pixels\u2019 are used here? \n-\tThis is followed by another Euclidean distance between R and Q \u2013 Again, I am missing the intuition/justification behind this. \n-\tThe authors claim that this then results in \u201ca 2-dimensional image of size (Nr x Nc)\u201d. How exactly is this obtained from computing the Euclidean distance between two (N,N) matrices? \n--- \nFurther details & justification: \n\n-\tHow are the ranked features arranged to form a 2D \u2018image\u2019? This should significantly affect the way how ConvNets perform on them! More detail is required here.\n-\tWhy would a ranking of the distances between features and pixels followed by rearrangement in any way resemble information presented in natural images? In natural images, the local relationship between pixels is defined by the occurrence of objects at a spatial location within the image. Why should a network pretrained on such data (in this case miniImageNet) be \u2018useful\u2019 to work on the artificially created tabular images? How do you overcome the (potentially significant) domain gap here? Or at least, what is the intuition behind it? (While the authors provide some insight in Figure 4, a 2D circle in t-SNE is not necessarily representative due to the hyperparameters involved in the projections); I'd invite the authors to further comment on this and their underlying intuitions.\n--- \n-\tAdditionally: Since common CNNs take in RGB images (3 channels) but the authors create only images w/ 1 channel, they simply repeat the same image 3x for each channel \u2013 this seems like unnecessary overhead and simply engineered to fit existing input layers. If the created images are simply grayscale (as they seem to be according to Figure 1), wouldn\u2019t it be more reasonable to pretrain the backbone on grayscale images?\n-\tIn the introduction, the authors state that \u201cfeatures within tabular data have independent distributions and ranges, and missing values may be present.\u201d Neglecting the missing values, how are the authors treating this challenge of different ranges? The Euclidean distance between features can largely vary if ranges differ, so how exactly are these values converted into image pixels which usually are defined within a fixed range of [0, 255] per channel?\n---\n\n### Experiments & Interpretation: \n\nTable 1 aims to demonstrate the benefit of \u201cPrior Knowledge Learnt from the image domain\u201d -> I\u2019d like the authors to further clarify the exact experimental setting that has been performed here: \n- Are the experiments without image-pretraining simply trained on the tabular images?  \n- Or are they using a \u2018randomly initialized\u2019 backbone? \n- Are the image-pretrained methods further fine-tuned on some tabular image data? \n\nAll this information will help the reader to better judge to which extend information is potentially \u2018transferred\u2019, what might be the risk of overfitting, etc.;"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd",
                        "ICLR.cc/2024/Conference/Submission3231/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631595629,
            "cdate": 1698631595629,
            "tmdate": 1700699650895,
            "mdate": 1700699650895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FtPsMUOGYB",
                "forum": "K8Mbkn9c4Q",
                "replyto": "BAyZrGaKEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "**Technical parts:**\n\n**Q1. Creating the (N,N) feature matrix R via Euclidean distance between N features -> What is the intuition behind this? Euclidean distance is symmetric (as squared), so isn\u2019t the (N,N) matrix symmetric (if unranked) or has double-entries (if sorted/ranked)?**\n\n- First, we have incorporated comments by adding detailed explanations to Section 3.2, \"Domain Transformation,\" on page 4 and to \"Algorithm 1\" on page 14 of the paper. As described in the paper, we aimed to create an (n, n) feature matrix to intuitively represent the similarities among features of tabular data, similar to an adjacency matrix. Therefore, as you have understood, the (n, n) matrix is a symmetric matrix.\n\n**Q2. The authors then go on to state: \u201cWe also measure the distance and rank between N elements [..] to generate an (N,N) pixel matrix, denoted as Q.\u201d -> What exactly is being compared/contrasted here? What \u2018pixels\u2019 are used here?**\n\n- Thank you for providing a detailed question. I recognize that the wording I used in that section did not clearly convey my intent, and we have revised section 3.2 on page 4 of the paper accordingly. As explained in the revised paper, when tabular data with n features is directly converted into a 2-dimensional image form, each pixel represents one of the n features. Therefore, the feature matrix calculates the distances between the coordinates of these n pixels. For instance, if there is tabular data where n=6=2*3, converting this data into a 2-dimensional image form would assign the 6 features coordinates like (0,0), (0,1), (0,2), (1,0), (1,1), (1,2). The feature matrix then calculates the distances between these coordinates, as illustrated in the figure.\n\n|   |   |   |   |   |   |\n|---|---|---|---|---|---|\n| 0 | 1 | 2 | 1 | \u221a2 | \u221a5 |\n| 1 | 0 | 1 | \u221a2 | 1  | \u221a2 |\n| 2 | 1 | 0 | \u221a5 | \u221a2 | 1  |\n| 1 | \u221a2 | \u221a5 | 0 | 1  | 2  |\n| \u221a2 | 1 | \u221a2 | 1 | 0  | 1  |\n| \u221a5 | \u221a2 | 1 | 2 | 1  | 0  |\n\n**Q3. This is followed by another Euclidean distance between R and Q \u2013 Again, I am missing the intuition/justification behind this.**\n\n- The feature matrix R represents the similarity between features when tabular data is simply reshaped into two dimensions. Q, on the other hand, signifies the distances between coordinates in an image of the same shape. Therefore, Q is a matrix of an image with the natural characteristic (spatial relation) where similar values are located near each other. Hence, we hypothesized that by calculating and reducing the distance between R and Q, we could impart spatial relations onto R. Indeed, our experiments confirmed that altering the order of features in R to minimize its distance from Q contributed to performance improvements in few-shot learning. The experimental results show that the method of directly converting tabular data into tabular images, termed 'direct', does not involve the process of reducing the distance between R and Q. It becomes evident that adding spatial relations indeed aids in enhancing performance.\n\n- If you look at the additional experiments added to Appendix G on page 16 of the paper, you will find a comparison of the performance of three methods: a direct method that manipulates shapes in 2D without considering spatial relations, a Graph method that represents the shapes as graphs, and our domain transformation method. The experimental results demonstrate that the presence or absence of spatial relations significantly impacts the performance of few-shot learning.\n\n**Q4. The authors claim that this then results in \u201ca 2-dimensional image of size (Nr x Nc)\u201d. How exactly is this obtained from computing the Euclidean distance between two (N,N) matrices?**\n\n- To provide a simple example, let's assume feature matrix R initially represents the similarity between (f1, f2, f3, f4) and (f1, f2, f3, f4), where f1, f2, f3, f4 are arbitrary features. If Nr=2 and Nc=2, the image formed would be [[f1, f2], [f3, f4]]. However, if the order of the features is changed, it can be seen as altering the similarity to that between (f3, f1, f4, f2) and (f3, f1, f4, f2). In this case, the image would become [[f3, f1], [f4, f2]]. This process is detailed in Algorithm 1 on page 14 of the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408288969,
                "cdate": 1700408288969,
                "tmdate": 1700408288969,
                "mdate": 1700408288969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ADCHVlftlZ",
                "forum": "K8Mbkn9c4Q",
                "replyto": "BAyZrGaKEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewers,\n\nI hope this message finds you well. As the deadline for the peer review of my academic paper is approaching tomorrow, I wanted to send a gentle reminder.\n\nThank you very much for your time and dedication to this process. Should you have any additional comments or questions regarding the paper, please feel free to ask, and I will do my utmost to provide comprehensive and helpful responses.\n\nWarm regards,\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654945671,
                "cdate": 1700654945671,
                "tmdate": 1700654945671,
                "mdate": 1700654945671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EQzzM9Vyk7",
                "forum": "K8Mbkn9c4Q",
                "replyto": "oajUuSvDgs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_NMSd"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the provided answers"
                    },
                    "comment": {
                        "value": "I'd like to genuinely thank the authors for providing detailed answers and making a number of improvements to the manuscript, I do appreciate the effort!    \nHaving read the new manuscript version, my concerns about the missing details on the crucial part of domain transformation have been mostly resolved.   \nHowever, I feel that your paper still somewhat misrepresents your contribution to the domain transformation, the main parts of which are mostly introduced in Zhu et al.'s work as the direct quote from their manuscript shows:  \n> *To meet this challenge, we develop a novel algorithm, image generator for tabular data (IGTD), to transform tabular data into images by assigning features to pixel positions so that similar features are close to each other in the image. The algorithm searches for an optimized assignment by minimizing the difference between the ranking of distances between features and the ranking of distances between their assigned pixels in the image.*  \n\nI further share the concerns of reviewer hBJa regarding the effectiveness of the approach.\n\n$\\rightarrow$ Given these new insights, I have slightly raised my score to  *5: marginally below the acceptance threshold* but remain still critical."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699604295,
                "cdate": 1700699604295,
                "tmdate": 1700699604295,
                "mdate": 1700699604295,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lnogrsfHaK",
            "forum": "K8Mbkn9c4Q",
            "replyto": "K8Mbkn9c4Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_hBJa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_hBJa"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors delve into the domain of few-shot tabular representation learning by introducing a novel perspective\u2014treating tabular data as images. They introduce a method called TablEye, which begins by converting tabular data into the image domain and subsequently harnesses image-based representations to enhance performance in few-shot tabular learning tasks. Notably, the experimental results showcase TablEye's efficacy as it outperforms existing methods such as TabLLM and STUNT in these tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "One of the notable strengths of this paper is the novel idea of utilizing image domain priors for few-shot tabular learning. This approach capitalizes on the inherent structure and relationships within image data to address the challenges of tabular learning, demonstrating its effectiveness in transferring knowledge to few-shot scenarios."
                },
                "weaknesses": {
                    "value": "While TablEye represents a promising approach, it is not without its limitations. One concern is the potential scalability issues that might arise when dealing with tabular data possessing a substantial number of features. The transformation of tabular data into an image format could lead to image dimensions that are impractically large, which may hinder the method's scalability and efficiency. Additionally, the authors acknowledge that for heterogeneous tabular data, establishing meaningful spatial relationships within the transformed images can be a daunting task. This limitation suggests that the proposed method may not be a universally applicable solution for all tabular learning problems, especially those with highly diverse data structures."
                },
                "questions": {
                    "value": "The paper raises intriguing questions regarding the choice of feature extraction techniques. While the primary focus of the paper lies in feature extraction using Convolutional Neural Networks (CNNs), the authors mention the possibility of utilizing pre-trained Vision Transformers (ViT). It prompts further exploration of whether ViT could serve as a viable alternative to CNNs for this specific application. The underlying assumption that inductive bias plays a crucial role in the success of TablEye raises the question of whether ViT, with its distinct characteristics, would be as effective in leveraging this bias.\n\nFurthermore, the paper highlights the potential challenge of handling tabular data with an exceedingly large number of features. It is worth considering how a conventional CNN architecture, or even alternative methods, could adapt to accommodate such datasets while maintaining computational efficiency. This consideration adds an interesting dimension to the discussion about the method's scalability and practicality in real-world applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773005381,
            "cdate": 1698773005381,
            "tmdate": 1699636271355,
            "mdate": 1699636271355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lc2fdN1EsM",
                "forum": "K8Mbkn9c4Q",
                "replyto": "lnogrsfHaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "**Q1. The paper raises intriguing questions regarding the choice of feature extraction techniques. While the primary focus of the paper lies in feature extraction using Convolutional Neural Networks (CNNs), the authors mention the possibility of utilizing pre-trained Vision Transformers (ViT). It prompts further exploration of whether ViT could serve as a viable alternative to CNNs for this specific application. The underlying assumption that inductive bias plays a crucial role in the success of TablEye raises the question of whether ViT, with its distinct characteristics, would be as effective in leveraging this bias.**\n\n- We conducted experiments using the CLIP pre-trained ViT encoder and also using ViT as a backbone with the Proto-layer as a classifier. Initially, we directly used CLIP's pre-trained ViT encoder as a backbone for few-shot classification. However, CLIP demonstrated almost no learning in all cases. We believe this is because pre-trained models are already trained in a specific direction in the case of CLIP, to narrow the distance with similar texts. When we used ViT as the backbone and trained it with prior knowledge from the mini-ImageNet dataset, T-P-V showed quite good performance. Although it was lower than other TablEye methods, this suggests that the characteristics of ViT can utilize the inductive bias of TablEye. This finding highlights the potential adaptability of ViT in different learning contexts, especially in tasks like few-shot learning where leveraging prior knowledge and inductive biases is crucial. We have added these experimental results to Appendix H on page 16 of the paper.\n\n**Q2. Furthermore, the paper highlights the potential challenge of handling tabular data with an exceedingly large number of features. It is worth considering how a conventional CNN architecture, or even alternative methods, could adapt to accommodate such datasets while maintaining computational efficiency. This consideration adds an interesting dimension to the discussion about the method's scalability and practicality in real-world applications.**\n\n- Thank you for raising the crucial issue of scale-up. We considered whether TablEye could function in scenarios with a large number of features. For TablEye to work, domain transformation must occur, and the backbone and classifier must be trainable. Since image models can adapt to the size of the input image, TablEye can operate as long as domain transformation is feasible. Given that the time complexity of Domain Transformation is O(n^2), this process will likely be the bottleneck when dealing with a large number of features. However, given enough time, it is still operational. We agree that this is an important issue worth further exploration. In future work, we plan to investigate new algorithms that can maintain the performance of domain transformation while reducing its time complexity. This would significantly enhance the scalability of TablEye, especially in large-scale applications."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407762283,
                "cdate": 1700407762283,
                "tmdate": 1700407762283,
                "mdate": 1700407762283,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GKVT6ATjWB",
                "forum": "K8Mbkn9c4Q",
                "replyto": "lnogrsfHaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewers,\n\nI hope this message finds you well. As the deadline for the peer review of my academic paper is approaching tomorrow, I wanted to send a gentle reminder.\n\nThank you very much for your time and dedication to this process. Should you have any additional comments or questions regarding the paper, please feel free to ask, and I will do my utmost to provide comprehensive and helpful responses.\n\nWarm regards,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654927991,
                "cdate": 1700654927991,
                "tmdate": 1700654927991,
                "mdate": 1700654927991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h8tGPPMDch",
                "forum": "K8Mbkn9c4Q",
                "replyto": "GKVT6ATjWB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_hBJa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Reviewer_hBJa"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the authors for the rebuttal. \n\nAfter reading the rebuttal, I am still questionable about the application of TablEye. For example, I believe TablEye rely on the representational knowledge of pre-trained Image encoder, but using better representations (e.g., CLIP encoder) do not work better than CNN trained on mini-ImageNet, which questions the effectiveness of proposed approach. Also, I think the proposed method would lose efficiency as the number of features increases. Lastly, from the public comment by Capsian, there is a prior work that used the idea of using image feature for tabular learning, which weakens the novelty of proposed method. Therefore, I keep my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657800401,
                "cdate": 1700657800401,
                "tmdate": 1700657800401,
                "mdate": 1700657800401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZInISvzd0e",
            "forum": "K8Mbkn9c4Q",
            "replyto": "K8Mbkn9c4Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_TNZH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3231/Reviewer_TNZH"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents TablEye, a novel framework for few-shot tabular learning. To overcome the limit of forming prior knowledge for tabular data, TablEye utilizes a two-stage process, transforming tabular data into tabular images and learning prior knowledge from labeled image data. The paper reports improved performance and applicability to medical datasets. Overall, I vote for accepting. TablEye introduces a novel approach to few-shot tabular learning, which is a relatively underexplored area in research. There are only several techniques for this task and they still have some constraints. The paper offers innovative solutions, using prior image knowledge through a few-shot learning method, and demonstrates clear performance improvements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "TablEye addresses a challenging problem in few-shot tabular learning using a unique approach. Few-shot tabular learning is a relatively new and underserved area, and the paper contributes to this domain. TablEye introduces an innovative approach to few-shot tabular learning by bridging the gap between tabular and image data domains.\n\nThe paper provides evidence of TablEye\u2019s effectiveness through a series of experiments. TablEye consistently outperforms existing methods in multiple scenarios, including 1-shot and 4-shot learning. The use of a significantly smaller model size compared to alternatives and less constraints on datasets are also strong points.\n\nThe paper demonstrates the applicability of TablEye to real medical datasets, which implies its potential value in practical applications, especially in domains where accurate few-shot tabular learning is crucial."
                },
                "weaknesses": {
                    "value": "While the paper presents positive results, it lacks detailed discussions regarding the differences in dataset performances. A more in-depth analysis of why certain structures perform better on specific datasets would provide a deeper understanding of TablEye's strengths. It seems that the improvement in performance is partly based on the variety of structures since none of them perform well in most of the experiments. \n\nThe experiments of T-M-C2, T-M-C3, T-M-C4 on comparison with TabLLM and in the context of medical results are lacking.\nLack of Detailed Implementation: The paper offers an overview of the framework but lacks detailed implementation specifics. It would be better if you could present the structure of classifiers and also some equations or pseudo-code for all the parts of the model, especially the domain transformation.\n\nAdditional Context for Few-Shot Learning: Providing a brief introduction to the general few-shot learning problem, its significance, and existing challenges would be beneficial for readers unfamiliar with the field."
                },
                "questions": {
                    "value": "Could you please provide more details on the differences in performances across different datasets, particularly explaining why some structures perform better on specific datasets? This would help in understanding TablEye's strengths and limitations better. Could you explain why STUNT performs better in the dataset Karhunen? Similarly, why in some cases Conv2 is better than Conv4, for example, the datasets Optdigits and Karhunen? \n\nMoreover, Could you please provide experiments on T-M-C2, T-M-C3, T-M-C4 on comparison with TabLLM and in the context of medical results? Are there any recommendations or guidelines for selecting the most appropriate structure when using datasets?\n\nCould you offer more detailed implementation specifics? The figures, equations, or pseudo-code can help understand each part of the model better, especially the domain transformation, which was kind of hard to understand at first. \nFor the part about repeating the matrix, have you tried other methods like resizing or padding besides tilling when dealing with the matrix? Is tilling the best solution? \n\nAnd at last, it would be better if you could present briefly the structure of the classifiers you used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892473322,
            "cdate": 1698892473322,
            "tmdate": 1699636271281,
            "mdate": 1699636271281,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RiVMWiFmgM",
                "forum": "K8Mbkn9c4Q",
                "replyto": "ZInISvzd0e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "**Q1. Could you please provide more details on the differences in performances across different datasets, particularly explaining why some structures perform better on specific datasets? This would help in understanding TablEye's strengths and limitations better. Could you explain why STUNT performs better in the dataset Karhunen? Similarly, why in some cases Conv2 is better than Conv4, for example, the datasets Optdigits and Karhunen?**\n\n- Generally, TablEye demonstrates consistent high performance across numeric, categorical, and mixed values. TabLLM excels particularly with categorical values, while STUNT tends to perform exceptionally well with numeric values. In the case of STUNT, being a semi-supervised learning approach, the performance improvement was significantly noticeable over the baseline (XGB) when there was an abundance of unlabeled data and a higher number of features (collected information for prediction).\n\n- A trend was observed where performance tended to reverse with an increasing number of features. This was not only evident in datasets like optdigits (64 features) and karhunen (64 features) but also in Cancer (32 features), Heart (12 features), and Lung (16 features). The authors speculate that this might be due to the training epochs being uniformly limited to 100. In simpler structures (small images), convergence up to conv4 leads to superior performance, whereas in more complex structures (large images), Conv3 or Conv2 tends to perform better. Taking into consideration the reviewer's comments, the authors agree on the necessity to experiment by increasing the number of training epochs and comparing accuracy, and plan to conduct and report this in the future.\n\n**Q2. Moreover, Could you please provide experiments on T-M-C2, T-M-C3, T-M-C4 on comparison with TabLLM and in the context of medical results?**\n\n- We conducted experiments on T-M-C2, T-M-C3, and T-M-C4 with TabLLM and in the context of medical results These findings are reflected in Table 2 on page 7 and Table 7 on page 15 of the paper. The results showed that methods using the MAML-layer achieved higher numbers in some cases compared to previous figures.\n\n\n**Q3. Are there any recommendations or guidelines for selecting the most appropriate structure when using datasets?**\n\nThe type of recommendation or guidance needed varies depending on the data and learning environment. If there are a few features, a complex structure (like conv4) might converge with a small amount of training, showing a high possibility of achieving high accuracy. However, if there are many features and training is minimal, a complex structure may not learn effectively, and a simpler structure could be more beneficial. If there are many features but sufficient resources for adequate training, we recommend without hesitation using a complex structure.\n\n| Epoch Count / Number of Features | Few      | Many                                      |\n|----------------------------------|----------|-------------------------------------------|\n| Few                              | Complex  | Simple                                    |\n| Many                             | Complex  | Unknown, but complex structure recommended |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407496062,
                "cdate": 1700407496062,
                "tmdate": 1700407496062,
                "mdate": 1700407496062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dCJ1h5sqXs",
                "forum": "K8Mbkn9c4Q",
                "replyto": "ZInISvzd0e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewers,\n\nI hope this message finds you well. As the deadline for the peer review of my academic paper is approaching tomorrow, I wanted to send a gentle reminder.\n\nThank you very much for your time and dedication to this process. Should you have any additional comments or questions regarding the paper, please feel free to ask, and I will do my utmost to provide comprehensive and helpful responses.\n\nWarm regards,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654900416,
                "cdate": 1700654900416,
                "tmdate": 1700654900416,
                "mdate": 1700654900416,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]