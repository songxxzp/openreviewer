[
    {
        "title": "Adversarial Causal Bayesian Optimization"
    },
    {
        "review": {
            "id": "18EHAAKHy4",
            "forum": "YcW8i9VCf5",
            "replyto": "YcW8i9VCf5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_VBSi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_VBSi"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for causal Bayesian optimization in non-stationary where the authors also allow for multi-agent environments. They present result on eight synthetic environment and one (very interesting) real environment where they demonstrate competitive results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main review, with comments and questions, are all in this section owing to the flow in which this review was conducted.\n\n### Abstract\n\n- I think there is some ambiguity w.r.t. the first sentence: the DAG can be known but the relationships (the mechanisms) of the DAG unknown, or vice versa. Which one do you mean? In the original paper (Aglietti et al) the DAG was always assumed known but there are other settings where the DAG is assumed unknown.\n- Good abstract. Perhaps a bit more information on the experiments you performed (just the one sentence ought to do it).\n\n### Introduction\n\n- The supply example is great, but the way you introduce it in the second paragraph, reads a bit forced. Consider rephrasing. It doesn't sound very good at the moment.\n- If you are modelling a phenomena in an environment that is changing, why is it not possible to model that with a temporal DAG? Or dynamical DAG? There is plenty of that type of work being done. \n- Fig 1c - I am confused. Your blue nodes sound very much like standard non-manipulative variables and the idea of non-manipulative variables in causal setting, was introduced a long time ago. How are your blue nodes different?\n\n### Background and problem statement\n\n- There is some confusion here. In the abstract you said that $\\mathcal{G}$ was unknown and now at the start of paragraph one you say that $\\mathcal{G}$ is in fact know. Which is it?\n- Is there a reason you deviate from the standard SCM definition from Pearl with $\\langle U,V,P,F \\rangle$? It seems unnecessary to introduce new notation for a setting which is well defined and well studied. You are just saying, in different words and notation, the interaction between the endogenous and exogenous variables in the SEM. More confusingly though you say that the $\\mathcal{G}$ is part of your SCM definition whereas in the standard setting (well, Pearl's setting) the causal diagram is induced by the SCM, not part of it. See chapter 3 (Pearl, 2009).\n- What is the reasoning behind using soft rather than hard interventions? What would happen if you used hard instead?\n- There are as many actions as there are nodes $m$ in the graph? But then you are also intervening on the reward variable which is non-manipulative?\n- To check my understanding: actions are continuous, but there are a finite amount of continuous actions, the cardinality of the domain of each action is then $K$? Why isn't each action just continuous?\n- I think you should rephrase the uncertain parts of your problem statement: it is not the case that the causal model is unknown (this typically means the graph) but rather that the mechanisms of the SCM are unknown. You are not being precise enough at the moment to ward off ambiguity. Please change.\n\n### Method\n\n- I think this very important part deserves a deeper treatment, you say \"Contrary to standard CBO (where algorithms can choose actions deterministically), in adversarial environments such as ACBO randomization is necessary to achieve no-regret\" - why is that the case? Are you then saying that if you are using deterministic action selection it would be impossible to attain no-regret?\n- Consider using left-pointing arrows in algorithm 2 to make it more procedural, in place of using equality signs on line 4 and 5. That goes in general for all your algorithms.\n- How many times do you have to initialise the neural networks in algorithm 2 for this to work?\n\n### Analysis\n\n- It would be helpful if you gave an example of a Lipschitz continuous kernel, for uninitiated reader. I would also like to know what the consequence would be if you did not make this continuity assumption and how realistic it is?\n\n### Computational considerations in larger action spaces\n\n- Can you please comment on this line: \"even with a large number of action variables $m$, $|\\mathcal{A}|$ may still be small and thus CBO-MW feasible to implement\" - what is 'large' here? When does it become unfeasible? Some numerical ballpark figures would be helpful. \n\n### Experiments\n\n- To confirm: you are considering the causally sufficient setting i.e. you assume there are no unobserved confounders? If so, please state that early on in the paper (apologies if I missed it).\n- Would it also make sense to also compare against CBO? Don't worry this review is not conditional upon you doing that, I am merely wondering why it is not part of your analysis, seeing as you talk about it early on.\n- The SMS example is _great_. Really enjoyed reading that."
                },
                "weaknesses": {
                    "value": "See strengths.\n\nNote: I have given this is a five to start with. I would be happy to increase my score following author engagement."
                },
                "questions": {
                    "value": "See strengths."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698255274744,
            "cdate": 1698255274744,
            "tmdate": 1699636058229,
            "mdate": 1699636058229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b4oHwnPuWZ",
                "forum": "YcW8i9VCf5",
                "replyto": "18EHAAKHy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer VBSi (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thorough review.\n\n**I think there is some ambiguity w.r.t. the first sentence: the DAG can be known but the relationships (the mechanisms) of the DAG unknown, or vice versa. Which one do you mean? In the original paper (Aglietti et al) the DAG was always assumed known but there are other settings where the DAG is assumed unknown.**\n\nWe know the graph $G$ but not that mechanism (the $f_i$). The same as Aglietti et al. We think that this is clear from the paper itself. We can make this more explicit in the abstract by changing this sentence to \u201c... on a structural causal model with known graph but unknown mechanisms\u2026\u201d.\n\n**If you are modelling a phenomena in an environment that is changing, why is it not possible to model that with a temporal DAG? Or dynamical DAG? There is plenty of that type of work being done.**\n\nHere we model a changing environment where we measure and react to potentially adversarial changes. Our understanding is that the use of a temporal or dynamical DAG requires an explicit probabilistic model for the changing environment, whereas we do not need to create a model for the adversary\u2019s behavior. We believe that it is not obvious how one could concretely use temporal and dynamical DAG models in our setup and with the notion of regret we study. \n\n**Your blue nodes sound very much like standard non-manipulative variables**\n\nAssuming the reviewer refers to the non-manipulable variables studied in [1],  these are variables in the graph that one cannot intervene on. As we describe in section 2, we consider a soft intervention model where the $a_i\u2019$ are parameters of the soft intervention on observation $X_i$. Therefore, we don\u2019t see a connection between $a\u2019$ and non-manipulative variables, because the former are the parameters of an intervention (performed by an adversary). \n\n[1] Lee, S., & Bareinboim, E. (2019). Structural Causal Bandits with Non-Manipulable Variables. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 4164-4172.\n\n**What is the reasoning behind using soft rather than hard interventions? What would happen if you used hard instead?**\n\nThanks for your question. We note that one could still run CBO-MW  with hard interventions without modifications. This would however be incompatible with some of the technical assumptions in the analysis (that all $\\mu_i, \\sigma_i$ are Lipschitz). Hence we have focused on the soft intervention model for simplicity.\n\n**There are as many actions as there are nodes m in the graph? But then you are also intervening on the reward variable which is non-manipulative?**\n\nWe note that the learner is not necessarily allowed to intervene on the reward, in which case $\\mid \\cal{A_m} \\mid = 1$ and all our results still go through. However, a soft intervention model generally allows the agent to directly intervene on the reward. Since soft interventions don\u2019t break the causal relationship between a variable and it\u2019s parents, it is still meaningful to consider cases where the reward can be intervened upon.\n\n**To check my understanding: actions are continuous, but there are a finite amount of continuous actions, the cardinality of the domain of each action is then K? Why isn't each action just continuous?**\n\nThe reviewer is correct. The reason why we consider finitely many actions, though, is for the learner to tractably achieve no regret. Indeed, in the adversarial setting, the learner must randomize its decision and sample actions from a mixed strategy. Such a strategy is tractable to compute, and update  (line 6 of algorithm 1), if there is a finite number of actions. In practice, however, continuous action spaces can be discretized. \n\n**\"Contrary to standard CBO (where algorithms can choose actions deterministically), in adversarial environments such as ACBO randomization is necessary to achieve no-regret\" - why is that the case?**\n\nIntuitively, because the adversary has access to the game history and to the learner\u2019s algorithm, if such an algorithm was deterministic then the adversary could anticipate the learner\u2019s moves and thus inflict positive regret at each round. This is a standard argument in adversarial online learning and multiplayer games. \n\n**How many times do you have to initialise the neural networks in algorithm 2 for this to work?**\n\nIn our experiments we performed a single initialization. \n\n**It would be helpful if you gave an example of a Lipschitz continuous kernel**\n\nMany commonly used kernels over continuous domains are Lipschitz continuous. Two examples: linear kernel and squared exponential kernel. This is mentioned in the cited work that we defer to in our paper. \n\n**what the consequence would be if you did not make this continuity assumption**\n\nWe note that such an assumption is only required for the analysis of our regret guarantee. It is not a required assumption to apply the algorithm in practice."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216497341,
                "cdate": 1700216497341,
                "tmdate": 1700216497341,
                "mdate": 1700216497341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vDF3DHRBim",
                "forum": "YcW8i9VCf5",
                "replyto": "18EHAAKHy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - end of discussion period soon"
                    },
                    "comment": {
                        "value": "We would like to ask the reviewer whether our review responses and the revision clarify the reviewer\u2019s concerns and change their score considerations. We are happy to provide further clarifications while the platform is still open for discussion, which is until the end of 22nd of November."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584491591,
                "cdate": 1700584491591,
                "tmdate": 1700584491591,
                "mdate": 1700584491591,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aK2OYcVVXi",
                "forum": "YcW8i9VCf5",
                "replyto": "vDF3DHRBim",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_VBSi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_VBSi"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging"
                    },
                    "comment": {
                        "value": "Thanks to the authors for a thorough response. I confirm that I have read their review and have no further questions or comments."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676090174,
                "cdate": 1700676090174,
                "tmdate": 1700676090174,
                "mdate": 1700676090174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dazz0FLB8m",
            "forum": "YcW8i9VCf5",
            "replyto": "YcW8i9VCf5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_aRF4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_aRF4"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a model where an agent interacts with an unknown causal model that is partly controlled by an adversary. The problem is formulated as a Bayesian optimization problem. The paper proposes an algorithm based on multiplicative weights to solve the problem, which also uses the idea of the upper confidence bound algorith, that adopts an optimistic view in the face of uncertainty. Regret bounds were derived in the paper, and the proposed algorithm was evaluated empirically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of studying an online causal model looks interesting."
                },
                "weaknesses": {
                    "value": "I don't see any fundamental difference between the studied model and a standard bandit or Bayesian optimization problem, where part of the model is stochastic and part of it is controlled by an adversary. Therefore, apart from having a causal model in the story, the novelty of the contribution seems limited."
                },
                "questions": {
                    "value": "- It is mentioned in the problem statement that the adversary does not know the action to be performed by the agent. Could you explain what the choice of the adversary's action is based on? If the worst-case analysis is applied here, does it matter whether the adversary know the agent's action or not since the adversary will always act in the worst way anyhow? Or does the adversary choose the worst action based only on history actions? But then do they know the agent's algorithm or not? The assumption that they don't know the agent's action seems a bit odd. \n\n- Could you explain the difference between your model and a model that combines stochanstic and adversary bandit?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Reviewer_aRF4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689299486,
            "cdate": 1698689299486,
            "tmdate": 1700693858625,
            "mdate": 1700693858625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WREYvj862w",
                "forum": "YcW8i9VCf5",
                "replyto": "Dazz0FLB8m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer aRF4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review. Below, we respond to the reviewer\u2019s concerns.\n\n**Could you explain what the choice of the adversary's action is based on?**\n\nFrom the text: \u201cWe assume the adversary does not have the power to know $a_{:, t}$ when selecting $a\u2032_{:,t}$, but only has access to the history of interactions until round $t$.\u201d Additionally, yes, we assume the adversary can know the agent\u2019s algorithm (we\u2019ll add a note to make this clear in the text). Since the agent can play a mixed strategy, the adversary can at best know the distribution the agent will select actions from at each round. This is not as strong of an adversary as knowing the agent\u2019s exact actions, and makes the problem more intricate as the learner can achieve no regret if it\u2019s mixed strategy is updated suitably (as we propose in the CBO-MW algorithm). We remark that this is a standard setup for the adversary in adversarial online learning.\n\nIf the adversary at time $t$ could see the agent\u2019s action $a_t$ before selecting $a_t\u2019$, it would be a Stackelberg game. We could also model this in our framework by directly modeling the adversary\u2019s action as an observation $X_i$, since it is a direct reaction to the action we chose. We describe this in appendix A.1.1. \n\n**Could you explain the difference between your model and a model that combines stochanstic and adversary bandit?**\n\nHere we request further information from the reviewer. It is not clear to us what the point of comparison is because it is not obvious what it means to just combine a stochastic and adversarial bandit. We think if the claim is that the contribution is limited, it would be easier for us to give a helpful response if the reviewer 1) refers to specific works in the literature and then 2) describes how the delta between our work and these is trivial. We think this would allow us to more concretely discuss the contribution. \n\n**I don't see any fundamental difference between the studied model and a standard bandit or Bayesian optimization problem, where part of the model is stochastic and part of it is controlled by an adversary.**\n\nWe respectfully disagree with the reviewer and we hope that the points discussed below can further clarify the novelty of our approach. In particular, we believe there is significantly novelty compared to previous Bayesian optimization methods:\n- Because some part of the model is adversarial, the learner must randomize its actions to achieve no regret. Although standard adversarial online learning algorithms (such as multiplicative weights) exist for this, they require full-information feedback which is not available in Bayesian optimization. \n- The only Bayesian optimization approach that can provably achieve no regret in such a setting is GP-MW. However, this algorithm does not build a causal graph and can thus be highly sample inefficient, as we show.\n- In light of the above, CBO-MW is the first Bayesian optimization algorithm which can: 1) achieve no regret using 2) the learning of a causal reward model.  Similar to GP-MW, CBO-MW also computes optimistic estimates for the full information feedback. However, because such estimates are obtained through computing a counterfactual using a causal graph, we provide a practical subroutine to compute these (optimistic) counterfactuals in an efficient way (section 4.3).\n- We provide a regret guarantee and can show significantly improved rates compared to GP-MW  (section 5)\n- We show how to implement CBO-MW in a distributed way to improve the computational efficiency (section 6) and demonstrate good empirical performance (section 7). \n\nWe hope the above points clarify the reviewer\u2019s concerns. We are happy to expand more based on the reviewer\u2019 feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215651423,
                "cdate": 1700215651423,
                "tmdate": 1700215651423,
                "mdate": 1700215651423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wGuZyfcfPQ",
                "forum": "YcW8i9VCf5",
                "replyto": "Dazz0FLB8m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder- end of discussion period soon"
                    },
                    "comment": {
                        "value": "We would like to ask the reviewer whether our review responses and the revision clarify the reviewer\u2019s concerns and change their score considerations. We are happy to provide further clarifications while the platform is still open for discussion, which is until the end of 22nd of November."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584129173,
                "cdate": 1700584129173,
                "tmdate": 1700584129173,
                "mdate": 1700584129173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1xD1c6jlj3",
                "forum": "YcW8i9VCf5",
                "replyto": "WREYvj862w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_aRF4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_aRF4"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "Thank you for your clarifications. I updated my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693827102,
                "cdate": 1700693827102,
                "tmdate": 1700693827102,
                "mdate": 1700693827102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xXo4AHFPIW",
            "forum": "YcW8i9VCf5",
            "replyto": "YcW8i9VCf5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_h5FK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_h5FK"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies causal bayesian optimization under certain kinds of adversaries who can pick additive variables in the causal graph post seeing the variables of the agent up to time t-1. \n\n- They derive regret upperbounds for a variant of the multiplicative weights algorithm and show scaling with sqrt(T). \n- The analysis is reasonably strong, but motivation could be made more clear -- the motivating examples are not necessarily adversarial."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Experiments show that the algorithm is strong for the use cases considered.\n- Well written problem statement.\n- For the model chosen, the analysis is sound."
                },
                "weaknesses": {
                    "value": "- The related work ignores causal bandit literature?\n- The problem is not well motivated. Why is SMS adversarial and not stochastic?\n- The graph notations are confusing. The typical graph has 1 root node. But in causal graphs, we may have multiple nodes without parents.\n- The adversary cannot see the action taken by agent before taking its action. Is this adversary weak? You have considered that the agent can see adversary's action before choosing their own action set, but what about vice-versa?\n- The additive term Beta^{N+1} does seem high.\n- Note that \"Causal Bandits for Linear Structural Equation Models\" Varici et al 2023 show that the regret scales as length of the longest causal path in the graph for linear SCMs, whereas you consider N - length of path to root node. The former (not cited in your work), seems tighter.\n- Is the assumption of usage of only finite action spaces chosen from continuous Reals_[0,1] feasible? If we draw an epsilon net over [0,1], then the computation complexity of Alg 1 may rise.\n- There are exponential combination of adversarial choices, for each of which a counterfactual computation may be taken up. This is computationally demanding."
                },
                "questions": {
                    "value": "- Page 2: If X_m is a leaf, and it is the reward variable, then it has no parents? Did you mean X_0 is the reward variable?\n- You speak of Adversarial CBO, but assume a SCM. Do Causal Bayesian Networks involve the functional relations between the variables?\n- Why does cumulative regret go down with increasing rounds for Dropwave Penny in Figure 2?\n- What is the lower bound for regret?\n- You speak to a sqrt(t) dependence on regret, but the regret curve flattens for your experiment (and even decreases) in the graphs. Why do you believe this is happening?\n- Notation question: fi: Zi \u00d7 Ai \u00d7 A\u2032i \u2192 Xi. Should this not be fi: Zi \u00d7 Ai \u00d7 A\u2032i \u00d7 \u03a9 \u2192 Xi\n- You say \"Because mechanisms can be non-monotonic and nonlinear, one cannot simply independently maximize the output of every mechanism. We thus defer this task to an algorithmic subroutine (denoted causal UCB oracle)\". In this algorithm, you use a neural network explicitly. Does the error in functional approximation due to nn use not flow into the regret term?\n- Can line 7 in Alg 1 be amended to optimize over a' in [0,1] as well?\n- Why is it necessary to learn the causal function at each node, and not just at node Y, or at parents of Y? To bound reward estimates at Y, do we need equally good estimates at all nodes in the graph? (If not the search space for a,a' goes lower and therefore the number of calls to Alg2).\n\n\n## Typos:\n- Page 2 - \"for the parents this node\"\n\n## Suggestions\n- Please expand on the literature review.\n\n---\nNote: May be willing to improve the score based on author responses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Reviewer_h5FK",
                        "ICLR.cc/2024/Conference/Submission1308/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788366467,
            "cdate": 1698788366467,
            "tmdate": 1700653205151,
            "mdate": 1700653205151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GDnjUsOw90",
                "forum": "YcW8i9VCf5",
                "replyto": "xXo4AHFPIW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer h5FK (part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thorough review. \n\n**\u201cThe related work ignores causal bandit literature?\u201d**\nAfter the related work sentence \u201cAgliettiet al.(2020) propose the first CBO setting with hard interventions and an algorithm that uses the do-calculus to generalise from observational to interventional data\u201d, we will add \u201cThe CBO line of work builds off the causal bandit setting [cite Lattimore], which similarly incorporates causal graph knowledge into the bandit setting usually considering discrete actions with categorical observations or linear mechanisms with continuous observations [cite Varici]..\u201d. \n\n**The problem is not well motivated. Why is SMS adversarial and not stochastic?**\n\nWe mention this in the experiments: \u201cAfter each day,we observe weather and demand data from the previous day which are highly non-stationary\u201d but we will add a sentence to the introduction that makes this clear from the beginning. In the 4th paragraph we will add: \u201cIn the SMS application, we observe the demand and weather which can only be observed at the day\u2019s end, and are highly non-stationary. For example, the weather distribution will vary seasonally.\u201d \n\n**\u201cThe graph notations are confusing. The typical graph has 1 root node. But in causal graphs, we may have multiple nodes without parents.\u201d**\nWe were not able to follow this comment. If you could expand on it we are happy to take a look. Based on one of your later comments (addressed below), we think you might be getting root and leaf node mixed up. \n\n**\u201cThe adversary cannot see the action taken by agent before taking its action. Is this adversary weak? You have considered that the agent can see adversary's action before choosing their own action set, but what about vice-versa?\u201d**\nIf the adversary at time $t$ could see our action $a_t$ and then respond with $a_t\u2019$, it would be a Stackelberg game and not a simultaneous action game. Our setting, algorithm, and guarantees can actually also model this Stackelberg game setting. Since the adversary would respond directly to our action $a_t$, we could model the adversary action as a function of $a_t$ (and therefore treat it as just another $X$ in the graph). We discuss this in appendix A.1.1. \n\n**\u201cYou have considered that the agent can see adversary's action before choosing their own action set\u201d**\nThis is not true. We consider a setting where the agent selects $a_t$ simultaneously with the adversary selecting $a_t\u2019$, so the agent does not observe the adversary action beforehand. We believe that this is unambiguous in the paper from section 2 under \u201cProblem Statement\u201d. \n\n**\u201cThe additive term Beta^{N+1} does seem high.\u201d**\nAfter the regret guarantee we compare our regret guarantee with a non-causal approach and give a strong case for why we have a significant improvement. We are also to our knowledge the first to give any kind of guarantee at all for this particular setting (besides the guarantee you get from applying GP-MW which uses no causal information). \n\n**\u201cNote that \"Causal Bandits for Linear Structural Equation Models\" Varici et al 2023 show that the regret scales as length of the longest causal path in the graph for linear SCMs, whereas you consider N - length of path to root node. The former (not cited in your work), seems tighter.\u201d**\n\nWe think that it is not meaningful to directly compare the guarantees from our paper and Varici et al.. Here are the two key differences in the setups:\n\n- They consider linear mechanisms. We consider much more general models for each mechanism where they can be modeled with a GP (come from an RKHS with bounded norm). Linear models are closed under composition, so for their model the reward is still a linear function of the actions. For our setting, GPs are not closed under composition, so we get this emergent complexity where the reward as a function of actions is a much more complicated function class than just a single GP (it is a deep GP [1]). \n- They consider the stochastic setting and therefore a much \u201ceasier\u201d notion of regret than the one we consider in the adversarial/non-stationary case (see equation 3). \n\n[1] Damianou, Andreas, and Neil D. Lawrence. \"Deep gaussian processes.\" Artificial intelligence and statistics. PMLR, 2013.\n\nWe compare our regret bound to the only baseline we know of that can achieve a guarantee in this setting (GP-MW) and give an argument for why our guarantee is favorable. Varici et al 2023 considers a less general problem and while we think it is a great result, it is for a different setting to our result. \n\nWe\u2019ll add a citation of Varici et al 2023 to the related work (see comment above on causal bandits)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214887624,
                "cdate": 1700214887624,
                "tmdate": 1700214938409,
                "mdate": 1700214938409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zu7rfIKSLe",
                "forum": "YcW8i9VCf5",
                "replyto": "xXo4AHFPIW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - end of discussion period soon"
                    },
                    "comment": {
                        "value": "We would like to ask the reviewer whether our review responses and the revision clarify the reviewer\u2019s concerns and change their score considerations. For example, our clarification of relation to the causal bandit literature and relation of the regret bound to prior work. We are happy to provide further clarifications while the platform is still open for discussion, which is until the end of 22nd of November."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584241037,
                "cdate": 1700584241037,
                "tmdate": 1700584241037,
                "mdate": 1700584241037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fx9ejrZhio",
                "forum": "YcW8i9VCf5",
                "replyto": "Zu7rfIKSLe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_h5FK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_h5FK"
                ],
                "content": {
                    "title": {
                        "value": "increasing score to 6"
                    },
                    "comment": {
                        "value": "I have read your comments and thank you for the details. I increase my score to a 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653182842,
                "cdate": 1700653182842,
                "tmdate": 1700653182842,
                "mdate": 1700653182842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AreeGeL3Jc",
            "forum": "YcW8i9VCf5",
            "replyto": "YcW8i9VCf5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_xNam"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1308/Reviewer_xNam"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an extension of the framework of CBO, named ACBO, where other agents (or external events) can also intervene on the system. This is to be able to model changes in the environment. The ACBO framework proposes a concrete algorithm to solve this problem, CBO-MW, which computes optimistic counterfactual reward estimates and enjoys cumulative regret bounds."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Interesting setting of doing BO with causal relationships among variables and external interventions\n- Interesting applied problem in the experiments"
                },
                "weaknesses": {
                    "value": "- The main weakness is the naming of the method, which refers to CBO (Aglietti et al 2020) and hereby its relationship with the CBO setting. The paper claims to show a generalization of CBO, which it seems to be an * algorithm * proposed in Aglietti et al 2020 as a solution to the \"Causal Global Optimization\" (CGO) problem. Here, in the abstract but also in the main paper, (1) no mention to the CGO problem is made (2) CBO seems to refer to the \"setting\" (somehow as a replacement to CGO), while the algorithm proposed here is CBO-MW. But CBO is not a setting, as said, it's an algorithm to solve the CGO under certain assumptions. In CBO, there are intervention *sets* and intervention *values/levels* (continuous-valued). I cannot see any of these here throughout, so it's unclear whether interventions on multiple variables here are excluded or what. In general, are you trying to also solve the CGO problem (a suitably modified version under external interventions, of course) or not ? Again on this point, the authors claim to compare using \"previous CBO benchmarks\", but there are no benchmarks actually from the CBO paper of Aglietti et al 2020, and the CBO-CW is actually *not* compared (nor experimentally, nor methodologically with a discussion) to CBO itself. There is also no \"causal prior\" associated to the GP as in CBO. \n- My understanding then from the above is that this work is actually not an extension of CBO at all (although I would like to hear from the authors), rather an extension of GP-MW which is mentioned a lot and compared to experimentally. \n- Strengthening my belief wrt to the \"distance\" of this work with CBO, the authors here evaluate performance with the **cumulative** regret. This does not seem to be used at all in CBO, where instead a simple regret seems to be used (Aglietti et al 2020)."
                },
                "questions": {
                    "value": "If the authors clarify significantly the relationship with CBO, and modify the claims and narrative in the paper accordingly, I am open to increasing my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1308/Reviewer_xNam",
                        "ICLR.cc/2024/Conference/Submission1308/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793031707,
            "cdate": 1698793031707,
            "tmdate": 1700647842756,
            "mdate": 1700647842756,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FOUnYmkios",
                "forum": "YcW8i9VCf5",
                "replyto": "AreeGeL3Jc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer xNam"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review. \n\n**Relation to CBO**\n\nWe use \u2018causal Bayesian optimization\u2019 (CBO) to describe the general causal Bayesian optimization problem, which is separate from the specific algorithm proposed by Aglietti et al.  There have been several works studying this CBO setting since the original work of Aglietti et al. These works consider multiple variations of their setting: including soft intervention models (instead of hard interventions) and the cumulative regret metric (instead of the simulated regret metric). These are technical differences within a conceptually very similar set of problems. \n\n**The paper claims to show a generalization of CBO, which it seems to be an algorithm**\n\nThank you for pointing out that in Aglietti et al. the authors specifically distinguish between causal global optimization (CGO) as their setting, and causal Bayesian optimization (CBO) as their algorithm. Therefore when specifically describing the setting of Aglietti et al 2019 we will refer to it as CGO in our paper too. We will however continue to refer to the general problem of global optimization in the presence of a causal graph and prior over mechanisms as causal Bayesian optimization. We will also refer to the setting of our paper as ACBO. This is consistent with the wider BO literature where BO - global optimization with a prior - is used to describe a setting and usually the algorithm is described based on the specific acquisition function e.g. UCB, UEI etc. We note that the paper of MCBO, a baseline we compare to, also describes their setting as \u2018CBO\u2019. The main difference here is whether you consider the prior as part of the setting or the algorithm. In Aglietti et al. they use a prior as part of their solution to the CGO problem, but in many other parts of the BO literature, the prior is considered part of the problem setting. \n\n**...CBO, there are intervention sets and intervention values/levels**\n\nCBO as in Aglietti et al. considers a hard intervention model. As discussed in the setup we consider a soft intervention model as in MCBO. Our model includes interventions on multiple variables. This explains the difference to what you read in Aglietti et al. Again we see this as more of a technical difference. \n\n**the authors claim to compare using \"previous CBO benchmarks\", but there are no benchmarks actually from the CBO paper of Aglietti et al 2020**\n\nFirstly, CBO assumes hard interventions so cannot be directly applied at least to the experimental settings we consider. We compare against MCBO on the synthetic experiments. In the MCBO paper, MCBO is compared to CBO with hard interventions in the stochastic BO setting under the cumulative regret metric, and MCBO performs favorably. We say \u201cadversarial versions of previously studied CBO benchmarks\u201d, meaning that we modify some of the environments from the MCBO paper (a paper about CBO) for the adversarial setting. \n\nWe hope the above points clarify the reviewer\u2019s concerns and any misunderstandings. We are happy to expand more on this based on the reviewer\u2019 feedback."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214584027,
                "cdate": 1700214584027,
                "tmdate": 1700214584027,
                "mdate": 1700214584027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bOO6bqxv48",
                "forum": "YcW8i9VCf5",
                "replyto": "AreeGeL3Jc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder- end of discussion period soon"
                    },
                    "comment": {
                        "value": "We would like to ask the reviewer whether our review responses and the revision clarify the reviewer\u2019s concerns and change their score considerations. In particular, our clarification over the naming of the algorithm and naming of the problem setting. We are happy to provide further clarifications while the platform is still open for discussion, which is until the end of 22nd of November."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584324598,
                "cdate": 1700584324598,
                "tmdate": 1700584324598,
                "mdate": 1700584324598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sjLUide0tu",
                "forum": "YcW8i9VCf5",
                "replyto": "AreeGeL3Jc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_xNam"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1308/Reviewer_xNam"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for engaging. \n\nRegarding the first two points, please make sure that readers will not be confused with these things. Yes the MCBO paper called CBO the problem as you said but it indeed confuses the literature. It would be better if you improved the literature and clarified things properly. \n\n**\"Again we see this as more of a technical difference\"** \n\nThis technical difference makes a lot of difference. What do you mean by  \"Our model includes interventions on multiple variables\" ? Do you have to select which variables to intervene as well or it is given ? Again this is a major difficulty in CBO since of course there are a lot of subsets of the DAG variables to search over. \n\n\n\"Firstly, CBO assumes hard interventions so cannot be directly applied at least to the experimental settings we consider\" \n\nAgain this is why it is confusing to say you are generalizing CBO throughout. I understand basically you want to use this name because it's simple and easy to remember, and I think it's even fine; but since there was something in the literature well defined and called CBO, it should be clear to the reader that \"CBO\" here is essentially not \"CBO\" of Aglietti et al 2020. \n\nThe differences are not just technical but major (cumulative regret vs simple regret, no causal priors here like in CBO) and make a lot of difference in what the algorithms should be doing, what are the challenges etc., so they are basically \"related\" research areas but on a superficial level. \n\nWith the hope that authors will clarify significantly the next version I will increase my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647826595,
                "cdate": 1700647826595,
                "tmdate": 1700647886785,
                "mdate": 1700647886785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]