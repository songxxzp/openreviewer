[
    {
        "title": "On Socially Fair Regression and Low-Rank Approximation"
    },
    {
        "review": {
            "id": "7IIjPpx0q1",
            "forum": "KJHUYWviZ6",
            "replyto": "KJHUYWviZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_5W2W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_5W2W"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the two problems of socially fair regression and socially fair low rank approximation (rank $k$). By \"socially fair\", it means the objective is to minimize the maximum value of loss over given 'protected' groups. The authors show simple polynomial time algorithms for fair regression however they show hardness results for even constant factor approximation to the fair LRA(Low rank approximation) problem. Under certain assumptions, the authors show a $2^{poly(k)}$ algorithm for the fair LRA problem. The authors use sketching techniques and several well-known results from randomized numerical linear algebra to achieve this. The authors support their theoretical claims with a set of proof -of -concept empirical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Regression and Low rank approximation are fundamental problems in Machine Learning and fairness is becoming a very essential aspect   of ML. As such the problems are important and will be of interest to the community.\n2) The algorithms for fair regression are very simple and intuitive.\n3) Though, according to me, the main and most technical contribution of the paper is theoretical in nature i.e., complexity analysis of the fair LRA problem, the authors have provided small set of experiments for the fair regression problem which allows a simple algorithm. The experiments complement well with the simple algorithms.\n4) The paper, for the most part, is written clearly and appears sound (only had a high-level look at the proofs). Related work section is quite thorough."
                },
                "weaknesses": {
                    "value": "1) The paper uses many existing results from randomized numerical linear algebra and other literature for the main technical results. Although I don't consider this by itself a weakness (in fact the ideas are combined nicely), it does have the unintended effect of making the paper less accessible to readers unfamiliar with the areas. It would be helpful if the writeup following theorems 1.5 and 1.6 is modified to make it more accessible."
                },
                "questions": {
                    "value": "1. The question might sound a little basic. However, correct me if I am wrong, when you talk about the exponential time, you are only talking about time in terms of $k$ and not $n$ right? If yes please make it clearer, else it may create confusion regarding theorems 1.4 and 1.5. \n\n2. What is the run time of the algorithm for bicriteria approximation in terms of $k$?\n3. I believe here the groups are non-overlapping. Can you comment on the results were the groups overlapping?\n4. What is the significance of the $\\Delta$ in theorems 1.1. and 1.2. Is it guaranteed that the optimal $\\mathbf{x}$ will also lie in the range given by $\\Delta$?\n\nMy score is weak accept due to some of the confusion. I will be happy to raise the score if the above questions are answered satisfactoritly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768938631,
            "cdate": 1698768938631,
            "tmdate": 1699636758433,
            "mdate": 1699636758433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t5yqIG6mcM",
                "forum": "KJHUYWviZ6",
                "replyto": "7IIjPpx0q1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5W2W"
                    },
                    "comment": {
                        "value": "> The paper uses many existing results from randomized numerical linear algebra and other literature for the main technical results. Although I don't consider this by itself a weakness (in fact the ideas are combined nicely), it does have the unintended effect of making the paper less accessible to readers unfamiliar with the areas. It would be helpful if the writeup following theorems 1.5 and 1.6 is modified to make it more accessible.\n\nWe apologize that the previous version may have been inaccessible to general audiences. We have added a new Appendix A.1 that describes high-level intuition for a number of common techniques, to provide additional guiding exposition intuition for the statements following Theorems 1.5 and 1.6, which we intend to incorporate into the main body in a full version of the paper.\n\n> The question might sound a little basic. However, correct me if I am wrong, when you talk about the exponential time, you are only talking about time in terms of $k$ and not $n$ right? If yes please make it clearer, else it may create confusion regarding theorems 1.4 and 1.5.\n\nYes, we are referring to exponential time in $k$. We have updated the discussion surrounding Theorems 1.4 and 1.5 in the revised version to clarify this point.\n\n> What is the run time of the algorithm for bicriteria approximation in terms of $k$?\n\nThe runtime for the bicriteria approximation algorithm is polynomial in $n$. Since $k\\le n$, the runtime for the bicriteria approximation is also polynomial in $k$. In particular, there is a setting of the constants in the trade-offs so that the accuracy is maintained up to asymptotic factors and the runtime of the bicriteria algorithm is $O(nd^2+n^2d)$ just using naive matrix multiplication methods.  \n\n> I believe here the groups are non-overlapping. Can you comment on the results were the groups overlapping?\n\nIn general, socially fair algorithms do not seem well-defined when the groups are overlapping, i.e., if an individual can belong to multiple sub-populations. However, we agree with the reviewer that it could make sense for an individual to be able to contribute to multiple groups through some fractional weighting mechanism, which would be an interesting direction to explore. \n\n> What is the significance of the $\\Delta$ in theorems 1.1. and 1.2. Is it guaranteed that the optimal $x$ will also lie in the range given by $\\Delta$?\n\nThe significance of $\\Delta$ in Theorems 1.1 and 1.2 is the size of the space in each dimension, which is related to the size of the overall search space by our algorithm, and thus the overall runtime by our algorithm. We can choose a sufficiently large $\\Delta$ to guarantee that the optimal $x$ will lie in the search space, given known bounds on the entries of the input matrices. We note that it is standard to assume bounds on the input matrices, since otherwise, the optimal solution can be arbitrarily large or small."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291244570,
                "cdate": 1700291244570,
                "tmdate": 1700291244570,
                "mdate": 1700291244570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OwsQ3jmt5Y",
                "forum": "KJHUYWviZ6",
                "replyto": "t5yqIG6mcM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Reviewer_5W2W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Reviewer_5W2W"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for your reply. \n\nI still feel there needs to be more clarity in terms of implications of theorems 1.3,1.4 and 1.5. At present they give a slightly confusing picture to me. Currently I will keep my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547081334,
                "cdate": 1700547081334,
                "tmdate": 1700547081334,
                "mdate": 1700547081334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1wU3mofq7U",
            "forum": "KJHUYWviZ6",
            "replyto": "KJHUYWviZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_wDtZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_wDtZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzed fair regression and fair low-rank approximation theoretically with an empirical evaluation of their proposed method of fair regression. The authors show by theorems that the low-rank approximation has a much larger computation complexity than the regression under fairness requirements. They propose two algorithms for approximation solutions for fair low-rank approximation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The theoretical results for the fair low-rank approximation look novel and valuable to me. \n\nThe difference between the computation complexity of fair regression and other fair machine learning tasks (in this paper, the low-rank approximation) is interesting."
                },
                "weaknesses": {
                    "value": "1. There is no real-world application or numerical evaluation of the proposed fair low-rank approximation method, which I think is more important than the evaluation of the fair regression method shown in Section 4. \n2. There is no comparison between the proposed methods and other methods in the existing literature. For example, for fair regression, the authors may want to compare their convex solver method to Abernethy et al. (2022). \n3. The contribution to socially fair regression seems limited since the method is simple and based on the observation in Abernethy et al. (2022). \n4. This paper is not well-organized. Section 1.1 about the contributions is overwhelmingly long (about 2.5 pages), most of which is about the fair low-rank approximation which I think can be put in Section 3, especially since some parts in Section 1.1 and Section 3 are overlapped.\n\nSome typos:\n1. Page 4. 'Thus we consider the multi-response regression problem $\\min_{B^{(i)}} \\max_{i\\in[\\ell]}$ ...' I don't understand the $B^{(i)}$ since $i$ in used in the $\\max$.\n2. Theorem 1.5. $\\delta$ is not defined.\n3. Page 26. 'paragraphSynthetic dataset' looks strange to me.\n4. In Algorithm 4 Line 2. It is unclear where S is generated from."
                },
                "questions": {
                    "value": "1. For the approximate fair low-rank approximation, is it possible to have corresponding lower bounds for Theorems 1.5 and 1.6?\n2. Can we replace the probability value 2/3 in Theorems 1.5 and 1.6 with some value $\\xi$ which can be as close to 1 as possible or converge to 1 w.r.t. n? Otherwise, it seems like finding the approximate solution is still not guaranteed with high confidence.\n3. The equation $\\max \\\\|x\\\\|_\\infty = (1\\pm \\epsilon)\\\\|x\\\\|_p$ in the beginning of Section 3.2 is surprising to me. Is there a typo?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815925836,
            "cdate": 1698815925836,
            "tmdate": 1699636758324,
            "mdate": 1699636758324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n4uoDuos9J",
                "forum": "KJHUYWviZ6",
                "replyto": "1wU3mofq7U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wDtZ"
                    },
                    "comment": {
                        "value": "> There is no real-world application or numerical evaluation of the proposed fair low-rank approximation method, which I think is more important than the evaluation of the fair regression method shown in Section 4.\n\nOur lower bounds show that under a common hardness conjecture, i.e., Exponential Time Hypothesis, exponential time is required for socially fair low-rank approximation algorithms. Thus, it does not seem feasible to implement proposed algorithms on large-scale real-world datasets. On the other hand, in the appendix, we provide experiments on synthetic datasets show\n\n> There is no comparison between the proposed methods and other methods in the existing literature. For example, for fair regression, the authors may want to compare their convex solver method to Abernethy et al. (2022). The contribution to socially fair regression seems limited since the method is simple and based on the observation in Abernethy et al. (2022).\n\nYes, we remark that for socially fair regression, the main observation is that the problem is convex, similar to Abernethy et. al. (2022). Whereas Abernethy then uses a different convex solver method, we reference the interior point methods for a full end-to-end theoretical guarantee. On the other hand, for our experiments, we use the standard convex solver packages in Python. We believe empirical comparisons between the standard packages and the convex solvers of Abernethy et. al. (2022) are more a question about convex optimization benchmarks rather than regression benchmarks. \n\nIn any case, we do not view the socially fair regression as our main result, but rather a simple warm-up that in conjunction our main socially fair low-rank approximation results, exhibits a somewhat surprising juxtaposition for the complexities of the two problems, given the abundance of shared techniques often used to approach these problems. \n\n> This paper is not well-organized. Section 1.1 about the contributions is overwhelmingly long (about 2.5 pages), most of which is about the fair low-rank approximation which I think can be put in Section 3\n\nWe remark that the purpose of Section 1.1 is both to highlight our contributions but also to provide a technical overview/intuition for our results. We have renamed Section 1.1 to emphasize this intention. \n\n> some typos\n\nThanks for pointing out this typos. We have addressed these points in the revised version. \n\n> For the approximate fair low-rank approximation, is it possible to have corresponding lower bounds for Theorems 1.5 and 1.6?\n\nNote that in some sense, Theorem 1.3 and Theorem 1.4 are lower bounds that correspond to Theorems 1.5 and 1.6. Theorem 1.3 notes that any one-sided constant-factor approximation in polynomial is unlikely under the assumption that P!=NP, which motivates the study of the bicriteria approximation in Theorem 1.6. Similarly, Theorem 1.4 shows that under the stronger Exponential Time Hypothesis, runtime that is exponential in $k$ is necessary, and Theorem 1.5 achieves runtime that has exponential dependnecies on $k$ (though admittedly not linear in the exponential). We agree that it would be an interesting question to resolve this remaining gap. \n\n> Can we replace the probability value 2/3 in Theorems 1.5 and 1.6 with some value $\\xi$ which can be as close to 1 as possible or converge to 1 w.r.t. n? Otherwise, it seems like finding the approximate solution is still not guaranteed with high confidence.\n\nYes, the success probability can be boosted to an arbitrarily high $1-\\delta$ by running $O\\left(\\log\\frac{1}{\\delta}\\right)$ independent instances of the algorithm and taking the best solution. In this case, the runtime will also increase by a factor of $O\\left(\\log\\frac{1}{\\delta}\\right)$. \n\n> The equation $\\max\\|x\\|_\\infty=(1\\pm\\epsilon)\\|x\\|_p$ in the beginning of Section 3.2 is surprising to me. Is there a typo?\n\nThe maximum is somewhat redundant, but the reason that the $L_\\infty$ norm is close to the $L_p$ for large $p$ is that the $p$-th power of the large entries are significantly larger than the $p$-th power of the small entries, and thus $\\|x\\|_\\infty=(1\\pm\\epsilon)\\|x\\|_p$ for $p=O\\left(\\log\\frac{n}{\\epsilon}\\right)$, where $n$ is the dimension of $x$. See the proof of Lemma C.10 in the appendix for a formal proof."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290831384,
                "cdate": 1700290831384,
                "tmdate": 1700290831384,
                "mdate": 1700290831384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nJjYxSrnIH",
                "forum": "KJHUYWviZ6",
                "replyto": "n4uoDuos9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Reviewer_wDtZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Reviewer_wDtZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "For Lemma C.10, I found it was necessary to make the statement clear: For the condition you give, $p=O(\\frac{1}{\\epsilon}\\log \\ell)$, it is equivalent to say that there exists $C$ such that $p\\leq \\frac{C}{\\epsilon} \\log\\ell$ which means $\\log\\ell \\geq \\frac{p\\epsilon}{C}$. We know that for fixed $C$, if $\\epsilon$ is sufficiently large, $\\frac{p\\epsilon}{C} > p\\log(1+\\epsilon)$. Then, we have $\\ell^{1/p} > (1+\\epsilon)$ which contradicts your statement in Lemma C.10.\n\nTheorem 1.3 and 1.4 cannot be treated as a lower bound for Theorem 1.5 and 1.6 since they have different goals, (and the lower bound is larger than the upper bound.)\n\nI still think that it is hard to evaluate the contribution of this work based on its current results. Therefore, I will keep my rating score as it is now."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427877697,
                "cdate": 1700427877697,
                "tmdate": 1700427877697,
                "mdate": 1700427877697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EjHAHrHOvu",
                "forum": "KJHUYWviZ6",
                "replyto": "1wU3mofq7U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the feedback!\n\nNote that Section 3.2 assumes $\\varepsilon\\in(0,1)$, which crucially uses this fact in writing $\\log(1+\\varepsilon)=O(\\varepsilon)$. We have clarified this in the statement and proof of Lemma C.10.\n\nWe agree that Theorem 1.3 and Theorem 1.4 cannot be treated as pure lower bounds for Theorem 1.5 and Theorem 1.6 and we apologize for any such implications. However, we remark that Theorem 1.3 and Theorem 1.4 provides strong evidence that we cannot achieve even an approximation to socially fair low-rank approximation in runtime exponential in $k$, and thus we require additional relaxations, such as runtime exponential in $\\text{poly}(k)$, i.e., Theorem 1.5, or a bicriteria approximation, i.e., Theorem 1.6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431738703,
                "cdate": 1700431738703,
                "tmdate": 1700609945499,
                "mdate": 1700609945499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NPpsk6y80q",
            "forum": "KJHUYWviZ6",
            "replyto": "KJHUYWviZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_2ktH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_2ktH"
            ],
            "content": {
                "summary": {
                    "value": "The paper solves socially fair regression and low-rank approximation problems. It shows that fair regression can be solved up to some accuracy in polynomial time. A constant factor approximation of a fair low-rank approximation problem requires longer time that exponentially depends on k."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper shows that an additive error approximation can be achieved in poly(n,d,eps^(-1)) for Lp regression where p in (1, infinity). It also gives an nnz(A) running time algorithm that ensures a relative error approximation of the problem.\n\nIt shows that getting a constant factor low-rank approximation for the socially fair low-rank problem is np-hard. Howeve, the running time reduces drastically when the approximation factors are functions of poly(l,k)."
                },
                "weaknesses": {
                    "value": "See questions."
                },
                "questions": {
                    "value": "k-means clustering can also be viewed as constraint low-rank approximations. Can the socially fair low-rank approximation be used to solve the socially fair k-means clustering problem?\n\nNotations are not well defined and difficult to follow, eg alpha is well defined in the context where ever it has been used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699541731731,
            "cdate": 1699541731731,
            "tmdate": 1699636758222,
            "mdate": 1699636758222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ah2TL4glTU",
                "forum": "KJHUYWviZ6",
                "replyto": "NPpsk6y80q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2ktH"
                    },
                    "comment": {
                        "value": "> k-means clustering can also be viewed as constraint low-rank approximations. Can the socially fair low-rank approximation be used to solve the socially fair k-means clustering problem?\n\nAlthough the view of $k$-means clustering as a constrained low-rank approximation problem has been useful in results such as dimensionality reduction, it is not immediately clear to us how to use the socially fair low-rank approximation problem to solve the socially fair $k$-means clustering problem, in part due to the fact that the linear combination of the low-rank factors can be both fractional and negative. However, we do think that in general the relationships between $k$-means clusteirng and low-rank approximation are interesting to explore. \n\n> Notations are not well defined and difficult to follow, eg alpha is well defined in the context where ever it has been used.\n\nWe first introduce and define $\\alpha$ in the discussion after Theorem 1.5. We again introduce and define $\\alpha$ is the discussion at the beginning of Section 3.1. Finally, we define $\\alpha$ in Line 1 of Algorithm 4. Regardless, we have made a pass to clarify notations and variables, including repeating the definition of a variable if it has not been recently discussed in the paper. We hope this improves reader accessibility."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290728061,
                "cdate": 1700290728061,
                "tmdate": 1700290728061,
                "mdate": 1700290728061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8LP3bYmMSS",
            "forum": "KJHUYWviZ6",
            "replyto": "KJHUYWviZ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_gYDC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6638/Reviewer_gYDC"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies two fair learning problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has its own technical merits."
                },
                "weaknesses": {
                    "value": "There are two major flaws:\n\nFirst of all, the fair regression notion is very different from literature. The referee is afraid that this notion may not be meaningful at all. Since each group is more desired to match the prediction values rather than the losses. Please carefully go over the literature on fair regression.\n\nSecond, the low-rank approximation has not too much to do with fair regression. Piling two topics into one paper is making referee wondering that the contributions of this paper are insufficient on both ends.\n\nWith that, the referee recommends a rejection."
                },
                "questions": {
                    "value": "There are two major flaws:\n\nFirst of all, the fair regression notion is very different from literature. The referee is afraid that this notion may not be meaningful at all. Since each group is more desired to match the prediction values rather than the losses. Please carefully go over the literature on fair regression.\n\nSecond, the low-rank approximation has not too much to do with fair regression. Piling two topics into one paper is making referee wondering that the contributions of this paper are insufficient on both ends.\n\nWith that, the referee recommends a rejection."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6638/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6638/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6638/Reviewer_gYDC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6638/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699650787338,
            "cdate": 1699650787338,
            "tmdate": 1699650787338,
            "mdate": 1699650787338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lscgh6RoaR",
                "forum": "KJHUYWviZ6",
                "replyto": "8LP3bYmMSS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gYDC"
                    },
                    "comment": {
                        "value": "> First of all, the fair regression notion is very different from literature. The referee is afraid that this notion may not be meaningful at all. Since each group is more desired to match the prediction values rather than the losses. Please carefully go over the literature on fair regression.\n\nWe respectfully disagree with the notion that the fair regression notion is very different from literature. Although there are many possible definitions for fairness, the notion of social fairness (also called min-max fairness) is well-studied. For example, in Section 1.2 on related works, we refer to various literature that study social fairness on principal component analysis/low-rank approximation [STM+18,TSS+19], regression [AAM+22], and clustering [GSV21,ABV21,MV21,CMV22].\n\n[STM+18] Samira Samadi, Uthaipon Tao Tantipongpipat, Jamie Morgenstern, Mohit Singh, Santosh S. Vempala: The Price of Fair PCA: One Extra dimension. NeurIPS 2018: 10999-11010\n\n[TSS+19] Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie Morgenstern, Santosh S. Vempala: Multi-Criteria Dimensionality Reduction with Applications to Fairness. NeurIPS 2019: 15135-15145\n\n[GSV21] Mehrdad Ghadiri, Samira Samadi, Santosh S. Vempala: Socially Fair k-Means Clustering. FAccT 2021: 438-448\n\n[ABV21] Mohsen Abbasi, Aditya Bhaskara, Suresh Venkatasubramanian: Fair Clustering via Equitable Group Representations. FAccT 2021: 504-514\n\n[MV21] Yury Makarychev, Ali Vakilian: Approximation Algorithms for Socially Fair Clustering. COLT 2021: 3246-3264\n\n[CMV22] Eden Chlamt\u00e1c, Yury Makarychev, Ali Vakilian: Approximating Fair Clustering with Cascaded Norm Objectives. SODA 2022: 2664-2683\n\n[AAM+22] Jacob D. Abernethy, Pranjal Awasthi, Matth\u00e4us Kleindessner, Jamie Morgenstern, Chris Russell, Jie Zhang:\nActive Sampling for Min-Max Fairness. ICML 2022: 53-65\n\n> Second, the low-rank approximation has not too much to do with fair regression. Piling two topics into one paper is making referee wondering that the contributions of this paper are insufficient on both ends.\n\nRegression and low-rank approximation (including column subset selection) are often studied together in approximation algorithms, because there is often a set of common techniques that can be applied to both problems. For example, [Woo14] showed that the popular \"sketch-and-solve\" approach can be applied to both problems by generating a random Gaussian matrix of appropriate dimension to apply dimensionality reduction and then solving the problem in the reduced space. Similarly, it is known that the related leverage score sampling and ridge leverage score sampling [CMM17] techniques can be applied to regression and low-rank approximation respectively. Moreover, there exist online versions of these techniques for the streaming model [BDM+20,CMP20], further relating the complexity of these two problems. \n\nThus as highlighted in Section 1.1, the main contribution of this paper is an arguably surprising message that for the problem of social fairness, linear regression and low-rank approximation exhibit drastically different behaviors. We show that this is because socially fair regression can be cast as a convex optimization problem while socially fair low-rank approximation has intrinsic combinatorial properties that allow it to compute the minimal distance between a set of $n$ points and a $k$-dimensional subspace, which is known to be a \"hard\" problem. \n\n[Woo14] David P. Woodruff: Sketching as a Tool for Numerical Linear Algebra. Found. Trends Theor. Comput. Sci. 10(1-2): 1-157 (2014)\n\n[CMM17] Michael B. Cohen, Cameron Musco, Christopher Musco: Input Sparsity Time Low-rank Approximation via Ridge Leverage Score Sampling. SODA 2017: 1758-1777\n\n[CMP20] Michael B. Cohen, Cameron Musco, Jakub Pachocki: Online Row Sampling. Theory Comput. 16: 1-25 (2020)\n\n[BDM+20] Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay, David P. Woodruff, Samson Zhou: Near Optimal Linear Algebra in the Online and Sliding Window Models. FOCS 2020: 517-528"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290669073,
                "cdate": 1700290669073,
                "tmdate": 1700290669073,
                "mdate": 1700290669073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]