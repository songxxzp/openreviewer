[
    {
        "title": "PhyloGFN: Phylogenetic inference with generative flow networks"
    },
    {
        "review": {
            "id": "FCi2nnFmvy",
            "forum": "hB7SlfEmze",
            "replyto": "hB7SlfEmze",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
            ],
            "content": {
                "summary": {
                    "value": "This is a nice application of GFlowNets to the phylogeny problem in computational biology. There is a main issue that needs to be resolved, namely the performance. First the marginal likelihood comparison with other methods, are the PhyloGFN better in terms of lower bound and what is the running time required to get these results. Second, regarding the better estimates of posterior probabilities for low posterior  phylogenetic tree (which may be the main selling point for this paper), what is the gold standard and how does your running time compare with that of he gold standard and those of the other methods? Running time doesn't have to be measured as wall-clock or using a formal analysis, but could be based on the number of so-called Felsenstein peeling operations (or whatever that make it likely that this approach may with sufficient effort yield a efficient method in the future, if that isn't already the case). The proper resolution of these issues should make this become a good contribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strength is really the novelty and that this method both obtains good performance and is not limited to a predefined set of trees."
                },
                "weaknesses": {
                    "value": "It is not really trying to explain the GFN approach to the reader, but rather enumerating required items. The running time and performance in general require further explanation. The parsimony case is clearly less interesting and could partially be move to the appendix."
                },
                "questions": {
                    "value": "high complexity of tree space\n\n\u25a0 Page 1\nIt is large! complex is less clear.  \n \n\n \n\nPhyloGFN is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods\n\n\u25a0 Page 1\n\n \n\nI don\u2019t know, but the better fit for lower part of the posterior is worth mentioning. It may be key.\n\n \n\n \n\ncontinuous vari- ables that capture the level of sequence divergence along each branch of the tree.\n\n\u25a0 Page 1\n\n \n\nAlso the ml version has those.\n\n \n\n \n\nComing from the intersection of variational inference and reinforcement learning is\n\n\u25a0 Page 1\n\n \n\nReformulate.\n\n \n\n \n\nPhyloGFN leverages a novel tree representation inspired by Fitch and Felsenstein\u2019s algorithms to represent rooted trees without introducing additional learnable parameters\n\n\u25a0 Page 2\n\n \n\nMake this clearer. In particular, in addition to what?\n\n \n\n \n\nexplores\n\n\u25a0 Page 2\n\n \n\nNo it has capacity or potential to do this.\n\n \n\n \n\nstate-of-the-art MCMC\n\n\u25a0 Page 2\n\n \n\nThe comparison must be made relative to resources since you use mcmc as gold standard\n\n \n\n \n\nWith their theoretical foundations laid out in Bengio et al. (2023);\n\n\u25a0 Page 2\n\n \n\nIs this the gfn sota? Then point out this fact.\n\n \n\n \n\nThe tree topology can be either a rooted binary tree or a bifurcating unrooted tree.\n\n\u25a0 Page 2\n\n \n\nThis is a potential weakness. Can you restrict you method to binary trees? Does this mean that the posterior support of a subsplit consist of both binary and multifurcating trees? On an earlier reading i got the impression that you considered binary trees. Please clarify.\n\n \n\n \n\nThis equation can be used to recursively compute \ud835\udc43(\ud835\udc3f\ud835\udc56 \ud835\udc62|\ud835\udc4e\ud835\udc56 \ud835\udc62) at all nodes of the tree and sites \ud835\udc56. The algorithm performs a post-order traversal of the tree,\n\n\u25a0 Page 3\n\n \n\nThis is called dynamic programming\n\n \n\n \n\nrecursively computed by (1)\n\n\u25a0 Page 3\n\n \n\nAgain DP\n\n \n\n \n\nThe algorithm traverses the tree two times, first in post-order (bottom-up) to calculate the character set at each node, then in pre- order (top-down) to assign optimal sequences.\n\n\u25a0 Page 4\n\n \n\nAlso this is DP\n\n \n\n \n\nGenerative flow networks (GFlowNets) are algorithms for learning generative models of complex distributions given by unnormalized density functions over structured spaces. Here, we give a con- cise summary of the the GFlowNet framework.\n\n\u25a0 Page 4\n\n \n\nThis section could provide more insight\n\n \n\n \n\nThe direct optimization of \ud835\udc43\ud835\udc39\u2019s parameters \ud835\udf03 is impossible since it involves an intractable sum over all complete trajectories.\n\n\u25a0 Page 4\n\n \n\nThis argument is incorrect. With polynomial length trajectories, which you need, exponentially many trajectories implies exponentially many states. The ladder also yields your optimization infeasible, in worst case.\n\n \n\n \n\nEach action chooses a pair of trees and join them at the root, thus creating a new tree\n\n\u25a0 Page 5\n\n \n\nThis suggests binary trees. \u201cAt the root\u201d is a poor formulation.\n\n \n\n \n\nhus, a state \ud835\udc60 consists of a set of rooted trees\n\n\u25a0 Page 5\n\n \n\nDisjoint rooted \u2026\n\n \n\n \n\nits two children\n\n\u25a0 Page 5\n\n \n\nThis should close the case!\n\n \n\n \n\nall of which can be reached by our sampler\n\n\u25a0 Page 5\n\n \n\nAre all probabilities always non zero ?\n\n \n\n \n\nState representation To represent a rooted tree in a non-terminal state, we compute features for each site independently by taking advantage of the Felsenstein features (\u00a73.1.1).\n\n\u25a0 Page 5\n\n \n\nCan you motivate this choice?\n\n \n\n \n\nAlthough the proposed feature representation \ud835\udf0c does not capture all the information of tree structure and leaf sequences, we show that \ud835\udf0c indeed contains sufficient informa- tion to express the optimal policy\n\n\u25a0 Page 6\n\n \n\nWhat is the optimal policy? How do you show this?\n\n \n\n \n\nProposition 1. Let \ud835\udc601 = {(\ud835\udc671, \ud835\udc4f1), (\ud835\udc672, \ud835\udc4f2) . . . (\ud835\udc67\ud835\udc59, \ud835\udc4f\ud835\udc59)} and \ud835\udc602 = {(\ud835\udc67\u2032 1, \ud835\udc4f\u2032 1), (\ud835\udc67\u2032 2, \ud835\udc4f2) . . . (\ud835\udc67\u2032 \ud835\udc59, \ud835\udc4f\ud835\udc59)} be two non-terminal states such that \ud835\udc601 \u2260 \ud835\udc602 but sharing the same features \ud835\udf0c\ud835\udc56 = \ud835\udf0c\u2032 \ud835\udc56. Let \ud835\udc82 be any sequence of actions, which applied to \ud835\udc601 and \ud835\udc602, respectively, results in full weighted trees \ud835\udc65 = (\ud835\udc67, \ud835\udc4f\ud835\udc67 ), \ud835\udc65\u2032 = (\ud835\udc67\u2032, \ud835\udc4f\u2032), with two partial trajectories \ud835\udf0f = (\ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65), \ud835\udf0f\u2032 = (\ud835\udc602 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc65\u2032). If \ud835\udc43\ud835\udc39 is the policy of an optimal GFlowNet with uniform \ud835\udc43\ud835\udc35, then \ud835\udc43\ud835\udc39 (\ud835\udf0f) = \ud835\udc43\ud835\udc39 (\ud835\udf0f\u2032\n\n\u25a0 Page 6\n\n \n\nPlease explain the importance.\n\n \n\n \n\n\u22127108.42 \u00b10.18 \u22127108.41 \u00b10.14 \u22127290.36 \u00b17.23 \u22127111.55 \u00b10.07 \u22127108.95 \u00b10.06\n\n\u25a0 Page 7\n\n \n\nAt least the second method is a lower bound and it entire interval os above yours. Isn\u2019t that better?\n\n \n\n \n\nPearson correlation of model sampling log-density and ground truth unnormalized posterior log-density for each dataset o\n\n\u25a0 Page 8\n\n \n\nHow is the ground truth obtained?\n\n \n\n \n\n589\n\n\u25a0 Page 8\n\n \n\n \n\n512\n\n\u25a0 Page 8\n\n \n\n \n\n0.624\n\n\u25a0 Page 8\n\n \n\nFails miserably\n\n \n\n \n\nBAYESIAN PHYLOGENETIC INFERENCE\n\n\u25a0 Page 8\n\n \n\nWhat is the computational resources required for these methods?\n\n \n\n \n\nThe base- lines we compare to are the MCMC-based MrBayes combined with the stepping-stone sampling tech- nique (Ronquist et al., 2012),\n\n\u25a0 Page 8\n\n \n\nHow do you obtain that and are you sure of how it is computed?\n\n \n\n \n\nTo select pairs of trees to join, we evaluate tree-pair features for every pair of trees in the state and pass these tree-pair features as input to the tree MLP to generate probability logits for all pairs of trees. The tree-pair feature for a tree pair (\ud835\udc56, \ud835\udc57) with representations \ud835\udc52\ud835\udc56, \ud835\udc60\ud835\udc57 is the concatenation of \ud835\udc52\ud835\udc56 + \ud835\udc52 \ud835\udc57 with the summary embedding of the state, i.e., the feature is [\ud835\udc52\ud835\udc60; \ud835\udc52\ud835\udc56 + \ud835\udc52 \ud835\udc57], where [\u00b7; \u00b7] denotes vector direct sum (concatenation). For a state with \ud835\udc59 trees, \ud835\udc59 2 = \ud835\udc59(\ud835\udc59\u22121) 2 such pairwise features are generated for all possible pairs\n\n\u25a0 Page 16"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6250/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6250/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6250/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698565049519,
            "cdate": 1698565049519,
            "tmdate": 1700668351136,
            "mdate": 1700668351136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8KnRYAtra1",
                "forum": "hB7SlfEmze",
                "replyto": "FCi2nnFmvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for reviewer 7zzf"
                    },
                    "comment": {
                        "value": "We appreciate your feedback and detailed comments. We may not have fully comprehended some of your questions. If this is the case, please let us know, and we will be happy to clarify.\n\n## GFlowNets introduction \n> It is not really trying to explain the GFN approach to the reader, but rather enumerating required items\n>\n>This section could provide more insight (background section on GFlowNets)\n\nSpace limitations prevent us from giving a complete introduction to GFlowNets, but we believe that our introduction is self-contained and sufficient for understanding the application to phylogenetic inference. However, we have tried to improve the presentation. We point the reader to [\"Trajectory balance...\", NeurIPS 2022] as an accessible and self-contained introduction to the GFlowNet framework as relevant to our setting.\n\n## Running time and computation resources \n> The running time and performance in general require further explanation\n>\n> The comparison must be made relative to resources since you use mcmc as gold standard\n>\n>What is the computational resources required for these methods?\n\nWe describe the computation resources for PhyloGFN and compare the running time with other benchmark algorithms in our answer to all reviewers. We hope this answers your question.\n\n## Move the parsimony analysis to appendix \n\n> The parsimony case is clearly less interesting and could partially be move to the appendix\n\nBayesian inference and parsimony analysis are two types of problems that are usually solved by different families of algorithms. We believe that the fact that PhyloGFN is robust enough to solve both of them under the same framework is an important contribution worth highlighting. In particular, the following technical contributions related to parsimony-based inference are noteworthy: \n\n- We solve the parsimony score minimization problem with a sampling approach by designing an energy distribution with the parsimony score as the energy. With sufficiently small temperature $T$, the distribution will be dominated by optimal solutions, and the learned GFlowNet can retrieve all parsimonious trees.\n- We propose a tree representation based on Fitch node features. While it has similarity to the Felsenstein-based tree features for Bayesian inference, the proof of its sufficiency for optimal policy is different. \n- We propose a temperature-conditioned PhyloGFN that takes $T$ as input and can thus trade off between diversity of tree topologies and low parsimony score at inference time.\n\n## GFlowNet SOTA\n> With their theoretical foundations laid out in Bengio et al. (2023); (Page 5)\n>\n> Is this the gfn sota? Then point out this fact.\n\nWe are not sure what you mean by this question. GFlowNets do indeed achieve state-of-the-art sampling performance for many problems, but they never been applied to phylogenetic inference. The adaptation of GFlowNets to phylogenetic inference is a major contribution of our work.\n\n## On multifurcating trees\n> The tree topology can be either a rooted binary tree or a bifurcating unrooted tree. (Page 2)\n> \n> This is a potential weakness. Can you restrict you method to binary trees? Does this mean that the posterior support of a subsplit consist of both binary and multifurcating trees? On an earlier reading i got the impression that you considered binary trees. Please clarify.\n\nWe believe there is a misunderstanding here. Like almost all approaches in phylogenetics (including all the recent work addressing the problem with machine learning), we do not consider multifurcating trees. The two types of trees considered are rooted and unrooted binary trees.\n\n## Question on trajectories/states of GFlowNets\n\n> With polynomial length trajectories, which you need, exponentially many trajectories implies exponentially many states. The ladder also yields your optimization infeasible, in worst case.\n\nWe respectfully disagree that this is a limitation. All machine learning-based phylogenetic inference algorithms learn to sample a distribution over an exponentially large space of of tree topologies, but they use the power of amortization and deep neural nets to generalize from a much smaller number of topologies seen in training.\n\nTo be precise, given $n$ sequences, there are indeed $(2n-5)!!$ tree topologies, which is superexponential. You are correct that it is unrealistic to let the model visit every single tree to learn the full distribution. This is exactly the reason we use deep neural networks to parameterize the action policy. With deep neural networks, the algorithm can learn the underlying statistical patterns of the trees and generalize to those unseen in training."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190130532,
                "cdate": 1700190130532,
                "tmdate": 1700190130532,
                "mdate": 1700190130532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gnJAhJrHIE",
                "forum": "hB7SlfEmze",
                "replyto": "FCi2nnFmvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Mr Bayes benchmark \n> The base- lines we compare to are the MCMC-based MrBayes combined with the stepping-stone sampling tech- nique (Ronquist et al., 2012) (page 8)\n> \n> How do you obtain that and are you sure of how it is computed?\n\nThank you for pointing out that our description for this benchmark result is unclear. Stepping stone is the MCMC-based sampling algorithm to generate phylogenetic trees and estimate MLL scores. MrBayes is the software that implements the stepping stone algorithm.  In table 1, the MR-Bayes results for MLL estimation are obtained by using MrBayes to run the Stepping stone algorithm.  We will update the manuscript to resolve this confusion.\n\nIn table 1, the displayed results are the MrBayes results recorded in GeoPhy, VBPI-GNN.  To perform the computation, they follow the same procedure described in  Zhang \\& Matsen IV (2018b). In appendix section F1, we provide the MrBayes script used in Zhang \\& Matsen IV (2018b). \n\n## Question on model setup \n\n> To select pairs of trees to join, we evaluate tree-pair features for every pair of trees ... (Page 16)\n\nWe didn't see a question or comment for the last section in your questions. Please let us know if you have any question on the model setup. \n\n## Question about continuous branch length variables\n\n> continuous vari- ables that capture the level of sequence divergence along each branch of the tree. (Page 1)\n>\n>Also the ml version has those.\n\nWe are not sure what you mean by this comment. We are happy to answer if you could clarify the question. \n\n\n## On rooted/unrooted trees\n\n> Each action chooses a pair of trees and join them at the root, thus creating a new tree (page 5)\n>\n> This suggests binary trees.  \u201cAt the root\u201d is a poor formulation.\n>\n> its two children (page 5)\n>\n> This should close the case!\n\nWe are not quite sure what you mean here, but we can revise the sentence to end \"each action chooses a pair of trees and joins their roots by a common parent node\".\n\n## Suggestions for edits\n\n> high complexity of tree space (Page 1) \n>\n> It is large! complex is less clear.\n\nThank you for pointing out this issue.  We will make the change \u201chigh complexity of tree space\u201d to \u201cextremely large tree space\u201d in the manuscript. \n\n> I don\u2019t know, but the better fit for lower part of the posterior is worth mentioning. It may be key.\n\nWe agree with you. We will update the manuscript to highlight the significance of modeling suboptimal trees.\n\n> Coming from the intersection of variational inference and reinforcement learning is\n> Reformulate.\n\nWe will reformulate this to make it more clear.\n\n> This is called dynamic programming (for Felsenstein's algorithm)\n\nYes, the recursive computation in Felsenstein's algorithm is an instance of dynamic programming. We can use the term \"dynamic programming\" if the text if you find it suitable.\n\n>a state $s$ consists of a set of rooted trees\n>\n>  Disjoint rooted \u2026\n\nIndeed, $s$ is a disjoint set of rooted trees (that is, a forest). We will update the manuscript accordingly to include the word \"disjoint\".\n\n> PhyloGFN explores and samples from the entire phylogenetic tree space, achieving a balance between exploration in this vast space and high-fidelity modeling of the modes. (Page 2)\n>\n> No it has capacity or potential to do this.\n\nThanks for the suggestion, we will update the manuscript to \"PhyloGFN has the capacity to explore ...\".\n\n> PhyloGFN leverages a novel tree representation inspired by Fitch and Felsenstein\u2019s algorithms to represent rooted trees without introducing additional learnable parameters (Page 2)\n>\n> Make this clearer. In particular, in addition to what? \n\nThanks for pointing this out. We will update the manuscript to \"without introducing additional learnable parameters to the model\" .\n\nWe hope we have addressed all of your concerns. Please let us know if something else needs further clarification or if we missed something."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190531358,
                "cdate": 1700190531358,
                "tmdate": 1700222771507,
                "mdate": 1700222771507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tPdNkZ22Uk",
                "forum": "hB7SlfEmze",
                "replyto": "FCi2nnFmvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 7zzf,\n\nWe have responded to your questions and comments above. Could you please let us know if you have any further questions before the end of the rebuttal period and if they have affected your assessment of the paper?\n\nWe have also just posted a revised pdf. Please see the comment to all reviewers for details of the changes.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525227190,
                "cdate": 1700525227190,
                "tmdate": 1700525227190,
                "mdate": 1700525227190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kv3gNl8XwF",
                "forum": "hB7SlfEmze",
                "replyto": "tPdNkZ22Uk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
                ],
                "content": {
                    "title": {
                        "value": "Mr Bayes and the joining of trees"
                    },
                    "comment": {
                        "value": "Thanks for all the clarifications.\n\nI still haven\u2019t understood exactly what the potential of the methods is. Can you provide evidence that this method with additional optimization etc will able to compete with MCMC approaches, in particular Mr Bayes. That is, can you get better accuracy or a faster inference, or both? Again, speed doesn\u2019t have to be wall-clock but measured in some relevant way, which at least partially removes the advantage of Mr Bayes, i.e.,  being an optimized mature software. \n\nRegarding \u201cTo select pairs of trees to join, we evaluate tree-pair features for every pair of trees ... (Page 16)\u201d. The intended question is so: do you have so consider all pairs of roots among the trees? Can you give a more extensive account of the transformer, which is crucial since it allows you to, in contrast to VBPI, potentially can consider any possible tree? Also, what is it\u2019s complextity etc?\n\nI find the paper very interesting and look forward to having also these issues clarified."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572760753,
                "cdate": 1700572760753,
                "tmdate": 1700572760753,
                "mdate": 1700572760753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FrQGI03GEf",
                "forum": "hB7SlfEmze",
                "replyto": "FCi2nnFmvy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Tree topology construction and transformer model \n\n>  do you have so consider all pairs of roots among the trees?\n\nYes, given a state consisting of $N$ disjoint trees, we need to choose a pair of trees out of all $N \\choose 2$ possible pairs. We do not use any heuristic to limit the set of pairs. In this way, PhyloGFN models the entire tree space. \n\n\n> Can you give a more extensive account of the transformer, which is crucial since it allows you to, in contrast to VBPI, potentially can consider any possible tree? \n\nWe want to emphasize that the capability of modeling the entire tree space does not depend on the specific architecture of the transformer model. As we explained above, PhyloGFN can sample all possible trees because the action policy covers all possible pairs of trees at every state. In the PhyloGFN architecture, this is achieved by applying an MLP to each pair of condensed tree features, yielding logits for all possible actions of joining two trees by a common parent node. The transformer encoder is utilized to generate condensed feature representations of trees. While a transformer architecture is suitable for our input, which consists of a set of disjoint trees, it is not the determining factor enabling PhyloGFN to model the entire tree space. This is also why we relegated the architecture details to the appendix in our paper.\n\nThat being said, we are happy to provide more details regarding the transformer architecture itself: the hyperparameters used for the transformer encoder are recorded in Table S3 in the appendix. For all datasets, the transformer encoder consists of 6 self-attention blocks with 4 heads in each block. The feature embedding size is 128 and the activation function is GELU. Each self-attention block's layout is similar to the self-attention block of the ViT model (Dosovitskiy et al., 2020). It consists of a layer normalization, a self-attention layer, another layer normalization, and finally an MLP block. For a problem with $N$ input sequences, the time complexity of an $L$-layer transformer is $O(N^2 L)$.\n\n\nWe hope that our responses have addressed your questions and concerns sufficiently. Please let us know if we can provide any more clarifications."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619076696,
                "cdate": 1700619076696,
                "tmdate": 1700619166512,
                "mdate": 1700619166512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JA7WmBsTVR",
                "forum": "hB7SlfEmze",
                "replyto": "Ci9RoR8S3T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_7zzf"
                ],
                "content": {
                    "title": {
                        "value": "Not convinced but believe in the paper's potential"
                    },
                    "comment": {
                        "value": "I'm not convinced by the division of the running time into training and sampling. In most situations, it is the overall time that matters. Nevertheless, I believe in the paper's potential and will raise my score."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668286542,
                "cdate": 1700668286542,
                "tmdate": 1700668286542,
                "mdate": 1700668286542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VoVjvYS6NR",
            "forum": "hB7SlfEmze",
            "replyto": "hB7SlfEmze",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_SMRb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_SMRb"
            ],
            "content": {
                "summary": {
                    "value": "The paper develops a method for inferring phylogenetic trees, i.e. graphical representations of the evolutionary relationships of species, using Generative Flow Networks (GFlowNets). The GFlowNet treats the tree building processes as a reinforcement learning problem, where the action set corresponds to joining the roots of the subtrees existing in the pool. The paper uses a transformer architecture to encode the input states that correspond to a set of features extracted from subtree structures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Both the proposed model architecture and the use of GFlowNets for the chosen application are novel and interesting.\n\n * The paper conducts a large body of well-planned experiments, compares against a properly chosen set of baselines and reports results favorable to the proposed method. \n\n * The fact that the comparison is not made against many alternative methods is understandable, as there probably are not many modern machine learning methods addressing the same problem.\n\n* The design choices used in the model architecture are well justified, for instance the one given at the end of Page 6 for the transformer makes perfect sense."
                },
                "weaknesses": {
                    "value": "* The biggest weakness looks to me like the results in Table 1. The log-likelihood scores look very similar to each other. For instance -7108.95 for PhyloGFN and 7108.95 VBPI-GNN. Similar for other data sets.\n\n * It is great that the paper makes lots of effort to ensure the reproducibility of the results. However, It looks to me like the main paper lacks some essential details about the experiments, making it a bit hard for the reader to evaluate the results. See my question below. \n\n * Likewise, the paper would be more readable by the machine learning community if the used tree-level features are explained. As far as I was able to detect, the main paper mentions only that they are Fitch and Felsenstein features, which may be obvious to an evolutionary biologist but they do not tell anything to me as a machine learning researcher. Now that this is not a biology journal but a machine learning research venue, an introduction to such basic concepts somewhere in the paper could be beneficial.\n\n * The paper reports log-likelihood results, which measures model fit. It measures parsimony, which appears to be about the computational aspect. The log densities reported in Figure 2 is an indicator of diversity. It may be better to have a more direct and interpretable score of the discovery performance, for instance prediction accuracy of evolution tree links or ancestral relationship detection between pairs of species in cases where there is agreement on the ground truth. The current result landscape looks a bit too exploratory. While the results make intuitive sense, they still leave many gray areas in their detailed interpretation."
                },
                "questions": {
                    "value": "Are the \\pm values given in Table 1 standard deviation or standard error? What are these standard deviations/error over? Experiment repetitions? If standard deviation across repetitions, the results may be good enough. They may be alarming otherwise.\n\nOverall this is an interesting piece of work with decent potential for impact. I also give sincere value to the effort the authors make to use advanced machine learning methods for such hard applications of natural sciences. However, the paper requires some work to improve, especially in terms of presentation and clarification before being ready for publication. The case looks to me like borderline at present but has potential to improve towards an accept after the rebuttal.\n\n---\n\nThe rebuttal has addressed my concerns, so I raise my score to an accept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6250/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6250/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6250/Reviewer_SMRb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6250/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658591884,
            "cdate": 1698658591884,
            "tmdate": 1700756357614,
            "mdate": 1700756357614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FVdyWkitng",
                "forum": "hB7SlfEmze",
                "replyto": "VoVjvYS6NR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and interesting questions. We address them in turn below.\n\n## MLL estimation similar among different algorithms\n\nWhile VBPI-GNN demonstrates MLL performance closely aligned with the state-of-the-art MrBayes, it is essential to re-emphasize two significant issues with VBPI-GNN:\n\n1. The algorithm requires a pre-generated high-quality tree set to constrain the action space for tree construction. The performance of VBPI-GNN is highly contingent on the quality of this pre-generated tree set.\n\n2. Due to the limitations on the action space imposed by the pre-generated tree set, VBPI-GNN cannot model the entire phylogenetic tree space. In other words, VBPI-GNN does not support the majority of phylogenetic trees. We are happy to offer a more detailed explanation of how VBPI-GNN constrains its model space. (In case there is confusion, this issue stands apart from our analysis in Table 2, Figure 2, and Figure S1, which highlight VBPI-GNN's limitations in learning suboptimal trees.)\n\nPrior to August 2023,  VaiPhy was the leading VI algorithm capable of modeling the entire tree space. However, as indicated in Table 1 (column $\\phi$-CSMC), its  MLL estimation is significantly inferior to the state-of-the-art. This observation serves as a key motivation for our work: the design of a phylogenetic inference algorithm leveraging GFlowNets, with the objective of modeling the entire tree space and improving MLL estimation.\n\nPlease also see the response to all reviewers, where we discuss the importance of modeling the entirety of the full phylogenetic tree space and VBPI-GNN's inferiority to PhyloGFN in this regard.\n\n## Experiment specification in main text\n\nThe $\\pm$ are standard deviation and they are obtained from a single PhyloGFN model estimating the marginal log likelihood 10 times, using 1000 trajectories in each round. We are currently repeating the experiments for several runs to ensure significance. As described in the response to all reviewers, we are also repeating 3 sets of experiments with reduced training examples. We will update the MLL results table with new runs once we have obtained all the results and detail the setup in the text.\n\n## Explanation of tree representation is unclear\nPerhaps it was not made clear how the node representations are derived: in Section 3.1, we describe Felsenstein\u2019s algorithm and Fitch\u2019s algorithm, which compute vectors of features for each node. For a given subtree with root node $r$, these features are exactly the vectors computed by these two dynamic programming algorithms for node $r$ (used for full Bayesian and parsimony-based inference, respectively).\n\nSection 3.1 describes in detail how these two features can be calculated recursively using the features of the child nodes.  We will update the manuscript to make the reference more clear in the tree representation section. \n\n## Possible misunderstanding regarding Table 2 and Figure 2\n\nThe analysis presented in Table 2 and Figure 2 is not intended to show diversity. Recall that we have obtained similar MLL compared to VBPI-GNN (e.g., -7018.6 for VBPI-GNN and -7018.9 for PhyloGFN on DS1). However, MLL mostly measures how well the algorithms model the high-probability trees. Therefore, in Table 2 and Figure 2, we are trying to answer the question of how well the algorithms model the rest of the tree space.\n\nIn Table 2, Figure 2, and Figure S1, we show that on PhyloGFN is better at modeling the entire tree space compared to VBPI-GNN. For a given set of trees, we compare the model-learned sampling probability against their ground truth posterior density. For each data set, we performed the analysis on three sets of data with high/medium/low posterior densities. The analysis shows that while both algorithms model high-probability trees well, PhyloGFN is much better at modeling the rest of tree space.\n\nPlease also see the responses to all reviewers regarding the importance of modeling the full space."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187768369,
                "cdate": 1700187768369,
                "tmdate": 1700187768369,
                "mdate": 1700187768369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "asDrNyQFo6",
                "forum": "hB7SlfEmze",
                "replyto": "VoVjvYS6NR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer SMRb,\n\nWe have responded to your questions and comments above. Could you please let us know if you have any further questions before the end of the rebuttal period and if they have affected your assessment of the paper?\n\nWe have also just posted a revised pdf. Please see the comment to all reviewers for details of the changes.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525192697,
                "cdate": 1700525192697,
                "tmdate": 1700525192697,
                "mdate": 1700525192697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j0ZIy50xje",
                "forum": "hB7SlfEmze",
                "replyto": "VoVjvYS6NR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer SMRb,\n\nWe have completed the repeated experiments for all datasets and we have updated the manuscript. We believed we have also addressed your questions and concerns in our previous responses. \n\nCould you please let us know if you have any further questions before the end of the rebuttal period and if they have affected your assessment of the paper?\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675062782,
                "cdate": 1700675062782,
                "tmdate": 1700675062782,
                "mdate": 1700675062782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8BzFnC0cwW",
            "forum": "hB7SlfEmze",
            "replyto": "hB7SlfEmze",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_iXKc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_iXKc"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the related problems of Bayesian phylogenetic inference and maximum parsimony phylogenetic inference.  These problems have recently seen a lot of development in the machine learning space with a flurry of variational approaches.  The novelty in this paper is framing the problem as a Markov Decision Problem, where one needs to sequentially build a tree from the bottom up by joining the roots of rooted trees (starting with each leaf being its own rooted tree with one node).  This paper tries to learn a good policy for the MDP (i.e., a generative model for building trees) using the recently developed GFlowNets.  The paper puts some effort into finding provably good features to use when learning the policy, and showing that the optimal policy would, in fact, result in a distribution over trees equivalent to the posterior.  The authors apply their approach to a number of standard benchmarking datasets and find reasonable performance, particularly for low probability tree topologies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is clear and well-written.\n* The approach is interesting, conceptually simple, and provides a nice, efficient way to generate distributions over trees that support all of tree space (as opposed to relying on a pre-defined subset of tree space like VBPI).\n* The theoretical results and connections to Felsenstein's and Fitch's algorithms are really nice.\n* The performance across the full space of trees is promising."
                },
                "weaknesses": {
                    "value": "* I know that it is common in the Bayesian Phylogenetics field, but I am uncomfortable with using the Marginal log-likelihood (MLL) as a measure of the accuracy of the posterior.  As the authors note, taking $K \\to \\infty$ in equation (5) or the log of (S1)results in the true MLL regardless of the distribution used.  For finite $K$, both the bias and variance of the estimated MLL will depend on the learned posterior and it is incredibly difficult for me to compare methods.  For example in Table 1, on DS7, GeoPhy is bolded for having the smallest mean MLL, but its standard error is huge, which suggests to me that by some measures it may not have learned a very good posterior.  Is there a different task that could get at the quality of the posterior in a more interpretable way?\n* I also find it somewhat surprising how much VBPI-GNN outperforms PhyloGFN on the MLL estimation task.\n* Saying that VBPI-GNN is \"severely\" limited in its applicability to postulating alternative phylogenetic theories feels like far too strong of a statement, especially in light of Table 2 -- for trees with non-negligible posterior probability VBPI-GNN is quite accurate.  I agree that VBPI being unable to put mass on all of tree space is conceptually displeasing, but I don't think the presented evidence supports the claim that VBPI would result in any real-world failures of inference.\n* Are the axes flipped on Figure 2? If I understand correctly the x-axis should be the unnormalized posterior under the long run of MrBayes, and the y-axis is the unnormalized posterior for either GFN or VBPI.  Why do the points fall on different x-axis ranges for the two columns but very similar y-axis ranges?  (Similarly for Figure S1)\n* It would be nice to include some information about runtime and to include some training curves.  Are these models difficult to train?  How sensitive is training to the choice of distribution over $\\tau$?  Etc...\n\nTypos:\n* \"Given a set of observed sequence\" --> \"Given a set of observed sequences\"\n* \"Each action chooses a pair of trees and join them\" --> \"Each action chooses a pair of trees and joins them\"\n* Is it important or a typo in Proposition 1, Lemma 1, and Lemma 2 that only the branch lengths of the first tree are different?  I.e., it is $(z_1, b_1)$ and $(z_1', b_1')$ but then $(z_2, b_2)$ and $(z_2', b_2)$ and so on.  I don't see that used anywhere.\n* In the statement of Lemma 1 I believe it should be $(b(e_{uv}), b(e_{uw}))$ not $(b(e_{uv}), (b(e_{uv}))$.\n* This sentence needs substantial rewording: \"Note that $R(x) \\ne R(x') even $s_1$ and $s_2$ share the same Fitch feature because two trees can have different parsimony score when their root level Fitch feature equals.\"\n* In the proof of proposition 2, I believe that one must multiply by $\\frac{\\exp \\frac{\\sum_i M(z_i)}{T}}{\\exp \\frac{\\sum_i M(z_i)}{T}}$, not $\\frac{\\frac{\\sum_i M(z_i)}{T}}{\\frac{\\sum_i M(z_i)}{T}}$.\n* At the bottom of the first paragraph on p. 17 should it be $-\\log P(\\mathbf{Y} | z,b) P(b)$ not $-\\log P(\\mathbf{Y} | z,b) P(z)$ in order to match what is in the reward function?\n* \"ground-trueth\" --> \"ground-truth\""
                },
                "questions": {
                    "value": "* I am being a bit of a devil's advocate here, and it is a minor point, but, who cares about extremely unlikely trees?  Difference in log-likelihood on Figure 2 suggests that the trees with lowest posterior support are about $10^{-66}$ less likely than the trees with highest posterior support.  Does saying that those trees have zero probability really matter? Is there a real world use-case where knowing the posterior probability of those trees more accurately would be useful?\n* Would it be possible to use the machinery presented in this paper just for topologies in the Bayesian setting by somehow marginalizing out branch lengths at each step?  This is obviously not necessary for the present manuscript, I am just curious.\n* A preprint that was posted shortly after the ICLR deadline is highly relevant: https://arxiv.org/abs/2310.09553.  Since that was posted after the ICLR deadline, it did not influence my review of this paper.  The method in that preprint, ARTree, is based on reinforcement learning and also seeks to learn a posterior via framing a tree as the outcome of sequential tree building.  Yet, the training objectives and MDP formulation are substantially different, and so I do not see ARTree as reducing in any way the novelty of the present work.  I just bring this up in the hopes that it is interesting/helpful to the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6250/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789858974,
            "cdate": 1698789858974,
            "tmdate": 1699636683828,
            "mdate": 1699636683828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PyybsmmPwc",
                "forum": "hB7SlfEmze",
                "replyto": "8BzFnC0cwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for reviewer iXKc"
                    },
                    "comment": {
                        "value": "Thank you for feedback. We answer your questions and concerns below.\n\n## On axes in Figure 2\n\nIn Table 2 and Figure 2, we compute log sampling probability of PhyloGFN and VBPI-GNN and compare them to the ground-truth unnormalized posterior density for optimal trees and suboptimal trees. Log sampling probability (X-axis) is calculated using the algorithms in comparison (PhyloGFN and VBPI-GNN). Ground-truth unnormalized density (Y-axis) of tree $(z,b)$ is directly calculated as $P(Y,z,b) = P(Y|z,b)P(z)P(b)$. We assume a uniform distribution over tree space, hence $P(z) = \\frac{1}{(2n-5)!!}$  and $P(b) = \\prod_e P(b(e)), \\ \\ b(e) \\sim \\text{Exp}(10)$.\n\nFor the reviewer\u2019s question \"why do points fall in similar Y ranges but different X ranges\", we see two interpretations.\n\nIf we interpret it as \"comparing the VBPI-GNN plot to the corresponding PhyloGFN plot, why do the points fall in similar Y ranges but different X ranges\": to ensure fair comparison, we are using the same set of trees. As the Y-axis corresponds to the ground truth joint density, they indeed should have the same range. The X-axis labels the sampling probability learned by the two models. In the VBPI-GNN plot, the points falling in a narrower X range indicate that VBPI-GNN's sampling probabilities for these suboptimal trees are learned incorrectly: VBPI-GNN has overestimated the posterior probability of these suboptimal trees.  \n\nIf we interpret it as \"why do the X-axis and Y-axis ranges within each plot differ\", this is because the Y-axis shows the joint log-density $\\log P(Y,z,b)$, not the posterior log-density $\\log P(z,b|Y)$. For a perfectly trained model, all points in the graph would lie on a diagonal line with slope 1 and intercept equal to the MLL $\\log P(Y,z,b)-\\log P(z,b|Y)=\\log P(Y)$.\n\n## Interpretable analysis for Bayesian inference algorithms\n\nWe agree with the reviewer\u2019s comment. Using the MLL alone, it can be difficult to assess and interpret the relative performance of different algorithms, particularly when the estimated MLLs are similar or have high variance. This is precisely what motivates our quantitative and qualitative analysis that compares the learned sampling probability of PhyloGFN and VBPI-GNN to the ground truth unnormalized posterior density (Table 2, Figure 2, and Figure S1). These analyses show that:\n\n1. For all datasets except DS7, although PhyloGFN has slightly lower MLL than VBPI-GNN, both algorithms are able to model the high-probability trees well. Therefore, from a practical standpoint, PhyloGFN is at least as good as VBPI-GNN in its tree sampling capability.\n\n2. For suboptimal trees, both the correlation scores (Table 2) and scatter plots (Figure 2, Figure S1) show that PhyloGFN is actually superior to VBPI-GNN at modeling suboptimal trees.\n\n## PhyloGFN'S MLL estimation is inferior to VBPI-GNN's\nWe have identified three potential reasons for VBPI-GNN's better MLL estimation:\n\n1. PhyloGFN models branch lengths using discrete multinomial distributions. When estimating MLL, the branch lengths model is converted into a piecewise-constant continuous form, inducing a small quantization error. In contrast, VBPI-GNN models branch lengths with a continuous distribution directly.\n\n2. VBPI-GNN utilizes a set of pre-generated high-likelihood trees to restrict the search space. Since most trees with high posterior density also have high likelihood, it is easier for VBPI-GNN to explore the high posterior density region. **(In this sense, the comparison is not entirely fair, since VBPI-GNN uses information that the training of PhyloGFN does not have access to.)**\n\n3. There is room for improvement in model fitting for DS7. Both the MLL study and the study on sampling probability against ground truth indicate that PhyloGFN poorly models the high-probability trees for DS7.\n\nIn future work, we intend to address these limitations and improve PhyloGFN's MLL estimation.\n\n## Significance of modeling the entire tree space well\nPlease see the response to all reviewers, where we describe important biological questions where properly sampling suboptimal trees is important."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186890747,
                "cdate": 1700186890747,
                "tmdate": 1700186890747,
                "mdate": 1700186890747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gTJg9rN881",
                "forum": "hB7SlfEmze",
                "replyto": "5pUVHgkHvj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_iXKc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_iXKc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and for the interesting paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340132754,
                "cdate": 1700340132754,
                "tmdate": 1700340132754,
                "mdate": 1700340132754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mai0GRB9DL",
                "forum": "hB7SlfEmze",
                "replyto": "8BzFnC0cwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer iXKc,\n\nThank you for taking the effort of reviewing our paper.\n\nThe authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525148734,
                "cdate": 1700525148734,
                "tmdate": 1700525148734,
                "mdate": 1700525148734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t3wjERFqei",
            "forum": "hB7SlfEmze",
            "replyto": "hB7SlfEmze",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_tiKV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6250/Reviewer_tiKV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an application of GFlowNets for phylogenetic inference. The approach encompasses both Bayesian posterior inference and parsimony-based inference (MLE with the assumption of site independence). To address the continuous nature of branch lengths, they are transformed into discrete bins, making the phylogenetic inference problem discrete. GFlowNets are parameterized by neural networks, teaching them a policy to generate phylogenetic trees by selecting and connecting pairs of subtrees. With its probabilistic policy, the system can produce a distribution of phylogenetic trees suitable for posterior inference. In the MLE scenario, the model employs a temperature parameter that, when annealed, ensures the GFlowNet samples align with the MLE samples. The training of GFlowNets utilizes standard trajectory balance. The results suggest that the approach matches the performance of variational inference-based methods for Bayesian posterior inference, and in the MLE case, it is comparable to traditional greedy heuristic-based search algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- *Innovative Application*: Utilizing GFlowNets for phylogenetic inference is a novel idea, showcasing the versatility of GFlowNets in unique problem settings.\n- *Competitive Performance*: The approach not only matches up to VBPI-GNN-based methods in Bayesian posterior inference but offers capabilities beyond them, like generating from arbitrarily filled-in subtrees which preceding methods could not tackle.\n- *Estimating probability of suboptimal structures*: The ability to outperform VBPI methods in estimating probabilities of suboptimal structures is a notable accomplishment."
                },
                "weaknesses": {
                    "value": "- *Performance vs. Efficiency*: While GFlowNets might perform comparably to PAUP* in the parsimony-based inference setting, the real differentiator would be computational efficiency on a new inference task. Unfortunately, no wall-clock time data is provided, making it challenging to discern any advantages of GFlowNets in this scenario.\n- *Methodological Novelty*: The paper does not seem to bring forth significant machine learning methodological advancements, with much of the methodology being straightforward applications without any notable novel ML innovations for the particular problem at hand. The biggest methodological contribution is the use of discretized bins for branch lengths, which is a compromised solution.\n- *Utility of Results*: While the GFlowNets surpass VBPI methods in estimating probabilities for suboptimal structures, the practical significance of this in the context of phylogenetic applications remains unexplained. The experiments focus solely on the accuracy of probability estimates and fail to provide insights into their relevance for the broader application."
                },
                "questions": {
                    "value": "- *Time Efficiency*: Given the comparable performance of GFlowNets and PAUP*, can the authors provide information on the computational efficiency (wall-clock time) of GFlowNets to discern its advantages or disadvantages?\n- *Utility of Suboptimal Structures*: Can the authors elucidate the practical implications of having estimates for suboptimal structures in the context of phylogenetics? Is there a tangible benefit in real-world applications?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6250/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794353148,
            "cdate": 1698794353148,
            "tmdate": 1699636683712,
            "mdate": 1699636683712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WzWHyaq9XK",
                "forum": "hB7SlfEmze",
                "replyto": "t3wjERFqei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for reviewer tiKV"
                    },
                    "comment": {
                        "value": "Thank you for your feedback.  We hope the answers below clarify your concerns. \n\n## Comparing running time with PAUP\\* \n\nFor parsimony analysis, our models are trained with 25.6 million examples on all datasets, with training duration of 3-8 days. The heuristic search algorithm of PAUP\\*; is able to discover optimal trees in less than 30 CPU seconds for all datasets. This is largely due to the optimization efforts that have been put into the software over the past two decades. \n\nIn this context, it is interesting to compare the number of trees evaluated by each method. For example in DS8, PAUP*; has attempted a total of 8,113,732 tree shape rearrangements (TBR branch swapping) for finding the 21 most parsimonious trees, while PhyloGFN is trained with 25,600,000 trajectories. \n\nIt is worth bearing in mind that, in contrast to PAUP*, (1) **Bayesian phylogenetic inference algorithms, such as PhyloGFN, tackle the far more challenging problem** of sampling from a distribution over the entire phylogenetic tree space where the most parsimonious trees are the global minima, and (2) the amortization technique allows us to control the distribution of tree samples using a single model conditioned on the temperature variable.\n\nDespite all models undergoing training with 25.6 million trajectories, they exhibit the capability to produce parsimonious trees at significantly earlier stages in the training process. At intervals of every 5\\% of the training process for each model, we generate 10,000 samples and evaluate the parsimony scores of the sampled trees. This table shows the percentage of training time required for each model to sample at least one of the most-parsimonious trees:\n\n| DS1 | DS2 | DS3 | DS4 | DS5 | DS6 | DS7 | DS8 |\n|-----|-----|-----|-----|-----|-----|-----|-----|\n| 15\\% | 20\\% | 20\\% | 45\\% | 65\\% | 75\\% | 45\\% | 45\\% |\n\nWe haven't fine-tuned the training setup specifically for fast parsimonious tree retrieval. By training with a more aggressive temperature annealing schedule, integrating an early stopping mechanism, and various code and hardware optimizations, it could become feasible to significantly reduce the number of training examples needed to generate the most parsimonious trees. However, we emphasize again that the goal of PhyloGFN and other Bayesian phylogenetic inference algorithms is to model the full distribution over trees, not simply to find the most parsimonious ones.\n\nFor running time for Bayesian inference and comparison with other variational inference algorithms, please refer to the response to all reviewers.\n\n## Utility of suboptimal structures\nThis is a good question; please refer to our answer to all reviewers."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186551256,
                "cdate": 1700186551256,
                "tmdate": 1700186551256,
                "mdate": 1700186551256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "notgzKScBE",
                "forum": "hB7SlfEmze",
                "replyto": "t3wjERFqei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer tiKV,\n\nWe have responded to your questions and comments above. Could you please let us know if you have any further questions before the end of the rebuttal period and if they have affected your assessment of the paper?\n\nWe have also just posted a revised pdf. Please see the comment to all reviewers for details of the changes.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525078307,
                "cdate": 1700525078307,
                "tmdate": 1700525078307,
                "mdate": 1700525078307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4ebFqaXA1V",
                "forum": "hB7SlfEmze",
                "replyto": "t3wjERFqei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_tiKV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_tiKV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \"PAUP*; is able to discover optimal trees in less than 30 CPU seconds for all datasets.\" It seems much faster than training a GFN. In this case, the best utility of a trained GFN might be zero-shot learning on a new inference task. Have you tried that and if so, how dos the test inference time compare with PAUP*?\n\nOn modeling the distribution of suboptimal trees, if this is what PhyloGFN is better at compared with MCMC and VI, i.e. provide probability estimate or identify substructures, I really hope to see experiments that illustrate this in experiments. Especially given that this is a ML-application paper, I would hope to see how this proposed method unlock interesting applications that previous methods fail to achieve."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527862650,
                "cdate": 1700527862650,
                "tmdate": 1700527876856,
                "mdate": 1700527876856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xwtiVFQXAl",
                "forum": "hB7SlfEmze",
                "replyto": "t3wjERFqei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer tiKV,\n\nWe have updated the manuscript to add more references delineating the importance of sampling suboptimal trees (section appendix H). We thank you again for suggesting interesting future work directions. We believed we have addressed your questions and concerns in our previous responses.\n\nCould you please let us know if you have any further questions before the end of the rebuttal period and if they have affected your assessment of the paper?\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675324524,
                "cdate": 1700675324524,
                "tmdate": 1700675383083,
                "mdate": 1700675383083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uMzCemRCyy",
                "forum": "hB7SlfEmze",
                "replyto": "t3wjERFqei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_tiKV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Reviewer_tiKV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. I fully agree with the importance of sampling suboptimal trees and believe this is what the model should focus on. I also agree that current experiments show that GFN is promising in sampling from the tree distribution. But just that I wish to see something more interesting that can be achieved with GFN (in the tree sampling setting) beyond log-likelihood since it is an application paper. The time comparison with MCMC is helpful in understanding how GFN trains. I don't have further questions at the moment. My current standpoint is neural towards accepting so I will not change my score, although 5.5 better reflects where I stand."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687252798,
                "cdate": 1700687252798,
                "tmdate": 1700687310286,
                "mdate": 1700687310286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YChIi6UFgM",
                "forum": "hB7SlfEmze",
                "replyto": "t3wjERFqei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6250/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer tiKV,\n\nThank you for your effort in reviewing our work and so thoroughly justifying your assessment. We respect your decision. \n\nNevertheless, we still want to re-emphasize the following, not to convince you to change the score, but to clear up any possible misunderstandings for you or other reviewers: while modeling suboptimal trees well is a highlight for PhyloGFN, it is only one consequence of our powerful approach to Bayesian inference via sequential construction, rather than the main focus. To elaborate:\n\n- Our improved modeling of the entire tree space without the need for pre-generated trees is an important contribution. Modeling the entire tree space is not quite the same as \"modeling well suboptimal trees\" or \"generating from arbitrarily filled-in subtrees\" as you described in your original comment. Without prior knowledge (for example, pre-generated trees), the algorithm would have no idea what trees are suboptimal or arbitrary unless it can properly explore and model the tree space. \n\n- Modeling the full tree space faithfully is perhaps the **primary** issue that researchers in Bayesian phylogenetics are currently addressing: the technical challenges intensify across multiple scales when the algorithm does not depend on pre-generated tree sets to constrain the model space. Past and concurrent works in the last two years (VaiPhy, GeoPhy, and the concurrent ARTree) have proposed innovative solutions to tackle these challenges, and our current work builds and improves upon this line of research, showing for the first time that deep reinforcement learning approaches bring unique advantages to this task.\n\nThank you for your consideration.\n\nThe authors"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6250/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700501134,
                "cdate": 1700700501134,
                "tmdate": 1700700561613,
                "mdate": 1700700561613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]