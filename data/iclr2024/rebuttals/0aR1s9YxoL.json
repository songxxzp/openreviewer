[
    {
        "title": "Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages"
    },
    {
        "review": {
            "id": "89rMrft0Hi",
            "forum": "0aR1s9YxoL",
            "replyto": "0aR1s9YxoL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the plasticity loss problem in visual reinforcement learning. By using Fraction of Active Units (FAU) as a metric to measure the plasticity loss, several observations are concluded:\n- Data augmentation is critical.\n- The critic part in the actor-critic framework suffers from the plasticity loss most.\n- Plasticity loss in early states is irrevocable.\n\nThe authors also propose a technique to mitigate the plasticity loss issue, by using adaptive replay ratio.  Experiments are mainly conducted across 6 DMC tasks, showing the improvements of the proposed technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Nice paper writing**. The motivation, experiments, and conclusions from observations are well stated, making this paper interesting to read.\n- **Good ablations**. The careful experiments on different modules and stages are interesting, which could give some insights into Visual RL.\n- **Interesting measurement**. The select measurement (FAU) is interesting. Though similar things have been well studied in pure deep RL [1,2], this work might be an early work to study the activation units in visual-based deep RL.\n\n[1] Nikishin, Evgenii, et al. \"The primacy bias in deep reinforcement learning.\" ICML 2022.\n\n[2] Sokar, Ghada, et al. \"The dormant neuron phenomenon in deep reinforcement learning.\" ICML 2023."
                },
                "weaknesses": {
                    "value": "- **Limited insights revealed**. As the analysis in this work shows, the most critical factor for visual RL is **data augmentation (DA)**. However, this has been a very well-known factor for the community, and a quite large amount of recent works have extensively studied DA in visual RL. I think this work mainly gives a new perspective to explain the effectiveness of DA. This is certainly interesting, but considering the metric is directly from the previous works and similar measurements have been also extensively studied, applying FAU simply to a well-known critical factor does not give enough insights to the community.\n\n- **Limited technical contributions**. The only technical contribution of this work is to apply a low replay ratio and a high replay ratio in different stages. This is simple yet not effective enough to support its simplicity and also is not very related to the entire story that this work tells.\n\n- **Limited evaluations**. The experiments are only conducted on 6 DMC tasks, and therefore the diversity in domains and tasks are both limited. It could be good to see the observation and the technique are more universal, considering their simplicity.\n\nThough I like the analysis given in this work, I think this paper is not qualified for the acceptance of ICLR, with the reasons above."
                },
                "questions": {
                    "value": "See *weakness* above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698579067146,
            "cdate": 1698579067146,
            "tmdate": 1700740619799,
            "mdate": 1700740619799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fqQALz4tmN",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 1-1"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! Here are our responses:\n\nWe acknowledge your perspective that data augmentation (DA) is a well-established factor in the field of Visual Reinforcement Learning (VRL). However, the relationship between DA and plasticity loss remains under-explored, which constitutes a primary contribution of our study. This aspect has also been favorably recognized by all the other three reviewers. Thus, we respectfully submit that our study provides more nuanced insights than a mere reaffirmation of DA's effectiveness. Specifically, our investigations yield substantial insights into at least the following three key areas:\n\n---\n\n### **1. For Data Augmentation:**\n\n\ud83c\udf1f **Effectively mitigating catastrophic plasticity loss presents the most plausible explanation for the indispensability of data augmentation in achieving sample-efficient VRL, rather than merely offering a new perspective on the effectiveness of DA.**\n\nContrary to its incremental role in enhancing test performance in supervised learning tasks such as image classification, **DA plays an indispensable and decisive role in numerous visual RL tasks**. As illustrated in [1], even simple random shift transformations applied to input observations can lead to significant performance improvements in previously unsuccessful algorithm. By analogy, if DA in supervised learning boosts algorithm performance from 80 to 90 points, in VRL it's more like transforming algorithms from a baseline of 20 points to an impressive 90.\n\nDespite the DA operations in VRL being similar to those in supervised CV tasks, the explanations for DA's underlying mechanisms in CV do not readily apply to VRL scenarios. For example, DA can be considered as an a priori mechanism in tasks with distribution shifts between training and testing phases, adeptly mitigating these discrepancies. However, [2] note that all the environments from DMC employ a camera that is fixed relative to the agent\u2019s position. Hence, robustness to shifts does not appear to introduce any useful inductive bias about the underlying tasks. Additionally, previous studies have widely held that in supervised vision tasks, DA can enhance representation learning by inducing invariance or equivariance constraints [3]. However, evidence from [4] demonstrates that adding explicit alignment regularization to DA does not improve its performance, but rather, can have a detrimental effect.\n\nIn summary, to the best of our knowledge, existing research fails to adequately explain how DA fundamentally impacts VRL training, turning previously ineffective algorithms into highly sample-efficient ones. Our study introduces a novel connection between DA and plasticity within VRL training, demonstrating that DA can significantly mitigate plasticity loss in the critic module. In the absence of DA, the detrimental impact of catastrophic plasticity loss hinders the effective training of VRL agents, thus elucidating the pronounced performance disparity in VRL scenarios with or without DA and providing a newperspective on the underlying working mechanism of DA.\n\n[1] Ma G, Zhang L, Wang H, et al. Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. NeurIPS 2023.\n\n[2] Cetin E, Ball P J, Roberts S, et al. Stabilizing off-policy deep reinforcement learning from pixels. ICML 2022.\n\n[3] Wang H, Huang Z, Wu X, et al. Toward learning robust and invariant representations with alignment regularization and data augmentation. KDD 2022\n\n[4] Klee D, Walters R, Platt R. Understanding the Mechanism behind Data Augmentation\u2019s Success on Image-based RL. RLDM 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332788626,
                "cdate": 1700332788626,
                "tmdate": 1700332949340,
                "mdate": 1700332949340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h6IAZpuYRt",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **2. For Visual RL Community:**\n\n\ud83c\udf1f **We uncover that the primary obstacle to achieving sample-efficient VRL is, in fact, the catastrophic plasticity loss of the critic module, rather than the previously widely attributed poor visual representation capability of the encoder. Our detailed deconstruction of plasticity loss in VRL offers a clear and novel pathway for enhancing its sample efficiency.**\n\nGiven that the primary distinction between VRL and state-based RL lies in the concurrent optimization of task-specific policies and learning of compact state representations from high-dimensional observations, previous studies have naturally attributed the sample inefficiency in VRL primarily to **inadequate visual representation capabilities**.Recent advancements in VRL, focusing on enhancing sample efficiency, have primarily centered around the development of advanced representation learning techniques to forge more effective encoders. However, recent empirical studies [5, 6] suggest that these methods do not consistently improve training efficiency, indicating that insufficient representation may not be the primary bottleneck hindering the sample efficiency of current algorithms.\n\nAlthough recent studies increasingly acknowledge that merely enhancing visual representation no longer yields significant improvements in sample efficiency, the primary impediment to further advancements in VRL sample efficiency remains elusive. This paper, through a deconstruction of plasticity loss in VRL, identifies that **the catastrophic plasticity loss in the critic module is indeed this key bottleneck**. As `reviewer Kom1` notes, 'the paper can provide a promising potential for a new methodology to deal with plasticity loss,' our research at least suggests two viable pathways for further enhancing VRL sample efficiency: 1. Designing more effective interventions to better maintain the critic's plasticity, with data augmentation (DA) being an example of such interventions; 2. Utilizing the critic's plasticity level as a metric to dynamically adjust the algorithmic design of RL training, as exemplified by Adaptive Reward Reshaping (ARR).\n\n[5] Li X, Shang J, Das S, et al. Does self-supervised learning really improve reinforcement learning from pixels?. NeurIPS 2022.\n\n[6] Hansen N, Yuan Z, Ze Y, et al. On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline. ICML 2023."
                    },
                    "title": {
                        "value": "Response to the Weakness 1-2"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332888817,
                "cdate": 1700332888817,
                "tmdate": 1700332965340,
                "mdate": 1700332965340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oyTgjfIHzf",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 2"
                    },
                    "comment": {
                        "value": "We recognize the reviewer's concerns in Weakness 2, mainly regarding the effectiveness and applicability of Adaptive RR, and its relevance to the concepts revisited in earlier sections. In our subsequent response, we'll further validate Adaptive RR's efficacy and versatility. For now, our focus is on clarifying the deep-seated link between our methods and the extensive exploration we conducted on data, modules, and training stages.\n\n\ud83c\udf1f **Our discovery that the critic\u2019s plasticity is a pivotal factor in sample efficiency suggests that the FAU of the critic module could be adaptively used to discern the current training stage.** Without a thorough investigation into how the plasticity of different modules differently impacts training, it would be challenging to precisely select the critic\u2019s FAU as a metric for dynamically guiding the adjustments in RR. In networks with an actor-critic architecture, it might seem intuitive to use the average FAU of both actor and critic as the metric for guiding RR adjustments. However, as illustrated in Figure 17 in our paper's appendix, the divergent trends of FAU in actor and critic can lead to confusion, thereby obscuring the observable trends in the agent's plasticity.\n\n\ud83c\udf1f **Our investigation into the distinct properties of plasticity across different training stages directly motivates the implementation of Adaptive RR.** Initially, a low RR is employed to prevent catastrophic plasticity loss. As training progresses and plasticity dynamics stabilize, increasing the RR can enhance reuse frequency. While the method of gradually increasing RR is simple, its effectiveness stems from our nuanced understanding of the complex issue of plasticity loss in DRL. Furthermore, our investigation provides robust empirical support for all the implementation details of the Adaptive RR approach.\n\n\ud83c\udf1f **The significant impact of DA on plasticity offers us a comparative experimental approach to study modules and stages.** Section 3 of our paper not only reveals DA as an effective intervention to mitigate plasticity loss, but also the pronounced plasticity differences caused by the presence or absence of DA provide a compelling basis for comparative analysis. This allows for a deeper exploration of how plasticity varies and evolves across different modules and training stages."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333165767,
                "cdate": 1700333165767,
                "tmdate": 1700333165767,
                "mdate": 1700333165767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kquAm2k43N",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 3"
                    },
                    "comment": {
                        "value": "### **1. Effectiveness:**\n\nThe emergence of the high RR dilemma stems from the fact that increasing the replay ratio can enhance sample efficiency by boosting data reuse frequency, yet this is often counteracted by the more severe plasticity loss that accompanies it. Adaptive RR effectively capitalizes on the sample efficiency benefits of a high RR, while skillfully circumventing the dire plasticity loss consequences. \n\nPrior to introducing Adaptive RR, the most effective method to address the high RR dilemma involved periodically resetting parts of the network while employing a high static RR. In the table below, we present a comparison between RR=2 (with Reset) and Adaptive RR across three representative tasks. Compared to high RR with Reset, Adaptive RR demonstrates superior advantages in at least three aspects: \n\n(1) **Higher sample efficiency.** While applying Resets to a high replay ratio of 2 does improve its sample efficiency, it does not match the performance levels attained by Adaptive RR.\n\n(2) **Greater computing efficiency.** Unlike high RR with Reset, which consistently maintains a high update frequency, Adaptive RR employs a lower RR in the initial stages of training, thereby enhancing computing efficiency.\n\n(3) **Enhanced safety during interaction.** As emphasized in [1], the performance collapse caused by Resets can lead to a higher likelihood of violating safety constraints during agent-environment interactions. Adaptive RR, on the other hand, effectively avoids this issue.\n\n\n|  Return after 2M Steps  |   | RR=0.5 (w/o Reset) |   | RR=2 (w/o Reset)|   |RR=2 (w/ Reset)|  | Adaptive RR |\n|:-------------|---|:------:|---|:----:|---|:------------:|---|:-----------:|\n| Cheetah Run   |   |$828\\pm59$||$793\\pm9$| |**$885\\pm20$**|   |  $880\\pm45$ |\n| Walker Run    |   |$710\\pm39$||$709\\pm7$| |  $749\\pm10$  |  |**$758\\pm12$**|\n| Quadruped Run |   |$579\\pm120$||$417\\pm110$||$511\\pm47$  |  |**$784\\pm53$**|\n\n\n[1] Kim W, Shin Y, Park J, et al. Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. NeurIPS 2023.\n\n### **Applicability:**\n\nFirstly, while our original manuscript primarily reported the experimental results from 6 DMC tasks, these tasks encompass a diverse range of challenging characteristics in **continuous control tasks**. For instance, `Finger Turn Hard` presents a typical sparse reward setting, while `Quadruped Run` features a large state-action space. The consistent performance improvement of our proposed Adaptive RR across these tasks demonstrates its broad applicability in diverse continuous control environments.\n\nFurthermore, we acknowledge the reviewer's concerns about Adaptive RR's universality. To validate its effectiveness across varied tasks, we extensively tested it in the **discrete Atari-100k tasks**. The ensuing data, as shown in the table below, demonstrate that Adaptive RR surpassed both lower and higher static RR settings in **15 of the 17 diverse tasks**. Conducted with 5 seeds per task, these findings will be elaborated in our forthcoming manuscript revision, highlighting Adaptive RR's broad applicability.\n\n\n| Task            |   | RR=0.5 |   | RR=2 |   | ARR (from 0.5 to 2) |\n|:----------------|---|:------:|---|:----:|---|:-------------------:|\n| Alien           |   |$815\\pm133$||$917\\pm132$||  **$935\\pm94$**   |\n| Amidar          |   |$114\\pm58$| |$133\\pm57$| |  **$200\\pm53$**   |\n| Assault         |   |$755\\pm119$||$579\\pm44$| |  **$823\\pm94$**   |\n| Asterix         |   |$470\\pm65$| |$442\\pm69$| |  **$519\\pm36$**   |\n| BankHeist       |   |$451\\pm114$||$91\\pm34$|  |  **$553\\pm68$**   |\n| Boxing          |   |$16\\pm9$|   |$6\\pm2$|    |  **$18\\pm5$**     |\n| Breakout        |   |**$17\\pm6$**||$13\\pm3$|  |    $16\\pm5$       |\n| ChopperCommand  |   |$1073\\pm318$||$1129\\pm114$||**$1544\\pm519$** |\n| CrazyClimber    |   |$18108\\pm2336$||$17193\\pm3883$||**$22986\\pm2265$**|\n| DemonAttack     |   |$1993\\pm678$||$1125\\pm191$||**$2098\\pm491$** |\n| Enduro          |   |$128\\pm45$|  |$138\\pm36$|  |**$200\\pm32$**   |\n| Freeway         |   |$21\\pm4$|    |$20\\pm3$|    |**$23\\pm2$**     |\n| KungFuMaster    |   |$5342\\pm4521$||$8423\\pm4794$||**$12195\\pm5211$**|\n| Pong            |   |$-16\\pm3$|   |**$3\\pm10$**|   |  $-10\\pm12$  |\n| RoadRunner      |   |$6478\\pm5060$||$9430\\pm3677$||**$12424\\pm2826$**|\n| Seaquest        |   |$390\\pm79$|  |$394\\pm85$|  |**$451\\pm69$**   |\n| SpaceInvaders   |   |$388\\pm122$| |$408\\pm93$|  |**$493\\pm93$**   |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333259979,
                "cdate": 1700333259979,
                "tmdate": 1700333259979,
                "mdate": 1700333259979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pu5JU2ncKU",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for the response"
                    },
                    "comment": {
                        "value": "I acknowledge the efforts made by the authors. I think my major concern \"limited insights revealed\" is partially alleviated, regarding the new discussion. However,  I slightly disagree with the new emphasis from authors:\n> We uncover that the primary obstacle to achieving sample-efficient VRL is, in fact, the catastrophic plasticity loss of the critic module, rather than the previously widely attributed poor visual representation capability of the encoder. Our detailed deconstruction of plasticity loss in VRL offers a clear and novel pathway for enhancing its sample efficiency.\n\nI think the visual representation ability is still the primary obstacle. As cited by the authors, the paper [6] actually shows that data augmentation is the key factor not only for visual RL but also for other settings like imitation learning. I think the claim made by authors \"the primary obstacle to achieving sample-efficient VRL is, in fact, the catastrophic plasticity loss of the critic module, rather than the previously widely attributed poor visual representation capability of the encoder\" lacks enough support, since the critic part also shares the visual representation.\n\nThank the authors for their detailed discussions and additional experiments and I would raise my score accordingly. Further considering the understanding from previous papers, I would tend to not give a positive score. \n\n[6] Hansen N, Yuan Z, Ze Y, et al. On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline. ICML 2023."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565015108,
                "cdate": 1700565015108,
                "tmdate": 1700565124460,
                "mdate": 1700565124460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mXkgAH0LcU",
                "forum": "0aR1s9YxoL",
                "replyto": "Uz5HSWS2SX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank the authors for their further discussions.\n\nI appreciate the overall analysis given in the paper, which has been emphasized in my primary review. My major concern about this paper is still its actual contribution to the Visual RL community. All complex analysis finally still points out that, data augmentation, more specifically, random shift, is the critical factor.\n\nI personally like the analysis from this paper. However, it has also been stated in my primary review,\n> As the analysis in this work shows, the most critical factor for visual RL is data augmentation (DA). However, this has been a very well-known factor for the community, and a quite large amount of recent works have extensively studied DA in visual RL. I think this work mainly gives a new perspective to explain the effectiveness of DA. \n\nConsidering the effectiveness of DA has been analyzed very sufficiently in previous works [1,2,3], utilizing the tools from [4,5,6,7] to analyze this factor again is not interesting enough to me. The only extra contribution from the experiments is that DA affects the critic most, instead of the actor part. This certainly reveals some insights but is not interesting enough for acceptance. \n\n**This paper might be interesting for people who are familiar with plasticity loss since they might be unfamiliar with the common sense that DA is the critical part,  but not interesting for people familiar with visual RL due to the context.**\n\nAgain I appreciate the efforts made by authors. The storytelling and analysis in this paper are still worth reading for the community. I would maintain my score considering the reasons above however.\n\n\n[1] Ma, Guozheng, et al. \"Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning.\" arXiv preprint arXiv:2305.16379 (2023).\n\n[2] Li, Lu, et al. \"Normalization Enhances Generalization in Visual Reinforcement Learning.\" arXiv preprint arXiv:2306.00656 (2023).\n\n[3] Hansen, Nicklas, et al. \"On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline.\" arXiv preprint arXiv:2212.05749 (2022).\n\n[4] Lyle C, Zheng Z, Nikishin E, et al. Understanding plasticity in neural networks. ICML 2023.\n\n[5] Sokar G, Agarwal R, Castro P S, et al. The dormant neuron phenomenon in deep reinforcement learning. ICML 2023.\n\n[6] Lyle C, Rowland M, Dabney W. Understanding and preventing capacity loss in reinforcement learning. ICLR 2022.\n\n[7] Abbas Z, Zhao R, Modayil J, et al. Loss of plasticity in continual deep reinforcement learning. Priprint 2023."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715564118,
                "cdate": 1700715564118,
                "tmdate": 1700715564118,
                "mdate": 1700715564118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pfBZrJL8wY",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for bringing me the attention to the potential of plasticity loss"
                    },
                    "comment": {
                        "value": "It is interesting to see that another recent work DrM explores a similar thing and shows the potential of mitigating plasticity loss. I would be glad to see a direct discussion and comparison with DrM in the paper, such as the main difference, deeper connection, and further potential. **I would consider raising my score accordingly.**\n\nAlso, I hope all the related papers that authors mention during the discussion period should be included in their main paper."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735941657,
                "cdate": 1700735941657,
                "tmdate": 1700736085186,
                "mdate": 1700736085186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hQD2HRUc3B",
                "forum": "0aR1s9YxoL",
                "replyto": "89rMrft0Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ],
                "content": {
                    "title": {
                        "value": "Score raising"
                    },
                    "comment": {
                        "value": "Thank you!\n\nIt would be good to see these discussions in the updated manuscript in the future and more in-depth experiments.\n\nI have raised my score."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740560031,
                "cdate": 1700740560031,
                "tmdate": 1700740595229,
                "mdate": 1700740595229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qZSIU3HmIh",
            "forum": "0aR1s9YxoL",
            "replyto": "0aR1s9YxoL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the interaction between data augmentation, replay ratios, and dormant neurons in deep reinforcement learning. In particular, it demonstrates a striking similarity in the efficacy of data augmentation and resets in preserving plasticity in deep RL agents, and shows that data augmentation mitigates a decline in the fraction of active units in the network. It further shows that the fraction of active units can also be used to design an adaptive replay ratio scheme which tailors the replay ratio to the phase of learning in the network."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is an enjoyable read. It is very well-written and presented; claims and evidence are clear and easy to follow, and the figures are clearly explained and easy to interpret\n  - The paper offers a fresh take on plasticity -- I wouldn't have predicted that data augmentation would be such an effective regularizer, and this phenomenon is not an obvious conclusion from past literature.\n  - The study of manipulation tasks presents a nice counterpoint to prior work on plasticity which has focused largely on Atari games.\n  - I appreciated the comparisons against a variety of interventions in Figure 2, highlighting the effect size of data augmentation in contrast to other regularizers that had been studied in the literature.\n  - The work is able to ground its evaluation of plasticity not only in the performance of the agents on the RL tasks, but also using the Fraction of Active Units metric, which gives some insight into what is going on inside the network. This is useful because performance-only metrics are unable to disentangle plasticity from other factors in the RL training process such as exploration and generalization.\n  - The idea of adapting the replay ratio based on the stability of the network's training process makes a lot of sense and, despite the potential issues I highlight below with this particular instantiation, could be a generally useful quantity to keep track of.\n  - Section 5, which identifies \"critical periods\" in the training process, is particularly insightful as it suggests that regularization can be modulated throughout training"
                },
                "weaknesses": {
                    "value": "- The paper highlights the FAU as a major indicator of plasticity in the network, but glaringly omits evaluations of ReDO as a baseline. \n    - The domains studied in this paper are all from similar environments, and I'm not sure how much this paper is telling us about general properties of data augmentation, replay ratios, and plasticity, vs how much it is telling us about the interaction between these things in a specific class of robotic manipulation tasks. \n    - The causality around FAU and plasticity is quite vague in this paper. I wasn't sure whether it was claiming that the FAU is a *symptom* of plasticity loss, and that DA and ARR are affecting some hidden causal factor which reduces both plasticity loss and increases FAU, or whether the claim was that *by maintaining the FAU* these interventions are able to avoid plasticity loss.\n    - Data augmentation has the opposite on FAU in the actor and the critic, but it is not explained why a higher FAU would be beneficial in the actor but not the critic. In particular, data augmentation *dramatically* reduces the FAU in the actor, without seeming to have any ill effects. This seems to suggest that FAU might not be a particularly useful measure of plasticity for all learning problems, or at least that the story isn't as simple as that described in the paper.\n    - The specific mechanism by which data augmentation is helping to prevent plasticity loss is unclear. While it is observed to correlate with a reduction in the number of dormant neurons, it isn't obvious whether this is a causal mechanism I would like to see an evaluation of a few data augmentation classes on at least one environment to see how the choice of data augmentation influences its effect on plasticity.\n    - Minor nit: the claim that using a fixed encoder network means that the effect of plasticity on the representation is completely eliminated is too strong, as there is still some representation learning happening in the actor and critic MLPs. This should be noted in the paper text."
                },
                "questions": {
                    "value": "- The definition of FAU is a bit unclear: is it measuring average activity of neurons over a large batch, i.e. the sparsity of the network's representation, or is the expression inside the indicator function nonzero if the unit is active for *any* input, i.e. FAU =  1 - (fraction of dormant neurons)?\n- The replay ratio adaptation method is not described in sufficient detail. How specifically is the \"recovery of plasticity\" measured? How robust is the method to different variations on this measurement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632278388,
            "cdate": 1698632278388,
            "tmdate": 1699636425294,
            "mdate": 1699636425294,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CUxS4K5TcK",
                "forum": "0aR1s9YxoL",
                "replyto": "qZSIU3HmIh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate that the authors have prioritized responding to low-scoring reviewers first, but I would like to emphasize that I think this paper, with a few changes, could merit a >6 score, as I think Figure 1 alone is especially striking and worth being highlighted in a conference paper. Specifically:\n\n1. The paper really needs to evaluate against ReDO. This will also help a lot to clarify the somewhat murky causality around dormant neurons and learning dynamics.\n\n2. Including even a single non-Mujoco domain (ideally a more discrete, visually rich setting like an environment from Atari-100K or ProcGen) would go a long way towards validating the generality of the observations in this paper. It is worth noting that [Schwarzer et al.](https://arxiv.org/pdf/2305.19452.pdf) noticed significant benefits from incorporating resets, though they evaluate on an expensive domain. Perhaps MinAtar could be a cheaper alternative?\n\n3. A more mechanistic characterization of the effect of data augmentation (in lieu of additional evaluation environments) would give some indication of how environment-specific this strategy is. For example, to what extent is data augmentation mitigating overfitting/generalization issues vs network optimization pathologies?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409354599,
                "cdate": 1700409354599,
                "tmdate": 1700409354599,
                "mdate": 1700409354599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9dks9OAmLV",
                "forum": "0aR1s9YxoL",
                "replyto": "WGnK9TsaNw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the follow-up. The Atari-100K results look promising, but to properly evaluate them it would be useful to see what DrQ($\\epsilon$) scores with the default RR used by Kostrikov et al. (for example, if the best RR value is 1 then the adaptive approach might outperform the suboptimal fixed values of 0.5 and 2 but underperform the optimal fixed value). The comparison of actor and critic architectures is also interesting, and is consistent with the results of [Raileanu et al.](https://proceedings.mlr.press/v139/raileanu21a/raileanu21a.pdf), [Lyle et al.](https://arxiv.org/abs/2206.02126), and [Cobbe et al.](https://arxiv.org/abs/2009.04416), which merit discussion as related work.\n\nI look forward to seeing the ReDO results."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435888276,
                "cdate": 1700435888276,
                "tmdate": 1700435888276,
                "mdate": 1700435888276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uRxsKv42Ws",
                "forum": "0aR1s9YxoL",
                "replyto": "qZSIU3HmIh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Questions:"
                    },
                    "comment": {
                        "value": "Here we provide clarifications and explanations in response to the questions raised by the reviewer.\n\n---\n\n**Q1:** **The definition of FAU** is a bit unclear: is it measuring average activity of neurons over a large batch, i.e. the sparsity of the network's representation, or is the expression inside the indicator function nonzero if the unit is active for any input, i.e. FAU = 1 - (fraction of dormant neurons)?\n\n**A:** In our paper, the Fraction of Active Units (FAU) is defined as a metric for measuring the average activity of neurons over a large batch, in line with the approaches previously outlined in [1] and [2].  Thank you for your attention to this detail. We will further clarify this concept in the subsequent versions of our manuscript. \n\n[1] Lyle C, Zheng Z, Nikishin E, et al. Understanding plasticity in neural networks. ICML 2023.\n\n[2] Lee H, Cho H, Kim H, et al. PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. NeurIPS 2023.\n\n---\n\n**Q2:** The replay ratio adaptation method is not described in sufficient detail. How specifically is the **\"recovery of plasticity\"** measured? How robust is the method to different variations on this measurement?\n\n**A:** After initiating training, we monitor the FAU of the critic module at every check interval of $I$ steps. When the FAU difference between consecutive checkpoints falls below the minimal threshold $\\tau$, it signifies the **'recovery of plasticity'**. In both DMC and Atari environments, the minimal threshold $\\tau$ is set to $0.001$; for DMC, the interval $I$ is set to $5 \\times 10^4$ steps, and for Atari, it is set to $2 \\times 10^3$ steps. This experimental design stems from the observation that the critic's FAU rapidly decreases during the early stages of training and subsequently recovers. Once its value stabilizes at a relatively high level, the trend in FAU change becomes more gradual. Consequently, a minimal FAU difference between successive checkpoints suggests that the agent has moved beyond the early stages, which are more susceptible to catastrophic plasticity loss. At this point, we can safely increase the replay ratio to pursue higher sample efficiency.\n\nFurthermore, compared to judging the critic's FAU against a specific absolute value, our method of observing FAU trends to determine its training stage offers greater robustness. As illustrated in Figure 17 of our Appendix, setting a specific FAU value as the criterion for 'recovery' often requires adjusting the threshold for each specific task. In contrast, our approach demonstrates considerable robustness in hyperparameters. The minimal threshold $\\tau$, set at $0.001$ for both DMC and Atari environments, effectively works across various tasks. The only adjustment needed is selecting an appropriate check interval based on the total allowed steps."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508063446,
                "cdate": 1700508063446,
                "tmdate": 1700508063446,
                "mdate": 1700508063446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NZm0clqzQh",
            "forum": "0aR1s9YxoL",
            "replyto": "0aR1s9YxoL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of plasticity loss where RL agents struggle to learn from new experiences after being trained on non-stationary data. The authors suggest three key findings. Firstly, they discovered that data augmentation (DA) holds greater significance in alleviating plasticity loss compared to reset. Secondly, their extensive experiments demonstrate that the bottleneck for training lies in the plasticity loss of the critic module. Lastly, restoring the critic's plasticity to an adequate level in the early stages is an important factor. They analyzed plasticity of the neural network for the static high Replay Ratio (RR) during RL agent training and based on their findings, they introduced an adaptive replay ratio which shows superior performance compared to its static counterpart and effectively addresses the high RR dilemma."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper is well-written and easy to follow.\n\n-\tTheir suggested method, adaptive RR, is straightforward and intuitive. The results demonstrate promising potential for the proposed approach.\n\n-\tWell-established experiments demonstrated the effectiveness of DA and the importance of Critic's plasticity."
                },
                "weaknesses": {
                    "value": "-\tTheir proposed methodology, adaptive RR, is based on the analysis of the correlation between replay ratio and plasticity, which is relatively unrelated to the DA. This makes main message of this paper unclear. Are the authors emphasizing the importance of DA or are the authors emphasizing the importance of adaptive RR? The manuscript would benefit if it were to make their main message clearer by aligning their findings and their proposed method. It seems that although the findings regarding the relationship between DA and reset is intriguing, the main method is unrelated with their explanation regarding the effectiveness of DA.\n\n-\tTheir experiments on data augmentation w/ and w/o reset contradict existing research. According to Nikishin et. al. [1], they conducted a comparison between DrQ w/ reset and DrQ w/o reset, finding that the DrQ w/ reset demonstrated superiority over DrQ w/o reset (Check Figure 18 in [1]). Perhaps the results driven by the author are driven by the high variance of data augmentation w/o reset as in [1] or due to the difference in the DMC environments selected for evaluation. I have intentions to raise my score if the authors are able to show experimental results that their findings regarding the relationship between DA & reset also holds for other DMC environments. \n\n-\tIn addition, the current replay ratio values utilized for adaptive RR seems to be limited to a small range (e.g., [0.5, 1, 2]). In order to properly evaluate the efficacy of the proposed adaptive RR method, the authors should conduct further experiments for RR=4 and RR=8. Although higher RR does require longer training time and can be computationally expensive, it is essential for the authors to provide the following results in order to verify whether the proposed adaptive RR is indeed superior compared to static RR. It will also be best if the authors are able to provide results for high RR & reset & DA to further validate their findings that DA is indeed better off along and not paired with reset.\n\n-\tAs the authors note as a limitation, their experimental environment is confined to DMC, and they only demonstrate the effectiveness of Adaptive RR under basic configurations. However, it appears that the paper can provide a promising potential for a new methodology to deal with plasticity loss.\n\n[1] The Primacy Bias in Deep Reinforcement Learning, Nikshin et. al., ICML 2022"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815997188,
            "cdate": 1698815997188,
            "tmdate": 1700635594621,
            "mdate": 1700635594621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ATqxlCXQNY",
                "forum": "0aR1s9YxoL",
                "replyto": "NZm0clqzQh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 1"
                    },
                    "comment": {
                        "value": "We appreciate your thorough review and insightful feedback. We will address each of your comments and concerns below and also in our revised manuscript.\n\n---\n\n**W1:** The manuscript would benefit if it were to make their main message clearer by **aligning their findings and their proposed method**.\n\n**A:** We are grateful to the reviewer for their attention to the structure of our paper. Indeed, the development of Adaptive RR is intricately linked to our in-depth analysis of plasticity loss in VRL. \n\nThe challenge of the high RR dilemma in DRL is well-recognized, stemming from a trade-off between improved sample efficiency due to greater data reuse and the accompanying catastrophic plasticity loss. Our thorough examination of plasticity from various perspectives provided a clear path to address this issue. Every part of our paper connects together to make a complete story. Without all these parts, we wouldn\u2019t have been able to suggest Adaptive RR as a simple but effective way to deal with the high RR problem.\n\n\ud83c\udf1f **Our discovery that the critic\u2019s plasticity is a pivotal factor in sample efficiency suggests that the FAU of the critic module could be adaptively used to discern the current training stage.** Without a thorough investigation into how the plasticity of different modules differently impacts training, it would be challenging to precisely select the critic\u2019s FAU as a metric for dynamically guiding the adjustments in RR. In networks with an actor-critic architecture, it might seem intuitive to use the average FAU of both actor and critic as the metric for guiding RR adjustments. However, as illustrated in Figure 17 in our paper's appendix, the divergent trends of FAU in actor and critic can lead to confusion, thereby obscuring the observable trends in the agent's plasticity.\n\n\ud83c\udf1f **Our investigation into the distinct properties of plasticity across different training stages directly motivates the implementation of Adaptive RR.** Initially, a low RR is employed to prevent catastrophic plasticity loss. As training progresses and plasticity dynamics stabilize, increasing the RR can enhance reuse frequency. While the method of gradually increasing RR is simple, its effectiveness stems from our nuanced understanding of the complex issue of plasticity loss in DRL. Furthermore, our investigation provides robust empirical support for all the implementation details of the Adaptive RR approach.\n\n\ud83c\udf1f **The significant impact of DA on plasticity offers us a comparative experimental approach to study modules and stages.** Section 3 of our paper not only reveals DA as an effective intervention to mitigate plasticity loss, but also the pronounced plasticity differences caused by the presence or absence of DA provide a compelling basis for comparative analysis. This allows for a deeper exploration of how plasticity varies and evolves across different modules and training stages."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424681988,
                "cdate": 1700424681988,
                "tmdate": 1700424681988,
                "mdate": 1700424681988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o2SW8cUvLM",
                "forum": "0aR1s9YxoL",
                "replyto": "NZm0clqzQh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 2"
                    },
                    "comment": {
                        "value": "**W2:** ... I have intentions to raise my score if the authors are able to show experimental results that their findings regarding **the relationship between DA & reset** also holds for other DMC environments.\n\n**A:** Thank you for your interest in exploring whether incorporating Resets into DA can further enhance sample efficiency. Indeed, beyond the three tasks discussed in the main text, we have presented comparative results for eight other distinct tasks in Figures 11 and 12 of the Appendix. These tasks represent a diverse range of challenges and types within the DMC environments. Among these, only in 'Reacher Hard' did applying Resets noticeably improve the algorithm's sample efficiency and convergence performance.\n\nWe also acknowledge the differences in our experimental results compared to the original study on Resets in [1]. We believe the primary reason for these discrepancies lies in our use of DrQ-v2[2] as the baseline algorithm, as opposed to the original DrQ[3] used in [1]. \ud83c\udf1f On one hand, **DrQ-v2, with its sophisticated design enhancements, significantly improves sample efficiency and can effectively train in tasks where the original DrQ struggles.** For instance, Figure 18 in [1] indicates that DrQ fails to train effectively in tasks like Quadruped Walk, Quadruped Run, and Reacher Hard, where the introduction of Resets greatly enhances performance. In contrast, DrQ-v2 with DA already achieves high sample efficiency in these tasks, leaving little room for further improvement through Resets. \ud83c\udf1f On the other hand, **DrQ sets the critic's replay ratio (RR) at 1, unlike the basic configuration of DrQ-v2, which is RR=0.5.** As discussed in [1], higher RR leads to more severe plasticity loss, making the improvements from Resets more pronounced. In the basic configuration of DrQ-v2 (RR=0.5), our experiments indeed show that DA alone is sufficient to maintain plasticity.\n\nHowever, as RR increases, the utility of Resets becomes apparent. We conducted comparative experiments in three tasks with and without Resets at RR=2. The results in the table below consistently show that Resets can further enhance sample efficiency by recovering plasticity in high RR scenarios, aligning with the conclusions in [1].\n\n\n|Task (after 2M Steps)||RR=2 (w/DA, w/o Reset)||RR=2 (w/ DA, w/ Reset)|\n|:-------------|---|:----:|---|:------------:|\n| Cheetah Run   ||$793\\pm9$| |  $885\\pm20$  |\n| Walker Run    ||$709\\pm7$| |  $749\\pm10$  |\n| Quadruped Run ||$417\\pm110$||$511\\pm47$  |\n\n[1] The Primacy Bias in Deep Reinforcement Learning, Nikshin et. al., ICML 2022\n\n[2] Yarats D, Fergus R, Lazaric A, et al. Mastering visual continuous control: Improved data-augmented reinforcement learning. ICLR 2022.\n\n[3] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. ICLR 2021."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424723339,
                "cdate": 1700424723339,
                "tmdate": 1700424723339,
                "mdate": 1700424723339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CzEV9EjYRX",
                "forum": "0aR1s9YxoL",
                "replyto": "NZm0clqzQh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 3 and Weakness 4"
                    },
                    "comment": {
                        "value": "**W3:** ... it is essential for the authors to provide the following results in order to verify whether the proposed **adaptive RR is indeed superior compared to static RR**.\n\n**A:** To demonstrate the scalability of Adaptive RR, we evaluated its effectiveness under configurations of RR=4 and RR=8. As shown in the table below, \ud83c\udf1f the performance of Adaptive RR (from 0.5 to 8) significantly exceed that of a static RR set at 8. \ud83c\udf1f Furthermore, **Adaptive RR (from 0.5 to 4) not only surpass static high or low RR but also outperform the original ARR (from 0.5 to 2) in 2 out of 3 tasks** presented in the main text. This strongly validates the universality of Adaptive RR and its potential to achieve even greater efficacy.\n\n\n\n|Task (after 2M Steps)|   | RR=0.5 |   | RR=4 ||ARR (from 0.5 to 4) ||orignal ARR (0.5 to 2)|\n|:----------------|---|:------:|---|:----:|---|:-------------------:|---|:-----------------:|\n| Cheetah Run     |   |$828\\pm59$ ||$802\\pm7$ ||    $856\\pm51$      ||  **$880\\pm45$**  |\n| Walker Run      |   |$710\\pm39$ ||$690\\pm14$||  **$773\\pm22$**    ||  $758\\pm12$      |\n| Quadruped Run   |   |$579\\pm120$||$297\\pm88$||  **$799\\pm42$**    ||  $784\\pm53$      |\n\n|Task (after 2M Steps)|   | RR=0.5 |   | RR=8 ||ARR (from 0.5 to 8) ||orignal ARR (0.5 to 2)|\n|:----------------|---|:------:|---|:----:|---|:-------------------:|---|:---------------:|\n| Quadruped Run   |   |$579\\pm120$||$162\\pm85$||  **$439\\pm86$**    ||  **$784\\pm53$**    |\n\n---\n\n**W4:** ...their experimental environment is confined to DMC...\n\n**A:** Firstly, while our original manuscript focused on 6 DMC tasks, these encompass a wide spectrum of challenging features within **continuous control tasks**. For example, `Finger Turn Hard` is characterized by a typical sparse reward environment, whereas `Quadruped Run` has an extensive state-action space. The consistent enhancement in performance with our proposed Adaptive RR across these tasks underscores its wide-ranging suitability for various continuous control scenarios. Additionally, as we have demonstrated in our `response to W3`, Adaptive RR has the potential to scale to even higher RR values.\n\nFurthermore, in response to the reviewer's concerns regarding the universality of Adaptive RR, we conducted comprehensive evaluations on Atari-100k tasks.  The results, as detailed in the forthcoming table, indicate that Adaptive RR consistently outperforms both lower and higher static RR **in 15 of 17 discrete control tasks**. Each task underwent five random runs, and these robust findings, which will be expanded upon in our revised manuscript, strongly validate Adaptive RR's broad applicability and effectiveness.\n\n\n| Task            |   | RR=0.5 |   | RR=2 |   | ARR (from 0.5 to 2) |\n|:----------------|---|:------:|---|:----:|---|:-------------------:|\n| Alien           |   |$815\\pm133$||$917\\pm132$||  **$935\\pm94$**   |\n| Amidar          |   |$114\\pm58$| |$133\\pm57$| |  **$200\\pm53$**   |\n| Assault         |   |$755\\pm119$||$579\\pm44$| |  **$823\\pm94$**   |\n| Asterix         |   |$470\\pm65$| |$442\\pm69$| |  **$519\\pm36$**   |\n| BankHeist       |   |$451\\pm114$||$91\\pm34$|  |  **$553\\pm68$**   |\n| Boxing          |   |$16\\pm9$|   |$6\\pm2$|    |  **$18\\pm5$**     |\n| Breakout        |   |**$17\\pm6$**||$13\\pm3$|  |    $16\\pm5$       |\n| ChopperCommand  |   |$1073\\pm318$||$1129\\pm114$||**$1544\\pm519$** |\n| CrazyClimber    |   |$18108\\pm2336$||$17193\\pm3883$||**$22986\\pm2265$**|\n| DemonAttack     |   |$1993\\pm678$||$1125\\pm191$||**$2098\\pm491$** |\n| Enduro          |   |$128\\pm45$|  |$138\\pm36$|  |**$200\\pm32$**   |\n| Freeway         |   |$21\\pm4$|    |$20\\pm3$|    |**$23\\pm2$**     |\n| KungFuMaster    |   |$5342\\pm4521$||$8423\\pm4794$||**$12195\\pm5211$**|\n| Pong            |   |$-16\\pm3$|   |**$3\\pm10$**|   |  $-10\\pm12$  |\n| RoadRunner      |   |$6478\\pm5060$||$9430\\pm3677$||**$12424\\pm2826$**|\n| Seaquest        |   |$390\\pm79$|  |$394\\pm85$|  |**$451\\pm69$**   |\n| SpaceInvaders   |   |$388\\pm122$| |$408\\pm93$|  |**$493\\pm93$**   |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424770582,
                "cdate": 1700424770582,
                "tmdate": 1700424770582,
                "mdate": 1700424770582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R7WX7Sf4Dw",
                "forum": "0aR1s9YxoL",
                "replyto": "NZm0clqzQh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, I thank the authors for their extensive and insightful discussion and would like to say that most of my concerns have been resolved. However, still I have some concerns regarding the author's comments and would like to leave a few comments below. \n\nWeakness 1 : Although I agree that the thorough investigation of whether FAU of the critic module or actor module is more important and investigating how adaptive RR operates is insightful, this still doesn't seem to be related to DA. How about removing the DA section from the manuscript? Although I understand that DA is an effective approach in mitigating plasticity loss, this still doesn't seem to be related with the major findings of this manuscript. \n\nWeakness 2 : Although it is interesting that Reset works differently between DrQ and DrQ-v2, your argument that DrQ's replay ratio is higher than of DrQ-v2 seems to be wrong. Check Appendix B.3 of [1] and Appendix B of [2]. From what I understand, DrQ seems to be using an update frequency of 1 while DrQ-v2 is using an update frequency of 2, which means that from the default setting DrQ-v2's replay ratio is higher, and naturally DrQ-v2 should enjoy Reset better than DrQ does (as shown by the extra experiments for Cheetah, Walker and Quadruped Run for RR=2). So, this raises doubt regarding the rationale of the phenomenon observed in the author's manuscript. \n\n[1] Image Augmentation is All You Need: Regularizing Deep Reinforcement Learning from Pixels, Kostrikov et. al., ICLR 2021\n\n[2] Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning, Yarats et. al., arXiv 2021\n\nDespite my remaining concerns, I personally found this manuscript insightful and enjoyed reviewing this paper. Therefore, I wish to increase my score to 6. Thank you for your hard work."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635546169,
                "cdate": 1700635546169,
                "tmdate": 1700726904308,
                "mdate": 1700726904308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j7wlbRQzYR",
                "forum": "0aR1s9YxoL",
                "replyto": "NZm0clqzQh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 2: Default RR in DrQ-v2"
                    },
                    "comment": {
                        "value": "> **W2:** DrQ seems to be using an update frequency of 1 while DrQ-v2 is using an update frequency of 2, which means that from the default setting DrQ-v2's replay ratio is higher, and naturally DrQ-v2 should enjoy Reset better than DrQ does.\n\n**A:** We agree with the reviewer that the default RR in DrQ = 1.\n\n> DrQ: We perform one training update every time we receive a new observation.\n\n**However, we emphatically assert that the default RR for DrQ-v2 is indeed set at 0.5.**\n\nIn light of the reviewer's comments, we revisited the 'default set of hyper-parameters' in the DrQ-v2 original paper's appendix and identified a potential source of the misunderstanding: **`Agent update frequency = 2`**. The naming of this hyperparameter could easily lead to confusion with the Replay Ratio (RR). However, in this context, `frequency=2` actually denotes that the agent is updated every two steps. Conversely, RR is defined as the number of gradient updates per environment step. **Hence, the default RR in DrQ-v2 should indeed be calculated as `1 / Agent update frequency`, equating to 0.5.**\n\nWe further checked the [DrQ-v2 code](https://github.com/facebookresearch/drqv2) and found that `Agent update frequency = 2` is named as `update_every_steps: 2` in the code, as seen in [config.yaml#L41](https://github.com/facebookresearch/drqv2/blob/main/cfgs/config.yaml#L41). The code utilizing `update_every_steps` to adjust the update frequency appears in [drqv2.py#L230-L234](https://github.com/facebookresearch/drqv2/blob/main/drqv2.py#L230-L234). Specifically, it is implemented as follows:\n```python\ndef update(self, replay_iter, step):\n    metrics = dict()\n\n    if step % self.update_every_steps != 0:\n        return metrics\n```\nThis section of the code refers to updating the agent when `step` is an integer multiple of `update_every_steps`. The default value for `update_every_steps` is set to 2, thereby resulting in a default replay ratio of 0.5.\n\nIt's also worth noting that the adjustment of the update frequency in the original implementation does not directly allow for a RR greater than 1. For example, setting `update_every_steps` to 0.5 still results in updating the agent every step, which effectively means an RR of 1. \n\nTo accommodate higher RR settings in our experiments, we introduced an additional hyperparameter `replay_ratio`. We modified the code in [train.py#L180-L182](https://github.com/facebookresearch/drqv2/blob/main/train.py#L180-L182) to implement these higher RR settings. Our code modifications, including the introduction of the `replay_ratio` hyperparameter and adjustments to accommodate higher RR, are detailed in the `Supplementary Material`.\n\n```python\n# try to update the agent (original)\nif not seed_until_step(self.global_step):\n    metrics = self.agent.update(self.replay_iter, self.global_step)\n    self.logger.log_metrics(metrics, self.global_frame, ty='train')\n```\n>Our Revision:\n>\n>When RR>1:  set update_every_steps=1, self.agent.replay_ratio=RR;\n>\n>When RR<=1:  set update_every_steps=1/RR, self.agent.replay_ratio=1;\n```python\n# try to update the agent (ours)\nif not seed_until_step(self.global_step):\n    for _iter in range(self.agent.replay_ratio):\n        metrics = self.agent.update(self.replay_iter, self.global_step)\n    self.logger.log_metrics(metrics, self.global_frame, ty='train')\n```\n\nIn conclusion, **the default RR for DrQ-v2 is indeed set at 0.5**. Furthermore, as demonstrated in `Figure 7` of our paper, the default RR of 0.5 achieves a high level of sample efficiency compared to settings of 1, 2, and 4. This optimal efficiency is attained with the lowest computational cost. Thus, **using the default setting as a high-level baseline, it is entirely reasonable for us to compare the effectiveness of DA and Reset in mitigating plasticity loss under these conditions**.\n\nDespite this, we greatly appreciate the reviewer's suggestion to compare DA and Reset under high RR conditions, which is crucial for enhancing our manuscript. Our experiments under high RR conditions are in complete agreement with the findings in [1]. We will add these comparisons to our forthcoming revision for a more thorough and meticulous examination of DA and Reset.\n\n[1] The Primacy Bias in Deep Reinforcement Learning, Nikshin et. al., ICML 2022.\n\n---\n\nThanks again for the valuable suggestions. We look forward to further discussions with the reviewer."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667654138,
                "cdate": 1700667654138,
                "tmdate": 1700671478297,
                "mdate": 1700671478297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OzenJ56N6N",
            "forum": "0aR1s9YxoL",
            "replyto": "0aR1s9YxoL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D"
            ],
            "content": {
                "summary": {
                    "value": "This paper scrutinizes plasticity loss in visual reinforcement learning (VRL) by exploring data augmentation (DA), agent modules, and training stages. It reveals that DA is pivotal in mitigating plasticity loss, particularly within the critic module of VRL agents, which is identified as the main contributor to the loss of plasticity. To this end, the paper proposes an Adaptive Replay Ratio (RR) strategy that adjusts the RR based on the critic module's plasticity, significantly enhancing sample efficiency over static RR methods, as validated on the DeepMind Control suite."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper offers a novel perspective on the loss of plasticity in a visual reinforcement learning setup which includes:\n\n-  **Analysis of Data Augmentation:**  A significant strength of this paper is its thorough analysis of why data augmentation is so effective in reinforcement learning. By conducting extensive experiments and analysis, the paper sheds light on the underlying mechanisms through which data augmentation enhances plasticity, particularly in visual reinforcement learning settings. This nuanced understanding helps bridge the gap between empirical success and theoretical underpinnings, providing valuable insights for future algorithm design.\n-  **Module-Specific Analysis of Plasticity Loss:** Another strength is the paper's detailed dissection of plasticity loss across different agent modules, namely the encoder, actor, and critic. The authors' methodical comparison uncovers that the critic module is the primary culprit behind plasticity loss, challenging previous assumptions about the encoder's role. \n- **A simple method to tackle the plasticity loss:** By proposing a simple, adaptive method to modulate the update ratio based on plasticity metrics, the paper presents a novel approach that improves upon the static replay ratio. This innovation has the potential to enhance the sample efficiency of diverse reinforcement learning models by aligning training intensity with the model's current capacity to learn."
                },
                "weaknesses": {
                    "value": "This paper offers interesting insights and experimental results within the domain of visual reinforcement learning (VRL). Despite this, there are aspects that call into question the robustness of the findings:\n\n- **Metrics-Related Concerns:** The primary metric used to assess plasticity loss, the fraction of active units, might be inadequate. The reliance on this metric is questionable since different activation functions such as CReLU and LeakyReLU can inherently bias the number of active units. In addition, as noted by Lyle et al. [1], fraction of active units does not fully encapsulate plasticity dynamics. I recommend that the authors broaden their analysis with additional metrics\u2014like feature rank, weight norms, or even the curvature of the loss surface (e.g., the Hessian's rank)\u2014to substantiate their claims more convincingly. While no single metric may definitively quantify plasticity loss, a consistency of findings across multiple measures would strengthen the empirical foundation of their arguments.\n\n- **Analytical Concerns:** The paper claims the indispensability of data augmentation (DA) in mitigating plasticity loss, but it lacks a comprehensive analysis of why DA is superior. Other methods like layer normalization may have similar effects; what happens when these are combined, as done by Lee et al.? The paper would benefit from deeper exploration into why DA outperforms other methods. Providing a theoretical framework or empirical evidence for DA's efficacy in reducing plasticity loss would be a valuable addition to the field, potentially guiding future development of more robust VRL systems.\n\n- **Applicability Concerns:**  I recognize that calling for additional experiments might seem an unfair request, as it can be levied against any paper. However, I do believe that extending the empirical validation to include at least one more analysis would greatly benefit the paper's credibility. It would be enlightening to see how the findings translate to non-actor-critic methods or across diverse benchmarks. For instance, assessing the proposed adaptive replay ratio method on a well-established sample-efficiency benchmark such as Atari-100k, with Rainbow algorithm, would improve the practical significance of the adaptive RR method. Without such additional validation, the scope and applicability of the conclusions remain in question.\n\nI'm willing to increase the score if the authors address the outlined limitations.\n\n[1] Understanding plasticity in neural networks., Lyle et al., ICML 2023.\n\n[2] PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning., Lee et al., NeurIPS 2023."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D",
                        "ICLR.cc/2024/Conference/Submission4493/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698922624684,
            "cdate": 1698922624684,
            "tmdate": 1700572562343,
            "mdate": 1700572562343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vga7Kwkay0",
                "forum": "0aR1s9YxoL",
                "replyto": "OzenJ56N6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 1-2"
                    },
                    "comment": {
                        "value": "We are grateful for your comprehensive review and valuable feedback. We apologize for the delayed response, as considerable time was required to complete the necessary experiments. We will address each of your comments and concerns in the following responses, as well as in our revised manuscript.\n\n---\n\n**W1:** **Metrics-Related Concerns**: ... While no single metric may definitively quantify plasticity loss, a consistency of findings across **multiple measures** would strengthen the empirical foundation of their arguments.\n\n**A:** We fully concur with the reviewer's point that introducing additional metrics is essential for a more comprehensive characterization of an agent's plasticity. Beyond FAU, we have employed Weight Norm and Feature Rank as indicators to quantify plasticity. Specifically, we utilized these more comprehensive metrics to more accurately compare the characteristics of different modules and to contrast the effects of data augmentation with other interventions. The results of these experiments have been updated in `Figures 19 and 20 of the paper's appendix`. This revisitation of VRL's plasticity using a broader range of metrics has provided us with deeper insights into the distinct characteristics of the actor and critic.\n \n\ud83c\udf1f DA significantly enhances the feature rank of both the actor and critic networks, effectively preventing implicit under-parameterization [1]. Combined with previous observations that DA results in a low FAU for the actor network and a high FAU for the critic network, we can infer that **a sparse actor network still retains the necessary expressivity**. Conversely, the **critic is required to maintain ample plasticity and active states to continuously learn from the non-stationary targets presented by newly collected experiences.**\n\n[1] Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning. ICLR 2021.\n\n---\n\n**W2:** **Analytical Concerns**: ... what happens when these are combined ... Providing a theoretical framework or empirical evidence for DA's efficacy in reducing plasticity loss would be a valuable addition to the field, potentially guiding future development of more robust VRL systems.\n\n**A:** The reviewer's suggestion for a more in-depth analysis and comparison of DA with other interventions is indeed necessary. It not only aids in understanding the powerful ability of DA to mitigate plasticity loss but also assists in designing more efficient VRL systems. As demonstrated in the PLASTIC study [2], combining various effective interventions can consistently maintain plasticity across a range of complex scenarios. Due to time and computational resource constraints, we plan to conduct experiments in future manuscript versions comparing DA with other interventions when combined, as [2] has already adequately demonstrated the potential of such combinations. During this rebuttal period, we will focus our discussion on **why DA, as opposed to interventions like layer normalization, possesses a stronger capability in maintaining plasticity**. To gain a deeper understanding, we employed a variety of metrics to measure the differences in training dynamics when using various interventions. The experimental results are displayed in `Figures 20 of the paper's appendix` and will be incorporated into the main text in subsequent versions of the manuscript. Our principal findings and understandings can be summarized as follows:\n\n\ud83c\udf1f Although CReLU, by altering the activation function, forces the FAU of the critic to always be 0.5, it does not significantly enhance the critic's feature rank. This implies that while this method ensures each neuron is activated, these neurons do not possess sufficient expressivity.\n\n\ud83c\udf1f Layer Normalization (LN) fails to effectively recover the FAU of the critic in the early stages of training. This inability prevents it from continuously learning from new experiences in later stages, aligning with the trend observed in its episode return training curve. After initial effective learning, algorithms using only LN quickly reach a training plateau, maintaining suboptimal performance without further improvement.\n\n\ud83c\udf1f Employing DA effectively elevates the critic's FAU to a high level in the initial training phase, thereby avoiding catastrophic plasticity loss. This allows the agent to continuously optimize the accuracy of Q estimation through interactions with the environment. Additionally, with DA, we also achieve an actor with high expressivity. These two aspects together contribute to the effectiveness of DA in maintaining plasticity, making it an essential method for achieving sample-efficient VRL.\n\n[2] PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. NeurIPS 2023."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570002337,
                "cdate": 1700570002337,
                "tmdate": 1700570002337,
                "mdate": 1700570002337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R2Sx8zjyjJ",
                "forum": "0aR1s9YxoL",
                "replyto": "OzenJ56N6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Weakness 3"
                    },
                    "comment": {
                        "value": "**W3:** **Applicability Concerns**: ... how the findings translate to **non-actor-critic methods** or across diverse benchmarks. For instance, assessing the proposed adaptive replay ratio method on a well-established sample-efficiency benchmark such as **Atari-100k, with Rainbow algorithm**, would improve the practical significance of the adaptive RR method.\n\n**A:** We thank the reviewer for the suggestion to extend our evaluations to **non-actor-critic methods and discrete control tasks**. This is an excellent complement to our previous experiments in continuous DMC tasks. We have completed additional experiments in the Atari-100k environment, utilizing DrQ($\\epsilon$) as our baseline, which is a Rainbow DQN-based approach. Due to time and computational resource constraints, we have directly adopted the default replay ratio (RR=1) results from the [BBF](https://arxiv.org/abs/2305.19452) for all tasks, except for SpaceInvaders and Enduro. For the other experiments we conducted, each was performed with 5 random runs. In addition to experiments with three different static RRs, we also reran the ReDO experiments to serve as our baseline. The results are presented in the following table.\n\n\ud83c\udf1f **Adaptive RR outperforms other configurations, including ReDO, in 11 out of the 17 tasks.** This not only demonstrates that Adaptive RR can strike a superior trade-off between reuse frequency and plasticity loss, but also shows that dynamically adjusting the RR based on the critic's overall plasticity level (specifically referring to the Q network in Atari) enables competitive performance through effective neuron-level network parameter resetting.\n\n|Task|RR=0.5|RR=2|ARR (from 0.5 to 2)|RR=1 (optimal static RR)|ReDO (RR=1)|\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|Alien|815\u00b1133|917\u00b1132|**935\u00b194**|865.2|794\u00b147|\n|Amidar|114\u00b158|133\u00b157|**200\u00b153**|137.8|163\u00b156|\n|Assault|755\u00b1119|579\u00b144|**823\u00b194**|579.6|675\u00b157|\n|Asterix|470\u00b165|442\u00b169|519\u00b136|**763.6**|684\u00b198|\n|BankHeist|451\u00b1114|91\u00b134|**553\u00b168**|232.9|61\u00b143|\n|Boxing|16\u00b19|6\u00b12|**18\u00b15**|9.0|9\u00b15|\n|Breakout|17\u00b16|13\u00b13|16\u00b15|**19.8**|15\u00b16|\n|ChopperCommand|1073\u00b1318|1129\u00b1114|1544\u00b1519|844.6|**1650\u00b1343**|\n|CrazyClimber|18108\u00b12336|17193\u00b13883|22986\u00b12265|21539.0|**24492\u00b14641**|\n|DemonAttack|1993\u00b1678|1125\u00b1191|**2098\u00b1491**|1321.5|**2091\u00b1588**|\n|Enduro|128\u00b145|138\u00b136|200\u00b132|**223.5**|**224\u00b135**|\n|Freeway|21\u00b14|20\u00b13|**23\u00b12**|20.3|19\u00b111|\n|KungFuMaster|5342\u00b14521|8423\u00b14794|**12195\u00b15211**|11467.4|11642\u00b15459|\n|Pong|-16\u00b13|**3\u00b110**|-10\u00b112|-9.1|-6\u00b114|\n|RoadRunner|6478\u00b15060|9430\u00b13677|**12424\u00b12826**|11211.4|8606\u00b14341|\n|Seaquest|390\u00b179|394\u00b185|**451\u00b169**|352.3|292\u00b168|\n|SpaceInvaders|388\u00b1122|408\u00b193|**493\u00b193**|402.1|379\u00b187|\n\n---\n\n**We look forward to further discussions with the reviewer.**"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570047448,
                "cdate": 1700570047448,
                "tmdate": 1700570047448,
                "mdate": 1700570047448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pg16462CmB",
                "forum": "0aR1s9YxoL",
                "replyto": "R2Sx8zjyjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response and the additional experiments. The inclusion of various metrics in the supplementary section strengthens your paper, and **I've accordingly increased my score to 6**. While the expanded analysis using metrics like feature rank, active units, and weight norm is valuable, I would like to understand better how we can quantify the loss of plasticity, as these measures do not sufficiently quantify this loss.\n\nI encourage you to complete the pending experiments, as they are essential for a comprehensive understanding of the topic. Moreover, considering the quality and impact of the research, I believe this paper deserves a poster presentation at ICLR."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572544979,
                "cdate": 1700572544979,
                "tmdate": 1700572544979,
                "mdate": 1700572544979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]