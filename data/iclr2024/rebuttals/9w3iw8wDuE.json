[
    {
        "title": "Entropy is not Enough for Test-time Adaptation: From the Perspective of Disentangled Factors"
    },
    {
        "review": {
            "id": "I6CScj5bYF",
            "forum": "9w3iw8wDuE",
            "replyto": "9w3iw8wDuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
            ],
            "content": {
                "summary": {
                    "value": "TTA approaches select good samples for TTA using entropy as a metric (some of the approaches, and not all as I point in my review). However, a sample can have low entropy but using spurious features for prediction. Including such samples for TTA can be hurtful as they use spurious features for low entropy. The authors propose patch shuffle augmentation to identify such samples and remove them. \nThe paper has missing important baseline for some core experiments (Table 2 and Table 3), and it is not clear why the approach helps in WILD distribution shift (label shift, batch size 1)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n- The problem is well motivated i.e. spurious features can cause entropy to be low."
                },
                "weaknesses": {
                    "value": "Weakness:\n- Unclear what the authors mean by \u201cdisentangled latent vector\u201d exactly for each image. No proper reference has been made as well to get details. It would be better to write more explicitly how does Equation 6 arise for fluency of the reading flow. Authors mention $x^{T>>C}$, please describe this notation (and might be not even required to introduce this as it is clear authors mean second term is more pronounced).\n- It is not a great practice to include the whole of related works in the appendix. Even the related works section on TTA in appendix is extremely sparse (a single paragraph!) and should be much more broader in it\u2019s scope.\n- I understand that authors aim is to reject samples which use spurious features for prediction as doing TTA on them might be harmful. However, past theoretical work (https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf) shows that self training avoids using spurious features. \n- There are many methods which assess augmentation + entropy (see MEMO, SENTRY https://arxiv.org/pdf/2012.11460.pdf). SENTRY does sample filtering based on consistency of predictions over various augmentations. It is a crucial and missing baseline in this work, especially in Table 2 and Table 3. I know SENTRY operates in UDA setting, but the consistency over augmentations part is pretty standard. \n- The authors mention existing works use entropy for selective minimization. There should be a baseline where we do selection based on average entropy over multiple augmentations. It is a trivial extension of previous works and should have been a baseline to asses the effectiveness of pseudo-label confidence drop proposed in this work.\n- Why is the MEMO baseline missing in Table 2 and Table 3, where I would guess MEMO to be most effective as it also does entropy average over multiple transformations. It is quite possible that the average entropy over augmentations is low for spurious correlations domainted prediction, making those samples contribute much less to the loss anyways.\n- Can the authors give any reason why their proposed approach should work better under label shift or for batch size 1 as shown in Table 5? I do not see any reason why augmentation based sample selection should help for label shift TTA (makes sense for spurious TTA). Similar is the question for Batch size 1 setting. Are the authors sure they did proper hyperparameter tuning for the baselines?\n- Further, the effect of such spurious correlations can be removed by incorporating such transforms (cutmix, random cropping, or the used patch shuffling) during pretraining itself. Will the proposed approach help in those cases as well?\n\n\nOverall, the paper does consider an important question. However, misses some very important baselines. Further, some of the results on some benchmarks have been left unexplained beyond simply stating the numbers. The writing also lacks heavily, with a complete miss on the related works section.\n\n\n_______post rebuttal_____\nAuthors addressed some of my major concerns with respect to the baselines. I am increasing my rating from 5 to 6."
                },
                "questions": {
                    "value": "See the weakness section for questions to answer during rebuttal"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1097/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1097/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617142749,
            "cdate": 1698617142749,
            "tmdate": 1700690535865,
            "mdate": 1700690535865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X3kV9ql0Xl",
                "forum": "9w3iw8wDuE",
                "replyto": "I6CScj5bYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NrgV (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the importance of the problem addressed in our paper. Firstly, based on your comments, we were able to improve the readability of the paper. Secondly, leveraging the papers you introduced, we could further demonstrate the robustness and effectiveness of DeYO by incorporating additional theoretical explanations and experiments. Lastly, we agree that there was a lack of intuitive descriptions for the performance of DeYO in wild scenarios. Following your comments, we added in-depth explanations about DeYO. We are truly grateful for your meticulous guidance on our work.  \n\n> W1. Unclear what the authors mean by \u201cdisentangled latent vector\u201d exactly for each image. No proper reference has been made as well to get details. It would be better to write more explicitly how does Equation 6 arise for fluency of the reading flow. Authors mention $x^{T \\gg C}$, please describe this notation?  \n\nA: Sorry for the lack of explanation. Following the approach of Olivia et al. [1], we decompose the data into a set of independent random variables. For example, an image of a human face can be decomposed into independent variables representing features like eyes, nose, and mouth. We refer to these, potentially numerous, independent $d_v$ random variables as factors, forming a disentangled latent vector in a $d_v$-dimensional space.  \n\nIn Eq. (6), by definition, CPR factors are positively correlated with the label. Consequently, the average of CPR factors ($v_{pp}$) in samples with a label of 1 should be greater than that in samples with a label of -1. The same process applies to TRAP factors ($v_{pn}$). We have incorporated the explanation of \"$x^{T \\gg C}$\" into the manuscript as well. Thank you for the valuable feedback.\n\n> W2. It is not a great practice to include the whole of related works in the appendix. Even the related works section on TTA in appendix is extremely sparse (a single paragraph!) and should be much more broader in it\u2019s scope.  \n\nA: Given our focus on demonstrating the effectiveness and robustness of DeYO, we overlooked incorporating sufficient content on related work. Following your comment, we revised the related work section to provide a more comprehensive and in-depth overview. Due to space constraints, we have not been able to include this in the main manuscript yet. However, in the final version, we will make an effort to incorporate at least some of it into the main manuscript.\n\n> W3. I understand that authors aim is to reject samples which use spurious features for prediction as doing TTA on them might be harmful. However, past theoretical work (https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf) shows that self training avoids using spurious features.  \n\nA: Thank you for recommending the paper. We included the recommended paper in related work. However, this paper does not seem practically feasible. Self-training requires conditions like \u201cthe spurious feature which correlates with the label in the source domain does not exist in the target domain\u201d to avoid using spurious features. Moreover, the source model must be accurately trained. However, it is challenging for the real source model (pre-trained model) and target dataset to satisfy these conditions. Thus, the theoretical proof presented in the recommended paper may not be fully applicable in real-world scenarios.  \n\nAlso, we empirically confirmed that the performance of existing TTA methods that utilize self-training (entropy minimization) without considering spurious features, is significantly lower than that of DeYO, which deliberately suppresses the learning of spurious features during self-training."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147262968,
                "cdate": 1700147262968,
                "tmdate": 1700153984261,
                "mdate": 1700153984261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WsoQpPDzfY",
                "forum": "9w3iw8wDuE",
                "replyto": "I6CScj5bYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NrgV (2/3)"
                    },
                    "comment": {
                        "value": "> W4. There are many methods which assess augmentation + entropy (see MEMO, SENTRY\u00a0https://arxiv.org/pdf/2012.11460.pdf). SENTRY does sample filtering based on consistency of predictions over various augmentations. It is a crucial and missing baseline in this work, especially in Tab. 2 and 3. I know SENTRY operates in UDA setting, but the consistency over augmentations part is pretty standard. \n\n> W6. Why is the MEMO baseline missing in Tab. 2 and 3, where I would guess MEMO to be most effective as it also does entropy average over multiple transformations. It is quite possible that the average entropy over augmentations is low for spurious correlations domainted prediction, making those samples contribute much less to the loss anyways.\n\nA: Thank you for the constructive comment. We added the results of MEMO and SENTRY to Tabs. 2 and 3 in the main manuscript. We implement SENTRY based on the implementation of MEMO and excluding the supervised loss component following the TTA setting. As shown in the results, SENTRY yields higher performance than MEMO. However, their performance is similar to other baselines and is significantly lower than DeYO.\n\nWe think that there are two reasons why SENTRY showed a higher performance than MEMO. Firstly, SENTRY utilizes additional memory to estimate class distribution and employs a loss function based on it. Secondly, instead of identifying confident samples using the average entropy of predictions, SENTRY identifies samples that are not influenced by augmentations.\n\nThe reason why the performance of MEMO and SENTRY is significantly lower than DeYO is due to the fundamental issue we highlighted in Sec. 2.1 regarding entropy: \u201crelying solely on entropy may not be consistently reliable in the presence of distribution shifts, as it cannot distinguish whether the model focuses on the desired target.\u201d \nIn other words, entropy cannot distinguish whether a sample is primarily represented by CPR factors or TRAP factors, regardless of its value (including those samples contributing much more to the loss you mentioned). This issue cannot be resolved by averaging entropy over multiple augmentations which is used by MEMO and SENTRY because they rely solely on entropy. To address this, we propose a new metric called PLPD, which can measure the influence of CPR factors. We revised the first paragraph of page 5 in the main manuscript to highlight and provide further explainations for the entropy's problem in distinguishing **harmful** samples.\n\n### MEMO & SENTRY\n||ColoredMNIST||WaterBirds||\n|-|:-:|:-:|:-:|:-:|\n||Avg Acc (%)|Worst Acc (%)|Avg Acc (%)|Worst Acc (%)|\n|MEMO|63.77|6.23|82.34|50.47|\n|SENTRY|63.23|15.78|85.77|60.90|\n|TENT|56.28|8.93|82.61|53.43|\n|EATA|60.29|16.99|83.42|57.48|\n|DeYO with $x'_{pat}$|77.61|65.51|86.56|74.18|\n\n> W5. The authors mention existing works use entropy for selective minimization. There should be a baseline where we do selection based on average entropy over multiple augmentations. It is a trivial extension of previous works and should have been a baseline to asses the effectiveness of pseudo-label confidence drop proposed in this work.\n\nA: We agree with your comment that the baseline you proposed can provide a further demonstration of the effectiveness of DeYO. Following your suggestion, we conducted experiments in EATA and DeYO using average entropy over four AugMix [2] augmentations for entropy filtering. As indicated in the table below, this modification resulted in only marginal performance improvements. The results show that the use of average entropy from augmented images is not beneficial to enhancing robustness against distribution shifts between training and inference phases. \n\n### The effect of multiple augmentations\n||ColoredMNIST||WaterBirds||\n|-|:-:|:-:|:-:|:-:|\n||Avg Acc (%)|Worst Acc (%)|Avg Acc (%)|Worst Acc (%)|\n|EATA|60.29|16.99|83.42|57.48|\n|EATA with 4*AugMix($x$)|60.29|17.00|83.6|57.94|\n|DeYO|77.61|65.51|86.56|74.18|\n|DeYO with 4*AugMix($x$)|77.83|65.58|86.81|74.45|\n\nAdditionally, in DeYO, we experimented using multiple AugMix($x$) as $x\u2019$ for calculating PLPD. AugMix employs various object-preserving augmentations such as autocontrast, equalize, rotate, and solarize, randomly applying them to create multiple augmented instances. Then, the augmented instances are mixed with the original input. AugMix is also utilized in MEMO. \n\nWhen using the average PLPD for a total of 8 mixed inputs in DeYO, it showed lower performance than all three object-destructive transformation methods in both ColoredMNIST and Waterbirds. The AURC, which evaluates the reliability of PLPD, also exhibited worse performance compared to the object-destructive transformations. Our proposed PLPD needs to disrupt the shape of the object to function effectively. Through the AURC values, we empirically confirmed that AugMix (object-preserving augmentation technique) cannot contribute to measuring the impact on the CPR factor."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149378456,
                "cdate": 1700149378456,
                "tmdate": 1700234294402,
                "mdate": 1700234294402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7pxkOU4oKu",
                "forum": "9w3iw8wDuE",
                "replyto": "I6CScj5bYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NrgV (3/3)"
                    },
                    "comment": {
                        "value": "### AugMix with PLPD\n||ColoredMNIST||WaterBirds||\n|-|:-:|:-:|:-:|:-:|\n||Avg Acc (%)|Worst Acc (%)|Avg Acc (%)|Worst Acc (%)|\n|DeYO with $8*x'_{AugMix}$|53.53|4.19|82.94|60.75|\n|DeYO with $x'_{pix}$|69.68|50.78|85.11|64.17|\n|DeYO with $x'_{occ}$|74.56|62.06|86.78|74.96|\n|DeYO with $x'_{pat}$|77.61|65.51|86.56|74.18|\n\n### Evaluation of the reliability of AugMix PLPD\n||Waterbirds|\n|-|:-:|\n||AURC (%)\n|Entropy|56.3|\n|Pixel PLPD|20.71|\n|Patch PLPD|14.75|\n|Occ PLPD|13.77|\n|4*AugMix PLPD|35.58|\n\n> W7. Can the authors give any reason why their proposed approach should work better under label shift or for batch size 1 as shown in Tab. 5? I do not see any reason why augmentation based sample selection should help for label shift TTA (makes sense for spurious TTA). Similar is the question for Batch size 1 setting. Are the authors sure they did proper hyperparameter tuning for the baselines?\n\nA: Sorry for missing an intuitive description of why our proposed method performs well in wild scenarios compared to existing approaches. The reason why DeYO excels in wild scenarios is that, for the first time, it constrains the direction of adaptation through entropy minimization in distribution shift situations by focusing on the shape of the object. Even in cases where ground-truth labels exist, such as in cross-entropy learning with label shift or batch size 1, training can be disrupted by bias (label shift) or variance (batch size 1). Furthermore, in situations like TTA, error accumulation becomes more severe. Existing methods in such scenarios aim to improve performance by enhancing training stability rather than restricting the direction of adaptation. However, this only delays the worsening of severe error accumulation once it has begun and does not fundamentally reduce it. \n\nWe have conducted sufficient hyperparameter tuning for the baselines, and it is already demonstrated at SAR that Tent and EATA excel only in mild scenarios.\n\n> W8. Further, the effect of such spurious correlations can be removed by incorporating such transforms (cutmix, random cropping, or the used patch shuffling) during pretraining itself. Will the proposed approach help in those cases as well?\n\nA: Thank you for suggesting the experiment. This experiment allowed us to further validate the scalability of our proposed method. In line with your comment, we experimented with utilizing PLPD-based filtering during pretraining. For Waterbirds, during a total of 200 epochs, the initial 50 epochs were trained conventionally, followed by 150 epochs where only samples with PLPD greater than the threshold of 0.2 were used for pretraining. After implementing PLPD filtering, the performance is 5.82% higher compared to the original result on Tent. Similarly, the performance of DeYO also improves by 3.48%.\n\n### Pretraining with PLPD filtering\n|Waterbirds|Tent||DeYO||\n|-|:-:|:-:|:-:|:-:|\n||Avg Acc (%)|Worst Acc (%)|Avg Acc(%)|Worst Acc(%)|\n|Pretraining w/o PLPD|82.98|52.9|86.64|74.18|\n|Pretraining w/ PLPD|84.45|58.72|87.90|77.66|\n\n> References  \n> [1] Wiles, Olivia, et al. \"A Fine-Grained Analysis on Distribution Shift.\"\u00a0International Conference on Learning Representations. 2022.  \n[2] Hendrycks, Dan, et al. \"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty.\"\u00a0International Conference on Learning Representations. 2020.\n\nThe final version of the manuscript will incorporate all the responses."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149683028,
                "cdate": 1700149683028,
                "tmdate": 1700154513232,
                "mdate": 1700154513232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EGpDVolwf5",
                "forum": "9w3iw8wDuE",
                "replyto": "I6CScj5bYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer NrgV,\n\nThank you again for the insightful comments and suggestions! Given the limited time remaining, we eagerly anticipate your subsequent feedback. It would be our pleasure to offer more responses to further demonstrate the effectiveness of our methodology.\n\nIn our previous response, we have thoroughly reviewed your comments and provided responses summarized as follows:\n\n- Addressed writing deficiencies by clarifying unclear terms and expanding the related work section. Our revised related work section covers much broader scopes, including test-time adaptation, disentangled factors, self-training (referencing Self-training Avoids Using Spurious Features Under Domain Shift [1]), and consistency under augmentations (referencing SENTRY [2]) based on your suggestions.\n- Explained why self-training cannot avoid using the spurious features in the real world.\n- Conducted the new experiments (including MEMO and SENTRY) to show the effectiveness of object-destructive transformations compared to common data augmentation methods.\n- Provided more explanations as to why DeYO is still effective in wild scenarios.\n- Conducted the new experiment to show the effectiveness of sample selection based on PLPD during pretraining.\n\nWe hope that the provided new experiments and the additional explanation have convinced you of the merits of this paper. If there are additional questions, please feel free to let us know.\n\nAdditionally, we wish to express our gratitude once again to you for your insightful feedback. Incorporating your suggestions has undoubtedly enhanced the clarity and robustness of our work.\n\nWe deeply appreciate your time and effort!\n\nBest regards, Authors\n\n> References  \n[1] Chen, Yining, et al. \"Self-training avoids using spurious features under domain shift.\"\u00a0Neural Information Processing Systems. 2020.\n[2] Prabhu, Viraj, et al. \"Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation.\"\u00a0International Conference on Computer Vision. 2021."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542721095,
                "cdate": 1700542721095,
                "tmdate": 1700542721095,
                "mdate": 1700542721095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a6WXCszcL2",
                "forum": "9w3iw8wDuE",
                "replyto": "EGpDVolwf5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "I am amazed by the amount of efforts the authors have put in to address my concerns. I will be raising my rating.\n\nCan the authors further clarify on why their approach works in label shift and batch size = 1. I didnt understand what they are trying to hint.\n\n>The reason why DeYO excels in wild scenarios is that, for the first time, it constrains the direction of adaptation through entropy minimization in distribution shift situations by focusing on the shape of the object. Even in cases where ground-truth labels exist, such as in cross-entropy learning with label shift or batch size 1, training can be disrupted by bias (label shift) or variance (batch size 1). \n\nHow does focusing on shape relate to helping resolve label shift? I would encourage authors to put in more thoughts on this and give a proper justification. Current justification doesn't make sense or else maybe I am missing something. If I understand why their approach works so well as given my numbers, I can raise my rating further. My only issue now with this paper is I don't understand the source of their gains. In general, I believe this paper should lacks ablation studies."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588097452,
                "cdate": 1700588097452,
                "tmdate": 1700588097452,
                "mdate": 1700588097452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m3Fa9AShH2",
                "forum": "9w3iw8wDuE",
                "replyto": "I6CScj5bYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Firstly, we are pleased that our responses have provided sufficient answers and have been helpful in enhancing your understanding. We are also pleased to address an additional question, as it contributes to a deeper comprehension of DeYO. \n> Q1: Can the authors further clarify on why their approach works in label shift and batch size 1?\n\nA: To clarify on why our DeYO works well in label shifts and batch size 1, we provide an interpretation more in line with our theoretical analysis. Essentially, the enhancement in performance arising from excluding TRAP factors during adaptation, thereby increasing robustness against covariate shifts and spurious correlations in the wild scenarios (label shifts and batch size 1), is the same as the mild scenario.\n\nLabel shifts and batch size 1 scenarios have the following characteristics.\n\n1. label shifts\n    - An Input data stream is dependent on the ground truth class.\n    - When i.i.d. samples are present within a batch (a mild scenario), it's possible to discern which disentangled factors are emphasized for each class. However, in a label shifts scenario, since all samples in a batch belong to the same class, this cannot be determined.\n2. batch size 1\n    - A model only uses a single image to update its parameter.\n    - When i.i.d. samples are present within a batch (a mild scenario), sample-specific noise is reduced through loss averaging. However, in a batch size 1 scenario, this noise cannot be reduced, increasing sample dependency.\n\nAccording to our theoretical analysis, the increase in the weight of disentangled factors is proportional to the value of the corresponding factors, as described in Appendix A. In general, without PLPD filtering, during updates with batches where all samples have the ground truth label $c$ in a label shifts scenario, the weight of disentangled factors common to samples in the batch increases proportionally to the corresponding factors' values. In a batch size 1 scenario, the weight of all disentangled factors in the current sample increases proportionally to the values of the corresponding factors.\n\nUsing only entropy, weights for class $c$ still increase regardless of its CPR and TRAP factors. Consequently, in samples with a different ground truth $c\u2019 \\ne c$, the increased weights related to class $c$ result in larger logits of $c$, raising the likelihood of an incorrect prediction as class $c$. To maintain the prediction of samples with ground truth $c\u2019$, the value of updated weights' corresponding factors should be small to reduce logit changes. Without prior knowledge of distribution shifts, we infer that CPR factors of class $c$ hold smaller values in class $c\u2019$. Thus, updating only the weights corresponding to CPR factors of class $c$ can preserve the prediction. Utilizing PLPD filtering, which focuses on samples where the influence of CPR factors outweighs that of TRAP factors, allows for the selective update of weights corresponding to CPR factors of class $c$ while minimizing the change in weights for TRAP factors. Therefore, using PLPD filtering can preserve the original predictions of samples with different ground truths.\n\nCompared to a mild scenario, wild scenarios suffer from a lack of diversity in batch information of the distribution of disentangled factors or even proceed with updates using a single image. It exacerbates optimization stability and provokes stochasticity issues. Consequently, the drawbacks of entropy are more pronounced in wild scenarios than in mild ones, leading to a significant performance decline of baseline methods. In contrast, information on CPR/TRAP factors, which only DeYO discriminates, is less affected by the batch size or class dependency of an input stream compared to entropy, offering improved robustness.\n\nWe hope that this response can resolve your concern.\n> W1: In general, I believe this paper should lacks ablation studies.\n\nA: Additional ablation studies on various sets of a model architecture, benchmark, and test scenario can be found in the response to Reviewer M673 (ViTBase, label shifts scenario, ImageNet-C) and in Tab 15. in the Appendix (ResNet-50-BN, biased scenario, Waterbirds). Notably, the ablation study in the label shifts scenario experimentally supports our claim that PLPD is more effective than entropy. We hope that these results can resolve your concern.\n\n### Ablation studies of DeYO on ImageNet-C under online imbalanced label shifts with ViTBase-LN\n|$S_{\\theta}(x)$|$S_{\\theta}(x)$|${\\alpha}_{\\theta}(x)$|${\\alpha}_{\\theta}(x)$||\n|-|-|-|-|-|\n|$Ent_{\\theta}$|$PLPD_{\\theta}$|$Ent_{\\theta}$|$PLPD_{\\theta}$|Avg Acc (%)|\n|||||52.709|\n||||\u2714|54.777|\n|||\u2714||54.041|\n|||\u2714|\u2714|54.078|\n||\u2714||| 59.975 |\n||\u2714||\u2714|61.744|\n||\u2714|\u2714||61.024|\n||\u2714|\u2714|\u2714|61.443|\n|\u2714|||| 52.714 |\n|\u2714|||\u2714|56.179|\n|\u2714||\u2714||52.150|\n|\u2714||\u2714|\u2714|54.212|\n|\u2714|\u2714|||59.963|\n|\u2714|\u2714||\u2714|61.580|\n|\u2714|\u2714|\u2714||60.776|\n|\u2714|\u2714|\u2714|\u2714|61.304|\n\nWe deeply appreciate your time and effort!\n\nThe final version of the manuscript will incorporate all the responses."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633414185,
                "cdate": 1700633414185,
                "tmdate": 1700633692974,
                "mdate": 1700633692974,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hrUmWu1T02",
                "forum": "9w3iw8wDuE",
                "replyto": "m3Fa9AShH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the efforts and explanations. I have updated my rating to 8 i..e accept.\n\nI really encourage the authors to incorporate all these intuitions which they have given during rebuttal in the main paper. This paper needs a lot of efforts on rewriting (including the comments from rebuttal) to make it much more impactful.\nAgain, thanks to the authors for incorporating all my comments, responding to all of them with experimental results."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690702270,
                "cdate": 1700690702270,
                "tmdate": 1700690702270,
                "mdate": 1700690702270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R16OniJLoe",
            "forum": "9w3iw8wDuE",
            "replyto": "9w3iw8wDuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
            ],
            "content": {
                "summary": {
                    "value": "The paper works on test-time adaptation with entropy minimization. While the online updates with entropy minimization can lead to error accumulation, the paper proposed to do sample selection and weighting by the proposed DeYO, which combines entropy and pseudo-label probability difference. Experiments on several datasets with various distribution shifts show the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The observation and theoretical demonstration of \"entropy is not enough\" on the spurious correlation shifts is interesting and motivating to the method.\n\n2. The results of the proposed method on several datasets with different distribution shifts are good, demonstrating the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The motivating observations and theoretical support are conducted on spurious correlation shifts, which is mainly on the semantic level. Can this also be found and theoretically proof for the other distribution shifts like covariate shifts?\n\n2. In 2.3, the paper theoretically demonstrates that entropy is not enough and it is better to incorporate the CPR factors for sample selection and reweighting in test time adaptation, which is done by the proposed PLPD. However, PLPD is then combined with the common entropy method in the experiments and implementations. Then what role does entropy play in sample selection? and how it helps  PLPD to incorporate the CPR factors?"
                },
                "questions": {
                    "value": "1. Did the authors try some other methods like data augmentation methods to replace the transformed one? Will they also work to incorporate CPR factors?\n\n2. How do the thresholds in eq. (9) defined?\n\n3. What are the numbers and sizes of the patches for patch shuffling? Will these also influence the adaptation?\n\n4. Why PLPD and entropy sample selections behave differently for different distribution shifts in Table 6? How to select different methods for different distribution shifts? Is there any theoretical support for this problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1097/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1097/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700718399,
            "cdate": 1698700718399,
            "tmdate": 1700661015955,
            "mdate": 1700661015955,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O1m8mY8Cmz",
                "forum": "9w3iw8wDuE",
                "replyto": "R16OniJLoe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7TAy (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging that our paper is interesting and well-demonstrated both theoretically and empirically. Building on the comments you provided, we were able to further demonstrate the robustness and effectiveness of DeYO by incorporating additional theoretical explanations and experiments. We are truly grateful for the valuable comments.\n\n> W1. The motivating observations and theoretical support are conducted on spurious correlation shifts, which is mainly on the semantic level. Can this also be found and theoretically proof for the other distribution shifts like covariate shifts?\n\nA: Thank you for the constructive comments improving the scalability of our work. Firstly, we have already demonstrated it empirically as shown in Appendix G. In contrast to ImageNet-C, ImageNet-R and VISDA-2021 (Tabs. 10-13) inherently include covariate shifts. DeYO consistently exhibits significant performance on these benchmarks as well. Also, theoretically, Chunting et al. [1] demonstrated that even in situations with only covariate shifts, deep models learn spurious features present in the training data. In other words, addressing the problem of spurious correlation shifts is closely associated with alleviating covariate shifts.\n\n> W2. In 2.3, the paper theoretically demonstrates that entropy is not enough and it is better to incorporate the CPR factors for sample selection and reweighting in test time adaptation, which is done by the proposed PLPD. However, PLPD is then combined with the common entropy method in the experiments and implementations. Then what role does entropy play in sample selection? and how it helps PLPD to incorporate the CPR factors?\n\nA: In Sec. 2.3, we theoretically demonstrated that when spurious correlations exist, incorrect biases are accumulated in the model while adapting to the target domain through entropy minimization. To address this issue, we propose a new metric called PLPD. By mitigating error accumulation based on the prominent CPR factor: the shape of the object with PLPD, entropy minimization can adapt to the target domain in unlabeled situations with less bias.\n\nThe impact of entropy on PLPD is as follows: even with identical PLPD values, entropy can vary depending on the sharpness of $x$'s output. Samples passing through PLPD filtering make predictions based on CPR factors, so those with sharper predictions denote a higher probability of the presence of CPR factors. Therefore, by utilizing entropy with PLPD, we can identify samples that emphasize CPR factors even within the same PLPD level.\n\n> Q1. Did the authors try some other methods like data augmentation methods to replace the transformed one? Will they also work to incorporate CPR factors?\n\nA: We compared the average PLPD measured by augmenting $x$ multiple times through AugMix [2] with the PLPD of pixel shuffling, patch shuffling, and center occlusion. AugMix employs various object-preserving augmentations such as autocontrast, equalize, rotate, and solarize, randomly applying them to create multiple augmented instances. Then, the augmented instances are mixed with the original input. AugMix is also utilized in MEMO. \n\nWhen using the average PLPD for a total of 8 mixed inputs in DeYO, it showed lower performance than all three object-destructive transformation methods in both ColoredMNIST and Waterbirds as shown in the tables below. The AURC, which evaluates the reliability of PLPD, also exhibited worse performance compared to the object-destructive transformations. Our proposed PLPD needs to disrupt the shape of the object to function effectively. Through the AURC values, we empirically confirmed that AugMix (object-preserving augmentation technique) cannot contribute to measuring the impact on the CPR factor. \n\n### AugMix with PLPD\n||ColoredMNIST||WaterBirds||\n|-|:-:|:-:|:-:|:-:|\n||Avg Acc (%)|Worst Acc (%)|Avg Acc (%)|Worst Acc (%)|\n|DeYO with $8*x'_{AugMix}$|53.53|4.19|82.94|60.75|\n|DeYO with $x'_{pix}$|69.68|50.78|85.11|64.17|\n|DeYO with $x'_{occ}$|74.56|62.06|86.78|74.96|\n|DeYO with $x'_{pat}$|77.61|65.51|86.56|74.18|\n\n### Evaluation of the reliability of AugMix PLPD\n||Waterbirds|\n|-|:-:|\n||AURC (%)\n|Entropy|56.3|\n|Pixel PLPD|20.71|\n|Patch PLPD|14.75|\n|Occ PLPD|13.77|\n|4*AugMix PLPD|35.58|"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149776638,
                "cdate": 1700149776638,
                "tmdate": 1700152973808,
                "mdate": 1700152973808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x9P3iYoI76",
                "forum": "9w3iw8wDuE",
                "replyto": "R16OniJLoe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7TAy (2/2)"
                    },
                    "comment": {
                        "value": "> Q2. How do the thresholds in eq. (9) defined?\n\nA: We defined $\\tau_{PLPD}$ based on empirical results. As shown in Fig. 7 (c), defining the threshold was not challenging. The results do not vary significantly with different thresholds, and this phenomenon remains consistent across different models, benchmarks, and test scenarios, as illustrated in Fig. 9 and Tab. 16 in Appendix I.\n\n> Q3. What are the numbers and sizes of the patches for patch shuffling? Will these also influence the adaptation?\n\nA: For the reported results, a patch size of 56x56 was employed. Consequently, a total of 16 (4x4) patches were shuffled into a 224x224 image. This information has been stated in Sec. 4.3 of the main manuscript. As demonstrated in Fig. 7 (b), unless using an excessively small patch size (similar to pixel shuffling), which could entirely distort the local features of the image, results remain similar.\n\n> Q4. Why PLPD and entropy sample selections behave differently for different distribution shifts in Table 6? How to select different methods for different distribution shifts? Is there any theoretical support for this problem?\n\nA: Tab. 6 pertains to the ablation studies on ImageNet-C. Are you inquiring about a different table? If you are referring to the ablation studies on different benchmarks between Tab. 6 in the main manuscript and Tab. 15 in Appendix, as convincingly demonstrated in Sec. 2, entropy-based sample selection is unstable in experiments under severe spurious correlations. However, fundamentally, the results from both tables are well-aligned. In other words, we believe that estimating the type or degree of distribution shift in advance is challenging. Thus, based on those results, in general, our proposed DeYO is expected to consistently deliver good performance under various circumstances. \n\nSeparate from Tab. 6, we can also discuss the effectiveness of DeYO in handling different distribution shifts. Chunting et al. [1] stated that even in situations with only covariate shifts, deep models learn spurious features present in the training data, and Olivia et al. [3] identified three types of distribution shifts: 1) spurious correlation, 2) low data drift, and 3) unseen data shift, affecting generalization performance in the real world. In this regard, DeYO addresses the inherent spurious correlation problem associated with changes in the real-world input distribution, making it theoretically effective for general input distribution shifts. \n\nAlso, DeYO is more robust than other baselines in wild scenarios. The reason why DeYO excels in wild scenarios is that, for the first time, it constrains the direction of adaptation through entropy minimization in distribution shift situations by focusing on the shape of the object. Even in cases where ground-truth labels exist, such as in cross-entropy learning with label shift or batch size 1, training can be disrupted by bias (label shift) or variance (batch size 1). Furthermore, in situations like TTA, error accumulation becomes more severe. Existing methods in such scenarios aim to improve performance by enhancing training stability rather than restricting the direction of adaptation. However, this only delays the worsening of severe error accumulation once it has begun and does not fundamentally reduce it. \n\n> References  \n[1] Zhou, Chunting, et al. \"Examining and combating spurious features under distribution shift.\"\u00a0International Conference on Machine Learning. PMLR, 2021.  \n[2] Hendrycks, Dan, et al. \"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty.\"\u00a0International Conference on Learning Representations. 2020.  \n[3] Wiles, Olivia, et al. \"A Fine-Grained Analysis on Distribution Shift.\"\u00a0*International Conference on Learning Representations*. 2022.\n\nThe final version of the manuscript will incorporate all the responses."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150040112,
                "cdate": 1700150040112,
                "tmdate": 1700154551386,
                "mdate": 1700154551386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RAV30AyxpX",
                "forum": "9w3iw8wDuE",
                "replyto": "R16OniJLoe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7TAy,\n\nThank you again for the insightful comments and suggestions! Given the limited time remaining, we eagerly anticipate your subsequent feedback. It would be our pleasure to offer more responses to further demonstrate the effectiveness of our methodology.\n\nIn our previous response, we have thoroughly reviewed your comments and provided responses summarized as follows:\n\n- Explained why DeYO also addresses covariate shifts in addition to spurious correlations.\n- Explained the role of entropy on sample selection when used in conjunction with PLPD.\n- Conducted the new experiments to show the effectiveness of object-destructive transformations compared to common data augmentation methods.\n- Explained how $\\tau_{PLPD}$ is selected and how many patches are used.\n- Provided more explanations related to Tab. 6 and the effectiveness of DeYO under different distribution shifts.\n\nWe hope that the provided new experiments and the additional explanation have convinced you of the merits of this paper. If there are additional questions, please feel free to let us know.\n\nAdditionally, we wish to express our gratitude once again to you for your insightful feedback. Incorporating your suggestions has undoubtedly enhanced the clarity and robustness of our work.\n\nWe deeply appreciate your time and effort!\n\nBest regards, Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542689339,
                "cdate": 1700542689339,
                "tmdate": 1700542689339,
                "mdate": 1700542689339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8hHxjgAxXZ",
                "forum": "9w3iw8wDuE",
                "replyto": "x9P3iYoI76",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response of the authors"
                    },
                    "comment": {
                        "value": "The rebuttal solves most of my concerns. I would improve my score to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660999566,
                "cdate": 1700660999566,
                "tmdate": 1700660999566,
                "mdate": 1700660999566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WuW1DZ4IGy",
            "forum": "9w3iw8wDuE",
            "replyto": "9w3iw8wDuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_xMUg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_xMUg"
            ],
            "content": {
                "summary": {
                    "value": "The authors found that using entropy as metrics is not enough in some biased scenarios. To address this, the authors devise a new metric namely Pseudo-Label Probability Difference. The experimental results demonstrate the effectiveness of the proposed metric."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe authors empirically and theoretically analysis the entropy metric for TTA.\n2.\tThe authors devise a metric namely Pseudo-Label Probability Difference that further improves the entropy metric.\n3.\tThe proposed method is easy to understand and implement. I believe it can be applied to real-world applications. In addition, the proposed metric only requires negligible computational cost to compute, which would not introduce obvious latency compared with EATA or SAR."
                },
                "weaknesses": {
                    "value": "1.\tCould the authors give simple explanation TRAP factors and CPR factors? With this, the readers may easily to capture the motivation of the proposed metric.\n2.\tCould the authors explain more about the motivation of the choice of patch shuffled input as x\u2019?"
                },
                "questions": {
                    "value": "It would be better if the authors could explain the motivations more clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698981135344,
            "cdate": 1698981135344,
            "tmdate": 1699636035995,
            "mdate": 1699636035995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m1Pseyv2p7",
                "forum": "9w3iw8wDuE",
                "replyto": "WuW1DZ4IGy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xMUg (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing our paper and offering a positive assessment. We appreciate your acknowledgment, particularly in relation to DeYO's exceptional performance and efficiency, highlighting its potential applicability in real-world scenarios.\n\n> W1 (Q). Could the authors give simple explanation TRAP factors and CPR factors? With this, the readers may easily capture the motivation of the proposed metric.  \n\nA: Sorry for the lack of clarity in defining the terms 'TRAP' and 'CPR' factors. To enhance comprehension of these terms, we revised the introduction by incorporating examples and more explanations (Currently, the revised portions are delineated in red in the manuscript).  \n\nWe revised the second paragraph of page 2 in the main manuscript as follows: \"Based on the observation, we introduce a theoretical proposition for identifying harmful samples, those that decrease the model's discriminability during adaptation: a sample is harmful if its prediction is more influenced by TRAin-time only Positively correlated with label (TRAP) factors (e.g., background, weather) rather than Commonly Positively-coRrelated with label (CPR) factors (e.g., structure, shape), even if its entropy is low. TRAP factors boost training performance but decrease inference performance, attributable to a discrepancy in the correlation sign with labels. If predictions mainly rely on TRAP factors, there is a high risk of wrong predictions under distribution shifts.\"\n\n> W2 (Q). Could the authors explain more about the motivation of the choice of patch shuffled input as x\u2019?  \n\nA: The choice of using the patch-shuffled input, denoted as $x'$, can be explained from three perspectives.  \nFirstly, to capture the prominent CPR factor: the shape of the object, we had to utilize an object-destructive transformation method rather than an object-preserving transformation method such as rotation or flip. DeYo is designed to measure the difference between $x$ and $x'$, which lies in the presence or absence of the shape of the object. \nSecondly, among various object-destructive transformation methods, patch shuffling and object occlusion are options to disrupt the shape of the object in $x$. However, object occlusion presents difficulties in locating the object. \nFurthermore, unlike augmentation techniques such as CutMix [1], our work aimed to be capable of the destruction of objects using only a single image. This enables the applicability of DeYO in a batch size 1 scenario. \nTherefore, we employ patch shuffling for $x'$ on DeYO.\n\n> References  \n[1] Yun, Sangdoo, et al. \"Cutmix: Regularization strategy to train strong classifiers with localizable features.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019.\n\nThe final version of the manuscript will incorporate all the responses."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146335579,
                "cdate": 1700146335579,
                "tmdate": 1700152459525,
                "mdate": 1700152459525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wVr8YJFk3t",
                "forum": "9w3iw8wDuE",
                "replyto": "WuW1DZ4IGy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer xMUg,\n\nThank you again for the insightful comments and suggestions! Given the limited time remaining, we eagerly anticipate your subsequent feedback. It would be our pleasure to offer more responses to further demonstrate the effectiveness of our methodology.\n\nIn our previous response, we have thoroughly reviewed your comments and provided responses summarized as follows:\n\n- Revised the introduction by incorporating examples and more explanations for TRAP/CPR factors.\n- Provided three reasons why we use a patch shuffling transformation.\n\nWe hope that the provided new experiments and the additional explanation have convinced you of the merits of this paper. If there are additional questions, please feel free to let us know.\n\nAdditionally, we wish to express our gratitude once again to you for your insightful feedback. Incorporating your suggestions has undoubtedly enhanced the clarity and robustness of our work.\n\nWe deeply appreciate your time and effort!\n\nBest regards, Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542673401,
                "cdate": 1700542673401,
                "tmdate": 1700542673401,
                "mdate": 1700542673401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mGX8qVKu0U",
                "forum": "9w3iw8wDuE",
                "replyto": "wVr8YJFk3t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_xMUg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_xMUg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for authors' responses"
                    },
                    "comment": {
                        "value": "I would keep my scoring."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643372489,
                "cdate": 1700643372489,
                "tmdate": 1700643372489,
                "mdate": 1700643372489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LlXYQCwQPI",
            "forum": "9w3iw8wDuE",
            "replyto": "9w3iw8wDuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_M673"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1097/Reviewer_M673"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new method for test-time adaptation (TTA) called Destroy Your Object (DeYO) that uses a novel confidence metric called Pseudo-Label Probability Difference (PLPD) to improve the adaptation performance and stability of test-time adaptation methods. The authors demonstrate the limitations of entropy as a confidence metric and compare the performance of DeYO with other TTA methods on the ImageNet-C and ImageNet-R benchmarks under various mild and wild test scenarios."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed DeYO method is simple yet effective for improving the stability and performance of entropy-based TTA.\n\nThe idea and motivation behind Pseudo-Label Probability Difference (PLPD) are novel and interesting, providing new insights for the community.\n\nThe paper is strong on the empirical side. Extensive experiments with various model architectures, datasets, and mild/wild test scenarios are thorough."
                },
                "weaknesses": {
                    "value": "The proposed terms \u201cTRAP\u201d and \u201cCRP\u201d are a bit hard to understand. The authors could refine the name and give more high-level/easy-understanding explanations about them in the Introduction. \n\nFor parameter sensitivity analyses in Figure 7, could the authors report more results under different model architectures, datasets and test scenarios? This helps demonstrate the hyperparameters\u2019 generality. \n\nAblation studies in Table 6 are also highly encouraged to be conducted on different models, datasets and test scenarios."
                },
                "questions": {
                    "value": "Pls refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699110706559,
            "cdate": 1699110706559,
            "tmdate": 1699636035924,
            "mdate": 1699636035924,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TDOhtErAHf",
                "forum": "9w3iw8wDuE",
                "replyto": "LlXYQCwQPI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M673 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for positively evaluating our proposed method, DeYO. We appreciate your recognition of the novelty in our idea and motivation, acknowledging its contribution to providing new insights to the community. Moreover, we are grateful for your acknowledgment of the strength of DeYO on the empirical side.\n\nBased on your comments, we revised the main manuscript to improve the readability of the manuscript and conducted experiments to further prove the robustness of DeYO.\n\n> W1. The proposed terms \u201cTRAP\u201d and \u201cCPR\u201d are a bit hard to understand. The authors could refine the name and give more high-level/easy-understanding explanations about them in the Introduction.  \n\nA: Sorry for the lack of clarity in defining the terms 'TRAP' and 'CPR' in the introduction. To enhance comprehension of these terms, we revised the introduction by incorporating examples and more explanations (Currently, the revised portions are delineated in red in the manuscript). \n\nWe revised the second paragraph of page 2 in the main manuscript as follows: \"Based on the observation, we introduce a theoretical proposition for identifying harmful samples, those that decrease the model's discriminability during adaptation: a sample is harmful if its prediction is more influenced by TRAin-time only Positively correlated with label (TRAP) factors (e.g., background, weather) rather than Commonly Positively-coRrelated with label (CPR) factors (e.g., structure, shape), even if its entropy is low. TRAP factors boost training performance but decrease inference performance, attributable to a discrepancy in the correlation sign with labels. If predictions mainly rely on TRAP factors, there is a high risk of wrong predictions under distribution shifts.\"\n\nWe have kept the term as it is, as we could not think of a good alternative. However, if it still remains unclear in the current revised manuscript, we will modify the term as well. Please feel free to share your thoughts.\n\n> W2. For parameter sensitivity analyses in Fig. 7, could the authors report more results under different model architectures, datasets and test scenarios? This helps demonstrate the hyperparameters\u2019 generality.\n\nA: Thank you for the suggestion. Following your comment, we experimented with the parameter sensitivity of DeYO under a different model architecture (ViTBase), dataset (WaterBirds), and test scenario (wild scenario: online imbalanced label shifts and biased scenario: spurious correlation). We still observed consistent results with those of Fig. 7 (ResNet-50-BN, ImageNet-C, mild scenario) as shown in the tables:\n### Augmentation Type\n|       | ImageNet-C (ViTBase-LN, wild scenario: label shifts) | WaterBirds (ResNet-50-BN, biased scenario) |  WaterBirds (ResNet-50-BN, biased scenario) |\n|-------|:----------------------------------------------------:|:------------------------------------------:|:-------------:|\n|       |                      Avg Acc (%)                     |                 Avg Acc (%)                | Worst Acc (%) |\n| Pixel |                         52.01                        |                    85.84                   |     65.42     |\n| Patch |                         61.30                        |                    88.16                   |     74.18     |\n| Occ   |                            60.24                         |                    87.69                   |     73.55     |\n### Number of Patches\n|     | ImageNet-C (ViTBase-LN, wild scenario: label shifts) | WaterBirds (ResNet-50-BN, biased scenario) | WaterBirds (ResNet-50-BN, biased scenario) |\n|-----|:----------------------------------------------------:|:------------------------------------------:|:------------------------------------------:|\n|     |                      Avg Acc (%)                     |                 Avg Acc (%)                |                Worst Acc (%)               |\n| 2X2 |                         62.47                        |                    86.98                   |                    74.96                   |\n| 3X3 |                         60.73                        |                    87.26                   |                    75.43                   |\n| 4X4 |                         61.30                        |                    88.16                   |                    74.18                   |\n| 5X5 |                         59.46                        |                    87.12                   |                    72.14                   |\n| 6X6 |                         61.04                        |                    87.40                   |                    72.14                   |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147357055,
                "cdate": 1700147357055,
                "tmdate": 1700151408601,
                "mdate": 1700151408601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8eBakVutIA",
                "forum": "9w3iw8wDuE",
                "replyto": "LlXYQCwQPI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M673 (2/2)"
                    },
                    "comment": {
                        "value": "### $\\tau_{PLPD}$\n|     | ImageNet-C (ViTBase-LN, wild scenario: label shifts) | WaterBirds (ResNet-50-BN, biased scenario) | WaterBirds (ResNet-50-BN, biased scenario) |\n|-----|:----------------------------------------------------:|:------------------------------------------:|:------------------------------------------:|\n|     |                      Avg Acc (%)                     |                 Avg Acc (%)                |                Worst Acc (%)               |\n| 0.0 |                         60.86                        |                    84.54                   |                    58.84                   |\n| 0.2 |                         61.30                        |                    87.31                   |                    71.67                   |\n| 0.3 |                         61.45                        |                    87.72                   |                    72.93                   |\n| 0.4 |                         61.25                        |                    87.45                   |                    71.67                   |\n| 0.5 |                         60.26                        |                    88.16                   |                    74.18                   |\n| 0.6 |                           -                          |                    88.06                   |                    74.80                   |\n| 0.7 |                           -                          |                    87.59                   |                    74.02                   |\n\nFor Waterbirds, it involves a 2-class classification task. Therefore, the value of $\\tau_{PLPD}$ should be larger than that of ImageNet-C, which has 1000 classes, to adequately capture the changes induced by transformations.\n\n> W3. Ablation studies in Tab. 6 are also highly encouraged to be conducted on different models, datasets and test scenarios.\n\nA: The ablation studies on WaterBirds (a different dataset and biased scenario: spurious correlation) have been provided in Appendix H. Additionally, based on your comment, we validated DeYO under a different model architecture (ViTBase), and test scenario (wild scenario: online imbalanced label shifts). We confirmed that the ablation results of ViTBase-LN under a wild scenario show a consistent trend with those of ResNet-50-BN under a biased scenario and ResNet-50-BN under a mild scenario (Tab. 6 in the main manuscript).\n\n### Ablation studies of DeYO on ImageNet-C under online imbalanced label shifts with ViTBase-LN\n| $S_{\\theta}(x)$ | $S_{\\theta}(x)$ | ${\\alpha}_{\\theta}(x)$ | ${\\alpha}_{\\theta}(x)$ |  |\n|:---:|:---:|:---:|:---:|:---:|\n| $Ent_{\\theta}$ | $PLPD_{\\theta}$ | $Ent_{\\theta}$ | $PLPD_{\\theta}$ | Avg Acc (%) |\n|  |  |  |  | 52.709 |\n|  |  |  | &#10004; | 54.777 |\n|  |  | &#10004; |  | 54.041 |\n|  |  | &#10004; | &#10004; | 54.078 |\n|  | &#10004; |  |  | 59.975 |\n|  | &#10004; |  | &#10004; | 61.744 |\n|  | &#10004; | &#10004; |  | 61.024 |\n|  | &#10004; | &#10004; | &#10004; | 61.443 |\n| &#10004; |  |  |  | 52.714 |\n| &#10004; |  |  | &#10004; | 56.179 |\n| &#10004; |  | &#10004; |  | 52.150 |\n| &#10004; |  | &#10004; | &#10004; | 54.212 |\n| &#10004; | &#10004; |  |  | 59.963 |\n| &#10004; | &#10004; |  | &#10004; | 61.580 |\n| &#10004; | &#10004; | &#10004; |  | 60.776 |\n| &#10004; | &#10004; | &#10004; | &#10004; | 61.304 |\n\nThe final version of the manuscript will incorporate all the responses."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148897897,
                "cdate": 1700148897897,
                "tmdate": 1700151483996,
                "mdate": 1700151483996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PEbVoOxrOt",
                "forum": "9w3iw8wDuE",
                "replyto": "LlXYQCwQPI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1097/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your post-rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer M673,\n\nThank you again for the insightful comments and suggestions! Given the limited time remaining, we eagerly anticipate your subsequent feedback. It would be our pleasure to offer more responses to further demonstrate the effectiveness of our methodology.\n\nIn our previous response, we have thoroughly reviewed your comments and provided responses summarized as follows:\n\n- Revised the introduction by incorporating examples and more explanations for TRAP/CPR factors.\n- Conducted the new experiments to show DeYO is not sensitive to hyperparameters regardless of various architectures, scenarios, and benchmarks.\n- Conducted the new experiments to show DeYO has a consistent trend regardless of various architectures, scenarios, and benchmarks.\n\nWe hope that the provided new experiments and additional explanation have convinced you of the merits of this paper. If there are additional questions, please feel free to let us know.\n\nAdditionally, we wish to express our gratitude once again to you for your insightful feedback. Incorporating your suggestions has undoubtedly enhanced the clarity and robustness of our work.\n\nWe deeply appreciate your time and effort!\n\nBest regards, Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1097/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542646472,
                "cdate": 1700542646472,
                "tmdate": 1700542646472,
                "mdate": 1700542646472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]