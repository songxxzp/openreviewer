[
    {
        "title": "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective"
    },
    {
        "review": {
            "id": "L6LwNswLyY",
            "forum": "BPHcEpGvF8",
            "replyto": "BPHcEpGvF8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the safety risks in machine learning posed by backdoor attacks, where triggers are embedded in models to activate malicious behavior under specific conditions. It focuses on evaluating the effectiveness of backdoor attacks with constant triggers and establishing performance boundaries for models on clean and compromised data. The study explores key issues: the factors determining an attack's success, the optimal strategy for an attack, and the conditions for success with human-imperceptible triggers. Applicable to both discriminative and generative models, the findings are validated through experiments using benchmark datasets and current backdoor attack scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper focuses on an important problem. It provides a fundamental understanding of the influence of backdoor attacks.\n- This paper provides extensive theoretical analysis.\n- This paper is easy to follow."
                },
                "weaknesses": {
                    "value": "- The observation that a high poisoning ratio adversely affects the performance of clean data lacks novelty. \n- The paper lacks clarity in some sections. For instance, Section 6.2.1 discusses the impact of backdoor trigger magnitudes, but fails to specify crucial details of the attack setting, such as the size of the trigger.\n- The authors assert that WaNet, Adaptive Patch, and Adaptive Blend attacks are more effective than BadNets, as evidenced by a greater relative change in dimensions with low variance. However, the term \"effectiveness\" needs clarification. BadNets is known for its high attack success rate, so how do these methods compare under identical attack settings, including trigger size and magnitude?\n- The methodology for measuring the Mean Squared Error (MSE) between clean training images and those altered by the backdoored DDPM is unclear. Given that DDPM generation is inherently a stochastic process, a more detailed explanation of this measurement technique would be beneficial."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8404/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8404/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698376596634,
            "cdate": 1698376596634,
            "tmdate": 1700664102940,
            "mdate": 1700664102940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yqWCE4QT5v",
                "forum": "BPHcEpGvF8",
                "replyto": "L6LwNswLyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for dedicating time reviewing our work. \n\n1. \"The observation that a high poisoning ratio adversely affects the performance of clean data lacks novelty.\"\n\n   **Response:** Our results are non-trivial in several aspects. **First,** we offer a **quantitative** analysis, beyond mere empirical observations, about the effects of backdoor attacks. For instance, instead of the empirical observation that a high poisoning ratio can damage the model performance, we establish both lower and upper bounds for the prediction risk, as presented in Theorem 1 and 2. Those results serve as foundations for more nuanced analysis. For example, we find that a vanishing backdoor data ratio may be sufficient for successful backdoor attacks, and can even derive the minimum required amount of backdoor data in specific scenarios. **Second,** our work is pioneering in theoretically studying the performance of backdoor attacks. The closest work, to our knowledge, is Manoj & Blum (2021), which focuses on the model\u2019s vulnerability, while ours is the first to quantify the effectiveness of these attacks. **Third,** the analytical techniques we used are also non-trivial. The main proof steps involve establishing the connection between the poisoned model and clean model, estimating the model deviation caused by backdoor data, and carefully optimizing the inequality so that the error bounds are tight in the sense of order.\n\n2. \"The paper lacks clarity in some sections. For instance, Section 6.2.1 discusses the impact of backdoor trigger magnitudes, but fails to specify crucial details of the attack setting, such as the size of the trigger.\"\n\n   **Response:** We present detailed specifications for the setup in Table 2 as follows. In the case of MNIST, the backdoor triggers are 2 by 2 square patches, while for CIFAR-10, 3 by 3 square patches are utilized. All backdoor triggers are positioned at the lower-right corner of the inputs, replacing the original pixels with identical values. The pixel value represents the magnitude of the backdoor trigger. The poisoning ratio is consistently set at 5% for both datasets, and ResNet models are utilized in the experiments. \n\n3. \"The authors assert that WaNet, Adaptive Patch, and Adaptive Blend attacks are more effective than BadNets, as evidenced by a greater relative change in dimensions with low variance. However, the term \"effectiveness\" needs clarification. BadNets is known for its high attack success rate, so how do these methods compare under identical attack settings, including trigger size and magnitude?\"\n\n      **Response:** When discussing 'effectiveness,' we are referring to the consideration that backdoor attacks, exhibiting comparable clean and backdoor accuracies yet featuring less visibly discernible triggers, are deemed more effective. This is due to the fact that these subtle triggers can easily elude human eye detection, thereby enhancing their overall stealthiness. For instance, consider the WaNet/BadNets method, achieving a clean accuracy of 94.15%/94.3% and a backdoor accuracy of 99.15%/99.9% on the CIFAR-10 dataset, respectively. Despite the similar clean and backdoor accuracies of the two methods, the WaNet method is considered more effective due to its less visible trigger, as illustrated in the Figure on the official GitHub page of WaNet (https://github.com/VinAIResearch/Warping-based_Backdoor_Attack-release).\n\n\n4. \"The methodology for measuring the Mean Squared Error (MSE) between clean training images and those altered by the backdoored DDPM is unclear. Given that DDPM generation is inherently a stochastic process, a more detailed explanation of this measurement technique would be beneficial.\"\n\n      **Response:** The MSE is computed by comparing the averaged pixel values of the generated images with those of the target images.\n\n      The goal of backdoor attacks on diffusion models is to create images with a predefined pattern triggered by specific backdoor text input. To evaluate the effectiveness of backdoor attacks, the assessment criteria should measure the similarity or distance between the generated backdoored images and the target images. Consistent with the approach outlined in [A], we employ MSE to quantify the distances between the generated images and the target images, as an approach of evaluating the effectiveness of backdoor attacks.\n      \n      Refs: [A] Chou et al, \"How to backdoor diffusion models?\", in CVPR 2023. \n\nWe will incorporate your constructive comments and our responses into the paper. An updated version of the paper, encompassing these clarifications and extensions, will be uploaded as soon as possible."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169798508,
                "cdate": 1700169798508,
                "tmdate": 1700169798508,
                "mdate": 1700169798508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GhVdsLk2RG",
                "forum": "BPHcEpGvF8",
                "replyto": "oOluSz06f6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response. While I maintain certain reservations regarding some of the conclusions and observations presented, I acknowledge that the author has addressed a number of my concerns. In light of this, I have adjusted my rating to a 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664049269,
                "cdate": 1700664049269,
                "tmdate": 1700664049269,
                "mdate": 1700664049269,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VBwR7i73JY",
            "forum": "BPHcEpGvF8",
            "replyto": "BPHcEpGvF8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_bWko"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_bWko"
            ],
            "content": {
                "summary": {
                    "value": "From the statistical perspective, this paper theoretically analyzed the efficiency of backdoor attacks. Specifically, focusing on the binary classification and generative model, the authors relied on two assumptions to calculate the tight lower and upper boundaries of the backdoor model\u2019s performance on the clean and poisoned test data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Their theoretical conclusion for the efficiency of backdoor attacks matches with the empirical results. For instance, the influence of the poisoning ratio and the magnitude of the trigger signal. Moreover, they also claimed that when fixing the poisoning ratio and the magnitude of the trigger, it is more efficient to choose the trigger along the direction the density of clean data drops quickly."
                },
                "weaknesses": {
                    "value": "One thing I want to mention is about the reference, as far as I know, there exist some references on the backdoor efficiency. The authors should cite them.\n[1] W. Guo, B. Tondi and M. Barni, \"A Temporal Chrominance Trigger for Clean-Label Backdoor Attack Against Anti-Spoof Rebroadcast Detection,\" in IEEE Transactions on Dependable and Secure Computing, doi: 10.1109/TDSC.2022.3233519.\n[2] Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all samples are born equal: Towards effective clean- label backdoor attacks. Pattern Recognition, 139:109512, 2023. 2, 3\n[3] Pengfei Xia, Ziqiang Li, Wei Zhang, and Bin Li. Data-efficient backdoor attacks. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 3992\u20133998, 2022"
                },
                "questions": {
                    "value": "Is it possible to extend this theoretical framework for multi-discriminator with more than 2 classes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8404/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8404/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8404/Reviewer_bWko"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698597774009,
            "cdate": 1698597774009,
            "tmdate": 1699637046832,
            "mdate": 1699637046832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q3WHXNfNUa",
                "forum": "BPHcEpGvF8",
                "replyto": "VBwR7i73JY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for the reviewer's time and effort in reviewing our work. \n\n1. \"One thing I want to mention is about the reference.\"\n    **Response:** We sincerely thank the reviewer for pointing out those related work. All three referenced papers focus on enhancing backdoor attack efficiency by the efficiency of backdoor attacks through strategic selection of data points for poisoning. This line of research indeed offers a valuable complement to our study that considers a random poisoning strategy. In particular, Gao et al. [2] and Xia et al. [3] propose selecting data instances that significantly influence the formation of the decision boundary in the learned model, while Guo et al. [1] suggest selecting data points that lie in close proximity to the decision boundary of the clean model.\n\n  In recognition of the relevance and contribution of these works to our research context, we have included a discussion of these works in the related work section of our paper. \n\n2. \"Is it possible to extend this theoretical framework for multi-discriminator with more than 2 classes.\"\n\n   **Response:** Thanks for your constructive comment. The short answer is yes. For example, one possible extension in multi-class scenarios is assuming that poisoning data have a common target label. The crux of our approach involves estimating the difference between the regression function on poisoned data, $f_*^{poi}$ , and that on clean data,  $f_*^{cl}$ (referenced in Eq. (1), (6), and (7) in the proof of Theorem 1). Since $f_*^{poi}$ stands for the regression function with respect to the distribution of poisoned data, which is known from the attacker's perspective, we can determine $f_*^{poi}$ in the multi-class cases as well. Although both $f_*^{poi}$ and $f_*^{cl}$ are multi-valued functions in the multi-class cases, we can bound their difference by controlling the difference at each coordinate respectively. Then, the methodology for estimating its difference from $f_*^{cl}$ and error bounds would follow the same logical framework as outlined in our paper.\n\nWe will incorporate your constructive comments and our responses into the paper. An updated version of the paper, encompassing these clarifications and extensions, will be uploaded as soon as possible."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169720884,
                "cdate": 1700169720884,
                "tmdate": 1700170315685,
                "mdate": 1700170315685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zGTB9DFO1z",
            "forum": "BPHcEpGvF8",
            "replyto": "BPHcEpGvF8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_DvbB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_DvbB"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducts a theoretical analysis of backdoor attacks, with a focus on addressing three key questions: (1) What are the factors that determine the effectiveness of a backdoor attack? (2) What is the optimal choice of trigger with a given magnitude? (3) What is the minimum required magnitude of the trigger for a successful attack? The paper utilizes finite-sample analysis to derive both upper and lower bounds for the success of a backdoor attack. The poisoning rate, trigger magnitude, and trigger direction are important factors influencing the success of a backdoor attack. Additionally, this paper carries out experiments on synthetic data as well as tasks involving image classification and generation. The empirical results validate the theoretical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a theoretical analysis on backdoor attacks, an important topic of machine learning security.\n\n2. A few factors that contribute to the success of a backdoor attack are studied in the paper. The choice of a trigger is particularly interesting. The insights shown in the paper can provide a theoretical guideline for further work.\n\n3. The empirical results on synthetic data validate the theoretical analysis and also provide an explanation for generative models."
                },
                "weaknesses": {
                    "value": "1. Some claims are not well validated empirically. The paper states \"a large backdoor data ratio \u03c1 will damage the performance on clean data.\" But there is no empirical evidence to support this claim. Also, according to the literature, a high poisoning rate usually does not significantly affect clean accuracy. It is recommended to empirically validate this claim and assess its consistency with the theories.\n\n2. The experiment conducted in Table 2 is not clear. What does the magnitude of backdoor triggers mean? Is it the L2 norm of \u03b7, or a fixed pixel value that replaces the original pixel on the input? How large is the backdoor trigger used in this study? In addition, the formalization of backdoor trigger as \u03b7 in X' = X + \u03b7 is not accurate. Backdoor attacks, such as BadNets replace the original pixel values with the backdoor trigger. Otherwise, the trigger pattern is not fixed and varies on different inputs.\n\n3. The paper seems to focus on dirty-label backdoor attacks, where the poisoned samples are assigned a target label. There is anther line of attacks that do not change the label, such as SIG [1] and reflection attack [2]. Is the proposed theoretical analysis applicable to these clean-label attacks?\n\n[1] Barni, M., Kallas, K., & Tondi, B. (2019, September). A new backdoor attack in cnns by training set corruption without label poisoning. ICIP 2019.\\\n[2] Liu, Y., Ma, X., Bailey, J., and Lu, F. Reflection backdoor: A natural backdoor attack on deep neural networks. ECCV 2020."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614450518,
            "cdate": 1698614450518,
            "tmdate": 1699637046682,
            "mdate": 1699637046682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IyaBIISde2",
                "forum": "BPHcEpGvF8",
                "replyto": "zGTB9DFO1z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for dedicating time reviewing our work. \n\n1. \"Some claims are not well validated empirically. The paper states \"a large backdoor data ratio \u03c1 will damage the performance on clean data.\" But there is no empirical evidence to support this claim. Also, according to the literature, a high poisoning rate usually does not significantly affect clean accuracy. It is recommended to empirically validate this claim and assess its consistency with the theories.\"\n\n   **Response:** Thanks for pointing it out. We have corrected our claim as \"a large backdoor data ratio $\\rho$ **could** damage the performance on clean data in some scenarios\".  From our finite-sample results, a larger $\\rho$  leads to the increase in both lower and upper bounds of prediction risk, thus the performance could be degraded. We have also performed additional experiments showing the influence of backdoor data ratio. Specifically, we perform BadNets attack on MNIST, replacing a 2 by 2 area at the lower-right corner with pixel value 5. The results are summarized in Table 1. As the poisoning ratio $\\rho$ increases, the clean data accuracy is pretty stable at the beginning, and then quickly drops when $\\rho$ approaches one. \n\n   Table 1. Effects of backdoor ratio on MNIST.\n   | Ratio \u2192     | 0.1% | 1%   | 5%   | 10%  | 50%    | 90%  | 95%  | 99%  |\n   | ------------| ---- | ---- | ---- | ---- | ----  | ---- | ---- | ---- |\n   | Clean Acc   | 0.99 | 0.98 | 0.98 | 0.98 | 0.98  | 0.94 | 0.91 | 0.42 |\n   | Backdoor Acc | 0.41| 0.92 | 0.98 | 0.99 | 1.0  | 1.0  | 1.0  | 1.0  |\n\n2. \"The experiment conducted in Table 2 is not clear. What does the magnitude of backdoor triggers mean? Is it the L2 norm of \u03b7, or a fixed pixel value that replaces the original pixel on the input? How large is the backdoor trigger used in this study? In addition, the formalization of backdoor trigger as \u03b7 in X' = X + \u03b7 is not accurate. Backdoor attacks, such as BadNets replace the original pixel values with the backdoor trigger. Otherwise, the trigger pattern is not fixed and varies on different inputs.\"\n\n   **Response:** We present detailed specifications for the setup in Table 2 as follows. In the case of MNIST, the backdoor triggers are 2 by 2 square patches, while for CIFAR-10, 3 by 3 square patches are utilized. All backdoor triggers are positioned at the lower-right corner of the inputs, replacing the original pixels with identical values. The pixel value represents the magnitude of the backdoor trigger. The poisoning ratio is consistently set at 5% for both datasets, and ResNet models are utilized in the experiments. \n   \n   This paper theoretically studies backdoor attacks with the formulation of $X^{\\prime} = X + \\eta$. While BadNets take the form of replacement, our formulation is consistent with the experimental setup for MNIST, where almost all images have pixel values of 0 at the lower-right corner. We will include this clarification in the paper.\n   \n3. \"The paper seems to focus on dirty-label backdoor attacks, where the poisoned samples are assigned a target label. There is anther line of attacks that do not change the label, such as SIG [1] and reflection attack [2]. Is the proposed theoretical analysis applicable to these clean-label attacks?\"\n\n   **Response:** Thanks very much for your insightful comment. The short answer is yes, our analytical techniques are indeed applicable to clean-label attack scenarios. Although our paper primarily focuses on dirty-label backdoor attacks, where the labels of poisoned samples are altered, the fundamental principles of our analysis can be seamlessly adapted to clean-label contexts.\n\n   Specifically, the crux of our approach involves estimating the difference between the regression function on poisoned data, $f_*^{poi}$ , and that on clean data,  $f_*^{cl}$ (referenced in Eq. (1), (6), and (7) in the proof of Theorem 1). Since $f_*^{poi}$ stands for the regression function with respect to the distribution of poisoned data, which is known from the attacker's perspective, we can determine $f_*^{poi}$ in the clean-label cases as well. Then, the methodology for estimating its difference from $f_*^{cl}$ and error bounds in the clean-label case would follow the same logical framework as outlined in our paper.\n\nWe will incorporate your constructive comments and our responses into the paper. An updated version of the paper, encompassing these clarifications and extensions, will be uploaded as soon as possible."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169673011,
                "cdate": 1700169673011,
                "tmdate": 1700169673011,
                "mdate": 1700169673011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GBGqqIGPxD",
                "forum": "BPHcEpGvF8",
                "replyto": "zGTB9DFO1z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate Reviewer DvbB for their insightful comments. In our response, we have addressed concerns regarding (1) experiments on the influence of backdoor data ratio, (2) clarification on experiment details, and (3) extension to clean-label attacks.\n\nAs the discussion phase nears its conclusion, we kindly inquire if the reviewer has any further comments on our response. We are readily available for any additional queries they may have.\n\nOnce again, we appreciate your time and effort in reviewing our paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622887717,
                "cdate": 1700622887717,
                "tmdate": 1700623430318,
                "mdate": 1700623430318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4RRnPmbGFF",
                "forum": "BPHcEpGvF8",
                "replyto": "GBGqqIGPxD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DvbB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DvbB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank authors for the response.\n\nI think the formulation of backdoor trigger is not quite accurate. MNIST is a very special case as it is in grey scale. But for RGB images such as CIFAR, there won't be all zero pixel values for a set of images at the same location.\n\nMy other concerns are addressed in the response. I will keep my original rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665872518,
                "cdate": 1700665872518,
                "tmdate": 1700665872518,
                "mdate": 1700665872518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wauxcm02eI",
            "forum": "BPHcEpGvF8",
            "replyto": "BPHcEpGvF8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies backdoor attacks with a constant trigger, assuming the trained classifiers are Bayesian optimal with respect to the poisoned training set.\n\nThrough this framework, they provide the following insights for backdoor attacks using a constant trigger:\n1. More backdoor data can harm clean performance and can help backdoor to success.\n2. Backdoor attacks can be more successful when the constant trigger has a larger magnitude.\n3. Backdoor attacks can be more successful when the direction of the constant trigger points towards less popular regions (i.e. regions with smaller density).\n4. Arbitrarily small backdoor data ratios may result in successful attacks.\n5. If there is a direction where for all samples the corresponding support of the marginal distribution is a single point, the magnitude of the trigger can be arbitrarily small to have a successful attack."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Theoretical understanding of backdoor attacks is an important topics.\n2. The authors demonstrate their skills in using statistical tools."
                },
                "weaknesses": {
                    "value": "While I appreciate the skills demonstrated by the authors, none of the obtained insights is interesting in a sense that they are either trivial or not true without assuming the model to be Bayesian optimal with respect to the poisoned training distribution. \n\nTo be specific, insight 1&2 listed in the above Summary section are trivial (even though it may generalize to other backdoor/poison attacks); Insight 3&4&5 are trivial only when assuming the model to be Bayesian optimal but may not generalize to other (actual) learning algorithms.\n\nTo sum up, my primary concerns regarding this submission include:\n1. Some key assumptions that oversimplify the problems and make the analysis probably irrelevant to practice, e.g. models are Bayesian optimal with respect to the poisoned distribution & Assumption 3 (Ordinary convergence rate) in the submission.\n\n2. Key insights are either trivial (insight 1&2) or likely not generalizable (insight 3&4&5).\n\n\nNotably, the experiments are thin but I find it acceptable for a theory paper. The major issue is not that experiments do not provide enough supports. The issue is that there is not really much insights worth supporting."
                },
                "questions": {
                    "value": "Please see the weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995907877,
            "cdate": 1698995907877,
            "tmdate": 1699637046561,
            "mdate": 1699637046561,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GJ5NTorXla",
                "forum": "BPHcEpGvF8",
                "replyto": "wauxcm02eI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for the reviewer's time and effort in reviewing our work. However, we think there are some misunderstandings in reviewer's comments and would like to clarify them.\n\n1. \"Some key assumptions that oversimplify the problems and make the analysis probably irrelevant to practice, e.g. models are Bayesian optimal with respect to the poisoned distribution & Assumption 3 (Ordinary convergence rate) in the submission.\"\n\n   **Response:** **First,** contrary to the reviewer's understanding, our work does not assume that models are Bayesian optimal. We focus on analyzing the prediction error of the poisoned model $\\hat{f}^{poi}$, which is evaluated as the difference between $\\hat{f}^{poi}$ and the unknown ground truth $f_*$. We kindly request the reviewer to reconsider our setup and theoretical results in this light. **Second,** our finite-sample analysis establishes error bounds on backdoor model performance without relying on Assumption 3. Our results quantitatively measure the impact of critical factors in backdoor attacks, such as data poisoning ratio and trigger direction. **Third,** under Assumption 3, we derive the rate of the poisoned model's risk in the asymptotic regime. This serves as a practical example of how our results can be applied for further analysis. Remark 4 clarifies that Assumption 3 is generally valid for commonly used function classes, such as the Holder class. **Fourth,** in practical applications where Assumption 3 cannot be verified or does not hold, we have conducted experiments to corroborate the developed theory and insights. Specifically, we performed state-of-the-art attacks (BadNets, WaNet, Adaptive Patch, and Adaptive Blend) on both MNIST and CIFAR10 datasets in Section 6.2.1, and the results align with our insights such as the attack is more successful when the trigger causes poisoned data and clean data are more separated in the sense of probability.\n\n\n2.   \"Key insights are either trivial (insight 1&2) or likely not generalizable (insight 3&4&5).\"\n\n     **Response:** Our insights are non-trivial in several aspects. **First,** we offer a **quantitative** analysis, beyond mere empirical observations, about the effects of backdoor attacks. For instance, instead of empirical observations such that a trigger larger in magnitude has a higher attack success rate, Theorem 3 specifies the minimum trigger length (i.e., $\\ln(n)$ ) necessary for a successful attack in Gaussian distribution scenarios. **Second,** our work is pioneering in theoretically studying the performance of backdoor attacks. The closest work, to our knowledge, is Manoj & Blum (2021), which focuses on the model\u2019s vulnerability, while ours is the first to quantify the effectiveness of these attacks. **Third,** the analytical techniques we used are also non-trivial. The main proof steps involve establishing the connection between the poisoned model and clean model, estimating the model deviation caused by backdoor data, and carefully optimizing the inequality so that the error bounds are tight in the sense of order.\n\n     The generalizability of our work are demonstrated in two aspects. **First,** we perform finite-sample analysis and provide tight error bounds with minimal assumptions, only requiring the loss function is Holder continuous and the input distribution is mostly concentrated in a bounded area. Those results can be the foundation for further analysis under various scenarios, including the asymptotic scenario discussed in our work.  **Second,** for practical applications where our technical assumptions may not be directly verifiable, we conducted experiments using both MNIST and CIFAR10 datasets and SOTA backdoor attacks (BadNets, WaNet, Adaptive Patch, and Adaptive Blend), and the results corroborate our theoretical findings.\n\nWe hope these clarifications address the concerns raised and further illuminate the significance and applicability of our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169635442,
                "cdate": 1700169635442,
                "tmdate": 1700169635442,
                "mdate": 1700169635442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bULeLyecH5",
                "forum": "BPHcEpGvF8",
                "replyto": "GJ5NTorXla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
                ],
                "content": {
                    "comment": {
                        "value": "If I understand correctly, a critical and specific claim in this rebuttal is that, and I quote, 'our work does not assume that models are Bayesian optimal'.\n\n**This is NOT true.** While I expect the authors to be more familiar with their own proofs, it is possible that the authors did not even realize they were making such an assumption during their analysis. Thus I will explain to you in details when you made this assumption for one of your results (actually), **as an example**. \n\nFor example, for the inequality in your Theorem 1,  \n$r_n^{cl}(\\hat{f}^{poi}) \\leq \\frac{1}{1-\\rho} r_n^{poi}(\\hat{f}^{poi}) + \\frac{C}{(1-\\rho)^\\alpha}\\left[\\max_{i=0,1}\\\\{h_i^\\eta(||\\eta||_2 / 4)\\\\}\\right]^\\alpha$, according to Appendix A:\n>The first term $\\frac{1}{1-\\rho} r_n^{poi}(\\hat{f}^{poi})$ \nis an upper bound of $E_{D_{\\eta}^{poi}}[E_{X\\sim\\mu_X^{cl}} \\\\{\\ell(\\hat{f}^{poi}(X),  f_*^{poi}(X))\\\\}]$, which is comparing the poisoned classifier \\hat{f}^{poi} and the **Bayesian classifier on the poisoned data** $f_*^{poi}$\n\n>The second term is an upper bound of \n$C\\cdot E_{X \\sim \\mu_X^{cl}}\\\\{|f_*^{poi}(X) - f_*^{cl}(X)|^\\alpha\\\\}$, which is comparing the **Bayesian classifier on the poisoned data** $f_*^{poi}$ to the clean Bayesian classifier $f_*^{cl}$.\n\nHowever, in Section 3, when deriving insights from this inequality, **the first term is ignored by claiming that 'For many classical learning algorithms, this statistical risk vanishes as the sample size goes to infinity' and only the second term is used in the analysis**. Thus the assumption is made here and your insights corresponding to this part are derived through assuming **Bayesian classifier with respect to the poisoned data**.\n\nFeel free to ask if this is not clear for you."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374639441,
                "cdate": 1700374639441,
                "tmdate": 1700374639441,
                "mdate": 1700374639441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bvyfn1Nk7q",
                "forum": "BPHcEpGvF8",
                "replyto": "FEHNa8Rn0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for their responses. \n\nWhile I agree it is not rigorous to simply use 'Bayesian optimal classifier' to describe your assumption, I believe the authors now understand my primary concerns based on their latest response and how it is not addressed.\n\n---\n\nHere are my comments:\n\nAgain, using Theorem 1 as an example. Yes, the proof of Theorem 1 does not rely on that Bayesian assumption of models, or as you would like to call it, vanishing statistical risk (with respect to a Bayesian model). **However**, after you ignore the first term in analyzing Theorem 1, any further insights you have are built on the assumption. In another word, the insights only apply to $f_*^{poi}(x) = \\frac{P_{\\mu^{poi}}(Y=1, X=x)}{\\mu_X^{poi}(X=x)}$ but not arbitrary models. At least for now, I do not find it well-motivated to claim that the insights from analyzing $f_*^{poi}$ are significant.\n\nI would suggest authors to properly include their assumptions when highlighting their theoretical results. \nTo summarize, while I totally believe the authors can be very skillful, I have major concerns regarding the significance of the insights (theoretical implications), especially given the primary assumptions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535729123,
                "cdate": 1700535729123,
                "tmdate": 1700535729123,
                "mdate": 1700535729123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jqkTPpD5Uh",
                "forum": "BPHcEpGvF8",
                "replyto": "wauxcm02eI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's time in reviewing our responses. We would like to clarify the misunderstanding in reviewer's comment saying that our results \"only apply to $f_*^{poi}$ but not arbitrary models.\"\n\n**First**, as acknowledged by the reviewer, our theorems are independent of the learning algorithms and thus the estimator (i.e., poisoned model) $\\hat{f}^{poi}$. We contextualized $\\hat{f}^{poi}$ in some concrete scenarios **to elucidate the potential implications of Theorem 1.** \n\n**Second**, the vanishing statistical risk, or consistency, meaning that $r_n^{poi}(\\hat{f}^{poi}) = E\\\\{\\ell(\\hat{f}^{poi},f_*^{poi})\\\\}$ will converge to zero as the sample size $n$ goes to infinity. Here, $f_*^{poi}$ denotes the underlying regression function with respect to the poisoned data distribution, **while $\\hat{f}^{poi}$ is the estimator we are analyzing**. In other words, the concept of consistency is specific to $\\hat{f}^{poi}$, and our insights are pertinent to any consistent estimator $\\hat{f}^{poi}$. As mentioned in the fourth point in the previous responses, **this consistency property is common for $\\hat{f}^{poi}$ obtained from various learning algorithms**, such as k-nearest neighbors, kernel estimates, and neural networks (see, e.g., [3]). Notably, most existing literature on backdoor attacks presupposes the use of neural networks as the learning algorithm, as seen in, e.g., [4,5]. As a result, our insights implied by Theorem 1 are applicable to most scenarios encountered in practice.\n\n[3] Gy\u00f6rfi L, Kohler M, Krzyzak A, et al. A distribution-free theory of nonparametric regression[M]. New York: Springer, 2002.\n\n[4] Li Y, Jiang Y, Li Z, et al. Backdoor learning: A survey[J]. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\n[5] Gao Y, Doan B G, Zhang Z, et al. Backdoor attacks and countermeasures on deep learning: A comprehensive review[J]. arXiv preprint arXiv:2007.10760, 2020.\n\nWe hope that these clarifications adequately address your concerns and look forward to continued dialogue."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541948157,
                "cdate": 1700541948157,
                "tmdate": 1700542176536,
                "mdate": 1700542176536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zRGKpmwHy9",
                "forum": "BPHcEpGvF8",
                "replyto": "wauxcm02eI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
                ],
                "content": {
                    "comment": {
                        "value": "I think one major disagreement here is whether we can consider this specific definition of 'vanishing statistical risk' used in this work as a weak assumption.\n\nIntuitively, I am conservative about such an assumption, especially in the context of deep learning and in the context of data poisoning and/or backdoor attacks. Here is a high-level explanation: In general, the behavior of learned models depend not only on data, but also on the prior knowledge encoded in the learning algorithms. For example, the design of convolutions in vision tasks incorporates the prior knowledge of translation invariance; When fine-tuning a pre-trained model, we are essentially using that pre-trained model as part of prior. In the context of poisoning/backdoor attacks, there are many filtering/detection based defenses and filtering/detection of poisoned/backdoor samples can never be possible without assuming some priors. Thus it looks logically problematic to claim that the 'vanishing statistical risk' defined in this paper is a weak assumption, since it is intuitively a 'distance' measure between a model and the $f_*^{poi}(x) = \\frac{P_{\\mu^{poi}}(Y=1, X=x)}{\\mu_X^{poi}(X=x)}$, which obviously fail to incorporate the effect of different priors.\n\nIt seems that authors are referring to [3] to support that the claimed 'vanishing statistical risk' is generally true for neural networks. It would help if the authors can (1) point out which part of [3] provides a proof for this; and (2) specify formally what is the theoretical assumptions for this claim (i.e. in what conditions this is true).\n\nI apologize to authors in advance since it is likely that I won't be able to reply very frequently in the following a day or two. But I will make sure to review any information you provide and help to reach a well-informed decision along with other reviewers.\n\n[3] Gy\u00f6rfi L, Kohler M, Krzyzak A, et al. A distribution-free theory of nonparametric regression[M]. New York: Springer, 2002."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543746362,
                "cdate": 1700543746362,
                "tmdate": 1700543907133,
                "mdate": 1700543907133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ABrdba8Qer",
                "forum": "BPHcEpGvF8",
                "replyto": "wauxcm02eI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8404/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for actively participating in the discussion. Your question on why a model does not incorporate priors can work well is actually an excellent illustration of the importance of statistical learning theory in understanding practical applications, highlighting the meaning and contribution of our work. In response, we focus on the following facts.\n\n1. **Consistency is a basic requirement for a \"good\" estimator, regardless of the underlying data distribution, or priors as you mentioned.** To see it, we quote the comments below from a highly-cited non-parametric regression book [3]:\n\n    > The first and weakest property an estimate should have is that, as the\n    > sample size grows, it should converge to the estimated quantity, i.e., the error of the estimate should converge to zero for a sample size tending to infinity.  [3, Section 1.6]\n\n    Specifically, let $m(x)=E(Y|X)$ for a distribution of $(X,Y)$ and $\\{m_n(x)\\}$ be a sequence of estimators given sample size $n$. With squared error loss, $m_n(x)$ is called weakly universally consistent [3, Section 1.6] if $\\lim_{n\\to\\infty} E\\{(m_n(X) - m(X))^2\\} = 0$ **for all distributions of $(X,Y)$ with $E(Y^2)<\\infty$**. Thus, a weakly universally consistent estimate $\\hat{f}^{poi}_n$ implies \"vanishing statistical risk.\"\n\n2. **Prevalence and proofs of universally consistent estimates in literature.** In particular, [3] gives proofs for k-nearest neighbors in Section 6.2, kernel estimates in Section 5.2, and two-layer neural networks in Section 16.2. More results on the consistency of deep neural networks, such as convolutional neural networks, are presented in works like [6, Section 17.1] and [7]. For rigorous derivations, we refer the reviewer to those works due to their technical complexities.\n\n    [3] Gy\u00f6rfi L, Kohler M, Krzyzak A, et al. A distribution-free theory of nonparametric regression[M]. New York: Springer, 2002.\n\n    [6] Anthony M, Bartlett P L, Bartlett P L. Neural network learning: Theoretical foundations[M]. Cambridge: Cambridge Univ. Press, 2009.\n\n    [7] Lin S B, Wang K, Wang Y, et al. Universal consistency of deep convolutional neural networks[J]. IEEE Transactions on Information Theory, 2022, 68(7): 4610-4617."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617648413,
                "cdate": 1700617648413,
                "tmdate": 1700617934383,
                "mdate": 1700617934383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]