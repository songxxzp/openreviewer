[
    {
        "title": "Ultra-sparse network advantage in deep learning via Cannistraci-Hebb brain-inspired training with hyperbolic meta-deep community-layered epitopology"
    },
    {
        "review": {
            "id": "JhWEJb7cXP",
            "forum": "iayEcORsGd",
            "replyto": "iayEcORsGd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1203/Reviewer_nVAm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1203/Reviewer_nVAm"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces sparse training in deep learning, aiming to replace fully connected neural networks with ultra-sparse ones inspired by brain networks. They propose Epitopological Sparse Meta-deep Learning (ESML) using Cannistraci-Hebb learning theory. ESML learns ultra-sparse hyperbolic topologies, showcasing meta-deep organization. Empirical experiments reveal ESML's ability to automatically sparse neurons through percolation. Cannistraci-Hebb training (CHT) is introduced and compared with fully connected networks on VGG16 and ResNet50. CHT acts as a gradient-free oracle, guiding link placement for sparse-weight gradient learning. It introduces parsimony dynamic sparse training by retaining performance through percolation and reducing node network size."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is praised for its density and detail.\n- The methods employed in the paper bring unique perspectives to sparse training.\n- The paper is commended for conducting various experiments, with promising results.\n- The inclusion of detailed experiment descriptions is noted as a positive aspect, providing transparency and allowing readers to understand the methodology.\n- The inclusion of visual illustrations is a positive aspect, aiding in the comprehension of dense presentation."
                },
                "weaknesses": {
                    "value": "- Simplifying the language could enhance accessibility to a wider audience.\n- Some results lack standard deviation reporting, raising questions about the reliability and robustness of those specific findings.\n- Addressing the legibility issue by enlarging text within figures would enhance the overall effectiveness of these visuals."
                },
                "questions": {
                    "value": "The proposed method appears effective in scenarios where neural networks are overparameterized. Does it maintain its efficacy when applied to non-overparameterized neural networks characterized by limited depth and width? Particularly, how does it perform in relation to a large dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1203/Reviewer_nVAm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698476758935,
            "cdate": 1698476758935,
            "tmdate": 1700703015852,
            "mdate": 1700703015852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FX4lZIu6Fg",
                "forum": "iayEcORsGd",
                "replyto": "JhWEJb7cXP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Reviewer 1203"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1203,\n\nThank you for your detailed review and insightful questions. We also thank your suggestions to improve the paper.\n\nBelow are the replies to your questions.\n\n**Weaknesses:**\n\n**1. Simplifying the language could enhance accessibility to a wider audience.**\n\n**Reply:** Because of the time limitation, we will continue to prepare a more simplified version of the language after this rebuttal. In addition, These are the actions that we implement or we intend to implement:\n+ We already simplified the message in the title of the article, proposing a new title. \n+ A commented video that shows how ESML shapes and percolates the network structure across the epochs for the example of Figure 2A is provided at this link  https://shorturl.at/blGY1\n+ we are planning to release in the final version of the article an Appendix with a glossary of the definition of the main new concepts such as: epitopological learning, network shape intelligence, Cannistraci-Hebb Network automata, meta-deep architecture, hyperbolic network topology, community network organization, etc\u2026 To this aim, we introduced in the new version of the Introduction section the following sentence: << To help the reader to get familiar with the concepts introduced in this article, we provide a glossary that summarizes their definitions in Appendix.>> \n\n**2. Some results lack standard deviation reporting, raising questions about the reliability and robustness of those specific findings.**\n\n**Reply:** Thanks. To address the reviewer\u2019s concern, we added the standard deviation of all the experiments.\n\n**3. Addressing the legibility issue by enlarging text within figures would enhance the overall effectiveness of these visuals.**\n\n**Reply:** Thanks, to address the reviewer\u2019s concern, we revised the figure and enlarged text within figures in the newly uploaded version of the manuscripts.\n\n**Questions:**\n\nThe proposed method appears effective in scenarios where neural networks are overparameterized. Does it maintain its efficacy when applied to non-overparameterized neural networks characterized by limited depth and width? Particularly, how does it perform in relation to a large dataset?\n\n**Reply:** Thanks to the Reviewer for this insightful comment. The largest dataset adopted in the previous version of the article is ImageNet2012 which is now still the biggest and comprehensive dataset in the computer vision field. However, to address the Reviewer's concerns we considered larger and more difficult datasets such as TinyImageNet instead of CIFAR10 on VGG16, for a more direct comparison with a new considered CNN model that is GoogLeNet. In general, the results show that CHT gains an ultra-sparse advantage on both VGG16 and GoogLeNet when applied to TinyImageNet.\n\nFinally, to investigate whether CHT maintains its efficacy when applied to non-overparameterized neural networks characterized by limited depth and width, we tested a scenario in which we applied CHT to CIFAR100 (for reason of time limitations we considered only this large dataset to attain results before rebuttal deadline) with ReseNet50 architecture whose intermediate layers depth was reduced from 4 to 2 (which is the minimum we can use).\n\nThe results of this test are provided in Appendix K in Table 2 and show that CHT performance is increased from around 78% to 80% of accuracy and the ultra-sparse network advantage is retained."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514400638,
                "cdate": 1700514400638,
                "tmdate": 1700514400638,
                "mdate": 1700514400638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AIperWpp4H",
                "forum": "iayEcORsGd",
                "replyto": "FX4lZIu6Fg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Reviewer_nVAm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Reviewer_nVAm"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your prompt attention to my concerns in this revision. The authors have addressed the majority of those concerns in this revision. As a result, I've adjusted my score to reflect the positive changes made by the authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703084284,
                "cdate": 1700703084284,
                "tmdate": 1700703084284,
                "mdate": 1700703084284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "29lNDcnJz5",
            "forum": "iayEcORsGd",
            "replyto": "iayEcORsGd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1203/Reviewer_cMYk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1203/Reviewer_cMYk"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about a new training methodology for deep learning called Cannistraci-Hebb training (CHT). CHT is a training methodology that uses epitopological sparse meta-deep learning (ESML) to learn artificial neural networks (ANNs) with ultra-sparse hyperbolic topology."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and all the methodological details are described clearly.\n2. CHT is a new 4-step training methodology for deep learning that uses ESML to learn ANNs with ultra-sparse hyperbolic topology.\n3. CHT has been shown to surpass fully connected networks on VGG16 and ResNet50.\n4. CHT could be used in a wide range of real-world applications, such as image classification, natural language processing, speech recognition, recommender systems, and medical diagnosis."
                },
                "weaknesses": {
                    "value": "1. For the node structure, this paper only compares CHT with fully connected graph, the author should take other graph structure into consideration, like ER, BA, WS graph. \n2. Compared with SOTA models, the performance of CHT is not competitive."
                },
                "questions": {
                    "value": "1. What's the inspiration of current architecture? \n2. Did you do extra experience on other graph structure? \n3. In Table 2 on page15. Why some accuracy result is not the best but still highlighted?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1203/Reviewer_cMYk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796396266,
            "cdate": 1698796396266,
            "tmdate": 1700688798415,
            "mdate": 1700688798415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oou9lLV8fn",
                "forum": "iayEcORsGd",
                "replyto": "29lNDcnJz5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Reviewer cMYk"
                    },
                    "comment": {
                        "value": "Dear Reviewer cMYk,\n\nThank you for your review of our paper. We appreciate your positive evaluation of the soundness, presentation, and contribution of our work, as well as the specific strengths you've highlighted. \n\nBelow are the replies to your weakness concerns and questions. \n\n**Weaknesses 1:** For the node structure, this paper only compares CHT with fully connected graph, the author should take other graph structure into consideration, like ER, BA, WS graph.\n\n**Reply:** We express our sincere gratitude for the reviewer's insightful feedback. To address the reviewer\u2019s concern, we have included a new section in Appendix L that delves into the selection of network initialization from a network science perspective. \nWe clarify that the ER model initialization is the one already used in our study and this is explained now better in Section 3.1 Epitopological sparse meta-deep learning (ESML) and section 3.2 CH training strategy. Revisions are highlighted in red.\n\nFurthermore, we have designed procedures to generate Watts\u2013Strogatz (WS) model and the Barab\u00e1si-Albert (BA) models on bipartite networks and we considered the following scenarios: static sparse training with BA, WS and ER (equivalent to WS for beta = 1); dynamic sparse training again with BA, WS and ER (equivalent to WS for beta = 1); CHT with initialization of the network using BA, WS and ER (equivalent to WS for beta = 1). We clarify that in our previous version of the study, we initialized our network employing Correlated Sparse Topological Initialization (CSTI) on layers that directly interact with input features from the dataset and apply Erdos-Renyi (ER) to intermediate layers. Now, thanks to the Reviewer suggestions, we could investigate to initialize the sparse network structure also by using the Watts\u2013Strogatz (WS) model and the Barab\u00e1si-Albert (BA) model.\n\nThe preliminary results that we gained on 3 datasets are consistent and show that in general the static sparse training via ER, BA or WS is outperformed by the respective dynamic sparse training that, in turn, is outperformed by CHT. In addition, in some cases, CHT initialized with BA and WS displays better performance than CHT initialized by ER or CSTI. We are expecting to obtain the results in the next few weeks on the other datasets and architectures, but we are quite confident that the trend obtained on the first 3 datasets is so consistent that might be confirmed also in the next datasets. \nWe thank again the Reviewer for these valuable suggestions. \n\n**Weakness 2.** Compared with SOTA models, the performance of CHT is not competitive.\n\n**Reply:** We thank the Reviewer because it allows us to discuss this point that certainly needs further clarification in the manuscript. To address the Reviewer's concern, we are adding this new text in the section on Discussion of Limitations and future challenges which is provided in Appendix C.\n\n<< With respect to the dynamic sparse training SOTA (RigL and MEST), the computational evidence in Fig. 3 on 5 empirical tests obtained in different combinations of network architectures/datasets (VGG16 on TinyImageNet; GoogLeNet on CIFAR100 and TinyImageNet; ResNet50 on CIFAR100 and ImageNet), demonstrate that CHT offers a remarkable ultra-sparse (1% connectivity) advantage on the fully connected baseline, whereas the current SOTA cannot. For the sake of clarity, we acknowledge that in this study we do not compare with the specific SOTA in the respective data or architecture types, because they often include methodologies with tailored technical features that are network and data-dependent. Including these specific SOTA, would make it difficult a fair comparison across data and networks because it would largely depend on the way we adapt the ultra-sparse training strategy to the specific case.  The attempt of this study is instead to evaluate whether, and in which cases, CHT can help to achieve ultra-sparse advantage in comparison to the fully connected baseline, regardless of the specific network or data considered>>"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513558507,
                "cdate": 1700513558507,
                "tmdate": 1700513558507,
                "mdate": 1700513558507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IbyhpFVXbq",
                "forum": "iayEcORsGd",
                "replyto": "29lNDcnJz5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Reviewer cMYk"
                    },
                    "comment": {
                        "value": "**Questions:**\n\n1. What's the inspiration of current architecture?\n\n**Reply:** We are grateful for this comment that we address in the Introduction section of the revised manuscript by introducing this paragraph:\n\n<< The inspiration behind this theory is that for many networks associated to complex systems such as the brain, the shape of their connectivity is learned during a training process which is the result of the dynamic evolution of the complex systems across the time (ref). This means that the evolution of the complex system carves the network structure forming typical features of network complexity such as clustering, small-worldness, power-lawness, hyperbolic topology and community organization (ref). In turn, given a network with a shape characterized by these recognizable features of complexity, a significant (better than random) part of its future connectivity can be predicted by local network automata which interpret the information learned in the engrams (memory traces) of the complex network topology (ref). Based on these notions, network shape intelligence is the intelligence displayed by any topological network automata to perform valid (significantly more than random) connectivity predictions without training, by only processing the input knowledge associated to the local topological network organization (ref). The network automaton training is not necessary because it performs predictions extracting information from the network topology, which can be regarded as an associative memory trained directly from the connectivity dynamics of the complex system (ref). >>\n\n2. Did you do extra experience on other graph structure?\n\n**Reply:** Please see the reply that we reported above to address the weakness 1.\n\n3. In Table 2 on page15. Why some accuracy result is not the best but still highlighted?\n\n**Reply:** We are sorry if this was not clear in the previous version of the manuscript. To address the reviewer\u2019s concern, we added the below content in all the captions of the Results Table. \n\n<<We executed all the DST methods in an ultra-sparse scenario (1%). The best performance among the sparse methods is highlighted in bold, and values marked with \u201c*\u201d indicate they surpass those of the fully connected counterparts.>>"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513723364,
                "cdate": 1700513723364,
                "tmdate": 1700657419584,
                "mdate": 1700657419584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eqhgb6PXwG",
                "forum": "iayEcORsGd",
                "replyto": "IbyhpFVXbq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Reviewer_cMYk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Reviewer_cMYk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your prompt response and extra experiences. The authors addressed most of my concerns. I'll consider to increase the score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688778868,
                "cdate": 1700688778868,
                "tmdate": 1700688778868,
                "mdate": 1700688778868,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mIO1F88lI1",
            "forum": "iayEcORsGd",
            "replyto": "iayEcORsGd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1203/Reviewer_M5bF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1203/Reviewer_M5bF"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new form of sparse network learning, which was brain-inspired. They compare to other methods in the field and evaluate on vision dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: I am not very familiar with the literature in this community, but from what I can tell the authors cite existing literature well and distinguish their approach adequately from other methods in the field.\nQuality: the paper is well written, and the empirical results are well done: they are thorough, include meaningful baselines, and cover a nice variety of datasets.\nClarity: the paper was well written\nSignificance: I am not familiar enough with this sub-field to judge, but given the result, this appears to be a solid step in the right direction, and is likely relevant to the subcommunity."
                },
                "weaknesses": {
                    "value": "While I understand the space constraints, having something like Table1 or Table2 in the main text (maybe in reduced form) would be nice."
                },
                "questions": {
                    "value": "No questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699023508452,
            "cdate": 1699023508452,
            "tmdate": 1699636046776,
            "mdate": 1699636046776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OOkSkMLciH",
                "forum": "iayEcORsGd",
                "replyto": "mIO1F88lI1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Reviewer M5bF"
                    },
                    "comment": {
                        "value": "Dear Reviewer M5bF,\n\nThank you for your insightful and constructive review of our manuscript. We greatly appreciate your recognition of its originality and quality. \n\nW1: While I understand the space constraints, having something like Table1 or Table2 in the main text (maybe in reduced form) would be nice.\n\nR1: Thanks for this value suggestion that we implemented by adding a reduced form of Table 1 directly below the Fig. 3, in order to simplify the reader in the visualizations and interpretations of the results. \n\nBest wishes,\n\nAuthors of Submission 1203"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513029861,
                "cdate": 1700513029861,
                "tmdate": 1700513057733,
                "mdate": 1700513057733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EgRREzTCtC",
                "forum": "iayEcORsGd",
                "replyto": "OOkSkMLciH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1203/Reviewer_M5bF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1203/Reviewer_M5bF"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for taking the time to responding to my review. Best of luck with your submission!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648073685,
                "cdate": 1700648073685,
                "tmdate": 1700648073685,
                "mdate": 1700648073685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]