[
    {
        "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"
    },
    {
        "review": {
            "id": "GUBKzQ9evq",
            "forum": "NL6bspkWft",
            "replyto": "NL6bspkWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_3Uq7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_3Uq7"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on open-vocabulary 3D instance segmentation. They introduce a new pipeline, namely, OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene understanding at the instance level. The OpenIns3D framework employs a \u201cMask Snap-Lookup\u201d scheme. The \u201cMask\u201d module learns class-agnostic mask proposals in 3D point clouds. The \u201cSnap\u201d module generates synthetic scene-level images at multiple scales and leverages 2D vision language models to extract interesting objects. The \u201cLookup\u201d module searches through the outcomes of \u201cSnap\u201d with the help of Mask2Pixel maps, which contain the precise correspondence between 3D masks and synthetic images, to assign category names to the proposed masks. This 2D input-free and flexible approach achieves state-of-the-art results on a wide range of indoor and outdoor datasets by a large margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Open-vocabulary instance segmentation is an important research topic in 3D scene understanding, this paper proposed a novel method to tackle this problem.\n\n- They propose a novel Mask-Snap-Lookup scheme, which distills knowledge from 2D foundation models to the 3D masked point clouds.\n\n- Their method outperforms the existing baseline approaches on several benchmarks, which demonstrates the effectiveness of their proposed method.\n\n- The paper writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The mask proposal module is adapted from an existing instance segmentation model Mask3D. Although the authors remove the class-specific information, essentially Mask3D is trained with a close set of categories. Hence the mask proposal module is not class-agnostic and the whole system is not an open-vocabulary system, as this system cannot handle the irregular point cloud clusters that don't belong to the indoor object classes. To evaluate this, I suggest the authors adapt their system to outdoor driving scenarios such as nuScenes, to see whether their approach can generate mask proposals of objects on a road such as traffic cones and barriers.\n\n- The authors need to compare with CLIP^2 [1] in Table 1, which can also generate open-vocabulary instance segmentation results.\n\n[1] Zeng et al. CLIP$^2$: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data. CVPR 2023."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1445/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1445/Reviewer_3Uq7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698226845405,
            "cdate": 1698226845405,
            "tmdate": 1699636073099,
            "mdate": 1699636073099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XoQbuFh9ge",
                "forum": "NL6bspkWft",
                "replyto": "GUBKzQ9evq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you so much for recognizing that 3D open-world instance segmentation is an important topic, as well as acknowledging the novelty and efficiency of our \"Mask-Snap-Lookup\" framework. We hereby address your other comments as below:\n\n1. Generalisation of Mask proposal Module\n\nIt is widely acknowledged that a substantial domain gap exists between indoor and outdoor scenes in 3D. This is why many attempts at 3D Foundation Models ([1],[2],[3])  and Unified 3D representation [4][5] models still require training separate weights for indoor and outdoor scenes.\n\nIt is true that generalizing the indoor scene trained Mask Proposal Module (MPM) to the outdoor scene would be a remaining challenge. However, we believe that within the same scene space, i.e., indoor or outdoor, the MPM possesses a decent level of generalization capability. To substantiate this claim, we conducted zero-shot experiments with OpenIns3D, trained on ScanNet, across four diverse types of indoor datasets. The visual results are presented in the following anonymous repository: https://anonymous.4open.science/r/rebuttal_image-6CBE/README.md. \n\n2. Comparison with CLIP^2\n\nWe thank the review for bringing our attention to this good work. Indeed, CLIP$^2$ can also generate instance-level results with the help of aligned 2D images. The results are rather good on ScanNetV2. We will update the paper and add this to the literature review.\n\nIntegrating the results into Table 1 might be challenging, though, as CLIP$^2$, unlike other 3D instance segmentation work, uses the top-1 accuracy metric rather than the Average Precision metrics. It would not be so cohesive to put it in Table 1.\n\nSpeaking of Table 1, an interesting insight that might be worth sharing is that, upon slight fine-tuning and testing (better mask proposal and better SNAP images), the performance of OpenIns3D on the novel 17 classes on ScanNet v2 can be massively improved compared to the original reported results. As shown in the table below. We think this indicates that the \"Mask-Snap-Lookup\" approach has huge potential that can be explored in the future.\n\n|                      |   AP  |  AP50 |  AP25 |\n|:--------------------:|:-----:|:-----:|:-----:|\n|  Mask3D + PointCLIP  |  3.20 |  4.50 | 14.40 |\n| OpenIns3D (reported) | 16.80 | 28.70 | 38.90 |\n|    OpenIns3D (new)   | **28.25** | **38.44** | **46.30** |\n\nWe hope this would be helpful, and please do let me know if you have any other questions about the work.\n\n\n[1] PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm\n\n[2] Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training\n\n[3] PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding\n\n[4] Uni3D: Exploring Unified 3D Representation at Scale\n\n[5] Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397534996,
                "cdate": 1700397534996,
                "tmdate": 1700397534996,
                "mdate": 1700397534996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4Srgc3SqvW",
            "forum": "NL6bspkWft",
            "replyto": "NL6bspkWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_A44f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_A44f"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel pipeline called OpenIns3D, which consists of three core steps: mask, snap, and lookup, eliminating the need for 2D image inputs and enabling 3D open-vocabulary scene understanding at the instance level.  This approach not only requires less rendering time and inference time but also achieves much stronger results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The experimental results are impressive. The OpenIns3D achieves superior quantitative results compared with other methods.\n2. The idea is interesting. The authors propose a novel framework that can achieve 3D open-vocabulary scene understanding without 2D images. \n2. This paper is well-written and maintains a smooth flow. The whole pipeline is easy to understand."
                },
                "weaknesses": {
                    "value": "1. As the method consists of multiple steps, the authors should provide more training details for all steps in the main text or appendix.\n2. Will the performance of 2D Open-world Detector influence the performance of the OpenIns3D? The authors seems not to provide experimental results in ablation study.\n3. Although the authors propose a 3D open-vocabulary scene understanding without 2D images, this method still needs well-prepared point clouds. It seems that 3D point clouds are also difficult to obtain in real world."
                },
                "questions": {
                    "value": "1. Input queries shown in experiments are a few words. The authors should provide additional experimental results on processing real whole sentences.\n2. The authors should provide visual comparisons with SOTA."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836737588,
            "cdate": 1698836737588,
            "tmdate": 1699636073037,
            "mdate": 1699636073037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xChbLqSwQD",
                "forum": "NL6bspkWft",
                "replyto": "4Srgc3SqvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe would like to start by thanking you for your feedback and your support of our work. It is great to see that you found the idea interesting. We reply your comments as below.\n\n## More details in the paper\nAs per our response to reviewer (rePE), we will include more information about Mask Scoring Module, as well as the Snap module in the revision (will be added later). More details about these modules can be found in the reply to the reviewer (rePE).\n\n## Will the 2D performance influence the performance of 3D? \n\nIndeed. This is because we completely outsource the detection tasks to 2D model. We provided a small piece of comparison results in Table 2, but here we offer a direct comparison of OpenIns3D using two different backbones, as shown in the table below.\n\n|Methods|Rendering|2Dbackbone|AP50|AP25|\n|-|-|:-:|:-:|:-:|\n|Mask-P-CLIP|Mask|CLIP|4.5|14.4|\n|OpenIns3D|Scene-level|Grounding-DINO|18.8(+14.4)|29.8(+15.4)|\n|OpenIns3D|Scene-level|ODISE|28.7|38.9|\n\nAn important note here is that once the rendering is switched to the scene level, the general performance improves significantly compared to that of the object/mask-level rendering, regarding less the 2D model used.\n\nA further note is that relying on the 2D detector, on the other hand, is also a good strategy, as their evolving speed is generally faster. This ensures that OpenIns3D can also evolve with the 2D detector. This design also offers more special abilities, such as understanding complex sentences when the 2D detector is integrated with the LLM. We demonstrated this in the paper in Figure 1 and 6.\n\n##\tDoes the point cloud need to be well-prepared? \n\nTo a certain extent, yes. At least RGB information is required, which is very common for all point clouds.\nTo demonstrate the performance of OpenIns3D on various point cloud datasets, we conducted quick zero-shot experiments on four diverse types of datasets. We showcase the results in the following **anonymous repository**:\n\n**https://anonymous.4open.science/r/rebuttal_image-6CBE/README.md**\n\nPlease feel free to check it out. \n\nIn conclusion, since OpenIns3D does not rely on aligned images, it can be deployed and used for 3D data rapidly. It also exhibits a relatively good generalisation capability on various datasets.\n\n## Questions:\n- Can the model work on long sentence queries? Yes, and this might also be a compelling advantage of OpenIns3D. While other CLIP-based models would perform reasonably on singular vocabulary or phrases, OpenIns3D, equipped with the right 2D detector model, can understand long sentences and carry out segmentation tasks. \n\n- Due to time constraints in the rebuttal period, we might not be able to include a comparison with other state-of-the-art models, but we will certainly include this as future work.\n\nThanks again for your input, and please let us know if you have any further questions regarding the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255886748,
                "cdate": 1700255886748,
                "tmdate": 1700255886748,
                "mdate": 1700255886748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6YmEZCrFrK",
                "forum": "NL6bspkWft",
                "replyto": "xChbLqSwQD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_A44f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_A44f"
                ],
                "content": {
                    "title": {
                        "value": "Official comments by Reviewer A44f"
                    },
                    "comment": {
                        "value": "Thanks for the author\u2019s detailed responses. Based on other reviewers\u2019 feedback and the authors\u2019 responses, I will keep my rate."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661332120,
                "cdate": 1700661332120,
                "tmdate": 1700661332120,
                "mdate": 1700661332120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VjMYd2xRwF",
            "forum": "NL6bspkWft",
            "replyto": "NL6bspkWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for open-vocabulary instance segmentation. It consists of three main steps, first around a 3D point cloud of a scene, several virtual cameras are placed (all pointed inwards) to record several synthetic images of the scene (snap). Some 2D open-vocabulary segmentation or detection approach is then applied to these images to find the sought after objects. In parallel, a class agnostic variant of Mask3D is used to extract object proposals (mask). Finally, the obtained class agnostic masks are matched to the obtained open vocabulary instances to assign them to a class (lookup). The key difference to previous methods is that this approach does not require aligned RGB images to be present in the data, but rather relies on synthetically created images to be fed into a 2D vision-language model. Compared to previous open vocabulary methods evaluated on ScanNetV2, S3DIS, and STPLS3D, the method achieves better performances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing of the paper is easy to follow.\n- The paper tackles the interesting task of OV point cloud instance segmentation.\n- The scores compared to some baselines look promising, even without the use of 2D images."
                },
                "weaknesses": {
                    "value": "- At its core, the method is built on top of a somewhat flawed assumption. How can we obtain RGB point clouds, without actually having aligned RGB images? Of course there might be LiDAR point clouds without aligned RGB images, but at that point we can also not create synthetic RGB images from an uncolored point cloud, to feed into a 2D model expecting RGB images. While I still see some potential benefit, like being able to render novel images that are more focused on certain objects or better suited for the downstream model, this aspect is not explored here. I can't really imagine another case, where we have RGB point clouds, but for some reason we had to delete each and every underlying RGB frame used for colorization. As such I don't see the true merit of this approach.\n\n- Ignoring the above issue, the \"mask\" module is not novel, it's very similar to the one used in OpenMask3D with some mask filtering from SAM, the \"lookup\" module is fairly simple matching and not super interesting as far as I can tell. Leaving the \"snap\" module to be the core novelty of this approach. The way it is described initially: \"Multiple synthetic scene-level images are generated with calibrated and optimized camera\nposes and intrinsic parameters.\" gives the impression that the camera poses and intrinsics might somehow be optimized on a scene by scene basis, to create the best possible synthetic images for a given scene, resulting in the best scores. However, I understand this module simply places a predefined number of cameras around the scene, looking at the center and optimizing the focal length according to some heuristics. The slightly more involved \"local enforced lookup\" is mostly explained in the supplementary, raising the question what the real novelty of this paper is supposed to be? Finally, the rendered images are still just points splatted to the camera? According to \"Challenge 4\" there is a domain gap between projected and natural images, which I agree with, but as far as I can tell, the paper does nothing to truly bridge this gap, apart from aiming virtual cameras in a certain way?\n\n- The training of Mask3D for class mask predictions can have a significant effect on how truly \"open vocabulary\" the downstream method really is. The way all of these methods are presented is that one can easily find objects as long as we can describe them. This method relies on Mask3D being able to create class agnostic masks for the objects though. To thus evaluate how well it generalized to novel classes, Mask3D should not be trained on these classes. Now I'm assuming that Mask3D was trained to exactly create proposals for the classes also evaluated on, which makes the whole evaluation questionable. The same weakness is actually also a problem in the evaluation on OpenMask3D. While the paper does show some qualitative results in the teaser figure, this does not really prove the overall generalization. To properly show this, it could have been a great opportunity to simply evaluate on ScanNet200, given this should not require any novel training.\n\n- In general the lack of evaluation on a larger set of classes such as ScanNet200 is a clear weakness in my opinion. In fact, if you can provide such results in the rebuttal, without retraining (which should thus be fast), I would be happy to change my opinion. This would also allow a direct comparison with OpenMask3D. And yes, while one might claim that this was not published prior to the ICLR deadline, the paper clearly acknowledges OpenMask3D exists by citing it and even contains a figure about it. As such a simple comparison would be more than fair.\n\n- An actual ablation about how real images compare to synthetic images for this would have been very valuable. An interesting setup could have been to use a set of real images from the dataset and compare it to rendered synthetic images using the same camera pose and intrinsics.\n\n- Certain parts of the text make it seem like it's a bad thing to use a pre-trained network such as 3DETR or DETR, clearly stating that MPM is trained from scratch (start of chapter 5). This feels a bit hypocritical given that this whole method relies on pretrained 2D VL models trained on huge amounts of data. In general I must sadly say the text seems to contain a lot of \"fluff\" for me, introducing new fancy names for all kinds of stuff."
                },
                "questions": {
                    "value": "- I think that the references to Table 5 and 6 in the text are swapped.\n\n- The introduction states \"and point clouds generated by photogrammetry frequently lack depth maps\", how is this relevant here? And is a point projected to an image not a sparse depth map?\n\n- As suggested above, an actual evaluation on ScanNet200 would make a major difference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849446608,
            "cdate": 1698849446608,
            "tmdate": 1699636072952,
            "mdate": 1699636072952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cgme1LU2qG",
                "forum": "NL6bspkWft",
                "replyto": "VjMYd2xRwF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the insight feedback, the following are our response:\n\nWeakness session:\n\n## Item 1:\n\nThe reviewer argues that since RGB point clouds are derived from RGB images (a point we acknowledge), developing a method that intentionally excludes the use of these 2D images for 3D scene understanding may not be considered meaningful (a viewpoint we strongly disagree with). Following this line of reasoning, it implies that the majority of point cloud processing methods, ranging from PointNet to PointTransformer, SpareNet, and Mask3D, which solely utilize 3D RGB point clouds as input without incorporating 2D images, do not offer a clear advantage over methods that demands both modalities as input. This argument is not valid logically.\n\nWhen OpenIns3D will be useful: \nImagine you've been given only a RGB 3D point cloud/mesh and need to conduct 3D open-world instance understanding. OpenIns3D remains easily applicable, while other methods may suffer.\nMore use cases, are explained in the introduction session, here we provide a condensed summary. \n- LiDAR point cloud often exists without matched 2D images. \nExample: Semantic3D, Tornoto3D, Paris-carla-3d, etc.\n- Photogrammetry point cloud often exists without depth maps. [without depth maps it is hard to build point-pixel correspondence]\t\nExamples: Sensaturban, Campus3d, STPLS3D, etc.\n- Point cloud generated from the registration of multiple scans or converted from 3D simulations/CAD models typically lacks 2D images.\nExample: SynthCity, etc.\n\n(Note: Example datasets are publicly available dataset under these described circumstances.)\n\nIn real Application, we may loss of pose, intrinsic, and depth information in the transferring process; we may remove of 2D images to reduce storage requirements. Example: ScanNetv2, 3D scan (3MB), image package (3GB). Clearly saving a 3MB mesh is easier than saving entire 3GB images.\n\n## Item 2:\n\nBefore addressing any questions related to this item, we wish to emphasize our belief that good and meaningful research lies not in proposing sophisticated methods but in suggesting simple yet effective approaches that yield sustainable gains for the tasks and new insights for the community.\n\n- Substantial Gains:\nOpenIns3D has proposed a 2D input-free framework, which is not only more flexible to use but has also achieved a wide range of state-of-the-art results in indoor and outdoor, instance segmentation and object detection tasks. This success is evident even when compared with methods using original 2D images.\n\n- New Insights: One of the main insights of \"Mask-Snap-Lookup\" is that scene-level rendering scheme is beneficial for 3D open-world understanding. Because scene-level images enable all masks to be rendered and comprehended at once, massively reducing inference time while providing robust results. Consider having 100 masks in a scene; other approaches that handle each mask one by one need to render/crop 200-400 images, while scene-level rendering approach only requires 8-16 images, which provide a significant benefits in overall runtime. Meanwhile, this new pipeline can seamlessly evolve with 2D open world models without retraining, and handle super complex text input when integrated with an LLM-powered 2D detector.\n\nThe Mask module indeed leverages the strengths of both Mask3D and SAM, and our scoring and filtering design makes it work really well, providing a solid foundation for further steps. \nAs previously explained, the Snap-Lookup module is designed to enable scene-level rendering. The lookup approach is not a simple match. The key idea behind the Lookup module is the construction and use of a mask2pixel map. 2D models work best when the image contains more background context and mask2pixel map allows us to search precisely in a global image for a specific 3D mask. We believe this design would be very useful for many 3D open-world models intending to leverage 2D image results. \n\nGap with the natural image: Point clouds are rendered into images using ray tracing add proper lighting rather than simple point projection.\n\nAs explained in the appendix (Fig 9), when rendering images for each mask individually\u2014whether using PointClip rendering or LAR rendering\u2014these images are challenging for 2D models to comprehend. Transitioning to scene-level rendering significantly enhances the ability of 2D detectors to recognize objects in the image. This narrowing of the gap is strongly supported by experiments on outdoor datasets STPLS3D, where sparse point clouds with scene-level rendering also yield good results with 2D open-world models.\n\nThe Local Enforced Lookup is arranged in the appendix due to space constraints. We prioritise presenting the complete flow of the Mask-Snap-Lookup module, which is proved to be effective and is what distinguishes OpenIns3D.\n\nWith these explanations, we hope the reviewer could appreciate the merits of OpenIns3D a bit better. We will address other comments in the next response. Thanks"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699679513128,
                "cdate": 1699679513128,
                "tmdate": 1699679513128,
                "mdate": 1699679513128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sa7A2E21z2",
                "forum": "NL6bspkWft",
                "replyto": "Cgme1LU2qG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "content": {
                    "comment": {
                        "value": "I'll try to quote parts of your comments and respond inline.\n\n---\n\n*Following this line of reasoning, it implies that the majority of point cloud processing methods, ranging from PointNet to PointTransformer, SpareNet, and Mask3D, which solely utilize 3D RGB point clouds as input without incorporating 2D images, do not offer a clear advantage over methods that demands both modalities as input.*\n\nI disagree here, these methods don't use the images, but they also do not claim that it is a major benefit not using the additional modality. This is the major difference to this submission.\n\n---\n\n- *LiDAR point cloud often exists without matched 2D images. Example: Semantic3D, Tornoto3D, Paris-carla-3d, etc.*\n- *Photogrammetry point cloud often exists without depth maps. [without depth maps it is hard to build point-pixel correspondence] Examples: Sensaturban, Campus3d, STPLS3D, etc.*\n- *Point cloud generated from the registration of multiple scans or converted from 3D simulations/CAD models typically lacks 2D images. Example: SynthCity, etc.*\n\nIf a colored point cloud exists, it's either synthetic, which means you could in principle create 2D images too, or there were 2D images at some point (and they might be lost). I feel it's important here to make clear that one might consider storage constraints or wants to work with old datasets, but if storage is no issue, there is no realistic scenario where 2D images don't exist, but a colored point cloud does. So I strongly dislike how this is argued in the paper, as well as in this comment, indicating that in any practical scenario colored point clouds without 2D images are realistic. I can follow along with the storage space argument, but that is not the argument in the paper.\n\n---\n\n*Substantial Gains*\n\nAs pointed out before, I'd like to see a ScanNet200 comparison. If the approach works as advertised, this should be super fast to compute.\n\n---\n\n*New Insights*\n\nI agree that those insights are somewhat interesting, but you write a paper about a whole \"mask-snap-lookup\" pipeline where most components are not very novel or interesting and then move the actual interesting and potentially novel contributions to the supplementary material. Would this paper have been written in a better structure with the same results, I might agree that it should be accepted. But in my opinion, accepting the paper in the current state, signals that the whole pipeline is indeed good and/or novel and warrants publications, whereas what is actually interesting is just a specific part of the paper that is completely out of focus.\n\n---\n\n*Gap with the natural image: Point clouds are rendered into images using ray tracing add proper lighting rather than simple point projection.*\n\nWhere can I find this information in the paper? Did you evaluate this? Is this important?\n\n---\n\n*The Local Enforced Lookup is arranged in the appendix due to space constraints. We prioritise presenting the complete flow of the Mask-Snap-Lookup module, which is proved to be effective and is what distinguishes OpenIns3D.*\n\nAs pointed out above, I don't agree this is good. The core of the paper should be the main contribution. The paper could roughly outline the whole flow on a single page with a more detailed explanation in code/supplementary, while highlighting the actually interesting parts with valuable experiments. A lot of space is lost in the main paper to some nice storytelling that is not needed to highlight the value of interesting core contributions.\n\n---\n\nSo I still have large concerns with this paper. I think a lot of the claims/arguments are somewhat misleading or not precise and the structure gives weight to the wrong contributions. While I do agree that some of the evaluations in the supplementary are actually interesting, I don't think the overall paper in it's current form should be published."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064895984,
                "cdate": 1700064895984,
                "tmdate": 1700064895984,
                "mdate": 1700064895984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6eQFjMMIzb",
                "forum": "NL6bspkWft",
                "replyto": "P87u4ZE61M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "content": {
                    "comment": {
                        "value": "# ScanNet200 Results\nThe ScanNet200 results for me clearly indicate OpenIns3D has a big issue with generalizing to the tail common and tail classes. While OpenMask3D is by far not perfect, its performance drops significantly less. This should be the core of any open vocabulary method. So I don't find these numbers very convincing. And if there is a clear weakness to a system, I don't think it's good to point this out somewhere deeply hidden in a huge appendix. Regardless of that, just because something is a tail class, doesn't obviously make it a small class, so this weakness would have to be investigated more closely.\n\nI'm also somewhat confused by the fact that Table 3 in the above comment lists higher performances for the novel classes than for the base classes. How is that explained?\n\n# Generalization\nIndeed Table 3 suggests that Mask3D can generalize, but as I pointed out above, I am very confused by the fact that it works better on novel classes. This is very much not in line with the overall degrading performance on novel classes in the tables before. The additional visualizations are beside the point here, given that this just shows that Mask3D can generalize somewhat across datasets, but it doesn't indicate how well it is able to propose class agnostic masks for new classes. (Which is also the case for Table 6)\n\nI also disagree with your points about Mask2Former and SAM. I am not aware of class agnostic Mask2Formers and SAM was trained in a very different way altogether. So the only real evidence for generalization to new classes is table 3 and I'd like to hear your comment on why it actually works better for novel classes."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593545315,
                "cdate": 1700593545315,
                "tmdate": 1700593545315,
                "mdate": 1700593545315,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p6ra5vUNY4",
                "forum": "NL6bspkWft",
                "replyto": "NCUneInNpc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "content": {
                    "comment": {
                        "value": "# Using original image's pose and intrinsic\nFor the sake of ablation studies, I don't care about scenarios where 2D images are not available. For ScanNet it would have been easy to compare OpenIns3D with existing camera intrinsics. You focused on object specific views and views from around and above the scene, however, the third and to me very natural option would be views from inside the scene as used during data recording. This would have given you the option to directly compare how well your rendering compares to the original images.\n\n#  Pretrained 3D DETR\nStill for most of the experiments you use a pretrained Mask3D model for the proposals, trained on ScanNet, evaluated on ScanNet, so in essence you do the same thing. I see that you took out the sentence about training from scratch though, so this point is indeed solved.\n\n# Last point\nEvery paper out there probably has a long story to it, with many failed attempts. If you want to tell them, tell them in a blog post, or a talk, or in the *appendix*. I strongly disagree that the front and center part of the paper should be used for this narrative. A lot of people will read a paper without the supplementary, especially when the supplementary is so long as the one of this paper. The story that this main paper tells, is simply not convincing to me. The masking module is not new, the way the main paper introduces the lookup module is extremely simplistic and suggests there is nothing new to it, and only the snap part feels somewhat interesting, but again, the only interesting stuff to me is in the supplementary. Furthermore, information about how you actually render the images, which feels very crucial, are not present at all."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594821647,
                "cdate": 1700594821647,
                "tmdate": 1700594821647,
                "mdate": 1700594821647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zIaCXw001p",
                "forum": "NL6bspkWft",
                "replyto": "QIRKumsZhz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
                ],
                "content": {
                    "comment": {
                        "value": "- Thank you for actually providing ScanNet200 results. I sadly don't feel they are very supporting of the method though, given how the performance drops for the less common classes. To me this means the method is not truly open vocabulary, since it only works for a rather constrained set of classes.\n\n- Yes, OpenInst3D works without 2D images, that is fully clear, this can be highlighted. I still find the setup contrived. I see you updated the introduction w.r.t. this. I still find this section partially to be problematic though. LiDAR datasets without images cannot be used by OpenIns3D because they have no color. Just because the images are not released, does not mean they don't exist. The way the paper is written suggests that I can now use a simple point cloud and that is simply *not true*. I see the arguments about storage space/bandwidth, but these should not be mixed! *If you want to use OpenInst3D, you either need to record 2D RGB(-D) images, or use fully synthetic colored data.* This to me also renders results on ARKitScenes irrelevant, you cannot get these point clouds without RGB images!\n\n\n\nIn Summary, I disagree with the structure of the paper, the interesting parts are in the supplementary material and the main paper does not give me a good understanding about why the method should be relevant or why this specific proposed method should be used. That has to be gathered from the supplementary which is still not complete, nor does the paper do a good job of pointing out which exact deeper insights can be found in there. I think that the ScanNet200 results should be the central part of the motivation, but they were only added to the supplementary, without a reference to them in the main paper. As such I will remain with my initial rating, but I somehow doubt that this will make a major difference given all the other reviews are generally more on the positive side. I still think the paper could have been way stronger with a slightly different motivation and a significantly different structure."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595834529,
                "cdate": 1700595834529,
                "tmdate": 1700595834529,
                "mdate": 1700595834529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7hReWjFoEU",
            "forum": "NL6bspkWft",
            "replyto": "NL6bspkWft",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_rEPE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1445/Reviewer_rEPE"
            ],
            "content": {
                "summary": {
                    "value": "OpenIns3D, a RGB-independent framework, addresses point cloud-based instance segmentation through the introduction of a multiple stages approach. Firstly, it generates mask proposals based on point clouds, rendering scene images following a specified strategy. Subsequently, 2D vision language models are employed to extract objects. Finally, the assignment process between 2D and 3D segments is applied. Experimental results underscore that OpenIns3D substantially outperforms previous baseline models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed framework focuses on an RGB-agnostic setting and achieves precise 3D instance segmentation through a multi-stage approach.\n\n- The framework primarily relies on 3D proposals, establishing connections between 2D and 3D segments, and subsequently employs filtering operations that effectively leverage large-scale 2D vision models.\n\n- Experimental results, when compared to those presented in previous papers, clearly illustrate a remarkable enhancement in performance."
                },
                "weaknesses": {
                    "value": "- The Mask Proposal Module is trainable using IoU as a form of supervision. It is strongly recommended to include comprehensive training details in the main draft of the paper.\n- Further clarification is needed regarding the adjustment of camera parameters in the Camera Intrinsic Calibration process.\n- It is advisable to incorporate a comparative analysis that includes segmentation results obtained from multiple pseudo-projected images. Given that your method heavily relies on prior knowledge from 2D models, solely comparing it with point-based methods may not provide a fully equitable evaluation.\n- The performance gain from 2D models or framework design should be separately discussed."
                },
                "questions": {
                    "value": "- Please provide a detailed explanation of the several modules proposed in the paper, especially in addressing Weaknesses 1 and 2.\n\n- Please include further comparisons with image-based segmentation. This can be achieved by projecting the point cloud onto different cameras and then unprojecting it back to the initial point clouds. Such additional analysis would help demonstrate the performance of the proposed method and offer valuable insights into why it may perform better or worse compared to pure image-based segmentation, and the gain from 2D models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699053972884,
            "cdate": 1699053972884,
            "tmdate": 1699636072892,
            "mdate": 1699636072892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QPJ5Fkpmut",
                "forum": "NL6bspkWft",
                "replyto": "7hReWjFoEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1445/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our gratitude for the detailed review and your support of our work. We believe that OpenIns3D, as the first 2D-input-free framework for 3D open-world scene understanding, will not only set new benchmark results but also offer a fresh perspective to the domain. Our response:\n\n# More training details of Mask scoring & Intrinsic parameter calibration:\nThanks for your suggestion. We have explained the details as follows and will include this in the revision (will update later).\n- Mask scoring: For each of the $N$ proposed masks, we utilized the Hungarian Match to match it with $n$ GT masks and calculated the IOU value between the each pairs (n). For the remaining masks $(N-n)$ that did not match with GT, we set the IoU value as zeros. With this method, a $N$ GT IoUs for all proposed masks are obtained. During training, a simple two-layer MLP is applied to process mask queries and predict their IoU value. This simple two-layer MLP is supervised by the $L_2$ loss between predicted IoU and GT IoU, as depicted in formula (1) in the main paper.\n\n- Intrinsic parameter calibration: \nOnce the pose matrix $Pose$ is obtained via the $Lookat$ function (A.2 in appx), we initialize the Intrinsic with an ordinary intrinsic matrix. With this parameter, we project all points into the images. This will lead to randomly positioned scene images. Once this is obtained, we uniformly rescale the values of fx, fy, cx, and cy in the initialised intrinsic matrix by the same factor. This process merely repositions and rescales the projected image. To illustrate, if the original projected point was located in image coordinates within the range of [-1000, -192] in x, our calibrated intrinsic metrics transform it to [0, 2000] in x.\n\nWe will update these parts in the the main paper later. Thanks again for the suggestion.\n\n#  Further Comparison with Image-based segmentation\n\n We will answer Weakness 3 and Question 2 in this section as they are closely related\n\n- Comparing with non-point-based methods: \nIn general, most 3D open worlds rely on 2D images, making it challenging to categorize them as truly point-based. If considered solely from an inference perspective, PLA-family and OpenScene might be point-based methods, although both heavily rely on 2D images during training. Methods like PointClip, PointClipV2, and OV-3DET are derived from multiple projections/croppings, and their comparisons are included in the paper. We present these comparisons in Table 7 (OVOD), Table 3 (outdoor dataset), ablation studies on Table 2, and pre-class results on Tables 10 and 11.\n\n-\tPros and Cons when comparing with pure-image projection-based methods:  For various pseudo-image projection methods, we showcase six attempts we made in Table 12 and compare them based on runtime and performance. Our scene-level rendering also proves to be robust in both performance and fast inference time. The downside, though, as we stated in the weakness session [point 3], is that it might not work well on small objects, as the visual information of small objects is diluted or lost in the 3D reconstruction process. For a more detailed analysis of this point, we compared our method with OpenMask3D on ScanNet200, as shown in the table below. OpenIns3D achieves competitive performance in the Head category (top 1/3 classes, generally medium size object) but shows weaknesses in common and tail categories. OpenMask3D achieves strong results in common and tail categories, at the cost of requiring well-aligned 2D image as input during inference. Compared with OpenScene, **in cases where only 3D is available during inference, OpenIns3D surpasses OpenScene in all categories.**\n\n|Model|Require Aligned Images in training|Require Aligned Images in Inference|Head AP|Common AP|Tail AP|AP|AP50|AP25|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|OpenScene(2DFusion O-Seg)|Yes|Yes|13.4|11.6|9.9|11.7|15.2|17.8|\n|OpenScene(2D/3DEns.)|Yes|Yes|11.0|3.2|1.1|5.3|6.7|8.1|\n|OpenScene(2DFusionL-seg)|Yes|Yes|14.5|2.5|1.1|6.0|7.7|8.5|\n|OpenMask3D|Yes|Yes|17.1|14.1|14.9|15.4|19.9|23.1|\n|OpenScene (3DDistill)|Yes|**No**|10.6|2.6|0.7|4.8|6.2|7.2|\n|OpenIns3D|**No**|**No**|**16.0**|6.5|4.2|8.8|10.3|14.4\n\n# performance gain from 2D models\n\nWe conducted ablation experiments on various 2D models and evaluated their performance on the ScanNet20 dataset. This is included in Table 2. The following is a breakdown version of it. Notably, when we switched the rendering method to scene-level rendering, there is a significant increase in performance. This effect underscores the usefulness of scene-level rendering for 3D open-world understanding.\n|Methods|Rendering|2Dbackbone|AP50|AP25|\n|-|-|:-:|:-:|:-:|\n|Mask-P-CLIP|Mask|CLIP|4.5|14.4|\n|OpenIns3D|Scene-level|Grounding-DINO|18.8(+14.4)|29.8(+15.4)|\n|OpenIns3D|Scene-level|ODISE|28.7|38.9|\n\nWe hope the rebuttal is helpful. Please let us know if you need any further clarification or have additional questions. Thanks again for your support."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247911856,
                "cdate": 1700247911856,
                "tmdate": 1700247911856,
                "mdate": 1700247911856,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]