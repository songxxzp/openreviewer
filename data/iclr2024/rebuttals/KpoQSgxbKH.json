[
    {
        "title": "Generative Pre-training for Speech with Flow Matching"
    },
    {
        "review": {
            "id": "q3ZlKwokiF",
            "forum": "KpoQSgxbKH",
            "replyto": "KpoQSgxbKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_STf2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_STf2"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to pretrain a flow-based model with unsupervised pre-training and supervised fine-tuning paradigms. The pre-trained generative model can be fine-tuned with task-specific data for speech enhancement, separation, and synthesis. According to the results across several benchmarks, the proposed models match or surpass existing expert models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper explores a novel direction to pre-train a general-purpose generative model with unlabeled speech using flow-based models. The most similar work is Voicebox, a flow-based model with supervised slot-filling training, and the authors conduct details discussion and experimental comparisons to show the advantages. The pre-trained model can be finetuned to support various tasks such as speech enhancement, separation, and synthesis. The experiments are convincing."
                },
                "weaknesses": {
                    "value": "One primary limitation of this work is the relatively limited range of supported task types. It would be beneficial for the authors to expand their support to include a wider variety of tasks, such as speech editing tasks, to further demonstrate the capabilities of their pre-trained models. By incorporating additional task types, the authors can provide a more comprehensive evaluation of the model's abilities and showcase its versatility across various domains. This would enhance the overall contribution and applicability of the proposed pre-training approach."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743063618,
            "cdate": 1698743063618,
            "tmdate": 1699636254988,
            "mdate": 1699636254988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6n4Sq65KXJ",
                "forum": "KpoQSgxbKH",
                "replyto": "q3ZlKwokiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive and explicit feedback that would definitely improve our work. We have included new results on speech editing in Section A4.1 in the revision, following closely to the prior work A3T [A]. We also note that with the three fundamental tasks supported by our model, there are potentially more variants that can be easily done in the future. E.g., target speaker extraction with audio prompt, text-guided speaker extraction via multi-task model, speech removal via removing enhancement result, etc.\n\nWe thank the reviewer again for helping improve this work, and we hope our response has addressed your concern and enhanced the overall contribution.\n\n[A] A3T:  Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing, Bai et al."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220008702,
                "cdate": 1700220008702,
                "tmdate": 1700220008702,
                "mdate": 1700220008702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P5kLauI7kk",
                "forum": "KpoQSgxbKH",
                "replyto": "6n4Sq65KXJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Reviewer_STf2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Reviewer_STf2"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks. I have read the response and maintain the same score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300853621,
                "cdate": 1700300853621,
                "tmdate": 1700300853621,
                "mdate": 1700300853621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iani4Eafnd",
            "forum": "KpoQSgxbKH",
            "replyto": "KpoQSgxbKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_d1Li"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_d1Li"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes pre-training a flow-based speech synthesizer using 60kh of untranscribed speech, then fine-tuning it for downstream tasks including speech enhancement, speech separation, and zero-shot TTS.  In any self-supervised paradigm, one must find a way to add labels during the fine-tuning process; here, that problem is solved by using masked spectrograms as pseudo-labels during pre-training, then replacing those with actual labels (noisy speech, mixed speech, or phone sequences) during fine-tuning.  The idea of using masked spectrograms to condition flow was also used in the VoiceBox flow synthesizer, but that paper did not include a self-supervised pre-training stage."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Generative pre-training for speech synthesis might have been first proposed in \"Semi-Supervised Training for Improving Data Efficiency in End-to-end Speech Synthesis\" by Chung et al., 2019.  Generative flow was used for vocoding in \"Waveglow: A flow-based generative network for speech synthesis,\" and was used for TTS in \"Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis\" --- neither of those papers used the flow matching paradigm, instead they trained the flow networks using end-to-end training criteria only.  The combination of these two ideas (flow-based TTS and pre-trained TTS) was not proposed in any paper I can find.  The new contribution of this manuscript, the use of generative flow in a self-supervised pre-training stage, is an elegant idea that forms a strong theoretical paradigm, and that is supported by strong experimental results compared to challenging baselines."
                },
                "weaknesses": {
                    "value": "By omitting key references, this paper seems to be suggesting that nobody has ever thought of using self-supervised pre-training for speech synthesis before, and it seems to be suggesting that nobody has ever used generative flow for speech synthesis before.  The manuscript should include references to key works in both areas, in order to more clearly articulate what is the actual contribution of the paper."
                },
                "questions": {
                    "value": "The paper should better describe the history of (1) the use of self-supervised pre-training for speech enhancement and speech synthesis, and (2) the use of generative flow in speech synthesis.  I recommend the following references, but I think there may be others that I'm missing:\n\nSelf-supervised training for TTS:  Yu-An Chung, Yuxuan Wang, Wei-Ning Hsu, Yu Zhang and RJ Skerry-Ryan, \"Semi-supervised training for improving data efficiency in end-to-end speech synthesis,\" ICASSP 2019, 6940-6944\n\nSelf-supervised training for speech enhancement: Yang, Shu-wen, Po-Han Chi, Yung-Sung Chuang, Cheng-I. Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu et al. \"Superb: Speech processing universal performance benchmark.\" arXiv preprint arXiv:2105.01051 (2021), and other papers that submitted entries to the Superb challenge.\n\nGenerative flow for speech synthesis:\n\nRyan Prenger, Rafael Valle and Bryan Catanzaro, \"Waveglow: A flow-based generative network for speech synthesis,\" ICASSP 2019, 3617-3621\n\nRafael Valle, Kevin Shih, Ryan Prenger and Bryan Catanzaro, \"Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis,\" arXiV 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The bibliography feels like it was filtered to remove papers written by authors at companies competing with the company at which these authors work."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3089/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3089/Reviewer_d1Li"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760438517,
            "cdate": 1698760438517,
            "tmdate": 1699636254914,
            "mdate": 1699636254914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VgXsh7TQZj",
                "forum": "KpoQSgxbKH",
                "replyto": "iani4Eafnd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to first thank the reviewer for the effort in providing a detailed review. Please see the following responses to the concerns/questions raised in the review and let us know if there are further questions.\n\n---\n\n> By omitting key references, this paper seems to be suggesting that nobody has ever thought of using self-supervised pre-training for speech synthesis before. The manuscript should include references to key works in both areas, in order to more clearly articulate what is the actual contribution of the paper.\n\n\nWe would like to highlight that the key innovation of our approach: SpeechFlow is, to the best of our knowledge, the first generative speech pre-training method that is not conditioned for any specific task (i.e., not just for speech synthesis, but also other generation tasks). We apologize if the writing is misleading in the submission and will do our best to avoid overstating our contribution. \n\n> The paper should better describe the history of (1) the use of self-supervised pre-training for speech enhancement and speech synthesis; \u2026\n\nWe thank the reviewer for pointing out the reference in semi-supervised TTS [Chung et al. 2019], we have updated Section 2 in our revision accordingly. \n\nFor enhancement, the generative tasks are covered in related work (Section 2, under the \u201cPre-trained Speech Models\u201d section) with references including the one suggested by the reviewer. We would also like to point out that the enhancement results in Superb-SG [A] are far behind recent enhancement models (that we compared against in this paper), results are more for benchmarking speech representation than building actual enhancement systems. This also showcased how SpeechFlow diverges from existing pre-trained \u200b\u200bspeech models \u2013 it is the first general-purpose pre-trained speech model for generative tasks.\n\n\n> \u2026 and it seems to be suggesting that nobody has ever used generative flow for speech synthesis before. \u2026The paper should better describe the history of \u2026 (2) the use of generative flow in speech synthesis. \n\nThe use of generative models in TTS has been discussed in Sections 1 and 2.  In particular, flow-based methods such as WaveGlow [Preger et at. 2019] are covered in the 1st paragraph of related work. We thank the reviewer for providing additional reference [Valle et at. 2020] for generative TTS, which is added in the revision.\n\nWe would also like to point out that our approach relied on Flow Matching [B], which is different from flow-based methods. Flow matching learns a time-dependent diffeomorphic map, called \u201cflow\u201d, to push samples from one distribution to another in a continuous (as the mapping is defined by ODE function) manner. The neural network can be any model that predicts the trajectory of flow at any given time, and inference is done iteratively similar to diffusion models [C]. This is different from the flow-based generative models [D,E] which rely on strictly invertible networks to maximize the likelihood of data, and inference can be done in a single forward step.\n\n---\n\nWe thank the reviewer again for the constructive and helpful feedback, we hope the revised version addressed the concern raised in the review.\n\n---\n### Reference\n- [A] SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities, Tsai et al. 2022\n- [B] Flow matching for generative modeling, Lipman et al. 2022\n- [C] Denoising Diffusion Probabilistic Models, Ho et al. 2020\n- [D] NICE: Non-linear Independent Components Estimation, Dinh et al. 2014\n- [E] Glow: Generative Flow with Invertible 1\u00d71 Convolutions, Kingma et al. 2018"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220552705,
                "cdate": 1700220552705,
                "tmdate": 1700220877101,
                "mdate": 1700220877101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ELOvJFqITK",
            "forum": "KpoQSgxbKH",
            "replyto": "KpoQSgxbKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_C8mS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_C8mS"
            ],
            "content": {
                "summary": {
                    "value": "Speechflow is a generative model for speech generation and various tasks of it. It is trained with unlabeled speech with the goal of estimating the underlying distribution of speech conditioning on masked audio. Then it is fine-tuned for each specific task using labeled data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-Novel idea on making a general-purpose speech generation model that can perform the following tasks outperforming the current SOTA approaches: speech enhancement, speech separation, zero-shot tts.\n-I have listened to the audio samples and the model seems to perform well and produce high quality audio samples for all the tasks.\n-Novelty of modeling speech directly."
                },
                "weaknesses": {
                    "value": "-No subjective evaluation is presented which could be useful for the users of this model. Most TTS works present both subjective and objective metrics for the evaluation.\n-The work is not a very good match for this venue. It would be more suitable in a speech-related venue like ICASSP or InternSpeech."
                },
                "questions": {
                    "value": "-What dataset are you using? for pre-training. You mention 60k hours of English speech.\n-Have you tried different mask instead of filling with zeros?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "It is a text-to-speech model that can be possibly misused for harmful purposes. That being said I think that it shouldn't be rejected for that reason from this venue."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3089/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3089/Reviewer_C8mS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812617701,
            "cdate": 1698812617701,
            "tmdate": 1699636254816,
            "mdate": 1699636254816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7I8PYGpfEO",
                "forum": "KpoQSgxbKH",
                "replyto": "ELOvJFqITK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to first thank the reviewer for the feedback. Please see the following responses to the concerns/questions raised in the review and let us know if there are further questions.\n\n---\n\n\n> No subjective evaluation is presented which could be useful for the users of this model. Most TTS works present both subjective and objective metrics for the evaluation. \n\nWe thank the reviewer for the helpful feedback that helps improve our evaluation.  We have updated the human evaluation for TTS in Section A.4.6 and Table 11 in the paper. \n\n\n> What dataset are you using? for pre-training. You mention 60k hours of English speech. \n\nFor pre-training, we use 60k hours of unlabeled speech from English audiobooks in the public domain recorded by thousands of speakers, which is a comparable setup as [A].\n\n> Have you tried different mask instead of filling with zeros?\n\nYes, we have tested using learnable mask embeddings and found no improvement over zeroing out masked inputs in the early stage of this work. Filling zero provides equal performance with fewer model parameters. \n\n---\n### Reference\n\n[A] Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers, Wang et al."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220866299,
                "cdate": 1700220866299,
                "tmdate": 1700220866299,
                "mdate": 1700220866299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Uj14qVynT",
            "forum": "KpoQSgxbKH",
            "replyto": "KpoQSgxbKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_dpH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3089/Reviewer_dpH8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use flow matching for speech generation. Experiments are conducted for speech enhancement, speech separation, and TTS."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of using flow matching for speech synthesis is sound."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. It's basically applying flow matching to the speech synthesis problem.\n2. The evaluation of the experimental results are weak. For speech generation, subjective human evaluation are expected, especially for TTS. Without such evaluation, the results are not persuasive.\n3. There is a disconnection between the main claim and the experimental results. The experimental results show strong performance with using flow matching for speech synthesis in a pretraining and fine-tuning matter (finetuning was done with large datasets, e.g. 360 / 960 hours speech) . However, it's not clear that it's a results of \"a foundational model\". The experiments of SpeechFlow without pretraining is not persuasive because it uses the same model size, which likely leads to overfitting.\n4. The description of the experiments are severely limited. For example, what datasets were used, and the details on the model architecture and hyperparameteres. I have a major concern on reproductivity."
                },
                "questions": {
                    "value": "- Sec 4.1 -- what are. the 60k hours of English speech data for training?\n- Sec 4.3 -- can you give details on the 360 hours of training data?\n- Sec 4.4 -- what's the 960 hours of transcribed English speech used for fine-tuning?\n- Sec 4.4. -- the term \"zero-shot TTS\" is improper because the model is trained with fully supervised data. the reference is improper either as there are earlier and more established works not cited.\n- Sec 3.1 -- typo: \"variational audio encoders\" => \"variational autoencoders\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699223619887,
            "cdate": 1699223619887,
            "tmdate": 1699636254753,
            "mdate": 1699636254753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZsMpqMgHfq",
                "forum": "KpoQSgxbKH",
                "replyto": "1Uj14qVynT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to concerns regarding novelty/evaluation"
                    },
                    "comment": {
                        "value": "(Reference will be appended to the end of the last response)\n\n---\n> The novelty is limited. It's basically applying flow matching to the speech synthesis problem.\n\nWhile speech synthesis[1] is indeed one of the applications studied here, **this paper focused more on developing pre-training methods (via flow matching) for generative tasks in speech**. Speech synthesis is only part of the experiment to showcase the use of a pre-trained model, alongside speech enhancement and separation.\n\nOur approach is different from existing generative modeling methods for speech. Generative models have been useful in speech generation tasks, such as GAN-based vocoders, flow-based TTS, diffusion-based enhancement, etc. Nevertheless, these generative models are trained with task-specific conditioning (e.g., vocoders are trained to model waveform conditioning on spectrograms), limiting their generalizability. In contrast, we propose to pre-train a generative model without a predefined application and show that the resulting model can be applied to different downstream generation tasks with strong performance.\n\nIn short, **pre-training general-purpose generative models for speech have not yet been explored to the best of our knowledge**. (To further justify our novelty, this is indeed listed as the biggest strength of our work according to all other reviewers.) Given the current review is as short as \u201cThe novelty is limited. It's basically applying flow matching to the speech synthesis problem\u201d, please kindly provide more detailed feedback:\nIf the reviewer thinks a better solution has been proposed before, please list it so we can discuss the differences. \nIf the reviewer thinks the problem we studied (a pre-training method that benefits multiple speech generative tasks) is irrelevant, please state why so we can discuss it. \n\nOtherwise, we hope our responses have clarified the contribution of this paper and we respectfully ask the reviewer to re-evaluate its novelty. \n\n[1] We assume the reviewer refers to text-to-speech synthesis as \u201cspeech synthesis\u201d following the convention in the field. It could be possible that the reviewer is referring to a more general set of tasks with speech as output. In that respect, our method is novel as 1) it is the first general-purpose pre-trained generative model as described above; 2) flow matching has not been used on speech generation tasks besides text-to-speech.\n\n---\n\n> The evaluation of the experimental results are weak. For speech generation, subjective human evaluation are expected, especially for TTS. Without such evaluation, the results are not persuasive.\n\nWe thank the reviewer for the helpful feedback that helps improve our evaluation. **We have updated the human evaluation for TTS in Section A.4.6 and Table 11 in the paper.** \n\nUnlike TTS where the optimal solution might not be unique and involves a certain degree of subjective judgment, speech enhancement and separation datasets provide clean reference that is the optimal solution. Following well-recognized recent works [A,B,C] in the field, we report objective metrics without subjective evaluation and we believe our result is significant.\n\n**In addition, we also provided audible samples in the attachment so anyone can judge the quality subjectively.** More importantly, those samples are not cherry-picked. To be more specific, here is how the samples on the demo page are drawn:\n- For speech enhancement, we rank all testing samples from easy to hard using PESQ, and show the exact sample at the 0/20/40/60/80/100th percentile rank.\n- For zero-shot TTS, we use all the recordings used by prior work [D] in their demo [E] to provide a side-by-side comparison.\n\nWhile evaluation for speech generation tasks is indeed hard, the evaluation metrics we presented all are examined and standardized by preceding works. We have also added subjective test results as suggested by the reviewer. We hope our results are now considered persuasive by the reviewer.\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221249677,
                "cdate": 1700221249677,
                "tmdate": 1700221249677,
                "mdate": 1700221249677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l8zeriY15c",
                "forum": "KpoQSgxbKH",
                "replyto": "1Uj14qVynT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to concerns regarding experiment and reproducibility"
                    },
                    "comment": {
                        "value": "> There is a disconnection between the main claim and the experimental results. The experimental results show strong performance with using flow matching for speech synthesis in a pretraining and fine-tuning matter (finetuning was done with large datasets, e.g. 360 / 960 hours speech) . However, it's not clear that it's a results of \"a foundational model\". The experiments of SpeechFlow without pretraining is not persuasive because it uses the same model size, which likely leads to overfitting.\n\n\n**Our main claim is that a pre-trained generative model can serve as the foundational model of different generation tasks with competitive performance and less fine-tuning resources.** This is justified by (1) comparing to strong expert models in each task; and (2) comparing to the same model (in terms of architecture/model size/training loss) that was not pre-trained. The trend is consistent on all three different tasks SpeechFlow is tested on, not just speech synthesis.\n\nIt is widely known that one of the key benefits of pre-training is that one can train a bigger model with fewer labeled data through pre-training. This has been proven effective in not just discriminative speech models [F,G], but also in image [H,I] and text [J,K] foundational models. **These models are called \u201cfoundation models\u201d because they have been trained with unlabeled data at scale and can be easily applied to different downstream tasks with minimum supervision, just like the proposed model.**\n\nTo further alleviate the reviewer's concern that the baseline might be overfitting, we conducted an additional experiment where the baseline model size is reduced from 24-layer (commonly known as \u201cLarge\u201d) Transformer Encoder to 12-layer (commonly known as \u201cBase\u201d). **We found the smaller baseline has little to no gain compared to the original baseline that the reviewer suspected to be overfitting.** Below we attach the results (setup/metric follows Table 3 of the paper):\n\n|   | |  | cross  |  |  | cont. |    |\n|---|---|---|---|---|---|---|---|\n|   |  labeled  speech (hr) | WER  | SIM-o  | SIM-r  | WER  | SIM-o  | SIM-r  |\n|  Voicebox [D] |   60k | 1.9  | 0.662  | 0.681  | 2.0  | 0.593 | 0.616 |\n|  SpeechFlow w/o fine-tuning  |   960 | 2.0  | 0.569 | 0.598 | 2.1 | 0.530 | 0.557 |\n| SpeechFlow w/o fine-tuning, 12-layer Base-size Transformer | 960 | 2.2 | 0.582 | 0.617 | 2.0 | 0.536 | 0.571 |\n| SpeechFlow | 960 | 2.1 | 0.700 | 0.715 | 2.1 | 0.630 | 0.644 |\n\nCombined with the additional results, we believe our experimental results sufficiently support our claims.\n\n---\n\n\n> The description of the experiments are severely limited. For example, what datasets were used, and the details on the model architecture and hyperparameteres. I have a major concern on reproductivity. \n\n(also associated to first 3 questions raised by the reviewer)\n\n- In all sections from 4.1 (pre-training), 4.2 (enhancement), 4.3 (separation), to 4.4 (zero-shot TTS), we have model/training/data paragraphs with bolded header. Besides description in text, the dataset for each downstream task is provided in the caption of the corresponding Table 1,2, and 3.  \n- Here we additionally discuss the data used:\n    - For the 60k hours of unlabeled English speech, we use audiobooks in the public domain. A setup with the same amount of data can be found in [N; cited Wang et al. 2023 in the paper] where the data they used is publicly available.\n    - The 960/360 hours of speech for TTS/separation are also from the same domain, A setup with the same amount of data can be found in [O; cited Ravanelli et al. 2021 in the paper] where the data they used is publicly available. \n- Model architecture and hyper-parameters (learning rate, # of GPUs, etc.) are already provided in Section 4.1, 4.2, 4.3, 4.4 for each task respectively. We understand that this might be hard to track as they are spreaded out in different sections, we have added a summarization in Table 5 in the appendix to make it clear at a glance.\n- In addition to Section 4.6 which already provided analysis on hyper-parameter selection, we have added the study on the model\u2019s robustness w.r.t. masking configuration in Section A.4.5.\n- Finally, we note that the model architecture (24-layer Transformer) and loss function (flow matching) of SpeechFlow is already publicly available [L]. \n\nTo conclude, we believe we have revealed as many details in the paper as we can to ensure SpeechFlow can be reproduced. \n\n---"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222377811,
                "cdate": 1700222377811,
                "tmdate": 1700222377811,
                "mdate": 1700222377811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FB8nt0Reku",
                "forum": "KpoQSgxbKH",
                "replyto": "1Uj14qVynT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the terminology; Conclusion; Reference"
                    },
                    "comment": {
                        "value": "> Sec 4.4. -- the term \"zero-shot TTS\" is improper because the model is trained with fully supervised data. the reference is improper either as there are earlier and more established works not cited.\n\nAs mentioned in the title and the first paragraph of Section 4.4, **the term \u201czero-shot TTS\u201d is short for zero-shot speaker adaptation text-to-speech synthesis, not unsupervised/semi-supervised TTS**. Zero-shot TTSs are still trained in a supervised manner. We followed the prior works [D,M,N] to use the term to refer to synthesizing speech with a voice that is unseen during training.\n\nWe are sorry if we are missing more early works in this direction, please also kindly provide additional references that should be cited.\n\n---\n\n**In conclusion, we hope our response has addressed the concerns/questions raised by the reviewer. We sincerely ask the reviewer to re-evaluate this work, especially in terms of novelty. Please kindly provide references if there are still concerns regarding novelty.**\n\n---\n### Reference\n\n- [A] Speech Enhancement and Dereverberation with Diffusion-based Generative Models, Ritchard et al.\n- [B] MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement, Fu et al.\n- [C] Attention is All You Need in Speech Separation, Subakan et al.\n- [D] Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale, Le et al.\n- [E] https://voicebox.metademolab.com/zs_tts.html\n- [F] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations, Baevski et al.\n- [G] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units, Hsu et al.\n- [H] Emerging Properties in Self-Supervised Vision Transformers, Caron et al.\n- [I] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning, Grill et al.\n- [J] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Raffel et al.\n- [K] Language Models are Unsupervised Multitask Learners, Radford et al.\n- [L] https://github.com/lucidrains/voicebox-pytorch\n- [M] YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone, Casanova et al.\n- [N] Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers, Wang et al.\n- [O] SpeechBrain: A general-purpose speech toolkit, Ravanelli et al."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222854758,
                "cdate": 1700222854758,
                "tmdate": 1700222854758,
                "mdate": 1700222854758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cuPc4c2EF9",
                "forum": "KpoQSgxbKH",
                "replyto": "1Uj14qVynT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Reviewer_dpH8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Reviewer_dpH8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses and the additional evaluation and audio samples.\n\n - Re: subjective evaluation -- thanks for providing these results. I believe that these subjective evaluation results are so important that they should appear in the main content instead of the appendix.\n\n - Re: audio samples -- thanks for providing the audio samples. I've listened to them and have a question: Compared to one of the baselines SGMSE+, all the samples from SpeechFlow in the speech enhancement group consistently have more artifacts. This seems not aligned with the quantitive evaluation results. Any explanation?\n\n - To clarify authors' question regarding novelty -- I believe that the problem this paper studies on is highly valuable and relevant, while its novelty and contribution is limited, for the reasons that I stated in the initial review.\n\n - To clarify authors' question on \"speech synthesis\" -- speech synthesis is a broader concept than text-to-speech synthesis.\n\n - I still have concerns regarding the disconnection between the main claim of \"foundation model\" and the experimental results, especially when finetuning was done with large datasets (360 / 960 hours speech), and the benefits in the experimental results are not very clear (the subjective MOS results in TTS is lower than one of the baselines VoiceBox; the audio samples on speech enhancement sound worse than one of the baselines SGMSE+).\n\n- Re: description of the experiments -- Could you make it clear if the experiments were done with public datasets, and if they are, the name of each specific datasets? Particularly, is the \"960 hours\" dataset LibriTTS? Similar for other datasets. Please use the name of the public datasets whenever possible.\n\n- While I recognize that the term \"zero-shot TTS\" is used in a few literatures indeed, it's still not a proper term because it literally means a different task (think about \"zero-shot ASR\"). For more established prior work: e.g. https://arxiv.org/abs/1806.04558."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699337525,
                "cdate": 1700699337525,
                "tmdate": 1700699606418,
                "mdate": 1700699606418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xLqYLez8Tc",
                "forum": "KpoQSgxbKH",
                "replyto": "1Uj14qVynT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to reviewer's clarification and follow up questions  (1/2)"
                    },
                    "comment": {
                        "value": "We would like to first thank the reviewer for the clarifications and additional feedback/questions.\n\n---\n\n> To clarify authors' question regarding novelty -- I believe that the problem this paper studies on is highly valuable and relevant, while its novelty and contribution is limited, for the reasons that I stated in the initial review.\n\nAs stated clearly in the paper and throughout the rebuttal, **this is the first work to pre-train a general purpose generative model for speech**. \n\nThe initial review referenced in the clarification was \u201cThe novelty is limited. It's basically applying flow matching to the speech synthesis problem.\u201d \n\nWe understand that the reviewer might not value our solution [1], but **the problem itself, proposed by and studied in this paper, is a novel contribution**. To backup our claim, here we quote reviews from all other reviewers:\n- Reviewer C8mS: *\u201c**Novel idea on making a general-purpose speech generation model** that can perform the following tasks outperforming the current SOTA approaches\u2026\u201d*\n- Reviewer d1Li: *\u201c**The new contribution of this manuscript, the use of generative flow in a self-supervised pre-training stage, is an elegant idea** that forms a strong theoretical paradigm, and that is supported by strong experimental results compared to challenging baselines.\u201d*\n- Reviewer STf2: *\u201cThe paper explores **a novel direction to pre-train a general-purpose generative model** with unlabeled speech using flow-based models.\u201d*\n\nIn short, this paper introduced a \u201chighly valuable and relevant\u201d (quoting reviewer dpH8\u2019s own word) problem that is novel by itself. And we confirmed that a good solution to this problem can make good contribution to speech generation tasks.\n\n\n[1] Flow matching is the objective we used to train the model, just like how common objectives are used for different purposes (e.g., diffusion for denoising, L1/L2 loss for TTS, etc.). Although using it for separation, denoising, and unsupervised learning with speech is indeed novel, **it is not the main reason why this work is novel**. The main contribution of this paper is exploring pre-trained model for generation tasks.\n\n---\n\n> Re: description of the experiments -- Could you make it clear if the experiments were done with public datasets, and if they are, the name of each specific datasets? Particularly, is the \"960 hours\" dataset LibriTTS? Similar for other datasets. Please use the name of the public datasets whenever possible.\n\nFor training data  of pre-training/separation/zero-shot speaker adaptation TTS, we use curated datasets that resemble the domain of the test data with different amounts of data as provided. For separation/TTS, the baselines are trained on the same curated datasets, and the comparison is on benchmark datasets (as cited in each table). This setup ensures the evaluation/comparison is fair and convincing while using curated datasets for training.\n\n---\n\n> I still have concerns regarding the disconnection between the main claim of \"foundation model\" and the experimental results, especially when finetuning was done with large datasets (360 / 960 hours speech), and the benefits in the experimental results are not very clear (the subjective MOS results in TTS is lower than one of the baselines VoiceBox; \n\nThe purpose of the TTS experiment is to show *how much we can reduce the need of labeled data?* and more importantly, *what is the performance cost?*\n\nVoicebox is the fully-supervised strong baseline that requires all the training data of SpeechFlow to be labeled. That\u2019s 60000 hours (6.8 years in total) of speech all labeled. It is expected to be better. But fine-tuning SpeechFlow using only 960 hours provides a good TTS with a very small gap. \n\nWe also showed how poorly Voicebox would perform if we restrict the amount of labeled data to be just 960 hours (denoted SpeechFlow w/o pre-training in Table 3).  Between the pre-trained SpeechFlow and randomly initialized Voicebox, there is a significant gap.\n\nThe small gap (between data-hungry strong baseline and low-resource fine-tuning) and the big gap (between random initialized v.s. pre-trained model)  are strong evidence that demonstrates how a pre-trained generative speech model is a good foundation model. We show 6250% labeled data can be reduced with a small cost on WER/MOS."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737764905,
                "cdate": 1700737764905,
                "tmdate": 1700737945680,
                "mdate": 1700737945680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]