[
    {
        "title": "Denoising Graph Dissipation Model Improves Graph Representation Learning"
    },
    {
        "review": {
            "id": "xcLz38Dq9p",
            "forum": "ExiBN1ZWJn",
            "replyto": "ExiBN1ZWJn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_yTz6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_yTz6"
            ],
            "content": {
                "summary": {
                    "value": "Existing graph representation learning methods mainly focus on task-specific factors rather than universal factors that can be used for any downstream tasks. This work proposes Graph Dissipation Model (GDM) to learn the latent intrinsic distributions of the graph based on the diffusion models, which enables the learned representations to be utilized for any downstream tasks. To encode both node feature and structural information, GDM introduces a coupled diffusion model framework consisting of a feature diffusion process and a structure diffusion process. Laplacian smoothing is innovatively used as a noise source for the feature diffusion process and edge removal is also defined as a noise source for the structure diffusion process. Experiments on both link prediction and node classification show that GDM achieves comparable performance for existing graph representation learning baselines on both tasks, demonstrating GDM's capability of learning universal factors that can be applied to any downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposes GDM, the first diffusion-based graph representation learning model that encodes both node feature and structure information. GDM is able to learn comprehensive and universal latent structures from a graph without explicit bias for specific tasks.\n\n2. The idea of utilizing Laplacian smoothing as a noise source for the feature diffusion process and over-smoothing as a convergence state is novel and interesting. Such a design for blurring node features is also more natural in the graph learning setting.\n\n3. Experiments indicate that GDM achieves comparable performance on the link prediction task compared to baselines, and outperforms baselines on a semi-supervised node classification with few training labels, demonstrating that GDM learns universal graph representations that can be applied to downstream tasks."
                },
                "weaknesses": {
                    "value": "1. Although GDM aims to learn comprehensive and universal graph representations, Equation 10 in the paper still contains the downstream task loss as a part of the final loss. I wonder if GDM without downstream task loss can learn universal graph representations, or we should regard GDM as a universal framework that can incorporate any downstream task loss. Have the authors done some experiments to evaluate the universal graph representations obtained by GDM without downstream task loss?\n\n2. In this work, the authors did not mention the time complexity of GDM and its runtime in experiments. As GDM requires eigendecomposition of the graph Laplacian matrix, I wonder if the authors could further discuss GDM's time complexity and also provide some results of the GDM's runtime compared to other baselines in the link prediction and node classification experiments.\n\n3. (Minor) I did not find any supplementary materials discussing the details of the implementation of GDM and the experiments conducted in the paper. There is also no code implementation of GDM to reproduce the experimental results presented in the paper.\n\n4. (Minor) Typo: In the Implementation Details of Section 5.1, \\\n\"Also we set iffusion state to 3 for OGB-Citation2\" $\\rightarrow$ \"Also we set diffusion state to 3 for OGB-Citation2\""
                },
                "questions": {
                    "value": "1. Please see the questions mentioned in the Weaknesses.\n\n2. As the over-smoothing issue appears after only several Laplacian smoothing operations (i.e., node representations converge to identical after only several steps), it seems the value of time step $t$ can be small if we set the over-smoothing as the convergence state. Therefore, I wonder how to choose a proper $t$ to ensure sufficient diffusion and if the authors have done some experiments on the selection of $t$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5365/Reviewer_yTz6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789448032,
            "cdate": 1698789448032,
            "tmdate": 1699636541522,
            "mdate": 1699636541522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C7mLj2dT8W",
                "forum": "ExiBN1ZWJn",
                "replyto": "xcLz38Dq9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yTz6 - Weakness (1), (2), Question (2), Minor"
                    },
                    "comment": {
                        "value": "Thank you very much for dedicating time to review our submission and providing crucial comments! We are pleased to address your concerns as follows:\n\n> Although GDM aims to learn comprehensive and universal graph representations, Equation 10 in the paper still contains the downstream task loss as a part of the final loss. I wonder if GDM without downstream task loss can learn universal graph representations, or we should regard GDM as a universal framework that can incorporate any downstream task loss. Have the authors done some experiments to evaluate the universal graph representations obtained by GDM without downstream task loss?\n\nThis is a great point! \nGDM is \"a universal framework that can incorporate any network graph downstream task loss\", which accurately expresses our approach. \nThe motivation behind GDM is that existing graph representation learning models are investigated for improving particular graph representation learning tasks, yet they are often vulnerable to other tasks. For instance, in the link prediction task, some existing models assume generalizing some graph heuristics to find missing links. However, those existing models are not capable of learning node embeddings for node classification tasks.\n\nConsequently, when we refer to \u201ccomprehensive and universal graph representations\u201d, which GDM aims to learn, it intends to convey a representation that is universally applicable to various network graph representation learning downstream tasks while naturally regarding specifics of those tasks without injecting task-dependent assumptions or biases. Note that it covers both features and structure information that are entailed from a given task. Unlike GDM, existing graph representation learning methods have focused on defining task-dependent / task-specific assumptions for particular graph learning tasks. For instance, in the link prediction task, some existing models assume generalizing some graph heuristics to find missing links that are not inferred by node embeddings. In node classification tasks, some approaches introduced biases that enhanced intraclass relationships and weakened interclass relations to improve node classification performance. Thus, there are no powerful models for both link prediction and node classification tasks. However, our GDM captures comprehensive latent factors needed for a given task faithfully without relying on such task-specific assumptions, achieving competitive performance in both tasks. \n\nTo the best of our knowledge, we initially raised such motivation and research for diffusion model for graph representation learning regarding both graph feature and structure has not been studied before, highlighting the novelty of our study.\n\n> In this work, the authors did not mention the time complexity of GDM and its runtime in experiments. As GDM requires eigendecomposition of the graph Laplacian matrix, I wonder if the authors could further discuss GDM's time complexity and also provide some results of the GDM's runtime compared to other baselines in the link prediction and node classification experiments.\n\nSince we leverage Laplacian smoothing as a noise source in the forward process of GDM, it does not require the eigendecomposition of graph Laplacian matrix. We mentioned eigendecomposition for providing insight on graph spectral domain. We are sorry for the misleading typo. We will immediately revise it. \nAs there is no algorithm in the training procedure, GDM's runtime is 55seconds on OGB-Collab which is comparable with conventional GNNs which take 20~30 seconds on the dataset. It is notable that GDM achieves competitive results in link prediction tasks compared to SEAL or Neo-GNNs where runtime is fairly longer ($>3 \\text{min}$) due to extracting enclosing subgraphs or generalizing algorithms.\n\n>  As the over-smoothing issue appears after only several Laplacian smoothing operations (i.e., node representations converge to identical after only several steps), it seems the value of time step $t$\n can be small if we set the over-smoothing as the convergence state. Therefore, I wonder how to choose a proper \n to ensure sufficient diffusion and if the authors have done some experiments on the selection of $t$.\n\nThis is a great point! Right. Unlike diffusion models proposed for image or graph generation, GDM does not require extremely large numbers as time step values, such as 1000 or 10000. The oversmoothing issue in GNNs is known to occur when there are at least five layers. Based on this, we determined that setting the time step ($T$) to a minimum of 5 is sufficient. Through experiments, we found that setting $T$ to 6 for Collab, PPA, and DDI is sufficient. Adaptive learning of $T$ value is something we consider for future work.\nAdditionally, we will include the sensitivity analysis for the selection of $T$ in the appendix.\n\n*Minor*\n(3) We will upload the code implementation of GDM soon.\n(4) Thanks to the reviewer, we will fix typos in the revised paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598798444,
                "cdate": 1700598798444,
                "tmdate": 1700598798444,
                "mdate": 1700598798444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V7vNEkA9CY",
                "forum": "ExiBN1ZWJn",
                "replyto": "C7mLj2dT8W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Reviewer_yTz6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Reviewer_yTz6"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer yTz6"
                    },
                    "comment": {
                        "value": "Thank you the authors for the response. Regarding the question \"a universal framework that can incorporate any network graph downstream task loss\", what I was trying to ask is whether the authors have done some experiments on some experiments to evaluate the universal graph representations obtained by GDM without downstream task loss, but the authors did not answer my question directly. However, I understand the general motivation of GDM and I would like to keep my score as is."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728567551,
                "cdate": 1700728567551,
                "tmdate": 1700728567551,
                "mdate": 1700728567551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iHGKl41pEb",
                "forum": "ExiBN1ZWJn",
                "replyto": "xcLz38Dq9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to answer the reviewer's question about experiments to evaluate the universal graph representation.\nWe did not conduct experiments on evaluating the universal graph representation since what we mean \"universal\" is that GDM is universally applicable to network graph representation learning tasks and shows competitive performance on both major graph representation learning tasks.\n\nAlso, we uploaded the revised paper.\nWe revised some points the reviewer suggested.\n- More detailed explanation of using Laplacian smoothing as noise source to diffuse and dissipate graph signal (Section 4)\n- Revise some confusing notations, terms, equations into more precise and consistent expressions (Section 1, 4)\n- Clarify motivation and novelty of GDM (Section 1)\n- Clarify GDM is for network graph representation learning (Section1)\n- Derivation of loss function (Appendix)\n- Hyperparameter sensitivity analysis (Appendix)\n\nWe hope that our response addresses the reviewer's concern and leads to stronger support. \nWe sincerely appreciate the feedback to improve the paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730115817,
                "cdate": 1700730115817,
                "tmdate": 1700730976996,
                "mdate": 1700730976996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TabPuF18zP",
            "forum": "ExiBN1ZWJn",
            "replyto": "ExiBN1ZWJn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_ntqg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_ntqg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Graph Dissipation model which is a coupled diffusion model operating on node feature and graph structure space simultaneously. The model utilizes the Laplacian smoothing to get the noised node features, promoting the denoising network to capture the structural information during training. The evaluation tasks include link prediction and node classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n- Using Laplacian smoothing to diffuse the node features is an interesting operation which sounds technique.\n- Experiments support the statements in the paper."
                },
                "weaknesses": {
                    "value": "- The novelty of the structure diffusion process with randomly removing edges is limited, which also appears in [3]. Further, this reverse process of the used structure diffusion cannot correspond to the forward process.\n- In Eq(9), the Feature prediction loss and structure dissipation loss are both confusing. How to calculate the $q(X_{t-1}|X_{t},X_{0})$ and $q(A_{t-1}|A_{t},A_{0})$\uff1f The relationship between ELOB(Eq. (8)) and final loss (Eq 9) should rigorously prove.\n- Eq (6) is confusing since the $A_t$ is sampled from eq 5, which is unrelated to $A_{t-1}$. So. How to calculate the elements of $A_t$?\n- The experimental results show the proposed method doesn\u2019t achieve competitive performance in Link prediction (https://ogb.stanford.edu/docs/leader_linkprop/). Some important baselines are missing, such as GIN, on the node classification task.\n\nMinor concerns:\n- Eq (9) is out of bounds.\n- The claim \u201cthere has been no work on diffusion models for graph representation learning in both feature and structural aspects\u201d is inappropriate because there exist related works such as MoleculeSDE[1],[2].\n- The formula at the bottom of page 3 lacks of the explanation of $x$.\n- Eq. (8) should be an inequality.\n- Is there  $\\zeta $ in Eq(5)?\n\n[1] A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining.\n\n[2] Fast Graph Generation via Spectral Diffusion\n\n[3] Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling"
                },
                "questions": {
                    "value": "- From the Leaderboards of OGB(https://ogb.stanford.edu/docs/leader_linkprop/), the experimental results of this paper are not very competitive. Why the GDM don\u2019t use a powerful GNN as the denoising network? In my understanding, the Loss $L_{diff}$ can be used in any GNN for graph representation learning.\n- What is the relationship between GDM and Digress[1]? The GDM seems to be a specific case of Digress.\n- What is the benefit of samping $A_{t}$ from Eq(5) instead of a random transition from $A_{t-1}$ like [2]\n\n[1] DIGRESS: DISCRETE DENOISING DIFFUSION FOR GRAPH GENERATION \n\n[2] Diffusion Models for Graphs Benefit From Discrete State Spaces"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5365/Reviewer_ntqg",
                        "ICLR.cc/2024/Conference/Submission5365/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808795964,
            "cdate": 1698808795964,
            "tmdate": 1700790435437,
            "mdate": 1700790435437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kiIlraUAyl",
                "forum": "ExiBN1ZWJn",
                "replyto": "TabPuF18zP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ntqg - Weakness(1), (3), Question(3)"
                    },
                    "comment": {
                        "value": "Thank you very much for dedicating time to review our submission and providing thoughtful comments! We are pleased to address your concerns as follows:\n\n> The novelty of the structure diffusion process with randomly removing edges is limited, which also appears in [3]. Further, this reverse process of the used structure diffusion cannot correspond to the forward process.\n\nWe acknowledge that the term 'edge removal' can cause misleading and it does not express the goal of the process. \n\nOur edge removal is completely different from random edge removal from [3].\nBecause we aim to reflect the decay of signal/information on the graph, we designed the subgraph sampling that can lift feature decay to the graph structure. \nSpecifically, edges are sampled to be dropped from the given graph structure stochastically based on the amount of feature smoothed as drop parameter $p$ during the structure diffusion process. \nThis is significantly different from [3] as this sampling is coupled with the feature diffusion process and incorporates removing edges in a way that is influenced by the degree of feature smoothing. This sampling procedure is proposed to lift dissipation of features to the structure.\n\nOn the contrary, [3] employs edge removal for generic graph generation by randomly selecting nodes and then probabilistically removing edges connected to those selected nodes solely relying on randomness. Edge removal of [3] may be suitable for graph generation since it requires diverse structures to generate unique structures but with similar graph statistics.\n\nTo clarify the novelty of GDM and to prevent misleading, we will use the term \"dissipative structure sampling\" instead of the term \"edge removal\" in the revised paper.\n\n> Eq (6) is confusing since the $A_t$ is sampled from eq 5, which is unrelated to $A_{t-1}$. So. How to calculate the elements of $A_t$?\n\nSorry for the confusion.\nEq.(5) expresses each edge is sampled to be dropped by the parameter $p$, lifting dissipation of feature to the graph structure. We meant to show Bernoulli sampling on each edge.\nEq.(6) expresses the sampling for whole adjacency $A_t$.\n\nWe will clarify the Eq.(5) as follows:\n\n$ A\\_{t}[ij] \\sim \\text{Bern}(A_{t} | A_{t-1}[ij]=1, p=s(  \\hat{\\mathbf{x}}_{i}^{(t-1)} , \\hat{\\mathbf{x}}\\_{j}^{(t-1)} )). $\n\n> What is the benefit of samping $A_{t}$ from Eq(5) instead of a random transition from $A_{t-1}$ like [2]?\n\nSince a network graph is complicated to be defined with some family of known distribution, we designed GDM to learn underlying factor distribution by the concept of \u2018dissipation\u2019. In the forward process, GDM uses Laplacian smoothing as a noise source, gradually converging into oversmoothing. We consider the decrease in the differences between node features as the dissipation of signal (in spectral) or feature information (in spatial). To learn feature-structure integrated representations, we lift the dissipating of features to the graph structure. This is why we define structure sampling as Eq. (5) and Eq. (6).\nOn the contrary, [2] appears to employ random flipping to learn diverse structures as many as possible, as the model targets only generic graph generation. The motivation that GDM addresses is different from [2], thus, we defined our own sampling method, \u2018Dissipative structure sampling\u2019 for GDM that aligns with our motivation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590923305,
                "cdate": 1700590923305,
                "tmdate": 1700591799196,
                "mdate": 1700591799196,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YD0YniCaLq",
                "forum": "ExiBN1ZWJn",
                "replyto": "TabPuF18zP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ntqg - Weakness (2)-1"
                    },
                    "comment": {
                        "value": "> In Eq(9), the Feature prediction loss and structure dissipation loss are both confusing. How to calculate the $ q( X\\_{t-1} | X_{t}, X\\_{0} ) $ and $ q( A\\_{t-1} | A_{t}, A\\_{0} ) $ ? The relationship between ELOB(Eq. (8)) and final loss (Eq 9) should rigorously prove.\n\nThis is a good point. First, we will derive the loss (Eq. 9) from the negative of evidence lower bound (ELBO), then explain q( X\\_{t-1} | X_{t}, X\\_{0} ) $ and $ q( A\\_{t-1} | A_{t}, A\\_{0} ) $.\n\nLet $G\\_0$ be a given observed graph data consisting of $(X, A), denoting node features and adjacency matrix, respectively. Taking negative log-likelihood with evidence lower bound (ELBO), we get\n\n$$\n -\\log p\\_{\\theta}(G_{0}) \\leq  -\\int q(G_{1:T} | G_{0}) \\log p_{\\theta}(G_{0}|G_{1:T}) \\frac{p_{\\theta}(G_{1:T})}{q(G_{1:T}|G_{G\\_{0}})} \\\\,dG\n$$\n$$\n= \\mathbb{E}\\_{q(G_{1:T} | G_{0})} \\left[ -\\log \\frac{p_{\\theta}(G_{0:T})}{q(G_{1:T}|G\\_{0})} \\right] .\n$$\n\nNote that the forward feature process is defined as $ q(X\\_{1:T} | X_{0}) = \\prod_{t=1}^{T} q(X_{t} | X\\_{0}) $. As we lift feature dissipation to the forward structural process, under mild assumption, approximated $q(A\\_{t}|A_{t-1}) \\approx q(A_{t}|A\\_{0}) $ can be used as follows:\n$$\n= \\mathbb{E}\\_{q(G_{1:T} | G_{0})} \\left[ -\\log \\frac{p_{\\theta}(G_{T}) \\prod_{t=1}^{T} p_{\\theta}(G_{t-1}|G_{t})}{\\prod_{t=1}^{T} q(G_{t}|G\\_{0})} \\right]\n$$\n$$\n= \\mathbb{E}\\_{q(G_{1:T} | G_{0})} \\left[ -\\log \\frac{p_{\\theta}(G_{T})}{q(G_{T}|G_{0})} -\\log \\prod_{t=2}^{T} \\frac{p_{\\theta}(G_{t-1}|G_{t})}{q(G_{t-1}|G_{0}) } -\\log p_{\\theta}(G_{0}|G\\_{1}) \\right]\n$$\n$$\n= \\mathbb{E}\\_{q(G_{1:T} | G_{0})} \\left[ -\\log \\cancel{\\frac{p(G_{T})}{q(G_{T}|G_{0})}} -\\sum_{t=2}^{T} \\log \\frac{p_{\\theta}( G_{t-1}| G_{t} ) }{q(G_{t-1}|G_{0})} -\\log p_{\\theta}(G_{0} | G\\_{1}) \\right].\n$$\nThe second term can be interpreted as KL divergence $D\\_{KL} [ q(G_{t-1}|G_{0}) \\Vert P_{\\theta}(G_{t-1}|G\\_{t}) ]$. Posterior $q( G\\_{t-1} | G\\_{0})$ cannot be expressed in a closed-form solution because $q(G\\_{t-1}|G\\_{0})$ is unknown as several well-known distributions are inadequate to define a network graph due to its structural characteristics. \nEspecially, it becomes more difficult because GDM targets to learn feature-structure integrated latent representation. Therefore, posterior $q( G\\_{t-1} | G\\_{0})$ is intaractable.\n\nHowever, we define $\\mathbb{E}\\_{q(G_{1:T} | G_{0})} \\left[ -\\sum_{t=2}^{T} \\log \\frac{p_{\\theta}( G_{t-1}| G_{t} ) }{q(G_{t-1}|G\\_{0})} \\right]$ by decomposing $G$ to feature $X$ and structure $A$.\nPosterior on feature $q(X\\_{t-1}|X\\_{0})$ is straight forward according to Eq.(3): $q(X\\_{t-1}|X_{0}) = (I - L)^{t-1}X\\_{0}$. \nThus, we define $\\mathbb{E}\\_{q(X_{1:T} | X_{0})} \\left[ -\\sum_{t=2}^{T} \\log \\frac{p_{\\theta}( X_{t-1}| X_{t} ) }{q(X_{t-1}|X\\_{0})} \\right]$ as predicting $(I-L)^{t-1}X_{0}$ from $X\\_{t}$, i.e., $\\lVert f_{\\theta}(X_{t}, A_{t})-X\\_{t-1} \\rVert\\_{2}^{2}$.\n\nPosterior on structure $q(A\\_{t-1}|A\\_{0})$ is approximately obtained as follows:\n$$\nq(A\\_{ij}^{(t-1)}|A_{ij}^{(0)}) = \n\\mathcal{B}(A_{ij}^{(t-1)};p \\propto  L^{t-1}X), \\quad  \\text{if } A\\_{ij}^{(0)}=1 \n$$\n$$\nq(A\\_{ij}^{(t-1)}|A_{ij}^{(0)}) = \n\\mathcal{B}(A_{ij}^{(t-1)};p=0),  \\quad  \\text{if }A\\_{ij}^{(0)}=0 \n$$\nOnly in the first case, edge existence probability $p$ is uncertain.\nNote that edge probability $p$ is correlated to Laplacian matrix which feature dissipation relied on. \nTo learn comprehensive and integrated latent representation, we defined the forward structural process with stochastic structure sampling based on connected node pairs' similarity. Consequently, the forward structural process makes the graph structure sparser as the node features converge to oversmoothing. However, due to its uncertainty, the edge probability $p$ cannot be estimated by just restoring feature similarity. \nThe intuition behind the forward structural process is to lift signal dissipation to graph structure. Leveraging this intuition, the edge probability $p$ is estimated by discrepancy of structural information which implies dissipation upon graph structure. Therefore, we define $ \\mathbb{E}\\_{q(A_{1:T} | A_{0})} \\left[ -\\sum_{t=2}^{T} \\log \\frac{p_{\\theta}( A_{t-1}| A_{t} ) }{q(A_{t-1}|A\\_{0})} \\right]$ with predicting the discrepancy between graph Laplacian where dissipation is dependent, i.e., $\\lVert f_{\\theta}(X_{t}, A_{t})-(L_{0}-L_{t-1})  \\rVert\\_{2}^{2}$."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619792533,
                "cdate": 1700619792533,
                "tmdate": 1700681435703,
                "mdate": 1700681435703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nt57bIZqnh",
                "forum": "ExiBN1ZWJn",
                "replyto": "TabPuF18zP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ntqg - Minor (1), Question (2)"
                    },
                    "comment": {
                        "value": "> The claim \u201cthere has been no work on diffusion models for graph representation learning in both feature and structural aspects\u201d is inappropriate because there exist related works such as MoleculeSDE[1],[2].\n\nOur statement \"there has been no work on diffusion models for graph representation learning in both feature and structural aspects\" holds true without any exaggeration. \n\nTo address potential misunderstanding about the meaning of our work, we will provide a clearer explanation of the significance of GDM. \nThe motivation behind GDM is that existing graph representation learning models have focused on defining task-dependent / task-specific assumptions for a particular graph learning task. For instance, existing GNNs that mostly rely on message-passing, usually show good performance in node classification tasks while showing limited results in some link prediction tasks. Consequently, there are no powerful models that are capable of solving both link prediction and node classification tasks. \nIt is noteworthy that we initially raised the need for this motivation since such motivation has not been raised before.\n\nTo mitigate this motivation, we introduce GDM, a diffusion model-based graph representation that is universally applicable to network graph representation learning tasks. Regarding both features and structure of a network graph, the goal of GDM is to learn comprehensive latent representations universally applicable to network graph learning tasks, capturing underlying latent factors from a graph that are inherent in a given task. Leveraging the intuition of diffusion models to capture arbitrary data distributions, GDM learns comprehensive and integrated latent representations crucial for given network graph representation learning tasks (e.g., link prediction) without task-oriented assumptions. Remarkably, it achieves competitive results in both major graph representation learning tasks, link prediction and node classification, without applying task-specific assumptions or biases into a model. As far as our knowledge extends, such research has not been introduced before, highlighting the novelty of our study.\n\nUnlike our GDM, MoleculeSDE[1] and [2] are diffusion models for molecular graph learning which requires readout/pooling to obtain final outputs and diffusion model for graph generation tasks.\nIn other words, GDM and [1], [2] target different types of graphs. Molecular graphs are distinctive from network graphs. Molecular graphs have node and edge categorical information, and there are multiple graphs in the dataset, making it possible to define the distribution needed for the diffusion model. This is why diffusion models for graph has been numerously studied. On the contrary, network graphs consist of a single given graph, and sometimes the node's categorical information is incomplete or absent, making it insufficient to define the distribution of the graph. Thus, the diffusion models for graph representation learning has not been investigated thoroughly as molecular graphs.\n\nIndeed, GDM and the reference [1], [2] that the reviewer mentioned are completely different research areas.\nTo address the misunderstanding, we will revise the statement it is for *network* graph representation learning. \n\n> What is the relationship between GDM and Digress[1]? The GDM seems to be a specific case of Digress.\n\nGDM is completely different from DiGress (Vignac et al., 2022). It differs in data domain, motivation, and noise source.\n\nDifference from DiGress (Vignac et al., 2022): While GDM is a model for graph representation learning in network-shaped graphs, DiGress is a model for molecular graph generation. In other words, GDM and DiGress target different types of graphs. Molecular graphs have node and edge categorical information, and there are multiple graphs in the dataset, making it easier to define the distribution needed for the diffusion model. However, network graphs consist of a single given graph, and sometimes the node's categorical information is incomplete or absent, making it insufficient to define the distribution of the graph."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625253982,
                "cdate": 1700625253982,
                "tmdate": 1700625253982,
                "mdate": 1700625253982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NNLX89f9xQ",
                "forum": "ExiBN1ZWJn",
                "replyto": "TabPuF18zP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ntqg - Question (1), Weakness (4)"
                    },
                    "comment": {
                        "value": "> Why the GDM don\u2019t use a powerful GNN as the denoising network? In my understanding, the Loss $L_{diff}$ can be used in any GNN for graph representation learning.\n\nThe reason for defining and using our own Denoising network in GDM is as follows: At the convergence state T, node features are strongly oversmoothed and graph structure is highly scarce. \nSince existing GNNs rely on a message-passing mechanism with an adjacency matrix, most of them suffered from oversmoothing problem, indicating limitations in finding the meaningful latent factors from graph data from the convergence state. Therefore, we define the Denoising network with a learnable parameter, the latent Laplacian, to ensure GDM can extract latent information as much as possible.\n\n> Q(1). From the Leaderboards of OGB(https://ogb.stanford.edu/docs/leader_linkprop/), the experimental results of this paper are not very competitive. \n/ W(4). The experimental results show the proposed method doesn\u2019t achieve competitive performance in Link prediction (https://ogb.stanford.edu/docs/leader_linkprop/). \n\nThe top-ranked performances on the OGB leaderboard are mostly a result of combining various auxiliary techniques such as augmentations or additional plug-in methods. \n\nIt is noteworthy that the goal of GDM is to learn latent graph representations universally applicable to network graph learning tasks but capturing underlying latent factors that are intrinsically capable of given network graph learning tasks.\nWe compared our model against widely-known effective GNN-based models without such plug-ins, including SEAL which exhibits powerful performance on link prediction tasks among other models in OGB Leaderboard. \n\nBesides, the significance of our work arises from the motivation behind the GDM that existing graph representation learning models are investigated for improving particular graph representation learning tasks, yet they are often vulnerable to other tasks. For instance, in the link prediction task, some existing models assume generalizing some graph heuristics to find missing links. However, those existing models are not capable of learning node embeddings for node classification tasks.\n\nWe demonstrate that GDM mitigates its motivation as GDM results in competitive and meaningful performance on both major graph representation learning tasks: link prediction and semi-supervised node classification. Especially, in link prediction tasks, each dataset entails some specifics regarding whether graph structure property is more essential than node embeddings or vice versa. Conventional GNNs show poor performance often since they heavily rely on locality of node embeddings, whereas some competitive models designed for link prediction often fall on a dataset that weights more attention to node embeddings than graph structure properties.\nHowever, GDM consistently show comparable outputs across all realistic large networks graphs in OGB.\n \n> Some important baselines are missing, such as GIN, on the node classification task.\n\nWe indeed know GIN is outstanding in graph representation learning, particularly in graph classification. Since its motivation is to  capture graph isomorphism, ultimately learning small graph representations that are invariant to the same isomorphism, GIN sometimes shows trivial result in network graph representation learning tasks.\n\nAs reviewer suggested, we conducted semi-supervised node classification task on OGB-Arxiv. The results are as follows:\n\n|        | **K=1** | **K=5** | **K=10** |\n|----------------|---------|---------|----------|\n| **GIN**        | $5.32\\pm{5.92}$  | $29.22\\pm{2.28}$ | $34.98\\pm{2.43}$  |\n| **GCN**        | $31.69\\pm{2.74}$ | $52.97\\pm{0.94}$ | $58.39\\pm{0.50}$  |\n| **GDM (Ours)** | $38.40\\pm{1.64}$ | $57.22\\pm{0.85}$ | $60.97\\pm{0.40}$  |\n\nNote that our semi-supervised node classification setting is extremely restrained, hence, some GNNs may show low performances if mechanism does not propagate information. We verified that GIN is not suitable for our task. \nIndeed, our experiments did not omit strong baselines."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625374731,
                "cdate": 1700625374731,
                "tmdate": 1700625374731,
                "mdate": 1700625374731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "onClLE1xM8",
            "forum": "ExiBN1ZWJn",
            "replyto": "ExiBN1ZWJn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_Uwe6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_Uwe6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a Graph Dissipation Model (GDM), an innovative framework designed for both link prediction and node classification tasks in graph-structured data. The novelty lies in a coupled diffusion process that merges structure-based and feature-based diffusion mechanisms. Through exhaustive experiments on multiple datasets from the Open Graph Benchmark (OGB), the authors empirically show that GDM outperforms several state-of-the-art methods across different metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Comprehensive Approach - The GDM model is versatile in its application as it targets both link prediction and node classification. This comprehensive scope extends its relevance to a broader set of graph-based tasks, making the paper potentially impactful in the field. \n\nStrong empirical results - The paper takes advantage of the Open Graph Benchmark, a standard and well-regarded set of datasets, providing a robust testing ground for the GDM. Additionally, the authors compare GDM against a wide variety of existing methods, both classical and state-of-the-art, to establish its superiority. Overall, the proposed method performs favorably compared with other baselines."
                },
                "weaknesses": {
                    "value": "Omission of graph generation performance - While the paper innovatively adapts the DDPM to graph-based tasks, it focuses solely on node classification and link prediction for evaluation. The absence of comparative performance analysis on graph generation tasks against existing algorithms leaves an important aspect of its applicability unexplored.\n\nAbsence of sensitivity analysis - The model introduces several hyperparameters, including weight tuning parameters and the length of diffusion steps. The paper lacks an examination of how variations in these parameters impact the model's performance, making it difficult to fully justify the model's design choices.\n\nInsufficient theoretical underpinning - Despite presenting a novel methodology, the paper falls short in providing an in-depth theoretical discussion to substantiate its claims. Specifically, it asserts that the model \"captures latent factors for any given downstream task,\" but fails to offer comprehensive evidence or discussion that would bolster such a statement."
                },
                "questions": {
                    "value": "This is a follow up of the weakness one: The paper's title claims \"DENOISING GRAPH DISSIPATION MODEL IMPROVES\nGRAPH REPRESENTATION LEARNING\". Is this claim only valid for the proposed denoising graph dissipation model? Do other DDPM model or more generally other graph generation model help improve graph representation learning? Also, have the authors tried to evaluate the graph generation performance method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812238704,
            "cdate": 1698812238704,
            "tmdate": 1699636541275,
            "mdate": 1699636541275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uyzFFhW01m",
                "forum": "ExiBN1ZWJn",
                "replyto": "onClLE1xM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Uwe6 - Weakness (1), (2)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating time to review our submission and providing important comments! We are pleased to address your feedback as follows:\n\n> Omission of graph generation performance - While the paper innovatively adapts the DDPM to graph-based tasks, it focuses solely on node classification and link prediction for evaluation. The absence of comparative performance analysis on graph generation tasks against existing algorithms leaves an important aspect of its applicability unexplored.\n\nGDM aims to effectively capture and learn the underlying latent factors of a network graph for graph representation learning tasks. Therefore, to demonstrate its capability in learning the comprehensive representation of network graphs, we conducted link prediction tasks and semi-supervised node classification tasks. However, graph generation tasks aim to generate a graph structure different from the input graph while maintaining certain graph statistics or properties. Since such graph generation tasks differ in nature from the motivation of GDM which is to learn feature-structure integrated representations for network graph learning, graph generation is not a suitable task for validating the effectiveness of GDM in addressing our motivation. Therefore, we conducted experiments on link prediction and node classification which are major graph representation learning tasks.\n\n\n> Absence of sensitivity analysis - The model introduces several hyperparameters, including weight tuning parameters and the length of diffusion steps. The paper lacks an examination of how variations in these parameters impact the model's performance, making it difficult to fully justify the model's design choices.\n\nThank you for the suggestion. To address your concerns about hyperparameters, we conducted the sensitivity analysis on OGB-Collab. The results are as follows: \n\n| Timestep T  | 6    | 15   | 20   | 30   |  $\\lambda$(Recon)  | 0.001 | 0.01  | 0.1   | 1     | $\\gamma$(Struc_Diss)  | 0.003 | 0.03  | 0.3   | 1  |  $\\beta_{r}$(feat_pred)  | 0.002 | 0.02  | 0.2   | 1     | $\\beta_1$(Recon_feat)  | 0.002 | 0.02  | 0.2   | 1     |\n|-----|------|------|------|------|------|-------|-------|-------|-------| ---|-------|-------|-------|-------| ---|-------|-------|-------|-------| ---|-------|-------|-------|-------|\n| **Accuracy** | 53.89| 53.92| 54.01| 53.84| **Accuracy** | 52.28 | 52.54 | 52.80 | 53.89| **Accuracy** | 52.73 | 53.89 | 52.32 | 52.98| **Accuracy** | 52.11 | 53.89 | 52.35 | 52.77| **Accuracy** | 52.95 | 53.44 | 53.89 | 53.36|\n\nAs can be seen from the above results, the overall hyperparameters show robust performance.\nWe add a more legible version of the hyperparameter analysis in the Appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576521113,
                "cdate": 1700576521113,
                "tmdate": 1700580076641,
                "mdate": 1700580076641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i3heUESmA4",
                "forum": "ExiBN1ZWJn",
                "replyto": "onClLE1xM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Uwe6 - Weakness (3)"
                    },
                    "comment": {
                        "value": "> Insufficient theoretical underpinning - Despite presenting a novel methodology, the paper falls short in providing an in-depth theoretical discussion to substantiate its claims. Specifically, it asserts that the model \"captures latent factors for any given downstream task,\" but fails to offer comprehensive evidence or discussion that would bolster such a statement.\n\nGDM entails two perspectives: the diffusion model perspective and graph spectral perspective, both of which were discussed in Preliminary (Sec.2) of our work. The goal of GDM is to learn the underlying latent factor (distribution) of the network graph necessary for a given graph representation learning task. However, existing GNNs struggle to capture this sufficiently. For instance, while existing GNNs perform well in node classification tasks, they may fail to find the necessary latent factors when homophily is low or the number of learnable labels is extremely limited. Prior studies have therefore focused on improving task performance by training specialized assumptions for a particular task. We raise the need for capturing graph latent factors for a given graph representation learning task without assumptions dependent on that task.\n\nThe diffusion models capture the latent data distributions of images in pixel space that are arbitrary and challenging to capture directly. Therefore, diffusion models utilize the known Gaussian distribution as the source, add randomly sampled noise, and denoise it progressively to learn the arbitrary data distribution. Based on the philosophy of the diffusion model, we introduce GDM for finding the underlying latent factor of a network graph.\nHowever, unlike images, network graph data do not exist on widely-known space such as a  2-dimensional plane. Therefore, we approach from a graph spectral perspective and define the noise source using Laplacian smoothing. Laplacian smoothing gradually reduces the differences between signals on the graph by decaying frequencies. This is connected to reducing feature/information discrepancy between different nodes from a graph spatial perspective. When this process occurs gradually, it converges into oversmoothing in graph representation learning.\nDefining noise source as Laplacian smoothing to a network graph causes features to mix, which can be interpreted as noising or blurring from the perspective of the diffusion model. \nIHDM (Rissanen et al., 2022) approximates the heat dissipation equation and defines Gaussian blur on an infinite plane. Based on this, when we iteratively apply graph Laplacian smoothing, decays of frequency can be interpreted as information or signal dissipation. In this context, unlike conventional diffusion models or molecular graph diffusion models, our approach does not heavily rely on Markcov chain property, resulting in the capability of learning latent factors by deblurring or directly predicting previous states, as shown in $X_{t} = (I - \\alpha {L})^{t} X_{0} = U (I - \\alpha \\Lambda)^{t} U^{\\top} X_{0}$ (Eq.2) in our study.\nBesides, our approach reflects dependencies between data instances (i.e., nodes), considering the structural characteristics of a network graph. \n\nThank you for pointing out this aspect. We will address this concern in the revised manuscript, and these revisions would enhance the theoretical discussion of our proposed model and provide readers with a more comprehensive understanding of GDM's capabilities."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579828162,
                "cdate": 1700579828162,
                "tmdate": 1700579828162,
                "mdate": 1700579828162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v7I1CSzUsd",
                "forum": "ExiBN1ZWJn",
                "replyto": "onClLE1xM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Uwe6 - Questions"
                    },
                    "comment": {
                        "value": "> The paper's title claims \"DENOISING GRAPH DISSIPATION MODEL IMPROVES GRAPH REPRESENTATION LEARNING\". Is this claim only valid for the proposed denoising graph dissipation model? Do other DDPM model or more generally other graph generation model help improve graph representation learning? Also, have the authors tried to evaluate the graph generation performance method?\n\nThis is a crucial point of our contribution! Yes, our claim that GDM improves graph representation learning is valid. Also, other DDPM models or diffusion model-based graph generation models would not effectively improve graph representation learning. \nTo emphasize contributions of our work, let us provide a detailed explanation of the motivation behind GDM. The motivation behind GDM is to learn latent graph representations universally applicable to network graph learning tasks, capturing underlying latent factors entailed from a given network graph learning task. We initially raise this motivation, implying the significant contribution of GDM.\nNote that, existing models do not align with our motivation.\n\nHowever, other DDPM models and diffusion model-based graph generation models introduced so far are designed methods to enhance image generation and graph generation, respectively.  DDPM models may not improve graph representation learning since they do not consider the structural characteristics of a network graph. The essence of graph generation and graph representation learning is completely different. Graph generation tasks aim to generate a graph structure different from the input graph while maintaining certain graph properties, whereas graph representation learning is to learn latent representations from a network graph that are essential for prediction or classification of instances within a given graph. \n\nFor the question of graph generation performance, as the same point has been mentioned in Weakness above, \nwe respond in Weakness (1). \n\nWe hope that our response addresses the reviewer's concern and leads to stronger support. We appreciate your feedback to improve our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579967626,
                "cdate": 1700579967626,
                "tmdate": 1700580337748,
                "mdate": 1700580337748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2UjmXyis2w",
                "forum": "ExiBN1ZWJn",
                "replyto": "v7I1CSzUsd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Reviewer_Uwe6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Reviewer_Uwe6"
                ],
                "content": {
                    "comment": {
                        "value": "I sincerely appreciate the authors' efforts in providing additional sensitivity analysis, which underscores their dedication to enhancing graph representation learning. The evidence put forth about the efficacy of the proposed DDPM-based algorithm is commendable. Nevertheless, the rationale behind how the introduction of a modified DDPM model leads to these improvements remains somewhat obscure. The current explanations, while insightful, seem to require further depth. For instance, claims like 'capturing underlying latent factors' in the paper would greatly benefit from more robust theoretical support or clearer examples. Although the response adequately explains the application of Laplacian smoothing, it doesn't fully bridge the gap in understanding how the proposed methods enhance the capacity to capture latent factors. I am keenly looking forward to a more rigorous theoretical or empirical analysis in the revised version of this paper. However, due to my lingering concerns, I find it necessary to maintain the current score for the time being."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723889206,
                "cdate": 1700723889206,
                "tmdate": 1700723889206,
                "mdate": 1700723889206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vct2GIkzAv",
            "forum": "ExiBN1ZWJn",
            "replyto": "ExiBN1ZWJn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_7MVc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5365/Reviewer_7MVc"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a graph denoising diffusion model using Laplacian smoothing and edge deletion as the noise source. Authors claimed their new model achieve better and more general graph representation learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is an interesting topic to apply DDPM on graph representation learning. The authors had some good ideas on using Laplacian smoother and a coupled node feature similarity based edge removal schedule to add noises. They claimed this helps learn a more general representation by capturing both the attributes and graph structures.. There are some experiment results to seem to support it."
                },
                "weaknesses": {
                    "value": "The extension of Rissanen et al., 22' work, using Laplacian smoothing for graphs, was natural and even mentioned in the original paper's discussion section. And the claim of *no work on diffusion models for graph representation learning in both feature and structural aspects* feels like an exaggeration. In Vignac et al. 22' (also cited in the manuscript) uses both node features and structural information.\n\nThe experiments are not convincing to support authors' claim on the new GDM. Does it learn both feature and structural level information: table 1 only showed it outperforms SEAL on DDI and underperforms on the other three tasks."
                },
                "questions": {
                    "value": "1. The authors need more experiments/analysis to support the claim that their model can learn both features/structural information well.\n2. It would be more helpful if the authors can explore a bit more on the spectral meanings of Laplacian smoothing aside from information dissipation...the authors did mention it decays the high-frequency components on the spectral domain. Can we expand this more? Do we gain additional insights from using Laplacian smoothing. \n3. I assume GDM was trained on sampled subgraphs (?) but there was no mentioned on how this was done. Does the model only work on smaller graphs? \n4. Minior:\n\n    a). In the abstract, *...model leverages Laplacian smoothing and subgraph sampling as a noise source.* What does subgraph sampling mean here? Edge removal? \n\n    b). In the abstract, *...Graph dissipation model that captures latent factors for any given downstream task.* need to tune down. \n\n    c). Some parts of the paper are overly verbose, for example is Corollary 3.1 truly needed? \n\n    d). typos...for example pg5 *graph-strudtured*, pg8, *iffusion*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828317868,
            "cdate": 1698828317868,
            "tmdate": 1699636541181,
            "mdate": 1699636541181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R38ZEIrL8n",
                "forum": "ExiBN1ZWJn",
                "replyto": "vct2GIkzAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7MVc - Weakness (1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating time to review our submission and providing pivotal comments! We are pleased to address your feedback as follows:\n\n> 1. The extension of Rissanen et al., 22' work, using Laplacian smoothing for graphs, was natural and even mentioned in the original paper's discussion section. And the claim of no work on diffusion models for graph representation learning in both feature and structural aspects feels like an exaggeration. In Vignac et al. 22' (also cited in the manuscript) uses both node features and structural information.\n\nOur statement holds true without any exaggeration. Also, the studies the reviewer mentioned are significantly different from GDM.\nTo address potential misleading about the meaning of our work, we will provide a clearer explanation of the significance of GDM. \n\nThe motivation behind GDM is to learn latent graph representations universally applicable to network graph learning tasks, capturing underlying latent factors inherent in a given network graph representation learning tasks. Existing work has focused on defining task-dependent / task-specific assumptions for a particular graph learning task. For instance, in the link prediction task, some existing models assume generalizing some graph heuristics to find missing links. However, those existing models are not capable of learning node embeddings for node classification tasks. On the other hand, existing GNNs usually show good performance in node classification tasks while showing limitation in link prediction tasks. Hence, there are no powerful models that can solve both link prediction and node classification tasks.\n\nTo address this issue, we introduce GDM, a diffusion model-based graph representation learning approach that considers both features and structure of a network graph. Leveraging the capabilities of diffusion models to capture arbitrary data distributions, GDM learns comprehensive and integrated latent representations crucial for given network graph representation learning tasks (e.g., link prediction). Remarkably, it achieves competitive results in both major graph representation learning tasks, link prediction and node classification, without applying task-specific assumptions or biases into a model. As far as our knowledge extends, such research has not been introduced before, highlighting the novelty of our study.\n\nBesides, GDM is completely different from IHDM (Rissanen et al., 2022) and DiGress (Vignac et al., 2022). \nIt differs in data domain, motivation, and noise source. \n- Difference from IHDM (Rissanen et al., 2022):\nUnlike GDM, IHDM incorporates the multi-resolution nature of the image domain as an inductive bias to enhance the quality of image generation. Not only is IHDM not in the graph domain, but its motivation is also different as it focuses on image generation. While both GDM and IHDM use smoothing/blurring in the forward process, their purposes and definitions differ. GDM defines the noise source as Laplacian smoothing since nodes, the data instances in the graph domain, have dependencies, necessitating the incorporation of this characteristic as a noise source. Therefore, we define GDM's noise source as Laplacian smoothing, resulting in an oversmoothed final state graph. In contrast, IHDM incorporates the multi-resolution nature of images and uses approximated Gaussian blur in continuous space.\n- Difference from DiGress (Vignac et al., 2022):\nWhile GDM is a model for graph representation learning in network-shaped graphs, DiGress is a model for molecular graph generation. In other words, GDM and DiGress target different types of graphs. Molecular graphs have node and edge categorical information, and there are multiple graphs in the dataset, making it easier to define the distribution needed for the diffusion model. However, network graphs consist of a single given graph, and sometimes the node's categorical information is incomplete or absent, making it insufficient to define the distribution of the graph."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469228714,
                "cdate": 1700469228714,
                "tmdate": 1700469228714,
                "mdate": 1700469228714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XZeovjRyjp",
                "forum": "ExiBN1ZWJn",
                "replyto": "vct2GIkzAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Resonpose to Reviewer 7MVc - Weakness (2), Question (1), (2)"
                    },
                    "comment": {
                        "value": "> 2. The experiments are not convincing to support authors' claim on the new GDM. Does it learn both feature and structural level information: table 1 only showed it outperforms SEAL on DDI and underperforms on the other three tasks.\n\nThe motivation behind GDM is to capture latent factors inherent in network graph representation learning tasks through the intuition of diffusion models, thereby learning integrated graph representations that encompass the feature- and structure-related latent information necessary for a given task. To validate this, we conducted experiments focusing on the Link Prediction task. This is because Link Prediction requires learning not only node embeddings but also structural information [1]. \nHowever, The dependencies on node feature information and graph structural information vary with the dataset. Referring to Table 1 in our work, it implies that OGB-PPA and OGB-DDI place higher importance on structural information and feature information, respectively. OGB-Collab and Citation2 imply the necessity of both types of information for optimal prediction. \n\nOur model, GDM, consistently shows competitive (best or second-best) performance across all four datasets. While SEAL, a model specialized for link prediction, also demonstrates good performance, it shows poor performance in OGB-DDI, compared to GDM. Given that OGB-DDI appears to heavily rely on node embeddings, this result demonstrates that GDM is capable of capturing both latent factors that are informative for solving a given task, unlike other models specialized to specific factors. In other words, GDM\u2019s competitive performance across the four datasets indicates its ability to effectively extract and learn integrated representation about the necessary structure and features for link prediction within a given graph. It is important to note that we did not incorporate task-specific biases into the model. Therefore, we empirically validated GDM has addressed its motivation, demonstrating its ability to capture essential underlying latent factors in a given graph required to handle the link prediction task.\n\n> Q1. The authors need more experiments/analysis to support the claim that their model can learn both features/structural information well.\n\nTo demonstrate that our model effectively learns both feature and structural information in network graphs, we conducted link prediction experiments, which are known to require not only node embeddings but also graph structural information. In these experiments, our model shows highly competitive performance, verifying its ability to capture both aspects effectively. Additionally, to demonstrate its ability to learn node embedding, we conducted semi-supervised node classification tasks. GDM outperforms on all datasets. Furthermore,  through the ablation study, we analyzed the validity of whether GDM effectively captures feature and structure latent factors by feature process and structural process.\n\n> Q2. It would be more helpful if the authors can explore a bit more on the spectral meanings of Laplacian smoothing aside from information dissipation...the authors did mention it decays the high-frequency components on the spectral domain. Can we expand this more? Do we gain additional insights from using Laplacian smoothing.\n\nYes, we would like to provide the explanation of the insight into the decay of high frequency.\n\n As high frequency gradually decays, the difference between signals will also gradually diminish. In the spatial domain of a graph, it is interpreted as a loss of information regarding the distinct features among nodes. This implies that the amount of lost signal or information varies for each node at each time step, suggesting that our model GDM can learn the latent factors of a given graph by recovering this dissipated signal or information. This aligns with the philosophy and characteristics of diffusion models.\nAdditionally, in the real world, there can be noise or missing information (e.g., missing links) in the features or adjacency of a network graph. In other words, observations may not constitute a perfect ground truth. In real-world scenarios, graph representation learning involves learning from a noisy observed graph to approach a more optimal graph representation. From the resolution perspective, our insight is analogous to utilizing a coarse-to-fine strategy to enhance image resolution quality.\n\n\n\n[1] Link prediction based on graph neural networks"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472409004,
                "cdate": 1700472409004,
                "tmdate": 1700472409004,
                "mdate": 1700472409004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lgKDUvSohM",
                "forum": "ExiBN1ZWJn",
                "replyto": "vct2GIkzAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7MVc - Question (3), Minor"
                    },
                    "comment": {
                        "value": "> Q3. I assume GDM was trained on sampled subgraphs (?) but there was no mentioned on how this was done. Does the model only work on smaller graphs?\n\nSorry for the confusion. Indeed, GDN can learn large network graphs. OGB datasets where we conduct experiments are realistic and large-scale graph datasets. Regarding the question about subgraph sampling mentioned in the abstract, subgraph sampling refers to edge removal. We define feature-dependent stochastic sampling to progressively lift feature dissipation into the structure, gradually dropping existing edges from a given graph. Regarding edge sampling, during the implementation of the reverse process, the denoising task is performed on randomly sampled edges from the set of dropped edges instead of all of them for efficiency. \nWe will clarify this in the revised manuscript.\n\n> Minor a). In the abstract, ...model leverages Laplacian smoothing and subgraph sampling as a noise source. What does subgraph sampling mean here? Edge removal?\n\nSorry for the confusion. As the same point has been mentioned in Question (3), we respond with the same explanation in Question (3).\nWe will clarify the term in the revised manuscript.\n\n> Minor b). In the abstract, ...Graph dissipation model that captures latent factors for any given downstream task. need to tune down.\n\nSince this point aligns with Weakness (1), we can tell our statement is without exaggeration. We raise the motivation of universally applicable model that learns network graph representation while naturally considering specifics of a given task without injecting task-oriented biases. To address the motivation, we proposed GDM and demonstrated effectiveness in major graph representation learning tasks: link prediction task and semi-supervised node classification task. \n\nHowever, we will rephrase \"for any given downstream task\" into \"for any given graph representation learning task.\" to distinguish it from graph generation tasks which is a very different research area.\n\n> Some parts of the paper are overly verbose, for example is Corollary 3.1 truly needed?\n\nSince understanding diffusion models and over-smoothing requires background knowledge, we decided to provide detailed explanations for a more straightforward understanding of intuition behind GDM, which is inseparably related to diffusion models and relation between Laplacian smoothing and over-smoothing.\n\nThanks to the reviewer, we will fix typos in the revised manuscript.\n\nWe hope that our response addresses the reviewer's concern and leads to stronger support. \nWe appreciate your feedback to improve our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474807263,
                "cdate": 1700474807263,
                "tmdate": 1700640538264,
                "mdate": 1700640538264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PMAqpRCabs",
                "forum": "ExiBN1ZWJn",
                "replyto": "vct2GIkzAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5365/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We uploaded the revised paper.\nWe revised some points the reviewer suggested.\n- More detailed explanation of using Laplacian smoothing as noise source to diffuse and dissipate graph signal (Section 4)\n- Revise some confusing notations, terms, equations into more precise and consistent expressions (Section 1, 4)\n- Clarify motivation and novelty of GDM (Section 1)\n- Clarify GDM is for network graph representation learning (Section1)\n- Derivation of loss function (Appendix)\n- Hyperparameter sensitivity analysis (Appendix)\n\nWe hope that our response addresses the reviewer's concern and leads to stronger support. \nWe sincerely appreciate the feedback to improve the paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731066157,
                "cdate": 1700731066157,
                "tmdate": 1700731066157,
                "mdate": 1700731066157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]