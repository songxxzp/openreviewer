[
    {
        "title": "A Unified Framework for Bayesian Optimization under Contextual Uncertainty"
    },
    {
        "review": {
            "id": "UZptIADEBl",
            "forum": "oMNkj4ER7V",
            "replyto": "oMNkj4ER7V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
            ],
            "content": {
                "summary": {
                    "value": "Many problem settings in Bayesian optimization (BayesOpt) require robust solutions to the optimization problem, where the decision maker needs to choose solutions that work well under different contexts that are not controllable.\nThis paper presents a formulation that generalizes many different BayesOpt settings related to robust optimization, and proposes using Thompson sampling as the optimization policy.\nThe authors first show a regret bound of this policy that is sublinear given specific assumptions and subsequently demonstrate the empirical performance of the policy under various optimization settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and has a clear exposition.\nI enjoyed reading through the formulation of the distributionally robust optimization problem and how it generalizes to previously proposed settings, especially by incorporating the right derivative of the objective.\nThis general framework allows relating many problem formulations that have been proposed in the literature.\nAs explained in the paper, by setting the hyperparameters, one can even realize novel optimization formulations that \"interpolate\" the previously proposed formulations at the extremes, which allows more expressiveness in designing optimization objectives that fit a user's preference.\nThe proposed TS seems to work well across the experiments."
                },
                "weaknesses": {
                    "value": "The authors can consider inspecting why in various cases (in the Infection problem for DRO), TS-BOCU has almost linear regret.\nIt would be interesting to see if there are types of problems that the policy tends to perform badly on.\n(Perhaps this is connected to the insight that the algorithm resulting from setting $\\alpha, \\beta > 0$ tends to be more robust?)\n\nI am a bit confused about the assumption that $\\mathcal{X}$ and $\\mathcal{C}$ are finite: Is it necessary for the theoretical guarantee (since Section 4.1 mentions that the result can be extended to infinite sets)?\nIn the experiments (for example, with the Hartmann functions), are you constraining the search spaces to be finite?\nMy understanding is that  the algorithm can run on continuous search spaces too.\n\nI think the paper can benefit from extending the discussion at the end of Section 3.1 and offer guidance for setting $\\alpha$, $\\beta$, and $\\epsilon$ in practice."
                },
                "questions": {
                    "value": "- Instead of having both $\\alpha$ and $\\beta$, can't we follow the formulation of mean-risk tradeoff and only vary the weight for the $\\delta(\\mathbf{x})$ term, unless, for example, $\\alpha = \\beta = 1$ gives a different objective than $\\alpha = \\beta = 0.5$ (assuming the same $\\epsilon$)?\n- As I understand, if $\\epsilon_t = d(\\mathbf{p}_t, \\mathbf{p}^*)$ does not approach $0$ with probability $1$, we don't obtain the sublinear regret result.\nHow does this might affect performance in practice?\nCan we still perform well if $\\mathbf{p}_t$ is sufficiently different from $\\mathbf{p}^*$?\nAre there situations where $\\mathbf{p}^*$ is very hard to learn?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4649/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4649/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698516849773,
            "cdate": 1698516849773,
            "tmdate": 1699636445272,
            "mdate": 1699636445272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xIwMYpSUI8",
                "forum": "oMNkj4ER7V",
                "replyto": "UZptIADEBl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time and effort spent writing this review, and for your interesting questions.\n\n1. **On the necessity of both** $\\alpha$ **and** $\\beta$: Both parameters are necessary, as robust satisficing and worst-case sensitivity have $\\alpha=0$ and do not involve the $v(\\mathbf x)$ term at all. If only $\\beta$ were present, we would not be able to set the weight on the $v(\\mathbf x)$ term to $0$ to recover these objectives.\n\n2. **On what happens if** $d(\\mathbf p_t, \\mathbf p^*)$ **does not approach** $0$: In practice, even if $\\mathbf p_t$ does not approach $\\mathbf p^*$, we are still able to get good performance. In our experiments, for simplicity, we keep $\\mathbf p_t$, $\\mathbf p^*$, and $\\epsilon_t > 0$ constants throughout, and we see that TS-BOCU still performs well and displays the sublinear property. The necessary condition for good performance is nuanced: informally speaking, we can perform well as long as the learner is able to reduce the uncertainty at all 'relevant' decision-context points, where 'relevance' is determined by the supports of the distributions within the uncertainty set $\\mathcal U_t$. Notice that $\\mathbf p^*$ only governs the distribution of contexts observed, and is not a part of the uncertainty objective: only the learner-chosen parameters $\\mathbf p_t$ and $\\epsilon_t$ are. This means that, if the true underlying function $f$ were completely known to the learner, it does not matter what $\\mathbf p^*$ is, the learner would be able to solve the problem exactly. $\\epsilon_t \\rightarrow 0$ (with the constraint $d(\\mathbf p_t, \\mathbf p^*) \\leq \\epsilon_t$) is a sufficient but not necessary condition for good performance. $\\mathbf p^*$ gets more difficult to learn as $|\\mathcal C|$ increases.\n\n3. **On the performance of TS-BOCU on the DRO uncertainty objective in Infection**: While the performance seems not great, we still observe the sublinear property towards the final iterations as the growth of the curve slows down. If we were to run the experiments for more iterations, this sublinear property would be more clear. Also, it may simply be the case that the baselines are very strong in this setting. UCB-BOCU is explicitly designed for the DRO uncertainty objective, whereas a possible reason that UCB-RO performs well is that the RO optimal decision coincides with the DRO optimal decision with this objective function. In any case, the sublinear property still assures us that our algorithm is able to handle the DRO objective as expected, although the relative performance between baselines will certainly vary depending on specific objective function.\n\n4. **On the finite decision and context spaces**: We have formalized the extension to continuous, infinite decision sets in the new uploaded revision. The regret bound maintains the same order with respect to $T$, and the conditions necessary for sublinear regret remain the same as explained in the body of the paper. Please see our global response titled \"Revision for infinite decision sets and intractability of infinite context sets\" (https://openreview.net/forum?id=oMNkj4ER7V&noteId=EjvMtBW1sK) for more details. In that post, we have also defended the assumption of a finite context set. To summarize the salient points, DRO with infinite context sets is intractable with general convex distribution distances $d$, and this difficulty leads to the assumption of a finite context set being standard in the DRBO literature. Our experiment results are still from an implementation with a finite decision set, but we imagine that the results would not be too different if the optimization were performed over an infinite decision set instead.\n\n5. **On guidance for setting** $\\alpha$, $\\beta$**, and** $\\epsilon$ **in practice**: We agree with you that such a discussion would be interesting and useful. However, we also fear that such a discussion would become more subjective and philosophical than technical, and may depend too much on the individual use case. For example, what arguments could be made for a risk-averse learner to optimize the value-at-risk versus the conditional value-at-risk versus the mean-variance tradeoff? We decided in the end that an extended discussion on this topic would be out of the scope of this paper. We leave the choice of uncertainty objective to the practitioner; our goal was to show that the practitioner can choose between a large diversity of uncertainty objectives with a single mental framework and a single algorithm.\n\nWe hope this response has improved your opinion of our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060483108,
                "cdate": 1700060483108,
                "tmdate": 1700060483108,
                "mdate": 1700060483108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JvIJWfl1b5",
                "forum": "oMNkj4ER7V",
                "replyto": "xIwMYpSUI8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
                ],
                "content": {
                    "title": {
                        "value": "thanks"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. The point about $\\epsilon_t$ gives me new insight into the setting. It does seem reasonable that there are finitely many context variables $\\mathbf{c}^{(i)}$."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250729997,
                "cdate": 1700250729997,
                "tmdate": 1700250729997,
                "mdate": 1700250729997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XX2vE9T675",
            "forum": "oMNkj4ER7V",
            "replyto": "oMNkj4ER7V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_B6kd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_B6kd"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies robust BO when there is context uncertainty, and designs a TS algorithm based on a general framework that incorporate a large number of risk-sensitive learning objectives. The authors substantiate its efficacy with theoretical analysis and validate the algorithm's performance and adaptability through experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clear and easy to follow. The propose method is solid and with decent theoretical and numerical evidence."
                },
                "weaknesses": {
                    "value": "The significance is of question to me - though it is good to have a unified form for multiple previously proposed objectives, the unification achieved in this paper seems straightforward (adding two parameters) and the additional technical challenge (e.g., in algrotihm design or analysis) is unclear, and it is not clear whether those new objectives are really of significance for practitioners."
                },
                "questions": {
                    "value": "1. Is the first-derivative objective only previously proposed for GP, or is also applicable in other areas? Is 3.1 only novel in GP literature, or for the first time also in other areas? \n\n2. Why finite context and action space? These read very limited. Are they also required by prior works?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607774402,
            "cdate": 1698607774402,
            "tmdate": 1699636445167,
            "mdate": 1699636445167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NBZBhdhYXX",
                "forum": "oMNkj4ER7V",
                "replyto": "XX2vE9T675",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time and effort taken to review our paper.\n\n1. **On the significance of the unification and the analysis**: We believe that the significance of the unification goes beyond adding two parameters, as the right-derivative of the DRO objective at any $\\epsilon$ was not known in the BOCU literature to be an uncertainty objective of interest. Tay et. al. (2022) introduced the idea of worst-case sensitivity to DRBO, but the worst-case sensitivity is only the right-derivative at $\\epsilon = 0$. Our work is the first to establish the general usefulness of the right-derivative by proving in Proposition 3.1 that the right-derivative at $\\epsilon > 0$ is of theoretical interest as well through the robust satisficing objective. We believe that this is a non-trivial and elegant extension of DRBO.  As you have pointed out, it is good to have a unified form for previously unrelated objectives, but the unification was anything but straightforward. \n\n    Recognizing and formally proving the relevance of the right-derivative was only the first part: the novel analysis of the regret bound for Thompson sampling when the right-derivative is part of the objective was not a straightforward undertaking. As we mention in the paper, the derivative term presents significant challenges: the limit term causes difficulties without a closed form, and the inner term is a difference of worst-case expected values which, informally speaking, `erases' the dependency on the uncertainty set and precludes bounding it in terms of the margin $\\epsilon$ as was done for the DRO objective. We encourage you to explore the supplementary material, in particular Lemma A.3 and its auxiliary lemmas Lemma A.4, A.5, A.11, A.12, and A.13, to see the work involved in bounding the DRO first-derivative regret. For rigor, we also prove with Proposition 4.2 that the upper bound sequence $U_t$ used in the proof of the regret bound is well-defined.\n\n2. **On the first-derivative objective**: Sections 2 and 3 on the general framework are general enough to be applicable outside of BO as well; note that the sequential learning setting and GPs are only introduced in Section 4. Proposition 3.1 is an original proposition that, to the best of our knowledge, has not been published anywhere else. It is entirely plausible that a similar result exists in the operations research literature that we are unaware of, and that we have simply re-discovered it. However, given that robust satisficing was only introduced this year (Long et. al., 2023), it seems unlikely.\n\n3. **On the finite decision and context spaces**: We have **extended our results to continuous, infinite decision sets in the new uploaded revision**. The regret bound maintains the same order with respect to $T$, and the conditions necessary for sublinear regret remain the same as explained in the body of the paper. Please see our global response titled \"Revision for infinite decision sets and intractability of infinite context sets\" (https://openreview.net/forum?id=oMNkj4ER7V&noteId=EjvMtBW1sK) for more details. In that post, we have also defended the assumption of a finite context set. To summarize the salient points, DRO with infinite context sets is intractable with general convex distribution distances $d$, and this difficulty leads to the assumption of a finite context set being standard in the DRBO literature.\n\nWe hope this response has improved your opinion of our paper. Please let us know if you have any more concerns, we are happy to engage further."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973776660,
                "cdate": 1699973776660,
                "tmdate": 1699974117462,
                "mdate": 1699974117462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zWSQ2mevQU",
                "forum": "oMNkj4ER7V",
                "replyto": "XX2vE9T675",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Hello, a gentle reminder that the author-reviewer discussion period ends in about 2 days. If you have any remaining questions or concerns about our work after reading our response, please let us know so we may address them. Thanks!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578252133,
                "cdate": 1700578252133,
                "tmdate": 1700578252133,
                "mdate": 1700578252133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e1FOa7vxjI",
            "forum": "oMNkj4ER7V",
            "replyto": "oMNkj4ER7V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
            ],
            "content": {
                "summary": {
                    "value": "A framework for Bayesian optimization under contextual uncertainty unifies various formulation of Bayesian optimization, including distributionally robust optimization, stochastic optimization, robust optimization, robust satisficing, worst-case sensitivity, and mean-risk tradeoff.  The authors provide theoretical analyses on regret bounds and experimental results to compare several Bayesian optimization algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This work serves the comprehensive understanding of Bayesian optimization under contextual uncertainty.\n* Paper is well-organized."
                },
                "weaknesses": {
                    "value": "* Paper is hard to follow.  For example,\n\nThe sentence \"While standard BO assumes that the learner has full control over all input variables to the objective function, in many practical scenarios, the learner only has control over a subset of variables (decision variables), while the other variables (context variables) may be randomly determined by the environment\" is too complex.  There are two whiles in one sentence.\n\nThe sentence \"We assume that, at every iteration, some reference distribution $\\boldsymbol p$ is known that captures the learner's prior knowledge of the distribution governing $\\boldsymbol c$\" is grammatically wrong.\n\n\"a probability vector in $\\mathbb{R}^n$\" should be \"a probability vector in $[0, 1]^n$\" for readability and understandability.\n\nI think there are other grammar and presentation issues.  Please revise your submission carefully.\n\n* I think that some assumptions are too strong.  For example, the assumptions on finite sets of $\\mathcal{X}$ and $\\mathcal{C}$ are not practical.  Moreover, I do not understand why $\\boldsymbol p$ is known at the beginning of the optimization.  This assumption is not practically meaningful.\n\n* Since theoretical results are built on the assumptions on finite sets of $\\mathcal{X}$ and $\\mathcal{C}$, they are limited.\n\n* Reasoning and justification behind experimental results are not appropriately provided."
                },
                "questions": {
                    "value": "* Table 1 can have a column for the corresponding references.  It would help understand and compare diverse algorithms\n\n* Could you explain the intuition and meaning of knowing $\\boldsymbol p$ at the beginning?\n\n* I am not sure that the ICLR paper format allows it, but the table captions should be located on top of the tables."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4649/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4649/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788933651,
            "cdate": 1698788933651,
            "tmdate": 1700681207681,
            "mdate": 1700681207681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TZbWvrJw8c",
                "forum": "oMNkj4ER7V",
                "replyto": "e1FOa7vxjI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review.\n\n1. **On the finite decision and context spaces**: We have **extended our results to continuous, infinite decision sets** in the new uploaded revision. The regret bound maintains the same order with respect to $T$, and the conditions necessary for sublinear regret remain the same as explained in the body of the paper. Please see our global response titled \"Revision for infinite decision sets and intractability of infinite context sets\" (https://openreview.net/forum?id=oMNkj4ER7V&noteId=EjvMtBW1sK) for more details. In that post, we have also defended the assumption of a finite context set. To summarize the salient points, DRO with infinite context sets is intractable with general convex distribution distances $d$, and this difficulty leads to the **assumption of a finite context set being standard in the DRBO literature**. This should address your concerns about the strengths of these assumptions and the corresponding usefulness of our theoretical results.\n2. **On the reference distribution $\\mathbf p$**: We wish to emphasize that the reference distribution $\\mathbf p$ **is a problem parameter chosen by the learner and captures the learner's prior belief of the context distribution, and is not the true distribution** $\\mathbf p^*$. It is very possible in real life to obtain a reasonable prior distribution for random contexts, such as from expert knowledge or estimation from historical data. For instance, in the introduction's example of sunlight in farming, the amount of solar radiation is a quantity that has been tracked by meteorological organizations for decades (e.g., https://nsrdb.nrel.gov/). A reasonable reference distribution of solar radiation can be estimated from this data. If the learner has no expert knowledge or historical data, they can simply set the reference distribution at iteration 1 $\\mathbf p_1$ to be the uniform distribution with a large margin $\\epsilon_1$ to capture the uncertainty, then update their reference distributions $\\mathbf p_t$ for $t>1$ based on the observed contexts over the learning procedure. This is the 'data-driven' setting in Kirschner et. al. (2020) described under 'Conditions for sublinear regret' in Sec. 4.\n\n    From a more technical perspective, it is not possible to even define the DRO problem without a reference distribution. Every problem definition under the BOCU framework needs some reference distribution to say anything useful, including BO for expected values (Toscano-Palmarin \\& Frazier, 2022), risk-averse BO (Cakmak et al., 2020; Nguyen et al., 2021a;b), and prior work in distributionally robust BO (Kirschner et al., 2020; Nguyen et al., 2020; Tay et al., 2022).\n3. **On the \"reasoning and justification behind experimental results\"**. We have to kindly ask you to elaborate on your concerns. What exactly about the design of the experiments do you not understand or agree with? Is it the choice of 1) objective functions, 2) uncertainty objectives, 3) distribution distances, 4) problem parameters, or 5) baselines? We believe we have made a reasonable effort to compare our proposed algorithm to suitable baselines across a diversity of objective functions, uncertainty objectives, and distribution distances. If there is anything that does not make sense to you, please specify them so that we can discuss them and improve our paper.\n4. Your style suggestions have been noted. In the latest revision, we have moved the caption of Table 1 to the top. Due to ICLR's citation style that requires the name and date, including the references in Table 1 would cause it to be too cluttered. We have instead included equation numbers beside each uncertainty objective; the relevant references are always right before each equation.\n\nWe hope these revisions and clarifications have improved your opinion of our paper. We look forward to hearing more from you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021072828,
                "cdate": 1700021072828,
                "tmdate": 1700021273093,
                "mdate": 1700021273093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jhJVi1uEZK",
                "forum": "oMNkj4ER7V",
                "replyto": "e1FOa7vxjI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Hello, a gentle reminder that the author-reviewer discussion period ends in about 2 days. If you have any remaining questions or concerns about our work after reading our response, please let us know so we may address them. Thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578209651,
                "cdate": 1700578209651,
                "tmdate": 1700578209651,
                "mdate": 1700578209651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uCzJ2Aic6a",
                "forum": "oMNkj4ER7V",
                "replyto": "jhJVi1uEZK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
                ],
                "content": {
                    "comment": {
                        "value": "It is hard to understand which part is updated (OpenReview does not provide the previous version now).  Could you highlight the changes in red or any colors?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592927971,
                "cdate": 1700592927971,
                "tmdate": 1700592927971,
                "mdate": 1700592927971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S6cXEYREwe",
                "forum": "oMNkj4ER7V",
                "replyto": "DAiL6ZeQGV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your update.\n\nSome of my concerns are resolved, so I am slightly increasing the score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681174836,
                "cdate": 1700681174836,
                "tmdate": 1700681174836,
                "mdate": 1700681174836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4Mc5wP9Oi0",
            "forum": "oMNkj4ER7V",
            "replyto": "oMNkj4ER7V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_Lem3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4649/Reviewer_Lem3"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the distributionally robust Bayesian optimization (DRBO) framework to a more general framework called \u201cBO under contextual uncertainty\u201d (BOCU). BOCU targets the problem of maximizing some uncertainty objective that takes the context distribution into account. Example problems include worst-case sensitivity, mean-risk trade-offs, DRBO, robust satisficing. The paper develops a general Thompson sampling algorithm that can optimize any objective within the framework. The paper also derives Bayesian regret bound for their developed framework. Finally, some experiments are conducted to illustrate the sublinear regret properties of the framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is very well written and easy to understand. The problem settings, the previous works, the concepts are all well described.\n+ The paper tackles an interesting problem which is to unify different problems (with the same theme of BO under contextual uncertainty) into one framework. The paper also proposes a general method that can solves this unified framework with different objectives. Theoretical analysis is also conducted to guarantee the performance of the proposed method.\n+ The proposed method seems to be sound and reasonable to me.\n+ The experiments (though a bit limited) are also conducted in order to understand the behaviours of the proposed method and to confirm the theoretical analysis."
                },
                "weaknesses": {
                    "value": "To me, the main weaknesses of the paper are in the experimental evaluation. I list in the below some weak points that I found from the experimental evaluation:\n+ The problems used in the evaluation (GP, Hartmann 3, plant growth simulator, COVID epidemic model) have quite low dimensions, ranging from 2 to 5.\n+ I found the analysis regarding the experiments could be further elaborated. Currently, there is only one paragraph explaining a lot of results in one figure (Figure 2), I have to think a lot in order to understand what the reported results convey.\n+ For the COVID infection problem, I found the results of DRO are not too good. I\u2019m just wondering what are the issues of these cases?"
                },
                "questions": {
                    "value": "Apart from my comments and questions in the Weaknesses section, the authors could answer the additional following questions:\n+ In Theorem 4.1, are the assumptions used common assumptions used in this particular research topic? What are the implications of these assumptions? Is it possible for these assumptions to be occurred in practice?\n+ Also, from Theorem 4.1, is the maximum information gain \\gamma_T bounded as in the standard BO algorithms? Which kernels will guarantee this maximum information gain to be bounded?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698912554541,
            "cdate": 1698912554541,
            "tmdate": 1699636445027,
            "mdate": 1699636445027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xzw0ZfQIG0",
                "forum": "oMNkj4ER7V",
                "replyto": "4Mc5wP9Oi0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and your insightful comments.\n\n1. **On the assumptions in Theorem 4.1.**: Note that we have uploaded a new revision of the paper to extend our results to infinite decision sets, and in so doing require an additional assumption.\n\n    a. **Assumption 1, that the GP kernel is either the squared exponential kernel or the Mat\u00e9rn kernel with $\\nu > 2$**: This assumption is required for the extension to infinite decision sets, see our global post for more details. It is a standard assumption for the discretization argument (Kandasamy et al., 2018; Srinivas et al., 2010). This assumption is a modeling choice and thus can certainly be true in practice, in fact, these are the most popular kernels used in practice.\n\n\tb. **Assumption 2, that the distribution distance $d$ is the TV distance**: This assumption is not a common assumption in the relevant literature, and was made in order to aid regret analysis due to the challenging derivative term in our novel objective function. As mentioned in the paper, the assumption that $d$ is the TV distance aids analysis considerably as TV admits a closed form for the optimal solution of the convex optimization problem Equation 2 (see Lemma A.11 in Appendix A.11). This assumption is also a modeling choice and thus can also be true in practice if the practitioner wishes it to be.\n\n\tc. **Assumption 3a, that for all** $t\\leq T$, $d(\\mathbf p_t, \\mathbf p_t^*) \\leq \\epsilon_t$ : This is a standard assumption in DRBO (Kirschner et. al., 2020; Tay et. al., 2022). This assumption is used to ensure that the true distribution $\\mathbf p_t^*$ is within the uncertainty set $\\mathcal U_t$, so that we can bound the distance between $\\mathbf p_t^*$ and the worst-case distribution $\\mathbf q_t$ via a triangle inequality with the reference distribution $\\mathbf p_t$. Informally speaking, we need $\\mathbf p_t^*$ to be within some known distance from $\\mathbf q_t$, otherwise the regret (computed with $\\mathbf q_t$) cannot be related to the observed contexts (drawn from $\\mathbf p_t^*$). In practice, while $\\mathbf p_t$ and the margin $\\epsilon_t$ are chosen by the learner, $\\mathbf p_t^*$ is unknown and thus it is not possible to guarantee that it holds, other than with a trivially large $\\epsilon_t$. This issue can be alleviated by choosing $\\mathbf p_t$ that is likely to be close to $\\mathbf p_t^*$ via expert knowledge or estimation from historical data, or by taking the 'data-driven' approach from Kirschner et. al. (2020) and adapting $\\mathbf p_t$ to the observed contexts during the learning procedure.\n\n\td. **Assumption 3b, that for all $t\\leq T$ and all** $c \\in [|\\mathcal C|]$, $p^*_{t, c} \\geq p_{\\text{min}} > 0$. This assumption was also made in Theorem 4.2 in Inatsu et. al. (2022). We require this assumption again in order to aid regret analysis due to the challenging derivative term in our novel objective function. Intuitively, if a context has $0$ probability of being observed, the uncertainty at decision-context pairs with that context is very difficult to reduce. In practice, since $\\mathbf p_t^*$ is not controllable, this may not hold. However, this problem may be alleviated by a careful choice of context domain such that it is unlikely that any of the contexts have $0$ probability of occuring.\n\t\n2. **On the maximum information gain $\\gamma_T$**: $\\gamma_T$ is not bounded in $T$, but as long as $\\gamma_T < \\mathcal O(T)$, the overall regret bound is sublinear in $T$ and the algorithm converges to the optimal solution. Examples of kernels that fulfill this condition are the linear, squared exponential, and Mat\u00e9rn ($\\nu > 1$) kernels. From Theorem 5 in Srinivas et. al. (2010), for the linear kernel, $\\gamma_T = \\mathcal O(d \\log T)$; for the squared exponential kernel, $\\gamma_T = \\mathcal O((\\log T)^{d+1})$; for the Mat\u00e9rn kernel with $\\nu > 1$, $\\gamma_T = \\mathcal O(T^{d(d+1)/(2v+d(d+1))}(\\log T))$, where in this context $d = m + \\ell$ is the dimensionality of the joint decision-context space.\n\n3. **On the performance of TS-BOCU on the DRO uncertainty objective in Infection**: While the performance seems not great, we still observe the sublinear property towards the final iterations as the growth of the curve slows down. If we were to run the experiments for more iterations, this sublinear property would be more clear. Also, it may simply be the case that the baselines are very strong in this setting. UCB-BOCU is explicitly designed for the DRO uncertainty objective, whereas a possible reason that UCB-RO performs well is that the RO optimal decision coincides with the DRO optimal decision with this objective function. In any case, the sublinear property still assures us that our algorithm is able to handle the DRO objective as expected, although the relative performance between baselines will certainly vary depending on specific objective function.\n\nWe hope our response has improved your opinion of our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046669542,
                "cdate": 1700046669542,
                "tmdate": 1700046669542,
                "mdate": 1700046669542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lrabnbfPmw",
                "forum": "oMNkj4ER7V",
                "replyto": "4Mc5wP9Oi0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Hello, a gentle reminder that the author-reviewer discussion period ends in about 2 days. If you have any remaining questions or concerns about our work after reading our response, please let us know so we may discuss them. Thanks!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578145775,
                "cdate": 1700578145775,
                "tmdate": 1700578145775,
                "mdate": 1700578145775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tvsMvB682b",
                "forum": "oMNkj4ER7V",
                "replyto": "lrabnbfPmw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_Lem3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_Lem3"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Hi author(s),\n\nThank you for the response. Your response have cleared majority of my concerns (except the comments regarding the low dimensions of the problems used in the paper). I can understand the contribution of the paper better now."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653502406,
                "cdate": 1700653502406,
                "tmdate": 1700653502406,
                "mdate": 1700653502406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]