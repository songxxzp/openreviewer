[
    {
        "title": "A Cooperative-Game-Theoretical Model for Ad Hoc Teamwork"
    },
    {
        "review": {
            "id": "S6t7Qw20gm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
            ],
            "forum": "Y0yz1pmVfE",
            "replyto": "Y0yz1pmVfE",
            "content": {
                "summary": {
                    "value": "The authors propose a framework for multi-agent ad-hoc teamwork based around cooperative game theory. They define solution concepts and algorithms based on this concept, and run extensive experiments to compare their method to previous methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I like the idea of using cooperative game theory to analyze ad-hoc cooperation. The ideas in the paper seem, to the best of my understanding, novel. The experimental results also seem fairly strong, but my ability to understand their significance is limited (see next section)."
                },
                "weaknesses": {
                    "value": "I am not too familiar with cooperative game theory, and I found the technical exposition rather hard to follow, and things took a long time for me to parse---perhaps because, while reading, I didn't have a mental model for where things were going/what to expect. I think the exposition would be greatly strengthed by the addition of a running example that the authors could use to demonstrate the various claims and definitions that they make, and by formalizing various definitions mathematically. An incomplete list of specific clarity concerns is listed in the \"Questions\" section.\n\nI gave up on attempting to parse the rest of the technical part of the paper as I have already spent considerable time and am still confused about several things that are quite fundamental (again, see \"Questions\" below for some of these). I think there could be an interesting contribution here, but the quality of writing needs to be improved before publication. \n\nFrom what I can tell, the experimental results seem strong, but my ability to understand the significance of the results is essentially limited to seeing that CA-GPL's line is higher than GPL's.\n\n\nNitpicks/minor errors (not affecting score):\n* I think $\\mathcal A_{\\mathcal N}$ and $\\Theta_{\\mathcal N}$ should have an $\\exists$ quantifier in their definitions, not a $\\forall$---otherwise, they're both empty sets, since there is not a joint action that is simultaneously in $\\mathcal A_{\\mathcal N_t}$ for all ${\\mathcal N_t}$.\n* In the formulation in Sec 2.2, it is unclear how the team $\\mathcal N_t$ evolves with time. It seems to me that this is formalized in the next subsection, but if so there should be a forward pointer."
                },
                "questions": {
                    "value": "1. In Theorem 1 it should be made clear exactly what \"maximizing the social welfare under the grand coalition\" means. I am interpreting it as finding a *joint* policy, that is, a map $\\pi : \\mathcal S \\times \\mathbb P(\\mathcal N) \\ni (s_t, \\mathcal N_t) \\mapsto a_t \\in \\mathcal A_{\\mathcal N_t}$ that, for every pair $(s_t, \\mathcal N_t)$, selects $a_t$ to maximize the local social welfare $\\sum_{i \\in \\mathcal N_t} R_i(s_t, a_t)$. Is that correct? In any case this should be formally stated.\n2. It seems like the paper is adopting a single-agent perspective, where there is a single learner $i \\in \\mathcal N$ and all other agents' policies are held fixed (is this correct?). But then how can we optimize social welfare, required by Theorem 1, if we only control the single learner $i$? In particular, what if the other agents are acting in such a way that learner $i$ alone cannot achieve social welfare optimality?\n1. At the beginning of Sec 3.3, I don't understand the point of defining $\\mathcal{CS}_t$ only to later set $\\mathcal{CS}_t := \\mathcal N_t$. Does this have a purpose? It seems cleaner to just not define the extra symbol.\n1. In section \"Representation of Preference Q-Values\", there is a clause beginning \"which is presumed ... goal\". What does \"which\" refer to here? Is this an assumption required by Theorem 2? If so it should be formalized mathematically.\n1. What's the purpose of the types? They don't seem to be doing anything, except perhaps affecting transitions---but my interpretation of Theorem 1 implies that we only need to perform local optimizations at each state anyway. Perhaps it would be cleaner---and just as interesting---to write the paper without types?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697410126972,
            "cdate": 1697410126972,
            "tmdate": 1699636385030,
            "mdate": 1699636385030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Sr2fdpgsBF",
                "forum": "Y0yz1pmVfE",
                "replyto": "S6t7Qw20gm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eRVD (1/2)"
                    },
                    "comment": {
                        "value": "1. **In Theorem 1 it should be made clear exactly what \"maximizing the social welfare under the grand coalition\" means. I am interpreting it as finding a joint policy, that is, a map $\\pi: \\mathcal{S} \\times \\mathbb{P}(\\mathcal{N}) \\in (s\\_{t}, \\mathcal{N}\\_{t}) \\mapsto a\\_{t} \\in \\mathcal{A}\\_{\\mathcal{N}\\_{t}}$ that, for every pair $(s\\_{t}, \\mathcal{N}\\_{t})$, selects $a\\_{t}$ to maximize the local social welfare $\\sum\\_{j \\in \\mathcal{N}\\_{t}} R_{j}(s\\_{t}, a\\_{t})$. Is that correct? In any case this should be formally stated.**\n\n    *Reply:* Thanks for your suggestion. Your understanding about the result of Theorem 1 for fitting the problem we would like to solve (open ad hoc teamwork) is correct. However, the result from Theorem 1 is general for any preference value profile in CAG. Note that in the original version of CAG, it does not consider action space, while in our theory we extend the definition of preference value with action as an decision variable (i.e. a realization of preference value). Nevertheless, we agree with you that the statement should be linked to our problem more directly to make it better comprehensive. For this reason, we have added a corollary to comprehend the result of Theorem 1 in our case in the revised paper.\n\n\n2. **It seems like the paper is adopting a single-agent perspective, where there is a single learner $i \\in \\mathcal{N}$ and all other agents' policies are held fixed (is this correct?). But then how can we optimize social welfare, required by Theorem 1, if we only control the single learner $i$? In particular, what if the other agents are acting in such a way that learner $i$ alone cannot achieve social welfare optimality?**\n\n    *Reply:* Thanks for your really thoughtful questions. Yes, in the common setting of ad hoc teamwork, we can only control one agent to collaborate with other teammates on the fly. To make the theoretical analysis simpler, we here assume all other agents' policies are held fixed (i.e., to keep the environment stationary to the learner we control). However, this work can be extended to scenarios where other agents' policies would vary, for which the analysis would become complicated due to the non-stationary envrionment. \n    \n    Although we only control the learner $i$, it can affect other agents' decision, since other agents are able to respond the learner's action with regarding the learner's behavior as partial input of their policies. As a result, other agents' individual preference reward as an evaluation of their policies would accordingly change. \n    \n    The final question is really insightful. Since the basic assumption of ad hoc teamwork is that all agents are assumed to have a common goal, so the situation you consider is out of scope. Nevertheless, if the situation you state really happens (that is an open question), then for the moment we can only say it could be extended to control a group of agents, where at least one agent is able to influence other uncontrollable agents. We hope our response addresses your concerns. \n\n\n3. **At the beginning of Sec 3.3, I don't understand the point of defining $\\mathcal{CS}\\_{t}$ only to later set $\\mathcal{CS}\\_{t} = \\mathcal{N}\\_{t}$. Does this have a purpose? It seems cleaner to just not define the extra symbol.**\n\n    *Reply:* Thanks for your suggestion. We agree that your suggestion would make the definition more concise. However, the definition of $\\mathcal{CS}_{t}$ here has two purposes. The main purpose is that in the future work this theoretical framework can be directly used to extend to the scenarios where multiple agents divided into different coalitions (e.g., to solve multi-task problems) can be controlled. The second purpose is that we would like to keep the tight connection to the past works. This would make the readers understand the overall relationship between our work and past works, which avoids the potential of reinvention in the future. \n\n4. **In section \"Representation of Preference Q-Values\", there is a clause beginning \"which is presumed ... goal\". What does \"which\" refer to here? Is this an assumption required by Theorem 2? If so it should be formalized mathematically.**\n\n    *Reply:* Sorry for the confusion. This \"which\" refers to the condition in Theorem 2 such that $w_{jj'} \\geq 0$. We have removed it in the revised paper to keep conciseness and avoid the confusion, since even if $w_{jj'} \\geq 0$ does not hold, we can still manually add bias during learning without changing the final result as we discussed in Section 4.4 in the revised paper about practical implementation. Thank you for your mention."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233322799,
                "cdate": 1700233322799,
                "tmdate": 1700233322799,
                "mdate": 1700233322799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KmQxJYpw2n",
                "forum": "Y0yz1pmVfE",
                "replyto": "Xrj0PEk6YY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "content": {
                    "comment": {
                        "value": "My opinion of the paper has not changed, and I will keep my score.\n\nThere seems to be a strong dependency on the GPL paper in this paper. As a general piece of advice, papers should make effort in writing where possible to reduce such dependency and be self-contained; I would advise the authors to do this, especially for readers (like myself) unfamiliar with GPL.\n\n**2)** (Single-agent perspective) This remark from the authors raises more questions to me than it answers. The assumption that all other agents are playing a fixed policy completely defeats the purpose of investigating a multi-agent system in the first place---you can treat agents with fixed policies as part of the environment and just solve the resulting (single-agent) POMDP.  If this paper is really about a single-agent perspective, the authors should do much more to justify why we should believe that this single-agent perspective is relevant to a multi-agent system. (For example, if all agents run your algorithm, what happens? There is a lot of work investigating multi-agent learning dynamics in games, and I believe that the authors should mention that line of work, and fit into it where possible.)\n\n> The final question is really insightful. Since the basic assumption of ad hoc teamwork is that all agents are assumed to have a common goal, so the situation you consider is out of scope. \n\nI'm not sure I understand. Are you saying that you're assuming that other agents are playing parts of an (welfare-)optimal joint policy? Or that, if all agents run your algorithm, they are guaranteed to converge to a optimal joint policy? (this would be very surprising to me if it were true, as most algorithms usually do not have this property unless explicitly designed for it!) Or that, regardless of the policies of other agents, there is always a policy for the learner such that the resulting joint policy is optimal? Or something else entirely? In any case, the claim here requires explicit statement and justification. But this comment raises yet another question: what does it mean for agents to have a \"common goal\"? They don't share a reward function, right? Doesn't that mean that their goals may be misaligned?\n\nThis response makes me feel like I am missing something very crucial here. Whatever it may be, though, it certainly requires clarification.\n\n**3)** (on $\\mathcal{CS}_t$) I feel that the importance of clear writing in this paper outweighs the concerns raised by the authors; as such, I'd advise streamlining the notation by removing $\\mathcal{CS}_t$.\n\n**5)** (on types). I understand the general use of having types/partial observability in games. My comment was more to do with the framing/cleanliness of notation of the paper: if the results of the paper is also interesting in a fully-observed setting and the generalization to partial observability is easy enough, maybe it is better to frame the paper mainly for fully-observed setting (because the notation is cleaner) and reserve the extension to partial observability to an appendix. I don't know whether this is possible, but I am raising the suggestion just in case, because anything that would reduce the notation/clarity burden in this paper would be a good thing. If the fully-observed setting is somehow trivial, the authors should point this out."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258209559,
                "cdate": 1700258209559,
                "tmdate": 1700258209559,
                "mdate": 1700258209559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CLZheuNZyH",
                "forum": "Y0yz1pmVfE",
                "replyto": "S6t7Qw20gm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer eRVD (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your so fast reply. \n## Before answering the further questions, we urge the reviewer to calm down when meeting an unfamiliar area during review and give full respect to this area. We are always patient to answer your questions.\n\n**1. (Single-agent perspective) This remark from the authors raises more questions to me than it answers. The assumption that all other agents are playing a fixed policy completely defeats the purpose of investigating a multi-agent system in the first place---you can treat agents with fixed policies as part of the environment and just solve the resulting (single-agent) POMDP. If this paper is really about a single-agent perspective, the authors should do much more to justify why we should believe that this single-agent perspective is relevant to a multi-agent system. (For example, if all agents run your algorithm, what happens? There is a lot of work investigating multi-agent learning dynamics in games, and I believe that the authors should mention that line of work, and fit into it where possible.)**\n\nReply: The main issue is that **the reviewer is not familiar with ad hoc teamwork**. What this work did follows a topic called ad hoc teamwork which is **totally different** from multi-agent learning. In ad hoc teamwork, the perspective is to control an agent, while in multi-agent learning, the perspective is to control multiple agents. **Also, we have mentioned the difference between ad hoc teamwork and multi-agent learning in Related Works. We hope the reviewer can check the paper carefully before raising questions.** As the reviewer said, the ad hoc teamwork can be regarded as POMDP, however, the key point is that POMDP is a very general model. If the details of such a model is unknown, how should we well solve the further complicated problem you raised that \"if all agents run your algorithm\"? The setting in the reviewer's question would make environment non-stationary (or time-varying from the perspective of control theory), which is impossible to be solved if we know nothing about the environment. This is the reason why we study the agent modelling first with the assumption of fixed teammates' policies, which we believe is the key step towards the more complicated problems. Does the reviewer agree with this?\n\n**2. I'm not sure I understand. Are you saying that you're assuming that other agents are playing parts of an (welfare-)optimal joint policy? Or that, if all agents run your algorithm, they are guaranteed to converge to a optimal joint policy? (this would be very surprising to me if it were true, as most algorithms usually do not have this property unless explicitly designed for it!) Or that, regardless of the policies of other agents, there is always a policy for the learner such that the resulting joint policy is optimal? Or something else entirely? In any case, the claim here requires explicit statement and justification. But this comment raises yet another question: what does it mean for agents to have a \"common goal\"? They don't share a reward function, right? Doesn't that mean that their goals may be misaligned?**\n\nReply: We **never** say that we would tend to find a joint optimal policy or we finally find the optimal joint policy. The learner's policy obtained (combined with other teammates' policies) is almost a sub-optimal joint policy, which is totally different from multi-agent learning (which aims to optimize a joint policy, so that it can find a joint optimal policy). The common goal can be interpreted into two scenarios: all agents share a reward function to describe the common goal, or each agent has a reward function that is the composition of a shared reward to describe the common goal and an individual function that is not conflicting with the common goal (see Section 2.1 in [1]). The key problem in ad hoc teamwork is that even if all agents have the shared reward, it is difficult to make them collaborate if they were never trained in the same environment before. Does it make more sense now?\n\n**3) (on $\\mathcal{CS}\\_{t}$) I feel that the importance of clear writing in this paper outweighs the concerns raised by the authors; as such, I'd advise streamlining the notation by removing $\\mathcal{CS}\\_{t}$.**\n\nReply: We respect the reviewer's viewpoint, however, this is not the critical point to decide the acceptance of the paper.\n\n## Reference\n[1] Mirsky, Reuth, et al. \"A survey of ad hoc teamwork research.\" European Conference on Multi-Agent Systems. Cham: Springer International Publishing, 2022."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262454053,
                "cdate": 1700262454053,
                "tmdate": 1700262485225,
                "mdate": 1700262485225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mJ4AmcqHlQ",
                "forum": "Y0yz1pmVfE",
                "replyto": "S6t7Qw20gm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the fast response as well. I am willing to engage in discussion about this paper, as should be evidenced by my own fast replies here. As I did say in my review (and you have definitely noticed), this specific area is not very familiar to me. I also cannot become a topic-area expert in a matter of a few days. That said, I do work more broadly in game theory, so I would think that I am at least somewhat in the target audience, enough that I should be able to understand the broad takeaways and motivations of the paper. \n\nI think I am understanding a bit better, but still missing something very crucial. I hope this time though my comments are more pertinent.\n\nTheorem 1 states that the *optimal joint policy* exhibits strict core stability. So, since you're not finding an optimal joint policy in any sense, Theorem 1 does not apply to your algorithm. Now, assuming I've understood this part correctly, I ask/recommend:\n\n1. I think the above should be explicitly stated in the paper immediately after the statement of Theorem 1, and also in the same place it should be explained why Theorem 1 is even present/relevant to the paper, given that it does not apply to your main algorithm. Otherwise a reader could easily assume (as I did!) that the paper will want to somehow achieve a joint optimal policy, or something like it, in order to apply Theorem 1, and get confused/fixated.\n1. The paper very quickly moves from jointly optimizing the social welfare (Theorem 1/Corollary 1) to a single-agent optimization problem (Eq. (3)). The relationship between these should be explained, since solving Eq (3) does not in itself guarantee welfare optimality. This may be related to the previous point.\n1. Which theoretical results of the paper *do* apply to your algorithm? Can you state a concrete result along the lines of \"if we run CA-GPL for one agent, with other agents' policies fixed and arbitrary, we will achieve strict core stability\"? If so, such a result should be explicitly stated. If not, you should explain what guarantees *are* achieved by your algorithm, because right now the theoretical results are about strict core stability in general and it is not clear how they relate to your algorithm.\n1. For what joint policies do the results of Section 4.2 hold? It looks to me like those results should hold for *all* joint policies, but if so, that should be explicitly stated. And, in that case, I ask what Section 4.2 has to do with learning a good policy at all, because if all joint policies satisfy the desired property then the satisfaction of the property isn't a constraint on the learning at all.\n1. In Section 4.2, it seems that you are making assumptions about the structure of weights $w$ (nonnegative) and rewards ($R_j(s_t, a_{t, j}) = (1/|{-i}|) R_i(s_t, a_{t, i})$). These assumptions should be justified; they seem quite strong to me.\n\nIf I have misunderstood something above, I'd be happy for the authors to correct me. I am pretty sure I am not the only reader who will have these confusions, so I do hope that this conversation leads to a stronger, more broadly understandable writeup.\n\nThe reply about types makes sense to me. Thank you for clarifying."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277044074,
                "cdate": 1700277044074,
                "tmdate": 1700277579673,
                "mdate": 1700277579673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "18QZjzVvOO",
                "forum": "Y0yz1pmVfE",
                "replyto": "S6t7Qw20gm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "content": {
                    "comment": {
                        "value": "(Note: You've numbered your responses off-by-one from my questions. For continuity, I'll switch to your numbering.)\n\n**3-4)** This answer is somewhat disturbing to me. Are you saying that, without further conditions (e.g., on how other agents are reacting to the learning agent), *none* of the theoretical results stated in the paper apply to the main algorithm? (Because the algorithm cannot guarantee welfare optimality, which is required for Theorem 1 and therefore according to your response is required in order to say anything about your algorithm).\n\nA previous comment in this thread stated very clearly that this paper is *not* claiming to find a welfare-optimal joint policy, which I agree with. But yet the theory seems to depend on an optimal joint policy being found. This seems to be a fundamental disconnect here between the algorithm and the theory that severely limits the relevance of the theoretical results.\n\n**5)** What, formally, is \"the assumption of ad-hoc teamwork\"? Is it a condition on the *rewards*/affiinity graph structure in addition to the structure in Section 3.1 and 3.2? A condition on the *training procedure* (Not sure what precisely that would entail, should specify)?  A condition on the *learned policies* (e.g. that the joint policy is welfare-optimal, as in Theorem 1)? Theorem 2's statement and proof doesn't seem to include any such additional assumption, so if there is an additional assumption it should be explicitly stated.\n\n**6)** Okay, seems like I've misunderstood something here. Are you saying that *your algorithm* is allowed to *change the weights* $w$? Section 3.1 makes it look like $w$ is part of the *input*/*problem definition*, so you can't change them as you see fit to achieve a desirable result. If the algorithm is allowed to set the weights $w$, this should be explicitly stated; it is yet another rather strong assumption that the paper seems to be implicitly making.\n\nActually, regarding the reward assumption, another question: does it have to hold for *every* $s_t, a_{t, i}, a_{t, j}$? Or only some? If only some, which ones? If every, I think we have a problem:\n1. By varying $a_{t, j}$, one immediately concludes that every action for every player must have the same reward, i.e., that the reward must be independent of the action.\n1. If there are $n>2$ agents, then applying the condition twice yields $R_j(s_t) = R_i(s_t)/(n-1) = R_j(s_t)/(n-1)^2$ (where I omit the actions because, by the previous point, the reward must be action-independent) but the only reward function that satisfies this is the reward function that is identically zero, which is obviously not very interesting.\nAm I  missing something here?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334616801,
                "cdate": 1700334616801,
                "tmdate": 1700334811286,
                "mdate": 1700334811286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aTZHJGDNJd",
                "forum": "Y0yz1pmVfE",
                "replyto": "S6t7Qw20gm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "content": {
                    "comment": {
                        "value": "**Regarding Theorem 1 and joint (welfare) optimality:** I think you are saying that the \"only gap\" in the application of Theorem 1 is that the joint optimal policy may not be reached. But, in an earlier message, you stated very clearly that your algorithm does not in any way claim to allow the joint optimal policy to be reached. So, this \"gap\" is not a small gap at all; rather, it is a fundamental limitation of the algorithm that prevents the application of the main theorem! \n\nTo me, this gap is a severe limitation that undermines not only the relevance of Theorem 1, but also the relevance of the whole approach of the paper since, if I understand the authors correctly, all the theoretical results about the main algorithm depend on joint optimality.\n\nAlso, if there is some way to formalize what it means to \"influence\" the other agents, what conditions are being implied here about how the other agents are learning/reacting to the learner's play, and what that means about the eventual joint policy of the players, that would be very important to include. But it is not really directly relevant to my main point above; that point only relies on the authors' own statements that:\n1. the main algorithm is not claiming to reach a joint optimal policy.\n2. positive results about the main algorithm depend on reaching a joint optimal policy (\"If we satify the results from Theorem 1 (optimizing social welfare) and Theorem 2 (confining the weights of the learned affinity graph), then it is possible for us to reach the strict core stability.\")\n\n Am I missing something? \n\n**Regarding being allowed to change weights:** Section 3 is written in such a way that it is implied that the weights are part of the input, i.e., they are not things that are learned by the algorithm. Section 3 should be rewritten to fix this.\n\n**Regarding the reward assumption:** I see. So, under the assumptions of that section, singleton coalitions are assumed to have reward zero when $n>2$, but non-singleton coalitions can still easily have nonzero reward. Both these points should be stated explicitly in the text."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362183514,
                "cdate": 1700362183514,
                "tmdate": 1700362314484,
                "mdate": 1700362314484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XSxTafCa4H",
                "forum": "Y0yz1pmVfE",
                "replyto": "S6t7Qw20gm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_eRVD"
                ],
                "content": {
                    "comment": {
                        "value": "I think we will agree to disagree here. Here's another way of phrasing where I am stuck: this work is introducing a new framework based on cooperative game theory to achieve ad-hoc teamwork. But, in some sense, one of the main premises of ad-hoc teamwork *is* that you can't expect to achieve the joint optimal policy because you don't control what the other agents are doing (otherwise, it'd be a MARL problem). So the field attempts to understand what *is* achievable in this setting when you have so little control. I understand that motivation. But, with that in mind, developing a framework whose entire conceptual/theoretical logic is \"if we achieve the thing that we just said is not a reasonable expectation (joint optimality), then we achieve a good outcome (strict core stability)\" now seems almost contradictory to the above premise. \n\nI would strongly suggest that the authors revisit the fundamental conceptual premises of the paper, without assuming joint optimality. There could still definitely be interesting directions to investigate at the intersection of cooperative game theory and ad-hoc teamwork. For example, there could be other interesting statements that can be made using cooperative game theory that do not assume joint optimality. Or perhaps, one could experimentally check whether your joint policies exhibit nice properties from cooperative game theory (such as, but perhaps not limited to, core stability), even if they are suboptimal? Of course, I cannot expect this to be done in what remains of the discussion period, so these are suggestions for a future revision.\n\n> Reply: Your understanding is generally correct. However, it could be helpful to rearrange the logic.\n\nThe paragraph that follows this sentence was not obvious to me. The information/intuition in that paragraph should be explicitly discussed in a revision. \n\nI think I will not have much more significant or new things to say at this point, and I have spent already a very large amount of time on this paper, so this will be my last message here and I will maintain my score. Thank you for being engaging in the review process. I hope that the discussion has been helpful."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414505472,
                "cdate": 1700414505472,
                "tmdate": 1700414654395,
                "mdate": 1700414654395,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JMX1aAav5d",
            "forum": "Y0yz1pmVfE",
            "replyto": "Y0yz1pmVfE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_23zx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_23zx"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of ad-hoc teamwork from a novel, cooperative game-theory perspective. Specifically, the paper considers conditions on the optimization of the single learner (in ad-hoc teamwork, a single learner is trained to collaborate with different teammates which she has never met before) so that the grand coalition, i.e., the coalition in which all teammates collaborate for the common goal, is stable and preferred by all agents. Importantly, this allows for an approach with variable number of teammates in every round that is not addressed by previous methods. The paper further uses a, possibly time-varying, star graph to model the interactions of the players with the learner in the centre and the non-interacting teammates in the leaves.\n\nThe main contribution of the paper is that it proves that the solution concept describing the stability of ad hoc teams, roughly the core of the cooperative game described above, is reached when the agents maximize the social welfare, i.e., the sum of agents\u2019 preference Q-values as the global Q-value. Based on this, the paper extends the Graph-based Policy Learning (GPL) algorithm to the coalition-affinity GPL (CA-GPL). CA-GPL is then experimentally evaluated in the Level-Based Foraging (LBF) and Wolfpack environments. The experiments suggest consistent improvements over GPL and also dilligently highlight the importance of each assumption used in the theoretical results. For instance, variations of the main model that relax sufficient (not necessary) conditions show similar performance to the optimal model sheding light into possibilities for further research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well placed in the relevant literature of ad-hoc teamwork. It considers a SoTA algorithm, identifies well-justified shortcomings, e.g., lack of theoretical foundation/suboptimal results in variable environments, and tries to propose solutions to these.\n- The paper is using novel techniques, specifically elements from cooperative game-theory, to provide a theoretical background for a simple algorithm that provides improvements on the above problems. \n- The environments used in the experiments and the experimental evaluation are comprehensive, include sensitivity (ablation) studies of all assumptions required for the theoretical results and are very well presented. The paper also provides a link to a website that includes demos of the experiments which provides further insight. I am not sure, though, if links are desirable or if these demos need to be provided in the supplementary material to make the paper self-contained.\n- Limitations, proofs and simulation details for reproducibility are thoroughly documented."
                },
                "weaknesses": {
                    "value": "- The major weakness of the paper is, in my subjective opinion, the overly complicated presentation in some critical places which does not allow the reader to appreciate the theoretical contributions of the paper and to verify their correctness. This affects both the theory and the experiments and I elaborate on this below in the experiments.\n\nIn general, I think that the paper achieves a solid contribution in the literature and has a clear potential, but given my concerns about proper understanding, I think that it requires a thorough revision prior to being ready for publication."
                },
                "questions": {
                    "value": "I elaborate below on my comment in the weaknesses:\n- Can you please elaborate on what do types represent since all teammates always want to collaborate? This is not explained in the text.\n- Can you please highlight the main theorem that addresses the motivating question at the bottom of page and top of page 2? I admit that I got lost in some parts of the theoretical presentation. \n- An algorithm environment for CA-GPL would have aided the reader to appeciate the novelties over GPL.\n- If I am not mistaken, the notation seems too complicated and redundant. For instance, if I understand correctly, it holds that v_j(C) = w_ji  = R_j and later on R_j = \\alpha_j + (other terms). Similarly for the social welfare that is defined as sum (of sums) of such terms. Thus, I had difficulty to follow the proofs. Is my concern valid?\n- More examples for complicated notation: \n    - what is $|-i|$? Is it the cardinality of the set of all agents other than $i$? If yes, then first, this is never explained, and second why not simply write $N_t-1$?\n    - what is $b_j$ in the bottom of page 2? Is this used later on?\n    - what is $a_{t,j}$ below equation (1)? Should this be $a_{t,-i}$?\n    - social welfare is used in Theorem 1, but it has not been defined before.\n- It seems to me that the definition of the joint policy $\\pi_{t,-i} :\\mathcal{S}\\times \\Theta_N \\to \\Delta(\\mathcal{A}_N)$ allows for _correlated_ policies. However, this contradicts the assumption that permeats the text, that teammates don't interact at all with each other. Is this concern valid?\n- Theorem 1 is hard to parse for me. If the grand coalition is strict core stable (with respect to what valuations?), then what does it mean to show that it is strict core stable? Apologies if my confusion is not justified.\n- Regarding the experiments: In Table 1, we see that the stability metric worsens for CA-GPL in both scenarios as training progresses (especially for the 4 teammates). Why is this? Also, in the figures, we see that the curves keep increasing (they have not converged). Is this a valid concern? And would it make sense to provide larger frames?\n\nThere are many typos or difficult to understand sentences. I name a few below:\n- page 2: \"we translate the achievement .... decision making\". incomprehensible\n- p2: the stability, of ad hoc teams (misplaced comma)\n- p2: LBF -> please name it properly the first time that this is used.\n- p3: \"if a coalition structure ... weakly blocking coalition\". incomprehensible\n- p3: \"There exist an affinity graph\" -> exists\n- p3: continually (you may want to check if continuously is better here)\n- p5: we aims at -> aim\n- p8: \"the relationship among ad hoc teammates is weak to\" -> shouldn't it be the opposite here, i.e., strong relationship?\n- p9: \"theoretical model Our theoretical model\" -> typo?\n- p16: \"...results in that prices ...\" (in Theorem 1) -> incomprehensible.\n- there are more, please check."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Reviewer_23zx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698233463617,
            "cdate": 1698233463617,
            "tmdate": 1699636384953,
            "mdate": 1699636384953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HWzLH1763k",
                "forum": "Y0yz1pmVfE",
                "replyto": "JMX1aAav5d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 23zx (1/2)"
                    },
                    "comment": {
                        "value": "1. **Can you please elaborate on what do types represent since all teammates always want to collaborate? This is not explained in the text.**\n\n    *Reply:* The types here is for emphasizing the heterogeneity of agents. Even if all teammates always want to collaborate, a group of different types of agents may collaborate in different manners. Thereby, in this paper we apply affinity graph with learnable weights (capture variant collaborative manner) to depict this property.\n    \n2. **Can you please highlight the main theorem that addresses the motivating question at the bottom of page and top of page 2? I admit that I got lost in some parts of the theoretical presentation.**\n\n    *Reply:* Sorry for making you lost owing to our presentation. For convenience, we will reply following the layout of the revised paper. The answer to the motivating question is constituted of the holistic theory. It starts from the introduction of CAG with the affinity graph and the solution concept for solving the weights of the affinity graph called strict core stability (see Definition 1). Then, we define the affinity graph as a star graph to better describe the scenario of open ad hoc teamwork (see Definition 2). After that, we derive a method to find the solution of weights (see Theorem 1 and the derived Eq. (4)) and an inductive bias for easily finding the weights (see Theorem 2 and Proposition 2). Finally, we show the feasibility of solving Eq. (4) under the open team scenario (see Lemma 1 and Theorem 3). In summary, our theory answers the question that why introducing and learning a graph is necessary and feasible to solve open ad hoc teamwork from the theoretical perspective rather than the pure intuition in the GPL paper, so the method derived from our theory is reliable.\n\n3. **An algorithm environment for CA-GPL would have aided the reader to appeciate the novelties over GPL.**\n\n    *Reply:* What do you mean by an algorithm environment? Could you please give some more information about it? We have added an achitecture of CA-GPL to illustrate the novelty over GPL in the revised paper as Figure 1 shows.\n    \n4. **If I am not mistaken, the notation seems too complicated and redundant. For instance, if I understand correctly, it holds that v_j\\(C\\) = w_ji = R_j and later on R_j = \\alpha_j + (other terms). Similarly for the social welfare that is defined as sum (of sums) of such terms. Thus, I had difficulty to follow the proofs. Is my concern valid?**\n\n    *Reply:* Thanks for your suggestion. Your understanding for our own theory is correct only for the teammates $j$. For learner $i$, the instantiation of $v_{i}(C)$ or $R_{i}$ to ad hoc teamwork is different, while the abstract definition of $v_{i}(C)$ is the same as $v_{j}(C)$ in the most general hedonic game. More specifically, $v_{j}(C)$ is the most general definition over any arbitrary coalition $C$ in hedonic game. In our case, we replace $v_{j}(C)$ by $R_{j}$ to make the symbols consistent with convention in reinforcement learning. Using $w_{ji}$ to specify $v_{j}(C)$ is only valid in CAG which is a subclass of hedonic game. $w_{ji} = R_j$ is only valid when we consider the star graph. If we initially define $w_{ji} = R_j$ with ignoring $v_{j}(C)$, this would make our theory too narrow and less valuable to be extended to more complicated scenarios as we mentioned in Conclusion. Similarly, \\alpha_j + (other terms) is an instantiation of $w_{ji}$. In summary, our theory is formulated from abstract (the known general theory) to detailed scenarios (the situation of ad hoc teamwork) step by step, with both concerns of the extension of our theory and inheriting past works. We wish you can understand our thought and are happy to have more discussion on it.\n\n5. **what is $|-i|$? Is it the cardinality of the set of all agents other than $i$? If yes, then first, this is never explained, and second why not simply write $N_{t} - 1$?**\n\n    *Reply:* Yes, thanks for the suggestion. We do not write it as $N_{t} - 1$ is for the conciseness in expression. \n\n6. **what is $b_{j}$ in the bottom of page 2? Is this used later on?**\n\n    *Reply:* $b_{j}$ was originally used to indicate that the singleton coalition value is a constant value greater than or equal to zero. However, we agree with your suggestion and modify it as $v_{j}(\\mathcal{C}) = v_{j}(\\{j\\}) \\geq 0$ in the revised paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232743716,
                "cdate": 1700232743716,
                "tmdate": 1700232743716,
                "mdate": 1700232743716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fnIiwJwri0",
                "forum": "Y0yz1pmVfE",
                "replyto": "SpV4QcLe3o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_23zx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_23zx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. With an algorithm environment, I meant a typical pseudocode environment. The figure is also welcomed, but indeed it requires some description. Overall, it is impossible for me to follow the long discussions about this paper in detail, but, based also on the other reviews, I will maintain my initial recommendation that the presentation of the paper is overly complicated to clear some doubts on its contribution, and to make it assessible by a wider audience and fit for publication at its current state. The positive elements that I see in the paper have been weighed-in in my initial score, thus, I will maintain it."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508836103,
                "cdate": 1700508836103,
                "tmdate": 1700508836103,
                "mdate": 1700508836103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lvs5bj5dr8",
            "forum": "Y0yz1pmVfE",
            "replyto": "Y0yz1pmVfE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates learning techniques for **open** ad-hoc teamwork, which is a setting in which an uncontrollably heterogeneous set of agents are put in the same environment, and we want to train a single agent (called learner) to operate in the environment in order to maximize his reward while potentially collaborating with other agents. The other agents are assumed to be of a fixed type (sampled from a distribution), corresponding to a specific policy that is adopted in the environment.\nThe main challenges that are faced are that the agents cannot leverage any form of pre-coordination and agents have to adapt in real time to a varying number of teammates in the environment.\n\nThe paper builds incrementally on a previous approach known as GPL. The GPL technique is a value-based reinforcement learning technique that uses a complex architecture to estimate:\n* types of the agents\n* joint-action Q-value (decomposed as sum of single action Q-values and pairwise actions Q-values) given types and state\n* behavioral model, i.e. the probability of other agents taking specific actions given types and state\nThose components are trained together in a RL fashion, by letting the learner pick actions according to a Q-function over its actions, derived from the joint Q-value and the behavioral model.\n\nThe contribution of the paper is a different approach to the decomposition of Q-values based on coalitional affinity games. In particular, the paper:\n* models the interactions between the agent as a star-shaped pattern around the learner, with symmetric values\n* theoretically justifies the optimization of the agent's reward done by GPL as it corresponds to the strict core of the affinity game centered around the learner\n* derives a Bellman optimality equation from the star structure\n* uses a GPL-like architecture to estimate Q-values of the learner, with the difference that the joint Q-function does not consider pairwise contributions between agents different from the learners"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strenghts of the paper can be summarized as follows:\n* introduction of a modelling framework that interprets open ad-hoc teamwork as a coalitional affinity game\n* slightly better empirical results in terms of value, more stable in terms of stability"
                },
                "weaknesses": {
                    "value": "**Severe** lack of clarity throughout the paper:\n* when introducing an affinity graph, the values from the singleton coalition are not ported to the new representation, and then they are used when discussing individual rationality.\n* in many instances of the paper, assumptions are introduced without justification and they are not properly highlighted (just a \"There exitst...\"). As an example the existance of an affinity graph is not guaranteed in general as far as I know.\n* Many different probability distributions are introduced ($P_E$, $P_O$, $\\mathcal T$ ,,,) in Section 3.1, making really obscure the dependencies across variables, Figure 5 from the appendix and a clearer text  may help.\n* lack of an example to clarify the settings and its peculiarities.\n* the term \"Preference Q-Value\" is unneeded, as it is just a Q value over the action space of the agent\n* the text between Theorem 2 and Propositoin 2 is unreadable and I could not understand it even reading it multiple times. Please rephrase it and provide proper space for the mathematical equations used.\n* the GPL framework is never defined, and it could really help to highlight the contributions of the paper.\n\nThe usefulness of the theoretical contributions is debatable. In particular, the whole coalitional affinity games(CAG) framework seems like a very elaborate way of justifying the star-shaped Q-value computation novelty introduced, However, such a conclusion derives rather directly from the star-shaped interaction graph between agents, that is assumed.\n*   up to theorem 2, the theoretical contributions are just saying that we need to optimize the learner's utility, which coincides with the social welfare thanks to the assumption of a star-shaped graph. Given the context at hand, this result is banal and the introduction of CAG actually makes the formalism uselessly complex\n\nThe final architecture developed for the CAG-GPL is not described in detail, nor properly compared with GPL. A picture is needed, as in GPL paper.\n\nEmpirical results are difficult to interpret.\n* in Figure 1 it seems that CAG-GPL has really similar performance to GPL\n* Figure 3 is unreadable: the legend's colors do not match with the plot, and in the Q-value plot there is written \"learner\" even on the rows not related to the learner"
                },
                "questions": {
                    "value": "* Is my interpretation of the architectural differences between GPL and CAG-GPL in the last point of the summary correct? Ie, this paper \"uses a GPL-like architecture to estimate Q-values of the learner, with the difference that the joint Q-function does not consider pairwise contributions between agents different from the learners\", so only Q_ij, Q_i and Q_j but not Q_jj'\n\n* I could not highlight an important gap between CAG-GPL and GPL apart from training stability. Is there anything I missed in my weakness summary?\n\n* similarly, what is the biggest novelty from a modelling perspective introduced by the CAGs? If possible, can you compare it in terms of differences with GPL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755509650,
            "cdate": 1698755509650,
            "tmdate": 1699636384864,
            "mdate": 1699636384864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H5xrXkYLll",
                "forum": "Y0yz1pmVfE",
                "replyto": "Lvs5bj5dr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UX5a (1/3)"
                    },
                    "comment": {
                        "value": "1. **when introducing an affinity graph, the values from the singleton coalition are not ported to the new representation, and then they are used when discussing individual rationality.**\n \n    *Reply:* Thanks for the insightful comment. We were also struggled with the definition of the reward for a singleton coalition. After deliberation, we believe that it is unnecessary to define it with a new representation, since we have defined the reward over any arbitrary coalition in Section 3.1, and the reward for a singleton coalition is just a concrete case.\n    \n2. **in many instances of the paper, assumptions are introduced without justification and they are not properly highlighted (just a \"There exitst...\"). As an example the existance of an affinity graph is not guaranteed in general as far as I know.**\n\n    *Reply:* Thanks for the interesting comment. \"There exists ...\" is a type of words to given a boudary, based on which we can analyze the results following logics. As for the existence of affinity graph, we now add discussion on the boundary in the revised paper (See Section 3.2 **Affinity Graph for Ad Hoc Teamwork**). We sincerely hope you could double check it.\n    \n3. **Many different probability distributions are introduced (...) in Section 3.1, making really obscure the dependencies across variables, Figure 5 from the appendix and a clearer text may help.**\n\n    *Reply:* Thanks for your comment. In our view, different probabilistic distributions defined here does not obscure the dependencies across probabilities. Instead, it clarifies the dependencies, since compared with expressing the transition function as a whole probability distribution in SBG, it points out the position of intervention (control) during the whole ad hoc teamwork process, following the view of causal inference. Nonetheless, we agree that a figure could help understanding, we have redrawn Figure 5 in the last version and it is now Figure 4 in the revised paper.\n    \n4. **lack of an example to clarify the settings and its peculiarities.**\n\n    *Reply:* Thanks for the insightful comment. This paper is not like the purpose of popular papers that capture a weakness that a prior work cannot solve and propose a method that can improve the weakness. The research question (motivation) of this paper is that the powerful GPL framework does not have a theoretical understanding of why the incorporation of graph can help. For this reason, we propose to incorporate CAG and surely its theory to mitigate this issue. More importantly, the incorporation of CAG can lead to the possibility the further investigation into more diverse situations (e.g., friend and enermies both exist). Although GPL seemly can address almost every situations, the further improvement of this architecture is a big problem. The main reason is that it does not consider any boudary from theoretical perspective. **However, the question is that we do not know whether the design is the best to any specific situation.** For example, in this paper, we propose to constrain the range of individual and pairwise Q-values to be greater than or equal to zero, according to our theory. Similarly, we can follow the route of the various and well studied research of cooperative game theory to analyze other scenarios (e.g. friend and foe).\n \n5. **the term 'Preference Q-Value' is unneeded, as it is just a Q value over the action space of the agent**\n\n    *Reply:* Thanks for this interesting point. We use the term \"Preference Q-Value\" is to credit and respect the work of CAG and cooperative game theory as well as to distinguish our work from others, since the Q-value is derived by the theory over preference value which is the terminology in cooperative game theory. **If following your logic, it is also unnecessary to define Q-value since it is just a function.** We sincerely hope you could reconsider your statement."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232499375,
                "cdate": 1700232499375,
                "tmdate": 1700232578727,
                "mdate": 1700232578727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MNL9630ZwR",
                "forum": "Y0yz1pmVfE",
                "replyto": "BdxAYThOS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed answer and the modification they made, which go towards what I think is a good direction for the paper.\n\nI will answer in a general form, instead than focusing on a point by point base. I have reviewed the changes to the main body of the paper, but due to time constraint I could not review the ones in the appendices.\n\n* adding the ca-gpl algorithm figure is definitely useful, but without any text accompanying it, it is unclear how to read it (unless people have already read GPL)\n* I keep my opinion that CAGs, as they are currently described in the paper, are just a distraction from the main contributions of the paper, which instead is the use of the concepts of the strict core stability, the assumption of full collaboration among agents, and the star-shaped graph assumption.\n\nMy score on the paper remains unchanged; I think that the paper should be structured in a way that clearly highlights how the paper first theoretically builds on top of ad-hoc teamwork (in the RL formalization) by adding the ideas of strict core stability (how about explaining the assumptions of of strict core stability in the RL formalism, adding the comparison with coalitional affinity games in the appendix?). My suggestion is then to go on introduction GPL in the main body, and modifying it as already explained in the paper. Comparison between the figures can be really helpful too.\nIn my opinion this would offer a much clearer understanding of the contributions of the paper (and also answer many of the difficulties raised from Reviewer eRVD, which in my opinion is right in his doubts about the paper. Those doubts are to be solved with proper introduction to the topic and to the modeling assumptions used)\n\nOverall, the results and the approach adopted for ad-hoc teamwork are interesting and worth of a publication, but the paper has to be rebuilt in my opinion."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479455078,
                "cdate": 1700479455078,
                "tmdate": 1700479455078,
                "mdate": 1700479455078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2YSJGODQ9o",
                "forum": "Y0yz1pmVfE",
                "replyto": "Lvs5bj5dr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer UX5a"
                    },
                    "comment": {
                        "value": "**We thank the reviewers for response.**\n\nThe main purpose of our paper is to propose a general theoretical model that can help improve ad hoc teamwork, **rather than just fix pitfall existing in GPL**. In other words, GPL is just a realization of the algorithm from our theoretical model. For this reason, we do not agree with the reviewer's suggestion.\n\nOn the other hand, the purpose of mentioning close relationship to GPL in details in our paper is to show **our respect to the GPL work**. If this instead distracts the main purpose of our paper, we would consider removing it from the main part of paper and just talking about it in Related Works. Thanks for your feedback to consolidate our determination to drop the unworthy part in our current contents. \n\nFinally, to our best knowledge, if our response has addressed your questions or concerns, it should be deserved to raise your score. This is main purpose of your questions from the policy of review: **Think of the things where a response from the author can change your opinion, clarify a confusion or address a limitation. This is important for a productive rebuttal and discussion phase with the authors.** \n\n**We strongly suggest the reviewer to follow the policy in the next review experience, otherwise, it is unfair to the authors' efforts. Please respect everyone's effort and take the resposibility of avoiding the phenomenon of withdrawing after receiving not positive scores which is not helpful to the community. To keep the healthy development of the community is the obligation of every reviewer.**\n\nWe sincerely thank you for your effort on the reviewing our work again."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481350440,
                "cdate": 1700481350440,
                "tmdate": 1700482034636,
                "mdate": 1700482034636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7xOpWEdTR",
                "forum": "Y0yz1pmVfE",
                "replyto": "2YSJGODQ9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_UX5a"
                ],
                "content": {
                    "comment": {
                        "value": "I think I gave you enough constructive comments on what I see is missing in the paper, and how I would improve the situation. \nYour answer addressed my questions, and I have a clearer picture of the contributions of the paper. However the paper as it is currently is not avoiding the same doubts that I had, and it still had the weaknesses that I highlighted previously.\n\nTherefore my score does not change. I also suggest the authors to do their work without arrogantly reminding me what my work as a reviewer is, which is not constructive at all and just exacerbates the situation. Thanks."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485890866,
                "cdate": 1700485890866,
                "tmdate": 1700485890866,
                "mdate": 1700485890866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kxc7d5fQXn",
                "forum": "Y0yz1pmVfE",
                "replyto": "Lvs5bj5dr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer UX5a"
                    },
                    "comment": {
                        "value": "As a reviewer also, I believe this is constructive to the whole community, whatever you change the score or not (which is your freedom). Thanks.\n\nBy the way, we have to mention that your words **\"but the paper has to be rebuilt in my opinion.\"** is one of the arrogant words we have never seen before!"
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486211524,
                "cdate": 1700486211524,
                "tmdate": 1700501451776,
                "mdate": 1700501451776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Twz4UoRAAI",
                "forum": "Y0yz1pmVfE",
                "replyto": "Lvs5bj5dr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appology to Reviewer UX5a"
                    },
                    "comment": {
                        "value": "We appologize for our quite aggresive words yesterday, due to our bad mood. Hope you can understand us.\n\nHowever, we still do not totally agree with your suggestion on emphasizing results (takeaways) through weakening the thinking process. We believe the thought of our paper is more significant than the resulting algorithm. We may have very different philosophy in research, but no one is incorrect.\n\nNevetheless, we may attempt to find a trade-off between our thoughts in our next revised paper, to convince you (if it is likely that you will be still a reviewer for our paper in the next venue).\n\nThanks for your service again!"
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560450805,
                "cdate": 1700560450805,
                "tmdate": 1700562403246,
                "mdate": 1700562403246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TC770BAZNj",
            "forum": "Y0yz1pmVfE",
            "replyto": "Y0yz1pmVfE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_XvTp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4187/Reviewer_XvTp"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates the ad hoc teamwork (AHT) problem. The work introduces coalitional affinity game (CAG) and its Bayesian variant to characterize the AHT problem with open teams. The work further designs the solution concept and an algorithm to tackle the problem. The work mainly compares its performance with graph-based policy learning (GPL). The work even provides an external link to show some gifs about their experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work combines open SBG and CAG to propose open-SB-CA-G. This combination seems natural. The work demonstrates that open CAG is well compatible with CAG.\n2. The work subsequently formulate the problem of finding the optimal stationary policy into reinforcement learning. This cast is very natural and it obtains a variant of the Bellman optimality equation.\n3. The work provides some lemmas and a significant amount of experimental results."
                },
                "weaknesses": {
                    "value": "Considering that the idea of combining open SBG and CAG is quite natural, and so does the RL formulation, the contribution of the work will concentrate on its practical performance. The amount of testing environments seems a bit lacking, and the difference of the outperformance seems marginal."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4187/Reviewer_XvTp"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699529931684,
            "cdate": 1699529931684,
            "tmdate": 1700659301154,
            "mdate": 1700659301154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JsUfIEM6mm",
                "forum": "Y0yz1pmVfE",
                "replyto": "TC770BAZNj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XvTp"
                    },
                    "comment": {
                        "value": "1. **Considering that the idea of combining open SBG and CAG is quite natural, and so does the RL formulation.**\n    \n    *Reply:* Thanks for your very critical comments. However, we would like to argue that the combination of SBG and CAG is not trivial or natural. The primary reason is that SBG is actually following the **non-cooperative game theory** framework, while CAG is under the **cooperative game theory**. These are totally different concepts, where SBG follows the solution concept of Nash equilibrium, whereas OSB-CAG or CAG follows the solution concept of strict core stability. In Section 4, we show the equivalence between these two concepts, then it establishes the connection between SBG and OSB-CAG. Therefore, it not only makes the GPL as an implementation of our framework, but also understands GPL from a theoretical perspective. Note that the original GPL only intuitively (empirically) extends the framework from SBG with a graph to handle open team settings without any theoretical understanding. **More importantly, according to the theory from CAG, we propose to set the range of individual utilities and pairwise utilities as greater than or equal to zero, which was never mentioned in GPL.**\n\n2. **the difference of the outperformance seems marginal.**\n\n    *Reply:* Thanks for your comments again. If possible, could you please mention the concreate cases where our method only outperforms GPL marginally? In our understanding, CAG-GPL (our method) ourperforms GPL in every scenario, and with a large margin in most of scenarios, which is also recognized by Reviewer eRVD."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232651851,
                "cdate": 1700232651851,
                "tmdate": 1700232651851,
                "mdate": 1700232651851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TY3k0Z0AX2",
                "forum": "Y0yz1pmVfE",
                "replyto": "TC770BAZNj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_XvTp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4187/Reviewer_XvTp"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "I thank the authors for the response.\n\nRegarding the margin, I believe Figure 2a, 2b, 2f have almost no margin, and the margin is literally \"marginal\" in 2c, 2d, 2e. Correct me if you do not believe so.\n\nI also read through other reviews and the discussions. I was a bit surprised that the authors decided to position the work as a \"theoretical framework\", which seems to be quite ambitious. The presentation did introduce many ambiguities and could this be one of the reasons? It is a natural question how much does this work contributes to *theory*."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656588324,
                "cdate": 1700656588324,
                "tmdate": 1700656588324,
                "mdate": 1700656588324,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]