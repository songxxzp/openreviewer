[
    {
        "title": "A Logical Framework for Verification of AI Fairness"
    },
    {
        "review": {
            "id": "o9Kw26v8uD",
            "forum": "a9xZqOqzEW",
            "replyto": "a9xZqOqzEW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission839/Reviewer_Wzut"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission839/Reviewer_Wzut"
            ],
            "content": {
                "summary": {
                    "value": "This paper is devoted to analysing ML model fairness and proposes a simple\napproach to determine whether (and how much) a model is biased towards/against\nsome groups of entities based on so-called spider diagrams. The idea is as\nfollows: given a dataset, one can compute the ratio of discrepancy between the\nexpected outcomes and actual outcomes reported by the model. If one\nadditionally measures similarity between various entities in the dataset, such\ndiscrepancy observed for similar entities gives us an indication that a bias\nis present. The authors claim a few theoretical results and propose an\nalgorithm that computes the \"degree of bias\" and evaluate their ideas\nexperimentally. Experimental results demonstrate that the proposed approach is\ncomputationally more efficient than confusion matrices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper seems clearly written. The ideas are simple and, as a result, easy\n  to follow.\n- Visualization of fairness / bias based on spider diagram looks nice.\n- The proposed approach works faster than the alternative based on confusion\n  matrices."
                },
                "weaknesses": {
                    "value": "- The first weakness is intertwined with one of the strengths, which is the\n  simplicity of the ideas. Unless I overlook something important, they look\n  too plain for a conference of this level. I mean computing the discrepancy\n  ratio between the actual outputs of the model and what is expected based on\n  the dataset and our (heuristic) similarity measure seems rather\n  straightforward to me.\n\n- The paper does not argue why this measure of fairness is valuable. For\n  instance, it is unclear to me what happens in the case when a dataset we\n  start from is biased on its own. There should be a way to alleviate this by\n  sacrificing model accuracy but the authors do not discuss this nor they say\n  how data bias affects their fairness measure.\n\n- The paper fails to relate with the state of the art in fairness analysis,\n  inclding previous works on the use of logic. For example, these papers and\n  references therein:\n\n  [A] Alexey Ignatiev, Martin C. Cooper, Mohamed Siala, Emmanuel Hebrard, Jo\u00e3o\n  Marques-Silva: Towards Formal Fairness in Machine Learning. CP 2020: 846-867\n\n  [B] Ulrich A\u00efvodji, Julien Ferry, S\u00e9bastien Gambs, Marie-Jos\u00e9 Huguet,\n  Mohamed Siala: FairCORELS, an Open-Source Library for Learning Fair Rule\n  Lists. CIKM 2021: 4665-4669\n\n  [C] Julien Ferry, Ulrich A\u00efvodji, S\u00e9bastien Gambs, Marie-Jos\u00e9 Huguet,\n  Mohamed Siala: Improving fairness generalization through a sample-robust\n  optimization method. Mach. Learn. 112(6): 2131-2192 (2023)\n\n- In my view, the presented experimental results are rather weak - the speedup\n  on N milliseconds compared to M milliseconds does not look important. The\n  authors argue that they reduce the number of function calls but I fail to\n  see why this is significant to them if the performance of the tool is only\n  slightly better than that of the competitor. There is no discussion /\n  comparison of the proposed metric and the corresponding approach in terms of\n  the quality of the produced fairness assessment."
                },
                "questions": {
                    "value": "- How is your fairness metric affected by dataset (not model) bias?\n\n- How does your work relate with state of the art in logic-based fairness\n  analysis?\n\n- What function calls are meant here? Why are they important?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission839/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712272531,
            "cdate": 1698712272531,
            "tmdate": 1699636011682,
            "mdate": 1699636011682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yfab1pkHmA",
                "forum": "a9xZqOqzEW",
                "replyto": "o9Kw26v8uD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1.How is your fairness metric affected by dataset (not model) bias? \n     \n- We have showcased the correlation between accuracy and fairness resulting from our methodology in Appendix F on page 17 (Figure 8).  We have added an explanation as follows: \u201cFigure 8 illustrates the relationship between accuracy and fairness calculated across 5 distinct datasets. Each bar in the figure corresponds to the accuracy of a specific dataset, while the connecting lines depict the fairness values. The result suggests a correlation where fairness tends to align with accuracy.\u201d (Appendix F, Para 1)Our findings suggest a close relationship between accuracy and fairness when employing our approach to detect bias within an AI model. While dataset bias often necessitates a tradeoff between accuracy and fairness, our current study predominantly addresses model bias rather than dataset bias. However, we acknowledge the importance of investigating dataset bias as a potential future avenue, as highlighted in the second paragraph of our conclusion \u201c Adding to this, the study can be extended to investigate accuracy- fairness tradeoff in case of dataset bias\u201d.\n\n2. How does your work relate with state of the art in logic-based fairness analysis? \n\n- It may be noted that  current logic-based fairness analysis evaluates the fairness of models through sensitive attributes \n(Counterfactual fairness, FTU). In our work, we emphasize statistical measures for this purpose. Please find the below-mentioned \nreferences:\n  1. Alexey Ignatiev, Martin C. Cooper, Mohamed Siala, Emmanuel Hebrard, Jo\u00e3o Marques-Silva: Towards Formal Fairness in \n   Machine Learning. CP 2020: 846-867\n  2. Kusner, Matt J., Joshua Loftus, Chris Russell, and Ricardo Silva. \"Counterfactual fairness.\" Advances in neural information \n    processing systems 30 (2017).\n\n\n3. What function calls are meant here? Why are they important? \n   \n- We have added clarification of this matter in (section 4 page 8 para 2).\"Here the number of recursion or function calls can be crucial in assessing a model's performance for a few reasons. Firstly, it indicates the computational load and efficiency of the model. A high number of recursive calls can suggest increased computational complexity, potentially leading to longer processing times or resource- intensive operations.\" Additionally, we are providing references that emphasize the need for function calls and explain the importance of employing \noptimization methods.  These references are mentioned in the draft also.\n\n   1. Ousterhout JK. Optimizing Program Performance\n\n   2. Grzeszczyk, M.K., 2018. Optimization of Machine Learning Process Using Parallel Computing. Advances in Science and \n       Technology. Research Journal, 12(4)\n\n   3. Nima Asadi, Jimmy Lin, and Arjen P De Vries. Runtime optimizations for tree-based machine\n       learning models. IEEE Transactions on Knowledge and Data Engineering, 26(9):2281\u20132292, 2013."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission839/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660202495,
                "cdate": 1700660202495,
                "tmdate": 1700661276291,
                "mdate": 1700661276291,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VIaUHiMqTz",
            "forum": "a9xZqOqzEW",
            "replyto": "a9xZqOqzEW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission839/Reviewer_cYyx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission839/Reviewer_cYyx"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops an approach to evaluating AI fairness using spider diagrams, a visualization of monadic first-order logic with equality based on Venn diagrams. Experiments are done showing that the current method is superior to some existing (confusion matrix) in terms of processing time and function calls required."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach and the use of spider diagrams is novel. The problem is timely and well-situated within the literature."
                },
                "weaknesses": {
                    "value": "I cannot clearly understand the contribution from the paper as currently written. I'm not an expert in the area of fairness and no doubt this is part of the reason, but I also think the presentation has a lot of issues.\n\nSection 2.1 introducing the diagrams is not clear. Please expand, including formal definitions and (especially) informal examples. Just adding the note that this is equivalent to monadic FOL with equality would be really helpful. I expect that this is not going to be familiar to most (including myself); I had to consult external references, and this should really be self-contained.\n\nCan the authors simply use first-order logic instead? This is going to be familiar to a lot more readers. I do not understand what about the approach relies on spider diagrams specifically. E.g., is it claimed that they are more intuitive? Then there should be an example showing how they add to that. I saw that Appendix E just uses Venn diagrams, there is no need to add spiders or anything else."
                },
                "questions": {
                    "value": "- What is phi in Theorem 1? Is this the psi from semantics for spider diagrams? Needs to be self-contained\n\nMinor comments:\n- page 3: \"where each instance is a tuple ...\" In the tuple, \"yhat in 0, 1\" should be \"yhat in {0, 1}\"\n- Definition 2: forall quantifier in S should probably just be in the set, i.e., {(e_i, a_i): e_i in E, a_i in A, i = 1, ..., N}"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission839/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810993148,
            "cdate": 1698810993148,
            "tmdate": 1699636011585,
            "mdate": 1699636011585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lQ2zv25kB2",
                "forum": "a9xZqOqzEW",
                "replyto": "VIaUHiMqTz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. What is phi in Theorem 1? Is this the psi from semantics for spider diagrams? Needs to be self-contained \n\n     - Phi is from the semantics of the spider diagram which basically means a zone outside the contours (sets). Please see the reference for further details. \n       \n     - Gem Stapleton, John Taylor, Simon Thompson, and John Howse. The expressiveness of spider diagrams augmented with \n       constants. Journal of Visual Languages & Computing, 20(1):30\u201349, 2009.\n\n2. Please expand, including formal definitions and (especially) informal examples. Just adding the note that this is equivalent to monadic FOL with equality would be really helpful.\u202f \n \n      - We have added \"Spider diagrams are higher-order representations of Venn diagrams and are equivalent to monadic FOL with \n       equality\" in the draft (subsection 2.1, para 1).\n\n3. I do not understand what about the approach relies on spider diagrams specifically. E.g., is it claimed that they are more intuitive? Then there should be an example showing how they add to that. \n \n      - We have included a demonstration that shows the working of our method using a toy example in Appendix B  page 15 (Figure 7). \n\n4. page 3: \"where each instance is a tuple ...\" In the tuple, \"yhat in 0, 1\" should be \"yhat in {0, 1}\" \n \n     - Thank you for bringing this to our attention. We have changed in the draft. \n \n5. Definition 2: forall quantifier in S should probably just be in the set, i.e., {(e_i, a_i): e_i in E, a_i in A, i = 1, ..., N} \n \n     - Thank you for bringing this to our attention. We have changed in the draft."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission839/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661183522,
                "cdate": 1700661183522,
                "tmdate": 1700661183522,
                "mdate": 1700661183522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VFupaer5sL",
                "forum": "a9xZqOqzEW",
                "replyto": "lQ2zv25kB2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission839/Reviewer_cYyx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission839/Reviewer_cYyx"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I maintain my current rating. I am still not able to grasp the approach and contribution completely."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission839/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728696765,
                "cdate": 1700728696765,
                "tmdate": 1700728696765,
                "mdate": 1700728696765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HyEKnoZkVy",
            "forum": "a9xZqOqzEW",
            "replyto": "a9xZqOqzEW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission839/Reviewer_iDyr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission839/Reviewer_iDyr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a logical framework  \"FairAI\" based on \"spider\"  - a generalisation of the Venn diagrams- as an alternative fairness metrics (alternative to equalised odds, statistical disparity etc,) , and experimentally show that their approach is   by large more performant compared to previous approaches in terms of function calls and performance times."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "My apologies but I am unable to list any."
                },
                "weaknesses": {
                    "value": "- Not clear which AI model that the authors use. (some AI model based on an ArXiv paper)\n\n- How is the threshold chosen (well average), and why such expected outcome should behave nicely across all groups is not clear.. \n\n-Counter-factual/Causal fairness  metrics  are totally disregarded. \n\n- Exposition has so many flaws, even if the results were significant, in its current form it would be hard to justify that it should be published. \n\n-  Theorems are almost trivial, and I don't see any \"verification\" to be honest. Overall  I have strong doubts about the correctness of the approach, let alone significance of the results."
                },
                "questions": {
                    "value": "I don't have any."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I don't have any."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission839/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699205409084,
            "cdate": 1699205409084,
            "tmdate": 1699636011502,
            "mdate": 1699636011502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6OZkGLLiQj",
                "forum": "a9xZqOqzEW",
                "replyto": "HyEKnoZkVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. Counter-factual/Causal fairness metrics are totally disregarded. \n \n- This paper primarily focuses on statistical fairness. We have added clarification of this matter in Section 1 para 2 as follows: \n\"In the realm of evaluating fairness in an AI model, there are multiple approaches. These include statistical measures, individual fairness considerations, Fairness Through Unawareness (FTU), counterfactual or causal fairness, and logic-based approaches. It's important to note that in the case of counterfactual fairness, a scenario where, for instance, the gender of an individual is hypothetically changed to a different value would lead to differences in other features as well. This complexity arises due to the interconnected nature between sensitive and non-sensitive attributes, making it challenging to accurately assess bias. Likewise, in the case of Fairness Through Unawareness (FTU), when certain features are linked or correlated with sensitive attributes, a model that overlooks these sensitive features doesn't guarantee fairness (Castelnovo et al., 2022). Hence this paper primarily focuses on statistical fairness criteria. \"\n In this approach, individual fairness will be spoiled as these methods don't depend on the actual scenario (ground truth) to evaluate fairness.\n\n2. Theorems are almost trivial, and I don't see any \"verification\" to be honest \n \n- Please note that understanding the theorems becomes more straightforward when there's a clear understanding of spider diagrams, their application in visualizing bias, and confirming fairness. We perceive it as a strength rather than a weakness."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission839/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661452501,
                "cdate": 1700661452501,
                "tmdate": 1700661452501,
                "mdate": 1700661452501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z5thgKDGf7",
            "forum": "a9xZqOqzEW",
            "replyto": "a9xZqOqzEW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission839/Reviewer_VPjM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission839/Reviewer_VPjM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework for visualizing and verifying the fairness of AI using spider diagrams.\n\nUnfortunately, I was not able to understand some very crucial and important aspects of the paper despite several attempts. This is not helped by several typos and grammatical errors, including using the singular form where perhaps the authors meant to use the plural form, making it very confusing to read. I found myself constantly trying to guess what the authors mean to say. I am happy to revise my review if the authors can help clarify things. I will try my best to state how I interpreted the paper, in the hope that the authors can jump in and help clarify if I get something wrong. \n\nThe authors consider a setting where we are given a dataset D. Each instance in the dataset has an actual label belonging to the set A. An AI model M (a function) maps each instance to a label, where the expected label for an instance belongs to the set E. The instances in D, together with their actual and expected labels are used to create a spider diagram. Each spider represents an instance. My understanding is that for each instance i, there is a spider, and the feet of the spider represent its actual or expected label, with a tie connecting the feet for the same instance. Now, if an instance i has a foot in the intersection of A and E, it means M correctly labels instance i. The degree of bias of M is described by comparing across the different classes (e.g. of a protected attribute) the frequencies of spiders (corresponding to instances of each class) that do not have a foot in the intersection of the sets E and A in the spider diagram.\n\nOverall, I think the paper would benefit greatly from the addition of simple toy examples to illustrate the usefulness of the proposed framework. I suggest adding an example of a binary classification task, with a dataset D with a small number of instances, with the actual labels, a simple biased model, and expected labels described clearly, and showing step by step how the spider diagram helps illustrate the bias of the model."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The use of spider diagrams and the proposed logical framework appears to be novel if it is sound. Unfortunately, I was unable to verify this.\n- If sound, the proposed approach appears to be a promising visualization tool to identify bias."
                },
                "weaknesses": {
                    "value": "The following are weaknesses in either the technical aspects or presentation of the paper, which if addressed may make the paper easier to understand. I will try to list them as they are encountered while reading the paper from the beginning.\n- In the abstract, it is not clear what is meant by actual outcome and expected outcome. Specifically, the phrases used are \"actual outcome of the algorithm\" and \"expected outcome of the model\". Is the actual outcome determined by an algorithm, or does it refer to some ground truth about the instance (say, determined by a target function)? It is actually not clear what is meant by model here. Is it a set of hypotheses, or is it a single hypothesis function? What is meant by expected outcome of the model? Does the model describe a probability distribution over possible labels? Or is it that depending on the available dataset (generated from some input distribution), a different function is learned? What is meant by algorithm and model here? How are they different? By model, do you mean the nearest-neighbor / similarity based method described in Section 2, Page 3?\n- I suggest changing the notation of the set of expected outcomes, as \\mathbb{E} is typically used to represent the expected value of a random variable.\n- Page 2, para 2: \"compares the set of expected outcome E to the set of actual outcome A\". Do you mean to use the singular or plural here? e.g. set of expected outcomes. In a binary classification task, what are E and A?\n- Page 2, Section 2: \"two groups of output\"? Do you mean protected attribute values? Typically the output refers to the predicted label. What am I missing?\n- Page 3, para 1: \"protected groups are advantaged ...\" Is this an assumption, or a requirement? It is not clear what is meant here.\n- Page 3, para 2: \"generator\": Is denoted by p, but then does not appear in Eq. (1). Is Q_1 the same as p? The sensitive attribute is denoted s, but then is not mentioned or discussed later in the paper.\n- There is also a claim here: \"If two entities Q_1 and Q_i are similar, the expected outcome of both ... should be classified into the same class label depending on the class of Q_1\" This reads like a very strong assumption about the problem setting. Consider e.g. the setting where there is a single integer attribute, and all instances with odd value for the attribute have ground truth label 0 and all even instances have ground truth label 1. How do we handle such a problem?\n- In Eq. (1), what does 'n' refer to? Earlier, 'n' was used as a variable to index the instances. Here, its use seems different.\n- What is the threshold for deciding whether the similarity between Q_1 and Q_i is sufficient to assign Q_i the same label as Q_1?\n- Page 3, last 2 lines: \"each closed curve is used to represent sets\": Do you mean multiple sets or a single set? If multiple, how to intepret this statement?\n- I did not find the discussion of Section 2.1 to be helpful. Referring back to the original papers by Howse et al. helped clarify some things, and I can see how spider diagrams are useful for logical reasoning, but I am unable to completely understand its use in evaluating an AI model. An example that builds from a toy AI problem with a small dataset and a simple biased model would be greatly appreciated.\n- Definition 1. Do you mean to say for each expected label e_i, there exists an actual label a_i, such that e_i = a_i? Could you illustrate how this works using the example of a binary classification problem? Can an instance i have multiple expected and actual labels? Is it possible for an instance to have an expected label but no actual label or vice-versa?\n- With a few assumptions, I can possibly see how in Section 3, the proposed algorithm can be used to compute the degree of bias. However, it would help to clarify the presentation and provide a running example to remove any ambiguities.\n- Figure 3 could be used to show the actual spider diagrams in addition to the bar plots."
                },
                "questions": {
                    "value": "Please see the questions in the comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission839/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission839/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission839/Reviewer_VPjM"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission839/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699528397755,
            "cdate": 1699528397755,
            "tmdate": 1699636011416,
            "mdate": 1699636011416,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g6z1tfMNbU",
                "forum": "a9xZqOqzEW",
                "replyto": "Z5thgKDGf7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission839/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1.In the abstract, it is not clear what is meant by actual outcome and expected outcome. Specifically, ...\n \n- We have changed the phrase to \"This framework compares the sets representing the actual outcome of the model and the expected outcome to identify bias in the model\" (Abstract). \"The expected outcome (ground truth) of the model is obtained by considering the similarity score between the individuals (how much alike the elements are, calculated based on the distance between the values of corresponding features). The actual outcome is the outcome given by the model's prediction\" (Section 1 para 3)\u201d. Model in this paper refers to a classification model and the definition is given in section 2 para 1. \n\n2. I suggest changing the notation of the set of expected outcomes, as \\mathbb{E}.\n \n- We have changed the notation to \"\\mathbb{G}\". \n\n3. Page 2, para 2: \"compares the set of expected outcome E to the set of actual outcome A\". \n\n- We have changed to  \" To verify fairness in the model, the framework compares the set of the expected outcomes (\"\\mathbb{G}) to the set of the actual outcomes (\"\\mathbb{A})\" (section 1 para 3).\n \"In this paper, we introduce the notations \\mathbb {G} and \\mathbb{A} to denote the sets of expected and actual outcomes respectively, produced by an AI model\". (section 2 para 1)\n\n4. Page 2, Section 2: \"two groups of output\"? \n\n- The term \u201ctwo groups of output\u201d was initially used to represent two demographic groups in the model. In the revised draft, we have changed to \"In this study, we use an AI model defined by Das & Rad (2020) with two demographic groups (subpopulation)\u2014non-protected (\u03b1) and protected (\u03c9)\u2014based on the sensitive attribute(s)\" for better clarity (Section 2 para1). \n \n5. Page 3, para 1: \"protected groups are advantaged ...\" Is this an assumption... \n \n- In an AI model, based on case studies such as COMPAS and Amazon\u2019s recruitment algorithms, there exist two or more demographic subpopulations that can influence a model\u2019s outcomes. The group for which the model's outcome aligns favorably is termed the \"protected group,\" while the group where the outcome is unfavorable or doesn't align with the ground truth is termed the \"non-protected group.\" In this scenario, we can describe the protected groups as benefiting from the model's predictions. This is an assumption that aligns with past experiences. (Section 2 para 1) (Case studies are referenced in the introduction of this work (Section 1 para 1)). \n\n6. Page 3, para 2: \"generator\": Is denoted by p...\n\n- The symbol 's' denotes the sensitive attribute.  To avoid confusion, the sentence is rephrased as follows:\" Let $a_1, a_2,\\ldots,a_m$ be the attributes, that include both sensitive attributes (i.e. race, ethnicity, sex) and non-sensitive attributes in the model\" (section 2 para 2). Q_1 is the same as p. We have changed in the draft. \n \n7. There is also a claim here: \"If two entities Q_1 and Q_i ... \n \n-  If a specific condition, like the one described earlier, helps the classification process for individuals, the proposed approach can be applied to ground truth value and categorize individuals. In this context, for a single integer attribute, we can compute the Euclidean distance between two points. Depending on this distance value, if it's odd, the individual is classified into class 0; otherwise, they're classified into class 1.\n \n8. In Eq. (1), what does 'n' refer to? \n \n- We have changed 'n' to 'm'. Here 'm' denotes the total number of attributes in the model.  (Section 2 para 2 page 3).\n \n9. What is the threshold for deciding whether the similarity...\n \n- The average value of Euclidean distance between the two individuals. (Discussed in page 3 para 3)  \n\n10. Page 3, last 2 lines: \"Each closed curve is used to represent sets\"...\n\n- We have changed to \" Spider diagrams are higher-order representations of Venn diagrams and are equivalent to monadic FOL with equality. Here each closed curve is used to represent a set and is labeled and enclosed in a rectangle.\" in the draft (Section 2.1 para 1). \n \n11. I did not find the discussion of Section 2.1 to be helpful...\n \n- We have included a demonstration that shows the working of our method using a toy example in Appendix B  page 15 (Figure 7).\n\n \n12. Definition 1. Do you mean to say for each expected label...\n \n- An instance \"i\" can have only one expected outcome (i.e., ground truth) and one actual outcome (i.e., predicted outcome). This is demonstrated with a toy example using a small example dataset (Appendix B  page 15 (Figure 7)). \n \n13. With a few assumptions, I can possibly see how in...\n \n- We have included a demonstration that shows the working of our method using a toy example in Appendix B  page 15 (Figure 7). It demonstrates the method used in calculating the degree of bias. \n \n \n14. Figure 3 could be...\n- Appendix E, figure 9 presents the actual spider diagrams illustrating bias visualization across five datasets, in addition to the information provided in Figure 3."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission839/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662644852,
                "cdate": 1700662644852,
                "tmdate": 1700662644852,
                "mdate": 1700662644852,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]