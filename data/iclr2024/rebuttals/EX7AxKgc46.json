[
    {
        "title": "Improved Generalization of cGAN using Vicinal Estimation and Early Stopping"
    },
    {
        "review": {
            "id": "HpoRICwkRb",
            "forum": "EX7AxKgc46",
            "replyto": "EX7AxKgc46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_JCHw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_JCHw"
            ],
            "content": {
                "summary": {
                    "value": "The paper improves conditional GAN learning using Vicinal Estimation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea to introduce a auxiliary distribution is interesting."
                },
                "weaknesses": {
                    "value": "I was still confused about how the auxiliary distribution helps after reading the paper, because I'm not familiar with related works."
                },
                "questions": {
                    "value": "The idea to use an auxiliary distribution is similar to importance sampling, are they related somehow?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555795241,
            "cdate": 1698555795241,
            "tmdate": 1699636275932,
            "mdate": 1699636275932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wGW2twKfD2",
                "forum": "EX7AxKgc46",
                "replyto": "HpoRICwkRb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JCHw"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer **JCHw** for the detailed comments and hope to address all of the questions and concerns below.\n\n> I was still confused about how the auxiliary distribution helps after reading the paper, because I'm not familiar with related works.\n\nThere are two main difficulties widely known in the literature in obtaining the generalization error of cGANs: Curse of Dimensionality and Lack of Conditional Sample. In one of the related works, (Weinen & E, 2021), a framework to solve Curse of Dimensionality in unconditional GAN has been published, but it is not possible to apply it to conditional GAN as it is. Technically, a conditional GAN is a collection of unconditional GANs for all label values, but the number of samples assigned to each of these unconditional GANs is extremely small compared to the total number of samples.\n\nThe intuitive explanation for the contribution of vicinal estimation (auxiliary distribution) to this problem is that, unlike the original empirical distribution, its VE provides sufficiently many conditional samples for each (auxiliary) label value. A more mathematical explanation is that the case $p\\_r(\\cdot|\\mathbf{x}) - p\\_{X,Y}(\\cdot|\\mathbf{x})$ is generally unbounded, but its VE, $\\tilde{p}\\_r(\\cdot|\\mathbf{x}') - \\tilde{p}\\_{X, Y}(\\cdot|\\mathbf{x}')$ is bounded by $||\\tilde{q}||\\_\\infty$, making it possible to obtain a PAC bound like **Theorem 2** (Please refer to the proofs for the more details).\n\n> The idea to use an auxiliary distribution is similar to importance sampling, are they related somehow?\n\nThe \"auxiliary distribution\" in importance sampling is the distribution function chosen similar to the target distribution to be sampled, whereas auxiliary distribution in our manuscript rather means the mapping between label space and auxiliary label space. Despite sharing the name \"auxiliary distribution\", we do not consider these two concepts to have anything in common."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123777801,
                "cdate": 1700123777801,
                "tmdate": 1700123777801,
                "mdate": 1700123777801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tktTbgcYrT",
            "forum": "EX7AxKgc46",
            "replyto": "EX7AxKgc46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_ushT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_ushT"
            ],
            "content": {
                "summary": {
                    "value": "The paper deals with the problem of sampling from the conditional distribution of $Y$ given $X=x$, based on a sample of size $N$ denoted by $(X_1,Y_1),\\ldots, (X_N,Y_N)$. The Vicinal Estimation method is introduced that, roughly speaking, consists in replacing the original problem by that of estimating the conditional distribution of $Y$ given $X'=x'$, where $X'$ is a random label artificially generated from a conditional distribution $q(.|X = x)$. The main contribution of the paper is theoretical: it provides an upper bound on the distance between the learned conditional density and the true conditional density. Taking the set of discriminators to be equal to a subset in the Barron space, it is shown that it is possible to get an error that behaves itself as $N^{-1/(6+2d_X)}$, where $d_X$ is the dimension of the set of the labels $X$."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Studies an important problem. \n2. Obtains a new bound on the error of conditional sampling"
                },
                "weaknesses": {
                    "value": "1. Section 4, which is one of the most important ones is poorly written. The definition of the generator that is analyzed is not given in full detail. In particular, neither the set of the discriminators nor the set of the generators involved in the min-max problem of the GAN are clearly specified. In addition, it is not clear in Eq 19 what is understood by $\\tilde p_{X,Y}(y|x')$.  \n2. Section 5 is very dense and hard to read. The results are quite technical so it is impossible to say whether they are plausible or not without checking the proofs line by line. Given the limited time I had to review the paper, I could not check the proofs provided in the supplementary material. I believe that it would be much better for such a paper to be submitted to a journal, where more space may be used for stating the main theorems and providing some explanations and intuitions, as well as more time could be left to the reviewers for checking the details of the proofs.\n3. It seems that the results of the paper do not imply that the vicinal estimation method is better than the vanilla cGAN. Indeed, the fact that the rate of convergence does not depend on the dimension of Y might very well be a consequence of the choice of the set of discriminators. In particular, it is well known that the lower bound (1) is due to the fact that the set of discriminators defining the $W_1$ (which is a lower bound on $W_2$) -- the set of 1-Lipschitz functions -- is too large.  Replacing this set by an RKHS with a bounded kernel leads to the dimension independent rate $N^{-1/2}$."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625813245,
            "cdate": 1698625813245,
            "tmdate": 1699636275840,
            "mdate": 1699636275840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hiyWDfC4tE",
                "forum": "EX7AxKgc46",
                "replyto": "tktTbgcYrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ushT"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer **ushT** for the detailed comments and hope to address all of the questions and concerns below.\n\n> Section 4, which is one of the most important ones is poorly written. The definition of the generator that is analyzed is not given in full detail. In particular, neither the set of the discriminators nor the set of the generators involved in the min-max problem of the GAN are clearly specified. In addition, it is not clear in Eq 19 what is understood by $\\tilde{p}\\_{X,Y}(y|x')$.\n\nThe details of the GANs used in the proposed framework are described in Section 4. Specifically, they are as follows:\n\n- Set of the discriminator : $\\tilde{D}\\_a$ defined by Equation (14) parameterized by $a$\n- Set of the generator : For an arbitrary generator $G$, we use **Algorithm 2** to sample from $\\tilde{p}\\_g(\\cdot|\\mathbf{x}')$. So the set of generators is not limited.\n- $\\tilde{p}\\_{X,Y}(y|x')$ is a conditional distribution, which is induced from the joint distribution $\\tilde{p}\\_{X,Y}$ which is a VE of the empirical distribution. We have added the desciption for $\\tilde{p}\\_{X,Y}$ in the revised manuscript.\n\n> Section 5 is very dense and hard to read. The results are quite technical so it is impossible to say whether they are plausible or not without checking the proofs line by line. Given the limited time I had to review the paper, I could not check the proofs provided in the supplementary material. I believe that it would be much better for such a paper to be submitted to a journal, where more space may be used for stating the main theorems and providing some explanations and intuitions, as well as more time could be left to the reviewers for checking the details of the proofs.\n\nTo the best of the authors' knowledge, there are many theoretic papers in ICLR, including generalization error analysis which our paper focuses on. While the journal will be more suitable for the papers with complex mathematical proofs, we believe that our manuscript does not contain such complex proofs as the proofs are not extremely long (3 pages in appendices) and do not involve extremely complex mathematical concepts. Our proposed model is validated by the experimental results which also support the theoretical analysis.\n\n> It seems that the results of the paper do not imply that the vicinal estimation method is better than the vanilla cGAN. Indeed, the fact that the rate of convergence does not depend on the dimension of Y might very well be a consequence of the choice of the set of discriminators. In particular, it is well known that the lower bound (1) is due to the fact that the set of discriminators defining the $W\\_1$ (which is a lower bound on $W\\_2$\n) -- the set of 1-Lipschitz functions -- is too large. Replacing this set by an RKHS with a bounded kernel leads to the dimension independent rate $N^{-1/2}$.\n\nFirst of all, we are not aware of any results showing that the generalization error in $W\\_1$ can be bounded to $N^{-1/2}$ by RKHS kernel regularization, as you mention in the last sentence. To the best of the authors' knowledge, Bias Potential Model (2021) was almost the first to achieve dimension-independent generalization error for unconditional GANs, except when bounding $p\\_g$ itself, and the best we know of is $N^{-1/4}$ for KL divergence and $N^{-1/6}$ for $W\\_2$.\n\nEven if dimension-independent generalization error can be easily achieved for unconditional GANs as mentioned, it cannot be extended to conditional GANs (vanilla cGANs?) due to a problem named \"Lack of Conditional Sample\" : even if a generalization error is obtained for each fixed label value, the conditional samples of that label value will be much smaller than the total number of samples.\n\nOne of the key contributions of our work is to introduce the concept of vicinal estimation, which allows us to obtain a sufficient number of conditional samples for arbitrary (auxiliary) label values."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123702998,
                "cdate": 1700123702998,
                "tmdate": 1700123702998,
                "mdate": 1700123702998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UUzpZkxbia",
                "forum": "EX7AxKgc46",
                "replyto": "hiyWDfC4tE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Reviewer_ushT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Reviewer_ushT"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors. However, I still find that the paper is poorly written and needs a substantial revision. In the current version, the lack of clarity makes it hard to assess the significance and the correctness of the mathematical results.   \n\nI repeat the example I gave in my review. The authors write \"$\\tilde D$ is trained to maximize $\\hat V'(G,\\tilde D)$ and $G$ is trained to minimize $\\hat V(G,\\tilde D)$. This is the last sentence of Section 4. Nothing in this sentence, nor in the text before, informs the reader over which sets the mentioned maximization and minimization are conducted. The reply above does not answer this question. The authors say that the discriminators are parameterized by $a$, without making it precise which set this function belongs to. If $a$ is the parameter, does it mean that one maximizes $\\hat V'$ with respect to $a$? My initial understanding was that the parameters are $w$ and $b$, and the maximization is wrt these parameters. Similarly, the answer above concerning the set over which the minimization wrt to $G$ is conducted is puzzling.   \n\nFinally, the conditional density $\\tilde p$ corresponding to the VE is defined in the paper by Eq (9). This equation is valid for joint distributions that admit a density $p$ wrt to the Lebesgue measure. The empirical distribution $p_{X,Y}$ is not absolutely continuous wrt the Lebesgue measure. Therefore, the corresponding VE should be defined without relying on densities. Of course, I am not saying that this is not possible. I am just pointing to the fact that it is not done in the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687620401,
                "cdate": 1700687620401,
                "tmdate": 1700687620401,
                "mdate": 1700687620401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JcRYOtY6ki",
            "forum": "EX7AxKgc46",
            "replyto": "EX7AxKgc46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_JqUD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_JqUD"
            ],
            "content": {
                "summary": {
                    "value": "This article studies the generalization property of conditional GANs to estimate conditional probabilities p(y|x). By adding conditional data samples with Vicinal estimation and using 1-layer neural network discriminator, the proposed model is able to achieve a Wasserstein error which does not grow with the dimension of y. As long as the dimension of of x is small, it overcomes the curse-of-dimensionality when the dimension of y is large."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This article makes a creative combination of 2 existing ideas: one based on 1-layer neural network discriminator based on Barron space and early stopping in learning to achieve dimension-free generalization. The other idea is based on vicinal estimation to define a new training loss to increase effective number of samples."
                },
                "weaknesses": {
                    "value": "The theoretical results are somehow not very clear, making it hard and inconsistent to understand how the whole idea is connected to the Wasserstein-2 error used in the evaluation of cGANs. I would suggest a clearer explanation in the rebuttal phase."
                },
                "questions": {
                    "value": "-\tWhat is the functional space of the discriminator D(y,x) in eq. 5, and how is it related to the W-1 distance in eq. 6? I suppose you are assuming the (y,x) -> D(y,x) is Lipschitz, but for the W-1 distance, we only need y -> D(y,x) to be Lipschitz for any x. \n-\tThe tilde D_alpha(y,x\u2019) defined in eq 14, is it also Lipschitz? I do not see why this is so (even based on eq. 22) and it seems to be inconsistent to your previous definition. \n-\tWhat is the definition of tilde p_{x,y} and p(x\u2019) in eq. 20? Is p(x\u2019) an empirical distribution as in eq. 7? It seems not to be the case according to theorem 2 and 3. Then I am confused of the over-all setting if you are the true distribution p(x\u2019) of x\u2019 in your training loss. \n-\tDo you need to assume that the kernel is characteristic in Theorem 3 ? It is not clear what are the above conditions in Theorem 5. Please make it clearer. Could you also explain why there is no d_y in the statement of Theorem 5?\n\nMinor: \n\n-\tWhy did you introduce a RKHS regulation in the loss V\u2019 in eq. 18? What would happen if there is no regulation?\n-\tWhat is y used for in eq. 25 ? I do not see it to appear on the right hand side. \n-\tHow do you get the optimal scale of ||tilde q||_inf after eq. 33 ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653707897,
            "cdate": 1698653707897,
            "tmdate": 1699636275772,
            "mdate": 1699636275772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v5bqR78Itc",
                "forum": "EX7AxKgc46",
                "replyto": "JcRYOtY6ki",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JqUD"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer **JqUD** for the detailed comments and hope to address all of the questions and concerns below.\n\n> What is the functional space of the discriminator D(y,x) in eq. 5, and how is it related to the W-1 distance in eq. 6? I suppose you are assuming the (y,x) -> D(y,x) is Lipschitz, but for the W-1 distance, we only need y -> D(y,x) to be Lipschitz for any x.\n\nAs mentioned, it is enough for $D$ to be Lipschitz continuous with respect to $\\mathbf{y}$. We have corrected the sentence.\n\n\n> The tilde D_alpha(y,x\u2019) defined in eq 14, is it also Lipschitz? I do not see why this is so (even based on eq. 22) and it seems to be inconsistent to your previous definition.\n\nIn the definition of Equation (14), $D_a(\\mathbf{y},\\mathbf{x})$ is not Lipschitz. In our proposed model, we do not need the Lipschitzness of $D$ because we are applying RKHS norm regularization rather than enforcing Lipschitz like Wasserstein cGAN.\n\n\n> What is the definition of tilde p_{x,y} and p(x\u2019) in eq. 20? Is p(x\u2019) an empirical distribution as in eq. 7? It seems not to be the case according to theorem 2 and 3. Then I am confused of the over-all setting if you are the true distribution p(x\u2019) of x\u2019 in your training loss.\n\nFor $\\tilde{p}\\_{X,Y}$ , it is a vicinal estimate of the empirical distribution defined in Equation (2), which can be obtained by substituting $\\tilde{p}\\_{X,Y}$ for $p$ in Equation (8). We clarify it before Equation (19) in the revised manuscript to indicate that $\\tilde{p}\\_{X,Y}$ is the VE of the empirical distribution.\n\nWhat you refer to as $p(\\mathbf{x}')$ in Equation (20) is $\\rho(\\mathbf{x}')$, which means a random distribution, not the actual distribution of auxiliary label values. Once $\\rho(\\mathbf{x}')$ is known, we can obtain a generalization bound. Moreover, we can consider various $\\rho(\\mathbf{x}')$ to our main results such as **Corollary 2**. For example, as mentioned in the manuscript, when $\\tilde{q}$ is a perturbation, we can substitute $p_r(\\mathbf{x})$ into $\\rho$ to get a generalization error between $p_g$ and $p_r$.\n\n\n> Do you need to assume that the kernel is characteristic in Theorem 3 ? It is not clear what are the above conditions in Theorem 5. Please make it clearer. Could you also explain why there is no d_y in the statement of Theorem 5?\n\nIn **Theorem 3**, it is not necessary for the kernel to be characteristic. As mentioned, the various conditions assumed in **Theorem 5** seem to be scattered, so we have summarized the exact conditions in **Theorem 5**. \n\nFinally, the reason why $d_\\mathbf{y}$ does not appear in **Theorem 5** and **Corollary 2** is because of the Big-O notation to emphasize the decay rate for a dataset size $N$. If we write the right-hand side correctly, it is multiplied by a sublinear term for $d_\\mathbf{y}$, which is omitted in Big-O Notation because it is a term independent of $N$.\n\n\n> Why did you introduce a RKHS regulation in the loss V\u2019 in eq. 18? What would happen if there is no regulation?\n\nThe RKHS norm regularization introduced in Equation (18) and elsewhere is a variation of prior work (Weinen & E, 2021) that introduced a dimension-free generalization error in unconditional GANs. If there is no such regularization, i.e. if the error had the form of $V'(G,\\tilde{D}) = \\mathbb{E}\\_{\\mathbf{x}' \\sim \\rho(\\mathbf{x}')} \\left[ \\mathbb{E}\\_{\\mathbf{y} \\sim \\tilde{p}\\_r(\\mathbf{y}|\\mathbf{x}')}[\\tilde{D}(\\mathbf{y},\\mathbf{x}')] - \\mathbb{E}\\_{\\mathbf{y} \\sim \\tilde{p}\\_g(\\mathbf{y}|\\mathbf{x}')}[\\tilde{D}(\\mathbf{y},\\mathbf{x}')] \\right]$, it is possible to convert it to a problem of minimizing $\\mathbb{E}\\_{\\mathbf{x}' \\sim \\rho(\\mathbf{x}')} W\\_1 (\\tilde{p}\\_g(\\cdot|\\mathbf{x}'), \\tilde{p}\\_r(\\cdot|\\mathbf{x}'))$ by assuming Lipschitzness on \\tilde{D}. In this case, we're still facing the curse of dimensionality and cannot get a reasonable generalization bound.\n\n> What is y used for in eq. 25 ? I do not see it to appear on the right hand side.\n\nNote that $\\mathbf{y}$ is used as the first argument of the kernel in Equation (25). We've omitted it because it's a notation commonly used in convolution, but it's technically equivalent to $\\int k(\\mathbf{y},\\mathbf{y}') d(\\tilde{p}_r(\\mathbf{y}'|\\mathbf{x}') - \\tilde{p}_g^t (\\mathbf{y}'|\\mathbf{x}'))(\\mathbf{y}')$.\n\n> How do you get the optimal scale of ||tilde q||_inf after eq. 33 ?\n\nIf we leave $N$ and $Q=||\\tilde{q}||\\_\\infty$ to find $||\\tilde{q}||\\_\\infty$ that minimizes the right-hand side of Equation (33), it becomes equal to $AN^{-1/6}Q^{1/3} + BQ^{-1/d_{\\mathbf{x}}}$. The $Q$ that minimizes this expression is easily obtained via differentiation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123459032,
                "cdate": 1700123459032,
                "tmdate": 1700123584634,
                "mdate": 1700123584634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jDaXvnAivC",
                "forum": "EX7AxKgc46",
                "replyto": "v5bqR78Itc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Reviewer_JqUD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Reviewer_JqUD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for addressing my questions"
                    },
                    "comment": {
                        "value": "I shall still maintain my score. A remark to improve the article is to study the case where you do not use the rho(x) in your training loss of cGANs, as this can be the true data distribution p_r (x), rather than the empirical distribution."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658323227,
                "cdate": 1700658323227,
                "tmdate": 1700658323227,
                "mdate": 1700658323227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JU7iPBbHju",
            "forum": "EX7AxKgc46",
            "replyto": "EX7AxKgc46",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_GnZt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3273/Reviewer_GnZt"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of learning a conditional distribution with a continuous input and potentially high-dimensional output, using a variant of a generative adversarial network (GAN) with a \"vicinal estimation\" mechanism to address the sparsity of observed labels in the label-space. The paper first provides theoretical guarantees showing that the proposed approach (together with an early stopping mechanism) learns the true conditional distribution in $2$-Wasserstein distance and then provides a toy experiment in which the proposed method is demonstrated to learn a synthetic conditional distribution more efficiently than a baseline cGAN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The vicinal estimation (VE) approach seems novel in the setting of conditional generation. The paper presents precise theoretical guarantees supporting the proposed method. The synthetic data experiment is clearly explained and its results are quite precise and illustrative."
                },
                "weaknesses": {
                    "value": "1. I suggest removing \"and Early Stopping\" from the title of the paper, as (a) this is hardly discussed in the paper and (b) it seems to be a direct adaptation of Yang & E (2021) to the conditional setting rather than a really novel contribution.\n\n2. Eq. (1): In this sentence, $p$ should be quantified more carefully; there are obvious counterexamples (e.g, when $p$ is a single point mass) for which this statement, as written, is false.\n\n3. I found it counterintuitive that $X$ is used for the label and $Y$ is used for the generated image. While this makes sense in terms of the inputs and outputs of a cGAN, it conflicts with much more common settings in the ML literature (e.g., image classification, where $X$ is the image and $Y$ is the label), causing me to be confused through much of the paper until I went back and re-read the beginning of Section 3. I don't necessarily suggest changing this (especially if it is consistent with other paper on conditional generative modeling), but perhaps it is worth adding a sentence to explicitly point out this possible point of confusion.\n\n4. Section 4: The notation in here seemed unnecessarily complicated. Isn't $q_x$ simply the conditional distribution of $X'$ given $X$. Why introduce a new notation rather than simply writing it as such? If I understand correctly, later parts could be written much more readably (e.g., in Lemma 1, $m_{\\tilde q}(x') = \\mathbb{E}[X|X']$ and $d_{\\tilde q}(x') = \\sqrt{\\mathbb{E}[||X - \\mathbb{E}[X|X']||^2|X']}$, etc.).\n\n5. Just before Eq. (11), a minor wording suggestion: \"a VE vanishes the conditional information\" -> \"a VE reduces the conditional information\"\n\n6. Theorem 1: Isn't $||\\tilde q^L||_\\infty = L^{-d_x} ||\\tilde q^1||_\\infty$? If so, perhaps explanding this would make the statement it a bit more intuitive.\n\n7. Although the work is intended to be theoretical, it would be strengthened by any discussion of real-world relevance (e.g., describing some applications where the proposed vicinal estimation approach might be useful). Experiments on real-world data would further strengthen the paper."
                },
                "questions": {
                    "value": "1. Just before Eq. (11)\n> uniform auxiliary distributions on $\\mathcal{X}'$ erase completely the information of labels in the data\n\nIf I understand correctly, what really matters here is how much information about $X$ is preserved by $X'$, rather than the particular distribution of $X'$. For example, doesn't any distribution of $X'$ that is independent of $X$ completely the information of labels? Or is there some reason the uniform distribution is special here?\n\n2. Just after Lemma 1\n> Note that the quantities $m_{\\tilde q}(x\u2032)$ and $d_{\\tilde q}(x\u2032)$ only depend on the auxiliary distribution $q$, not on the distribution $p$.\n\nI found this sentence confusing, as $m_{\\tilde q}(x\u2032)$ and $d_{\\tilde q}(x\u2032)$ depend on the inverse auxiliary distribution $\\tilde q$, which is related to the auxiliary distribution $q$ in a way that depends on $p$ (namely, through Eq. (10)). Perhaps this was a typo (i.e., \"auxiliary distribution $q$\" should have been \"inverse auxiliary distribution $\\tilde q$\")? Even so, I don't really understand the point of this sentence."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3273/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3273/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3273/Reviewer_GnZt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699281796309,
            "cdate": 1699281796309,
            "tmdate": 1699636275686,
            "mdate": 1699636275686,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WK9UZSzZPO",
                "forum": "EX7AxKgc46",
                "replyto": "JU7iPBbHju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GnZt (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer **GnZt** for the detailed comments and hope to address all of the questions and concerns below.\n\n> I suggest removing \"and Early Stopping\" from the title of the paper, as (a) this is hardly discussed in the paper and (b) it seems to be a direct adaptation of Yang & E (2021) to the conditional setting rather than a really novel contribution.\n\nWe have removed \"and Early Stopping\" from the title in the revised manuscript. The OpenReview post title is not currently editable, but we will edit it when it is.\n\n> Eq. (1): In this sentence, $p$ should be quantified more carefully; there are obvious counterexamples (e.g, when $p$ is a single point mass) for which this statement, as written, is false.\n\nEquation (1) requires the condition that $p$ is absolutely continuous with respect to the Lebesgue measure. We have added this condition after Equation (1).\n\n\n> I found it counterintuitive that $X$ is used for the label and $Y$ is used for the generated image. While this makes sense in terms of the inputs and outputs of a cGAN, it conflicts with much more common settings in the ML literature (e.g., image classification, where is $X$ the image and $Y$ is the label), causing me to be confused through much of the paper until I went back and re-read the beginning of Section 3. I don't necessarily suggest changing this (especially if it is consistent with other paper on conditional generative modeling), but perhaps it is worth adding a sentence to explicitly point out this possible point of confusion.\n\nIn this study, we followed the notation and considered an error analysis from the perspective of Conditional Density Estimation. For this purpose, we used $\\mathbf{x}$ to denote a label and $\\mathbf{y}$ to denote an image, as used in the studies related to Conditional Density Estimation. We have added a sentence at the beginning of Section 3 to clarify that $\\mathbf{x}$ is a label and $\\mathbf{y}$ is an image.\n\n\n> Section 4: The notation in here seemed unnecessarily complicated. Isn't $q_{x}$ simply the conditional distribution of $X'$ given $X$. Why introduce a new notation rather than simply writing it as such? If I understand correctly, later parts could be written much more readably (e.g., in Lemma 1, $m_q(x') = \\mathbb{E}[X|X']$ and $d_q(x') = \\sqrt{\\mathbb{E}[||X-\\mathbb{E}[X|X']||^2|X']}$ , etc.).\n\nThe reason for defining the value of $q$ or $\\tilde{q}$ is that we need the value of an explicit probability density function. For example, in our analysis, $||\\tilde{q}||_{\\infty}$ is an important factor in the generalization bound, and the definition of this value requires the definition of the probability density function $\\tilde{q}$.\n\n\n> Just before Eq. (11), a minor wording suggestion: \"a VE vanishes the conditional information\" -> \"a VE reduces the conditional information\"\n\nWe have modified the sentence.\n\n> Theorem 1: Isn't $||\\tilde q^L||_\\infty = L^{-d_x} ||\\tilde q^1||_\\infty$? If so, perhaps explanding this would make the statement it a bit more intuitive.\n\nJust before **Theorem 1**, we added a sentence to emphasize the fact that $d_{\\tilde{q}^L}(\\textbf{x}') \\propto L^{-1}$ and $||\\tilde{q}^L||_{\\infty} \\propto L^{-d_{\\textbf{x}}}$.\n\n\n> Although the work is intended to be theoretical, it would be strengthened by any discussion of real-world relevance (e.g., describing some applications where the proposed vicinal estimation approach might be useful). Experiments on real-world data would further strengthen the paper.\n\nIn Conditional cGAN (CcGAN), Ding et al. (2023) utilizes the concept of perturbing the label value in one-dimensional continuous labels and demonstrates the effectiveness of their proposed method for real-world conditional image generation tasks. As perturbation is a special example of vicinal estimation proposed in our manuscript, we can expect that VE and our proposed method can also improve the performance of cGAN models.\n\nThere is a technical issue to undertake a supplementary experiment for real-world data, as we require a more intricate discriminator model based on the Barron space. However, we are currently constructing a model to conduct experiments on a real-world image dataset. We will report the results if we obtain them before the rebuttal period."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123244153,
                "cdate": 1700123244153,
                "tmdate": 1700123603246,
                "mdate": 1700123603246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AaqLEcCE6X",
                "forum": "EX7AxKgc46",
                "replyto": "JU7iPBbHju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GnZt (Part 2)"
                    },
                    "comment": {
                        "value": "> Just before Eq. (11)\n>   > uniform auxiliary distributions on $\\mathcal{X}'$ erase completely the information of labels in the data\n>\n> If I understand correctly, what really matters here is how much information about $X$ is preserved by $X'$, rather than the particular distribution of $X'$. For example, doesn't any distribution of $X'$ that is independent of $X$ completely the information of labels? Or is there some reason the uniform distribution is special here?\n\nWe use the uniform auxiliary distribution as an extreme example where the information of $X$ is not preserved in $X'$.\n\n\n> Just after Lemma 1\n>   > Note that the quantities $m_{\\tilde{q}}(x')$ and $d_{\\tilde{q}}(x')$ only depend on the auxiliary distribution $q$, not on the distribution $p$.\n> \n> I found this sentence confusing, as $m_{\\tilde{q}}(x')$ and $d_{\\tilde{q}}(x')$ depend on the inverse auxiliary distribution $\\tilde{q}$, which is related to the auxiliary distribution in a way that depends on $p$ (namely, through Eq. (10)). Perhaps this was a typo (i.e., \"auxiliary distribution\" should have been \"inverse auxiliary distribution \")? Even so, I don't really understand the point of this sentence.\n\nWe have modified \"auxiliary distribution $q$\" to \"inverse auxiliary distribution $\\tilde{q}$\". The reason we emphasize this fact is that $\\tilde{q}$ is the most central function in Vicinal Estimation, as only the condition on $\\tilde{q}$ is needed to obtain these values and the Generalization Bound that follows. Moreover, only $\\tilde{q}$ is used in actual implementation in **Algorithm 2**. We've modified the sentence after **Lemma 1** to emphasize this fact."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123257499,
                "cdate": 1700123257499,
                "tmdate": 1700123609936,
                "mdate": 1700123609936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]